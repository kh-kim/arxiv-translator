# Arxiv Translation Project

이 레포는 쏟아지는 페이퍼들에 대응하기 위하여, 빠르게 Arxiv 페이퍼를 살펴볼 수 있도록 한글화된 웹페이지를 제공하는 것을 목표로 합니다.
각기 다른 형태의 PDF 파일을 번역하기 위해서, 텍스트를 추출할 때 nougat OCR 라이브러리를 활용합니다.
따라서 추출이 원활하지 않을 수 있습니다.
처음에는 Ar5iv를 번역할까 생각했지만, Ar5iv도 한달이 지나서야 페이퍼가 업데이트 되며, 최초 버전만 HTML화 하고 최종 버전은 반영되어 있지 않기 때문에, 자체적으로 내용을 추출하기로 결정하였습니다.
정확한 내용을 파악하기 위해서는 원본 페이퍼를 읽는 것을 추천합니다.

## Paper List

새 창 열기가 지원되지 않습니다. 직접 새 창으로 열기를 통해 열기를 권장합니다.

| ArXiv ID | Title | ArXiv | Go to |
|:---:|:---|:---:|:---:|
| 2404.07965v1 | Rho-1 Not All Tokens Are What You Need | [arXiv](https://arxiv.org/abs/2404.07965v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.07965v1) |
| 2404.07647v1 | Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck | [arXiv](https://arxiv.org/abs/2404.07647v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.07647v1) |
| 2404.07143v1 | Leave No Context Behind Efficient Infinite Context Transformers with Infini-attention | [arXiv](https://arxiv.org/abs/2404.07143v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.07143v1) |
| 2404.06395v1 | MiniCPM Unveiling the Potential of Small Language Models with Scalable Training Strategies | [arXiv](https://arxiv.org/abs/2404.06395v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.06395v1) |
| 2404.05875v1 | CodecLM Aligning Language Models with Tailored Synthetic Data | [arXiv](https://arxiv.org/abs/2404.05875v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.05875v1) |
| 2404.05405 | Physics of Language Models Part 33 Knowledge Capacity Scaling Laws | [arXiv](https://arxiv.org/abs/2404.05405) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.05405) |
| 2404.04167v3 | Chinese Tiny LLM Pretraining a Chinese-Centric Large Language Model | [arXiv](https://arxiv.org/abs/2404.04167v3) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.04167v3) |
| 2404.03414v1 | Can Small Language Models Help Large Language Models Reason Better? LM-Guided Chain-of-Thought | [arXiv](https://arxiv.org/abs/2404.03414v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.03414v1) |
| 2404.01261v1 | FABLES Evaluating faithfulness and content selection in book-length summarization | [arXiv](https://arxiv.org/abs/2404.01261v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.01261v1) |
| 2404.01204 | The Fine Line Navigating Large Language Model Pretraining with Down-streaming Capability Analysis | [arXiv](https://arxiv.org/abs/2404.01204) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2404.01204) |
| 2403.19270v1 | sDPO Dont Use Your Data All at Once | [arXiv](https://arxiv.org/abs/2403.19270v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.19270v1) |
| 2403.18058v1 | COIG-CQIA Quality is All You Need for Chinese Instruction Fine-tuning | [arXiv](https://arxiv.org/abs/2403.18058v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.18058v1) |
| 2403.16971v2 | AIOS LLM Agent Operating System | [arXiv](https://arxiv.org/abs/2403.16971v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.16971v2) |
| 2403.16952v1 | Data Mixing Laws Optimizing Data Mixtures by Predicting Language Modeling Performance | [arXiv](https://arxiv.org/abs/2403.16952v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.16952v1) |
| 2403.15796v2 | Understanding Emergent Abilities of Language Models from the Loss Perspective | [arXiv](https://arxiv.org/abs/2403.15796v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.15796v2) |
| 2403.13799v1 | Reverse Training to Nurse the Reversal Curse | [arXiv](https://arxiv.org/abs/2403.13799v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.13799v1) |
| 2403.13187v1 | Evolutionary Optimization of Model Merging Recipes | [arXiv](https://arxiv.org/abs/2403.13187v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.13187v1) |
| 2403.10131v1 | RAFT Adapting Language Model to Domain Specific RAG | [arXiv](https://arxiv.org/abs/2403.10131v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.10131v1) |
| 2403.09629 | Quiet-STaR Language Models Can Teach Themselves to Think Before Speaking | [arXiv](https://arxiv.org/abs/2403.09629) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.09629) |
| 2403.08763 | Simple and Scalable Strategies to Continually Pre-train Large Language Models | [arXiv](https://arxiv.org/abs/2403.08763) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.08763) |
| 2403.06634 | Stealing Part of a Production Language Model | [arXiv](https://arxiv.org/abs/2403.06634) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.06634) |
| 2403.06563v1 | Unraveling the Mystery of Scaling Laws Part I | [arXiv](https://arxiv.org/abs/2403.06563v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.06563v1) |
| 2403.04706v1 | Common 7B Language Models Already Possess Strong Math Capabilities | [arXiv](https://arxiv.org/abs/2403.04706v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.04706v1) |
| 2403.04652v1 | Yi Open Foundation Models by 01AI | [arXiv](https://arxiv.org/abs/2403.04652v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.04652v1) |
| 2403.03883v2 | SaulLM-7B A pioneering Large Language Model for Law | [arXiv](https://arxiv.org/abs/2403.03883v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.03883v2) |
| 2403.02178v1 | Masked Thought Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models | [arXiv](https://arxiv.org/abs/2403.02178v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.02178v1) |
| 2403.01432v2 | Fine Tuning vs Retrieval Augmented Generation for Less Popular Knowledge | [arXiv](https://arxiv.org/abs/2403.01432v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2403.01432v2) |
| 2402.18815v1 | How do Large Language Models Handle Multilingualism? | [arXiv](https://arxiv.org/abs/2402.18815v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.18815v1) |
| 2402.18563v1 | Approaching Human-Level Forecasting with Language Models | [arXiv](https://arxiv.org/abs/2402.18563v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.18563v1) |
| 2402.16837v1 | Do Large Language Models Latently Perform Multi-Hop Reasoning? | [arXiv](https://arxiv.org/abs/2402.16837v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.16837v1) |
| 2402.16819v2 | Nemotron-4 15B Technical Report | [arXiv](https://arxiv.org/abs/2402.16819v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.16819v2) |
| 2402.14714v1 | Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models | [arXiv](https://arxiv.org/abs/2402.14714v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.14714v1) |
| 2402.12847v1 | Instruction-tuned Language Models are Better Knowledge Learners | [arXiv](https://arxiv.org/abs/2402.12847v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.12847v1) |
| 2402.08939v1 | Premise Order Matters in Reasoning with Large Language Models | [arXiv](https://arxiv.org/abs/2402.08939v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.08939v1) |
| 2402.07043v1 | A Tale of Tails Model Collapse as a Change of Scaling Laws | [arXiv](https://arxiv.org/abs/2402.07043v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.07043v1) |
| 2402.06196v2 | Large Language Models A Survey | [arXiv](https://arxiv.org/abs/2402.06196v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.06196v2) |
| 2402.05120v1 | More Agents Is All You Need | [arXiv](https://arxiv.org/abs/2402.05120v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.05120v1) |
| 2402.00838v3 | OLMo Accelerating the Science of Language Models | [arXiv](https://arxiv.org/abs/2402.00838v3) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2402.00838v3) |
| 2401.16380v1 | Rephrasing the Web A Recipe for Compute and Data-Efficient Language Modeling | [arXiv](https://arxiv.org/abs/2401.16380v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2401.16380v1) |
| 2401.10225v1 | ChatQA Building GPT-4 Level Conversational QA Models | [arXiv](https://arxiv.org/abs/2401.10225v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2401.10225v1) |
| 2401.08417v3 | Contrastive Preference Optimization Pushing the Boundaries of LLM Performance in Machine Translation | [arXiv](https://arxiv.org/abs/2401.08417v3) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2401.08417v3) |
| 2401.05654v1 | Towards Conversational Diagnostic AI | [arXiv](https://arxiv.org/abs/2401.05654v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2401.05654v1) |
| 2401.03129v1 | Examining Forgetting in Continual Pre-training of Aligned Large Language Models | [arXiv](https://arxiv.org/abs/2401.03129v1) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2401.03129v1) |
| 2401.01055v2 | LLaMA Beyond English An Empirical Study on Language Capability Transfer | [arXiv](https://arxiv.org/abs/2401.01055v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2401.01055v2) |
| 2312.05934v3 | Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs | [arXiv](https://arxiv.org/abs/2312.05934v3) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2312.05934v3) |
| 2311.13647 | Language Model Inversion | [arXiv](https://arxiv.org/abs/2311.13647) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2311.13647) |
| 2310.11511 | Self-RAG Learning to Retrieve Generate and Critique through Self-Reflection | [arXiv](https://arxiv.org/abs/2310.11511) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2310.11511) |
| 2310.08754v4 | Tokenizer Choice For LLM Training Negligible or Crucial? | [arXiv](https://arxiv.org/abs/2310.08754v4) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2310.08754v4) |
| 2310.04799v2 | Chat Vector A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages | [arXiv](https://arxiv.org/abs/2310.04799v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2310.04799v2) |
| 2309.12288 | The Reversal Curse LLMs trained on A is B fail to learn B is A | [arXiv](https://arxiv.org/abs/2309.12288) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2309.12288) |
| 2308.12284 | D4 Improving LLM Pretraining via Document De-Duplication and Diversification | [arXiv](https://arxiv.org/abs/2308.12284) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2308.12284) |
| 2306.01116 | The RefinedWeb Dataset for Falcon LLM Outperforming Curated Corpora with Web Data and Web Data Only | [arXiv](https://arxiv.org/abs/2306.01116) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2306.01116) |
| 2305.18290v2 | Direct Preference Optimization Your Language Model is Secretly a Reward Model | [arXiv](https://arxiv.org/abs/2305.18290v2) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2305.18290v2) |
| 2304.08177v3 | Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca | [arXiv](https://arxiv.org/abs/2304.08177v3) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2304.08177v3) |
| 2110.03215 | Towards Continual Knowledge Learning of Language Models | [arXiv](https://arxiv.org/abs/2110.03215) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2110.03215) |
| 2107.06499 | Deduplicating Training Data Makes Language Models Better | [arXiv](https://arxiv.org/abs/2107.06499) | [page](https://github.com/kh-kim/arxiv-translator/tree/main/papers/2107.06499) |

## Procedure

Arxiv 페이퍼를 번역하기 위해서 총 4단계를 거칩니다.

#### ArXiv Paper Download

Arxiv는 wget 등의 명령어를 통해서 pdf 파일을 다운로드 받을 수 없게 하였습니다.
아마도 무분별한 scrapping에 대응하기 위한 것으로 생각됩니다.
따라서 pdf 파일을 다운로드 받기 위해서 [arxiv-dl](https://pypi.org/project/arxiv-dl/) 패키지를 활용합니다.

#### PDF to Markdown

[Nougat OCR](https://github.com/facebookresearch/nougat)을 활용하여 Mathpix Markdown 파일로 변환합니다.

#### Translation

자체 번역 모델을 활용하여 번역을 수행합니다.
다음과 같이 페이퍼의 번역을 위해 사용된 번역기의 성능(초록색)은 DeepL과 Google, Naver의 중간쯤에 위치합니다.

![NMT Evaluation Results](assets/nmt_eval.png)

#### Markdown to HTML

Mathpix Markdown을 HTML로 변환합니다.
변환 방법은 [여기](https://github.com/Mathpix/mathpix-markdown-it/tree/master?tab=readme-ov-file#using-mathpix-markdown-it-in-web-browsers)에 설명되어 있습니다.
그리고 저장된 github에 push되어 저장된 HTML 파일을 githack.com을 통해 렌더링하도록 합니다.

## Future Work

페이퍼 중간의 이미지들은 Nougat OCR에서 추출해주지 않기 때문에 빠져 있습니다.
따라서 이미지도 함께 포함하여 결과물을 만들어내도록 하고자 합니다.

## Contact

Kim Ki Hyun
pointzz.ki@gmail.com
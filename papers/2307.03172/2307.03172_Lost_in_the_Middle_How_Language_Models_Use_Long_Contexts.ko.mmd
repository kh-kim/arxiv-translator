# Lost in the Middle: How Language Models Use Long Contexts

 Nelson F. Liu\({}^{1}\) Kevin Lin\({}^{2}\) John Hewitt\({}^{1}\) Ashwin Paranjape\({}^{3}\)

**Michele Bevilacqua\({}^{3}\) Fabio Petroni\({}^{3}\) Percy Liang\({}^{1}\)**

스탠포드 대학교\({}^{1}\)스탠포드 대학교\({}^{2}\)캘리포니아 대학교, 버클리 대학교\({}^{3}\)사마야 AI

nfliu@cs.stanford.edu

사마야 AI 인턴으로 부분적으로 일을 마쳤습니다.

###### Abstract

최근 언어 모델은 긴 컨텍스트를 입력으로 사용하는 기능이 있지만 더 긴 컨텍스트를 얼마나 잘 사용하는지에 대해서는 알려져 있는 바가 거의 없습니다. 본 논문에서는 다중 문서 질의 응답과 키 값 검색의 입력 문맥에서 관련 정보를 식별해야 하는 두 가지 태스크에 대한 언어 모델의 성능을 분석한다. 본 논문에서 제안하는 방법은 관련 정보의 위치를 변경할 때 성능이 크게 저하될 수 있음을 알 수 있으며, 이는 현재 언어 모델이 긴 입력 컨텍스트에서 정보를 강력하게 사용하지 않는다는 것을 나타낸다. 특히, 입력 컨텍스트의 시작 또는 끝에서 관련 정보가 발생할 때 성능이 가장 높고, 명시적으로 긴 컨텍스트 모델의 경우에도 긴 컨텍스트의 중간에서 모델이 관련 정보에 액세스해야 할 때 성능이 크게 저하된다는 것을 관찰한다. 우리의 분석은 언어 모델이 입력 컨텍스트를 사용하는 방법에 대한 더 나은 이해도를 제공하고 미래의 긴 컨텍스트 언어 모델에 대한 새로운 평가 프로토콜을 제공한다.

## 1 Introduction

언어 모델은 대화 인터페이스, 검색 및 요약, 및 협력 작문을 포함하는 다양한 사용자 대면 언어 기술에서 중요하고 유연한 빌딩 블록이 되었다(Shuster et al., 2022; Thoppilan et al., 2022; Lee et al., 2022, _inter alia_). 이러한 모델은 주로 프롬프트를 통해 다운스트림 작업을 수행합니다. 처리할 모든 관련 작업 사양 및 데이터는 텍스트 입력 컨텍스트로 형식이 지정되고 모델은 생성된 텍스트 완료를 반환합니다. 이러한 입력 컨텍스트들은 수천 개의 토큰들을 포함할 수 있는데, 특히 언어 모델들이 긴 문서들(예를 들어, 법적 또는 과학적 문서들, 대화 이력들 등)을 처리하는데 사용될 때 또는 언어 모델들이 외부 정보로 증강될 때(예를 들어, 검색 엔진으로부터의 관련 문서들, 데이터베이스 질의 결과들 등; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, _inter alia_).

이러한 사용 사례를 처리하려면 언어 모델이 긴 시퀀스에 걸쳐 성공적으로 작동해야 한다. 기존의 언어 모델은 일반적으로 시퀀스 길이가 2차적으로 증가하는 메모리 및 컴퓨팅을 필요로 하는 Transformers(Vaswani et al., 2017)로 구현된다. 그 결과, 트랜스포머 언어 모델들은 종종 비교적 작은 컨텍스트 윈도우들(512-2048 토큰들 사이)로 트레이닝되었다. 하드웨어(예를 들어, 더 많은 메모리를 갖는 더 빠른 GPU) 및 알고리즘(Dai et al., 2019; Dao et al., 2022; Poli et al.,

도 1: 언어 모델의 입력 컨텍스트 내에서 관련 정보의 위치(이 경우, 입력 질문에 답하는 구절의 위치)를 변경하면 U자형 성능 곡선이 생성되는데, 모델은 입력 컨텍스트의 바로 시작(친밀도 편향) 또는 끝(최신 편향)에서 발생하는 관련 정보를 사용하는 것이 더 우수하고, 모델이 입력 컨텍스트의 중간에 위치한 정보에 접근하여 사용해야 할 때 성능이 크게 저하된다.

2023; Rubin and Berant, 2023, _inter alia_는 더 큰 컨텍스트 윈도우들(예를 들어, 4096, 32K, 및 심지어 100K 토큰들)을 갖는 언어 모델들을 초래했지만, 이러한 확장-컨텍스트 언어 모델들이 다운스트림 태스크들을 수행할 때 그들의 입력 컨텍스트들을 어떻게 사용하는지는 불분명하다.

다양한 최신 개방형(MPT-30B-Instruct, LongChat-13B(16K)) 언어 모델과 폐쇄형(OpenAI의 GPT-3.5-Turbo 및 Anthropic의 Claude-1.3) 언어 모델을 사용하여 입력 컨텍스트 내에서 정보에 액세스하고 사용해야 하는 환경에서 통제된 실험을 통해 이 질문을 경험적으로 조사한다. 특히, 본 논문에서는 입력 컨텍스트 크기 및 입력 컨텍스트 내의 관련 정보의 위치를 제어하여 언어 모델 성능에 미치는 영향을 연구한다. 언어 모델이 긴 입력 컨텍스트 내에서 정보를 강력하게 사용할 수 있는 경우 입력 컨텍스트에서 관련 정보의 위치에 따라 성능이 _최소 영향을 받아야 합니다.

본 논문에서는 먼저 다중 문서 질의응답을 실험한다. 이는 주어진 문서를 추론하여 관련 정보를 찾고 주어진 질의에 응답하기 위한 모델을 필요로 한다. 이 작업은 많은 상업적 생성 검색 및 질의응답 애플리케이션(예: Bing Chat)의 기초가 되는 검색-증강 생성 설정을 모방한다. 이 설정에서는 (i) 입력 컨텍스트의 문서 수를 변경함으로써 입력 컨텍스트 길이를 제어하고(검색 증강 생성에서 더 많은 또는 더 적은 문서를 검색하는 것으로 아킨), (ii) 컨텍스트의 시작, 중간 또는 끝에 관련 문서를 배치하기 위해 문서의 순서를 변경함으로써 입력 컨텍스트 내의 관련 정보의 위치를 제어한다.

우리는 입력 컨텍스트에서 관련 정보의 위치를 변경하는 것이 모델 성능에 실질적으로 영향을 미칠 수 있음을 발견했으며, 이는 현재 언어 모델이 긴 입력 컨텍스트에서 정보에 강력하게 액세스하고 사용하지 않는다는 것을 나타낸다. 또한, 독특한 U자형 성능 곡선을 관찰(그림 1); 언어 모델 성능은 관련 정보가 입력 컨텍스트의 맨 처음(친밀도 편향) 또는 끝(최신 편향)에서 발생할 때 가장 높고 모델이 입력 컨텍스트 중간에 정보에 액세스하고 사용해야 할 때 성능이 크게 저하된다(SS2.3). 예를 들어, 관련 정보가 입력 컨텍스트의 중간에 배치될 때, 다중 문서 질문 태스크에 대한 GPT-3.5-터보의 성능은 _문서가 없음_을 예측할 때의 성능보다 낮다(즉, 폐쇄-북 설정; 56.1%). 또한, 확장 문맥 모델이 확장 문맥 모델과 동일한 성능을 갖는 경우가 많다는 것을 발견하여, 확장 문맥 모델이 입력 문맥을 사용하는 데 반드시 더 나은 것은 아님을 나타낸다(SS2.3).

언어 모델이 다중 문서 질문 응답 작업에서 관련 정보를 검색하고 사용하는 데 어려움을 겪는다는 점을 감안할 때 언어 모델은 입력 컨텍스트에서 어느 정도까지 _검색_할 수 있는가? 본 논문은 입력 문맥에서 일치하는 토큰을 검색할 수 있는 기본적인 기능을 위한 최소한의 테스트베드로 설계된 합성 키-값 검색 태스크를 사용하여 이 질문을 연구한다. 이 작업에서 모델에는 JSON 형식의 키-값 쌍 컬렉션이 제공되며 특정 키와 연결된 값을 반환해야 합니다. 다중 문서 QA 태스크와 유사하게, 키-값 검색 태스크는 입력 컨텍스트 길이(더 많은 키-값 쌍 추가) 및 관련 정보의 위치에 대한 제어된 변경을 인정한다. 일부 모델은 합성 키-값 검색 작업을 완벽하게 수행하지만, 다른 모델은 입력 컨텍스트 중간에 발생하는 일치하는 토큰을 단순히 검색하고 U자형 성능 곡선을 계속 나타내기 위해 고군분투한다.

언어 모델이 입력 컨텍스트에서 정보에 강건하게 접근하고 사용하기 위해 어려움을 겪는 이유를 더 잘 이해하기 위해 모델 아키텍처의 역할(디코더 전용 vs. 인코더-디코더), 질의 인식 상황화, 및 명령어 미세 조정(SS4). 우리가 찾은 건

* 인코더-디코더 모델은 입력 컨텍스트 내에서 관련 정보의 위치 변화에 대해 상대적으로 강력하지만 학습 시간 시퀀스 길이 내의 시퀀스에 대해 평가할 때만 적용됩니다. 훈련 중에 보이는 것보다 더 긴 시퀀스에서 평가될 때, 우리는 U자형 성능 곡선(SS4.1)을 관찰한다.
* 쿼리 인식 컨텍스트화(문서 또는 키-값 쌍 뒤에 _and_ 앞에 쿼리를 배치)는 합성 키-값 작업에서 거의 완벽한 성능을 가능하게 하지만 다중 문서 QA(SS4.2)의 추세를 최소화합니다.
* 기본 언어 모델(즉, 명령 미세 조정 없이)도 입력 컨텍스트에서 관련 정보의 위치를 변경함에 따라 U자형 성능 곡선을 보여 줍니다.

우리의 결과는 더 긴 입력 컨텍스트를 가진 언어 모델을 프롬프트하는 것이 더 많은 정보를 언어 모델에 제공하는 절충안임을 나타내며, 이는 다운스트림 작업을 수행하는 데 도움이 될 수 있지만 모델이 추론해야 하는 콘텐츠의 양을 증가시켜 잠재적으로 정확도를 감소시킨다. 이 트레이드오프를 실제로 더 잘 이해하기 위해 개방형 질문 응답(SS5)에 대해 리트리버 판독기 모델을 사용한 사례 연구를 수행한다. 문맥이 항상 질문에 답하는 정확히 _1_ 문서를 포함하는 통제된 다중 문서 QA 태스크와 대조적으로, 상위 \(k\) 문서 중 어느 것도 개방 도메인 QA 설정에서 답변을 포함할 수 없다. Wikipedia에서 검색하여 NaturalQuestions-Open의 질의에 응답할 때, 모델 성능이 회복되기 훨씬 전에 포화된다는 것을 발견했는데, 이는 현재 모델이 20개의 검색된 문서 대신 50개의 문서를 사용하는 것이 추가적인 검색된 문서를 효과적으로 사용하지 못한다는 것을 나타낸다(GPT-3.5-Turbo의 경우 \(\sim\)1.5%, Claude-1.3의 경우 \(\sim\)1%).

우리의 분석은 언어 모델이 입력 컨텍스트를 어떻게 사용하는지에 대한 더 나은 이해를 제공하고 미래의 긴 컨텍스트 모델에 대한 새로운 평가 프로토콜을 도입한다; 언어 모델이 긴 입력 컨텍스트 내에서 정보를 견고하게 사용할 수 있다고 주장하기 위해, 언어 모델의 성능이 입력 컨텍스트에서 관련 정보의 위치(예를 들어, 최상의 경우와 최악의 경우의 성능의 최소 차이)에 의해 최소한으로 영향을 받는다는 것을 보여줄 필요가 있다. 언어 모델이 입력 컨텍스트를 사용하는 방법을 이해하고 개선하는 데 대한 추가 작업을 용이하게 하기 위해 코드 및 평가 데이터를 릴리스한다.

각주 1: nelsonliu.me/paper/lost-in-the-middle

## 2 다중 문서 질문 응답

우리의 목표는 언어 모델이 입력 컨텍스트를 사용하는 방법을 더 잘 이해하는 것입니다. 이를 위해 다중 문서 질의 응답에 대한 모델 성능을 분석하여 입력 컨텍스트 내에서 관련 정보를 찾고 이를 사용하여 질문에 응답해야 한다. 특히, 입력 컨텍스트의 길이와 관련 정보의 위치에 대한 제어된 변경을 수행하고 태스크 수행의 변화를 측정한다.

### Experimental Setup

다중 문서 질의 응답 태스크에서, 모델 입력들은 (i) 답변할 질문 및 (ii) \(k\) 문서들(예를 들어, 위키피디아로부터의 구절들)이고, 여기서 문서들 중 _정확하게는 하나_는 질문에 대한 답변을 포함하고 \(k-1\) "분산기" 문서들은 그렇지 않다. 이 작업은 모델이 입력 컨텍스트 내에 답변을 포함하는 문서에 액세스하고 질문에 응답하는 데 사용할 것을 요구한다. 도 2는 예를 제시한다.

이 작업은 NaturalQuestions-Open Lee et al. (2019); Google 검색 엔진에 발행된 과거 쿼리를 포함하는 Kwiatkowski et al. (2019)와 위키피디아에서 추출한 인간 주석 답변으로 인스턴스화된다. 특히, 주석이 달린 긴 대답이 단락(목록 또는 표와 반대)인 2655개의 질의를 취한다. 우리는 위키피디아의 구절(최대 100개 토큰의 청크)을 입력 컨텍스트 내의 문서로 사용한다. 각각의 질의에 대해, 우리는 답변을 포함하는 문서와 답변을 포함하지 않는 \(k-1\) 분산기 문서가 필요하다. 질문에 답하는 문서를 얻기 위해 우리는 자연 질문 주석에서 답을 포함하는 위키피디아 문단을 사용한다.

응답을 포함하지 않는 \(k-1\) 분산기 문서를 수집하기 위해 검색 시스템 Contierer(2010)를 사용하여 질의와 가장 관련이 있고 자연 질의에 주석된 응답을 포함하지 않는 \(k-1\) 위키피디아 청크를 검색한다. 2,3 입력 컨텍스트에서 분산기 문서는 관련성이 감소하는 순서로 제시된다. 4

각주 2: NaturalQuestions-Open의 모호성은 소수의 산만자 구절이 합리적인 답을 포함할 수 있음을 의미한다. 우리는 또한 모호한 질문의 하위 집합에 대한 실험을 실행하여 유사한 결과와 결론을 찾으며, 부록 A를 참조한다.

각주 3: 우리는 또한 무작위 문서를 산만자로 사용하여 탐구했으며 자세한 내용은 부록 B를 참조한다.

각주 4: 순위순으로 나타나는 "검색 결과"에 대한 사전이 있을 수 있으므로 \(k-1\) 분산기 문서를 무작위로 정렬하고 태스크 설명에서 문서가 무작위로 정렬되었음을 언급하는 것을 탐색했지만 동일한 경향을 발견했다. 자세한 내용은 부록 C를 참조하십시오.

입력된 문맥 내에서 관련 정보의 위치를 변조하기 위해, 우리는 대답이 포함된 문서의 위치를 변경하기 위해 문서의 순서를 조정한다(도 3). 이 작업에서 입력 컨텍스트 길이를 변조하기 위해 답변이 포함되지 않은 검색된 문서의 수를 늘리거나 줄인다(그림 4).

Kandpal 등(2022) 및 Mallen 등(2023)에 이어서, (NaturalQuestions 주석으로부터 취해진) 정답들 중 임의의 것이 예측된 출력에 나타나는지 여부를 판단하여 정확도를 우리의 일차 평가 메트릭으로서 사용한다.

우리의 실험 설정은 Ivgi 등(2023)의 니들-인-헤이즈 스택 실험과 유사하며, 이는 관련 단락이 (i) 입력의 시작에 배치되거나 (ii) 입력 내의 임의의 위치에 배치될 때 질의 응답 성능을 비교한다. 그들은 인코더-디코더 모델들이 관련 정보가 입력 컨텍스트의 시작에 배치될 때 상당히 더 높은 성능을 갖는다는 것을 발견한다. 대조적으로, 우리는 관련 정보의 위치에서 더 미세한 변화를 연구한다.

### Models

우리는 몇 가지 최신 개방형 언어 모델과 폐쇄형 언어 모델을 분석한다. 우리는 출력을 생성할 때 그리디 디코딩을 사용하고 다른 디코딩 방법에 대한 탐구를 향후 작업에 맡긴다. 우리는 각 모델에 대한 표준 프롬프트 세트를 사용한다(그림 2).

개방형 모델.최대 컨텍스트 길이가 8192 토큰인 MPT-30B-Instruct로 실험합니다. 이 모델은 처음에 2048-토큰 시퀀스를 사용하여 1조 토큰에 대해 사전 훈련된 다음 8192-토큰 시퀀스를 사용하여 500억 토큰에 대한 추가 시퀀스 길이 적응 사전 훈련 단계가 뒤따랐다. MPT-30B-Instruct는 ALIBi(Press et al., 2022)를 이용하여 위치 정보를 나타낸다. 또한, 16384-토큰 시퀀스로 미세 조정하기 전에 축약된 회전 위치 임베딩을 사용하여 LLaMA-13B(Touvron et al., 2023) 컨텍스트 윈도우를 2048에서 16384 토큰으로 확장하는 LongChat-13B(16K)(Li et al., 2023)를 평가한다.

Closed models.OpenAI API를 사용하여 GPT-3.5-Turbo 및 GPT-3.5-Turbo를 실험

도 4: 도 2에 제시된 다중 문서 질의 응답 예제의 입력 문맥 길이를 변조한다. 답변을 포함하지 않는 문서를 추가하는 것은 입력 문맥의 길이를 증가시키지만 원하는 출력에는 영향을 미치지 않는다.

도 3: 도 2에 제시된 다중 문서 질의 응답 예시를 위해 입력 컨텍스트 내에서 관련 정보의 위치를 변조한다. 입력 컨텍스트에서 문서를 재정렬하는 것은 원하는 출력에 영향을 미치지 않는다.

도 2: 입력 컨텍스트 및 원하는 모델 답변을 갖는, 다중 문서 질문 응답 태스크의 예시. 답변을 포함하는 문서는 명료성을 위해 여기의 입력 컨텍스트 내에서 굵게 표시된다.

(16K).5 GPT-3.5-터보는 최대 컨텍스트 길이가 4K 토큰이고, GPT-3.5-터보(16K)는 확장된 최대 컨텍스트 길이가 16K 토큰인 버전이다. 우리는 Anthropic API로 Claude-1.3과 Claude-1.3 (100K)를 평가한다. Claude-1.3의 최대 컨텍스트 길이는 8K 토큰이고 Claude-1.3 (100K)의 확장된 컨텍스트 길이는 100K 토큰이다. 6

각주 5: 0613 OpenAI 모델 버전을 사용합니다.

각주 6: 다중 문서 QA 실험의 하위 집합에서 GPT-4(8K)를 평가하여 다른 모델과 유사한 결과와 경향을 찾는다(GPT-4가 더 높은 절대 성능을 갖지만). 전체 다중 문서 QA 및 키-값 검색 실험에서 GPT-4를 평가하는 것은 6000달러 이상의 비용이 들 것이다. GPT-4 결과 및 논의는 부록 D를 참조한다.

### 결과 및 논의

총 10, 20, 30개의 문서를 포함하는 입력 컨텍스트를 실험한다. 그림 5는 입력 컨텍스트 내에서 관련 정보의 위치를 가변할 때 다중 문서 질의 응답 성능을 나타낸다. 모델 성능을 맥락화하기 위해 폐쇄형 및 오라클 설정(표 1)에 대해서도 평가한다. 폐쇄-북 설정에서, 모델들은 그들의 입력 컨텍스트에서 어떠한 문서도 주어지지 않으며, 정답을 생성하기 위해 그들의 파라메트릭 메모리에 의존해야 한다. 한편, 오라클 설정에서 언어 모델들은 답을 포함하는 단일 문서가 주어지고, 질문에 답하기 위해 그것을 사용해야 한다.

모델 성능은 관련 정보가 입력 컨텍스트의 시작 또는 끝에서 발생할 때 가장 높다. 그림 5에 예시된 바와 같이, 입력 컨텍스트에서 관련 정보의 위치를 변경하면 모델 성능이 크게 감소한다. 특히, 우리는 독특한 U자형 성능 곡선을 볼 수 있는데, 모델은 종종 컨텍스트의 매우 시작(친밀도 편향)과 매우 끝(최신 편향)에서 발생하는 관련 정보를 사용하는 데 훨씬 더 뛰어나고 입력 컨텍스트 중간에 정보를 사용하도록 강요할 때 성능이 저하된다. 예를 들어, GPT-3.5-터보의 다중 문서 QA 성능은 20% 이상 떨어질 수 있다--최악의 경우, 20- 및 30-문서 설정에서의 성능은 _any_ 입력 문서가 없는 성능보다 낮다(즉, 폐쇄-북 성능; 56.1%). 이러한 결과는 현재 모델이 다운스트림 작업에 대한 프롬프트가 있을 때 전체 컨텍스트 창에서 효과적으로 추론할 수 없음을 나타낸다.

확장 컨텍스트 모델이 입력 컨텍스트를 사용하는 데 반드시 더 나은 것은 아니다. 입력 컨텍스트가 모델과 확장 컨텍스트 대응의 컨텍스트 창에 모두 적합할 때, 우리는 이들 사이의 성능이 거의 동일하다는 것을 알 수 있다. 예를 들어, 10- 및 20-문서 설정은 둘 다 GPT-3.5-터보 및 GPT-3.5-터보(16K)의 컨텍스트 윈도우에 적합하고, 상대 정보의 위치의 함수로서의 그들의 성능이 거의 중첩된다는 것을 관찰한다(도 5에서 솔리드 퍼플 및 파선 브라운 시리즈). 이러한 결과는

\begin{table}
\begin{tabular}{l r r} \hline \hline Model & Closed-Book & Oracle \\ \hline LongChat-13B (16K) & 35.0\% & 83.4\% \\ MPT-30B-Instruct & 31.5\% & 81.9\% \\ GPT-3.5-Turbo & 56.1\% & 88.3\% \\ GPT-3.5-Turbo (16K) & 56.0\% & 88.6\% \\ Claude-1.3 & 48.3\% & 76.1\% \\ Claude-1.3 (100K) & 48.2\% & 76.4\% \\ \hline \hline \end{tabular}
\end{table}
표 1: 다중 문서 질의 응답 태스크에 대한 언어 모델의 폐쇄-북 및 오라클 정확도.

도 5: 관련 정보(답변이 포함된 문서)의 위치 변경이 다중 문서 질의 응답 성능에 미치는 영향. 더 낮은 위치들은 입력 컨텍스트의 시작에 더 가깝다. 성능은 관련 정보가 컨텍스트의 시작 또는 끝에서 발생할 때 가장 높고 모델이 입력 컨텍스트 중간에 정보를 추론해야 할 때 급격히 저하된다.

확장 컨텍스트 모델이 입력 컨텍스트를 사용할 때 확장되지 않은 대응 모델보다 반드시 더 나은 것은 아님을 나타낸다.

## 3 언어 모델이 입력 컨텍스트에서 검색할 수 있습니까?

언어 모델이 다중 문서 질의 응답 작업에서 입력 컨텍스트의 중간에서 정보를 검색하고 사용하는 데 어려움을 겪는다는 점을 감안할 때 입력 컨텍스트에서 단순히 _검색_할 수 있는 정도는 어느 정도일까? 본 논문은 입력 문맥에서 일치하는 토큰을 검색할 수 있는 기본적인 기능을 위한 최소한의 테스트베드를 제공하도록 설계된 합성 키-값 검색 태스크를 사용하여 이 질문을 연구한다.

### Experimental Setup

합성 키-값 검색 태스크에서, 입력들은 (i) \(k\) 키-값 쌍을 갖는 문자열-직렬화된 JSON 객체이며, 여기서 키들 및 값들 각각은 고유하고, 랜덤하게 생성된 UUID들 및 (ii) 전술한 JSON 객체 내의 키이다. 목표는 지정된 키와 연관된 값을 반환하는 것입니다. 따라서 각 JSON 개체에는 하나의 관련 키-값 쌍(여기서 값이 반환됨)과 \(k-1\) 관련 없는 "distractor" 키-값 쌍이 포함됩니다. 도 6은 예시적인 입력 컨텍스트 및 그에 대응하는 원하는 출력을 제공한다. 예측된 출력에서 정확한 값이 나타나는지 평가하여 정확도를 다시 측정한다.

우리의 합성 키-값 검색 작업은 Papailiopoulos et al.(2023)의 Little Retrieval Test와 Li et al.(2023)의 Fine-grained line retrieval 작업과 유사한 목표를 공유하지만, 언어 특징이 잠재적인 혼란을 야기할 수 있기 때문에 가능한 한 많은 자연어 의미론을 제거하여 작업을 증류하고 단순화하려고 명시적으로 모색한다. 예를 들어, Transformer 언어 모델들은 그들의 입력(O'Connor and Andreas, 2021)에서 상이한 언어 특징들에 대한 다양한 민감도를 가질 수 있다.

입력 컨텍스트 내에서 관련 정보의 위치를 변조하기 위해, 직렬화된 JSON 객체 내에서 검색하기 위해 키의 위치를 변경한다. 입력 컨텍스트 길이를 조절하기 위해 랜덤 키를 추가하거나 제거하여 입력 JSON 키-값 쌍의 수를 변경하고 분산기 키-값 쌍의 수를 변경한다.

### 결과 및 논의

75, 140, 300개의 키-값 쌍(각각 500개의 예제)을 포함하는 입력 컨텍스트를 실험한다. 다중 문서 질의 응답 실험과 동일한 모델 세트를 사용하므로 자세한 내용은 SS2.2를 참조하십시오.

도 7은 키-값 검색 성능을 나타낸다. Claude-1.3 및 Claude-1.3(100K)은 평가된 모든 입력 컨텍스트 길이에서 거의 완벽하게 수행하지만, 다른 모델들은 특히 컨텍스트가 140 또는 300개의 키-값 쌍을 가질 때 어려움을 겪는다 - 비록 합성 키-값 검색 태스크는 입력 컨텍스트 내에서 정확한 일치를 식별하는 것만 필요로 하지만, 모든 모델이 높은 성능을 달성하는 것은 아니다.

우리의 다중 문서 QA 결과와 유사하게, GPT-3.5-터보, GPT-3.5-터보(16K), 및 MPT-30B-강사는 입력 컨텍스트의 중간에 키-값 쌍에 액세스해야 할 때 가장 낮은 성능을 갖는다. LongChat-13B(16K)는 140 키-값 설정에서 다른 경향을 나타내며; 우리는 관련 정보가 있을 때 질적으로 관찰한다

도 6: 입력 컨텍스트 및 원하는 모델 출력을 갖는 키-값 검색 태스크의 예. 키가 주어지면 관련 값을 반환하는 것이 목표입니다. 모든 키와 값은 128비트 UUID입니다. 쿼리에 응답하기 위한 관련 키-값 쌍은 명료성을 위해 입력 컨텍스트 내에서 여기서 굵게 표시된다.

입력 컨텍스트의 시작시에 배치된 LongChat-13B(16K)는 값을 직접 출력하기보다는 키를 검색하기 위한 코드를 생성하는 경향이 있다.

## 4 언어 모델이 관련 정보의 위치 변경에 강하지 않은 이유입니다.

다중 문서 질의 응답과 키 값 검색 결과는 언어 모델이 관련 정보의 위치를 변경할 때 성능이 크게 저하되기 때문에 긴 입력 컨텍스트에서 정보에 견고하게 접근하고 사용하기 어렵다는 것을 보여준다. 이유를 더 잘 이해하기 위해 모델 아키텍처의 역할에 대한 몇 가지 예비 조사를 수행한다(디코더 전용 vs. 인코더-디코더). 쿼리 인식 상황화 및 명령어 미세 조정입니다.

### 모델 아키텍처의 효과

우리가 평가한 개방형 모델은 모두 디코더 전용 모델이며 각 시간 단계에서 이전 토큰에만 참석할 수 있다. 언어 모델이 컨텍스트를 사용하는 방식에 대한 모델 아키텍처의 잠재적 영향을 더 잘 이해하기 위해 디코더 전용 언어 모델과 인코더-디코더 언어 모델을 비교한다.

Flan-T5-XXL(Raffel et al., 2020; Chung et al., 2022) 및 Flan-UL2(Tay et al., 2023)로 실험하였다. Flan-T5-XXL은 512 토큰(인코더 및 디코더)의 시퀀스로 트레이닝된다. Flan-UL2는 초기에 512 토큰(인코더 및 디코더)의 시퀀스로 트레이닝되지만, 이후 인코더에서 2048 토큰 및 디코더에서 512 토큰을 갖는 시퀀스에 대한 미세 조정을 명령하기 전에 1024 토큰(인코더 및 디코더)을 갖는 여분의 100K 단계에 대해 사전 트레이닝된다. 그러나, 이러한 모델들은 상대적인 위치 임베딩을 사용하기 때문에, 이들은 (원칙적으로) 이러한 최대 컨텍스트 길이들을 넘어서 외삽할 수 있다; Shaham 등(2023)은 두 모델들이 최대 8K 토큰들의 시퀀스들로 잘 수행할 수 있다는 것을 발견한다.

그림 8은 디코더 전용 모델과 인코더-디코더 모델의 성능을 비교한 것이다. Flan-UL2가 그의 2048-토큰 트레이닝-시간 컨텍스트 윈도우 내의 시퀀스들에 대해 평가될 때(도 8; 좌측 서브-플롯), 그의 성능은 입력 컨텍스트 내의 관련 정보의 위치의 변화들에 비교적 강건하다(최상의 경우와 최악의 경우의 성능 사이의 1.9% 절대 차이). 2048 토큰보다 긴 시퀀스를 갖는 설정에서 평가될 때(도 8; 중앙 및 우측), Flan-UL2 성능은 관련 정보가 중간에 배치될 때 저하되기 시작한다. Flan-T5-XXL은 유사한 경향을 보이며, 입력 컨텍스트가 길어지면 관련 정보를 입력 컨텍스트 중간에 배치할 때 성능 저하가 더 커진다. 인코더-디코더 모델은 양방향 인코더가 미래 문서의 컨텍스트에서 각 문서를 처리할 수 있기 때문에 컨텍스트 윈도우를 더 잘 사용할 수 있으며, 잠재적으로 문서 간의 상대적 중요도 추정을 개선할 수 있다고 가정한다.

### 쿼리 인식 컨텍스트의 효과

다중문서 질의응답과 키-값 검색 실험은 데이터(문서 또는 키-값 쌍) 뒤에 질의(질의응답 또는 키-값 검색)를 배치한다. 결과적으로, 디코더 전용 모델들은 문서들 또는 키-값 쌍들을 컨텍스트화할 때 질의 토큰들에 참석할 수 없는데, 이는 질의가 마지막에만 나타나기 때문이다.

그림 7: 입력 문맥 길이와 관련 정보의 위치 변화가 키-값 검색 성능에 미치는 영향. 더 낮은 위치들은 입력 컨텍스트의 시작에 더 가깝다. 일부 모델은 이 합성 태스크(예: Claude-1.3 및 Claude-1.3 (100K))에 대해 완벽한 정확도를 보여주지만, 컨텍스트의 바로 시작 또는 끝에서 관련 정보가 발생할 때 성능이 종종 가장 높고, 모델이 입력 컨텍스트의 중간에서 검색해야 할 때 급격히 저하된다는 것을 다시 알 수 있다.

프롬프트 및 디코더 전용 모델의 경우 각 시간 단계에서 이전 토큰에만 참석할 수 있습니다. 대조적으로, 인코더-디코더 모델(관련 정보의 위치의 변화에 더 견고해 보이는; SS4.1)은 입력 컨텍스트를 맥락화하기 위해 양방향 인코더를 사용한다 - 데이터 _and_ 뒤에 질의를 배치함으로써 디코더 전용 모델을 개선하여 문서(또는 키-값 쌍)의 질의 인식 맥락화를 가능하게 하는 이 관찰을 사용할 수 있는가?

우리는 질의 인식 상황화가 키-값 검색 태스크에서 성능을 극적으로 향상시킨다는 것을 발견한다. 모든 모델은 75, 140, 300개의 키-값 쌍 설정에서 거의 완벽한 성능을 달성한다. 예를 들어, 질의 인식 컨텍스트화를 갖는 GPT-3.5-터보(16K)는 300개의 키-값 쌍으로 평가될 때 완벽한 성능을 달성한다.

대조적으로, 질의-인식 맥락화 없이, 최악의 경우 성능은 45.6%이다(도 7). 키-값 검색 성능에 중요한 영향에도 불구하고, 질의 인식 상황화는 다중 문서 질의 응답 태스크의 성능 경향에 최소한의 영향을 미치며(그림 9), 관련 정보가 입력 컨텍스트의 맨 처음에 위치할 때 성능은 약간 향상되지만 다른 설정에서는 성능이 약간 감소한다.

### 명령어 미세 조정 효과

우리가 평가한 모델은 모두 세부 조정된 명령어이며, 초기 사전 훈련 후 명령어와 응답 데이터 세트에 대한 감독된 세부 조정을 거친다. 태스크 사양 및/또는 명령은 감독된 명령 미세 조정 데이터에서 입력 컨텍스트의 시작에 일반적으로 배치되며, 이는 명령 미세 조정된 언어 모델이 입력 컨텍스트의 시작에 더 많은 가중치를 둘 수 있도록 유도할 수 있다. 언어 모델이 긴 입력 컨텍스트를 사용하는 방식에 대한 명령어 미세 조정의 잠재적 영향을 더 잘 이해하기 위해, 우리는 MPT-30B-강사의 다중 문서 질문 응답 성능을 기본 모델(즉, 명령어 미세 조정 전) MPT-30B와 비교한다. 우리는 SS2와 동일한 실험 설정을 사용한다.

도 10은 MPT-30B와 MPT-30B-강사의 다중 문서 QA 성능을 관련자의 위치에 따른 함수로 비교한 것이다

도 8: 인코더-디코더 모델들(Flan-UL2 및 Flan-T5-XXL)이 그들의 인코더의 트레이닝-시간 최대 시퀀스 길이(각각 2048 및 512 토큰)보다 _shorter_인 시퀀스들에 대해 평가될 때, 이들은 그들의 입력 컨텍스트 내의 관련 정보의 위치(좌측 서브플롯)의 변화에 상대적으로 강건하다. 대조적으로, 이러한 모델이 훈련 중에 볼 수 있는 것보다 시퀀스 _더 긴_에서 평가될 때(중앙 및 오른쪽 서브플롯) U자형 성능 곡선을 관찰하며, 성능은 입력 컨텍스트의 중간이 아닌 입력 컨텍스트의 시작 또는 끝에서 관련 정보가 발생할 때 더 높다.

도 9: 질의 인식 맥락화(문서의 _and_ 이전에 질의를 배치함)는 다중 문서 QA에서 관련 정보의 위치를 변경하는 것에 대한 언어 모델의 견고성을 실질적으로 향상시키지 않는다; 성능은 관련 정보가 맨 처음에 발생할 때 약간 증가하지만 그렇지 않으면 약간 감소한다.

입력 컨텍스트의 형식입니다. 놀랍게도, MPT-30B 및 MPT-30B-강사 둘 다 U자형 성능 곡선을 나타내며, 여기서 관련 정보가 컨텍스트의 매우 시작 또는 매우 끝에서 발생할 때 성능이 가장 높다는 것을 알 수 있다. MPT-30B-Instruct의 절대 성능은 MPT-30B에 비해 일률적으로 높지만, 전체 성능 추세는 유사하다. 또한 명령어 미세 조정은 기본 모델과 최악 모델 사이의 최악 성능 차이를 거의 10%에서 약 4%로 약간 감소시킨다는 것을 관찰했다.

이러한 관찰은 비-명령어 미세 조정 언어 모델이 최근 토큰(즉, 입력 컨텍스트의 끝; Khandelwal et al., 2018; Press et al., 2021)에 편향된다는 것을 발견한 이전 작업을 보완한다. 이러한 최근 편향은 언어 모델이 장거리 정보로부터 최소한으로 이익을 얻는 설정인 연속 텍스트의 다음 단어 예측에 대한 모델을 평가할 때 과거 작업에서 관찰되었다(Sun et al., 2021). 이와는 대조적으로, 우리의 결과는 언어 모델이 명령어 형식의 데이터로 프롬프트될 때 더 긴 범위의 정보(즉, 입력 컨텍스트의 시작)를 사용할 수 있음을 보여준다. 비명령어 미세 조정 언어 모델은 사전 훈련 중에 볼 수 있는 유사한 형식의 데이터, 예를 들어 스택 오버플로 질문 및 답변에서 이러한 긴 컨텍스트를 사용하는 방법을 배운다고 가정한다.

추가 미세 조정 및 모델 척도의 효과를 더 잘 이해하기 위해 인간 피드백(부록 E)에서 추가 감독 미세 조정 및 강화 학습이 있거나 없는 다양한 크기(7B, 13B 및 70B)의 Llama-2 모델을 실험했다. U자형 성능 곡선은 충분히 큰 언어 모델에서만 나타나는데(추가 미세 조정 유무에 관계없이) 7B 라마-2 모델은 최신성 편향만 있는 반면 13B 및 70B 모델은 U자형 성능 곡선을 나타낸다. 또한, 인간 피드백 절차로부터의 Llama-2 감독된 미세 조정 및 강화 학습이 더 작은 모델들(13B, MPT-30B와 MPT-30B-강사를 비교할 때 보여지는 경향과 유사함)에서 위치 편향을 약간 완화시키지만, 더 큰 모델들(70B)에서의 경향에는 최소한으로 영향을 미친다는 것을 알 수 있다.

## 5 Is More Context Is Always better?

개방형 QA를 이용한 사례 연구

우리의 결과는 더 긴 입력 컨텍스트를 가진 언어 모델을 프롬프트하는 것이 더 많은 정보를 언어 모델에 제공하는 절충안임을 나타내며, 이는 다운스트림 작업을 수행하는 데 도움이 될 수 있지만 모델이 추론해야 하는 콘텐츠의 양을 증가시켜 잠재적으로 정확도를 감소시킨다. 언어 모델이 16K 토큰을 가져올 수 있다고 해도, 실제로 16K 토큰의 컨텍스트를 제공하는 것이 유익합니까? 이 질문에 대한 답은 추가된 컨텍스트의 한계 가치와 긴 입력 컨텍스트를 효과적으로 사용할 수 있는 모델의 능력에 달려 있기 때문에 궁극적으로 다운스트림 태스크별이지만, 우리는 기존 언어 모델에서 이러한 균형을 더 잘 이해하기 위해 NaturalQuestions-Open에 대한 개방형 질문 답변으로 사례 연구를 수행한다.

표준 리트리버 판독기 설정에서 언어 모델을 사용합니다. 검색 시스템(Contriever, fine-tuned on MS-MARCO)은 NaturalQuestions-Open에서 입력 질의를 받아 Wikipedia의 \(k\) 문서를 가장 높은 관련성 점수로 반환한다. 검색된 문서에 대한 언어 모델을 조건화하기 위해 프롬프트에 포함하기만 하면 됩니다. 검색된 문서 수 \(k\)의 함수로서 검색자 재현율과 판독자 정확도(주석이 달린 답변이 예측된 출력에 나타나는지 여부)를 평가한다. 우리는 긴 답이 (표나 목록과는 반대로) 단락인 자연 질문-열림의 부분 집합을 사용한다.

도 11은 리트리버 리콜 및 개방을 나타낸 도면

도 10: MPT-30B-Instruct의 다중 문서 QA 성능은 그것의 기본 모델(즉, 명령어 미세 조정 전) MPT-30B와 비교된다. 두 모델 모두 U자형 성능 곡선을 가지며, 여기서 관련 정보가 입력 컨텍스트의 시작 또는 끝에서 발생할 때 성능이 훨씬 더 높으며, 이는 명령어 미세 조정 프로세스 자체가 이러한 성능 경향에 반드시 책임이 있는 것은 아님을 나타낸다.

도메인 QA 결과. 리트리버 성능이 포화되기 훨씬 전에 리더 모델 성능이 포화된다는 것을 알 수 있으며, 이는 독자들이 여분의 컨텍스트를 효과적으로 사용하지 않는다는 것을 나타낸다. 검색된 20개 이상의 문서를 사용하는 경우 GPT-3.5-Turbo의 경우 \(\sim\)1.5%, Claude-1.3의 경우 \(\sim\)1%의 읽기 성능을 약간 향상시키지만, 입력 컨텍스트 길이(지연시간 및 비용)는 크게 증가한다. 이러한 결과는, 모델이 종종 입력 컨텍스트의 시작 또는 끝에서 정보를 검색하고 사용하는 데 더 우수하다는 관찰과 결합되어, 검색된 문서의 효과적인 리랭킹(입력 컨텍스트의 시작에 더 가깝게 관련 정보를 푸시함) 또는 랭킹된 리스트 절단(적절한 경우 더 적은 문서를 검색하는 것; Arampatzis 등, 2009)이 언어 모델 기반 판독기가 검색된 컨텍스트를 사용하는 방법을 개선하기 위한 유망한 방향일 수 있음을 시사한다.

## 6 관련 작업

### Long-Context 언어 모델

컨텍스트 길이에서 트랜스포머보다 더 저렴한 스케일링을 갖는 성능 언어 모델을 설계하기 위한 많은 선행 연구가 있다. 많은 작업 라인은 재발(Dai et al., 2019), 계산적으로 덜 집중적인 근사치로 주의를 인수분해(Beltagy et al., 2020; Zaheer et al., 2020), 또는 낮은 순위 근사치(Wang et al., 2020; Peng et al., 2021)와 같은 주의 수정을 갖는 Transformer 변형을 추구한다. Dao 등(2022)은 대신에 신중하게-크래프트된 IO-인식 CUDA 커널에 의해 더 빠른 정확한 주의를 제공한다. 별도로, 종종 컨볼루션 및/또는 선형 RNN을 통해, 예를 들어 RWKV(Peng, 2023), S4(Gu 등, 2022), 또는 하이에나(Poli 등, 2023)에서 이차 시퀀스 길이 복잡성을 제거하려는 주의를 완전히 없애려는 시도들이 있다. 많은 선행 연구들은 긴 컨텍스트를 처리하는 능력에 대한 대용물로서 다양한 웹 코퍼스에 대한 복잡성을 평가하는데, 이 연구는 긴 컨텍스트에 대한 정확한 지식 액세스가 추가 과제가 될 수 있음을 보여준다.

### 언어 모델에서 컨텍스트를 사용하는 방법

Khandelwal 등(2018)의 선구적인 작업은 작은 LSTM 언어 모델이 더 긴-term context를 점점 더 거칠게 사용한다는 것을 보여주었다; Sankar 등(2019)은 대화 모델에서 유사한 결과를 발견하였다. 유사한 맥락에서, Daniluk 등(2017)은 주의력 있는 LSTM 언어 모델이 주로 최근 역사를 사용하는 경향이 있음을 발견한다. Petroni et al.(2020)은 비지도 질의응답을 위한 사전 훈련된 언어 모델과 정보 검색 시스템으로부터의 컨텍스트를 결합하는 가능성을 처음으로 입증한 사람 중 하나였다. O'Connor and Andreas (2021)는 많은 정보 파괴 작업이 변압기 LMs의 예측에 미미한 영향을 미친다는 것을 발견했다. Krishna et al.(2022)은 Modestly-size Transformer 언어 모델에서의 Long-context 신경망 생성은 모델이 긴 컨텍스트를 적절하게 조건화하지 못하기 때문에 퇴화된다는 것을 발견했다. 마지막으로, Long-context 모델을 연구한 Sun et al.(2021)은 긴 컨텍스트가 소수의 토큰에 대한 예측을 개선한다는 것을 발견했으며, Sharan et al.(2018)의 이론과 일치하는 경험적 발견으로, 경계 상호 정보를 가진 시퀀스 분포가 필연적으로 점점 더 긴 컨텍스트로부터 한계 _평균_ 예측 이점으로 이어진다는 것을 보여주었다. Qin 등(2023)은 다양한 롱-컨텍스트 다운스트림 NLP 태스크들에 대해 트랜스포머들이 얼마나 효율적인지를 분석하여, 롱-컨텍스트 트랜스포머들이 최근 편향되고 장거리 컨텍스트를 효과적으로 사용하지 않는다는 것을 발견한다.

### 직렬 위치 효과

이 작업에서 관찰할 수 있는 U자형 곡선은 _직렬 위치 효과_(에빙하우스, 1913; 머독 주니어, 1962)로 알려진 심리학에서 연결되며, 목록의 요소를 자유롭게 연상할 때 인간은 목록의 첫 번째 요소와 마지막 요소를 가장 잘 기억하는 경향이 있다. 일련 위치 효과는 인간이 장단기 밈을 개발하는 방법을 이해하는 데 역할을 한다.

도 11: 검색된 문서 수의 함수로서의 검색기 리콜 및 모델 성능. 모델 성능은 리트리버 리콜 이전에 포화되며, 이는 모델이 추가로 검색된 문서를 사용하는 데 어려움을 겪는다는 것을 나타낸다.

ory. 트랜스포머 언어 모델의 기초가 되는 자기 주의 메커니즘이 기술적으로 동등하게 컨텍스트에서 토큰을 검색할 수 있기 때문에 언어 모델에서 직렬 위치 유사 효과를 관찰하는 것은 아마도 놀라운 일이다.

## 7 Conclusion

우리는 일련의 통제된 실험을 통해 언어 모델이 긴 입력 컨텍스트를 사용하는 방법을 경험적으로 연구한다. 우리는 언어 모델 성능이 관련 정보의 위치를 변경할 때 크게 저하된다는 것을 보여주며, 이는 모델이 긴 입력 컨텍스트에서 정보에 강력하게 액세스하고 사용하기 위해 어려움을 겪는다는 것을 나타낸다. 특히, 모델이 긴 입력 컨텍스트의 중간에서 정보를 사용해야 할 때 성능이 가장 낮은 경우가 많다. 이를 위해 (i) 모델 아키텍처, (ii) 질의 인식 상황화 및 (iii) 명령어 미세 조정의 역할에 대한 사전 조사를 수행하여 언어 모델이 컨텍스트를 사용하는 방식에 어떻게 영향을 미치는지 더 잘 이해한다. 마지막으로, 개방형 질문 응답의 실제 사례 연구를 통해 언어 모델 독자의 성능이 리트리버 회상 훨씬 전에 포화된다는 결론을 내렸다. 우리의 결과와 분석은 언어 모델이 입력 컨텍스트를 사용하는 방법에 대한 더 나은 이해도를 제공하고 미래의 장기 컨텍스트 모델에 대한 새로운 평가 프로토콜을 제공한다.

## Acknowledgments

TACL 액션 에디터를 역임한 루크 제틀모이어와 익명의 평론가들의 논평과 피드백에 감사드린다. 우리는 또한 클라우디우 레오바누-콘드레이, 메건 레즈친스키, 드미트로 오혼코, 마이트라 라후, 에릭 월리스, 상 마이클 시에게 이 작업을 개선하는 데 도움이 되는 피드백과 토론에 감사드린다. 또한, AmbigQA 데이터 세트에 대한 세원민의 도움에 감사한다. 이 작업은 스탠포드 기초 모델 연구 센터(CRFM), 오픈AI가 스탠포드 CRFM에 대한 API 크레딧 부여를 통해, 인류학이 클로드 학술 접근 프로그램을 통해 지원했다.

## References

* A. Arampatzis, J. Kamps, and S. 로버트슨(2009)은 순위표를 읽는 것을 그만둘까요? threshold optimization using truncated score distributions. Proc. of SIGIR, Cited by: SS1.
* I. Beltagy, M. E. Peters, and A. Cohan(2020)Longformer: the long-document transformer. ArXiv:2004.05150. Cited by: SS1.
* H. W. Chung, L. 허성 롱프리비조프영 태원 페두스 이석호 왕민 Dehghani, S. B. Brahma, A. Webson, S. 셰인구즈 대민 수즈건 첸, A. 차우더리, A. 카스트로-로스, M. 펠랏 로빈슨 디발터 나랑기미슈라 조영호 황애대 Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei(2022)Scaling instruction-finetuned language models. ArXiv:2210.11416. Cited by: SS1.
*Z. 대진 양영 양진카보넬 Le, R. Salakhutdinov (2019)Transformer-XL: 고정 길이 컨텍스트를 넘어서는 주의 깊은 언어 모델. Proc. of ACL, Cited by: SS1.
*M. 다닐룩 Rocktaschel, J. Welbl, S. Riedel (2017) Frustrly short attention span in neural language modeling. Proc. of ICLR, Cited by: SS1.
* T. 도대영 Ermon, A. Rudra, and C. Re(2022)FlashAttention: 빠르고 메모리 효율적인 정확한 attention with IO-awareness. ArXiv:2205.14135. 인용: SS1.
* H. Ebbinghaus(1913)Memory: a contribution to experimental psychology. H. A. Ruger and C. E. Bussenius(Eds.). Trans. Cited by: SS1.
* A. 구, K. Goel, 및 C. Re(2022)는 구조화된 상태 공간을 갖는 긴 시퀀스를 효율적으로 모델링한다. Proc. of ICLR, Cited by: SS1.
*M. 이비우 Shaham, and J. Berant (2023)Efficient long-text understanding with short-text models. Transactions of the Association for Computational Linguistics11, pp. 284-299. Cited by: SS1.
* G. Izacard, M. 카론 호세이니 Riedel, P. Bojanowski, A. Joulin, and E. Grave (2021) Unsupervised dense information retrieval with contrastive learning. ArXiv:2112.09118. Cited by: SS1.
* G. Izacard and E. Grave (2021) Leveraging passage retrieval with generating models for open domain question answer. _Proc. 의 EACL_.
* Kandpal 등(2022) Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. 대형 언어 모델들은 롱테일 지식을 배우는데 어려움을 겪는다. ArXiv:2211.08411.
* Khandelwal 등(2018) Urvashi Khandelwal, He He, Peng Qi, Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. _Proc. of ACL_.
* Krishna 등(2022) Kalpesh Krishna, Yapei Chang, John Wieting, and Mohit Iyyer. 2022. RankGen: 큰 순위 모델을 갖는 텍스트 생성의 개선. _Proc. 의 EMNLP_.
* Kwiatkowski 등(2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov 2019. Natural Questionions: A benchmark for question answer research. _ Transactions of the Association for Computational Linguistics_, 7:452-466.
* Lee et al.(2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answer. _Proc. of ACL_.
* Lee 등(2022) Mina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: 언어 모델 능력을 탐색하기 위한 인간-AI 협력 작문 데이터세트의 설계. _Proc. 의 CHI_.
* Li 등(2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. 오픈 소스 LLM이 컨텍스트 길이에 대해 얼마나 오래 약속할 수 있습니까?
* Mallen 등(2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. 언어가 모델을 신뢰하지 않을 때: 모수적 및 비모수적 메모리의 효과성 탐색. _Proc. of ACL_.
* Min 등(2020) Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: 모호한 오픈 도메인 질문에 답변합니다. _Proc. 의 EMNLP_.
* 머독 주니어. (1962) Bennet B. Murdock Jr. 1962. serial position effect of free recall. _ Journal of experimental psychology_, 64(5):482.
* O'Connor and Andreas (2021) Joe O'Connor and Jacob Andreas. 2021. Transformer 언어 모델에서 사용할 수 있는 컨텍스트 기능은 무엇입니까? _Proc. of ACL_.
* Papailiopoulos et al.(2023) Dimitris Papailiopoulos, Lee Kangwook, and Jyyong Sohn. 2023. A little retrieval test for large language models. [https://github.com/anadim/the-little-retrieval-test] (https://github.com/anadim/the-little-retrieval-test).
*펭(2023) 보펭. 2023. RWKV-LM. [https://github.com/BlinkDL/RNKV-LM] (https://github.com/BlinkDL/RNKV-LM).
* Peng 등(2021) Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random feature attention. _Proc. 의 ICLR_.
* Petroni 등(2020) Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktaschel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2020. 컨텍스트가 언어 모델의 사실적 예측에 어떤 영향을 미치는지. _Proc. (p<0.05).
* Poli 등(2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Re. 2023. 하이에나 계층구조: 더 큰 콘볼루션 언어 모델들을 향한다. _Proc. 의 ICML_.
* Press et al.(2021) Ofir Press, Noah A. Smith, and Mike Lewis. 2021. Shortformer: 더 짧은 입력들을 사용하는 더 나은 언어 모델링. _Proc. of ACL_.
* Press et al.(2022) Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear bias enables input length extrapolation. _Proc. 의 ICLR_.
* Qin 등(2023) Guangui Qin, Yukun Feng, and Benjamin Van Durme. 2023. 장거리 변압기의 NLP 태스크 효과. _Proc. 의 EACL_.
* Raffel 등(2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Unified text-to-text Transformer를 이용한 전이학습의 한계 탐색. _ Journal of Machine Learning Research_, 21(140):1-67.
* Raffel 등(2020)Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. InContext retrieval-augmented language models. ArXiv:2302.00083
* Rubin and Berant (2023) Ohad Rubin and Jonathan Berant. 2023. self-retrieval을 이용한 Long-range language modeling. ArXiv:2306.13421.
* Sankar 등(2019) Chinnadhurai Sankar, Sandeep Subramanian, Chris Pal, Sarath Chandar, and Yoshua Bengio. 2019. 신경망 대화 시스템은 대화 이력을 효과적으로 사용하는가? 실증 연구. _Proc. of ACL_.
* Schick 등(2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessl, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. 툴포머: 언어 모델들은 도구들을 사용하는 것을 그들 스스로 가르칠 수 있다.
* Shaham 등(2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. ZeroSCROLLS: 긴 텍스트 이해를 위한 제로 샷 벤치마크. ArXiv:2305.14196
* Sharan 등(2018) Vatsal Sharan, Sham Kakade, Percy Liang, and Gregory Valiant. 2018. 짧은 메모리로 예측. _Proc. STOC_.
* Shi 등(2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrieval-augmented black-box language models. ArXiv:2301.12652
* Shuster 등(2022) Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. 블렌더봇 3: 책임감 있게 참여하도록 지속적으로 학습하는 배치된 대화형 에이전트. ArXiv:2208.03188
* Sun 등(2021) Simeng Sun, Kalpesh Krishna, Andrew Mattarellak-Micke, and Mohit Iyyer. 2021. 장거리 언어 모델이 실제로 장거리 컨텍스트를 사용합니까? _Proc. 의 EMNLP_.
* Tay 등(2023) Yi Tay, Mostafa Dehghani, Vinh Q. 트란, 사비에르 가르시아, 제이슨 웨이, 수에지 왕, 형원 정, 시아막 샤케리, 다라 바리, 탈 슈스터, 화이시우 스티븐 정, 데니 저우, 닐 홀스비, 도날드 메츨러 등이다. 2023. UL2: Unifying language learning paradigms. ArXiv:2205.05131
*토필란 등(2023)은 로말 토필란, 다니엘 드 프라이타스, 제이미 홀, 노암 샤제르, 아푸레프 쿨슈레사, 헝-티제 쳉, 알리시아 진, 테일러 보스, 레슬리 베이커, 유두, 야광 리, 홍래 리, 화이시우 스테벤 정, 아민 가푸리, 마르셀로 메네갈리, 얀핑 황, 막심 크라이쿤, 드미트리 레피킨, 제임스 퀸, 데하오 첸, 유안중 쉬, 지펑 첸, 아담 로버츠, 마르텐 보스마, 빈센트 자오, 옌치 저우, 청칭 창, 이고르 크리보콘, 윌 러쉬, 마크 피켓, 프라네시 스리니바산, 라히 만, 카틀린 마이어-헬스턴, 메레디스 링겔 모리스, 툴제 도시, 레넬리토 델로스 산토스, 토주 듀크, 조니 소레이커, 벤 제벤베르겐, 비노도쿠마르 프라바카란, 마크 디아즈, 벤 허친슨, 크리스틴 2022. LaMDA: 대화 애플리케이션을 위한 언어 모델. ArXiv:2201.08239
* Touvron 등(2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. LLAMA: 개방형 및 효율적인 기초 언어 모델입니다. ArXiv:2302.13971.
* Touvron 등(2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Anthony Hartshorn, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan 2023b. 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. ArXiv:2307.09288
* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저, 일리아 폴로수킨 2017년입니다. 주의만 하면 됩니다. _Proc. NeurIPS_.
* Wang 등(2020) Sinong Wang, Belinda Z. 리, 매디안 캅사, 한팡, 하오마 2020. Linformer: Self-attention with linear complexity. ArXiv:2006.04768
* Zaheer 등(2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for longer sequence. _Proc. NeurIPS_.

## 부록 다중 문서 QA 분산기 문서의 모호성

NaturalQuestions-Open Izacard 등(2021); Izacard and Grave(2021), _inter alia_에 대한 과거 작업에 이어, 우리는 검색 코퍼스로 2018년 말 위키피디아 덤프를 사용한다. 그러나 이 표준 위키피디아 덤프는 자연 질문 주석과 약간의 시간적 불일치가 있다.

예를 들어, "로버트 그리핀 iii가 어떤 nfl 팀을 위해 플레이하는가?"라는 질문을 고려한다. 주석이 달린 자연 질문은 "현재 자유 에이전트"입니다. 그러나, 위키피디아 검색 코퍼스는 그가 위키피디아 덤프의 타임스탬프와 자연 질문 주석 프로세스 사이의 팀에서 방출되었기 때문에, "볼티모어 라벤스"를 위해 플레이하는 정보를 포함한다.

Min 등(2020)의 모호성 주석을 사용하여 모호하지 않은 부분 집합을 생성한다. 데이터의 이 명확한 하위 집합에 대한 실험은 전체 질문 모음(그림 12)에 대한 실험과 유사한 결과와 결론을 보여준다.

## 부록 B Random Distractors in Multi-Document QA

또한 무작위 위키피디아 문서를 분산자로 사용하여 다중 문서 질의 응답 실험을 실행하여 검색된 분산자의 영향을 제거할 수 있다(하드 네거티브). 이 설정에서, 답변을 포함하는 문서는 종종 간단한 휴리스틱(예를 들어, 질의와의 어휘 중첩)으로 식별될 수 있다는 점에 유의한다. 도 13은 본 실험의 결과를 나타낸다. 모든 모델이 이 설정에서 더 높은 절대 정확도를 갖지만 놀랍게도 여전히 전체 입력 컨텍스트에 대해 추론하는 데 어려움을 겪으며, 이는 성능 저하가 관련 문서를 식별할 수 없기 때문만은 아님을 나타낸다.

## 부록 C Randomizing Distractor Order in Multi-Document QA

우리의 프롬프트는 제공된 검색 결과를 사용하여 질문에 답하도록 언어 모델에 지시합니다. 사전-트레이닝 또는 명령어 미세 조정 데이터에서 검색 결과들을 관련성 감소에 의해 정렬된 것으로 취급하기 위한 사전이 있을 수 있다(즉, 입력 컨텍스트의 시작 부근의 문서들은 마지막에 있는 문서들보다 유용할 가능성이 더 높다). 본 논문의 결론들이 단순히 이러한 편향의 부산물이 아니라는 것을 검증하기 위해, 우리는 수정된 명령어로 실험을 실행한다. "제공된 검색 결과만을 사용하여 주어진 질문에 대한 고품질 답변을 작성하라(그 중 일부는 관련이 없을 수 있다). 검색 결과는 무작위로 정렬된다." 그리고 \(k-1\) 산만기 문서를 무작위로 섞는다.

그림 12: 모호하지 않은 질문 하위 집합에 대한 언어 모델 성능입니다.

도 14는 본 실험의 결과를 나타낸다. 우리는 언어 모델이 입력 컨텍스트의 중간에 정보를 사용해야 할 때 성능이 저하되는 U자형 성능 곡선을 계속 본다. SS2.3의 결과와 분산자 순서를 랜덤화하고 프롬프트에서 언급한 결과를 비교하면, 랜덤화는 관련 정보가 컨텍스트의 맨 처음에 있을 때 성능을 약간 감소시키고 컨텍스트의 중간과 끝에 있는 정보를 사용할 때 성능을 약간 증가시킨다는 것을 알 수 있다.

## Appendix D GPT-4 Performance

각 입력 컨텍스트에서 총 20개의 문서를 사용하여 500개의 무작위 다중 문서 QA 예제의 하위 집합에서 GPT-4(8K)를 평가한다(그림 15). GPT-4는 다른 언어 모델보다 높은 절대 성능을 달성하지만 여전히 U자 형태의 성능 곡선을 보여주는데, 관련 정보가 컨텍스트의 시작 또는 끝에서 발생할 때 성능이 가장 높고 입력 컨텍스트 중간에 정보를 사용해야 할 때 성능이 저하된다.

## Appendix E Llama-2 Performance

본 논문에서는 Llama-2 (Touvron et al., 2023)의 다중 문서 QA를 이용하여 각 입력 문맥에서 총 20개의 문서를 평가한다. 라마 토큰라이저는 이전에 연구된 모델에 대해 토큰라이저보다 더 긴 시퀀스를 생성하므로 20개의 검사를 폐기한다.

도 14: (관련성이 감소하는 순서로 제시하기보다는) 산만자들의 순서를 랜덤화하고 프롬프트에서 그와 같이 언급할 때의 언어 모델 성능.

도 13: 검색된 산만기가 아닌 랜덤 산만기를 사용할 때 다중 문서 QA에 대한 언어 모델 성능.

도 15: GPT-4가 다른 모델들보다 더 높은 절대 성능을 갖지만, 그 성능은 관련 정보가 입력 컨텍스트의 중간에 발생할 때 여전히 저하된다.

Llama-2의 최대 컨텍스트 길이인 4096 토큰을 초과하는 샘플(2655개 중)입니다. 우리는 다양한 크기(7B, 13B, 70B 파라미터)의 모델을 실험하고, 추가적인 감독 미세 조정 및 인간 피드백으로부터의 강화 학습("-chat-" 모델)을 포함하거나 포함하지 않는다. 결과는 그림 16에 나와 있다.

다양한 크기의 Llama-2 모델을 비교한 결과, 더 큰 모델(13B 및 70B)만이 U자형 성능 곡선(즉, 우선성과 최신성 편향)을 나타내는 것으로 나타났으며, 가장 작은 Llama-2 모델(7B)은 최신성 편향만 있는 것으로 나타났다. 이러한 결과를 감안할 때 이전 작업(예: Khandelwal et al., 2018; Sun et al., 2021)이 그들이 연구한 모델이 너무 작기 때문에 언어 모델에서 프라이머시 편향을 이전에 관찰하지 않았다고 가정한다.

추가적인 감독 미세 조정과 인간 피드백으로부터의 강화 학습이 있는 Llama-2 모델과 없는 Llama-2 모델을 비교하면, 추가 미세 조정이 다중 문서 QA 태스크에서 성능을 극적으로 향상시킨다는 것을 알 수 있다. 추가 미세 조정이 있거나 없는 7B 모델은 최소 프라이머시 편향을 나타내며 대체로 최신 편향이다. 13B 기본 모델은 극적인 우선성과 최신 편향을 가지고 있으며, 최상의 경우와 최악의 경우 성능 사이에는 20포인트 정확도 차이가 있다. 13B에 추가 미세 조정을 적용하면 이러한 편향(10점 최악의 경우 저하)이 약간 감소하는 것으로 보이지만 편향은 여전히 중요하다. 그러나 추가 미세 조정이 있거나 없는 70B 모델은 대체로 유사한 경향(프라이머시와 최신 편향을 모두 나타냄)을 가지며 추가 미세 조정은 위치 편향 심각도를 최소한으로 변경한다.

도 16: 다양한 크기의 Llama-2 모델(7B, 13B, 70B 파라미터)의 다중 문서 QA 성능(총 문서 20개)으로서, 인간 피드백으로부터 추가적인 감독된 미세 조정 및 강화 학습(“-챗-” 모델)을 갖거나 갖지 않는다.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

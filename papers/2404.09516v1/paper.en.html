<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# State Space Model for New-Generation Network Alternative to Transformers: A Survey\n' +
      '\n' +
      'Xiao Wang,, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bo Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian,, Jin Tang\n' +
      '\n' +
      'Xiao Wang, Shihao Wang, Yuhe Ding, Yuehang Li, Yao Rong, Ju Huang, Haoxiang Yang, Ziwen Wang, Bo Jiang, and Jin Tang are with the School of Computer Science and Technology, Anhui University, Hefei 230601, China. (email: xiawang@ahu.edu.cn) Weizhe Kong, Wentao Wu, Shihao Li, and Chenglong Li are with the School of Artificial Intelligence, Anhui University, Hefei 230601, China. (email: lcl1314@famil.com) Yaowei Wang is with Peng Cheng Laboratory, Shenzhen, China; Harbin Institute of Technology (HITS2), Shenzhen, China. (email: wangyu@pcl.ac.cn) Yonghong Tian is with Peng Cheng Laboratory, Shenzhen, China; National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, China; School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University, China (email: yhitant@pku.edu.cn) Corresponding author: Bo Jiang (jiangbo@ahu.edu.cn)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks. However, the enormous computational demands of this architecture have deterred many researchers. To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years. In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM. Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM. After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks. Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM. More related works will be continuously updated on the following GitHub [https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List](https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List).\n' +
      '\n' +
      ' State Space Model, Mamba, Transformer, Linear Attention, Computer Vision, Natural Language Processing\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Artificial intelligence develops rapidly in the third wave which starts from the year 2010, among them, connectionism-based deep learning technology plays an extremely important role. The singular point of deep learning can be traced back to the proposal of AlexNet [1] which achieves the best performance (a far better result than second place) in the ImageNet [2] competition. After that, various Convolutional Neural Networks (CNN) are proposed one after another, such as VGG [3], ResNet [4], GoogleNet [5], etc. The ideas of blocks, residual connection, and inception inspire the design of many follow-up deep neural networks [6, 7]. On the other hand, the family of Recurrent Neural Networks (RNN), such as Long Short-Term Memory (LSTM) [8] and Gated Recurrent Unit (GRU) [9], dominates the sequence-based learning community, including natural language processing, and audio processing. Graph Neural Networks (GNNs) [10, 11] are proposed to further extend the applications of deep neural networks on graph data. However, these mainstream models still encounter bottlenecks when the datasets and computing power support are at their maximum.\n' +
      '\n' +
      'To handle the issues of only local relations captured by CNN/RNN/GNN models, the Transformer [13] proposed in the year 2017 learns the long-range feature representations well. The core operation is the self-attention mechanism which transforms the input tokens into query, key, and value features, and outputs the long-range features by multiplying the similarity matrix (obtained via product between the query and key features) with the value features. The Transformer architecture first swept the NLP community with the help of _pre-training and fine-tuning_ paradigm [14], such as BERT [15], ERNIE [16], BART [17], GPT [18]. Then, other communities are also boosted with such networks, for example, the ViT [19] and Swin-Transformer [20] released in computer vision. Many researchers also exploit the hybrid network architectures by combining Transformer and other networks, or adapting the Transformer for multi-modal research problems [21, 22]. In the current stage, large foundation models are emerging, and Parameter-Efficient Fine-Tuning (PEFT) strategies [23] also have been greatly developed. However, the current Transformer-based models still require high-end graphics cards with larger memory for training and testing/deployment, which greatly limits their wider application.\n' +
      '\n' +
      'To further decrease the computing cost, while capturing long-range dependency and maintaining high performance, many new sparse attention based models or new neural network paradigms are proposed [24, 25, 26, 27, 28]. Among them, State Space Model (e.g., Mamba [12], S4 [29], S4nd [30]), as shown in Fig. 1, becomes the center of attention. As shown in the left part of Fig. 2, the amount of SSM-related papers released shows the trend of explosive growth. The State Space Model (SSM) is a framework initially proposed to model a dynamic system using state variables in the field of control theory, computational neuroscience, etc 1. When adapting this concept for deep learning, we usually refer to linear invariant (or stationary) systems. The original SSM is a continuous-dynamic system that can be discretized for _recurrent_ and _convolutional_ views for the computer to handle. SSMs can be adopted for various data processing and feature learning, including image/video data, text data, structured graph data, event streams/point cloud data, multi-modal/multi-media data, audio and speech, time series data, tabular data, etc. It can also be utilized to build efficient generative models, such as SSMs-based diffusion generative models [31, 32, 33]. In order to help readers better understand the SSM and keep track of the latest research progress and various applications, this paper conducts a systematic review of the field and verifies the performance of the SSM model in downstream tasks experimentally. It is hoped that this review can better lead and promote the development of the field of SSM.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/blog/lbourdois/get-on-the-ssm-train](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)\n' +
      '\n' +
      '**Organization of this review.** In this paper, we first give a preliminary preview of the working principle of the State Space Model in Section 2. Then, in Section 3, we focus on reviewing the related works of SSMs from multiple aspects, including origin and variation of SSMs, natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. An overview of the structure and key State Space Model related papers reviewed in this survey is illustrated in Fig. 3. More importantly, we conduct extensive experiments on multiple downstream tasks to validate the effectiveness of SSMs in Section 4. The downstream tasks involve single-/ multi-label classification, visual object tracking, pixel-level segmentation, image-to-text generation, and person/vehicle re-identification. We also propose several possible research directions to the theory and applications of SSMs in Section 5. Finally, we give a conclusion about this paper in Section 6.\n' +
      '\n' +
      '## 2 Formulation of SSM\n' +
      '\n' +
      'State Space Model (SSM) originates from the classic Kalman filter [35], as illustrated in Fig. 1, it takes the 1-D input signal \\(\\mathbf{U}(t)\\) and maps it into N-D latent state \\(\\mathbf{X}(t)\\), then, it projects into a 1-D output signal \\(\\mathbf{y}(t)\\). The general computing procedure can be defined in Eq. 1:\n' +
      '\n' +
      '\\[\\begin{split}\\dot{\\mathbf{X}}(t)&=\\mathbf{A}(t) \\mathbf{X}(t)+\\mathbf{B}(t)\\mathbf{U}(t)\\\\ \\mathbf{y}(t)&=\\mathbf{C}(t)\\mathbf{X}(t)+\\mathbf{D} (t)\\mathbf{U}(t)\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{X}(t)\\in\\mathbb{R}^{n}\\), \\(\\mathbf{y}(t)\\in\\mathbb{R}^{q}\\), \\(\\mathbf{U}(t)\\in\\mathbb{R}^{p}\\) denotes the _state vector_, _output vector_, and _input (or control) vector_. \\(\\dot{\\mathbf{X}}(t)=\\frac{d}{dt}\\mathbf{X}(t)\\). \\(\\mathbf{A}(t)\\in\\mathbb{R}^{n\\times p}\\), \\(\\mathbf{B}(t)\\in\\mathbb{R}^{n\\times p}\\), \\(\\mathbf{C}(t)\\in\\mathbb{R}^{q\\times n}\\), and \\(\\mathbf{D}(t)\\in\\mathbb{R}^{q\\times p}\\) represents state matrix, input matrix, output matrix, and feed-forward matrix. When there is no direct feedthrough in the system model, \\(\\mathbf{D}(t)\\) is a zero matrix, thus, we get the following simplified equations:\n' +
      '\n' +
      '\\[\\begin{split}\\dot{\\mathbf{X}}(t)&=\\mathbf{A}(t) \\mathbf{X}(t)+\\mathbf{B}(t)\\mathbf{U}(t)\\\\ \\mathbf{y}(t)&=\\mathbf{C}(t)\\mathbf{X}(t).\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'As the raw system is continuous, we need to first discretize them before feeding the computer, as shown in Fig. 2. For the Mamba architecture, the zero-order hold (ZOH) 2 is adopted for the discretization and we have:\n' +
      '\n' +
      'Footnote 2: [https://en.wikipedia.org/wiki/Zero-order_hold](https://en.wikipedia.org/wiki/Zero-order_hold)\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{X}_{t}&=\\overline{\\mathbf{A}} \\mathbf{X}_{t-1}+\\overline{\\mathbf{B}}\\mathbf{U}_{t}\\\\ \\mathbf{y}_{t}&=\\mathbf{C}\\mathbf{X}_{t}\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\overline{\\mathbf{A}}=exp(\\Delta\\mathbf{A})\\), \\(\\overline{\\mathbf{B}}=(\\Delta\\mathbf{A})^{-1}(exp(\\Delta\\mathbf{A})-\\mathbf{ I})\\cdot\\Delta\\mathbf{B}\\), \\(\\Delta\\) denotes the step size. If we denote the _state vector_ and _input vector_ using \\(\\mathbf{h}\\) and \\(\\mathbf{x}\\), we obtain the following functions\n' +
      '\n' +
      'Fig. 1: [left gray sub-figure] Block diagram representation of the linear state-space equations (re-draw based on state-space representation); [right sub-figure] The formulation of widely used Mamba architecture (re-draw from [12]).\n' +
      '\n' +
      'Figure 3: Structure and key State Space Model papers reviewed in this survey.\n' +
      '\n' +
      'Figure 2: [left sub-figure] Number of papers released to date (from year 2021 to year 2024.04); [right sub-figure] Three different representations of SSM can be viewed and computed, i.e., continuous-time, recurrent, or convolutional model. This figure is re-draw based on [34].\n' +
      '\n' +
      'similar to the computing procedure of the Recurrent Neural Network (RNN) model, as shown in Fig. 5(b):\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{h}_{t}&=\\mathbf{\\overline{A}} \\mathbf{h}_{t-1}+\\mathbf{\\overline{B}}\\mathbf{x}_{t}\\\\ \\mathbf{y}_{t}&=\\mathbf{C}\\mathbf{h}_{t}.\\end{split} \\tag{4}\\]\n' +
      '\n' +
      'However, similar to the RNN model, we face the dilemma that the computation cannot be _parallelized_. By simply expanding the above formula, we have:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{y}_{0}&=\\mathbf{C}\\mathbf{ \\bar{A}}^{0}\\mathbf{\\bar{B}}\\mathbf{x}_{0}\\\\ \\mathbf{y}_{1}&=\\mathbf{C}\\mathbf{\\bar{A}}^{1} \\mathbf{\\bar{B}}\\mathbf{x}_{0}+\\mathbf{C}\\mathbf{\\bar{A}}^{0}\\mathbf{\\bar{B}} \\mathbf{x}_{1}\\\\ \\mathbf{y}_{2}&=\\mathbf{C}\\mathbf{\\bar{A}}^{2} \\mathbf{\\bar{B}}\\mathbf{x}_{0}+\\mathbf{C}\\mathbf{\\bar{A}}^{1}\\mathbf{\\bar{B}} \\mathbf{x}_{1}+\\mathbf{C}\\mathbf{\\bar{A}}^{0}\\mathbf{\\bar{B}}\\mathbf{x}_{2}. \\end{split} \\tag{5}\\]\n' +
      '\n' +
      'It is easy to find that the multiplier of the last and penultimate term is always \\(\\mathbf{C}\\mathbf{\\bar{A}}^{0}\\mathbf{\\bar{B}}\\) and \\(\\mathbf{C}\\mathbf{\\bar{A}}^{1}\\mathbf{\\bar{B}}\\). Therefore, we can treat these multipliers as the convolutional kernel \\(\\mathbf{\\overline{K}}=\\mathbf{C}\\mathbf{\\bar{B}}\\cdot(\\mathbf{\\bar{A}}^{0}, \\mathbf{\\bar{A}}^{1},\\mathbf{\\bar{A}}^{2},...,\\mathbf{\\bar{A}}^{L})\\), here, \\(L\\) is the length of the given input sequence. We can rewrite the Equ. (4) as the following convolutional formulations:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{\\overline{K}}&=(\\mathbf{C} \\mathbf{\\overline{B}},\\mathbf{C}\\mathbf{\\overline{A}}\\mathbf{\\overline{B}},...,\\mathbf{C}\\mathbf{\\bar{A}}^{L}\\mathbf{\\overline{B}},...)\\\\ \\mathbf{y}&=\\mathbf{x}*\\mathbf{\\overline{K}}. \\end{split} \\tag{6}\\]\n' +
      '\n' +
      'At this moment, we get the complete SSM model that can realize the parallelism of training and is suitable for the recurrent form of linear complexity of inference. In the Transformer architecture, the context information is stored in the similarity matrix, however, the SSM doesn\'t have a similar module which makes it perform poorly in contextual learning.\n' +
      '\n' +
      'To address this issue, Gu et al. propose the Mamba [12] architecture which improves the SSM from the following two aspects: _1). Selective Scan Operator_ allows the model to filter relevant information out. In practical implementation, the \\(\\Delta\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) become the functions of the input, meanwhile, the matrix \\(\\mathbf{A}\\) keeps unchanged. _2). Hardware-aware Algorithm_ that allows efficient storage of (intermediate) results through parallel scanning, kernel fusion, and recalculation. An illustration of the architecture of the Mamba block is provided in the right part of Fig. 1. Due to the key features, many researchers attempt to design their model using SSM or Mamba architectures.\n' +
      '\n' +
      '## 3 State Space Model\n' +
      '\n' +
      'In this section, we focus on reviewing the related works on the SSM architectures and applications. We divide the related works into the following domains, i.e., the origin and variation of SSM, natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and others. In the following subsections, we will introduce these algorithms one after another.\n' +
      '\n' +
      '### _Origin and Variation of SSM_\n' +
      '\n' +
      'The State Space Model originates from Kalman filtering [35] which mainly introduces a linear filtering and prediction method. Kalman filtering can be divided into two steps, i.e., the prediction and correction step. The prediction is to estimate the current state based on the state of the previous time, and the correction is to estimate the optimal state by integrating the estimated state and the observed state of the current time. The State Space Model is a mathematical model that describes the behavior of a dynamic system using a set of first-order differential equations (continuous-time systems) or difference equations (discrete-time systems) to represent the evolution of the internal state of the system, and another set of equations to describe the relationship between the state and the output of the system. These equations can be expressed in matrix and vector form to deal with multivariable systems. Subsequently, Gu et al. [34] introduces a Linear State Space Layer (LSSL) that combines the advantages of recurrent neural networks (RNNs), temporal convolutional networks, and neural differential equations (NDEs) while addressing their shortcomings in model power and computational efficiency. This new sequence model is inspired by control systems and implemented through the linear state space layer (LSSL).\n' +
      '\n' +
      'Similar to RNNs, SSM also suffers from the vanishing/exploding gradients problem when modeling longer sequences. To tackle this issue, HiPPO [36] model combines the concepts of Recurrent Memory and Optimal Polynomial Projections, which can significantly improve the performance of recursive memory, This mechanism is very helpful for SSM to handle long sequences and long-term dependencies. The formula can be expressed as follows:\n' +
      '\n' +
      '\\[A_{nk}=\\begin{cases}(2n+1)^{1/2}(2k+1)1/2&\\text{if }n>k\\\\ n+1&\\text{if }n=k\\\\ 0&\\text{if }n<k\\end{cases} \\tag{7}\\]\n' +
      '\n' +
      'Fig. 4: The timeline of representative SSMs-based algorithms (from year 2020 to 2024.04.)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}} \\hline \\hline\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:8]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'RetNet [127], Mega [128], H3 [129]. Specifically, Acceptance Weighted Key Value (short for RWKV) [125] is a kind of RNN architecture and is developed based on attention-free Transformer [130] for natural language processing. It simultaneously features in the efficient parallelizable training of transformers and the efficient inference of RNNs. Duan et al. further adapt this framework to the computer vision tasks and propose the Vision-RWKV (VRWKV) [126] model. Their results demonstrate that it beats the ViT in the image classification task and also has significant advantages in speed, and memory usage (when processing the high-resolution inputs). The RWKV architecture is also widely used in many other tasks, such as time series related task [131], online action detection [132], diffusion model [133]. RetNet [127] is short for Retentive Network, which also targets building a large language model that achieves training parallelism, low-cost inference, and high performance, simultaneously. It supports parallel, recurrent, and chunkwise recurrent computation paradigms.\n' +
      '\n' +
      'Based on the origin and variations of SSM mentioned above, many SSM-based works are constantly emerging, including but not limited to natural language processing, computer vision, and so on. The following summaries will respectively introduce the expansion and application of various fields.\n' +
      '\n' +
      '### _Natural Language Processing_\n' +
      '\n' +
      'In recent years, the development of large language models has revolutionized the field of natural language processing, however, the widely used Transformer architecture is limited by high computational and memory requirements. To address these issues, many researchers devoted themselves to simplifying the Transformers to achieve efficient computation and limited memory requirements. Among them, State Space Model is one of the most effective solutions we reviewed in this paper.\n' +
      '\n' +
      'With the emergence of the Mamba [12], the SSM model is increasingly attracting attention and favor from current researchers. The following works are currently explored in language modeling task [41][45][46][47][48], deep noise suppression task [42], and clinical note understanding task [49]. To be specific, for the language modeling task, [41] mainly studies the application of gated state spaces in the direction of long-range language modeling and introduces a novel method called GSS (Gated State Space). It can be used on long sequence modeling and effectively reduce the number of participants. Their experiments demonstrate that it achieves 2-3 times faster than the DSS [38]. Grazzi et al. [45] exploit the Mamba on simple function estimation and natural language processing in context learning tasks and validate that the overall performance is indeed better than the S4 version and comparable to other Transformer networks. S4++ [46] finds two issues of S4 architecture, i.e., the non-stationary state (NSS) and dependency bias, and proposes the State Memory Reply (SMR) mechanism to integrate multi-state information into the current state. They also integrate complex dependency bias via an interactive cross-attention mechanism and extensive experimental results show that S4++ outperforms S4 on multiple sequence modeling tasks, demonstrating significant performance gains. [47] synthesizes State Space Model and local attention mechanism to reduce memory consumption and\n' +
      '\n' +
      'Fig. 5: A comparison between CNN, RNN, Transformer, Mamba, and Linear Attention.\n' +
      '\n' +
      'speed up training efficiency while ensuring performance. The authors use local attention to extract local information, and then use the state-space model to extract the global information missing from local attention. [48] argues that existing State Space Models, while efficient, lack in performance. The authors believe the reason for this is that too many state transitions make the model lose shallow information. So the authors propose a design that integrates the hidden states in the previous layers into the subsequent layers to retain more shallow information. Eventually, after pre-training on Pile, the experimental results of Zero-shot and four-shot on other datasets are significantly improved. For voice tasks, Du et al. [42] combine the high efficiency of impulse neural networks and the ability to model long distances with the state-space model S4 to obtain an spiking neural network, which has a low number of parameters but comparable performance to that of some artificial neural network (ANN) in deep noise suppression task. In addition, for speech separation task, DPMamba [43] proposed by Jiang et al. uses the selective State Space Model Mamba to replace the traditional transformer architecture. DPMamba simultaneously models the short-term and long-term forward and backward dependencies of speech signals through selective state space, achieving comparable results to the dual-path Transformer model Sepformer [134]. SPAMba [44] proposed by Li et al. uses TF-GridNet [135] as the basic framework and replaces the Transformer module with a bidirectional Mamba module to capture a wider range of language information. Experimental results show that Mamba-based models play an important role in performance.\n' +
      '\n' +
      'In the clinical notes Understanding task, Yang et al. [49] exploits the linear computational complexity of Mamba to model very long sequences of clinical notes, with sequence lengths of up to 16k. The authors use the MIMIC-III dataset to pre-train the Mamba model, which is then tested on a cohort selection task and an ICD coding task, and demonstrates superior performance in modeling clinical language, especially at longer text lengths, when compared to both the Mamba and the clinical Llama models. Compared to Mamba and clinical Llama models, it shows superior performance in modeling clinical language, especially at longer text lengths. In the translation task, [50] formulates the problem of generating dance choreography as a translation task and proposes the MDLT which utilizes existing datasets to learn how to translate audio sequences into corresponding dance poses.\n' +
      '\n' +
      '### _Computer Vision_\n' +
      '\n' +
      'Recently, the linear time series modeling of the State Space Model has attracted widespread attention, demonstrating strong performance in the field of natural language processing. Inspired by these progress, many SSM-based vision models have been proposed, including classification task [60, 61, 68, 61, 68, 69, 76, 83, 92, 93, 94, 95, 96, 97, 98, 99, 101, 112, 137, 138], detection task [109, 110, 117], segmentation task [70, 82, 85, 89, 91, 106, 108, 111], medical tasks [63, 64, 69, 72, 87, 97, 98, 100], restoration task [77, 110, 56], generation task [31, 32, 33], video understanding [56, 58, 90], track task [88], and others task [32, 59, 62, 73, 74, 75, 103, 104, 105, 107, 116, 118, 120, 121].\n' +
      '\n' +
      'In the classification task, S4nd [30] proposes a multi-dimensional and multi-polar graphics component to expand the modeling capability of multi-dimensional data continuous signals, which can model large-scale visual data into dynamic multi-dimensional linear signals. VMamba [60] uses linear complexity to capture the full range of sensory fields, introduces traversal of spatial information across scan blocks, and converts non-causal visual images into ordered patch sequences. Vim [61] uses a bidirectional state-space model to compress visual representation information and understand the global context through location embedding and visual information. Li et al. present Mamba-ND [68], an extension of Mamba designed to handle arbitrary multi-dimensional data by processing input data across dimensions in a row-major order. The authors of [57] design S5 based on S4 to establish the relationship between S5 and S4, utilize multi-input multi-output SSM, and use the state space layer of parallel scanning for long-distance sequence modeling. Baron et al. [76] design a new 2-dimensional State Space Layer for Spatial Inductive Bias. The core goals of this layer are to achieve perception of 2-D position, dynamic spatial localization, and translation and alignment invariance. Chen et al. [79] are the first to integrate residuals into the original VMamba, and maintain the inherent global and local state characteristics of the original VMamba for food classification. Yang et al. [101] propose the PlainMamba, which further adapts Mamba\'s selective scanning process\n' +
      '\n' +
      'Fig. 6: Illustration of Jamba block [136] and used different types of layers.\n' +
      '\n' +
      'to the visual field. By improving spatial continuity through the continuous 2D scanning process and updating direction perception, the model can distinguish spatial relationships of labels by encoding direction information, thereby enhancing its ability to learn features from 2D images. Wang et al. propose InsectMamba [112] that can be used in the insect classification task and improve the classification ability of the model by integrating a State Space Model, a convolutional neural network (CNN), a multi-head self-attention mechanism (MSA), and multi-layer perceptrons (MLPs) in a hybrid state space module (Mix-SSM).\n' +
      '\n' +
      'Huang et al. introduce LocalMamba [92], which proposes a new local scanning strategy to preserve the two-dimensional dependencies of spatial tokens. They conduct extensive experiments on various tasks and demonstrate that LocalMamba improves over Viim-T by +3.1% on the ImageNet classification. Xu et al. [93] introduce an SSM model termed MambaTalk which focuses on gesture synthesis and supports long and various sequences. Pei et al. [94] proposes the EfficientVAMba by incorporating additional convolutional branches and further improves the baseline significantly on the ImageNet-1K and COCO detection datasets. Du et al. [95] explore the robustness of VAMba from various aspects, for example, they investigate the resilience to adversarial attacks using both whole-image and patch-specific methods, revealing superior robustness compared to Transformer architectures but with scalability weaknesses. They also assess VAMba\'s general robustness across diverse scenarios. Shi et al. [96] propose a new image restoration method, VambaIR, which overcomes some of the shortcomings of traditional methods by introducing the linear complexity of state-space modeling to a compressive image restoration task. Fang et al. [83] present the MamMI framework to address the classifying of whole slide images which is the first work to combine the selective structured State Space Model (Mamba) and a multi-instance learning (MIL) approach. MamMIL outperforms existing state-of-the-art MIL frameworks based on Transformer in terms of classification performance and memory usage. Li et al. introduce a novel approach to wearable sensor human activity recognition (HAR) called HARMamba [137], which utilizes a lightweight selective State Space Model (SSM) designed to address computational resource constraints typical of real-time mobile applications. Yang et al. introduce a novel hyperspectral image classification framework called HSIMamba [138], which aims to address the complexity and high-dimensional nature of hyperspectral imaging data in remote sensing. The proposed framework incorporates a bidirectional reversed CNN to efficiently extract spectral features, alongside a specialized block for spatial analysis.\n' +
      '\n' +
      'For the detection task, Chen et al. [80] propose a Mamba-in-Mamba (MiM-ISTD) structure to detect the infrared small targets. In this structure, the images are evenly divided into "visual sentences" (patches) and further subdivided into "visual words" (sub-patches), and a pure Mamba-based MiM pyramid encoder is designed to extract global and local features. Chen et al. [109] explore the potential of the Mamba architecture for remote sensing image change detection tasks by employing visual Mamba as an encoder which is capable of fully learning the global contextual\n' +
      '\n' +
      'Fig. 7: An overview of the VAMba model (V Mamba-T) [61] and Vision Mamba (Vim) [60].\n' +
      '\n' +
      'information of the input image. Three methods for the modeling of spatio-temporal relationships are proposed for the decoder, taking full advantage of the properties and benefits of the Mamba architecture. He et al. [117] capture long-range and local information effectively through parallel cascaded hybrid state space and multi-kernel convolution operations.\n' +
      '\n' +
      'For the segmentation task, a semi-supervised medical image segmentation method termed Semi-Mamba-UNet [70] is proposed which combines a visual manb-based UNet architecture with the conventional UNet. It utilizes dual networks to generate pseudo labels and mutually cross-supervise each other. Additionally, it employs a self-supervised pixel-level contrastive learning strategy to bolster feature learning capabilities. P-Mamba [71] is designed for Efficient Pediatric Echocardiographic Left Ventricular Segmentation, which tackles the challenges associated with accurately segmenting the left ventricle in pediatric echocardiograms. Liao [89] introduces the LightM-UNet, a streamlined framework that merges Mamba and UNet to tackle computational constraints in medical image segmentation. It extracts profound semantic features and captures extensive spatial dependencies with linear computational complexity. Empirical evaluations on real-world datasets underscore LightM-UNet\'s supremacy over current leading methods, showcasing substantial reductions in both parameter count and computational overhead. Zhang et al. [91] propose an SSM-based U-Net variant medical image segmentation model, VM-UNetV2, which fully utilizes the capabilities of SSM models. By initializing the encoder using VAMba pretrained weights and employing a deep supervision mechanism, VM-UNetV2 demonstrates competitive segmentation performance on multiple datasets. U-mamba [63], as a general-purpose CNN-SSM network, enhances biomedical image segmentation by integrating local CNN features with long-range dependencies of SSMs. Leveraging ImageNet-based pretraining, Swin-umamba [64], a novel Mamba-based model, outperforms CNNs, ViTs, and existing Mamba models. It demonstrates superior performance with lower memory and computational burden and reveals the essential role of ImageNet-based pretraining in promoting the performance of Mammba family models.\n' +
      '\n' +
      'VM-UNet [65] establishes a baseline as the first pure SSM-based model for medical image segmentation. It competes effectively on ISIC17, ISIC18, and Synapse datasets, offering insights for future SSM-based segmentation systems. Gong et al. [66] propose the nnMamba which combines CNNs\' detailed feature extraction with SSMs\' broad dependency modeling, excelling in 3D medical image tasks. It proposes the Mamba-In-Convolution with Channel-Spatial Siamese learning (MICCSS) block to model the long-range relationship of the voxels. The superior performance is gained in 3D segmentation, classification, and landmark detection across 6 datasets. LMa-UNet [85] is a novel Large Window-based Mamba U-shape Network, leveraging large windows for improved spatial modeling compared to CNNs and Transformers, maintaining efficiency with linear complexity. It introduces a hierarchical and bidirectional Mamba block to enhance global and local spatial modeling. Tang et al. [82] use triple state space to fuse features in spatial and channel dimensions, and residual blocks to extract dense context features. Kazi et al. [102] take advantage of Mamba-UNet and the lighter version of the Hierarchical Upsampling Network (HUNet), the local feature extraction ability of the convolutional neural network is combined with the remote dependency modeling ability of the State Space Model. RS3Mamba [108] propose a novel two-branch network called RS3Mamba, which introduces a novel visual state space (VSS) model, Mamba, to the task of semantic segmentation of remotely sensed imagery. RS3Mamba constructs an auxiliary branch using the VSS blocks to provide additional global information for the main branch and introduces a co-completion module (CCM) to augment and fuse features from the dual encoder. Hao et al. [106] introduce a 3D CBCT segmentation method for teeth, termed T-Mamba, which enhances spatial position preservation and feature enhancement in the frequency domain by fusing shared position coding and frequency-based features. T-Mamba is the first work that introduces frequency features into the visual mamba architecture. Zhu et al. [113] propose a new semantic segmentation framework Samba based on the Mamba architecture and design specifically for high-resolution remote sensing images. Samba demonstrates the effectiveness and potential of the Mamba architecture in semantic segmentation of remote sensing images, surpassing current state-of-the-art CNN and ViT-based methods. Ma et al. [108] propose a novel dual branch network called RS3Mamba,which utilizes VSS blocks to construct auxiliary branches, providing additional global information for convolutional based main branches. In addition, considering the feature differences between the two branches, a Collaborative Completion Module (CCM) is introduced to enhance and fuse features from the dual encoder. Archit et al. [119] propose a new medical image segmentation network architecture, ViM-UNet. It is based on the latest Vision Mamba architecture and compared with traditional UNet and Transformer-based UNETR.\n' +
      '\n' +
      'For the medical image based analysis, Guo et al. [97] introduce a medical MR-CT deformable registration method based on the Mamba framework, named MambaMorph. The key to this method lies in achieving voxel-level spatial correspondence capture across different imaging modalities, which is crucial for medical image analysis. Xie et al. introduce a new polyp segmentation model ProMamba [99] based on the Vision Mamba architecture and prompts technology. It is the first time to introduce the Vision Mamba and prompts into polyp segmentation. Wu et al. [100] introduce a novel neural network for medical image segmentation based on SSM and SS2D, called High-order Vision Mamba UNet (H-vmunet), which gradually reduces the introduction of redundant information through advanced interaction and enhances the ability of SS2D to learn local features at each interaction stage. Vivim is proposed by Yang et al. [98], which targets effectively compressing long-term spatio-temporal representations into sequences of different scales through the designed time Mamba blocks for medical video object segmentation. Due to the scarcity of large labeled medical datasets, Wang et al. [72] propose the Weak-Mamba-Unet architecture, which attempts to address this challenge by training a Mamba-based UNet in a weakly-supervised manner. It leverages convolutional neural networks (CNNs), Vision Transformers (ViTs), and Vmamba to predict the data labels, then, generate dense pseudo labels.\n' +
      '\n' +
      'Zheng et al. [69] describe a novel network architecture called FD-Vision Mamba (FDVM-Net) designed to correct exposure abnormalities in endoscopic images. This is crucial for maintaining image quality to assist healthcare professionals in their decision-making process. FDVM-Net operates in the frequency domain and reconstructs the endoscopic image\'s frequency representation to improve exposure. Xing et al. [62] propose the SegmamBA module to enhance 3D feature modeling, which uses gated spatial convolution internally to enhance feature representation in spatial dimension to deal with long-distance dependency. Yue et al. [81] propose the MedMamba based on the State Space Model and convolutional layers for medical image classification. It can effectively capture long-range dependencies while maintaining the ability to extract local features, which is suitable for medical images of different modalities. Huang et al. [78] inherit the advantages of the original Mamba linear complexity and global receptive field, and propose an arbitrary mask mechanism to adapt the Mamba to the image reconstruction task, termed MambaMIR-GAN. Schiff et al. [87] extend the Mamba block into a component supporting bi-directional BiMamba and a MambaDNA supporting RC equivariant. Then, they use the MambaDNA as a basis for Caducucus and incorporate pre-training and fine-tuning strategies for DNA Sequence Modeling.\n' +
      '\n' +
      'For the restoration tasks, Guo et al. [77] propose a new image restoration model, termed MambaIR, which aims to explore the potential of Mamba in low-level vision, the model leverages the long-range dependent modeling capabilities of the Mamba state-space model while combining prior knowledge unique to image restoration tasks, such as local block repetition and channel interaction. Serpent [110] uses the State Space Model to maintain a global receptive field with linear scaling of input sizes which significantly reduces the cost of computing resources and GPU memory.\n' +
      '\n' +
      'For the generation task, ZigMa [32] introduces a new diffusion model based on the Mamba structure, called ZigMa, which targets addressing the scalability and quadratic complexity issues of existing diffusion models, especially in Transformer structures. DiffuSSM [31] is a scalable state-space model that handles higher resolutions and can retain a detailed image representation throughout the diffusion process, as it does not use global compression. DiS [33] is a new category of diffusion models that are based on a state space architecture. It aims to train diffusion models for image data, replacing the traditional U-Net-like backbone with a state space backbone that operates on raw patches or latent space.\n' +
      '\n' +
      'For video understanding, ViS4mer [56] utilizes a multi-scale temporal structured state-space sequence decoder for long-term inference. The resolution of spatiotemporal features and channel dimension of each decoder layer are gradually reduced, enabling the learning of complex long-range spatiotemporal dependencies. Wang et al. propose the LSMCL [58] which is a learning method of short and long mask contrast and can predict long-range spatiotemporal information. Chen et al. [90] evaluate the Mamba\'s potential as an alternative to Transformers in video understanding, explore different roles that the Mamba can play in video modeling, and assess its performance across diverse video understanding tasks. Li et al. [84] propose a video understanding model based on the State Space Model, Video-Mamba, which can efficiently process long videos.\n' +
      '\n' +
      'To maintain the consistency of dual-camera tracking and address the large variation in the endoscope tip appearance, Zhang et al. [88] propose a cross-camera mutual template strategy (CMT) and introduce a dynamic transient mutual template during tracking. A Mamba-based motion-guided predictive head (MMH) is introduced to minimize the interference caused by large area occlusion and the distortion caused by the endoscope tip light source.\n' +
      '\n' +
      'In the other tasks, as more researchers recognize the advantages of Mamba, this model has gained traction across various fields. Pan-Mamba [73] represents the first foray into the pan-sharpening domain. It comprises two main modules: the channel swapping Mamba and the cross-modal Mamba. The channel swapping Mamba aims to fuse and enhance the diversity of features from PAN channels and LRMS channels in a lightweight and efficient manner. The latter module, the cross-modal Mamba, is deployed after the channel swapping Mamba to filter redundant modal characteristics through gating mechanisms. DreamerV3 [59]\n' +
      '\n' +
      'Fig. 8: Different selective scan methods used in SSMs proposed for image and video processing. (a) VMamba [61], (b) Vision Mamba [60], (c) RSMamba [139], (d-f) Video Mamba [84].\n' +
      '\n' +
      'is a universal and extensible method based on the world model, which overcomes the limitations of fixed parameter range in various fields in terms of the input, dimension, and reward of data. For long-range prediction, Naman et al. [74] introduce a novel approach to sequence modeling, called Spectral State SSM, which is based on learning linear dynamical systems (LDS) using the spectral filtering algorithm. This architecture guarantees stable and efficient learning even for marginally stable symmetric LDS. For reinforcement learning tasks, HIEROS [75], a hierarchical policy aims at improving sample efficiency. HIEROS utilizes a hierarchical world model, specifically an S5 layer-based world model (S5WM), and an efficient time-balanced sampling method. It outperforms existing approaches in terms of mean and median normalized human scores on the Atari 100k benchmark and demonstrates superior exploration capabilities. Cheng et al. [86] explore how modern State Space Models, Vim, can enhance the performance of convolutional neural networks (CNN) and visual Transformers (ViT) in the field of single image super-resolution (SISR) through a wider range of activation regions. VMRNN [103] is a new recurrent unit that combines Vision Mamba blocks with LSTM for precise and efficient spatiotemporal forecasting. Shen et al. [104] introduce Gamba, an end-to-end, amortized 3D reconstruction model from single-view images. Their main discovery involves utilizing a substantial number of 3D Gaussians to enhance the efficiency of the 3D Gaussian splitting process. Additionally, they introduce a Mamba-based sequential network, enabling context-dependent reasoning and linear scalability with sequence (token) length, aiming to tackle high memory demands and resource-intensive rendering processes. Wang et al. [105] introduce a novel visual Mamba-based framework called VMambaMorph, which has cross-scanning modules for deformable 3D image registration. Li et al. [107] propose a novel approach called SpikeMba for dealing with temporal video localization tasks. SpikeMba integrates Impulse Neural Networks and State Space Models (SSMs) to efficiently capture the fine-grained relationships between multimodal features. Zou et al. [116] proposes a new remote photoplethysmography (rPPG) signal detection method based on Mamba, called RhythmMamba. Rhythmamba is an end-to-end method that employs multi-temporal constraints to capture both periodic patterns and short-term trends in rPPG. Additionally, it utilizes frequency domain feed-forward to enhance Mamba\'s ability to robustly interpret the quasi-periodic rPPG patterns.\n' +
      '\n' +
      '### _Graph_\n' +
      '\n' +
      'In addition to the standard grid data (e.g., image), structured graph data is also widely studied in artificial intelligence, such as the social network and protein structure data. Because its input type is sequential data, thus, we can apply the SSMs to process the graph-structured data. Specifically, Graph54mer [52] leverages the Structured State Space (S4) architecture to capture long-range temporal dependencies and introduces a graph structure learning layer to dynamically evolve graph structures, adapting to the data\'s spatial correlations over time. GMNs [53] is a new class of Graph Neural Networks (GNNs) based on selective State Space Models, tackling the limitations of traditional GNNs in capturing long-range dependencies and computational efficiency. The framework introduces a graph tokenization process that bridges node-level and subgraph-level tokenization, facilitating efficient learning of graph structures. Another concurrent work Graph-Mamba [51] is also developed based on the Mamba architecture. Graph-Mamba includes a node prioritization technique to prioritize important nodes for more access to context and employs a permutation-based training recipe to minimize sequence-related biases. Ali Behrouz et al. propose the GRED [53] which is a new graph representation learning architecture that aggregates other nodes based on their shortest distance to the target for a given target node. They adopt the linear RNN to encode the skip representation sequence. Gregor et al. [140] discuss the issues of teachers being unable to accurately learn the next token predictor through mandatory training, and demonstrate the failure of Transformer and Mamba architectures in multi-token prediction training through the simplest planning task. Li et al. [55] introduce STG Mamba which is the first attempt to process STG learning using a powerful selective State Space Model.\n' +
      '\n' +
      '### _Multi-modal and Multi-media_\n' +
      '\n' +
      'The State Space Model can also be adapted for multi-modal/multi-media tasks. Specifically, 54ND [30] extends State Space Models (SSMs) to multidimensional signals, enabling the modeling of large-scale visual data as continuous multidimensional signals. This method has been demonstrated to be effective across different dimensions (1D, 2D, and 3D), encompassing applications in image and video classification. Grazzi et al. [45] evaluate Mamba with in-context learning (ICL) capabilities similar to Transformer. The analysis shows that, like Transformer, Mamba appears to solve the ICL problem by gradually improving its internal representation like an iterative optimization strategy. For ICL tasks involving longer input sequences, Mamba can be an effective alternative to Transformer. Park et al. [141] also evaluate Mamba\'s performance and their results show that Mamba\'s performance in standard regression ICL tasks is comparable to Transformer\'s. Its performance in sparse parity learning tasks is better. However, it performed poorly on tasks involving non-standard retrieval functions. The MambaFormer [141], consisting of Mamba together with attention blocks, was used to solve the above challenges, outperforming any single model in each task. Zucchet et al. [142] reveal a closer conceptual relationship between RNN and Transformer. The experimental results prove that RNN and Transformer are not completely exclusive models. It is also shown that linear self-attention can be achieved in theory and practice by learning gated RNNs with multiplicative interactions, bridging the gap between these two architectures. Ali et al. [143] explore the learning mechanisms of Mamba models, in particular how dependencies are captured and their similarity to other established layers, such as RNN, CNN, or attention mechanisms. An important relationship between the Mamba and the self-attention layer is established. The basic properties of the Mamba model are clarified by showing that they depend on implicit attention to be realized by a unique data-controlledlinear operator, indicating that the selective state-space layer is an attention model. By utilizing the obtained attention matrices, a set of interpretability techniques based on these hidden attention matrices are provided. MamboMIL [144] integrates the Mambo framework into MIL, with SR-Mambo as the core component, which is good at capturing remote dependencies between dispersed positive instances. MamboMIL can efficiently capture more discriminant features and mitigate the challenges associated with overfitting and high computational overhead, marking the first application of the Mambo framework in computational pathology.\n' +
      '\n' +
      'Motion Mamba [88], composed of Hierarchical Temporal Mambo (HTM) and Bidirectional Spatial Mamba (BSM), represents the first integration of Mamba models in the field of motion generation. HTM and BSM are designed for temporal and spatial modeling, respectively, while integrating selective scanning mechanisms into motion generation tasks. Compared with the previous diffusion-based motion generation method, which mainly uses a Transformer, motion Mamba achieves state-of-the-art (SOTA) performance. Note that the inference speed is also sped up by four times. VL-Mambo [145] is the first effort to explore the state-space model Mamba to solve the expensive computational overhead in the Transformer architecture in multimodal learning tasks. CMViM [146] focuses on the application of multimodal representation learning to 3D high-resolution medical images, especially Alzheimer\'s disease (AD). It is developed based on MAE [150] framework and replaces ViT [151] module with Vim [61] module, therefore, reducing complexity from quadratic to linear level. Moreover, the intra-modality and inter-modality contrastive learning methods are introduced to enhance the ability of the multimodal Vim encoder to model discriminative features in the same modality and mitigate the misaligned representation between modalities. Cobra [147] explores the combination of language models with linear computational complexity and\n' +
      '\n' +
      'Fig. 9: Representative blocks designed based on State Space Model ( a [61], b [60], c [98], d [86], e [99], f [31], g [88], h [46], i [42], j [47], k [52], [103]).\n' +
      '\n' +
      'multimodal inputs. It replaces the Transformer networks commonly used in current models with a more efficient Mamba architecture.\n' +
      '\n' +
      'In terms of visual and linguistic information fusion, Zhao et al. [147] optimizes the internal information integration of the Mamba language model to achieve a more effective expression. Decision Mamba(DMamba) [148] integrates the Mamba framework into the Decision Transformer (DT) [152]. A series of experiments comparing Decision Mamba and DT show that Mamba is effective in reinforcement learning (RL) tasks. But simply applying Mamba blocks to DT does not improve efficiency, because the RL tasks considered by the authors have a large number of CPU and GPU interactions. Another deficiency is the absence of a hyper-parameter search and an analysis of how to use the Mamba block more effectively to reflect the data structure of RL tasks. Sigma [149] is the first State Space Model successfully applied in multi-modal semantic segmentation. It is composed of VMamba, an attention-based Mamba fusion mechanism and a channel-aware Mamba decoder, and has shown excellent performance in various experiments. However, Sigma is underutilized in handling longer sequences, and the memory consumption of Mamba encoders is still relatively large, making it difficult to deploy on lightweight edge devices.\n' +
      '\n' +
      '### _Event Stream/Point Cloud Data_\n' +
      '\n' +
      'Inspired by the success of the State Space Model (SSM) in natural language processing, PointMamba [153] leverages the strengths of SSM to introduce a framework boasting global modeling capabilities while maintaining linear complexity. This innovative model operates by taking embedded point patches as inputs, employing a reordering strategy, and subsequently feeding these point patches into a series of Mamba blocks to bolster the global modeling capability of SSM. PCM [154] proposes a consistent traverse serialization\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c} \\hline \\hline\n' +
      '**\\#ID** & **Algorithm** & **Publish** & **Domain** & **Parameters** & **Architecture** & **Downstream Tasks** & **Accuracy** & **Code** \\\\ \\hline \\multirow{3}{*}{96} & \\multirow{3}{*}{**SS [57]**} & \\multirow{3}{*}{ICLR23} & Multi-modal & \\multirow{3}{*}{280K} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{Classification} & Speech Commands & \\multirow{3}{*}{URL} \\\\  & & & Multi-media & & & & (164kHz)96.52 & & \\\\  & & & & & & & (8kHz)94.53 & \\\\ \\hline \\multirow{3}{*}{97} & \\multirow{3}{*}{**Grazzi et al. [45]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} \\\\  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{98} & \\multirow{3}{*}{**MambaFormer [141]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba+Former & - & - & - \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{99} & \\multirow{3}{*}{**Zucchet et al. [142]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{100} & \\multirow{3}{*}{**Mamba [12]**} & \\multirow{3}{*}{arXiv24} & Multi-media & Mamba-130M & \\multirow{3}{*}{Mamba} & Synthetic & Synthetic tasks99.8 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & Mamba-370M & & & Zero-shot(Average ACC) & URL \\\\  & & & Mamba-90M & & & Zero-shot & 447(130M) 30.037(M) & \\multirow{3}{*}{URL} \\\\  & & & Mamba-1.4B & & & 57.1(79Mb) 59.7(14B) & & \\\\ \\hline \\multirow{3}{*}{101} & \\multirow{3}{*}{**Ali et al.[143]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{VM-S-VIT-S} & \\multirow{3}{*}{Mamba Transformer} & \\multirow{3}{*}{Segmentation} & VIM-S-(mAP) (mIoU) & \\multirow{3}{*}{URL} \\\\  & & & & & & Raw-Attention 74.88, 45.09 & \\multirow{3}{*}{Attn-Rollout 578.51, 51} & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & Mamba -Attr 81.70, 542.0 & \\multirow{3}{*}{URL} \\\\  & & & & & & VTS-(mAP) (mIoU) & \\multirow{3}{*}{VTS-(mAP) (mIoU)} & \\multirow{3}{*}{} \\\\  & & & & & & Raw-Attention 7.25, 36.94 & \\multirow{3}{*}{Attn-Rollout 50.34, 47.85} & \\multirow{3}{*}{65.63} & \\multirow{3}{*}{\\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } } \\\\  & & & & & & Mamba-Attr 84.85, 65.63 & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{102} & \\multirow{3}{*}{**MambaMI [144]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba & Survival Prediction & Survival Prediction & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } \\\\  & & Multi-media & & & & & & \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{103} & \\multirow{3}{*}{**Motion Mamba [88]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba & Motion Synthesis & \\multirow{3}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } \\\\  & & Multi-media & & & & & & & \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{104} & \\multirow{3}{*}{**VL-Mamba [145]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{Mamba LLM-2.8B} & \\multirow{3}{*}{Mamba} & \\multirow{3}{*}{Multimodal Learning} & VQA-M-27.66, 632.6 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & SOA-M-M-G56.4 & TextVO-A48.9 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & & POPE-84.4 & Mam:1369.6 & \\multirow{3}{*}{6} \\\\  & & & & & & MMS 57.0 Mam-Net-32.6 & \\multirow{3}{*}{URL} \\\\ \\cline{1-1}  & & & & & & & \\\\ \\hline \\multirow{3}{*}{105} & \\multirow{3}{*}{**CMVM [146]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{50M} & Mamba & AD Classification & ACC@9.3 AUC-81.4 & - \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{107} & \\multirow{3}{*}{**DMamba [148]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba & RL & HalfCheetsh=m42.8\\(\\pm\\)0.08 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Summary of existing SSM-based Multi-modal and Multi-media algorithms.\n' +
      '\n' +
      'strategy that converts a point cloud to a 1-D sequence of points and ensures that adjacent points in the sequence are also adjacent in space. Consistent traverse serialization strategy produces six variants by arranging the order of x, y, and z coordinates, and the author introduces order prompt to inform Mampa of the arrangement rules of the sequence. In addition, a positional embedding method based on spatial coordinate mapping is proposed to add point cloud position information. Point Mamba [155] designs an octree-based ordering mechanism for irregular points,ensuring the preservation of their spatial proximity and causal dependence. The points undergo a sequence of Point Mamba blocks and downsampling layers to extract layered point characteristics. Zhou et al. propose 3DMambaIPF [156], which integrates Mamba into the point cloud filtering task and introduces a fast differentiable rendering loss. This approach has demonstrated strong performance in handling large-scale point clouds. Li et al. propose 3DMambaComplete [157], which incorporates the HyperPoint Generation module, HyperPoint Spread module, and deformation method for point cloud reconstruction. The HyperPoint Generation module introduces Mamba\'s selection mechanism to encode point cloud features. Zubi\'c et al. [158] introduce a state-space models (SSMs) with learnable time-scale parameters to the event-based vision, enabling adaptation to different frequency time inputs without having to retrain the network at different frequencies.\n' +
      '\n' +
      '### _Time Series Data_\n' +
      '\n' +
      'As the SSM is a sequence model, it is very intuitive and effective to adapt the SSMs to handle multivariate time series data [180, 184, 185]. Specifically, the primary challenges in the task of long-term time-series forecasting (LTSF) lie in the difficulty of capturing long-term dependency relationships and the poor linear scalability. TimeMachine [180] addresses these issues by introducing a method that leverages Mamba to capture long-term dependencies in multivariate time series data. By utilizing an integrated architecture with multiple Mamba modules, TimeMachine effectively resolves the challenges associated with channel mixing and channel independence. This approach enables selective prediction of global and local contextual information across different scales. Experimental validation demonstrates that TimeMachine significantly improves accuracy while maintaining excellent scalability.\n' +
      '\n' +
      '### _Others_\n' +
      '\n' +
      'In addition to the aforementioned domains, the SSM can also be adopted in many other applications. Real-world sensors are mostly nonlinear and subject to interference from external variables, which renders traditional local linear prediction algorithms ineffective in practical scenarios. Bhirangi [179] et al. established a benchmark for the task of continuous sequence prediction (CSP) and concurrently proposed the Hierarchical State Space Model (HiSS). This model constructs a temporal hierarchy by stacking multiple layers of structured spatial state models with varying resolutions. Experimental results demonstrate that HiSS outper\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \\hline \\hline\n' +
      '**\\#ID** & **Algorithm** & **Publish** & **Domain** & **Parameters** & **Architecture** & **Downstream Tasks** & **Accuracy** & **Efficiency** & **Code** \\\\ \\hline \\multirow{3}{*}{109} & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:12:M} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{Classification:ConObject:NN} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & OBJ-BC-88:3 & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:12:M} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & OBJ-D3D3:57.78 & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:17:4M} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & PB-P50-852:48 & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [155]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      'forms other sequence models by at least \\(23\\%\\) in terms of mean squared error (MSE) across multiple real-world sensor datasets. LOCOST [168] introduced an encoder-decoder architecture based on state-space models for conditional text generation tasks with long-context inputs. This approach effectively reduced computational complexity and memory usage, significantly enhancing the speed of both training and inference stages. MambaStock [178] utilizes the structured state space (S4) architecture to capture the nonlinearity in stock data, enabling accurate predictions of future stock prices. Lu et al. [163] proposed an improvement to the simplified structured state-space sequence model (S5), enabling the reset of hidden states within trajectories during the model training phase. Specifically, to enable the model to handle variable-length sequences, this paper modifies the association operator and introduces a reset annotation that preserves association properties in S5. Additionally, to test the generalization ability of the model, a challenging Meta-RL setup is also introduced. [188] introduces a new method for developing a realistic digital dynamic range compressor model for digital audio production by analyzing its simulation prototype. The learned representations are often affected by the high order of the model, which makes them unsuitable for control design. [189] proposes a system theory based model order reduction technique specifically for linear dynamic blocks in SSM to address this challenge.\n' +
      '\n' +
      'Wang et al. [191] demonstrate that the approximation of any continuous sequence-to-sequence relationship can be achieved by stacking state-space models with inter-layer nonlinear activation. Furthermore, experimental results indicate that this approach enhances the model\'s capacity to learn complex sequence patterns. Finally, through the\n' +
      '\n' +
      'Fig. 10: (a). Comparison between RNN/SSM for Event stream processing and their SSM-VIT block structure [158]; (b). Point Cloud Mamba [154]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \\hline \\hline\n' +
      '**\\#ID** & **Algorithm** & **Publish** & **Domain** & **Parameters** & **Architecture** & **Downstream Tasks** & **Accuracy** & **Efficiency** & **Code** \\\\ \\hline \\multirow{3}{*}{142} & \\multirow{3}{*}{S/D-Mamba [184]} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{TSF} & \\multirow{3}{*}{-} & \\multirow{3}{*}{Mamba} & & \\multirow{3}{*}{LTSF} & 0.066(Q-Mamba) & & \\\\  & & & & & & & 0.171(Q-Mamba) & & \\\\ \\hline \\multirow{3}{*}{143} & \\multirow{3}{*}{SiMBA [185]} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{others} & Small: & \\multirow{3}{*}{Mearach: 18.5M} & \\multirow{3}{*}{MSM-VIT: \\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{ImageNet: IK(acc/top)} & FLOPs: & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{143} & \\multirow{3}{*}{SiMBA [185]} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{others} & Small: & \\multirow{3}{*}{MSM-VIT: \\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{URL} \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{142} & Xu et al. [186] & arXiv242 & NIT & - & Mamba & Language modeling & - & - & - \\\\ \\hline \\multirow{3}{*}{145} & Sharma et al. [187] & arXiv24 & others & - & Mamba & Factual Recall & - & - & - \\\\ \\hline \\multirow{3}{*}{146} & Yin et al. [188] & arXiv24 & Audio & - & SSM & Audio Production & - & - & - \\\\ \\hline \\multirow{3}{*}{147} & Marco et al. [199] & arXiv24 & others & - & SSM & Prediction & fit index: 80.5 & - & - \\\\ \\cline{2-7}  & & & & & & & \\begin{tabular}{} \\end{tabular} & \\begin{tabular}{} \\end{tabular} & \\begin{tabular}{} \\end{tabular} & \\begin{tabular}{} \\end{tabular} \\\\ \\hline \\multirow{3}{*}{148} & Yang et al. [190] & arXiv24 & others & - & Mamba & \\multirow{3}{*}{prediction} & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{URL} \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IX: Summary of existing SSM-based other algorithms.\n' +
      '\n' +
      'retical analysis and numerical verification, it is concluded that state-space models do not fundamentally address the issue of exponential decaying memory. Samsami et al. [169] propose a method named R2I to enhance long-term memory and long-term credit, which integrates a set of state-space models into the world model of model-based reinforcement learning (MBRL) agents, thereby solving the issue that existing MBRL agents were unable to deal with the long-term intervals between actions and outcomes. Experimental results demonstrated that R2I achieved state-of-the-art performance in memory and credit assignment RL tasks, while also exhibiting faster convergence speed. Black-Mamba [167] merging SSMs with mixture-of-experts (MoE) significantly reduces inference costs, paving the way for efficient and scalable text generation tasks. MambaByte [166] is a token-free selective State Space Model designed for learning directly from raw bytes and its recurrent nature enables fast text generation, highlighting its practicality for large-scale models and paving the way for future developments in token-free language modeling. Self pretraining (SPT) [192] challenges the conventional approach of comparing long-sequence models from scratch, revealing that pre-training with data-driven priors significantly alters performance evaluations. By leveraging pre-training, the research achieves substantial improvements across various architectures, closing the performance gap between Transformers and SSMs and even surpassing previous SSM results on tasks. Laughing Hyena Distillery [161] proposes Laughing Hyena, a new distillation method that extracts compact state-space models from pre-trained long convolutional sequence models without loss of quality, utilizes rational function approximation and model downscaling techniques to extract low-dimensional state-space models in the convolutional layers, and achieves automated regression generation with constant memory and constant time complexity. Improved Hyena improves pre-training quality and reduces the number of filters to be distilled by binding filter weights to heads across channels.\n' +
      '\n' +
      'GateLoop [170] introduces GateLoop, a fully data-controlled linear RNN using data-controlled gated inputs and outputs for efficient auto-regressive language modeling. A memory horizon dataset for synthetic language modeling is also presented to highlight the superiority of GateLoop by comparing the advantages and disadvantages of data-controlled and non-data-controlled state transitions. A parallel scanning training strategy and an equivalent attention substitution model are also demonstrated. A multi-cohort study on the prediction of acute brain dysfunction states using selective State Space Models [176] develop a data-driven, automated data-driven approach for the prediction of ABD in critically ill patients in the ICU by utilizing rich electronic health record (EHR) data. Their study demonstrated high performance by dynamically predicting delirium, coma, and death during ICU stays and validated on two public datasets. S/D-Mamba [184] introduces two simple Mamba-based models: S-Mamba and D-Mamba, both of which use Mamba Block to extract variable correlations (VC). S-Mamba uses a Mamba Block to handle correlations between variables. D-Mamba is more sensitive to VC by adjusting the parameters compared to S-Mamba. D-Mamba adds an extra Mamba block to the Mamba layer to handle VCs compared to S-Mamba, and the extra Mamba block is more sensitive to correlations between variables by adjusting the parameters. Experimental results show that both models outperform the traditional methods in terms of performance while saving GPU memory and training time. Liu et al. propose a novel method, Mamba4Rec [175], which is proposed to model dependencies between sequences. To demonstrate the performance of Mamba4Rec, this paper experiments on MovieLen-1M, Amazon-Beauty, and Amazon-Video-Games, which reached state-of-the-art performance. Quan et al. [177], propose a novel model to handle multi-channel speech enhancement based on original SpatialNet. They propose oSpatialNet-Mamba which reaches top performance, whose core advantage is the State Space Model. Note that, various tasks have been tested on the model, which all performed well. Schiff et al. introduce a novel bioinformatics model called Caduceus [87], which can bi-directionally and equivalently model the DNA sequence. Based on MambaDNA, the Caduceus is the first family of RC-equivalent and bi-directional long-range DNA language models, which introduces pre-training and fine-tuning strategies. Also, it outperforms previous long-range models on the Genomic benchmarks. Zhang et al. introduce a novel motion-guided tracker and a motion-guided prediction head [88] based on Mamba. Karan et al. [159] proposes SaShiMi, a multi-level structure based on S4 model long sequence modeling. It provides a simple improvement to its parameterization by drawing connections to Hurwitz matrices. In addition, SaShiMi improves non-recursive generation performance in non-recursive states. A new prediction model that combines discrete state space hidden Markov models has been proposed by David et al. [173]. It introduces a variable separated posterior distribution and a two-stage training program to alternately train the parameters of the latent state and the emission distribution. By learning a set of emission laws and dynamically activating them based on hidden processes. FlashFFTConv [174] proposes a convolutional optimization method called FlashFFTConv, which uses matrix factorization to calculate the fast Fourier transform and utilizes matrix multiplication units for kernel fusion of long sequences, and reduce I/O costs effectively.\n' +
      '\n' +
      'Several related works have explored different aspects of state-space models (SSMs) and their applications in various domains. [181] propose a deep state-space model (DSSM) called Continuous Learning DSSM (CLDSSM). CLDSSM integrates regularization-based continual learning (CL) methods to efficiently update multiple dynamic systems without catastrophic forgetting. [172] focuses on enhancing the robustness of SSMs for long sequence tasks through approximate diagonalization. The authors propose a method that approximates diagonalization to improve the robustness of SSMs. By simplifying the problem and considering pure diagonal structures, the proposed approach achieves computational efficiency and allows channel communication. In [171], the _curse of memory_ in SSMs is addressed through stable re-parameterization. The authors introduce a parameterization technique for SSMs that effectively enhances their memory limits. [164] provide a data-dependent generalization bound for SSMs, highlighting the interplay between the SSM parameters and the temporal dependencies of training sequences. Building upon this generalization bound, they propose a scaling rule for model initialization to improve the robustness of SSMs in accommodating different temporal patterns in the sequence data. Wang et al. [160] propose a sequence routing method based on State Space Model and attempt to pre-train a big model without attention. Bidirectional Gated SSM (BiGS) proposed in the paper combines an SSM layer and a multiplicative gating architecture to effectively simplify the sequence modelling task. It reduces computational resources and time for model training by omitting the computationally intensive attention matrix. This approach successfully reduces resource consumption by about \\(30\\%\\) while maintaining comparable performance to traditional pre-trained models. Although the BiGS model does not consider pairwise interactions, it is able to match BERT\'s pre-training accuracy on the GLUE benchmark test and can be scaled up to 4096 tokens of long-form pre-training without approximation. This approach provides an effective pre-training solution for NLP tasks in resource-limited environments and has the potential to be applied to a wider range of NLP scenarios.\n' +
      '\n' +
      'Poli et al. [182] propose a new framework named MAD (Mechanistic Architecture Design),which aims at evaluating and predicting the design and scalability of hybrid architectures through synthetic tasks. The purpose of this research is to simplify the process by employing an end-to-end pipeline, including small-scale capability unit tests that can predict scaling laws, thereby identifying and testing new hybrid architectures. The study not only focuses on the design and scalability issues of hybrid architectures but also validates the effectiveness of its theories and methods through large-scale data analysis.\n' +
      '\n' +
      'Smith [162] is a novel spatiotemporal modeling approach focused on improving the efficiency and performance of modeling long sequence data. It combines the advantages of Convolutional Neural Networks (CNNs) and state space methods to effectively overcome the limitations of traditional models in dealing with complex spatial correlations and long time dependencies. ConvSSM significantly improves the speed of model training and prediction through parallel scanning and fast auto-regressive generation techniques. Meanwhile, it draws on state-space methods such as S4 and S5 to provide effective parameterization and initialization strategies for long-distance dependency modeling. This approach not only reduces the consumption of computational resources but also maintains a performance level comparable to that of complex models. Compared to other models, such as Transformer, whose computational cost grows significantly with sequence length, and ConvLSTM, which has a slower training speed, ConvSSM demonstrates its potential in long sequence spatiotemporal modeling. In addition, researchers are also exploring ways to further improve model performance by optimizing convolutional kernel design and combining the advantages of different models. [165] proposes an approach that fuses a hybrid Mixture of Expert (MoE) mechanism and a selective State Space Model (SSM) with the aim of optimising the efficiency and performance of sequence modeling. This approach introduces MoE on top of the Mampa model, achieving similar performance in fewer training steps and maintaining the inference performance advantage over the Transformer model. The MoE model utilizes dynamic gating, expert caching, and load balancing techniques to address the communication and memory issues faced by large models during the inference phase, while conditionally computing to extend the model without significantly increasing the computational cost while expanding the model capacity. It is shown that MoE models with instruction fine-tuning and task-specific fine-tuning are able to achieve better performance than denser models with the same computational complexity. The success of MoE-Mampa demonstrates that the performance and efficiency of sequence modeling tasks can be effectively improved by a well-designed MoE architecture, which opens up new possibilities for processing large-scale sequence data. Xu et al. [186] demonstrate the potential of Mampa models for classical information retrieval tasks by evaluating the performance of models based on the Mampa architecture in a document ranking task. Compared to Transformer-based language models, Mampa models achieve competitive performance in the same training configuration. However, the Mampa model falls short in terms of training throughput compared to efficient attention implementations, limiting its potential for efficient training and deployment. Although the study focuses on models with parameters less than 1B, they find that this may change when scaling to larger models and employing different training configurations. Thus, the effectiveness and performance of Mampa models in other classical information retrieval tasks remain to be further investigated. Amo et al. [193] introduce the SSM system and summarize the research progress in the field of control-theoretic. Sharma et al. [187] are inspired by the research found in the autoregressive transformer language model that knowledge recall can be specific to specific modules and marker positions, and aims to locate factual recall in Mampa. Olucha et al. proposed an overview of the state-of-the-art and comparative study of linear parameter-varying state-space (LPV-SS) model downscaling [194]. These comparisons can help to select the best downscaling method for a given LPV-SS model. Yang et al. propose the Rec-Mampa [190] model for sequence recommendation tasks. This model better models users\' preference information that changes over time through the Mampa module, improving personalized recommendation performance and reducing computational complexity. Compared to SASRec, the training time is reduced by approximately \\(70\\%\\). LaRocque et al. [183] compares the performance of Convolutional Neural Networks (CNNs) and the novel State Space Model (SSM)-based Mampa architecture, revealing intriguing insights into their respective strengths and the benefits of dataset fusion for improved terrain.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we give an experimental comparison of five downstream tasks, including single-/multi-label classification, visual object tracking, pixel-level segmentation, image-to-text generation, and person/vehicle re-identification. More details will be introduced in the following subsections, respectively.\n' +
      '\n' +
      '### _Single-/Multi-label Classification_\n' +
      '\n' +
      'For the single-label classification problem, we calculate the accuracy of existing works on the widely used ImageNet-1K [2] dataset. As shown in Fig. 12 (d), we can find that the base version of VMamba [60] and Mamba-2D [68] achieves better results on the ImageNet1K dataset, i.e., 83.2% and 83% on the top-1 accuracy, respectively. It is also easy to find that current Mamba-based vision models are all tiny, small, or base versions, and seldom pre-train a large or huge version of the Mamaba network. The overall performance is comparable to some Transformer based models, but still inferior to the state-of-the-art on the ImageNet classification dataset.\n' +
      '\n' +
      'For the multi-label classification, we select the Pedestrian Attribute Recognition (PAR) task [6]3 and conduct experiments on the PA100K [208] and PETA [209] datasets. The PA100K dataset contains 100,000 samples collected from 598 scenarios and involves 26 pedestrian attributes. We split the training, validation, and testing subset based on default settings (8:1:1). The PETA dataset involves 61 binary at\n' +
      '\n' +
      'Fig. 11: Representative input data that can be processed using State Space Model.\n' +
      '\n' +
      'Fig. 12: Experimental results on (a, b, c) Video-based recognition, (d) Image-based recognition, (e, f, g) Medical image-based segmentation.\n' +
      '\n' +
      'tributes and 19,000 person images. The training, validation, and testing subset contains 9500, 1900, and 7600 images, respectively. By following its default settings, 35 pedestrian attributes are selected for the experiment.\n' +
      '\n' +
      'The ViT-S [19] and Mamba-based network Vim-S [61] are adopted as the backbone for this experiment. We follow the vision-language fusion based PAR framework VTB [207] which takes the pedestrian image and attribute set as the input and predicts the logistic scores of each attribute. From the experimental results reported in Table X, we can find that the Vim-S based PAR model achieves 81.08/73.75/80.91/84.96/82.52 on the PETA dataset, and 80.41/78.03/85.39/88.37/86.39 on the PA100K dataset. These results are significantly better than the ViT-S based model, but still significantly inferior to the compared PAR algorithms developed based on the Transformer network. For example, the ViT-B based VTB achieves 85.31/79.60/86.76/87.17/86.71, 83.72/80.89/87.88/89.30/88.21 on the PETA and PA100K datasets.\n' +
      '\n' +
      '### _Visual Object Tracking_\n' +
      '\n' +
      'In this subsection, we compare the Mamba with Transformer, and CNN based backbone for the tracking task4 based on OSTrack [210]. Specifically, the CNN based trackers are TrDiMP [211], ToMP50 [212], DiMP50 [213], PrDiMP [214], KYS [215], and ATOM [216]; the Transformer based trackers are HDETrack [217], AiATrack [218], STARK [219], TransT [220], MiFormer [212], and SimTrack [222]. To achieve a fair comparison, we train and test these trackers on a large-scale event-based tracking dataset, EventVOT [217], which contains 841, 18, and 282 videos, respectively. The detailed experimental results are reported in Table XI and Fig. 13. Note that, three widely used evaluation metrics are used for the comparison, including Success Rate (SR), Precision Rate (PR), and Normalized Precision Rate (NPR). According to Table XI, we can find that the performance is slightly decreased when replacing the ViT using the Mamba backbone network, meanwhile, it brings about a huge reduction in the number of parameters (4.1M only). Therefore, we can draw the conclusion that the Mamba network will be a promising choice for the event-based tracking.\n' +
      '\n' +
      'Footnote 4: [https://github.com/wangxiao5791509/Single_Object_Tracking_Paper_List](https://github.com/wangxiao5791509/Single_Object_Tracking_Paper_List)\n' +
      '\n' +
      '### _Pixel-level Segmentation_\n' +
      '\n' +
      'Recently, the Mamba network has been widely exploited in medical image segmentation, as illustrated in Fig. 12 (e, f, g). For example, the Swin-Transformer based model Swin-UNet [223] attains 89.33/99.57/88.46 (Dice, IoU, Accuracy) on the MRI Cardiac dataset. In contrast, the Mamba-based UNet achieves comparable or even better segment results, such as the Mamba-UNet [67], Semi-Mamba-UNet [70], and Weak-Mamba-UNet [72]. These results fully demonstrate the effectiveness of Mamba architecture for medical image segmentation.\n' +
      '\n' +
      '### _Image-to-Text Generation_\n' +
      '\n' +
      'For the image-to-text generation, we select the X-ray report generation task which takes the X-ray medical\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c c c} \\hline \\hline \\multicolumn{1}{c|}{**Methods**} & \\multicolumn{1}{c|}{**Backbone**} & \\multicolumn{1}{c}{**PETA**} & \\multicolumn{1}{c}{**PA100K**} & \\\\ \\cline{3-11} \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{mA} & \\multicolumn{1}{c}{Acc} & \\multicolumn{1}{c}{Prec} & Recall & F1 & mA & Acc & Prec & Recall & F1 \\\\ \\hline\n' +
      '**JLAC** (AAAI 2020) [195] & ResNet50 & 86.96 & 80.38 & 87.81 & 87.09 & 87.50 & 82.31 & 79.47 & 87.45 & 87.77 & 87.61 \\\\\n' +
      '**SCCR** (TCSVT 2020) [196] & ResNet50 & 87.2 & - & 89.20 & 87.5 & 88.3 & 80.6 & - & 88.7 & 84.9 & 82.1 \\\\\n' +
      '**SSCsoft** (ICCV 2021) [197] & ResNet50 & 86.52 & 78.95 & 86.02 & 87.12 & 86.99 & 81.87 & 78.89 & 85.98 & 89.10 & 86.87 \\\\\n' +
      '**IAA-Caps** (PR 2022) [198] & OSNet & 85.27 & 78.04 & 86.08 & 85.80 & 85.64 & 81.94 & 80.31 & 83.86 & 88.01 & 87.80 \\\\\n' +
      '**MCFI** (NCA 2022) [199] & ResNet-50 & 86.83 & 78.89 & 84.57 & 88.84 & 86.65 & 81.53 & 77.80 & 85.11 & 88.20 & 86.62 \\\\\n' +
      '**DRFormer** (NC 2022) [200] & ViT-B/16 & 89.96 & 81.30 & 85.68 & 91.08 & 88.30 & 82.47 & 80.27 & 87.60 & 88.49 & 88.04 \\\\\n' +
      '**VAC-Combine** (IJCV 2022) [201] & ResNet50 & - & - & - & - & - & 82.19 & 80.66 & 88.72 & 88.10 & 88.41 \\\\\n' +
      '**DART** (AAAI 2022) [202] & ResNet50 & 87.07 & 78.88 & 87.87 & 87.03 & 86.40 & 83.54 & 80.13 & 87.01 & 89.19 & 88.09 \\\\\n' +
      '**CGCCN** (TMM 2023) [203] & ResNet ResNet & 87.08 & 79.30 & 83.97 & 89.38 & 86.59 & - & - & - & - \\\\\n' +
      '**CAS-SAL-FR** (IJCV 2022) [204] & ResNet50 & 86.40 & 79.93 & 87.03 & 87.18 & 82.86 & 79.64 & 86.81 & 87.79 & 85.18 \\\\\n' +
      '**PromptPAR** (arXiv24) [205] & ViT-L/14 & 88.76 & 82.84 & 89.04 & 89.74 & 89.18 & 87.47 & 83.78 & 89.27 & 91.70 & 90.15 \\\\\n' +
      '**SequencePAR** (arXiv24) [206] & ViT-L/14 & - & 84.92 & 90.44 & 90.73 & 90.46 & - & 83.94 & 90.38 & 90.23 & 90.10 \\\\ \\hline\n' +
      '**VTB** (TCSVT 2022) [207] & ViT-B/16 & 85.31 & 79.60 & 86.76 & 87.17 & 86.71 & 83.72 & 80.89 & 87.88 & 89.30 & 88.21 \\\\\n' +
      '**VTB*** (TCSVT 2022) [207]** & ViT-L/14 & 86.34 & 79.59 & 86.66 & 87.82 & 86.97 & 85.30 & 81.76 & 87.87 & 90.67 & 88.86 \\\\\n' +
      '**VTB** (TCSVT 2022) [207] & ViT-S & 82.51 & 77.23 & 85.75 & 84.95 & 85.01 & 78.76 & 77.61 & 87.41 & 85.35 & 85.94 \\\\\n' +
      '**Vim-PAR** & ViT-S & 81.08 & 73.75 & 80.91 & 84.96 & 82.52 & 80.41 & 78.03 & 85.39 & 88.37 & 86.39 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE X: Comparison between different trackers on the EventVOT dataset.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c|c c} \\hline \\hline\n' +
      '**Trackers** & **Source** & **Backbone** & **SR** & **PR** & **NPR** & **Params(M)** & **FPS** \\\\ \\hline\n' +
      '**THOMP** & CVPR21 & & 39.9 & 34.8 & 48.7 & 26.3 & 26 \\\\\n' +
      '**ToMP50** & CVPR22 & & 37.6 & 32.8 & 47.4 & 26.1 & 25 \\\\\n' +
      '**PIM50** & ICCV19 & & 52.6 & 51.1 & 67.2 & 26.1 & 43 \\\\\n' +
      '**PIM50** & CVPR20 & & 55.5 & 57.2 & 70.4 & 26.1 & 30 \\\\\n' +
      '**KYS** & ECCV20 & & 38.7 & 37.3 & 49.8 & - & 20 \\\\\n' +
      '**AOTV** & **CVPR19 & & 44.4 & 40.7 & 55.5 & 84 & 30 \\\\ \\hline\n' +
      '**HDETrack** & CVPR24 & & 57.8 & 62.2 & 73.5 & 92.1 & 105 \\\\\n' +
      '**AiATrack** & ECCV22 & & 57.4 & 59.7 & 22.8 & 15.8 & 38 \\\\\n' +
      '**STARK** & ECCV21 & & ViT & 44.5 & 39.6 & 58.7 & 28.1 & 42 \\\\\n' +
      '**TransT** & CVPR21 & & 54.3 & 56.5 & 68.8 & 18.5 & 50 \\\\\n' +
      '**Misformer** & CVPR22 & & 49.9 & 49.6 & 63.0 & 35.6 & 25 \\\\\n' +
      '**SimTrack** & ECCV22 & & 54.7 & 57.5 & 69.9 & 57.8 & 40 \\\\ \\hline\n' +
      '**OSTrack**image as the input and generates the medical report5. For the experiment, we select the R2GenGPT6 as our baseline and evaluated its performance on the IU-Xray dataset [224]. R2GenGPT consists of a visual encoder (Swin Transformer [20]), a linear layer, and a large language model (llama-2-7B-chat [225]). The training approach involves freezing the language model initially and subsequently fine-tuning the visual encoder and the linear layer. We replaced the Swin Transformer with Vim model [61] and compared the results with other methods in Table XII. As both models utilize pre-trained components, Vision Mamba demonstrates superior performance over the Swin Transformer model in terms of BLEU-4 and ROUGE-L scores.\n' +
      '\n' +
      'Footnote 5: [https://github.com/Event-AHU/Medical_Image_Analysis](https://github.com/Event-AHU/Medical_Image_Analysis)\n' +
      '\n' +
      'Footnote 6: [https://github.com/wang-zhangyu/R2GenGPT](https://github.com/wang-zhangyu/R2GenGPT)\n' +
      '\n' +
      '### _Person/Vehicle Re-Identification_\n' +
      '\n' +
      'As shown in Table XIII, we conduct experiments on two re-identification (re-ID) tasks, i.e., the person re-identification [257] and vehicle re-identification [256]. For the person re-ID, four widely used datasets are used, including MSMT17 [258], Market1501 [259], DukeMTMC [260], and Occluded-Duke [261] dataset. These datasets are captured from different scenes, and the samples are collected from surveillance systems with overlapping coverage of cameras, which has challenges such as cross-time span, occlusion, and background interference. For the vehicle re-ID, VeRi-776 [262] and VehicleID [263] datasets are utilized for the experimental validation. Different from pedestrian samples, the change of observation viewpoints also brings significant appearance differences for vehicles, for thus the vehicle datasets are additionally provided with viewpoint labels to mark the different viewpoints of the vehicle samples. For the above datasets, we use the Cumulative Matching Characteristic (CMC) curve and mean Average Precision (mAP) as evaluation metrics.\n' +
      '\n' +
      'Referring to mainstream frameworks such as TransReID [255] and Strong Baseline [264], we retained ID Loss, Triplet Loss, and BN Layer, and replaced the CNN and Transformer backbones using Vim [61] and VAMba [60] to explore the potential of Mamba for re-identification tasks, and the compared results are shown in Table XIII. The selective scanning mechanism (SSM) proposed by the Mamba model allows for sequence modeling with low complexity, and Vim and VAMba further build on it by proposing an SSM modeling approach for 2D image data. Compared to CNN-based models that require complex module design, the simple Mamba network already has effectiveness. Even compared with the models with high complexity such as DeiT [265] and ViT [19], the bidirectional scanning mechanism proposed by Vim has fewer training parameters, and it shows effectiveness on the VehicleID dataset. In contrast, VAMba\'s cross-scanning mechanism, which does not rely on Transformer\'s structure (_e.g._, position embedding and class token), has achieved comparable results on the Market1501, DukeMTMC, and VeRi-776 datasets. For this reason, we expect more Mamba-based studies applicable to the re-identification task in the future.\n' +
      '\n' +
      '## 5 Challenge and Opportunity\n' +
      '\n' +
      'State Space Model has been widely studied and applied in many applications, however, the research in this direction is still in its early stages. To help the readers quickly grasp the frontiers, this paper puts forward several research points worthy of attention.\n' +
      '\n' +
      '\\(\\bullet\\)**Current SSMs model still performs inferior to the mainstream of Transformer networks.** From the experimental results reported in Section 4, we can find that there is\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c} \\hline \\hline\n' +
      '**Methods** & **Backbone** & **CIDEr** & **BLEU-4** & **ROUGE-L** \\\\ \\hline R2Gen [226] & CNN & 0.398 & 0.165 & 0.371 \\\\ KERP [227] & CNN & 0.280 & 0.162 & 0.339 \\\\ HRGP [228] & CNN & 0.343 & 0.151 & 0.322 \\\\ MKG [229] & CNN & 0.304 & 0.147 & 0.367 \\\\ PPKPC [230] & CNN & 0.351 & 0.168 & 0.376 \\\\ MGSK [231] & CNN & 0.382 & 0.178 & 0.381 \\\\ CA [232] & ResNet-50 & - & 0.169 & 0.381 \\\\ CMCL [233] & CNN & - & 0.162 & 0.378 \\\\ DCL [234] & CNN & 0.586 & 0.163 & 0.383 \\\\ \\hline R2GenGPT & Swin-B & 0.524 & 0.152 & 0.352 \\\\ R2GenGPT & Vim-S & 0.388 & 0.152 & 0.355 \\\\ R2GenGPT & Vim-S* & 0.382 & 0.171 & 0.371 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XII: Comparison between the performance of R2Gen-GPT-Vim-Small and with other methods on IU-Xray dataset. R2Gen-GPT-Vim-S* and R2GenGPT-Vim-S denote the Vim-S are initialized with and without pre-trained parameters, respectively.\n' +
      '\n' +
      'Fig. 13: Visualization of the tracking results on EventVOT dataset.\n' +
      '\n' +
      'still room for performance improvement based on SSM. The SSMs pre-trained on the large-scale dataset, such as ImageNet [2], play a critical role in many downstream tasks, however, the base, large, and huge versions of SSMs are rarely released. We believe this may be an obstacle to the high performance on the CV tasks.\n' +
      '\n' +
      '\\(\\bullet\\)**The advantages of the SSMs in GPU usage are worth further exploration and research.** According to our experiments, the memory consumption is lower or comparable to the Transformer networks on some downstream tasks. A significant improvement in this aspect can be observed, but some tasks are not. The study on mining the lower GPU memory consumption is worth further exploration and research.\n' +
      '\n' +
      '\\(\\bullet\\)**To further explore its advantages in high-resolution or long-term vision data is a direction worthy of attention and research.** Since the SSMs architecture significantly reduces the complexity of the model theoretically, its modeling capability on high-resolution data (remote sensing data, X-ray medical images) or long-term sequence data (long-term video frames) is of great value. However, these aspects are not addressed well using other strong models like the Transformer network.\n' +
      '\n' +
      '\\(\\bullet\\)**Pre-trained big models using SSMs architecture.** In the pre-trained big model era, the scaling of deep neural networks is an important step for general artificial intelligence. Current big models are built based on CNN or Transformer networks, and seldom of them adopt the SSMs architecture. Recently, Jamba [136] released by AI21Labs is a novel large language model built by fusing the Transformer, Mamba, and MoE (Mixture-of-Experts). It supports the input of context length up to 256K tokens and also achieves comparable performance with Mixtral-8x7B [266] and Llama-2 7OB [225]. The study on building pure Mamba or hybrid architectures will be a promising direction for pre-trained big models.\n' +
      '\n' +
      '\\(\\bullet\\)**Multi-modal learning using SSMs architecture.** Early multi-modal related works focused on how to learn modality-specific and modality-shared representations. Influenced by the Transformer network, current multi-modal algorithms usually directly encode and fuse the multiple cues in a unified Transformer network [267, 268]. Thus, the cost of the inference phase may be twice compared with a single modality only. How to design new SSMs-based backbones for cost-sensitive multi-modal learning is an important research topic.\n' +
      '\n' +
      '\\(\\bullet\\)**Developing novel scan operators for the SSMs.** The scan is a key operator for the SSMs architecture and the 1D and 2D data are usually processed with different scan mechanisms. For example, VMamba [60] scans an image using _CSM_ (scan expand) and merges the four output features as the final 2D feature map. To deal with more special remote sensing data, some researchers have proposed additional scanning mechanisms to capture skewed feature representations to obtain more comprehensive features [139]. A comparison of different scan schemes can be found in Fig. 8. Therefore, it is natural to design novel scan schemes to enhance the feature learning of SSMs. For example, it is possible to develop new track-changing scan methods to better encode the point cloud and event streams.\n' +
      '\n' +
      '\\(\\bullet\\)**The generalization performance of SSMs still deserves attention and further research and improvement.** Compared with the limited receptive field and greater complexity of CNN and the Transformer, SSMs have linear complexity and global receptive fields, which may have greater advantages and potential in the field of domain generalization. However, current SSM based networks illustrate limited domain generalization ability, as noted in DGMamba [120]. Long et al. [120] attempt to address this issue from the perspective of hidden states and inappropriate scan mechanisms by proposing the Hidden State Suppressing (HSS) and Semantic-aware Patch Refining (SPR) strategies. We believe more insights and improvements can be conducted to further improve the overall performance of domain generalization.\n' +
      '\n' +
      '\\(\\bullet\\)**Use the latest SSM model to empower the existing deep neural network model.** In the early stage of the third wave of deep learning, many clever neural network modules or designs are proposed, for example, knowledge distillation, pyramid structure, network in network [269], diffusion model, GAN, etc. Enhancing the SSM based on these successful modules or introducing SSM into these modules may bring us better performance.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c|c c|c c||c c|c c} \\hline \\hline  & & \\multicolumn{2}{c}{**MSMT17**} & \\multicolumn{2}{c}{**Market1501**} & \\multicolumn{2}{c}{**DukeMTMC**} & \\multicolumn{2}{c}{**Occluded-Duke**} & \\multicolumn{2}{c}{**Veti-776**} & \\multicolumn{2}{c}{**VehicleID**} \\\\\n' +
      '**Backbone** & **Method** & mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 & **Method** & mAP & R1 & R1 & R5 \\\\ \\hline \\hline  & CBN [235] & 42.9 & 72.8 & 77.3 & 91.3 & 67.3 & 82.5 & - & - & PFReID [236] & 72.5 & 93.3 & 72.6 & 88.6 \\\\  & OSNet [237] & 52.9 & 78.7 & 84.9 & 94.8 & 73.5 & 88.6 & - & - & SAN [238] & 72.5 & 93.3 & 79.7 & 94.3 \\\\  & MCN [239] & 52.1 & 76.9 & 86.9 & 95.7 & 78.4 & 88.7 & - & - & UMTS [240] & 75.9 & 95.8 & 80.9 & 87.0 \\\\  & RAG-SC [241] & 57.5 & 80.3 & 88.4 & 96.1 & - & - & - & - & VANet [242] & 66.3 & 89.8 & 83.3 & 96.0 \\\\  & SAN [243] & 55.7 & 79.2 & 88.0 & 96.1 & 75.7 & 87.9 & - & - & SPAN [244] & 68.9 & 94.0 & - & - \\\\  & SCSN [245] & 58.5 & 83.8 & 88.5 & 95.7 & 79.0 & 91.0 & - & - & PGAN [246] & 79.3 & 96.5 & 78.0 & 93.2 \\\\  & ABDNet [247] & 60.8 & 82.3 & 88.3 & 95.6 & 78.6 & 89.0 & - & - & PVEN [248] & 79.5 & 95.6 & 84.7 & 97.0 \\\\  & PGFA [249] & - & - & 76.8 & 91.2 & 65.5 & 82.6 & 37.3 & 51.4 & SAVER [250] & 79.6 & 96.4 & 79.9 & 95.2 \\\\  & HOReID [251] & - & - & 84.9 & 94.2 & 75.6 & 86.9 & 43.8 & 55.1 & CFVMNet [252] & 77.1 & 95.3 & 81.4 & 94.1 \\\\  & ISP [253] & - & - & 88.6 & 95.3 & 80.0 & 89.6 & 52.3 & 62.8 & GLAMOR [254] & 80.3 & 96.5 & 78.6 & 93.6 \\\\ \\hline  & DeiT-B/16 [255] & 61.4 & 81.9 & 86.6 & 94.4 & 78.9 & 89.3 & 53.1 & 60.6 & DeiT-B/16 [255] & 78.4 & 95.9 & 83.1 & 96.8 \\\\\n' +
      '**Transformer** & ViT-B/16 [255] & 61.0 & 81.8 & 86.8 & 94.7 & 79.3 & 88.8 & 53.1 & 60.5 & ViT-B/16 [255] & 78.2 & 96.5 & 82.3 & 96.1 \\\\  & VehicleMTA [256] & - & - & - & - & - & - & - & VehicleMTAE [256] & 85.6 & 97.9 & - & - \\\\ \\hline  & VimT-1/6 & 40.1 & 62.6 & 75.7 & 89.4 & 66.5 & 81.8 & 35.4 & 45.1 & Vim-T/16 & 62.9 & 89.2 & 67.0 & 88.2 \\\\\n' +
      '**Mamba** & ViT-S/16 & 42.2 & 66.2 & 77.5 & 89.7 & 67.4 & 83.0 & 40.8 & 51.3 & Vim-S/16 & 61.6 & 89.6 & 78.2 & 94.8 \\\\  & VMamba-T/16 & 51.0 & 75.6 & 83.3 & 92.8 & 74.9 & 87.3 & 49.4 & 58.3 & VMamba-T/16 & 77.3 & 95.9 & 78.5 & 93.5 \\\\  & VMamba-B/16 & 51.1 & 75.3 & 84.3 & 93.2 & 77.4 & 88.0 & 48.1 & 57.4 & VMamba-B/16 & 77.5 & 95.6 & 82.5 & 96.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Comparison with methods based on CNN and Transformer on Person Re-identification and Vehicle Re-identification datasets.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:27]\n' +
      '\n' +
      'langer, L. J. Colwell, and A. Weller, "Rethinking attention with performers," _arXiv preprint arXiv:2009.14794_, 2020.\n' +
      '* [5] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim, "Gated linear attention transformers with hardware-efficient training," _arXiv preprint arXiv:2312.06635_, 2023.\n' +
      '* [6] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [7] E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, and C. Re, "Stnd: Modeling images and videos as multidimensional signals with state spaces," in _Proceedings of Advances in Neural Information Processing Systems_, 2022, pp. 2846-2861.\n' +
      '* [8] J. N. Yan, J. Gu, and A. M. Rush, "Diffusion models without _arXiv preprint arXiv:2311.18527_, 2023.\n' +
      '* [9] V. T. Hu, S. A. Baumann, M.-S. Gui, O. Grebenkova, P. Ma, J. S. Fischer, and B. Ommer, "Zigma: A di-style zigzag mamba diffusion model," _arXiv preprint arXiv:2403.13802_, 2024.\n' +
      '* [10] Z. Fei, M. Fan, C. Yu, and J. Huang, "Scalable diffusion models with state space backbone," _arXiv preprint arXiv:2402.05608_, 2024.\n' +
      '* [11] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Re, "Combining recurrent, convolutional, and continuous-time models with linear state space layers," in _Proceedings of Advances in Neural Information Processing Systems_, 2021, pp. 572-585.\n' +
      '* [12] R. E. Kalman, "A new approach to linear filtering and prediction problems," _Journal of Basic Engineering_, vol. 82, no. 1, pp. 35-45, 1960.\n' +
      '* [13] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re, "Hippo: Recurrent memory with optimal polynomial projections," _arXiv preprint arXiv:2008.07669_, 2020.\n' +
      '* [14] A. Gu, K. Goel, A. Gupta, and C. Re, "On the parameterization and initialization of diagonal state space models," in _Proceedings of Advances in Neural Information Processing Systems_, 2022, pp. 35971-35983.\n' +
      '* [15] A. Gupta, A. Gu, and J. Berant, "Diagonal state spaces are as effective as structured state spaces," _Proceedings of Advances in Neural Information Processing Systems_, pp. 22 982-22 994, 2022.\n' +
      '* [16] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De, "Resurrecting recurrent neural networks for long sequences," in _Proceedings of the International Conference on Machine Learning_, 2023, pp. 26 670-26 698.\n' +
      '* [17] A. Gu, I. Johnson, A. Thalsina, A. Rudra, and C. Re, "How to train your hipro: State space models with generalized orthogonal basis projections," _arXiv preprint arXiv:2206.120370_, 2022.\n' +
      '* [18] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, "Long range language modeling via gated state spaces," _arXiv preprint arXiv:2206.13947_, 2022.\n' +
      '* [19] Y. Du, X. Liu, and Y. Chua, "Spiking structured state space model for monaural speech enhancement," _arXiv preprint arXiv:2309.03641_, 2023.\n' +
      '* [20] X. Jiang, C. Han, and N. Mesgarani, "Dual-path mamba: Short and long-term bidirectional selective structured state space models for speech separation," _arXiv preprint arXiv:2403.12857_, 2024.\n' +
      '* [21] K. Li and G. Chen, "Spmamba: State-space model is all you need in speech separation," _arXiv preprint arXiv:2403.02063_, 2024.\n' +
      '* [22] R. Garazzi, J. Siems, S. Schrodi, T. Brox, and F. Hutter, "Is mamba capable of in-context learning?" _arXiv preprint arXiv:2402.03170_, 2024.\n' +
      '* [23] B. Qi, J. Gao, D. Li, K. Zhang, J. Liu, L. Wu, and B. Zhou, "St++: Elevating long sequence modeling with state memory reply," 2024. [Online]. Available: [https://openreview.net/forum:id=bdmw4qjdf19](https://openreview.net/forum:id=bdmw4qjdf19)\n' +
      '* [24] S. Zuo, X. Liu, J. Jiao, D. X. Charles, E. Manavoglu, T. Zhao, and J. Gao, "Efficient long sequence modeling via state space augmented transformer," _arXiv preprint arXiv:2212.08136_, 2022.\n' +
      '* [25] W. He, K. Han, Y. Tang, C. Wang, Y. Yang, T. Guo, and Y. Wang, "Densenmamba: State space models with dense hidden connection for efficient large language models," _arXiv preprint arXiv:2403.00818_, 2024.\n' +
      '* [26] Z. Yang, A. Mitra, S. Kwon, and H. Yu, "ClinicalLambda: A generative clinical language model on longitudinal clinical notes," _arXiv preprint arXiv:2403.05795_, 2024.\n' +
      '* [27] A. R. de Sousa Porfitiro Correia and L. A. Alexandre, "Music dance as language translation using sequence models," _arXiv preprint arXiv:2403.15569_, 2024.\n' +
      '* [28] C. Wang, O. Tsepa, J. Ma, and B. Wang, "Graph-mamba: Towards long-range graph sequence modeling with selective state spaces," _arXiv preprint arXiv:2402.00789_, 2024.\n' +
      '* [29] S. Tang, J. A. Dunnmon, Q. Liangqiong, K. K. Saab, T. Baykaner, C. Lee-Messer, and D. L. Rubin, "Modeling multivariate biosignals with graph neural networks and structured state space models," in _Proceedings of the International Conference on Learning Representations Workshops_, 2023.\n' +
      '* [30] A. Behrova and F. Hashemi, "Graph mamba: Towards learning on graphs with state space models," _arXiv preprint arXiv:2402.08678_, 2024.\n' +
      '* [31] G. Bachmann and V. Nagarajan, "The pitfalls of next-token prediction," _arXiv preprint arXiv:2403.06963_, 2024.\n' +
      '* [32] L. Li, H. Wang, W. Zhang, and A. Coster, "Stg-mamba: Spatial-temporal graph learning via selective state space model," _arXiv preprint arXiv:2403.12418_, 2024.\n' +
      '* [33] M. M. Islam and G. Bertasius, "Long movie clip classification with state-space video models," in _Proceedings of European Conference on Computer Vision_, 2022, pp. 87-104.\n' +
      '* [34] J. T. Smith, A. Warrington, and S. Linderman, "Simplified state space layers for sequence modeling," in _Proceedings of the International Conference on Learning Representations_, 2022.\n' +
      '* [35] J. Wang, W. Zhu, P. Wang, X. Yu, L. Liu, M. Omar, and R. Hamid, "Selective structured state-spaces for long-form video understanding," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 6387-6397.\n' +
      '* [36] D. Hafner, J. Fasukonis, J. Ba, and T. Lillicrap, "Mastering diverse domains through world models," _arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* [37] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu, "Vmamba: Visual state space model," _arXiv preprint arXiv:2401.10166_, 2024.\n' +
      '* [38] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, "Vision mamba: Efficient visual representation learning with bidirectional state space model," _arXiv preprint arXiv:2401.09417_, 2024.\n' +
      '* [39] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu, "Segmamba: Long-range sequential modeling mambo for 3d medical image segmentation," _arXiv preprint arXiv:2401.13560_, 2024.\n' +
      '* [40] J. Ma, F. Li, and B. Wang, "U-mamba: Enhancing long-range dependency for biomedical image segmentation," _arXiv preprint arXiv:2401.04722_, 2024.\n' +
      '* [41] J. Liu, H. Yang, H.-Y. Zhou, Y. Xi, L. Yu, Y. Yu, Y. Liang, G. Shi, S. Zhang, H. Zheng _et al._, "Swin-ummaba: Mamba-based unet with imagenet-based pretraining," _arXiv preprint arXiv:2402.03302_, 2024.\n' +
      '* [42] J. Ruan and S. Xiang, "Vm-unet: Vision mamba unet for medical image segmentation," _arXiv preprint arXiv:2402.02491_, 2024.\n' +
      '* [43] H. Gong, L. Kang, Y. Wang, X. Wan, and H. Li, "nmmab: 3d biomedical image segmentation, classification and landmark detection with state space model," _arXiv preprint arXiv:2402.03526_, 2024.\n' +
      '* [44] Z. Wang, J.-Q. Zheng, Y. Zhang, G. Cui, and L. Li, "Mamba-unet: Unet-like pure visual mamba for medical image segmentation," _arXiv preprint arXiv:2402.05079_, 2024.\n' +
      '* [45] S. Li, H. Singh, and A. Grover, "Mamba-nd: Selective state space modeling for multi-dimensional data," _arXiv preprint arXiv:2402.05892_, 2024.\n' +
      '* [46] Z. Zheng and J. Zhang, "Fd-vision mamba for endoscopic exposure correction," _arXiv preprint arXiv:2402.06378_, 2024.\n' +
      '* [47] Z. Wang and C. Ma, "Semi-mamba-unet: Pixel-level contrastive cross-supervised visual mamba-based unet for semi-supervised medical image segmentation," _arXiv preprint arXiv:2402.07245_, 2024.\n' +
      '* [48] Z. Ye and T. Chen, "P-mamba: Marrying perona milk diffusion with mamba for efficient pediatric echocardiographic left ventricular segmentation," _arXiv preprint arXiv:2402.08506_, 2024.\n' +
      '* [49] Z. Wang and C. Ma, "Weak-mamba-unet: Visual mamba makes cnn and vitt work better for scribble-based medical image segmentation," _arXiv preprint arXiv:2402.10887_, 2024.\n' +
      '* [50] X. He, K. Cao, K. Yan, R. Li, C. Xie, J. Zhang, and M. Zhou, "Pamamba: Effective pan-sharpening with state space model," _arXiv preprint arXiv:2402.12192_, 2024.\n' +
      '* [51] N. Agarwal, D. Suo, X. Chen, and E. Hazan, "Spectral state space models," _arXiv preprint arXiv:2312.06837_, 2023.\n' +
      '* [52] P. Mattes, R. Schlosser, and R. Herbrich, "Hieros: Hierarchical imagination on structured state space sequence world models," _arXiv preprint arXiv:2310.05167_, 2023.\n' +
      '\n' +
      '* [66] E. Baron, I. Zimerman, and L. Wolf, "A 2-dimensional state space layer for spatial inductive bias," in _Proceedings of the International Conference on Learning Representations_, 2023.\n' +
      '* [67] H. Guo, J. Li, T. Dai, Z. Ouyang, X. Ren, and S.-T. Xia, "Mambair: A simple baseline for image restoration with state-space model," _arXiv preprint arXiv:2402.15648_, 2024.\n' +
      '* [68] J. Huang, L. Yang, F. Wang, Y. Wu, Y. Nan, A. I. Aviles-Rivero, C.-B. Schonlieb, D. Zhang, and G. Yang, "Mambamir: An arbitrary-masked manbmba for joint medical image reconstruction and uncertainty estimation," _arXiv preprint arXiv:2402.18451_, 2024.\n' +
      '* [69] C.-S. Chen, G.-Y. Chen, D. Zhou, D. Jiang, and D.-S. Chen, "Resvrmamba: Fine-grained food category visual classification using selective state space models with deep residual learning," _arXiv preprint arXiv:2402.15761_, 2024.\n' +
      '* [70] T. Chen, Z. Tan, T. Gong, Q. Chu, Y. Wu, B. Liu, J. Ye, and N. Yu, "Mim-istid: Mamba-in-mamba for efficient infrared small target detection," _arXiv preprint arXiv:2403.02148_, 2024.\n' +
      '* [71] Y. Yue and Z. Li, "Medammba: Vision mambo for medical image classification," _arXiv preprint arXiv:2403.03849_, 2024.\n' +
      '* [72] H. Tang, L. Cheng, G. Huang, Z. Tan, J. Lu, and K. Wu, "Rotate to scan: Unet-like manb with triplet svm module for medical image segmentation," _arXiv preprint arXiv:2403.17701_, 2024.\n' +
      '* [73] Z. Fang, Y. Wang, Z. Wang, J. Zhang, X. Ji, and Y. Zhang, "Mammil: Multiple instance learning for whole slide images with state space models," _arXiv preprint arXiv:2403.05160_, 2024.\n' +
      '* [74] K. Li, X. Li, Y. Wang, Y. He, Y. Wang, L. Wang, and Y. Qiao, "Video/mambo: State space model for efficient video understanding," _arXiv preprint arXiv:2403.06977_, 2024.\n' +
      '* [75] J. Wang, J. Chen, D. Chen, and J. Wu, "Large window-based manbure under for medical image segmentation: Beyond convolution and self-attention," _arXiv preprint arXiv:2403.07332_, 2024.\n' +
      '* [76] C. Cheng, H. Wang, and H. Sun, "Activating wider areas in image super-resolution," _arXiv preprint arXiv:2403.08330_, 2024.\n' +
      '* [77] Y. Schiff, C.-H. Kao, A. Gokaslan, T. Dao, A. Gu, and V. Kuleshov, "Caducucus: Bi-directional equivariant long-range dna sequence modeling," _arXiv preprint arXiv:2403.03234_, 2024.\n' +
      '* [78] Y. Zhang, W. Yan, K. Yan, C. P. Lam, Y. Qiu, P. Zheng, R. S.-Y. Tang, and S. S. Cheng, "Motion-guided dual-camera tracker for low-cost skill evaluation of gastric endoscopy," _arXiv preprint arXiv:2403.05146_, 2024.\n' +
      '* [79] W. Liao, Y. Zhu, X. Wang, C. Pan, Y. Wang, and L. Ma, "Lightm-unet: Mambas assists in lightweight unert for medical image segmentation," _arXiv preprint arXiv:2403.05246_, 2024.\n' +
      '* [80] G. Chen, Y. Huang, J. Xu, B. Pei, Z. Chen, Z. Li, J. Wang, K. Li, T. Lu, and L. Wang, "Video manbure: State space model as a versatile alternative for video understanding," _arXiv preprint arXiv:2403.09626_, 2024.\n' +
      '* [81] M. Zhang, Y. Yu, L. Gu, T. Lin, and X. Tao, "Vm-unet-v2 rethinking vision manbure went for medical image segmentation," _arXiv preprint arXiv:2403.09157_, 2024.\n' +
      '* [82] T. Huang, X. Pei, S. You, F. Wang, C. Qian, and C. Xu, "Local-mamba: Visual state space model with windowed selective scan," _arXiv preprint arXiv:2403.09338_, 2024.\n' +
      '* [83] Z. Xu, Y. Lin, H. Han, S. Yang, R. Li, Y. Zhang, and X. Li, "Mambatalk: Efficient holistic gesture synthesis with selective state space models," _arXiv preprint arXiv:2403.09471_, 2024.\n' +
      '* [84] X. Pei, T. Huang, and C. Xu, "Efficientvmmba: Atrous selective scan for light weight visual mamba," _arXiv preprint arXiv:2403.09977_, 2024.\n' +
      '* [85] C. Du, Y. Li, and C. Xu, "Understanding robustness of visual state space models for image classification," _arXiv preprint arXiv:2403.10935_, 2024.\n' +
      '* [86] Y. Shi, B. Xia, X. Jin, X. Wang, T. Zhao, X. Xia, X. Xiao, and W. Yang, "Vmambair: Visual state space model for image restoration," _arXiv preprint arXiv:2403.11423_, 2024.\n' +
      '* [87] T. Guo, Y. Wang, and C. Meng, "Mambamorph: a mamba-based backbone with contrastive feature learning for deformable mr-ct registration," _arXiv preprint arXiv:2401.13394_, 2024.\n' +
      '* [88] Y. Yang, Z. Xing, and L. Zhu, "Vivim: a video vision mambo for medical video object segmentation," _arXiv preprint arXiv:2401.14168_, 2024.\n' +
      '* [89] J. Xie, R. Liao, Z. Zhang, S. Yi, Y. Zhu, and G. Luo, "Promaxmba: Prompt-mamba for polyp segmentation," _arXiv preprint arXiv:2403.13660_, 2024.\n' +
      '* [90] R. Wu, Y. Liu, P. Liang, and Q. Chang, "H-vmunet: High-order vision mamu met for medical image segmentation," _arXiv preprint arXiv:2403.13642_, 2024.\n' +
      '* [91] C. Yang, Z. Chen, M. Espinosa, L. Ericsson, Z. Wang, J. Liu, and E. J. Crowley, "Plainmba: Improving non-hierarchoral mamba in visual recognition," _arXiv preprint arXiv:2403.17695_, 2024.\n' +
      '* [92] K. S. Sanjadi, M. T. Hossain, M. S. S. Junayed, and D. M. M. Uddin, "Integrating mamba sequence model and hierarchical upsampling network for accurate semantic segmentation of multiple sclerosis lesion," _arXiv preprint arXiv:2403.17432_, 2024.\n' +
      '* [93] Y. Tang, P. Dong, Z. Tang, X. Chu, and J. Liang, "Vmrnn: Integrating vision mambo and lstm for efficient and accurate spatiotemporal forecasting," _arXiv preprint arXiv:2403.163636_, 2024.\n' +
      '* [94] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang, "Gamba: Marry gaussian splitting with mambo for single view 3d reconstruction," _arXiv preprint arXiv:2403.18795_, 2024.\n' +
      '* [95] Z. Wang, J.-Q. Zheng, C. Ma, and T. Guo, "Vmambamorph: a visual mamba-based framework with cross-scan module for deformable 3d image registration," _arXiv preprint arXiv:2404.05105_, 2024.\n' +
      '* [96] J. Hao, L. He, and K. F. Hung, "T-mamba: Frequency-enhanced gated long-range dependency for tooth 3d cbct segmentation," _arXiv preprint arXiv:2404.01065_, 2024.\n' +
      '* [97] W. Li, X. Hong, and X. Fan, "Spikemba: Multi-modal spiking saliency mamba for temporal video grounding," _arXiv preprint arXiv:2404.01174_, 2024.\n' +
      '* [98] X. Ma, X. Zhang, and M.-O. Pun, "Rsqamba: Visual state space model for remote sensing images semantic segmentation," _arXiv preprint arXiv:2404.02457_, 2024.\n' +
      '* [99] H. Chen, J. Song, C. Han, J. Xia, and N. Yokoya, "Changemmaba: Remote sensing change detection with spatio-temporal state space model," _arXiv preprint arXiv:2404.03425_, 2024.\n' +
      '* [100] M. Shahab Sepehri, Z. Fabian, and M. Soltanolkotabi, "Serpent: Scalable and efficient image restoration via multi-scale structured state space models," _arXiv preprint arXiv:2403.17902_, 2024.\n' +
      '* [101] Y. Yang, C. Ma, J. Yao, Z. Zhong, Y. Zhang, and Y. Wang, "Remember: Referring image segmentation with mamba twister," _arXiv preprint arXiv:2403.17839_, 2024.\n' +
      '* [102] Q. Wang, C. Wang, Z. Lai, and Y. Zhou, "Insectmamba: Insect pest classification with state space model," _arXiv preprint arXiv:2404.03611_, 2024.\n' +
      '* [103] Q. Zhu, Y. Cai, Y. Fang, Y. Yang, C. Chen, L. Fan, and A. Nguyen, "Samba: Semantic segmentation of remotely sensed images with state space model," _arXiv preprint arXiv:2404.01705_, 2024.\n' +
      '* [104] A. Behrouz, M. Santactaterina, and R. Zabib, "Mambamixer: Efficient selective state space models with dual token and channel selection," _arXiv preprint arXiv:2403.19888_, 2024.\n' +
      '* [105] R. Wu, Y. Liu, P. Liang, and Q. Chang, "Ultralight vvm-unet: Parallel vision mamba significantly reduces parameters for skin lesion segmentation," _arXiv preprint arXiv:2403.20035_, 2024.\n' +
      '* [106] B. Zou, Z. Guo, X. Hu, and H. Ma, "Rhythmamamba: Fast remote physiological measurement with arbitrary length videos," _arXiv preprint arXiv:2404.06483_, 2024.\n' +
      '* [107] H. He, Y. Bai, J. Zhang, Q. He, H. Chen, Z. Gan, C. Wang, X. Li, G. Tian, and L. Xie, "Mambaad: Exploring state space models for multi-class unsupervised anomaly detection," _arXiv preprint arXiv:2404.06564_, 2024.\n' +
      '* [108] S. Chaudhuri and S. Bhattacharya, "Simba: Mamba augmented u-shifengton for skeletal action recognition in videos," _arXiv preprint arXiv:2404.07645_, 2024.\n' +
      '* [109] A. Archit and C. Pape, "Vim-unet: Vision mambo for biomedical segmentation," _arXiv preprint arXiv:2404.07705_, 2024.\n' +
      '* [110] S. Long, Q. Zhou, X. Li, X. Lu, C. Ying, Y. Luo, L. Ma, and S. Yan, "Dpmamba: Domain generalization via generalized state space model," _arXiv preprint arXiv:2404.07794_, 2024.\n' +
      '* [111] S. Peng, X. Zhu, H. Deng, Z. Lei, and L.-J. Deng, "Fusionmamba: Efficient image fusion with state space model," _arXiv preprint arXiv:2404.07932_, 2024.\n' +
      '* [112] A. Gu, A. Gupta, K. Goel, and C. Re, "On the parameterization and initialization of diagonal state space models," _arXiv preprint arXiv:2206.11893_, 2022.\n' +
      '* [113the Association for Computational Linguistics: EMNLP 2023, pp. 14 048-14 077.\n' +
      '* [126] Y. Duan, W. Wang, Z. Chen, X. Zhu, L. Lu, T. Lu, Y. Qiao, H. Li, J. Dai, and W. Wang, "Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures," _arXiv preprint arXiv:2403.02308_, 2024.\n' +
      '* [127] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, "Retentive network: A successor to transformer for large language models," _arXiv preprint arXiv:2307.08621_, 2023.\n' +
      '* [128] X. Ma, C. Zhou, X. Kong, J. He, L. Gui, G. Neubig, J. May, and L. Zettlemoyer, "Mega: Moving average equipped gated attention," in _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [129] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, "Hungry hungry hungry hungry hungos: Towards language modeling with state space models," in _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [130] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind, "An attention free transformer," _arXiv preprint arXiv:2105.14103_, 2021.\n' +
      '* [131] H. Hou and F. R. Yu, "Rwkv-ts: Beyond traditional recurrent neural network for fine series tasks," _arXiv preprint arXiv:2401.09093_, 2024.\n' +
      '* [132] Z. Zhu, W. Shao, and D. Jiao, "Tls-rwkv: Real-time online action detection with temporal label smoothing," _Neural Processing Letters_, vol. 56, no. 2, pp. 1-13, 2024.\n' +
      '* [133] Z. Fei, M. Fan, C. Yu, D. Li, and J. Huang, "Diffusion-rwkv: Scaling rkwv-like architectures for diffusion models," _arXiv preprint arXiv:2404.04478_, 2024.\n' +
      '* [134] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, "Attention is all you need in speech separation," in _Proceedings of International Conference on Acoustics, Speech and Signal Processing_, 2020, pp. 21-25.\n' +
      '* [135] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B. Kim, and S. Watanabe, "Tf-grindet: Making time-frequency domain models great again for monanural speaker separation," in _Proceedings of International Conference on Acoustics, Speech and Signal Processing_, 2022, pp. 1-5.\n' +
      '* [136] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meiriony, Y. Belinkov, S. Shalev-Shwartz _et al._, "Jamba: A hybrid transformer-mamba language model," _arXiv preprint arXiv:2403.19887_, 2024.\n' +
      '* [137] S. Li, T. Zhu, F. Duan, L. Chen, H. Ning, and Y. Wan, "Harmamba: Efficient wearable sensor human activity recognition based on bidirectional selective ssm," _arXiv preprint arXiv:2403.20183_, 2024.\n' +
      '* [138] J. X. Yang, J. Zhou, J. Wang, H. Tian, and A. W. C. Liew, "Hismamba: Hyperspectral imaging efficient feature learning with bidirectional state space for classification," _arXiv preprint arXiv:2404.00272_, 2024.\n' +
      '* [139] S. Zhao, H. Chen, X. Zhang, P. Xiao, L. Bai, and W. Ouyang, "Rsmamba for large remote sensing image dense prediction," _arXiv preprint arXiv:2404.02668_, 2024.\n' +
      '* [140] Y. Ding, A. Orvieto, B. He, and T. Hofmann, "Recurrent distance filtering for graph representation learning," _arXiv preprint arXiv:2312.01538_, 2024.\n' +
      '* [141] J. Park, J. Park, Z. Xiong, N. Lee, J. Cho, S. Oymak, K. Lee, and D. Papailiopoulos, "Can mamba learn how to learn? a comparative study on in-context learning tasks," _arXiv preprint arXiv:2402.04248_, 2024.\n' +
      '* [142] N. Zucchet, S. Kobayashi, Y. Akram, J. von Oswald, M. Larcher, A. Steger, and J. Sacarmento, "Gated recurrent neural networks discover attention," _arXiv preprint arXiv:2309.01775_, 2023.\n' +
      '* [143] A. Ali, I. Zimerman, and L. Wolf, "The hidden attention of mamba models," _arXiv preprint arXiv:2403.01590_, 2024.\n' +
      '* [144] S. Yang, Y. Wang, and H. Chen, "Mambahl: Enhancing long sequence modeling with sequence reordering in computational pathology," _arXiv preprint arXiv:2403.06800_, 2024.\n' +
      '* [145] G. L. C. S. Z. Z. S. M. W. Q. Qiao Yanyuan, Yu Zheng and L. Jing, "V1-mamba: Exploring state space models for multimodal learning," _arXiv preprint arXiv:2403.13600_, 2024.\n' +
      '* [146] G. Yang, K. Du, Z. Yang, Y. Du, Y. Zheng, and S. Wang, "Cmvim: Contrastive masked vim autoencoder for 3d multimodal representation learning for ad classification," _arXiv preprint arXiv:2403.16520_, 2024.\n' +
      '* [147] H. Zhao, M. Zhang, W. Zhao, P. Ding, S. Huang, and D. Wang, "Cobra: Extending mamba to multi-modal large language model for efficient inference," _arXiv preprint arXiv:2403.14520_, 2024.\n' +
      '* [148] T. Ota, "Decision mamba: Reinforcement learning via sequence modeling with selective state spaces," _arXiv preprint arXiv:2403.19925_, 2024.\n' +
      '* [149] Z. Wan, Y. Wang, S. Yong, P. Zhang, S. Stepputtis, K. Sycara, and Y. Xie, "Sigma: Siamese mamba network for multi-modal semantic segmentation," _arXiv preprint arXiv:2404.04256_, 2024.\n' +
      '* [150] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, "Masked autoencoders are scalable vision learners," _arXiv preprint arXiv:2111.06377_, 2021.\n' +
      '* [151] A. Dosovitskiy, I. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2101.11929_, 2021.\n' +
      '* [152] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch, "Decision transformer: Reinforcement learning via sequence modeling," in _Proceedings of Advances in Neural Information Processing Systems_, 2021, pp. 15 084-15 097.\n' +
      '* [153] D. Liang, X. Zhou, X. Wang, X. Zhu, W. Xu, Z. Zou, X. Ye, and X. Bai, "Pointmamba: A simple state space model for point cloud analysis," _arXiv preprint arXiv:2402.10739_, 2024.\n' +
      '* [154] T. Zhang, X. Li, H. Yuan, S. Ji, and S. Yan, "Point cloud mamba: Point cloud learning via state space model," _arXiv preprint arXiv:2403.00762_, 2024.\n' +
      '* [155] J. Liu, R. Yu, Y. Wang, Y. Zheng, T. Deng, W. Ye, and H. Wang, "Point mamba: A novel point cloud backbone based on state space model with octree-based ordering strategy," _arXiv preprint arXiv:2403.06467_, 2024.\n' +
      '* [156] Q. Zhou, W. Yang, B. Fei, J. Xu, R. Zhang, K. Liu, Y. Luo, and Y. He, "3dmanbaipf: A state space model for iterative point cloud filtering via differentiable rendering," _arXiv preprint arXiv:2404.05522_, 2024.\n' +
      '* [157] Y. Li, W. Yang, and B. Fei, "3dmanbacomplete: Exploring structured state space model for point cloud completion," _arXiv preprint arXiv:2404.07106_, 2024.\n' +
      '* [158] N. Zubir, M. Gehrig, and D. Scaramurza, "State space models for event cameras," _arXiv preprint arXiv:2402.15584_, 2024.\n' +
      '* [159] K. Goel, A. Gu, C. Donahue, and C. Re, "It\'s raw! audio generation with state-space models," in _Proceedings of the International Conference on Machine Learning_, 2022, pp. 7616-7633.\n' +
      '* [160] J. Wang, J. N. Yan, A. Gu, and A. M. Rush, "Pretraining without attention," _arXiv preprint arXiv:2212.10544_, 2022.\n' +
      '* [161] S. Massaroli, M. Poli, D. Fu, H. Kumbong, R. Parnichkun, D. Romero, A. Timalsina, Q. McIntyre, B. Chen, A. Rudra _et al._, "Laughing hyeqn distillery: Extracting compact recurrences from convolutions," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [162] J. Smith, S. De Mello, J. Kautz, S. Linderman, and W. Byeon, "Convolutional state space models for long-range spatiotemporal modeling," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [163] C. Lu, Y. Schroecker, A. Gu, E. Parisotto, J. Foerster, S. Singh, and F. Behbahani, "Structured state space models for in-context reinforcement learning," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [164] S. Wang and Q. Li, "Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization," _arXiv preprint arXiv:2311.14495_, 2023.\n' +
      '* [165] M. Pioro, K. Ciebiera, K. Krol, J. Ludziejewski, and S. Jaszczur, "Moe-mamba: Efficient selective state space models with mixture of experts," _arXiv preprint arXiv:2014.04810_, 2024.\n' +
      '* [166] J. Wang, T. Gangavarapu, J. N. Yan, and A. M. Rush, "Mambabyte: Token-free selective state space model," _arXiv preprint arXiv:2403.13660_, 2024.\n' +
      '* [167] Q. Anthony, Y. Tokpanov, P. Glorioso, and B. Millidge, "Black-mamba: Mixture of experts for state-space models," _arXiv preprint arXiv:2402.01717_, 2024.\n' +
      '* [168] F. L. Bronne, S. Duong, M. Ravaut, A. Allauzen, N. F. Chen, V. Guigue, A. Lumbreras, L. Soulier, and P. Gallinari, "Locost: State-space models for long document abstractive summarization," _arXiv preprint arXiv:2201.17919_, 2024.\n' +
      '* [169] M. R. Samsami, A. Zholus, J. Rajendran, and S. Chandar, "Mastering memory tasks with world models," _arXiv preprint arXiv:24* [171] F. Liu and Q. Li, "From generalization analysis to optimization designs for state space models," 2024. [Online]. Available: [https://openreview.net/forum?id=E6yJMcGrI](https://openreview.net/forum?id=E6yJMcGrI)\n' +
      '* [172] A. Yu, A. Nigmentov, D. Morozov, M. W. Mahoney, and N. B. Erichson, "Robustifying state-space models for long sequences via approximate diagonalization," in _Proceedings of the International Conference on Learning Representations_, 2024.\n' +
      '* [173] E. David, J. Bellot, and S. L. Corff, "Variational quantization for state space models," 2024. [Online]. Available: [https://openreview.net/forum?id=EAkjVCH02](https://openreview.net/forum?id=EAkjVCH02)\n' +
      '* [174] D. Y. Fu, H. Kumbong, E. Nguyen, and C. Re, "Flashfftconv: Efficient convolutions for long sequences with tensor cores," _arXiv preprint arXiv:2311.05908_, 2023.\n' +
      '* [175] C. Liu, J. Lin, J. Wang, H. Liu, and J. Caverlee, "Mambatrec: Towards efficient sequential recommendation with selective state space models," _arXiv preprint arXiv:2403.03900_, 2024.\n' +
      '* [176] B. Silva, M. Contreras, S. Bandyopadhyay, Y. Ren, Z. Guan, J. Balch, K. Khezei, T. O. Baslanti, B. Shickel, A. Bibrac _et al._, "A multi-cohort study on prediction of acute brain dysfunction states using selective state space models," _arXiv preprint arXiv:2403.07201_, 2024.\n' +
      '* [177] C. Quan and X. Li, "Multichannel long-term streaming neural speech enhancement for static and moving speakers," _arXiv preprint arXiv:2403.07675_, 2024.\n' +
      '* [178] Z. Shi, "Mambatstock: Selective state space model for stock prediction," _arXiv preprint arXiv:2402.18959_, 2024.\n' +
      '* [179] R. Bhirangi, C. Wang, V. Pattabiraman, C. Majidi, A. Gupta, T. Hellebrekers, and L. Pinto, "Hierarchical state space models for continuous sequence-to-sequence modeling," _arXiv preprint arXiv:2402.10211_, 2024.\n' +
      '* [180] M. A. Ahamed and Q. Cheng, "Timemachine: A time series is worth 4 mambas for long-term forecasting," _arXiv preprint arXiv:2403.08989_, 2024.\n' +
      '* [181] Y. Zhang, Z. Lin, Y. Sun, F. Yin, and C. Fritsche, "Regularization-based efficient continual learning in deep state-space models," _arXiv preprint arXiv:2403.10123_, 2024.\n' +
      '* [182] M. Poli, A. W. Thomas, E. Nguyen, P. Ponnusamy, B. Deiseroth, K. Kersting, T. Suzuki, B. Hie, S. Ermon, C. R\'e, C. Zhang, and S. Massarodi, "Mechanistic design and scaling of hybrid architectures," _arXiv preprint arXiv:2403.17844_, 2024.\n' +
      '* [183] D. LaRocque, W. Guinont-Martin, D.-A. Duclos, P. Giguere, and F. Pomerleau, "Proprioception is all you need: Terrain classification for boreal forests," _arXiv preprint arXiv:2403.16877_, 2024.\n' +
      '* [184] Z. Wang, F. Kong, S. Feng, M. Wang, H. Zhao, D. Wang, and Y. Zhang, "Is mamba effective for time series forecasting?" _arXiv preprint arXiv:2403.11144_, 2024.\n' +
      '* [185] B. N. Patro and V. S. Agneswaran, "Simba: Simplified mambased architecture for vision and multivariate time series," _arXiv preprint arXiv:2403.15360_, 2024.\n' +
      '* [186] Z. Xu, "Rankmamba, benchmarking mamba\'s document ranking performance in the era of transformers," _arXiv preprint arXiv:2403.18276_, 2024.\n' +
      '* [187] A. S. Sharma, D. Atkinson, and D. Bau, "Locating and editing factual associations in mamba," 2024.\n' +
      '* [188] H. Yin, G. Cheng, C. J. Steinmetz, R. Yuan, R. M. Stern, and R. Dannenberg, "Modeling analog dynamic range compressors using deep learning and state-space models," _arXiv preprint arXiv:2403.16331_, 2024.\n' +
      '* [189] M. Forgione, M. Mejari, and D. Piga, "Model order reduction of deep structured state-space models: A system-theoretic approach," _arXiv preprint arXiv:2403.14833_, 2024.\n' +
      '* [190] J. Yang, Y. Li, J. Zhao, H. Wang, M. Ma, J. Ma, Z. Ren, M. Zhang, X. Yin, Z. Chen _et al._, "Uncovering selective state space model" capabilities in lifelong sequential recommendation," _arXiv preprint arXiv:2403.16371_, 2024.\n' +
      '* [191] S. Wang and B. Xue, "State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [192] I. Amos, J. Berant, and A. Gupta, "Never train from scratch: Fair comparison of long-sequence models requires data-driven priors," _arXiv preprint arXiv:2310.02980_, 2023.\n' +
      '* [193] C. A. Alonso, J. Sieber, and M. N. Zeilinger, "State space models as foundation models: A control theoretic overview," _arXiv preprint arXiv:2403.16899_, 2024.\n' +
      '* [194] E. J. Olucha, B. Terzin, A. Das, and R. Toth, "On the reduction of linear parameter-varying state-space models," _arXiv preprint arXiv:2404.01817_, 2024.\n' +
      '* [195] Z. Tan, Y. Yang, J. Wan, G. Guo, and S. Z. Li, "Relation-aware pedestrian attribute recognition with graph convolutional networks," _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 34, no. 7, pp. 12055-12062, 2020.\n' +
      '* [196] J. Wu, H. Liu, J. Jiang, M. Qi, B. Ren, X. Li, and Y. Wang, "Person attribute recognition by sequence contextual relation learning," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 30, no. 10, pp. 3398-3412, 2020.\n' +
      '* [197] J. Jia, X. Chen, and K. Huang, "Spatial and semantic consistency regularizations for pedestrian attribute recognition," _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 942-951, 2021.\n' +
      '* [198] "Inter-attribute awareness for pedestrian attribute recognition," _Pattern Recognition_, vol. 131, p. 108865, 2022.\n' +
      '* [199] L. Chen, J. Song, X. Zhang, and M. Shang, "Mcfi: multi-label contrastive focal loss for deep imbalanced pedestrian attribute recognition," _Neural Computing and Applications_, vol. 34, no. 19, pp. 16 701-16 715, 2022.\n' +
      '* [200] Z. Tang and J. Huang, "Drformer: Learning dual relations using transformer for pedestrian attribute recognition," _Neurocomputing_, vol. 497, pp. 159-169, 2022.\n' +
      '* [201] H. Guo, X. Fan, and S. Wang, "Visual attention consistency for human attribute recognition," _International Journal of Computer Vision_, vol. 130, no. 4, pp. 1088-1106, 2022.\n' +
      '* [202] J. Jia, N. Gao, F. He, X. Chen, and K. Huang, "Learning disentangled attribute representations for robust pedestrian attribute recognition," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022, pp. 1069-1077.\n' +
      '* [203] H. Fan, H.-M. Hu, S. Liu, W. Lu, and S. Pu, "Correlation graph convolutional network for pedestrian attribute recognition," _IEEE Transactions on Multimedia_, vol. 24, pp. 49-60, 2020.\n' +
      '* [204] Y. Yang, Z. Tan, P. Tiwari, H. M. Pandey, J. Wan, Z. Lei, G. Guo, and S. Z. Li, "Cascaded split-and-aggregate learning with feature recombination for pedestrian attribute recognition," _International Journal of Computer Vision_, vol. 129, no. 10, pp. 2731-2744, 2021.\n' +
      '* [205] X. Wang, J. Jin, C. Li, J. Tang, C. Zhang, and W. Wang, "Pedestrian attribute recognition via clip based prompt vision-language fusion," _arXiv preprint arXiv:2312.10692_, 2023.\n' +
      '* [206] J. Jin, X. Wang, C. Li, L. Huang, and J. Tang, "Sequencepar: Understanding pedestrian attributes via a sequence generation paradigm," _arXiv preprint arXiv:2312.01640_, 2023.\n' +
      '* [207] X. Cheng, M. Jia, Q. Wang, and J. Zhang, "A simple visual-textual baseline for pedestrian attribute recognition," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 32, no. 10, pp. 6994-7004, 2022.\n' +
      '* [208] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, J. Yan, and X. Wang, "Hydraplus-net: Attentive deep features for pedestrian analysis," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2017, pp. 350-359.\n' +
      '* [209] Y. Deng, P. Luo, C. C. Loy, and X. Tang, "Pedestrian attribute recognition at far distance," in _Proceedings of the ACM International Conference on Multimedia_, 2014, pp. 789-792.\n' +
      '* [210] B. Ye, H. Chang, B. Ma, S. Shan, and X. Chen, "Joint feature learning and relation modeling for tracking: A one-stream framework," in _Proceedings of European Conference on Computer Vision_, 2022, pp. 341-357.\n' +
      '* [211] N. Wang, W. Zhou, J. Wang, and H. Li, "Transformer meets tracker: Exploiting temporal context for robust visual tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, p. 1571-1580.\n' +
      '* [211] C. Mayer, M. Danelljan, G. Bhat, M. Paul, D. P. Paudel, F. Yu, and L. V. Gool, "Transforming model prediction for tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, p. 8731-8740.\n' +
      '* [212] G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, "Learning discriminative model prediction for tracking," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, p. 6182-6191.\n' +
      '* [213] M. Danelljan, L. V. Gool, and R. Timofte, "Probabilistic regression for visual tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, p. 7183-7192.\n' +
      '* [214] G. Bhat, M. Danelljan, L. Van Gool, and R. Timofte, "Know your surroundings: Exploiting scene information for object tracking,"in _Proceedings of European Conference on Computer Vision_, 2020, p. 205-221.\n' +
      '* [21] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, "Atom: Accurate tracking by overlap maximization," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, p. 4660-4669.\n' +
      '* [22] X. Wang, S. Wang, C. Tang, L. Zhu, B. Jiang, Y. Tian, and J. Tang, "Event stream-based visual object tracking: A high-resolution benchmark dataset and a novel baseline," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2024.\n' +
      '* [23] S. Gao, C. Zhou, C. Ma, X. Wang, and J. Yuan, "Aiatrack: Attention in attention for transformer visual tracking," in _Proceedings of European Conference on Computer Vision_, 2022, p. 146-164.\n' +
      '* [24] B. Ye, H. Chang, B. Ma, and S. Shan, "Joint feature learning and relation modeling for tracking: A one-stream framework," _arXiv preprint arXiv:2203.11991_, 2022.\n' +
      '* [25] X. Chen, J. Yan, Bin Zhu, D. Wang, X. Yang, and H. Lu, "Transformer tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, p. 8126-8135.\n' +
      '* [26] Y. Cui, C. Jiang, L. Wang, and W. Gangshan, "Miformor: End-to-end tracking with iterative mixed attention," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, p. 13608-13618.\n' +
      '* [27] B. Chen, P. Li, L. Bai, L. Qiao, Q. Shen, B. Li, W. Gan, W. Wu, and W. Ouyang, "Backbone is all your need: A simplified architecture for visual object tracking," in _Proceedings of European Conference on Computer Vision_, 2021, p. 375-392.\n' +
      '* [28] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, "Swin-unet: Unet-like pure transformer for medical image segmentation," in _Proceedings of European Conference on Computer Vision_, 2022, pp. 205-218.\n' +
      '* [29] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, "Preparing a collection of radiology examinations for distribution and retrieval," _Journal of the American Medical Informatics Association_, vol. 23, no. 2, pp. 304-310, 2016.\n' +
      '* [30] H. Touvron, L. Martin, K. R. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlyko, S. Batra, P. Bhargava, S. Bhosale, D. M. Bliek, I. Blekec, C. C. Ferrer, M. Chen, G. Cucurill, D. Esbub, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. S. Hartshorn, S. Hossein, R. Hou, H. Inan, M. Kardas, V. Kerrez, M. Khabsa, I. M. Klourann, A. V. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Rez Rez Rezenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambardu, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Seiolam, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [26] Z. Chen, Y. Song, T.-H. Chang, and X. Wan, "Generating radiology reports via memory-driven transformer," in _Proceedings of the Conference on Empirical Methods in Natural Language Processing_, 2020, pp. 1439-1449.\n' +
      '* [27] C. Y. Li, X. Liang, Z. Hu, and E. P. Xing, "Knowledge-driven encode, retrieve, paraphrase for medical image report generation," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019, pp. 6666-6673.\n' +
      '* [28] Y. Li, X. Liang, Z. Hu, and E. P. Xing, "Hybrid retrieval-generation reinforced agent for medical image report generation," in _Proceedings of Advances in Neural Information Processing Systems_, 2018.\n' +
      '* [29] Y. Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, and D. Xu, "When radiology report generation meets knowledge graph," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020, pp. 12910-12917.\n' +
      '* [30] F. Liu, X. Wu, S. Ge, W. Fan, and Y. Zou, "Exploring and distilling posterior and prior knowledge for radiology report generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 13753-13762.\n' +
      '* [31] S. Yang, X. Wu, S. Ge, S. K. Zhou, and L. Xiao, "Knowledge matters: Chest radiology report generation with general and specific knowledge," _Medical Image Analysis_, vol. 80, p. 102510, 2022.\n' +
      '* [32] F. Liu, C. Yin, X. Wu, S. Ge, P. Zhang, and X. Sun, "Contrastive attention for automatic chest x-ray report generation," in _Proceedings of Findings of the Association for Computational Linguistics_, 2021, pp. 269-280.\n' +
      '* [33] F. Liu, S. Ge, and X. Wu, "Competence-based multimodal curriculum learning for medical report generation," in _Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing_, 2021, pp. 3001-3012.\n' +
      '* [34] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang, "Dynamic graph enhanced contrastive learning for chest x-ray report generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 3334-3343.\n' +
      '* [35] Z. Zhuang, L. Wei, L. Xie, T. Zhang, H. Zhang, H. Wu, H. Ai, and Q. Tian, "Rethinking the distribution gap of person re-identification with camera-based batch normalization," in _Proceedings of European Conference on Computer Vision_. Springer, 2020, pp. 140-157.\n' +
      '* [36] B. He, J. Li, Y. Zhao, and Y. Tian, "Part-regularized near-duplicate vehicle re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 3997-4005.\n' +
      '* [37] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, "Omni-scale feature learning for person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 3702-3712.\n' +
      '* [38] J. Qian, W. Jiang, H. Luo, and H. Yu, "Stripe-based and attribute-aware network: A two-branch deep model for vehicle re-identification," _Measurement Science and Technology_, vol. 31, no. 9, p. 09504, 2020.\n' +
      '* [39] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, "Learning discriminative features with multiple granularities for person re-identification," in _Proceedings of the ACM International Conference on Multimedia_, 2018, pp. 274-282.\n' +
      '* [40] X. Jin, C. Lan, W. Zeng, and Z. Chen, "Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020, pp. 11165-1172.\n' +
      '* [41] Z. Zhang, C. Lan, W. Zeng, X. Jin, and Z. Chen, "Relation-aware global attention for person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 3186-3195.\n' +
      '* [42] R. Chu, Y. Sun, Y. Li, Z. Liu, C. Zhang, and Y. Wei, "Vehicle re-identification with viewpoint-aware metric learning," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 8282-8291.\n' +
      '* [43] X. Jin, C. Lan, W. Zeng, G. Wei, and Z. Chen, "Semantics-aligned representation learning for person re-identification," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020, pp. 11 173-111 80.\n' +
      '* [44] T.-S. Chen, C.-T. Liu, C.-W. Wu, and S.-Y. Chien, "Orientation-aware vehicle re-identification with semantics-guided part attention network," in _Proceedings of European Conference on Computer Vision_, 2020, pp. 330-346.\n' +
      '* [45] X. Chen, C. Fu, Y. Zhao, F. Zheng, J. Song, R. Ji, and Y. Yang, "Salience-guided cascaded suppression network for person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 3300-3310.\n' +
      '* [46] X. Zhang, R. Zhang, J. Cao, D. Gong, M. You, and C. Shen, "Parti-guided attention learning for vehicle re-identification," _arXiv preprint arXiv:1909.06023_, 2019.\n' +
      '* [47] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang, "Abd-net: Attentive but diverse person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 8351-8361.\n' +
      '* [48] D. Meng, L. Li, X. Liu, Y. Li, S. Yang, Z.-J. Zha, X. Gao, S. Wang, and Q. Huang, "Parsing-based view-aware embedding network for vehicle re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 7103-7112.\n' +
      '* [49] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, "Pose-guided feature alignment for occluded person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 542-551.\n' +
      '* [50] P. Khorramshahi, N. Peri, J.-c. Chen, and R. Chellappa, "The devil is in the details: Self-supervised attention for vehicle re-identification," in _Proceedings of European Conference on Computer Vision_, 2020, pp. 369-386.\n' +
      '\n' +
      '* [251] G. Wang, S. Yang, H. Liu, Z. Wang, Y. Yang, S. Wang, G. Yu, E. Zhou, and J. Sun, "High-order information matters: Learning relation and topology for occluded person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 6449-6458.\n' +
      '* [252] Z. Sun, X. Nie, X. Xi, and Y. Yin, "Cfvmnet: A multi-branch network for vehicle re-identification based on common field of view," in _Proceedings of the ACM International Conference on Multimedia_, 2020, pp. 3523-3531.\n' +
      '* [253] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, "Identity-guided human semantic parsing for person re-identification," in _Proceedings of European Conference on Computer Vision_, 2020, pp. 346-363.\n' +
      '* [254] A. Suprem and C. Pu, "Looking glamorous: Vehicle re-id in heterogeneous cameras networks with global and local attention," _arXiv preprint arXiv:2002.02256_, 2020.\n' +
      '* [255] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, "Transreid: Transformer-based object re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021, pp. 15 013-15 022.\n' +
      '* [256] X. Wang, W. Wu, C. Li, Z. Zhao, Z. Chen, Y. Shi, and J. Tang, "Structural information guided multimodal pre-training for vehicle-centric perception," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2024, pp. 5624-5632.\n' +
      '* [257] X. Shu, X. Wang, X. Zang, S. Zhang, Y. Chen, G. Li, and Q. Tian, "Large-scale spatio-temporal person re-identification: Algorithms and benchmark," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 32, no. 7, pp. 4390-4403, 2021.\n' +
      '* [258] L. Wei, S. Zhang, W. Gao, and Q. Tian, "Person transfer gan to bridge domain gap for person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2018, pp. 79-88.\n' +
      '* [259] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, "Scalable person re-identification: A benchmark," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2015, pp. 1116-1124.\n' +
      '* [260] Z. Zhang, J. Wu, X. Zhang, and C. Zhang, "Multi-target, multi-camera tracking by hierarchical clustering: Recent progress on dukemtmc project," _arXiv preprint arXiv:1712.09531_, 2017.\n' +
      '* [261] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, "Pose-guided feature alignment for occluded person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 542-551.\n' +
      '* [262] X. Liu, W. Liu, H. Ma, and H. Fu, "Large-scale vehicle re-identification in urban surveillance videos," in _Proceedings of the IEEE International Conference on Multimedia and Expo_, 2016, pp. 1-6.\n' +
      '* [263] H. Liu, Y. Tian, Y. Yang, L. Pang, and T. Huang, "Deep relative distance learning: Tell the difference between similar vehicles," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2016, pp. 2167-2175.\n' +
      '* [264] H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang, "Bag of tricks and a strong baseline for deep person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, 2019, pp. 1487-1495.\n' +
      '* [265] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, "Training data-efficient image transformers & distillation through attention," in _Proceedings of the International Conference on Machine Learning_, 2021, pp. 10 347-10 357.\n' +
      '* [266] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l Casas, E. B. Hanna, F. Bressand _et al._, "Mirxtal of experts," _arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* [267] C. Tang, X. Wang, J. Huang, B. Jiang, L. Zhu, J. Zhang, Y. Wang, and Y. Tian, "Revisiting color-event based tracking: A unified network, dataset, and metric," _arXiv preprint arXiv:2211.11010_, 2022.\n' +
      '* [268] X. Wang, J. Huang, S. Wang, C. Tang, B. Jiang, Y. Tian, J. Tang, and B. Luo, "Long-term frame-event visual tracking: Benchmark dataset and baseline," _arXiv preprint arXiv:2403.05839_, 2024.\n' +
      '* [269] M. Lin, Q. Chen, and S. Yan, "Network in network," _arXiv preprint arXiv:1312.4400_, 2013.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
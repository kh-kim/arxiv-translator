<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 변압기 대체 신세대 네트워크 상태공간 모델: 설문조사\n' +
      '\n' +
      '샤오왕, 샤오왕, 유허딩, 위앙리, 웬타오우, 야오룽, 위제콩, 주황, 시하오리, 하오샹양, 즈웬왕, 보강, 천룽리, 야오웨이왕, 용홍톈, 진탕\n' +
      '\n' +
      '샤오왕, 시하오왕, 유허딩, 류항리, 야오룽, 주황, 하오샹양, 즈웬왕, 보장, 진탕은 중국 허페이 230601, 안후이 대학교 컴퓨터 과학 기술 학파와 함께 있다. (이메일: xiawang@ahu.edu.cn) Weizhe Kong, Wentao Wu, Shihao Li, 그리고 Chenglong Li는 중국 허페이 230601, 안후이 대학의 인공지능 학부와 함께 있습니다. (이메일: lcl1314@famil.com) Yaowei Wang은 중국 Shenzhen의 Peng Cheng Laboratory; 중국 Shenzhen의 Harbin Technology Institute(HITS2)와 함께 있습니다. (전자 메일: wangyu@pcl.ac.cn) 용홍톈은 중국 선전 펑청 연구소와 함께 있다; 멀티미디어 정보 처리를 위한 국가 핵심 연구소, 컴퓨터 과학 학교, 북경 대학, 중국, 선전 대학원, 북경 대학, 전자 및 컴퓨터 공학 학교(전자 메일: yhitant@pku.edu.cn) 교신저자: 보 장(강보@ahu.edu.cn)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '포스트 딥 러닝 시대에 트랜스포머 아키텍처는 사전 훈련된 빅 모델과 다양한 다운스트림 태스크에 걸쳐 강력한 성능을 입증했다. 그러나, 이 아키텍처의 막대한 계산 요구는 많은 연구자들을 단념시켰다. 어텐션 모델의 복잡성을 더욱 줄이기 위해 보다 효율적인 방법을 설계하기 위한 많은 노력이 있었다. 이 중 State Space Model(SSM)은 Self-Attention 기반 Transformer 모델을 대체할 수 있는 모델로서 최근 더욱 주목받고 있다. 본 논문에서는 이러한 연구들에 대한 첫 번째 포괄적인 검토를 하고, 또한 SSM의 특징과 장점을 더 잘 입증하기 위한 실험적 비교 및 분석을 제공한다. 특히, 먼저 독자들이 SSM의 핵심 아이디어를 신속하게 포착할 수 있도록 원리에 대한 자세한 설명을 제공합니다. 그 후, 우리는 자연어 처리, 컴퓨터 비전, 그래프, 멀티모달 및 멀티 미디어, 포인트 클라우드/이벤트 스트림, 시계열 데이터 및 기타 도메인을 포함한 기존 SSM 및 그 다양한 응용 프로그램에 대한 리뷰를 조사한다. 또한 이러한 모델에 대한 통계적 비교 및 분석을 제공하고 독자가 다양한 작업에 대한 다양한 구조의 효과를 이해하는 데 도움이 되기를 바란다. 그런 다음 SSM의 이론적 모델의 개발과 적용을 더 잘 촉진하기 위해 이러한 방향으로 가능한 연구 지점을 제안한다. 더 많은 관련 작업은 다음 GitHub [https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List](https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List)에서 계속 업데이트됩니다.\n' +
      '\n' +
      ' 상태 공간 모델, 맘바, 트랜스포머, 선형 주의, 컴퓨터 비전, 자연어 처리\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인공지능은 2010년부터 시작된 제3의 물결 속에서 빠르게 발전하는데, 그 중에서도 연결주의 기반의 딥러닝 기술은 매우 중요한 역할을 한다. 딥러닝의 특이점은 이미지넷[2] 경쟁에서 최고의 성능(2위보다 훨씬 나은 결과)을 달성하는 AlexNet[1]의 제안으로 거슬러 올라갈 수 있다. 이후 VGG[3], ResNet[4], GoogleNet[5] 등 다양한 CNN(Convolutional Neural Networks)이 속속 제안되고 있다. 블록, 잔차 연결 및 시작의 아이디어는 많은 후속 심층 신경망의 설계를 고무시킨다[6, 7]. 한편, LSTM(Long Short-Term Memory) [8], GRU(Gated Recurrent Unit) [9]와 같은 RNN(Recurrent Neural Networks) 계열은 자연어 처리, 오디오 처리 등을 포함한 시퀀스 기반 학습 커뮤니티를 지배하고 있다. 그래프 신경망(GNN) [10, 11]은 그래프 데이터에 대한 심층 신경망의 응용을 더욱 확장하기 위해 제안된다. 그러나 이러한 주류 모델은 데이터 세트와 컴퓨팅 파워 지원이 최대일 때 여전히 병목 현상에 직면한다.\n' +
      '\n' +
      'CNN/RNN/GNN 모델에 의해 캡처된 로컬 관계만의 문제를 처리하기 위해, 2017년에 제안된 트랜스포머[13]는 장거리 특징 표현을 잘 학습한다. 핵심 연산은 입력 토큰을 질의, 키, 가치 자질로 변환하고, 유사도 행렬(질의와 키 자질 사이의 곱)과 가치 자질을 곱하여 장거리 자질을 출력하는 자기 주의 메커니즘이다. 트랜스포머 아키텍처는 먼저 BERT [15], ERNIE [16], BART [17], GPT [18]와 같은 _사전 훈련 및 미세 조정_ 패러다임 [14]의 도움으로 NLP 커뮤니티를 휩쓸었다. 그런 다음, 다른 커뮤니티도 그러한 네트워크, 예를 들어 컴퓨터 비전으로 출시된 ViT[19] 및 Swin-Transformer[20]로 부스팅된다. 또한 많은 연구자들은 트랜스포머와 다른 네트워크를 결합하거나 트랜스포머를 다중 모드 연구 문제에 적용하여 하이브리드 네트워크 아키텍처를 활용한다[21, 22]. 현재 단계에서는 대규모 기초모델이 등장하고 있으며, 매개변수 효율적인 미세조정(PEFT: Parameter-Efficient Fine-Tuning) 전략[23]도 크게 개발되었다. 그러나, 현재의 트랜스포머-기반 모델들은 트레이닝 및 테스트/배치를 위해 더 큰 메모리를 갖는 고급 그래픽 카드들을 여전히 필요로 하고, 이는 그들의 광범위한 적용을 크게 제한한다.\n' +
      '\n' +
      '컴퓨팅 비용을 더욱 감소시키기 위해, 장거리 의존성을 포착하고 높은 성능을 유지하면서, 많은 새로운 희소 주의 기반 모델들 또는 새로운 신경망 패러다임들이 제안된다[24, 25, 26, 27, 28]. 이 중 State Space Model(예를 들어, Mamba[12], S4[29], S4nd[30])은 Fig. 도 1을 참조하면, 주목의 중심이 된다. 그림 1의 왼쪽 부분에 나와 있습니다. 도 2에서 SSM 관련 논문의 발표량은 폭발적인 증가 추세를 보이고 있다. 상태 공간 모델(State Space Model, SSM)은 제어 이론, 계산 신경 과학 등의 분야에서 상태 변수를 사용하여 동적 시스템을 모델링하기 위해 처음 제안된 프레임워크이다. 딥 러닝에 이 개념을 적용할 때 일반적으로 선형 불변(또는 정지) 시스템을 참조한다. 원래 SSM은 컴퓨터가 처리할 수 있는 _recurrent_ 및 _convolutional_ 뷰에 대해 이산화할 수 있는 연속 동적 시스템입니다. SSM은 이미지/비디오 데이터, 텍스트 데이터, 구조화된 그래프 데이터, 이벤트 스트림/포인트 클라우드 데이터, 멀티모달/멀티미디어 데이터, 오디오 및 스피치, 시계열 데이터, 테이블 데이터 등을 포함하는 다양한 데이터 처리 및 특징 학습에 채택될 수 있다. 또한 SSMs 기반 확산 생성 모델[31, 32, 33]과 같은 효율적인 생성 모델을 구축하는 데 활용될 수 있다. 본 논문은 독자들이 SSM을 더 잘 이해하고 최신 연구 진행 상황과 다양한 응용 프로그램을 추적할 수 있도록 하기 위해 현장에 대한 체계적인 검토를 수행하고 다운스트림 태스크에서 SSM 모델의 성능을 실험적으로 검증한다. 이 검토가 SSM 분야의 발전을 더 잘 이끌고 촉진할 수 있기를 바란다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/blog/lbourdois/get-on-the-ssm-train](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)\n' +
      '\n' +
      '**이 검토의 구성** 이 문서에서는 먼저 섹션 2에서 상태 공간 모델의 작동 원리에 대한 사전 미리 보기를 제공합니다. 그런 다음 섹션 3에서는 SSM의 출처 및 변형, 자연어 처리, 컴퓨터 비전, 그래프, 다중 모드 및 다중 미디어, 포인트 클라우드/이벤트 스트림, 시계열 데이터 및 기타 도메인을 포함한 여러 측면에서 SSM의 관련 작업을 검토하는 데 중점을 둡니다. 이 조사에서 검토된 구조 및 주요 상태 공간 모델 관련 논문의 개요는 그림 3에 나와 있다. 더 중요한 것은 섹션 4에서 SSM의 유효성을 검증하기 위해 여러 다운스트림 작업에 대한 광범위한 실험을 수행한다. 다운스트림 작업은 단일/다중 레이블 분류, 시각적 객체 추적, 픽셀 수준 분할, 이미지 대 텍스트 생성 및 사람/차량 재식별을 포함한다. 또한 5절에서 SSM의 이론과 적용에 대한 몇 가지 가능한 연구 방향을 제안한다. 마지막으로 6절에서 이 논문에 대한 결론을 내린다.\n' +
      '\n' +
      '## 2 SSM의 공식화\n' +
      '\n' +
      '상태 공간 모델(SSM)은 그림에 예시된 바와 같이 고전적인 칼만 필터[35]로부터 유래한다. 1은 1차원 입력신호 \\(\\mathbf{U}(t)\\)를 취하여 N차원 잠재상태 \\(\\mathbf{X}(t)\\)로 맵핑한 후, 1차원 출력신호 \\(\\mathbf{y}(t)\\)로 투사한다. 일반적인 컴퓨팅 절차는 식에서 정의할 수 있다. 1:\n' +
      '\n' +
      '\\[\\begin{split}\\dot{\\mathbf{X}}(t)&=\\mathbf{A}(t) \\mathbf{X}(t)+\\mathbf{B}(t)\\mathbf{U}(t)\\\\mathbff{y}(t)&=\\mathbf{C}(t)\\mathbff{X}(t)+\\mathbf{D}(t)\\mathbff{U}(t)\\end{split} \\tag{1}\\]\n' +
      '\n' +
      '여기서, \\(\\mathbf{X}(t)\\in\\mathbb{R}^{n}\\), \\(\\mathbf{y}(t)\\in\\mathbb{R}^{q}\\), \\(\\mathbf{U}(t)\\in\\mathbb{R}^{p}\\)는 _상태 벡터_, _출력 벡터_ 및 _입력(또는 제어) 벡터_를 나타낸다. \\ (\\dot{\\mathbf{X}}(t)=\\frac{d}{dt}\\mathbf{X}(t)\\). \\ (\\mathbf{A}(t)\\in\\mathbb{R}^{n\\times p}\\), \\(\\mathbf{B}(t)\\in\\mathbb{R}^{n\\times p}\\), \\(\\mathbf{C}(t)\\in\\mathbb{R}^{q\\times n}\\), \\(\\mathbf{D}(t)\\in\\mathbb{R}^{q\\times p}\\)는 상태 행렬, 입력 행렬, 출력 행렬, 피드포워드 행렬을 나타낸다. 시스템 모델에 직접 피드스루가 없을 때, \\(\\mathbf{D}(t)\\)는 제로 행렬이므로, 우리는 다음과 같은 단순화된 방정식을 얻는다:\n' +
      '\n' +
      '\\[\\begin{split}\\dot{\\mathbf{X}}(t)&=\\mathbf{A}(t) \\mathbf{X}(t)+\\mathbf{B}(t)\\mathbf{U}(t)\\\\mathbf{y}(t)&=\\mathbf{C}(t)\\mathbf{X}(t).\\end{split} \\tag{2}\\]\n' +
      '\n' +
      '원시 시스템이 연속적이기 때문에 그림 2와 같이 컴퓨터에 공급하기 전에 먼저 이산화해야 한다. 맘바 아키텍처의 경우 이산화에는 영차 홀드(ZOH) 2가 채택되며 다음과 같다.\n' +
      '\n' +
      '각주 2: [https://en.wikipedia.org/wiki/Zero-order_hold](https://en.wikipedia.org/wiki/Zero-order_hold)\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{X}_{t}&=\\overline{\\mathbf{A}} \\mathbf{X}_{t-1}+\\overline{\\mathbf{B}}\\mathbf{U}_{t}\\\\\\mathbf{y}_{t}&=\\mathbf{C}\\mathbf{X}_{t}\\end{split} \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\overline{\\mathbf{A}}=exp(\\Delta\\mathbf{A})\\), \\(\\overline{\\mathbf{B}}=(\\Delta\\mathbf{A})^{-1}(exp(\\Delta\\mathbf{A})-\\mathbf{ I})\\cdot\\Delta\\mathbf{B}\\), \\(\\Delta\\)는 스텝 크기를 나타낸다. 우리가 \\(\\mathbf{h}\\)와 \\(\\mathbf{x}\\)를 사용하여 _상태 벡터_와 _입력 벡터_를 나타내면 다음과 같은 함수를 얻는다.\n' +
      '\n' +
      '도. 1: [좌회색 서브-도형] 선형 상태-공간 방정식들의 블록도 표현(상태-공간 표현에 기초하여 다시 그려짐); [우회색 서브-도형] 널리 사용되는 맘바 아키텍처의 공식([12]로부터 다시 그려짐).\n' +
      '\n' +
      '그림 3: 본 조사에서 검토된 구조 및 주요 상태 공간 모델 논문이다.\n' +
      '\n' +
      '그림 2: [왼쪽 하위 그림] 현재까지 발표된 논문 수(2021년부터 2024.04년); [오른쪽 하위 그림] SSM의 세 가지 다른 표현, 즉 연속 시간, 반복 또는 콘볼루션 모델을 보고 계산할 수 있다. 이 도면은 [34]를 기준으로 다시 그려진다.\n' +
      '\n' +
      'RNN(Recurrent Neural Network) 모델의 계산 절차와 유사하다. 5(b):\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{h}_{t}&=\\mathbf{\\overline{A}} \\mathbf{h}_{t-1}+\\mathbf{\\overline{B}}\\mathbf{x}_{t}\\\\mathbf{y}_{t}&=\\mathbf{C}\\mathbf{h}_{t}.\\end{split} \\tag{4}\\]\n' +
      '\n' +
      '그러나 RNN 모델과 유사하게 계산이 _병렬화_될 수 없다는 딜레마에 직면한다. 위의 공식을 간단히 확장하면 다음과 같다.\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{y}_{0}&=\\mathbf{C}\\mathbf{ \\bar{A}}^{0}\\mathbf{x}_{0}\\mathbf{C}\\mathbf{\\bar{A}}^{1}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{C}\\mathbf{\\bar{A}}_{1}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{C}\\mathbf{\\bar{A}}_{1}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{C}\\mathbf{\\bar{A}}_{1}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf{\\bar{B}}\\mathbf\n' +
      '\n' +
      '마지막 항과 끝에서 두 번째 항의 승수는 항상 \\(\\mathbf{C}\\mathbf{\\bar{A}}^{0}\\mathbf{\\bar{B}}\\)와 \\(\\mathbf{C}\\mathbf{\\bar{A}}^{1}\\mathbf{\\bar{B}}\\)임을 쉽게 알 수 있다. 따라서 이 곱셈기들을 합성곱 커널 \\(\\mathbf{\\overline{K}}=\\mathbf{C}\\mathbf{\\bar{B}}\\cdot(\\mathbf{\\bar{A}}^{0}, \\mathbf{\\bar{A}}^{1},\\mathbf{\\bar{A}}^{2},...,\\mathbf{\\bar{A}}^{L})\\로 처리할 수 있다. 여기서 \\(L\\)는 주어진 입력 시퀀스의 길이이다. 우리는 Equ를 다시 쓸 수 있다. (4) 하기의 컨볼루션 제형으로서:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{\\overline{K}}&=(\\mathbf{C} \\mathbf{\\overline{B}},\\mathbf{C}\\mathbf{\\overline{A}}\\mathbf{\\overline{B}},...,\\mathbf{C}\\mathbf{\\bar{A}}^{L}\\mathbf{\\overline{B}},...)\\\\\\mathbf{y}&=\\mathbf{x}*\\mathbf{\\overline{K}}. \\end{split} \\tag{6}\\\n' +
      '\n' +
      '이 때, 우리는 훈련의 병렬성을 실현할 수 있고 추론의 선형 복잡성의 반복 형태에 적합한 완전한 SSM 모델을 얻는다. 트랜스포머 구조에서는 문맥 정보를 유사도 행렬에 저장하지만, SSM은 유사한 모듈을 가지고 있지 않아 문맥 학습에서 성능이 떨어진다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 Gu 등은 다음 두 가지 측면에서 SSM을 개선하는 Mamba [12] 아키텍처를 제안한다: _1. 선택적 스캔 연산자_를 사용하면 모델이 관련 정보를 필터링할 수 있습니다. 실제 구현에서는 \\(\\Delta\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\)이 입력의 함수가 되는 반면 행렬 \\(\\mathbf{A}\\)은 변하지 않는다. _2. 병렬 스캐닝, 커널 융합 및 재계산을 통해 (중간) 결과의 효율적인 저장을 허용하는 하드웨어 인식 알고리즘_. Mamba 블록의 아키텍처에 대한 예시가 그림 1의 오른쪽 부분에 제공된다. 주요 특징으로 인해 많은 연구자들이 SSM 또는 Mamba 아키텍처를 사용하여 모델을 설계하려고 시도한다.\n' +
      '\n' +
      '## 3 상태 공간 모델\n' +
      '\n' +
      '이 섹션에서는 SSM 아키텍처 및 애플리케이션에 대한 관련 작업을 검토하는 데 중점을 둔다. 우리는 관련 작업을 SSM의 출처와 변화, 자연어 처리, 컴퓨터 비전, 그래프, 멀티모달 및 멀티 미디어, 포인트 클라우드/이벤트 스트림, 시계열 데이터 등 다음 영역으로 나눈다. 다음 하위 섹션에서는 이러한 알고리즘을 차례로 소개할 것이다.\n' +
      '\n' +
      '### _SM_ 의 원본 및 변동\n' +
      '\n' +
      '상태 공간 모델은 주로 선형 필터링 및 예측 방법을 도입하는 칼만 필터링[35]에서 유래한다. 칼만 필터링은 두 단계, 즉 예측 및 보정 단계로 나눌 수 있다. 예측은 이전 시간의 상태를 기초로 현재 상태를 추정하는 것이고, 보정은 추정된 현재 시간의 상태와 관측된 상태를 통합하여 최적의 상태를 추정하는 것이다. 상태 공간 모델은 시스템의 내부 상태의 진화를 나타내기 위한 1차 미분 방정식(연속 시간 시스템) 또는 차분 방정식(이산 시간 시스템)의 집합과 상태와 시스템의 출력 사이의 관계를 설명하기 위한 또 다른 방정식 집합을 사용하여 동적 시스템의 거동을 설명하는 수학적 모델이다. 이러한 방정식은 다변수 시스템을 다루기 위해 행렬과 벡터 형태로 표현될 수 있다. 이어서, Gu et al. [34]는 모델 전력 및 계산 효율의 단점을 해결하면서 순환 신경망(RNN), 시간 컨볼루션 신경망 및 신경 미분 방정식(NDE)의 장점을 결합한 선형 상태 공간 계층(Linear State Space Layer; LSSL)을 소개한다. 이 새로운 시퀀스 모델은 제어 시스템에서 영감을 받아 선형 상태 공간 레이어(LSSL)를 통해 구현된다.\n' +
      '\n' +
      'RNN과 유사하게, SSM은 또한 더 긴 시퀀스를 모델링할 때 소실/폭발 구배 문제를 겪는다. 이 문제를 해결하기 위해 HiPPO [36] 모델은 Recurrent Memory와 Optimal Polynomial Projection의 개념을 결합하여 재귀 메모리의 성능을 크게 향상시킬 수 있다. 이 메커니즘은 SSM이 긴 시퀀스와 장기 종속성을 처리하는 데 매우 유용하다. 수식은 다음과 같이 나타낼 수 있다:\n' +
      '\n' +
      '\\[A_{nk}=\\begin{cases}(2n+1)^{1/2}(2k+1)1/2&\\text{if }n>k\\\\ n+1&\\text{if }n=k\\\\ 0&\\text{if }n<k\\end{cases} \\tag{7}\\]\n' +
      '\n' +
      '도. 4: 대표적인 SSMs 기반 알고리즘의 타임라인(2020년부터 2024.04년까지)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}} \\hline \\hline\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:8]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'RetNet[127], Mega[128], H3[129]. 구체적으로, Acceptance Weighted Key Value (short for RWKV)[125]는 RNN 아키텍처의 일종으로 자연어 처리를 위해 attention-free Transformer[130]를 기반으로 개발되었다. 변압기의 효율적인 병렬화 훈련과 RNN의 효율적인 추론에 동시에 특징이 있다. Duan 등은 이 프레임워크를 컴퓨터 비전 작업에 추가로 적용하고 Vision-RWKV (VRWKV) [126] 모델을 제안한다. 그들의 결과는 이미지 분류 작업에서 ViT를 능가하고 속도 및 메모리 사용(고해상도 입력 처리 시)에서 상당한 이점을 가지고 있음을 보여준다. RWKV 아키텍처는 또한 시계열 관련 태스크[131], 온라인 액션 검출[132], 확산 모델[133]과 같은 많은 다른 태스크들에서 널리 사용된다. RetNet[127]은 Retentive Network의 줄임말로, 훈련 병렬성, 저비용 추론 및 고성능을 동시에 달성하는 대규모 언어 모델을 구축하는 것을 목표로 한다. 병렬, 반복 및 청크 단위의 반복 계산 패러다임을 지원합니다.\n' +
      '\n' +
      '위에서 언급한 SSM의 기원과 변형을 바탕으로 자연어 처리, 컴퓨터 비전 등을 포함하지만 이에 국한되지 않는 많은 SSM 기반 작품들이 끊임없이 등장하고 있다. 다음의 요약은 다양한 분야의 확장과 적용을 각각 소개할 것이다.\n' +
      '\n' +
      '### _자연어 처리_\n' +
      '\n' +
      '최근, 대규모 언어 모델의 개발은 자연어 처리 분야에 혁명을 일으켰지만, 널리 사용되는 트랜스포머 아키텍처는 높은 계산 및 메모리 요구 사항에 의해 제한된다. 이러한 문제를 해결하기 위해 많은 연구자들은 효율적인 계산과 제한된 메모리 요구 사항을 달성하기 위해 트랜스포머를 단순화하는 데 전념했다. 그 중 State Space Model은 본 논문에서 검토한 가장 효과적인 해결책 중 하나이다.\n' +
      '\n' +
      '맘바[12]의 등장으로 SSM 모델은 현재 연구자들의 관심과 호평을 점점 더 많이 받고 있다. 다음 작업은 현재 언어 모델링 작업 [41][45][46][47][48], 심층 노이즈 억제 작업 [42], 임상 노트 이해 작업 [49]에서 탐색된다. 구체적으로 언어 모델링 작업을 위해 [41]은 주로 장거리 언어 모델링 방향으로 게이트 상태 공간의 적용을 연구하고 GSS(Gated State Space)라는 새로운 방법을 소개한다. 긴 시퀀스 모델링에 사용할 수 있으며 참가자 수를 효과적으로 줄일 수 있습니다. 그들의 실험은 그것이 DSS[38]보다 2-3배 더 빨리 달성한다는 것을 보여준다. Grazzi et al. [45]는 문맥 학습 태스크에서 단순 함수 추정 및 자연어 처리에 Mamba를 활용하고 전체 성능이 실제로 S4 버전보다 우수하고 다른 트랜스포머 네트워크와 유사하다는 것을 검증한다. S4++ [46]은 S4 아키텍처의 두 가지 이슈, 즉 NSS(non-stationary state)와 의존 바이어스(dependency bias)를 발견하고, 다중 상태 정보를 현재 상태로 통합하기 위한 SMR(State Memory Reply) 메커니즘을 제안한다. 또한 상호 작용 교차 주의 메커니즘을 통해 복잡한 종속성 편향을 통합하고 광범위한 실험 결과를 통해 S4++가 다중 시퀀스 모델링 작업에서 S4보다 우수한 성능을 보여 상당한 성능 향상을 보여준다. [47] 합성 State Space Model and local attention mechanism to reduce memory consumption and\n' +
      '\n' +
      '도. 5: CNN, RNN, Transformer, Mamba, Linear Attention의 비교.\n' +
      '\n' +
      '성능을 보장하면서 훈련 효율을 높입니다. 저자들은 지역 주의력을 사용하여 지역 정보를 추출한 다음 상태 공간 모델을 사용하여 지역 주의력에서 누락된 전역 정보를 추출한다. [48] 기존 상태 공간 모델은 효율적이지만 성능이 부족하다고 주장합니다. 저자들은 이러한 이유가 너무 많은 상태 전환이 모델을 얕은 정보를 잃게 만들기 때문이라고 믿는다. 따라서 저자들은 더 얕은 정보를 유지하기 위해 이전 계층의 숨겨진 상태를 후속 계층에 통합하는 설계를 제안한다. 결국, Pile에 대한 사전 훈련 후, Zero-shot과 다른 데이터셋에 대한 4-shot의 실험 결과가 크게 개선되었다. 음성 태스크의 경우, Du et al. [42]는 임펄스 신경망의 높은 효율 및 장거리 모델링 능력을 상태-공간 모델 S4와 결합하여 스파이킹 신경망을 획득하는데, 스파이킹 신경망은 적은 수의 파라미터를 갖지만 심층 잡음 억제 태스크에서 일부 인공 신경망(ANN)과 유사한 성능을 갖는다. 또한, 음성 분리 작업을 위해, Jiang 등이 제안한 DPMamba[43]은 전통적인 변압기 구조를 대체하기 위해 선택적 상태 공간 모델 Mamba를 사용한다. DPMamba는 선택적 상태 공간을 통해 음성 신호의 단기 및 장기 순방향 및 역방향 종속성을 동시에 모델링하여 이중 경로 트랜스포머 모델 Sepformer[134]와 유사한 결과를 달성한다. Li 등이 제안한 SPAMba[44]는 TF-GridNet[135]를 기본 프레임워크로 사용하고 Transformer 모듈을 양방향 Mamba 모듈로 대체하여 보다 넓은 범위의 언어 정보를 캡처한다. 실험 결과는 Mamba 기반 모델이 성능에 중요한 역할을 한다는 것을 보여준다.\n' +
      '\n' +
      '임상 노트 이해 태스크에서 Yang et al. [49]는 Mamba의 선형 계산 복잡도를 활용하여 최대 16k의 시퀀스 길이를 갖는 매우 긴 임상 노트 시퀀스를 모델링한다. 저자들은 MIMIC-III 데이터 세트를 사용하여 Mamba 모델을 사전 훈련시킨 다음 코호트 선택 작업 및 ICD 코딩 작업에서 테스트하고, 특히 Mamba 및 임상 라마 모델 둘 다와 비교할 때 더 긴 텍스트 길이에서 임상 언어를 모델링하는 데 우수한 성능을 보여준다. Mamba 및 임상 라마 모델에 비해 임상 언어 모델링, 특히 더 긴 텍스트 길이에서 우수한 성능을 보여준다. 번역 작업에서 [50]은 춤 안무를 생성하는 문제를 번역 작업으로 공식화하고, 오디오 시퀀스를 대응하는 춤 포즈로 번역하는 방법을 학습하기 위해 기존 데이터 세트를 활용하는 MDLT를 제안한다.\n' +
      '\n' +
      '### _Computer Vision_\n' +
      '\n' +
      '최근 State Space Model의 선형 시계열 모델링이 널리 주목받고 있으며, 자연어 처리 분야에서 강력한 성능을 보여주고 있다. 이러한 진행에 영감을 받아 많은 SSM 기반 비전 모델이 제안되었는데, 분류 태스크[60, 61, 68, 61, 68, 69, 76, 83, 92, 93, 94, 95, 96, 97, 98, 99, 101, 112, 137, 138], 검출 태스크[109, 110, 117], 분할 태스크[70, 82, 85, 89, 91, 106, 108, 111], 의료 태스크[63, 64, 69, 72, 87, 97, 98, 100], 복원 태스크[77, 110, 56], 생성 태스크[31, 32, 33], 비디오 이해[56, 58, 90], 트랙 태스크[88], 및 기타 태스크[32, 59, 62, 73, 74, 75, 103, 104, 105, 107, 116, 118, 120, 121]가 있다.\n' +
      '\n' +
      '분류 작업에서 S4nd[30]은 다차원 데이터 연속 신호의 모델링 능력을 확장하기 위해 다차원 및 다극 그래픽 구성 요소를 제안하며, 이는 대규모 시각 데이터를 동적 다차원 선형 신호로 모델링할 수 있다. VMamba[60]는 선형 복잡도를 사용하여 전체 범위의 감각 필드를 캡처하고, 스캔 블록에 걸쳐 공간 정보의 횡단을 도입하며, 인과 관계가 아닌 시각적 이미지를 정렬된 패치 시퀀스로 변환한다. Vim[61]은 양방향 상태 공간 모델을 사용하여 시각적 표현 정보를 압축하고 위치 임베딩 및 시각적 정보를 통해 글로벌 컨텍스트를 이해한다. Li 등은 행-주 순서로 차원에 걸쳐 입력 데이터를 처리함으로써 임의의 다차원 데이터를 처리하도록 설계된 Mamba의 확장인 Mamba-ND[68]을 제시한다. [57]의 저자들은 S5와 S4의 관계를 확립하고, 다중 입력 다중 출력 SSM을 활용하며, 장거리 시퀀스 모델링을 위해 병렬 스캐닝의 상태 공간 레이어를 사용하기 위해 S4를 기반으로 S5를 설계한다. Baron et al. [76]은 새로운 2차원 State Space Layer for Spatial Inductive Bias를 설계한다. 이 계층의 핵심 목표는 2차원 위치 인식, 동적 공간 위치 인식, 병진 및 정렬 불변성을 달성하는 것이다. Chen et al. [79]는 잔차를 원래 VMamba에 통합하고, 식품 분류를 위해 원래 VMamba의 고유한 전역 및 로컬 상태 특성을 유지하는 최초의 것이다. Yang et al. [101] propose PlainMamba, which further adapts Mamba의 selective scanning process\n' +
      '\n' +
      '도. 도 6: 잠바 블록[136]의 일러스트레이션 및 상이한 유형의 층들을 사용하였다.\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 영상표시장치. 연속적인 2D 스캐닝 프로세스를 통해 공간적 연속성을 향상시키고 방향 인식을 업데이트함으로써, 모델은 방향 정보를 인코딩함으로써 라벨들의 공간적 관계를 구별할 수 있고, 이에 의해 2D 이미지들로부터 특징들을 학습하는 능력을 강화한다. Wang 등은 Hybrid State Space Module(Mix-SSM)에 State Space Model, Convolutional Neural Network(CNN), Multi-head Self-attention Mechanism(MSA), Multi-layer Perceptrons(MLPs)를 통합하여 곤충 분류 작업에 사용될 수 있고 모델의 분류 능력을 향상시킬 수 있는 InsectMamba[112]를 제안한다.\n' +
      '\n' +
      'Huang 등은 LocalMamba[92]를 소개하며, 이는 공간 토큰들의 2차원 의존성을 보존하기 위한 새로운 로컬 스캐닝 전략을 제안한다. 다양한 작업에 대해 광범위한 실험을 수행하고 LocalMamba가 ImageNet 분류에서 Viim-T보다 +3.1% 향상되었음을 보여준다. Xu et al. [93]은 제스처 합성에 초점을 맞추고 길고 다양한 시퀀스를 지원하는 MambaTalk라는 SSM 모델을 소개한다. Pei et al. [94]는 추가적인 컨벌루션 분기를 통합하여 EfficientVAMba를 제안하고 ImageNet-1K 및 COCO 탐지 데이터 세트에서 기준선을 크게 개선한다. Du et al. [95]는 VAMba의 견고성을 다양한 측면에서 탐색하는데, 예를 들어, 그들은 전체 이미지 및 패치-특정 방법들 모두를 사용하여 적대적 공격들에 대한 복원력을 조사하여 트랜스포머 아키텍처들에 비해 우수한 견고성을 나타내지만 확장성 약점을 갖는다. 그들은 또한 다양한 시나리오에 걸쳐 VAM바의 일반적인 견고성을 평가한다. Shi et al. [96]은 압축 이미지 복원 작업에 상태-공간 모델링의 선형 복잡도를 도입함으로써 전통적인 방법들의 단점들 중 일부를 극복하는 새로운 이미지 복원 방법인 VambaIR을 제안한다. Fang et al. [83]은 선택적 구조 상태 공간 모델(Mamba)과 다중 인스턴스 학습(MIL) 접근법을 결합한 첫 번째 작업인 전체 슬라이드 이미지의 분류를 다루기 위한 MamMI 프레임워크를 제시한다. MamMIL은 분류 성능 및 메모리 사용량 측면에서 Transformer 기반의 기존 최신 MIL 프레임워크를 능가한다. Li 등은 실시간 모바일 애플리케이션들의 전형적인 계산 자원 제약들을 다루기 위해 설계된 경량 선택 상태 공간 모델(SSM)을 이용하는 HARMamba[137]이라고 불리는 웨어러블 센서 인간 활동 인식(HAR)에 대한 새로운 접근법을 소개한다. Yang et al.은 HSIMamba[138]이라는 새로운 초분광 영상 분류 프레임워크를 소개하며, 이는 원격탐사에서의 초분광 영상 자료의 복잡성과 고차원적 특성을 다루는 것을 목표로 한다. 제안된 프레임워크는 공간 분석을 위한 특수 블록과 함께 스펙트럼 특징을 효율적으로 추출하기 위해 양방향 역전 CNN을 통합한다.\n' +
      '\n' +
      '탐지 작업을 위해 Chen et al. [80]은 적외선 소형 표적을 탐지하기 위한 Mamba-in-Mamba(MiM-ISTD) 구조를 제안한다. 이러한 구조에서, 이미지들은 "시각적 문장"(패치)으로 균등하게 분할되고, "시각적 단어"(서브 패치)로 더 세분화되고, 순수한 맘바 기반 MiM 피라미드 인코더는 글로벌 및 로컬 특징들을 추출하도록 설계된다. Chen et al. [109] explore the potential of the Mamba architecture for remote sensing image change detection tasks by employing visual Mamba as the encoder is capable to fully learning the global context\n' +
      '\n' +
      '도. 7: VAMba 모델(V Mamba-T)[61] 및 Vision Mamba(Vim)[60]의 개요.\n' +
      '\n' +
      '상기 입력 영상의 정보인 것을 특징으로 하는 영상 처리 방법. Mamba 구조의 특성과 장점을 최대한 활용하여 시공간 관계를 모델링하는 세 가지 방법을 디코더에 제안한다. He et al. [117]은 병렬 캐스케이드 하이브리드 상태 공간과 다중 커널 컨볼루션 연산을 통해 원거리 및 지역 정보를 효과적으로 포착한다.\n' +
      '\n' +
      '세그멘테이션 작업을 위해 Visual manb 기반의 UNet 구조와 기존의 UNet을 결합한 Semi-Mamba-UNet [70]이라는 준지도 의료 영상 분할 방법을 제안한다. 이중 네트워크를 활용하여 의사 레이블을 생성하고 상호 교차 감독합니다. 또한 자체 감독 픽셀 수준의 대비 학습 전략을 사용하여 피쳐 학습 기능을 강화합니다. P-Mamba [71]은 소아 심초음파에서 좌심실을 정확하게 분할하는 것과 관련된 문제를 해결하는 효율적인 소아 심초음파 좌심실 분할을 위해 설계되었다. Liao [89]는 Mamba와 UNet을 병합하여 의료 영상 분할에서 계산상의 제약을 해결하는 간소화된 프레임워크인 LightM-UNet을 소개한다. 심오한 의미 특징을 추출하고 선형 계산 복잡도로 광범위한 공간 의존성을 포착합니다. 실제 데이터 세트에 대한 경험적 평가는 현재 선도적인 방법에 비해 LightM-UNet의 우월성을 강조하여 매개변수 수와 계산 오버헤드의 상당한 감소를 보여준다. Zhang et al. [91]은 SSM 모델의 기능을 완전히 활용하는 SSM 기반 U-Net 변종 의료 이미지 분할 모델, VM-UNetV2를 제안한다. VAMba 사전 훈련된 가중치를 사용하여 인코더를 초기화하고 심층 감독 메커니즘을 채택함으로써 VM-UNetV2는 여러 데이터 세트에 대한 경쟁적 분할 성능을 보여준다. U-mamba[63]는 범용 CNN-SSM 네트워크로서, SSM들의 장거리 의존성과 로컬 CNN 특징들을 통합함으로써 생물의학적 이미지 분할을 향상시킨다. 이미지넷 기반 사전 훈련, 새로운 맘바 기반 모델인 Swin-umamba[64]를 활용하면 CNN, ViT, 기존 맘바 모델보다 성능이 우수하다. 메모리 및 연산 부담이 적은 우수한 성능을 보여주며, 맘바 패밀리 모델의 성능 촉진에 있어 ImageNet 기반 사전 훈련의 필수적인 역할을 드러낸다.\n' +
      '\n' +
      'VM-UNet[65]는 의료 영상 분할을 위한 첫 번째 순수 SSM 기반 모델로서 기준선을 설정한다. ISIC17, ISIC18 및 Synapse 데이터 세트에서 효과적으로 경쟁하여 향후 SSM 기반 분할 시스템에 대한 통찰력을 제공합니다. Gong et al. [66]은 CNN의 세부 특징 추출과 SSM의 광범위한 종속성 모델링을 결합하여 3D 의료 영상 작업에 탁월한 nnMamba를 제안한다. 복셀의 장거리 관계를 모델링하기 위해 Mamba-In-Convolution with Channel-Spatial Siamese learning (MICCSS) 블록을 제안한다. 6개의 데이터 세트에서 3D 분할, 분류 및 랜드마크 검출에서 우수한 성능을 얻었다. LMa-UNet[85]는 CNN 및 트랜스포머에 비해 향상된 공간 모델링을 위해 대형 윈도우를 활용하여 선형 복잡도로 효율성을 유지하는 새로운 대형 윈도우 기반 Mamba U자형 네트워크이다. 글로벌 및 로컬 공간 모델링을 강화하기 위해 계층적 및 양방향 Mamba 블록을 도입한다. Tang et al. [82]는 공간 및 채널 차원에서의 특징들을 융합하기 위해 삼중 상태 공간을 사용하고, 조밀한 컨텍스트 특징들을 추출하기 위해 잔차 블록들을 사용한다. Kazi et al. [102]는 Mamba-UNet 및 HUNet(Hierarchical Upsampling Network)의 더 가벼운 버전을 이용하며, 컨볼루션 신경망의 국부적 특징 추출 능력은 State Space Model의 원격 종속성 모델링 능력과 결합된다. RS3Mamba [108]은 RS3Mamba라는 새로운 두 가지 네트워크(two-branch network)를 제안하며, 이 네트워크는 원격 감지 이미지의 의미론적 분할 작업에 새로운 시각 상태 공간(visual state space, VSS) 모델인 Mamba를 도입한다. RS3Mamba는 VSS 블록들을 이용하여 보조 분기를 구성하여 주 분기에 대한 추가적인 전역 정보를 제공하고, 이중 인코더로부터 특징을 증강 및 융합하기 위한 공동 완성 모듈(CCM)을 도입한다. Hao et al. [106]은 공유 위치 코딩과 주파수 기반 특징을 융합하여 주파수 도메인에서 공간 위치 보존 및 특징 향상을 향상시키는 T-Mamba라고 하는 치아에 대한 3D CBCT 분할 방법을 소개한다. 시각적 맘바 건축에 주파수 특징을 도입한 최초의 작품은 T-Mamba이다. Zhu et al. [113]은 Mamba 아키텍처를 기반으로 하는 새로운 시맨틱 세분화 프레임워크 Samba를 제안하고 고해상도 원격탐사 영상을 위해 특별히 설계한다. 삼바는 현재 최첨단 CNN 및 ViT 기반 방법을 능가하는 원격 감지 이미지의 시맨틱 세분화에서 맘바 아키텍처의 효과와 잠재력을 보여준다. Ma et al. [108]은 RS3Mamba라는 새로운 듀얼 브랜치 네트워크를 제안하는데, 이는 VSS 블록들을 활용하여 보조 브랜치들을 구성하고, 컨볼루션 기반 메인 브랜치들에 대한 추가적인 글로벌 정보를 제공한다. 또한, 두 가지 가지 사이의 특징 차이를 고려하여 협업 완성 모듈(Collaborative Completion Module, CCM)을 도입하여 듀얼 인코더의 특징을 강화 및 융합한다. Archit et al. [119]는 새로운 의료 영상 분할 네트워크 아키텍처인 ViM-UNet을 제안한다. 최신 Vision Mamba 아키텍처를 기반으로 하며 기존의 UNet 및 트랜스포머 기반 UNETR과 비교합니다.\n' +
      '\n' +
      '의료 영상 기반 분석을 위해 Guo et al. [97]은 Mamba Morph라는 Mamba 프레임워크를 기반으로 하는 의료용 MR-CT 변형 정합 방법을 소개한다. 이 방법의 핵심은 의료 영상 분석에 중요한 다양한 영상 양식에 걸쳐 복셀 수준의 공간 대응 캡처를 달성하는 데 있다. Xie 등은 Vision Mamba 아키텍처를 기반으로 한 새로운 폴립 분할 모델 ProMamba[99]를 소개하고 프롬프트 기술을 소개한다. 비전 맘바를 도입하고 용종 분할에 프롬프트하는 것은 이번이 처음이다. Wu et al. [100]은 SSM 및 SS2D 기반의 의료 영상 분할을 위한 새로운 신경망을 소개하는데, H-vmunet(High-order Vision Mamba UNet)이라 불리며, 이는 진보된 상호 작용을 통해 중복 정보의 도입을 점진적으로 감소시키고, 각각의 상호 작용 단계에서 로컬 특징을 학습하는 SS2D의 능력을 향상시킨다. Vivim은 Yang et al. [98]에 의해 제안되었으며, 이는 의료 비디오 객체 분할을 위해 설계된 시간 Mamba 블록을 통해 장기 시공간 표현을 서로 다른 스케일의 시퀀스로 효과적으로 압축하는 것을 목표로 한다. 대형 라벨링된 의료 데이터 세트의 부족으로 인해 Wang et al. [72]는 약 감독 방식으로 Mamba 기반 UNet을 훈련함으로써 이러한 문제를 해결하려고 시도하는 Weak-Mamba-Unet 아키텍처를 제안한다. CNN(Convolutional Neural Network), ViT( Vision Transformers), Vmamba를 활용하여 데이터 레이블을 예측한 후 밀집된 의사 레이블을 생성한다.\n' +
      '\n' +
      'Zheng et al. [69]는 내시경 영상에서 노출 이상을 보정하도록 설계된 FD-Vision Mamba(FDVM-Net)라는 새로운 네트워크 아키텍처를 설명한다. 이는 의료 전문가의 의사 결정 과정을 지원하기 위해 이미지 품질을 유지하는 데 중요하다. FDVM-Net은 주파수 영역에서 동작하고 내시경 영상의 주파수 표현을 재구성하여 노출을 개선한다. Xing et al. [62]는 3D 특징 모델링을 향상시키기 위해 SegmamBA 모듈을 제안하며, 이는 원거리 의존성을 다루기 위해 공간 차원에서의 특징 표현을 향상시키기 위해 내부적으로 게이티드 공간 컨벌루션을 사용한다. Yue et al. [81]은 의료 영상 분류를 위해 State Space Model과 Convolutional Layer를 기반으로 MedMamba를 제안한다. 다양한 양식의 의료 영상에 적합한 국부적 특징 추출 능력을 유지하면서 원거리 의존성을 효과적으로 포착할 수 있다. Huang et al. [78]은 원래의 Mamba 선형 복잡도 및 전역 수용 필드의 장점을 계승하고, Mamba를 MambaMIR-GAN이라고 하는 이미지 재구성 태스크에 적응시키기 위한 임의의 마스크 메커니즘을 제안한다. Schiff et al. [87]은 Mamba 블록을 양방향 BiMamba를 지원하는 컴포넌트 및 RC equivariant를 지원하는 MambaDNA로 확장한다. 그런 다음 MambaDNA를 카두쿠쿠스의 기반으로 사용하고 DNA 서열 모델링을 위한 사전 훈련 및 미세 조정 전략을 통합한다.\n' +
      '\n' +
      '복원 작업에 대해 Guo et al. [77]은 저수준 비전에서 Mamba의 잠재력을 탐구하는 것을 목표로 하는 MambaIR이라고 하는 새로운 이미지 복원 모델을 제안하며, 모델은 지역 블록 반복 및 채널 상호 작용과 같은 이미지 복원 작업에 고유한 사전 지식을 결합하면서 Mamba 상태 공간 모델의 장거리 종속 모델링 능력을 활용한다. Serpent[110]은 상태 공간 모델을 사용하여 컴퓨팅 자원 및 GPU 메모리의 비용을 상당히 감소시키는 입력 크기의 선형 스케일링으로 전역 수용 필드를 유지한다.\n' +
      '\n' +
      '생성 작업을 위해 ZigMa[32]는 기존 확산 모델의 확장성과 2차 복잡성 문제를 해결하는 것을 목표로 하는 ZigMa라는 Mamba 구조를 기반으로 하는 새로운 확산 모델을 소개한다. DiffuSSM[31]은 확장 가능한 상태 공간 모델로서, 더 높은 해상도를 처리하고 전역 압축을 사용하지 않기 때문에 확산 과정 전반에 걸쳐 상세한 이미지 표현을 유지할 수 있다. DiS[33]은 상태 공간 아키텍처를 기반으로 하는 확산 모델의 새로운 범주이다. 이미지 데이터에 대한 확산 모델을 훈련하는 것을 목표로 하며, 기존의 U-Net과 같은 백본을 원시 패치 또는 잠재 공간에서 작동하는 상태 공간 백본으로 대체한다.\n' +
      '\n' +
      '비디오 이해를 위해, ViS4mer[56]는 장기 추론을 위해 다중 스케일 시간 구조화된 상태-공간 시퀀스 디코더를 이용한다. 각 디코더 레이어의 시공간 특징의 해상도와 채널 차원이 점차 줄어들어 복잡한 장거리 시공간 의존성을 학습할 수 있다. Wang 등은 짧은 마스크 대비와 긴 마스크 대비의 학습 방법으로 장거리 시공간 정보를 예측할 수 있는 LSMCL [58]을 제안한다. Chen et al. [90]은 Mamba가 비디오 이해에서 트랜스포머의 대안으로서 잠재력을 평가하고, Mamba가 비디오 모델링에서 수행할 수 있는 다양한 역할을 탐색하고, 다양한 비디오 이해 작업에 걸쳐 성능을 평가한다. Li et al. [84]는 긴 동영상을 효율적으로 처리할 수 있는 State Space Model, Video-Mamba 기반의 동영상 이해 모델을 제안한다.\n' +
      '\n' +
      '이중 카메라 추적의 일관성을 유지하고 내시경 팁 외관의 큰 변동을 다루기 위해 Zhang 등[88]은 교차 카메라 상호 템플릿 전략(CMT)을 제안하고 추적 중에 동적 과도 상호 템플릿을 도입한다. 대면적 폐색에 의한 간섭과 내시경 팁 광원에 의한 왜곡을 최소화하기 위해 Mamba 기반 모션 유도 예측 헤드(MMH)가 도입되었다.\n' +
      '\n' +
      '다른 과제에서 더 많은 연구자들이 맘바의 이점을 인식함에 따라 이 모델은 다양한 분야에서 주목을 받았다. Pan-Mamba [73]은 팬 샤프닝 도메인으로의 첫 번째 진출을 나타낸다. 이는 채널 스와핑 Mamba와 크로스 모달 Mamba의 두 가지 주요 모듈로 구성된다. 채널 스와핑 맘바는 가볍고 효율적인 방식으로 PAN 채널들 및 LRMS 채널들로부터 피처들의 다양성을 융합하고 향상시키는 것을 목표로 한다. 후자의 모듈인 크로스 모달 맘바는 채널 스왑핑 맘바 이후에 전개되어 게이팅 메커니즘을 통해 중복 모달 특성을 필터링한다. DreamerV3[59]\n' +
      '\n' +
      '도. 8: 이미지 및 비디오 프로세싱을 위해 제안된 SSM들에서 사용되는 상이한 선택적 스캔 방법들. (a) VMamba[61], (b) Vision Mamba[60], (c) RSMamba[139], (d-f) Video Mamba[84].\n' +
      '\n' +
      '이는 데이터의 입력, 차원, 보상 측면에서 다양한 분야에서 고정된 파라미터 범위의 한계를 극복한 세계 모델에 기반한 보편적이고 확장 가능한 방법이다. 장거리 예측을 위해 Naman et al. [74]는 스펙트럼 필터링 알고리즘을 사용하여 선형 동적 시스템(LDS)을 학습하는 것을 기반으로 하는 스펙트럼 상태 SSM이라고 하는 시퀀스 모델링에 대한 새로운 접근법을 소개한다. 이 아키텍처는 약간 안정적인 대칭 LDS에 대해서도 안정적이고 효율적인 학습을 보장한다. 강화 학습 작업의 경우 계층적 정책인 HIEROS[75]는 표본 효율성 향상을 목표로 한다. HIEROS는 계층적 세계 모델, 구체적으로 S5 계층 기반 세계 모델(S5WM)과 효율적인 시간 균형 샘플링 방법을 활용한다. 아타리 100k 벤치마크에서 평균 및 중위 정규화된 인간 점수 측면에서 기존 접근법을 능가하며 우수한 탐색 능력을 보여준다. Cheng et al. [86]은 모던 스테이트 스페이스 모델인 Vim이 보다 넓은 범위의 활성화 영역을 통해 단일 이미지 슈퍼-해상도(SISR) 분야에서 컨볼루션 신경망(CNN) 및 비주얼 트랜스포머(ViT)의 성능을 향상시킬 수 있는 방법을 탐색한다. VMRNN[103]은 정확하고 효율적인 시공간 예측을 위해 Vision Mamba 블록과 LSTM을 결합한 새로운 순환 단위이다. Shen et al. [104]는 단일 뷰 이미지들로부터 엔드 투 엔드, 상각된 3D 복원 모델인 Gamba를 소개한다. 그들의 주요 발견은 3D 가우시안 분할 프로세스의 효율성을 향상시키기 위해 상당한 수의 3D 가우시안들을 활용하는 것을 포함한다. 또한, Mamba 기반 순차 네트워크를 도입하여 시퀀스(토큰) 길이를 갖는 문맥 의존적 추론과 선형 확장성을 가능하게 하여 높은 메모리 요구와 자원 집약적인 렌더링 프로세스를 다룬다. Wang et al. [105]는 변형 가능한 3D 이미지 등록을 위한 교차 스캐닝 모듈을 갖는 VMambaMorph라는 새로운 시각적 Mamba-기반 프레임워크를 소개한다. Li et al. [107]은 시간적 비디오 로컬라이제이션 태스크들을 다루기 위한 SpikeMba라는 새로운 접근법을 제안한다. SpikeMba는 임펄스 신경망과 상태 공간 모델(SSM)을 통합하여 다중 모드 특징 간의 세밀한 관계를 효율적으로 포착한다. Zou et al. [116]은 RhythmMamba라 불리는 Mamba를 기반으로 하는 새로운 원격 광용적맥파(rPPG) 신호 검출 방법을 제안한다. Rhythmamba는 rPPG의 주기 패턴과 단기 추세를 모두 포착하기 위해 다중 시간 제약을 사용하는 종단 간 방법이다. 또한, 주파수 영역 피드포워드를 사용하여 준주기 rPPG 패턴을 강력하게 해석하는 맘바의 능력을 향상시킨다.\n' +
      '\n' +
      '### _Graph_\n' +
      '\n' +
      '정형 그래프 데이터는 표준 격자 데이터(예를 들어, 이미지) 외에도 소셜 네트워크 및 단백질 구조 데이터와 같은 인공 지능에서도 널리 연구되고 있다. 입력 형태는 순차적인 데이터이기 때문에 그래프 구조 데이터를 처리하기 위해 SSM을 적용할 수 있다. 구체적으로, Graph54mer[52]는 구조화된 상태 공간(Structured State Space, S4) 아키텍처를 활용하여 장거리 시간 종속성을 포착하고 그래프 구조를 동적으로 진화하기 위해 그래프 구조 학습 계층을 도입하여 시간에 따른 데이터의 공간 상관성에 적응한다. GMNs[53]은 선택적 상태 공간 모델을 기반으로 하는 새로운 종류의 그래프 신경망(GNNs)으로, 장거리 종속성과 계산 효율성을 포착하는 데 있어 전통적인 GNNs의 한계를 해결한다. 이 프레임워크는 노드-레벨 및 서브그래프-레벨 토큰화를 브릿지하여 그래프 구조의 효율적인 학습을 용이하게 하는 그래프 토큰화 프로세스를 소개한다. 또 다른 동시 작업인 그래프-맘바[51]도 맘바 아키텍처를 기반으로 개발되었다. 그래프-맘바는 컨텍스트에 대한 더 많은 액세스를 위해 중요한 노드들의 우선순위를 매기는 노드 우선순위화 기술을 포함하고 시퀀스-관련 바이어스를 최소화하기 위해 순열-기반 트레이닝 레시피를 채용한다. Ali Behrouz 등은 주어진 목표 노드에 대해 목표까지의 최단 거리에 기초하여 다른 노드들을 집합시키는 새로운 그래프 표현 학습 구조인 GRED[53]을 제안한다. 그들은 스킵 표현 시퀀스를 인코딩하기 위해 선형 RNN을 채택한다. Gregor et al. [140]은 필수 훈련을 통해 교사가 다음 토큰 예측기를 정확하게 학습하지 못하는 문제를 논의하고, 가장 간단한 계획 작업을 통해 다중 토큰 예측 훈련에서 트랜스포머 및 맘바 아키텍처의 실패를 입증한다. Li et al. [55]는 강력한 선택적 상태 공간 모델을 사용하여 STG 학습을 처리하기 위한 첫 번째 시도인 STG Mamba를 소개한다.\n' +
      '\n' +
      '### _멀티 모달 및 멀티 미디어_\n' +
      '\n' +
      '상태 공간 모델은 또한 멀티-모달/멀티-미디어 태스크들에 대해 적응될 수 있다. 구체적으로, 54ND[30]은 SSM(State Space Models)을 다차원 신호로 확장하여, 대규모 시각 데이터를 연속적인 다차원 신호로 모델링할 수 있게 한다. 이 방법은 이미지 및 비디오 분류에서의 애플리케이션들을 포괄하는 상이한 차원들(1D, 2D, 및 3D)에 걸쳐 효과적인 것으로 입증되었다. Grazzi et al. [45]는 Transformer와 유사한 in-context learning (ICL) 능력으로 Mamba를 평가한다. 분석 결과, Mamba는 Transformer와 마찬가지로 반복 최적화 전략처럼 내부 표현을 점진적으로 개선하여 ICL 문제를 해결하는 것으로 나타났다. 더 긴 입력 시퀀스를 포함하는 ICL 태스크의 경우, Mamba는 트랜스포머의 효과적인 대안이 될 수 있다. Park et al. [141]은 또한 Mamba의 성능을 평가하였고, 그 결과 표준 회귀 ICL 태스크에서 Mamba의 성능은 Transformer의 성능과 유사하며 희소 패리티 학습 태스크에서 Mamba의 성능이 더 우수함을 보였다. 그러나 비표준 검색 기능이 포함된 작업에서는 성능이 좋지 않았다. Mamba와 어텐션 블록으로 구성된 MambaFormer[141]은 위의 과제를 해결하는 데 사용되었으며 각 과제에서 단일 모델을 능가했다. Zucchet et al. [142]는 RNN과 Transformer 사이의 보다 긴밀한 개념적 관계를 밝히고 있다. 실험 결과는 RNN과 Transformer가 완전히 배타적인 모델이 아님을 증명한다. 또한 곱셈적 상호작용으로 게이티드 RNN을 학습하여 이 두 아키텍처 사이의 격차를 해소함으로써 이론과 실습에서 선형 자기 주의를 달성할 수 있음을 보여준다. Ali et al. [143]은 Mamba 모델의 학습 메커니즘, 특히 의존성이 캡처되는 방법 및 RNN, CNN, 또는 주의 메커니즘과 같은 다른 확립된 계층과의 유사성을 탐색한다. 맘바와 자기 주의층 사이의 중요한 관계가 성립한다. Mamba 모델의 기본 특성은 고유한 데이터 제어 선형 연산자에 의해 실현되기 위해 암시적 주의에 의존한다는 것을 보여줌으로써 명확해지며, 이는 선택적 상태 공간 계층이 주의 모델임을 나타낸다. 획득된 주의 행렬들을 이용함으로써, 이러한 숨겨진 주의 행렬들에 기초한 해석가능성 기법들의 세트가 제공된다. MamboMIL [144]는 SR-Mambo를 핵심 구성 요소로 하여 Mambo 프레임워크를 MIL에 통합하며, 이는 분산된 포지티브 인스턴스 간의 원격 종속성을 캡처하는 데 능하다. MamboMIL은 더 많은 판별 기능을 효율적으로 포착하고 과적합 및 높은 계산 오버헤드와 관련된 문제를 완화하여 계산 병리학에서 Mambo 프레임워크의 첫 번째 적용을 표시할 수 있다.\n' +
      '\n' +
      'Hierarchical Temporal Mambo (HTM)와 Bidirectional Spatial Mamba (BSM)로 구성된 Motion Mamba [88]은 Mamba 모델이 모션 생성 분야에서 처음으로 통합되는 것을 나타낸다. HTM과 BSM은 각각 시간 및 공간 모델링을 위해 설계되는 동시에 선택적 스캐닝 메커니즘을 모션 생성 작업에 통합한다. 기존의 Transformer를 주로 사용하는 확산 기반 모션 생성 방법과 비교하여 모션 Mamba는 SOTA(state-of-the-art) 성능을 달성한다. 추론 속도도 4배 빨라집니다. VL-Mambo[145]는 다중 모드 학습 태스크에서 트랜스포머 아키텍처의 값비싼 계산 오버헤드를 해결하기 위해 상태 공간 모델 Mamba를 탐색하기 위한 첫 번째 노력이다. CMViM[146]은 3D 고해상도 의료 이미지, 특히 알츠하이머병(AD)에 멀티모달 표현 학습의 적용에 초점을 맞추고 있다. MAE[150] 프레임워크를 기반으로 개발되었으며 ViT[151] 모듈을 Vim[61] 모듈로 대체하여 복잡도를 2차 수준에서 선형 수준으로 줄였다. 또한, 멀티모달 Vim 인코더가 동일한 모달리티에서 판별 특징을 모델링하고 모달리티 간의 잘못된 표현을 완화할 수 있는 능력을 향상시키기 위해 모달 내 및 모달 간 대비 학습 방법을 도입한다. Cobra[147]은 선형 계산 복잡도를 갖는 언어 모델의 조합을 탐색하고,\n' +
      '\n' +
      '도. 9: State Space Model(a[61], b[60], c[98], d[86], e[99], f[31], g[88], h[46], i[42], j[47], k[52], [103])을 기반으로 설계된 대표 블록.\n' +
      '\n' +
      '멀티모달 입력. 현재 모델에서 일반적으로 사용되는 트랜스포머 네트워크를 보다 효율적인 맘바 아키텍처로 대체합니다.\n' +
      '\n' +
      '시각적, 언어적 정보 융합 측면에서 Zhao et al. [147]은 Mamba 언어 모델의 내부 정보 통합을 최적화하여 보다 효과적인 표현을 달성한다. Decision Mamba(DMamba) [148]은 Mamba 프레임워크를 Decision Transformer(DT) [152]에 통합한다. Decision Mamba와 DT를 비교하는 일련의 실험은 Mamba가 강화 학습(RL) 작업에 효과적임을 보여준다. 그러나 단순히 Mamba 블록을 DT에 적용하는 것은 효율성이 향상되지 않는데, 이는 저자들이 고려하는 RL 태스크들이 많은 수의 CPU와 GPU 상호작용을 갖기 때문이다. 또 다른 결함은 하이퍼-파라미터 검색의 부재와 RL 태스크의 데이터 구조를 반영하기 위해 맘바 블록을 더 효과적으로 사용하는 방법에 대한 분석이다. 시그마[149]는 다중 모달 의미 분할에 성공적으로 적용된 최초의 상태 공간 모델이다. 어텐션 기반의 Mamba 융합 메커니즘과 채널 인식 Mamba 디코더인 VMamba로 구성되어 다양한 실험에서 우수한 성능을 보였다. 그러나, Sigma는 더 긴 시퀀스를 처리하는데 활용도가 낮고, Mamba 인코더의 메모리 소모가 여전히 상대적으로 커서 경량 에지 디바이스에 배치하기가 어렵다.\n' +
      '\n' +
      '### _Event Stream/Point Cloud Data_\n' +
      '\n' +
      '자연어 처리에서 상태 공간 모델(SSM)의 성공에 영감을 받은 PointMamba[153]는 SSM의 장점을 활용하여 선형 복잡성을 유지하면서 글로벌 모델링 기능을 자랑하는 프레임워크를 도입한다. 이 혁신적인 모델은 임베디드 포인트 패치를 입력으로 사용하여 재정렬 전략을 사용하고 이러한 포인트 패치를 일련의 맘바 블록에 공급하여 SSM의 글로벌 모델링 능력을 강화함으로써 작동한다. PCM[154]는 일관된 트래버스 직렬화를 제안한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c} \\hline \\hline\n' +
      '**\\#ID** & **Algorithm** & **Publish** & **Domain** & **Parameters** & **Architecture** & **Downstream Tasks** & **Accuracy** & **Code** \\\\ \\hline \\multirow{3}{*}{96} & \\multirow{3}{*}{**SS [57]**} & \\multirow{3}{*}{ICLR23} & Multi-modal & \\multirow{3}{*}{280K} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{Classification} & Speech Commands & \\multirow{3}{*}{URL} \\\\  & & & Multi-media & & & & (164kHz)96.52 & & \\\\  & & & & & & & (8kHz)94.53 & \\\\ \\hline \\multirow{3}{*}{97} & \\multirow{3}{*}{**Grazzi et al. [45]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} \\\\  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{98} & \\multirow{3}{*}{**MambaFormer [141]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba+Former & - & - & - \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{99} & \\multirow{3}{*}{**Zucchet et al. [142]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{100} & \\multirow{3}{*}{**Mamba [12]**} & \\multirow{3}{*}{arXiv24} & Multi-media & Mamba-130M & \\multirow{3}{*}{Mamba} & Synthetic & Synthetic tasks99.8 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & Mamba-370M & & & Zero-shot(Average ACC) & URL \\\\  & & & Mamba-90M & & & Zero-shot & 447(130M) 30.037(M) & \\multirow{3}{*}{URL} \\\\  & & & Mamba-1.4B & & & 57.1(79Mb) 59.7(14B) & & \\\\ \\hline \\multirow{3}{*}{101} & \\multirow{3}{*}{**Ali et al.[143]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{VM-S-VIT-S} & \\multirow{3}{*}{Mamba Transformer} & \\multirow{3}{*}{Segmentation} & VIM-S-(mAP) (mIoU) & \\multirow{3}{*}{URL} \\\\  & & & & & & Raw-Attention 74.88, 45.09 & \\multirow{3}{*}{Attn-Rollout 578.51, 51} & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & Mamba -Attr 81.70, 542.0 & \\multirow{3}{*}{URL} \\\\  & & & & & & VTS-(mAP) (mIoU) & \\multirow{3}{*}{VTS-(mAP) (mIoU)} & \\multirow{3}{*}{} \\\\  & & & & & & Raw-Attention 7.25, 36.94 & \\multirow{3}{*}{Attn-Rollout 50.34, 47.85} & \\multirow{3}{*}{65.63} & \\multirow{3}{*}{\\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } } \\\\  & & & & & & Mamba-Attr 84.85, 65.63 & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{102} & \\multirow{3}{*}{**MambaMI [144]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba & Survival Prediction & Survival Prediction & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } \\\\  & & Multi-media & & & & & & \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{103} & \\multirow{3}{*}{**Motion Mamba [88]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba & Motion Synthesis & \\multirow{3}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } \\\\  & & Multi-media & & & & & & & \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\hline \\multirow{3}{*}{104} & \\multirow{3}{*}{**VL-Mamba [145]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{Mamba LLM-2.8B} & \\multirow{3}{*}{Mamba} & \\multirow{3}{*}{Multimodal Learning} & VQA-M-27.66, 632.6 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & SOA-M-M-G56.4 & TextVO-A48.9 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & & POPE-84.4 & Mam:1369.6 & \\multirow{3}{*}{6} \\\\  & & & & & & MMS 57.0 Mam-Net-32.6 & \\multirow{3}{*}{URL} \\\\ \\cline{1-1}  & & & & & & & \\\\ \\hline \\multirow{3}{*}{105} & \\multirow{3}{*}{**CMVM [146]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{50M} & Mamba & AD Classification & ACC@9.3 AUC-81.4 & - \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & Multi-media & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{107} & \\multirow{3}{*}{**DMamba [148]**} & \\multirow{3}{*}{arXiv24} & Multi-media & \\multirow{3}{*}{-} & Mamba & RL & HalfCheetsh=m42.8\\(\\pm\\)0.08 & \\multirow{3}{*}{URL} \\\\  & & Multi-media & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\cline{1-1}  & & & & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Summary of existing SSM-based Multi-modal and Multi-media algorithms.\n' +
      '\n' +
      '점 구름을 점들의 1-D 시퀀스로 변환하고 시퀀스 내의 인접한 점들이 또한 공간 상에서 인접하도록 보장하는 전략. 일관된 트래버스 직렬화 전략은 x, y, z 좌표의 순서를 배열하여 6개의 변형을 생성하며, 저자는 Mampa에게 순서 프롬프트를 도입하여 시퀀스의 배열 규칙을 알려준다. 또한, 포인트 클라우드 위치 정보를 추가하기 위해 공간 좌표 매핑 기반의 위치 임베딩 방법을 제안한다. 포인트 맘바[155]는 불규칙한 포인트들에 대한 옥트리 기반 순서화 메커니즘을 설계하여, 그들의 공간적 근접성 및 인과적 의존성의 보존을 보장한다. 점들은 층화된 점 특성들을 추출하기 위해 점 맘바 블록들과 다운샘플링 층들의 시퀀스를 거친다. Zhou 등은 Mamba를 포인트 클라우드 필터링 태스크에 통합하고 빠른 미분 가능한 렌더링 손실을 도입하는 3DMambaIPF[156]을 제안한다. 이 접근법은 대규모 포인트 클라우드를 처리하는 데 강력한 성능을 입증했다. Li 등은 포인트 클라우드 재구성을 위한 하이퍼포인트 생성 모듈, 하이퍼포인트 스프레드 모듈 및 변형 방법을 통합한 3DMambaComplete [157]을 제안한다. HyperPoint Generation 모듈은 Mamba의 선택 메커니즘을 도입하여 포인트 클라우드 특징을 인코딩한다. Zubi\'c et al. [158]은 이벤트-기반 비전에 학습가능한 시간-스케일 파라미터들을 갖는 상태-공간 모델들(SSMs)을 도입하여, 상이한 주파수들에서 네트워크를 재훈련할 필요 없이 상이한 주파수 시간 입력들에 대한 적응을 가능하게 한다.\n' +
      '\n' +
      '### _Time Series Data_\n' +
      '\n' +
      'SSM은 시퀀스 모델이기 때문에 다변량 시계열 데이터를 처리하기 위해 SSM을 적용하는 것이 매우 직관적이고 효과적이다[180, 184, 185]. 구체적으로, 장기 시계열 예측(LTSF) 과제의 주요 과제는 장기 종속 관계를 포착하는 어려움과 열악한 선형 확장성에 있다. 타임머신[180]은 다변량 시계열 데이터에서 Mamba를 활용하여 장기 종속성을 포착하는 방법을 도입하여 이러한 문제를 해결한다. 다중 맘바 모듈을 갖는 통합 아키텍처를 이용함으로써, 타임머신(TimeMachine)은 채널 혼합 및 채널 독립성과 관련된 문제들을 효과적으로 해결한다. 이 접근법은 서로 다른 스케일에 걸쳐 글로벌 및 로컬 컨텍스트 정보의 선택적 예측을 가능하게 한다. 실험적 검증은 타임머신이 우수한 확장성을 유지하면서 정확도를 크게 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '### _Others_\n' +
      '\n' +
      '전술한 도메인들 외에도, SSM은 또한 많은 다른 애플리케이션들에서 채택될 수 있다. 실제 센서는 대부분 비선형적이며 외부 변수의 간섭을 받아 기존 로컬 선형 예측 알고리즘이 실제 시나리오에서 비효율적이다. Bhirangi[179] 등은 연속 시퀀스 예측(CSP)의 태스크에 대한 벤치마크를 확립하고 동시에 계층적 상태 공간 모델(HiSS)을 제안하였다. 이 모델은 다양한 해상도로 구조화된 공간 상태 모델의 여러 계층을 적층하여 시간적 계층을 구성한다. 실험 결과는 HiSS가 더 우수하다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \\hline \\hline\n' +
      '**\\#ID** & **Algorithm** & **Publish** & **Domain** & **Parameters** & **Architecture** & **Downstream Tasks** & **Accuracy** & **Efficiency** & **Code** \\\\ \\hline \\multirow{3}{*}{109} & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:12:M} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{Classification:ConObject:NN} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & OBJ-BC-88:3 & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:12:M} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & OBJ-D3D3:57.78 & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:17:4M} & \\multirow{3}{*}{SSM} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & PB-P50-852:48 & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [155]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{Classification:} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{*}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} \\\\  & & & & & & & & & \\\\  & \\multirow{3}{**Point Mamba [153]**} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{point} & \\multirow{3}{*}{-} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}{} & \\multirow{3}{*}\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '여러 실제 센서 데이터 세트에서 평균 제곱 오차(MSE) 측면에서 다른 시퀀스 모델을 최소 \\(23\\%\\)만큼 형성한다. LOCOST [168]은 긴 컨텍스트 입력을 갖는 조건부 텍스트 생성 태스크들에 대한 상태-공간 모델들에 기초한 인코더-디코더 아키텍처를 도입하였다. 이 접근법은 계산 복잡도와 메모리 사용량을 효과적으로 감소시켜 훈련 단계와 추론 단계 모두의 속도를 크게 향상시켰다. MambaStock[178]은 구조화된 상태 공간(S4) 아키텍처를 활용하여 주식 데이터의 비선형성을 포착하여 미래 주가에 대한 정확한 예측을 가능하게 한다. Lu et al. [163]은 단순화된 구조화된 상태-공간 시퀀스 모델에 대한 개선을 제안하여(S5), 모델 트레이닝 단계 동안 궤적들 내의 숨겨진 상태들의 재설정을 가능하게 한다. 특히, 모델이 가변 길이 시퀀스를 처리할 수 있도록 하기 위해, 본 논문은 연관 연산자를 수정하고 S5에서 연관 속성을 보존하는 리셋 주석을 도입한다. 또한, 모델의 일반화 능력을 테스트하기 위해 도전적인 메타-RL 설정도 도입한다. [188] 본 논문에서는 디지털 오디오 제작을 위한 실감형 디지털 다이나믹 레인지 압축기 모델을 개발하기 위한 시뮬레이션 프로토타입을 분석한다. 학습된 표상은 종종 모델의 높은 차수에 의해 영향을 받으므로 제어 설계에 적합하지 않다. [189] 이 문제를 해결하기 위해 SSM에서 선형 동적 블록에 대한 시스템 이론 기반 모델 차수 감소 기법을 제안한다.\n' +
      '\n' +
      'Wang et al. [191]은 임의의 연속적인 시퀀스-대-시퀀스 관계의 근사화가 층간 비선형 활성화를 갖는 상태-공간 모델들을 적층함으로써 달성될 수 있음을 입증한다. 또한, 실험 결과는 이 접근법이 복잡한 시퀀스 패턴을 학습하는 모델의 능력을 향상시킨다는 것을 나타낸다. 마지막으로,\n' +
      '\n' +
      '도. 10: (a). 이벤트 스트림 처리를 위한 RNN/SSM과 그들의 SSM-VIT 블록 구조[158]; (b). 포인트 클라우드 맘바[154]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \\hline \\hline\n' +
      '**\\#ID** & **Algorithm** & **Publish** & **Domain** & **Parameters** & **Architecture** & **Downstream Tasks** & **Accuracy** & **Efficiency** & **Code** \\\\ \\hline \\multirow{3}{*}{142} & \\multirow{3}{*}{S/D-Mamba [184]} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{TSF} & \\multirow{3}{*}{-} & \\multirow{3}{*}{Mamba} & & \\multirow{3}{*}{LTSF} & 0.066(Q-Mamba) & & \\\\  & & & & & & & 0.171(Q-Mamba) & & \\\\ \\hline \\multirow{3}{*}{143} & \\multirow{3}{*}{SiMBA [185]} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{others} & Small: & \\multirow{3}{*}{Mearach: 18.5M} & \\multirow{3}{*}{MSM-VIT: \\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{ImageNet: IK(acc/top)} & FLOPs: & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{143} & \\multirow{3}{*}{SiMBA [185]} & \\multirow{3}{*}{arXiv24} & \\multirow{3}{*}{others} & Small: & \\multirow{3}{*}{MSM-VIT: \\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{URL} \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\ \\hline \\multirow{3}{*}{142} & Xu et al. [186] & arXiv242 & NIT & - & Mamba & Language modeling & - & - & - \\\\ \\hline \\multirow{3}{*}{145} & Sharma et al. [187] & arXiv24 & others & - & Mamba & Factual Recall & - & - & - \\\\ \\hline \\multirow{3}{*}{146} & Yin et al. [188] & arXiv24 & Audio & - & SSM & Audio Production & - & - & - \\\\ \\hline \\multirow{3}{*}{147} & Marco et al. [199] & arXiv24 & others & - & SSM & Prediction & fit index: 80.5 & - & - \\\\ \\cline{2-7}  & & & & & & & \\begin{tabular}{} \\end{tabular} & \\begin{tabular}{} \\end{tabular} & \\begin{tabular}{} \\end{tabular} & \\begin{tabular}{} \\end{tabular} \\\\ \\hline \\multirow{3}{*}{148} & Yang et al. [190] & arXiv24 & others & - & Mamba & \\multirow{3}{*}{prediction} & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & \\multirow{3}{*}{URL} \\\\  & & & & & & & & \\\\  & & & & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IX: Summary of existing SSM-based other algorithms.\n' +
      '\n' +
      '수치해석과 수치적 검증 결과, 상태공간 모델은 지수적 기억감소 문제를 근본적으로 해결하지 못하는 것으로 나타났다. Samsami et al. [169]는 모델 기반 강화 학습(MBRL) 에이전트의 세계 모델에 상태 공간 모델 세트를 통합하여 기존 MBRL 에이전트가 행동과 결과 사이의 장기 간격을 처리할 수 없었던 문제를 해결하는 장기 기억 및 장기 신용을 향상시키기 위해 R2I라는 방법을 제안한다. 실험 결과는 R2I가 메모리 및 크레딧 할당 RL 태스크에서 최첨단 성능을 달성하는 동시에 더 빠른 수렴 속도를 나타냄을 입증했다. Black-Mamba[167]는 SSM과 전문가 혼합(Mixed-of-experts, MoE)을 병합함으로써 추론 비용을 현저히 감소시켜 효율적이고 확장 가능한 텍스트 생성 작업의 길을 열어준다. MambaByte [166]은 원시 바이트에서 직접 학습하도록 설계된 토큰 없는 선택적 상태 공간 모델이며 반복적인 특성으로 인해 빠른 텍스트 생성이 가능하여 대규모 모델에 대한 실용성을 강조하고 토큰 없는 언어 모델링의 향후 개발을 위한 길을 열어준다. 자가 프리트레이닝(SPT) [192]은 처음부터 롱-시퀀스 모델들을 비교하는 종래의 접근법에 도전하며, 데이터 기반 프리트레이닝이 성능 평가들을 상당히 변경한다는 것을 드러낸다. 사전 훈련을 활용하여 다양한 아키텍처에 걸쳐 상당한 개선을 달성하여 트랜스포머와 SSM 간의 성능 격차를 줄이고 작업에 대한 이전 SSM 결과를 능가한다. Laughing Hyena Distillery[161]는 품질의 손실 없이 미리 훈련된 긴 컨볼루션 시퀀스 모델들로부터 컴팩트한 상태-공간 모델들을 추출하고, 합성곱 계층들에서 저차원 상태-공간 모델들을 추출하기 위해 유리 함수 근사화 및 모델 다운스케일링 기법들을 활용하고, 일정한 메모리 및 일정한 시간 복잡도를 갖는 자동화된 회귀 생성을 달성하는 새로운 증류 방법인 Laughing Hyena를 제안한다. 개선된 하이에나는 사전 훈련 품질을 개선하고 필터 무게를 채널에 걸쳐 헤드에 바인딩하여 증류할 필터 수를 줄입니다.\n' +
      '\n' +
      'GateLoop[170]은 효율적인 자동 회귀 언어 모델링을 위해 데이터 제어 게이트 입력과 출력을 사용하는 완전 데이터 제어 선형 RNN인 GateLoop을 소개한다. 또한 데이터 제어 상태와 비데이터 제어 상태 천이의 장단점을 비교하여 GateLoop의 우수성을 강조하기 위해 합성 언어 모델링을 위한 메모리 수평 데이터 세트를 제시한다. 병렬 스캐닝 훈련 전략과 동등한 주의 대체 모델도 입증된다. 선택적 상태 공간 모델[176]을 사용한 급성 뇌 기능 장애 상태 예측에 대한 다중 코호트 연구는 풍부한 전자 건강 기록(EHR) 데이터를 활용하여 중환자실에서 중증 환자의 ABD 예측을 위한 데이터 기반 자동 데이터 기반 접근법을 개발한다. 그들의 연구는 중환자실 체류 중 섬망, 혼수상태 및 사망을 동적으로 예측하여 높은 성능을 보여주었고 두 개의 공개 데이터 세트에서 검증되었다. S/D-Mamba [184]는 두 가지 간단한 Mamba 기반 모델인 S-Mamba와 D-Mamba를 소개하며, 둘 다 Mamba 블록을 사용하여 변수 상관(VC)을 추출한다. S-Mamba는 Mamba 블록을 사용하여 변수 간의 상관 관계를 처리합니다. D-Mamba는 S-Mamba에 비해 매개변수를 조정하여 VC에 더 민감하다. D-Mamba는 S-Mamba에 비해 VC를 처리하기 위해 Mamba 레이어에 여분의 Mamba 블록을 추가하고, 여분의 Mamba 블록은 매개변수를 조정하여 변수 간의 상관 관계에 더 민감하다. 실험 결과는 두 모델 모두 GPU 메모리와 훈련 시간을 절약하면서 성능 면에서 기존 방법보다 우수함을 보여준다. Liu 등은 시퀀스 간의 종속성을 모델링하기 위해 제안된 새로운 방법인 Mamba4Rec [175]를 제안한다. 본 논문에서는 Mamba4Rec의 성능을 입증하기 위해 최신 성능에 도달한 MovieLen-1M, Amazon-Beauty, Amazon-Video-Games를 대상으로 실험을 진행하였다. Quan et al. [177]은 원본 SpatialNet에 기반한 다채널 음성 향상을 처리하기 위한 새로운 모델을 제안한다. 그들은 최상의 성능에 도달하는 oSpatialNet-Mamba를 제안하며, 이의 핵심 이점은 State Space Model이다. 다양한 작업이 모델에 대해 테스트되었으며 모두 잘 수행되었습니다. Schiff 등은 DNA 서열을 양방향 및 등가적으로 모델링할 수 있는 카두세우스[87]이라는 새로운 생물정보학 모델을 소개한다. MambaDNA를 기반으로 카두세우스는 RC 등가 및 양방향 장거리 DNA 언어 모델의 첫 번째 계열로 사전 훈련 및 미세 조정 전략을 도입한다. 또한 Genomic 벤치마크에서 이전 장거리 모델보다 성능이 우수합니다. Zhang 등은 Mamba에 기초한 새로운 모션-유도 추적기 및 모션-유도 예측 헤드[88]를 소개한다. Karan et al. [159]는 S4 모델 긴 시퀀스 모델링에 기반한 다단계 구조인 SaShiMi를 제안한다. Hurwitz 행렬에 대한 연결을 그리면 매개변수화에 대한 간단한 개선을 제공한다. 또한, SaShiMi는 비재귀 상태에서의 비재귀 생성 성능을 향상시킨다. 이산 상태 공간 은닉 마르코프 모델을 결합한 새로운 예측 모델이 David et al. [173]에 의해 제안되었다. 잠재 상태와 배출 분포의 매개 변수를 교대로 훈련하기 위해 변수 분리 사후 분포와 2단계 훈련 프로그램을 도입한다. 배출 법칙의 집합을 학습하고 숨겨진 프로세스에 따라 동적으로 활성화합니다. FlashFFTConv [174]는 FlashFFTConv라는 합성곱 최적화 방법을 제안한다. FlashFFTConv는 고속 푸리에 변환을 계산하기 위해 행렬 인수분해를 사용하고 긴 수열의 커널 융합을 위해 행렬 곱셈 단위를 사용하며 입출력 비용을 효과적으로 감소시킨다.\n' +
      '\n' +
      '여러 관련 연구에서 상태 공간 모델(State-Space Model, SSM)의 다양한 측면과 다양한 도메인에서의 응용을 탐구하였다. [181] 연속 학습 DSSM(Continuous Learning DSSM, CLDSSM)이라고 불리는 심층 상태 공간 모델(Deep State-Space Model, DSSM)을 제안한다. CLDSSM은 정규화 기반 연속 학습(CL) 방법을 통합하여 치명적인 망각 없이 다중 동적 시스템을 효율적으로 업데이트한다. [172] 근사 대각화를 통해 긴 시퀀스 작업에 대한 SSM의 견고성을 향상시키는 데 중점을 둡니다. 저자들은 SSM의 견고성을 향상시키기 위해 대각화를 근사화하는 방법을 제안한다. 문제를 단순화하고 순수한 대각선 구조를 고려함으로써 제안된 방법은 계산 효율성을 달성하고 채널 통신을 허용한다. [171]에서 SSM의 _메모리 저주_ 는 안정적인 재매개 변수화를 통해 해결됩니다. 저자들은 메모리 한계를 효과적으로 향상시키는 SSM을 위한 파라미터화 기법을 소개한다. [164] SSM에 대한 데이터 의존적 일반화를 제공하여, SSM 파라미터들과 트레이닝 시퀀스들의 시간적 종속성들 사이의 상호작용을 강조한다. 이러한 일반화 한계를 기반으로 시퀀스 데이터에서 서로 다른 시간 패턴을 수용하는 SSM의 견고성을 개선하기 위해 모델 초기화를 위한 스케일링 규칙을 제안한다. Wang et al. [160]은 State Space Model에 기반한 시퀀스 라우팅 방법을 제안하고, 주목하지 않고 빅 모델을 사전 훈련하고자 한다. 본 논문에서 제안한 양방향 게이티드 SSM(BiGS)은 시퀀스 모델링 작업을 효과적으로 단순화하기 위해 SSM 계층과 곱셈 게이팅 구조를 결합한다. 계산 집약적 어텐션 매트릭스를 생략함으로써 모델 트레이닝을 위한 계산 자원 및 시간을 감소시킨다. 이 방법은 기존의 사전 훈련 모델과 유사한 성능을 유지하면서 자원 소모를 약 30\\%\\ 줄일 수 있다. BiGS 모델은 쌍별 상호작용을 고려하지 않지만 GLUE 벤치마크 테스트에서 BERT의 사전 훈련 정확도를 일치시킬 수 있으며 근사치 없이 최대 4096 토큰의 롱폼 사전 훈련을 확장할 수 있다. 이 접근법은 자원이 제한된 환경에서 NLP 작업에 대한 효과적인 사전 훈련 솔루션을 제공하며 광범위한 NLP 시나리오에 적용될 가능성이 있다.\n' +
      '\n' +
      'Poli et al. [182]는 합성 작업을 통해 하이브리드 아키텍처의 설계 및 확장성을 평가하고 예측하는 것을 목표로 하는 MAD(Mechanistic Architecture Design)라는 새로운 프레임워크를 제안한다. 본 연구의 목적은 확장 법칙을 예측할 수 있는 소규모 능력 단위 테스트를 포함하여 종단 간 파이프라인을 사용하여 프로세스를 단순화하여 새로운 하이브리드 아키텍처를 식별하고 테스트하는 것이다. 본 연구는 하이브리드 아키텍처의 설계 및 확장성 문제에 초점을 맞출 뿐만 아니라 대규모 데이터 분석을 통해 하이브리드 아키텍처의 이론과 방법의 유효성을 검증한다.\n' +
      '\n' +
      'Smith[162]는 긴 시퀀스 데이터를 모델링하는 효율성과 성능을 향상시키는 데 초점을 맞춘 새로운 시공간 모델링 접근법이다. CNN(Convolutional Neural Networks)의 장점과 상태 공간 방법을 결합하여 복잡한 공간 상관과 긴 시간 종속성을 다루는데 있어 전통적인 모델의 한계를 효과적으로 극복한다. ConvSSM은 병렬 스캐닝과 빠른 자동 회귀 생성 기법을 통해 모델 학습과 예측의 속도를 크게 향상시킨다. 한편, S4 및 S5와 같은 상태 공간 방법을 사용하여 장거리 의존성 모델링을 위한 효과적인 매개변수화 및 초기화 전략을 제공한다. 이 접근법은 계산 자원의 소비를 줄일 뿐만 아니라 복잡한 모델에 필적하는 성능 수준을 유지한다. 시퀀스 길이에 따라 계산 비용이 크게 증가하는 트랜스포머, 훈련 속도가 느린 ConvLSTM 등 다른 모델에 비해 ConvSSM은 긴 시퀀스 시공간 모델링에서 잠재력을 보여준다. 또한, 연구자들은 합성곱 커널 설계를 최적화하고 서로 다른 모델의 장점을 결합하여 모델 성능을 더욱 향상시키는 방법도 모색하고 있다. [165] 시퀀스 모델링의 효율성과 성능을 최적화하기 위해 하이브리드 Mixture of Expert(MoE) 메커니즘과 선택적 상태 공간 모델(SSM)을 융합하는 접근법을 제안한다. 이 접근법은 Mampa 모델 위에 MoE를 도입하여 더 적은 훈련 단계에서 유사한 성능을 달성하고 Transformer 모델보다 추론 성능 이점을 유지한다. MoE 모델은 추론 단계에서 대규모 모델이 직면한 통신 및 메모리 문제를 해결하기 위해 동적 게이팅, 전문가 캐싱 및 부하 분산 기술을 활용하는 반면, 모델 용량을 확장하면서 계산 비용을 크게 증가시키지 않으면서 모델을 확장하기 위해 조건부 컴퓨팅을 사용한다. 명령어 미세 조정과 태스크-특정 미세 조정을 갖는 MoE 모델은 동일한 계산 복잡도를 갖는 조밀한 모델보다 더 나은 성능을 달성할 수 있음을 보여준다. MoE-Mampa의 성공은 잘 설계된 MoE 아키텍처를 통해 시퀀스 모델링 작업의 성능과 효율성을 효과적으로 향상시킬 수 있음을 보여주며, 이는 대규모 시퀀스 데이터를 처리할 수 있는 새로운 가능성을 열어준다. Xu et al. [186]은 문서 랭킹 작업에서 Mampa 아키텍처를 기반으로 한 모델의 성능을 평가함으로써 고전적인 정보 검색 작업에 대한 Mampa 모델의 잠재력을 보여준다. Mampa 모델은 Transformer 기반 언어 모델과 비교하여 동일한 학습 구성에서 경쟁 성능을 달성한다. 그러나 Mampa 모델은 효율적인 어텐션 구현과 비교하여 훈련 처리량 측면에서 부족하므로 효율적인 훈련 및 배치에 대한 잠재력을 제한한다. 비록 연구가 1B 미만의 파라미터를 갖는 모델에 초점을 맞추지만, 그들은 이것이 더 큰 모델로 스케일링하고 상이한 트레이닝 구성을 채용할 때 변할 수 있다는 것을 발견한다. 따라서, 다른 고전적인 정보 검색 작업에서 Mampa 모델의 효과와 성능은 더 조사되어야 한다. Amo et al. [193]은 SSM 시스템을 소개하고 제어 이론 분야의 연구 진행 상황을 요약한다. Sharma et al. [187]은 지식 리콜이 특정 모듈 및 마커 위치에 특정될 수 있다는 자기회귀 변압기 언어 모델에서 발견된 연구에서 영감을 받았으며 Mampa에서 사실 리콜을 찾는 것을 목표로 한다. Olucha 등은 선형 파라미터 변동 상태 공간(LPV-SS) 모델 다운스케일링의 최신 및 비교 연구의 개요를 제안하였다[194]. 이러한 비교는 주어진 LPV-SS 모델에 대한 최상의 다운스케일링 방법을 선택하는 데 도움이 될 수 있다. Yang 등은 시퀀스 추천 태스크를 위한 Rec-Mampa [190] 모델을 제안한다. 이 모델은 맘파 모듈을 통해 시간에 따라 변화하는 사용자의 선호 정보를 더 잘 모델링하여 개인화된 추천 성능을 향상시키고 계산 복잡도를 줄인다. SASRec에 비해 훈련 시간은 약 \\(70\\%\\) 단축되었다. LaRocque et al. [183]은 합성곱 신경망(CNNs)과 새로운 상태 공간 모델(SSM) 기반 맘파 아키텍처의 성능을 비교하여 개선된 지형에 대한 데이터 세트 융합의 장점과 이점에 대한 흥미로운 통찰력을 보여준다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 단일/다중 레이블 분류, 시각적 객체 추적, 픽셀 수준 분할, 이미지 대 텍스트 생성 및 사람/차량 재식별을 포함한 5개의 다운스트림 작업에 대한 실험적 비교를 제공한다. 더 자세한 내용은 각각 다음 하위 섹션에서 소개될 것이다.\n' +
      '\n' +
      '### _Single-/Multi-label Classification_\n' +
      '\n' +
      '단일 레이블 분류 문제를 위해 널리 사용되는 ImageNet-1K [2] 데이터셋에 대해 기존 작업의 정확도를 계산한다. 를 더 포함할 수 있다. 도 12의 (d)에서, 우리는 VMamba [60] 및 Mamba-2D [68]의 기본 버전이 ImageNet1K 데이터 세트에서 각각 83.2% 및 83%의 상위-1 정확도에서 더 나은 결과를 달성한다는 것을 발견할 수 있다. 또한 현재의 맘바 기반 비전 모델은 모두 소형, 소형 또는 기본 버전이며 거대하거나 거대한 버전의 마마바 네트워크를 사전 훈련하는 경우는 거의 없다는 것을 쉽게 알 수 있다. 전체 성능은 일부 Transformer 기반 모델과 비슷하지만 ImageNet 분류 데이터 세트의 최신 모델에는 여전히 열등합니다.\n' +
      '\n' +
      '다중 레이블 분류를 위해 보행자 속성 인식(PAR) 작업[6]3을 선택하고 PA100K [208] 및 PETA [209] 데이터 세트에 대한 실험을 수행한다. PA100K 데이터 세트는 598개의 시나리오에서 수집된 100,000개의 샘플을 포함하고 26개의 보행자 속성을 포함한다. 기본 설정(8:1:1)을 기반으로 훈련, 유효성 검사 및 테스트 하위 집합을 분할합니다. PETA 데이터 세트는 61 바이너리를 포함한다.\n' +
      '\n' +
      '도. 11: State Space Model을 이용하여 처리할 수 있는 대표 입력 데이터.\n' +
      '\n' +
      '도. 12: (a, b, c) 비디오 기반 인식, (d) 이미지 기반 인식, (e, f, g) 의료 이미지 기반 분할에 대한 실험 결과.\n' +
      '\n' +
      '헌정 및 19,000명의 인물 이미지. 훈련, 유효성 검사 및 테스트 하위 집합에는 각각 9500개, 1900개 및 7600개의 이미지가 포함되어 있습니다. 기본 설정을 따라 35개의 보행자 속성이 실험에 선택됩니다.\n' +
      '\n' +
      'ViT-S[19]와 Mamba 기반 네트워크 Vim-S[61]이 이 실험의 백본으로 채택되었다. 보행자 이미지와 속성을 입력으로 하고 각 속성의 로지스틱 점수를 예측하는 비전 언어 융합 기반 PAR 프레임워크 VTB[207]를 따른다. 표 X에 보고된 실험 결과로부터, 우리는 Vim-S 기반 PAR 모델이 PETA 데이터세트에서 81.08/73.75/80.91/84.96/82.52, PA100K 데이터세트에서 80.41/78.03/85.39/88.37/86.39를 달성한다는 것을 알 수 있다. 이러한 결과는 ViT-S 기반 모델보다 상당히 우수하지만 트랜스포머 네트워크를 기반으로 개발된 비교 PAR 알고리즘보다 여전히 상당히 열등하다. 예를 들어, ViT-B 기반 VTB는 PETA 및 PA100K 데이터 세트에서 85.31/79.60/86.76/87.17/86.71, 83.72/80.89/87.88/89.30/88.21을 달성한다.\n' +
      '\n' +
      '### _Visual Object Tracking_\n' +
      '\n' +
      '본 부분에서는 OSTrack[210] 기반의 추적 태스크4에 대해 Mamba와 Transformer, CNN 기반 백본을 비교한다. 구체적으로, CNN 기반 추적기는 TrDiMP[211], ToMP50[212], DiMP50[213], PrDiMP[214], KYS[215], ATOM[216]이고, Transformer 기반 추적기는 HDETrack[217], AiATrack[218], STARK[219], TransT[220], MiFormer[212], SimTrack[222]이다. 공정한 비교를 위해, 우리는 각각 841, 18 및 282개의 비디오를 포함하는 대규모 이벤트 기반 추적 데이터 세트인 EventVOT [217]에서 이러한 추적기를 훈련하고 테스트한다. 자세한 실험 결과는 표 XI 및 그림 1에 보고되어 있다. 13. 주의할 점은, 널리 사용되는 세 가지 평가 메트릭이 비교를 위해 사용되며, 성공률(SR), 정밀률(PR) 및 정규화된 정밀률(NPR)을 포함한다. 표 XI에 따르면, Mamba 백본 네트워크를 사용하여 ViT를 교체할 때 성능이 약간 감소하는 반면 매개변수 수(4.1M만)가 크게 감소한다는 것을 알 수 있다. 따라서, 우리는 Mamba 네트워크가 이벤트 기반 추적을 위한 유망한 선택이 될 것이라는 결론을 도출할 수 있다.\n' +
      '\n' +
      '각주 4: [https://github.com/wangxiao5791509/Single_Object_Tracking_Paper_List](https://github.com/wangxiao5791509/Single_Object_Tracking_Paper_List)\n' +
      '\n' +
      '### _Pixel-level Segmentation_\n' +
      '\n' +
      '최근 Mamba 네트워크는 그림과 같이 의료 영상 분할에 널리 활용되고 있다. 도 12(e, f, g)에 도시된 바와 같다. 예를 들어, Swin-Transformer 기반 모델 Swin-UNet [223]은 MRI Cardiac 데이터 세트에서 89.33/99.57/88.46(Dice, IoU, Accuracy)에 도달한다. 대조적으로, Mamba-기반 UNet은 Mamba-UNet[67], Semi-Mamba-UNet[70], 및 Weak-Mamba-UNet[72]와 같은 비교가능하거나 심지어 더 나은 세그먼트 결과들을 달성한다. 이러한 결과는 의료 영상 분할을 위한 Mamba 아키텍처의 유효성을 충분히 입증한다.\n' +
      '\n' +
      '### _Image-to-Text Generation_\n' +
      '\n' +
      '이미지-텍스트 생성을 위해, 우리는 X-선 의학을 취하는 X-선 보고서 생성 태스크를 선택한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c c c c c} \\hline \\hline \\multicolumn{1}{c|}{**Methods**} & \\multicolumn{1}{c|}{**Backbone**} & \\multicolumn{1}{c}{**PETA**} & \\multicolumn{1}{c}{**PA100K**} & \\\\ \\cline{3-11} \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{mA} & \\multicolumn{1}{c}{Acc} & \\multicolumn{1}{c}{Prec} & Recall & F1 & mA & Acc & Prec & Recall & F1 \\\\ \\hline\n' +
      '**JLAC** (AAAI 2020) [195] & ResNet50 & 86.96 & 80.38 & 87.81 & 87.09 & 87.50 & 82.31 & 79.47 & 87.45 & 87.77 & 87.61 \\\\\n' +
      '**SCCR** (TCSVT 2020) [196] & ResNet50 & 87.2 & - & 89.20 & 87.5 & 88.3 & 80.6 & 88.7 & 84.9 & 82.1 \\\\\n' +
      '**SSCsoft** (ICCV 2021) [197] & ResNet50 & 86.52 & 78.95 & 86.02 & 87.12 & 86.99 & 81.87 & 78.89 & 85.98 & 89.10 & 86.87 \\\\\n' +
      '**IAA-Caps** (PR 2022) [198] & OSNet & 85.27 & 78.04 & 86.08 & 85.80 & 85.64 & 81.94 & 80.31 & 83.86 & 88.01 & 87.80 \\\\\n' +
      '**MCFI** (NCA 2022) [199] & ResNet-50 & 86.83 & 78.89 & 84.57 & 88.84 & 81.53 & 77.80 & 85.11 & 88.20 & 86.62 \\\\\n' +
      '**DRFormer** (NC 2022) [200] & ViT-B/16 & 89.96 & 81.30 & 85.68 & 91.08 & 88.30 & 82.47 & 80.27 & 87.60 & 88.49 & 88.04 \\\\\n' +
      '**VAC-Combine** (IJCV 2022) [201] & ResNet50 & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & 82.19 & 80.66 & 88.72 & 88.10 & 88.41 \\\\\n' +
      '**DART** (AAAI 2022) [202] & ResNet50 & 87.07 & 78.88 & 87.87 & 87.03 & 86.40 & 83.54 & 80.13 & 87.01 & 89.19 & 88.09 \\\\\n' +
      '**CGCCN** (TMM 2023) [203] & ResNet ResNet & 87.08 & 79.30 & 83.97 & 89.38 & 86.59 & - & - \\\\\n' +
      '**CAS-SAL-FR** (IJCV 2022) [204] & ResNet50 & 86.40 & 79.93 & 87.03 & 87.18 & 82.86 & 79.64 & 86.81 & 87.79 & 85.18 \\\\\n' +
      '**PromptPAR** (arXiv24) [205] & ViT-L/14 & 88.76 & 82.84 & 89.04 & 89.74 & 89.18 & 87.47 & 83.78 & 89.27 & 91.70 & 90.15 \\\\\n' +
      '**SequencePAR** (arXiv24) [206] & ViT-L/14 & - & 84.92 & 90.44 & 90.73 & 90.46 & - & 83.94 & 90.38 & 90.23 & 90.10 \\\\ \\hline\n' +
      '**VTB** (TCSVT 2022) [207] & ViT-B/16 & 85.31 & 79.60 & 86.76 & 87.17 & 86.71 & 83.72 & 80.89 & 87.88 & 89.30 & 88.21 \\\\\n' +
      '**VTB*** (TCSVT 2022) [207]** & ViT-L/14 & 86.34 & 79.59 & 86.66 & 87.82 & 86.97 & 85.30 & 81.76 & 87.87 & 90.67 & 88.86 \\\\\n' +
      '**VTB** (TCSVT 2022) [207] & ViT-S & 82.51 & 77.23 & 85.75 & 84.95 & 85.01 & 78.76 & 77.61 & 87.41 & 85.35 & 85.94 \\\\\n' +
      '**Vim-PAR** & ViT-S & 81.08 & 73.75 & 80.91 & 84.96 & 82.52 & 80.41 & 78.03 & 85.39 & 88.37 & 86.39 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE X: Comparison between different trackers on the EventVOT dataset.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c|c c} \\hline \\hline\n' +
      '**Trackers** & **Source** & **Backbone** & **SR** & **PR** & **NPR** & **Params(M)** & **FPS** \\\\ \\hline\n' +
      '**THOMP** & CVPR21 & & 39.9 & 34.8 & 48.7 & 26.3 & 26 \\\\\n' +
      '**ToMP50** & CVPR22 & 37.6 & 32.8 & 47.4 & 26.1 & 25 \\\\\n' +
      '**PIM50** & ICCV19 & 52.6 & 51.1 & 67.2 & 26.1 & 43 \\\\\n' +
      '**PIM50** & CVPR20 & 55.5 & 57.2 & 70.4 & 26.1 & 30 \\\\\n' +
      '**KYS** & ECCV20 & & 38.7 & 37.3 & 49.8 & & 20 \\\\\n' +
      '**AOTV** & **CVPR19 & & 44.4 & 40.7 & 55.5 & 84 & 30 \\\\ \\hline\n' +
      '**HDETrack** & CVPR24 & 57.8 & 62.2 & 73.5 & 92.1 & 105 \\\\\n' +
      '**AiATrack** & ECCV22 & 57.4 & 59.7 & 22.8 & 15.8 & 38 \\\\\n' +
      '**STARK** & ECCV21 & ViT & 44.5 & 39.6 & 58.7 & 28.1 & 42 \\\\\n' +
      '**TransT** & CVPR21 & 54.3 & 56.5 & 68.8 & 18.5 & 50 \\\\\n' +
      '**Misformer** & CVPR22 & & 49.9 & 49.6 & 63.0 & 35.6 & 25 \\\\\n' +
      '**SimTrack** & ECCV22 & & 54.7 & 57.5 & 69.9 & 57.8 & 40 \\\\ \\hline\n' +
      '**OSTrack** 이미지를 입력으로 하고 의료 보고서 5를 생성합니다. 실험을 위해 R2GenGPT6을 기준선으로 선택하고 IU-Xray 데이터 세트 [224]에서 성능을 평가합니다. R2GenGPT는 visual encoder(Swin Transformer[20])와 linear layer, large language model(llama-2-7B-chat[225])로 구성된다. 트레이닝 접근법은 초기에 언어 모델을 동결시키고 후속적으로 시각적 인코더 및 선형 계층을 미세 조정하는 것을 포함한다. 스윈 트랜스포머를 Vim 모델[61]로 교체하고 결과를 표 XII의 다른 방법과 비교했다. 두 모델 모두 사전 학습된 컴포넌트를 활용하기 때문에 Vision Mamba는 BLEU-4 및 ROUGE-L 점수 측면에서 Swin Transformer 모델보다 우수한 성능을 보여준다.\n' +
      '\n' +
      '각주 5: [https://github.com/Event-AHU/Medical_Image_Analysis](https://github.com/Event-AHU/Medical_Image_Analysis)\n' +
      '\n' +
      '각주 6: [https://github.com/wang-zhangyu/R2GenGPT](https://github.com/wang-zhangyu/R2GenGPT)\n' +
      '\n' +
      '### _Person/Vehicle Re-Identification_\n' +
      '\n' +
      '표 XIII에 나타낸 바와 같이, 두 개의 재식별(re-ID) 작업, 즉 사람 재식별[257]과 차량 재식별[256]에 대한 실험을 수행한다. 개인 re-ID의 경우 MSMT17 [258], Market1501 [259], DukeMTMC [260] 및 Occluded-Duke [261] 데이터 세트를 포함하여 널리 사용되는 4개의 데이터 세트가 사용된다. 이러한 데이터 세트는 서로 다른 장면에서 캡처되고 샘플은 교차 시간 범위, 폐색 및 배경 간섭과 같은 문제가 있는 카메라의 중첩 적용 범위를 가진 감시 시스템에서 수집된다. 차량 재식별을 위해 VeRi-776 [262] 및 VehicleID [263] 데이터 세트가 실험 검증에 활용된다. 보행자 샘플과 달리, 관측 시점의 변화는 또한 차량에 대해 상당한 외관 차이를 가져오며, 따라서 차량 데이터 세트에는 차량 샘플의 상이한 시점을 표시하기 위한 시점 라벨이 추가로 제공된다. 위의 데이터 세트에 대해 평가 메트릭으로 누적 매칭 특성(CMC) 곡선과 평균 평균 정밀도(mAP)를 사용한다.\n' +
      '\n' +
      'TransReID[255]와 Strong Baseline[264]와 같은 주류 프레임워크를 참조하면, ID Loss, Triplet Loss, BN Layer를 유지하고, Vim[61]과 VAMba[60]을 이용하여 CNN과 Transformer Backbones를 대체하여 Mamba의 재식별 작업 가능성을 탐색하였으며, 그 비교 결과를 표 XIII에 나타내었다. Mamba 모델에 의해 제안된 선택적 스캐닝 메커니즘(SSM)은 낮은 복잡도로 시퀀스 모델링을 가능하게 하고, Vim 및 VAMba는 2D 이미지 데이터에 대한 SSM 모델링 접근법을 제안함으로써 이를 추가로 구축한다. 복잡한 모듈 설계가 필요한 CNN 기반 모델에 비해 간단한 맘바 네트워크는 이미 효과가 있다. DeiT[265]와 ViT[19]와 같이 복잡도가 높은 모델들과 비교하여도 Vim이 제안한 양방향 스캐닝 메커니즘은 학습 파라미터가 적으며, VehicleID 데이터셋에 대한 유효성을 보인다. 대조적으로, 트랜스포머의 구조(_e.g._, 위치 임베딩 및 클래스 토큰)에 의존하지 않는 VAM바의 교차 스캐닝 메커니즘은 Market1501, DukeMTMC 및 VeRi-776 데이터 세트에서 유사한 결과를 달성했다. 이러한 이유로 향후 재식별 작업에 적용할 수 있는 Mamba 기반 연구가 더 많을 것으로 기대한다.\n' +
      '\n' +
      '## 5 도전과 기회\n' +
      '\n' +
      'State Space Model은 많은 응용 분야에서 널리 연구되고 적용되었지만, 이러한 방향으로의 연구는 아직 초기 단계에 있다. 독자들이 프론티어를 신속하게 파악할 수 있도록 하기 위해 본 논문에서는 주목할 만한 몇 가지 연구 포인트를 제시한다.\n' +
      '\n' +
      '\\(\\bullet\\)**현재 SSM 모델은 여전히 변압기 네트워크의 주류보다 열등한 성능을 수행합니다.** 섹션 4에 보고된 실험 결과에서 다음과 같이 확인할 수 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c} \\hline \\hline\n' +
      '**Methods** & **Backbone** & **CIDEr** & **BLEU-4** & **ROUGE-L** \\\\ \\hline R2Gen [226] & CNN & 0.398 & 0.165 & 0.371 \\\\ KERP [227] & CNN & 0.280 & 0.162 & 0.339 \\\\ HRGP [228] & CNN & 0.343 & 0.151 & 0.322 \\\\ MKG [229] & CNN & 0.304 & 0.147 & 0.367 \\\\ PPKPC [230] & CNN & 0.351 & 0.168 & 0.376 \\\\ MGSK [231] & CNN & 0.382 & 0.178 & 0.381 \\\\ CA [232] & ResNet-50 & - & 0.169 & 0.381 \\\\ CMCL [233] & CNN & - & 0.162 & 0.378 \\\\ DCL [234] & CNN & 0.586 & 0.163 & 0.383 \\\\ \\hline R2GenGPT & Swin-B & 0.524 & 0.152 & 0.352 \\\\ R2GenGPT & Vim-S & 0.388 & 0.152 & 0.355 \\\\ R2GenGPT & Vim-S* & 0.382 & 0.171 & 0.371 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XII: Comparison between the performance of R2Gen-GPT-Vim-Small and with other methods on IU-Xray dataset. R2Gen-GPT-Vim-S* and R2GenGPT-Vim-S denote the Vim-S are initialized with and without pre-trained parameters, respectively.\n' +
      '\n' +
      '도. 13: EventVOT 데이터셋에 대한 추적 결과의 시각화.\n' +
      '\n' +
      'SSM에 기반한 성능 향상을 위한 여백. 이미지넷[2]과 같은 대규모 데이터 세트에서 미리 훈련된 SSM은 많은 다운스트림 작업에서 중요한 역할을 하지만 기본, 대규모 및 대규모 버전의 SSM은 거의 출시되지 않는다. 우리는 이것이 CV 작업에서 높은 성능에 장애가 될 수 있다고 믿는다.\n' +
      '\n' +
      '\\(\\bullet\\)** GPU 사용에서 SSM의 이점은 추가 탐색 및 연구 가치가 있습니다.* * 실험에 따르면 메모리 소비는 일부 다운스트림 태스크에서 트랜스포머 네트워크와 비슷하거나 낮습니다. 이러한 측면에서 상당한 개선이 관찰될 수 있지만, 일부 과제는 그렇지 않다. 낮은 GPU 메모리 소모량을 마이닝하는 연구는 더 많은 탐색과 연구가 필요하다.\n' +
      '\n' +
      '\\(\\bullet\\)** 고해상도 또는 장기 비전 데이터에서 이점을 더 탐구하는 것은 관심과 연구에 가치가 있는 방향입니다.* * SSMs 아키텍처는 이론적으로 모델의 복잡성을 크게 감소시키므로 고해상도 데이터(원격 감지 데이터, X선 의료 이미지) 또는 장기 시퀀스 데이터(장기 비디오 프레임)에 대한 모델링 기능은 큰 가치가 있습니다. 그러나 트랜스포머 네트워크와 같은 다른 강력한 모델을 사용하면 이러한 측면이 잘 해결되지 않는다.\n' +
      '\n' +
      '\\(\\bullet\\)** SSMs 아키텍처를 사용하여 미리 훈련된 빅 모델입니다.* * 미리 훈련된 빅 모델 시대에 심층 신경망의 스케일링은 일반적인 인공 지능에 중요한 단계입니다. 현재 빅모델은 CNN 또는 Transformer 네트워크를 기반으로 구축되며, SSM 구조를 채택하는 경우는 거의 없다. 최근 AI21랩스가 출시한 잠바[136]는 트랜스포머, 맘바, 모에(Mixture-of-Experts)를 융합해 구축한 참신한 대형 언어 모델이다. 최대 256K 토큰의 컨텍스트 길이의 입력을 지원하고 Mixtral-8x7B [266] 및 Llama-2 7OB [225]와 유사한 성능을 달성한다. 순수한 맘바 또는 하이브리드 아키텍처를 구축하는 연구는 사전 훈련된 빅 모델의 유망한 방향이 될 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)** SSMs 아키텍처를 사용하여 다중 모달 학습** 초기 다중 모달 관련 작업은 모달리티별 및 모달리티 공유 표현을 학습하는 방법에 중점을 둡니다. 트랜스포머 네트워크의 영향을 받아, 현재의 멀티모달 알고리즘들은 일반적으로 단일화된 트랜스포머 네트워크에서 다수의 큐들을 직접 인코딩하고 융합한다[267, 268]. 따라서, 추론 단계의 비용은 단일 모달리티와만 두 번 비교될 수 있다. 비용에 민감한 멀티모달 학습을 위한 새로운 SSM 기반 백본을 설계하는 방법은 중요한 연구 주제이다.\n' +
      '\n' +
      '\\(\\bullet\\)** SSM에 대한 새로운 검색 연산자를 개발합니다.* * 검색은 SSM 아키텍처의 핵심 연산자이며 1D 및 2D 데이터는 일반적으로 다른 검색 메커니즘으로 처리됩니다. 예를 들어 VMamba [60]은 _CSM_ 을 사용하여 이미지를 스캔하고(scan expand), 네 개의 출력 피쳐를 최종 2D 피쳐 맵으로 병합합니다. 보다 특수한 원격탐사 데이터를 다루기 위해, 일부 연구자들은 보다 포괄적인 특징을 얻기 위해 왜곡된 특징 표현을 포착하기 위한 추가적인 스캐닝 메커니즘을 제안했다[139]. 다른 스캔 방식의 비교는 그림 8에서 찾을 수 있다. 따라서 SSM의 특징 학습을 향상시키기 위해 새로운 스캔 방식을 설계하는 것은 당연하다. 예를 들어, 포인트 클라우드 및 이벤트 스트림들을 더 잘 인코딩하기 위해 새로운 트랙-변경 스캔 방법들을 개발하는 것이 가능하다.\n' +
      '\n' +
      '\\(\\bullet\\)** SSM의 일반화 성능은 여전히 주의와 추가 연구 및 개선이 필요하다.** SSM은 제한된 수용 분야와 CNN 및 트랜스포머의 더 큰 복잡성에 비해 선형 복잡성과 전역 수용 분야를 가지고 있어 도메인 일반화 분야에서 더 큰 장점과 잠재력을 가질 수 있다. 그러나, 현재의 SSM 기반 네트워크들은 DGMamba[120]에 언급된 바와 같이 제한된 도메인 일반화 능력을 예시한다. Long et al. [120]은 Hidden State Suppressing (HSS) 및 Semantic-aware Patch Refining (SPR) 전략을 제안함으로써 숨겨진 상태 및 부적절한 스캔 메커니즘의 관점에서 이 문제를 해결하려고 시도한다. 도메인 일반화의 전반적인 성능을 더욱 향상시키기 위해 더 많은 통찰력과 개선이 수행될 수 있다고 믿는다.\n' +
      '\n' +
      '\\(\\bullet\\)**최신 SSM 모델을 사용 하 여 기존 심층 신경망 모델에 권한을 부여 합니다.* * 딥 러닝의 세 번째 웨이브의 초기 단계에서는 지식 증류, 피라미드 구조, 네트워크 내 네트워크 [269], 확산 모델, GAN 등 많은 영리한 신경망 모듈 또는 설계가 제안 됩니다. 이러한 성공적인 모듈을 기반으로 SSM을 향상시키거나 SSM을 이러한 모듈에 도입하면 성능이 향상될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c|c c|c c||c c|c c} \\hline \\hline  & & \\multicolumn{2}{c}{**MSMT17**} & \\multicolumn{2}{c}{**Market1501**} & \\multicolumn{2}{c}{**DukeMTMC**} & \\multicolumn{2}{c}{**Occluded-Duke**} & \\multicolumn{2}{c}{**Veti-776**} & \\multicolumn{2}{c}{**VehicleID**} \\\\\n' +
      '**Backbone** & **Method** & mAP & R1 & mAP & R1 & mAP & R1 & mAP & R1 & **Method** & mAP & R1 & R1 & R5 \\\\ \\hline \\hline  & CBN [235] & 42.9 & 72.8 & 77.3 & 91.3 & 67.3 & 82.5 & - & - & PFReID [236] & 72.5 & 93.3 & 72.6 & 88.6 \\\\  & OSNet [237] & 52.9 & 78.7 & 84.9 & 94.8 & 73.5 & 88.6 & - & - & SAN [238] & 72.5 & 93.3 & 79.7 & 94.3 \\\\  & MCN [239] & 52.1 & 76.9 & 86.9 & 95.7 & 78.4 & 88.7 & - & - & UMTS [240] & 75.9 & 95.8 & 80.9 & 87.0 \\\\  & RAG-SC [241] & 57.5 & 80.3 & 88.4 & 96.1 & - & - & - & - & VANet [242] & 66.3 & 89.8 & 83.3 & 96.0 \\\\  & SAN [243] & 55.7 & 79.2 & 88.0 & 96.1 & 75.7 & 87.9 & - & - & SPAN [244] & 68.9 & 94.0 & - & - \\\\  & SCSN [245] & 58.5 & 83.8 & 88.5 & 95.7 & 79.0 & 91.0 & - & - & PGAN [246] & 79.3 & 96.5 & 78.0 & 93.2 \\\\  & ABDNet [247] & 60.8 & 82.3 & 88.3 & 95.6 & 78.6 & 89.0 & - & - & PVEN [248] & 79.5 & 95.6 & 84.7 & 97.0 \\\\  & PGFA [249] & - & - & 76.8 & 91.2 & 65.5 & 82.6 & 37.3 & 51.4 & SAVER [250] & 79.6 & 96.4 & 79.9 & 95.2 \\\\  & HOReID [251] & - & - & 84.9 & 94.2 & 75.6 & 86.9 & 43.8 & 55.1 & CFVMNet [252] & 77.1 & 95.3 & 81.4 & 94.1 \\\\  & ISP [253] & - & - & 88.6 & 95.3 & 80.0 & 89.6 & 52.3 & 62.8 & GLAMOR [254] & 80.3 & 96.5 & 78.6 & 93.6 \\\\ \\hline  & DeiT-B/16 [255] & 61.4 & 81.9 & 86.6 & 94.4 & 78.9 & 89.3 & 53.1 & 60.6 & DeiT-B/16 [255] & 78.4 & 95.9 & 83.1 & 96.8 \\\\\n' +
      '**Transformer** & ViT-B/16 [255] & 61.0 & 81.8 & 86.8 & 94.7 & 79.3 & 88.8 & 53.1 & 60.5 & ViT-B/16 [255] & 78.2 & 96.5 & 82.3 & 96.1 \\\\  & VehicleMTA [256] & - & - & - & - & - & - & - & VehicleMTAE [256] & 85.6 & 97.9 & - & - \\\\ \\hline  & VimT-1/6 & 40.1 & 62.6 & 75.7 & 89.4 & 66.5 & 81.8 & 35.4 & 45.1 & Vim-T/16 & 62.9 & 89.2 & 67.0 & 88.2 \\\\\n' +
      '**Mamba** & ViT-S/16 & 42.2 & 66.2 & 77.5 & 89.7 & 67.4 & 83.0 & 40.8 & 51.3 & Vim-S/16 & 61.6 & 89.6 & 78.2 & 94.8 \\\\  & VMamba-T/16 & 51.0 & 75.6 & 83.3 & 92.8 & 74.9 & 87.3 & 49.4 & 58.3 & VMamba-T/16 & 77.3 & 95.9 & 78.5 & 93.5 \\\\  & VMamba-B/16 & 51.1 & 75.3 & 84.3 & 93.2 & 77.4 & 88.0 & 48.1 & 57.4 & VMamba-B/16 & 77.5 & 95.6 & 82.5 & 96.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Comparison with methods based on CNN and Transformer on Person Re-identification and Vehicle Re-identification datasets.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:27]\n' +
      '\n' +
      'Langer, L. J. Colwell, and A. Weller, "Rethinking attention with performers," _arXiv preprint arXiv:2009.14794_, 2020.\n' +
      '* [5] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim, "Gated linear attention transformers with hardware-efficient training," _arXiv preprint arXiv:2312.06635_, 2023.\n' +
      '* [6] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [7] E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, and C. Re, "Stnd: Modeling images and videos as multidimensional signals with state spaces," in _Proceedings of Advances in Neural Information Processing Systems_, 2022, pp. 2846-2861.\n' +
      '* [8] J. N. Yan, J. Gu, and A. M. Rush, "Diffusion models without _arXiv preprint arXiv:2311.18527_, 2023.\n' +
      '* [9] V. T. Hu, S. A. Baumann, M.-S. Gui, O. Grebenkova, P. Ma, J. S. Fischer, and B. Ommer, "Zigma: A di-style zigzag mamba diffusion model," _arXiv preprint arXiv:2403.13802_, 2024.\n' +
      '* [10] Z. Fei, M. Fan, C. Yu, and J. Huang, "Scalable diffusion models with state space backbone," _arXiv preprint arXiv:2402.05608_, 2024.\n' +
      '* [11] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Re, "Combining recurrent, convolutional, and continuous-time models with linear state space layers," in _Proceedings of Advances in Neural Information Processing Systems_, 2021, pp. 572-585.\n' +
      '* [12] R. E. Kalman, "A new approach to linear filtering and prediction problems," _Journal of Basic Engineering_, vol. 82, no. 1, pp. 35-45, 1960.\n' +
      '* [13] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re, "Hippo: Recurrent memory with optimal polynomial projections," _arXiv preprint arXiv:2008.07669_, 2020.\n' +
      '* [14] A. Gu, K. Goel, A. Gupta, and C. Re, "On the parameterization and initialization of diagonal state space models," in _Proceedings of Advances in Neural Information Processing Systems_, 2022, pp. 35971-35983.\n' +
      '* [15] A. Gupta, A. Gu, and J. Berant, "Diagonal state spaces are as effective as structured state spaces," _Proceedings of Advances in Neural Information Processing Systems_, pp. 22 982-22 994, 2022.\n' +
      '* [16] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De, "Resurrecting recurrent neural networks for long sequences," in _Proceedings of the International Conference on Machine Learning_, 2023, pp. 26 670-26 698.\n' +
      '* [17] A. Gu, I. Johnson, A. Thalsina, A. Rudra, and C. Re, "How to train your hipro: State space models with generalized orthogonal basis projections," _arXiv preprint arXiv:2206.120370_, 2022.\n' +
      '* [18] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, "Long range language modeling via gated state spaces," _arXiv preprint arXiv:2206.13947_, 2022.\n' +
      '* [19] Y. Du, X. Liu, and Y. Chua, "Spiking structured state space model for monaural speech enhancement," _arXiv preprint arXiv:2309.03641_, 2023.\n' +
      '* [20] X. Jiang, C. Han, and N. Mesgarani, "Dual-path mamba: Short and long-term bidirectional selective structured state space models for speech separation," _arXiv preprint arXiv:2403.12857_, 2024.\n' +
      '* [21] K. Li and G. Chen, "Spmamba: State-space model is all you need in speech separation," _arXiv preprint arXiv:2403.02063_, 2024.\n' +
      '* [22] R. Garazzi, J. Siems, S. Schrodi, T. Brox, and F. Hutter, "Is mamba capable of in-context learning?" _arXiv preprint arXiv:2402.03170_, 2024.\n' +
      '* [23] B. Qi, J. Gao, D. Li, K. Zhang, J. Liu, L. Wu, and B. Zhou, "St++: Elevating long sequence modeling with state memory reply," 2024. [Online]. Available: [https://openreview.net/forum:id=bdmw4qjdf19](https://openreview.net/forum:id=bdmw4qjdf19)\n' +
      '* [24] S. Zuo, X. Liu, J. Jiao, D. X. Charles, E. Manavoglu, T. Zhao, and J. Gao, "Efficient long sequence modeling via state space augmented transformer," _arXiv preprint arXiv:2212.08136_, 2022.\n' +
      '* [25] W. He, K. Han, Y. Tang, C. Wang, Y. Yang, T. Guo, and Y. Wang, "Densenmamba: State space models with dense hidden connection for efficient large language models," _arXiv preprint arXiv:2403.00818_, 2024.\n' +
      '* [26] Z. Yang, A. Mitra, S. Kwon, and H. Yu, "ClinicalLambda: A generative clinical language model on longitudinal clinical notes," _arXiv preprint arXiv:2403.05795_, 2024.\n' +
      '* [27] A. R. de Sousa Porfitiro Correia and L. A. Alexandre, "Music dance as language translation using sequence models," _arXiv preprint arXiv:2403.15569_, 2024.\n' +
      '* [28] C. Wang, O. Tsepa, J. Ma, and B. Wang, "Graph-mamba: Towards long-range graph sequence modeling with selective state spaces," _arXiv preprint arXiv:2402.00789_, 2024.\n' +
      '* [29] S. Tang, J. A. Dunnmon, Q. Liangqiong, K. K. Saab, T. Baykaner, C. Lee-Messer, and D. L. Rubin, "Modeling multivariate biosignals with graph neural networks and structured state space models," in _Proceedings of the International Conference on Learning Representations Workshops_, 2023.\n' +
      '* [30] A. Behrova and F. Hashemi, "Graph mamba: Towards learning on graphs with state space models," _arXiv preprint arXiv:2402.08678_, 2024.\n' +
      '* [31] G. Bachmann and V. Nagarajan, "The pitfalls of next-token prediction," _arXiv preprint arXiv:2403.06963_, 2024.\n' +
      '* [32] L. Li, H. Wang, W. Zhang, and A. Coster, "Stg-mamba: Spatial-temporal graph learning via selective state space model," _arXiv preprint arXiv:2403.12418_, 2024.\n' +
      '* [33] M. M. Islam and G. Bertasius, "Long movie clip classification with state-space video models," in _Proceedings of European Conference on Computer Vision_, 2022, pp. 87-104.\n' +
      '* [34] J. T. Smith, A. Warrington, and S. Linderman, "Simplified state space layers for sequence modeling," in _Proceedings of the International Conference on Learning Representations_, 2022.\n' +
      '* [35] J. Wang, W. Zhu, P. Wang, X. Yu, L. Liu, M. Omar, and R. Hamid, "Selective structured state-spaces for long-form video understanding," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 6387-6397.\n' +
      '* [36] D. Hafner, J. Fasukonis, J. Ba, and T. Lillicrap, "Mastering diverse domains through world models," _arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* [37] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu, "Vmamba: Visual state space model," _arXiv preprint arXiv:2401.10166_, 2024.\n' +
      '* [38] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, "Vision mamba: Efficient visual representation learning with bidirectional state space model," _arXiv preprint arXiv:2401.09417_, 2024.\n' +
      '* [39] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu, "Segmamba: Long-range sequential modeling mambo for 3d medical image segmentation," _arXiv preprint arXiv:2401.13560_, 2024.\n' +
      '* [40] J. Ma, F. Li, and B. Wang, "U-mamba: Enhancing long-range dependency for biomedical image segmentation," _arXiv preprint arXiv:2401.04722_, 2024.\n' +
      '* [41] J. Liu, H. Yang, H.-Y. Zhou, Y. Xi, L. Yu, Y. Yu, Y. Liang, G. Shi, S. Zhang, H. Zheng _et al._, "Swin-ummaba: Mamba-based unet with imagenet-based pretraining," _arXiv preprint arXiv:2402.03302_, 2024.\n' +
      '* [42] J. Ruan and S. Xiang, "Vm-unet: Vision mamba unet for medical image segmentation," _arXiv preprint arXiv:2402.02491_, 2024.\n' +
      '* [43] H. Gong, L. Kang, Y. Wang, X. Wan, and H. Li, "nmmab: 3d biomedical image segmentation, classification and landmark detection with state space model," _arXiv preprint arXiv:2402.03526_, 2024.\n' +
      '* [44] Z. Wang, J.-Q. Zheng, Y. Zhang, G. Cui, and L. Li, "Mamba-unet: Unet-like pure visual mamba for medical image segmentation," _arXiv preprint arXiv:2402.05079_, 2024.\n' +
      '* [45] S. Li, H. Singh, and A. Grover, "Mamba-nd: Selective state space modeling for multi-dimensional data," _arXiv preprint arXiv:2402.05892_, 2024.\n' +
      '* [46] Z. Zheng and J. Zhang, "Fd-vision mamba for endoscopic exposure correction," _arXiv preprint arXiv:2402.06378_, 2024.\n' +
      '* [47] Z. Wang and C. Ma, "Semi-mamba-unet: Pixel-level contrastive cross-supervised visual mamba-based unet for semi-supervised medical image segmentation," _arXiv preprint arXiv:2402.07245_, 2024.\n' +
      '* [48] Z. Ye and T. Chen, "P-mamba: Marrying perona milk diffusion with mamba for efficient pediatric echocardiographic left ventricular segmentation," _arXiv preprint arXiv:2402.08506_, 2024.\n' +
      '* [49] Z. Wang and C. Ma, "Weak-mamba-unet: Visual mamba makes cnn and vitt work better for scribble-based medical image segmentation," _arXiv preprint arXiv:2402.10887_, 2024.\n' +
      '* [50] X. He, K. Cao, K. Yan, R. Li, C. Xie, J. Zhang, and M. Zhou, "Pamamba: Effective pan-sharpening with state space model," _arXiv preprint arXiv:2402.12192_, 2024.\n' +
      '* [51] N. Agarwal, D. Suo, X. Chen, and E. Hazan, "Spectral state space models," _arXiv preprint arXiv:2312.06837_, 2023.\n' +
      '* [52] P. Mattes, R. Schlosser, and R. Herbrich, "Hieros: Hierarchical imagination on structured state space sequence world models," _arXiv preprint arXiv:2310.05167_, 2023.\n' +
      '\n' +
      '* [66] E. Baron, I. Zimerman, and L. Wolf, "A 2-dimensional state space layer for spatial inductive bias," in _Proceedings of the International Conference on Learning Representations_, 2023.\n' +
      '* [67] H. Guo, J. Li, T. Dai, Z. Ouyang, X. Ren, and S.-T. Xia, "Mambair: A simple baseline for image restoration with state-space model," _arXiv preprint arXiv:2402.15648_, 2024.\n' +
      '* [68] J. Huang, L. Yang, F. Wang, Y. Wu, Y. Nan, A. I. Aviles-Rivero, C.-B. Schonlieb, D. Zhang, and G. Yang, "Mambamir: An arbitrary-masked manbmba for joint medical image reconstruction and uncertainty estimation," _arXiv preprint arXiv:2402.18451_, 2024.\n' +
      '* [69] C.-S. Chen, G.-Y. Chen, D. Zhou, D. Jiang, and D.-S. Chen, "Resvrmamba: Fine-grained food category visual classification using selective state space models with deep residual learning," _arXiv preprint arXiv:2402.15761_, 2024.\n' +
      '* [70] T. Chen, Z. Tan, T. Gong, Q. Chu, Y. Wu, B. Liu, J. Ye, and N. Yu, "Mim-istid: Mamba-in-mamba for efficient infrared small target detection," _arXiv preprint arXiv:2403.02148_, 2024.\n' +
      '* [71] Y. Yue and Z. Li, "Medammba: Vision mambo for medical image classification," _arXiv preprint arXiv:2403.03849_, 2024.\n' +
      '* [72] H. Tang, L. Cheng, G. Huang, Z. Tan, J. Lu, and K. Wu, "Rotate to scan: Unet-like manb with triplet svm module for medical image segmentation," _arXiv preprint arXiv:2403.17701_, 2024.\n' +
      '* [73] Z. Fang, Y. Wang, Z. Wang, J. Zhang, X. Ji, and Y. Zhang, "Mammil: Multiple instance learning for whole slide images with state space models," _arXiv preprint arXiv:2403.05160_, 2024.\n' +
      '* [74] K. Li, X. Li, Y. Wang, Y. He, Y. Wang, L. Wang, and Y. Qiao, "Video/mambo: State space model for efficient video understanding," _arXiv preprint arXiv:2403.06977_, 2024.\n' +
      '* [75] J. Wang, J. Chen, D. Chen, and J. Wu, "Large window-based manbure under for medical image segmentation: Beyond convolution and self-attention," _arXiv preprint arXiv:2403.07332_, 2024.\n' +
      '* [76] C. Cheng, H. Wang, and H. Sun, "Activating wider areas in image super-resolution," _arXiv preprint arXiv:2403.08330_, 2024.\n' +
      '* [77] Y. Schiff, C.-H. Kao, A. Gokaslan, T. Dao, A. Gu, and V. Kuleshov, "Caducucus: Bi-directional equivariant long-range dna sequence modeling," _arXiv preprint arXiv:2403.03234_, 2024.\n' +
      '* [78] Y. Zhang, W. Yan, K. Yan, C. P. Lam, Y. Qiu, P. Zheng, R. S.-Y. Tang, and S. S. Cheng, "Motion-guided dual-camera tracker for low-cost skill evaluation of gastric endoscopy," _arXiv preprint arXiv:2403.05146_, 2024.\n' +
      '* [79] W. Liao, Y. Zhu, X. Wang, C. Pan, Y. Wang, and L. Ma, "Lightm-unet: Mambas assists in lightweight unert for medical image segmentation," _arXiv preprint arXiv:2403.05246_, 2024.\n' +
      '* [80] G. Chen, Y. Huang, J. Xu, B. Pei, Z. Chen, Z. Li, J. Wang, K. Li, T. Lu, and L. Wang, "Video manbure: State space model as a versatile alternative for video understanding," _arXiv preprint arXiv:2403.09626_, 2024.\n' +
      '* [81] M. Zhang, Y. Yu, L. Gu, T. Lin, and X. Tao, "Vm-unet-v2 rethinking vision manbure went for medical image segmentation," _arXiv preprint arXiv:2403.09157_, 2024.\n' +
      '* [82] T. Huang, X. Pei, S. You, F. Wang, C. Qian, and C. Xu, "Local-mamba: Visual state space model with windowed selective scan," _arXiv preprint arXiv:2403.09338_, 2024.\n' +
      '* [83] Z. Xu, Y. Lin, H. Han, S. Yang, R. Li, Y. Zhang, and X. Li, "Mambatalk: Efficient holistic gesture synthesis with selective state space models," _arXiv preprint arXiv:2403.09471_, 2024.\n' +
      '* [84] X. Pei, T. Huang, and C. Xu, "Efficientvmmba: Atrous selective scan for light weight visual mamba," _arXiv preprint arXiv:2403.09977_, 2024.\n' +
      '* [85] C. Du, Y. Li, and C. Xu, "Understanding robustness of visual state space models for image classification," _arXiv preprint arXiv:2403.10935_, 2024.\n' +
      '* [86] Y. Shi, B. Xia, X. Jin, X. Wang, T. Zhao, X. Xia, X. Xiao, and W. Yang, "Vmambair: Visual state space model for image restoration," _arXiv preprint arXiv:2403.11423_, 2024.\n' +
      '* [87] T. Guo, Y. Wang, and C. Meng, "Mambamorph: a mamba-based backbone with contrastive feature learning for deformable mr-ct registration," _arXiv preprint arXiv:2401.13394_, 2024.\n' +
      '* [88] Y. Yang, Z. Xing, and L. Zhu, "Vivim: a video vision mambo for medical video object segmentation," _arXiv preprint arXiv:2401.14168_, 2024.\n' +
      '* [89] J. Xie, R. Liao, Z. Zhang, S. Yi, Y. Zhu, and G. Luo, "Promaxmba: Prompt-mamba for polyp segmentation," _arXiv preprint arXiv:2403.13660_, 2024.\n' +
      '* [90] R. Wu, Y. Liu, P. Liang, and Q. Chang, "H-vmunet: High-order vision mamu met for medical image segmentation," _arXiv preprint arXiv:2403.13642_, 2024.\n' +
      '* [91] C. Yang, Z. Chen, M. Espinosa, L. Ericsson, Z. Wang, J. Liu, and E. J. Crowley, "Plainmba: Improving non-hierarchoral mamba in visual recognition," _arXiv preprint arXiv:2403.17695_, 2024.\n' +
      '* [92] K. S. Sanjadi, M. T. Hossain, M. S. S. Junayed, and D. M. M. Uddin, "Integrating mamba sequence model and hierarchical upsampling network for accurate semantic segmentation of multiple sclerosis lesion," _arXiv preprint arXiv:2403.17432_, 2024.\n' +
      '* [93] Y. Tang, P. Dong, Z. Tang, X. Chu, and J. Liang, "Vmrnn: Integrating vision mambo and lstm for efficient and accurate spatiotemporal forecasting," _arXiv preprint arXiv:2403.163636_, 2024.\n' +
      '* [94] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang, "Gamba: Marry gaussian splitting with mambo for single view 3d reconstruction," _arXiv preprint arXiv:2403.18795_, 2024.\n' +
      '* [95] Z. Wang, J.-Q. Zheng, C. Ma, and T. Guo, "Vmambamorph: a visual mamba-based framework with cross-scan module for deformable 3d image registration," _arXiv preprint arXiv:2404.05105_, 2024.\n' +
      '* [96] J. Hao, L. He, and K. F. Hung, "T-mamba: Frequency-enhanced gated long-range dependency for tooth 3d cbct segmentation," _arXiv preprint arXiv:2404.01065_, 2024.\n' +
      '* [97] W. Li, X. Hong, and X. Fan, "Spikemba: Multi-modal spiking saliency mamba for temporal video grounding," _arXiv preprint arXiv:2404.01174_, 2024.\n' +
      '* [98] X. Ma, X. Zhang, and M.-O. Pun, "Rsqamba: Visual state space model for remote sensing images semantic segmentation," _arXiv preprint arXiv:2404.02457_, 2024.\n' +
      '* [99] H. Chen, J. Song, C. Han, J. Xia, and N. Yokoya, "Changemmaba: Remote sensing change detection with spatio-temporal state space model," _arXiv preprint arXiv:2404.03425_, 2024.\n' +
      '* [100] M. Shahab Sepehri, Z. Fabian, and M. Soltanolkotabi, "Serpent: Scalable and efficient image restoration via multi-scale structured state space models," _arXiv preprint arXiv:2403.17902_, 2024.\n' +
      '* [101] Y. Yang, C. Ma, J. Yao, Z. Zhong, Y. Zhang, and Y. Wang, "Remember: Referring image segmentation with mamba twister," _arXiv preprint arXiv:2403.17839_, 2024.\n' +
      '* [102] Q. Wang, C. Wang, Z. Lai, and Y. Zhou, "Insectmamba: Insect pest classification with state space model," _arXiv preprint arXiv:2404.03611_, 2024.\n' +
      '* [103] Q. Zhu, Y. Cai, Y. Fang, Y. Yang, C. Chen, L. Fan, and A. Nguyen, "Samba: Semantic segmentation of remotely sensed images with state space model," _arXiv preprint arXiv:2404.01705_, 2024.\n' +
      '* [104] A. Behrouz, M. Santactaterina, and R. Zabib, "Mambamixer: Efficient selective state space models with dual token and channel selection," _arXiv preprint arXiv:2403.19888_, 2024.\n' +
      '* [105] R. Wu, Y. Liu, P. Liang, and Q. Chang, "Ultralight vvm-unet: Parallel vision mamba significantly reduces parameters for skin lesion segmentation," _arXiv preprint arXiv:2403.20035_, 2024.\n' +
      '* [106] B. Zou, Z. Guo, X. Hu, and H. Ma, "Rhythmamamba: Fast remote physiological measurement with arbitrary length videos," _arXiv preprint arXiv:2404.06483_, 2024.\n' +
      '* [107] H. He, Y. Bai, J. Zhang, Q. He, H. Chen, Z. Gan, C. Wang, X. Li, G. Tian, and L. Xie, "Mambaad: Exploring state space models for multi-class unsupervised anomaly detection," _arXiv preprint arXiv:2404.06564_, 2024.\n' +
      '* [108] S. Chaudhuri and S. Bhattacharya, "Simba: Mamba augmented u-shifengton for skeletal action recognition in videos," _arXiv preprint arXiv:2404.07645_, 2024.\n' +
      '* [109] A. Archit and C. Pape, "Vim-unet: Vision mambo for biomedical segmentation," _arXiv preprint arXiv:2404.07705_, 2024.\n' +
      '* [110] S. Long, Q. Zhou, X. Li, X. Lu, C. Ying, Y. Luo, L. Ma, and S. Yan, "Dpmamba: Domain generalization via generalized state space model," _arXiv preprint arXiv:2404.07794_, 2024.\n' +
      '* [111] S. Peng, X. Zhu, H. Deng, Z. Lei, and L.-J. Deng, "Fusionmamba: Efficient image fusion with state space model," _arXiv preprint arXiv:2404.07932_, 2024.\n' +
      '* [112] A. Gu, A. Gupta, K. Goel, and C. Re, "On the parameterization and initialization of diagonal state space models," _arXiv preprint arXiv:2206.11893_, 2022.\n' +
      '* [113the Association for Computational Linguistics: EMNLP 2023, pp.14 048-14 077].\n' +
      '* [126] Y. Duan, W. Wang, Z. Chen, X. Zhu, L. Lu, T. Lu, Y. Qiao, H. Li, J. Dai, and W. Wang, "Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures," _arXiv preprint arXiv:2403.02308_, 2024.\n' +
      '* [127] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, "Retentive network: A successor to transformer for large language models," _arXiv preprint arXiv:2307.08621_, 2023.\n' +
      '* [128] X. Ma, C. Zhou, X. Kong, J. He, L. Gui, G. Neubig, J. May, and L. Zettlemoyer, "Mega: Moving average equipped gated attention," in _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [129] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, "Hungry hungry hungry hungry hungos: Towards language modeling with state space models," in _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [130] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind, "An attention free transformer," _arXiv preprint arXiv:2105.14103_, 2021.\n' +
      '* [131] H. Hou and F. R. Yu, "Rwkv-ts: Beyond traditional recurrent neural network for fine series tasks," _arXiv preprint arXiv:2401.09093_, 2024.\n' +
      '* [132] Z. Zhu, W. Shao, and D. Jiao, "Tls-rwkv: Real-time online action detection with temporal label smoothing," _Neural Processing Letters_, vol. 56, no. 2, pp. 1-13, 2024.\n' +
      '* [133] Z. Fei, M. Fan, C. Yu, D. Li, and J. Huang, "Diffusion-rwkv: Scaling rkwv-like architectures for diffusion models," _arXiv preprint arXiv:2404.04478_, 2024.\n' +
      '* [134] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, "Attention is all you need in speech separation," in _Proceedings of International Conference on Acoustics, Speech and Signal Processing_, 2020, pp. 21-25.\n' +
      '* [135] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B. Kim, and S. Watanabe, "Tf-grindet: Making time-frequency domain models great again for monanural speaker separation," in _Proceedings of International Conference on Acoustics, Speech and Signal Processing_, 2022, pp. 1-5.\n' +
      '* [136] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meiriony, Y. Belinkov, S. Shalev-Shwartz _et al._, "Jamba: A hybrid transformer-mamba language model," _arXiv preprint arXiv:2403.19887_, 2024.\n' +
      '* [137] S. Li, T. Zhu, F. Duan, L. Chen, H. Ning, and Y. Wan, "Harmamba: Efficient wearable sensor human activity recognition based on bidirectional selective ssm," _arXiv preprint arXiv:2403.20183_, 2024.\n' +
      '* [138] J. X. Yang, J. Zhou, J. Wang, H. Tian, and A. W. C. Liew, "Hismamba: Hyperspectral imaging efficient feature learning with bidirectional state space for classification," _arXiv preprint arXiv:2404.00272_, 2024.\n' +
      '* [139] S. Zhao, H. Chen, X. Zhang, P. Xiao, L. Bai, and W. Ouyang, "Rsmamba for large remote sensing image dense prediction," _arXiv preprint arXiv:2404.02668_, 2024.\n' +
      '* [140] Y. Ding, A. Orvieto, B. He, and T. Hofmann, "Recurrent distance filtering for graph representation learning," _arXiv preprint arXiv:2312.01538_, 2024.\n' +
      '* [141] J. Park, J. Park, Z. Xiong, N. Lee, J. Cho, S. Oymak, K. Lee, and D. Papailiopoulos, "Can mamba learn how to learn? a comparative study on in-context learning tasks," _arXiv preprint arXiv:2402.04248_, 2024.\n' +
      '* [142] N. Zucchet, S. Kobayashi, Y. Akram, J. von Oswald, M. Larcher, A. Steger, and J. Sacarmento, "Gated recurrent neural networks discover attention," _arXiv preprint arXiv:2309.01775_, 2023.\n' +
      '* [143] A. Ali, I. Zimerman, and L. Wolf, "The hidden attention of mamba models," _arXiv preprint arXiv:2403.01590_, 2024.\n' +
      '* [144] S. Yang, Y. Wang, and H. Chen, "Mambahl: Enhancing long sequence modeling with sequence reordering in computational pathology," _arXiv preprint arXiv:2403.06800_, 2024.\n' +
      '* [145] G. L. C. S. Z. Z. S. M. W. Q. Qiao Yanyuan, Yu Zheng and L. Jing, "V1-mamba: Exploring state space models for multimodal learning," _arXiv preprint arXiv:2403.13600_, 2024.\n' +
      '* [146] G. Yang, K. Du, Z. Yang, Y. Du, Y. Zheng, and S. Wang, "Cmvim: Contrastive masked vim autoencoder for 3d multimodal representation learning for ad classification," _arXiv preprint arXiv:2403.16520_, 2024.\n' +
      '* [147] H. Zhao, M. Zhang, W. Zhao, P. Ding, S. Huang, and D. Wang, "Cobra: Extending mamba to multi-modal large language model for efficient inference," _arXiv preprint arXiv:2403.14520_, 2024.\n' +
      '* [148] T. Ota, "Decision mamba: Reinforcement learning via sequence modeling with selective state spaces," _arXiv preprint arXiv:2403.19925_, 2024.\n' +
      '* [149] Z. Wan, Y. Wang, S. Yong, P. Zhang, S. Stepputtis, K. Sycara, and Y. Xie, "Sigma: Siamese mamba network for multi-modal semantic segmentation," _arXiv preprint arXiv:2404.04256_, 2024.\n' +
      '* [150] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, "Masked autoencoders are scalable vision learners," _arXiv preprint arXiv:2111.06377_, 2021.\n' +
      '* [151] A. Dosovitskiy, I. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2101.11929_, 2021.\n' +
      '* [152] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch, "Decision transformer: Reinforcement learning via sequence modeling," in _Proceedings of Advances in Neural Information Processing Systems_, 2021, pp. 15 084-15 097.\n' +
      '* [153] D. Liang, X. Zhou, X. Wang, X. Zhu, W. Xu, Z. Zou, X. Ye, and X. Bai, "Pointmamba: A simple state space model for point cloud analysis," _arXiv preprint arXiv:2402.10739_, 2024.\n' +
      '* [154] T. Zhang, X. Li, H. Yuan, S. Ji, and S. Yan, "Point cloud mamba: Point cloud learning via state space model," _arXiv preprint arXiv:2403.00762_, 2024.\n' +
      '* [155] J. Liu, R. Yu, Y. Wang, Y. Zheng, T. Deng, W. Ye, and H. Wang, "Point mamba: A novel point cloud backbone based on state space model with octree-based ordering strategy," _arXiv preprint arXiv:2403.06467_, 2024.\n' +
      '* [156] Q. Zhou, W. Yang, B. Fei, J. Xu, R. Zhang, K. Liu, Y. Luo, and Y. He, "3dmanbaipf: A state space model for iterative point cloud filtering via differentiable rendering," _arXiv preprint arXiv:2404.05522_, 2024.\n' +
      '* [157] Y. Li, W. Yang, and B. Fei, "3dmanbacomplete: Exploring structured state space model for point cloud completion," _arXiv preprint arXiv:2404.07106_, 2024.\n' +
      '* [158] N. Zubir, M. Gehrig, and D. Scaramurza, "State space models for event cameras," _arXiv preprint arXiv:2402.15584_, 2024.\n' +
      '* [159] K. Goel, A. Gu, C. Donahue, and C. Re, "It\'s raw! audio generation with state-space models," in _Proceedings of the International Conference on Machine Learning_, 2022, pp. 7616-7633.\n' +
      '* [160] J. Wang, J. N. Yan, A. Gu, and A. M. Rush, "Pretraining without attention," _arXiv preprint arXiv:2212.10544_, 2022.\n' +
      '* [161] S. Massaroli, M. Poli, D. Fu, H. Kumbong, R. Parnichkun, D. Romero, A. Timalsina, Q. McIntyre, B. Chen, A. Rudra _et al._, "Laughing hyeqn distillery: Extracting compact recurrences from convolutions," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [162] J. Smith, S. De Mello, J. Kautz, S. Linderman, and W. Byeon, "Convolutional state space models for long-range spatiotemporal modeling," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [163] C. Lu, Y. Schroecker, A. Gu, E. Parisotto, J. Foerster, S. Singh, and F. Behbahani, "Structured state space models for in-context reinforcement learning," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [164] S. Wang and Q. Li, "Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization," _arXiv preprint arXiv:2311.14495_, 2023.\n' +
      '* [165] M. Pioro, K. Ciebiera, K. Krol, J. Ludziejewski, and S. Jaszczur, "Moe-mamba: Efficient selective state space models with mixture of experts," _arXiv preprint arXiv:2014.04810_, 2024.\n' +
      '* [166] J. Wang, T. Gangavarapu, J. N. Yan, and A. M. Rush, "Mambabyte: Token-free selective state space model," _arXiv preprint arXiv:2403.13660_, 2024.\n' +
      '* [167] Q. Anthony, Y. Tokpanov, P. Glorioso, and B. Millidge, "Black-mamba: Mixture of experts for state-space models," _arXiv preprint arXiv:2402.01717_, 2024.\n' +
      '* [168] F. L. Bronne, S. Duong, M. Ravaut, A. Allauzen, N. F. Chen, V. Guigue, A. Lumbreras, L. Soulier, and P. Gallinari, "Locost: State-space models for long document abstractive summarization," _arXiv preprint arXiv:2201.17919_, 2024.\n' +
      '* [169] M. R. Samsami, A. Zholus, J. Rajendran, and S. Chandar, "Mastering memory tasks with world models," _arXiv preprint arXiv:24* [171] F. Liu and Q. Li, "From generalization analysis to optimization designs for state space models," 2024. [Online]. Available: [https://openreview.net/forum?id=E6yJMcGrI](https://openreview.net/forum?id=E6yJMcGrI)\n' +
      '* [172] A. Yu, A. Nigmentov, D. Morozov, M. W. Mahoney, and N. B. Erichson, "Robustifying state-space models for long sequences via approximate diagonalization," in _Proceedings of the International Conference on Learning Representations_, 2024.\n' +
      '* [173] E. David, J. Bellot, and S. L. Corff, "Variational quantization for state space models," 2024. [Online]. Available: [https://openreview.net/forum?id=EAkjVCH02](https://openreview.net/forum?id=EAkjVCH02)\n' +
      '* [174] D. Y. Fu, H. Kumbong, E. Nguyen, and C. Re, "Flashfftconv: Efficient convolutions for long sequences with tensor cores," _arXiv preprint arXiv:2311.05908_, 2023.\n' +
      '* [175] C. Liu, J. Lin, J. Wang, H. Liu, and J. Caverlee, "Mambatrec: Towards efficient sequential recommendation with selective state space models," _arXiv preprint arXiv:2403.03900_, 2024.\n' +
      '* [176] B. Silva, M. Contreras, S. Bandyopadhyay, Y. Ren, Z. Guan, J. Balch, K. Khezei, T. O. Baslanti, B. Shickel, A. Bibrac _et al._, "A multi-cohort study on prediction of acute brain dysfunction states using selective state space models," _arXiv preprint arXiv:2403.07201_, 2024.\n' +
      '* [177] C. Quan and X. Li, "Multichannel long-term streaming neural speech enhancement for static and moving speakers," _arXiv preprint arXiv:2403.07675_, 2024.\n' +
      '* [178] Z. Shi, "Mambatstock: Selective state space model for stock prediction," _arXiv preprint arXiv:2402.18959_, 2024.\n' +
      '* [179] R. Bhirangi, C. Wang, V. Pattabiraman, C. Majidi, A. Gupta, T. Hellebrekers, and L. Pinto, "Hierarchical state space models for continuous sequence-to-sequence modeling," _arXiv preprint arXiv:2402.10211_, 2024.\n' +
      '* [180] M. A. Ahamed and Q. Cheng, "Timemachine: A time series is worth 4 mambas for long-term forecasting," _arXiv preprint arXiv:2403.08989_, 2024.\n' +
      '* [181] Y. Zhang, Z. Lin, Y. Sun, F. Yin, and C. Fritsche, "Regularization-based efficient continual learning in deep state-space models," _arXiv preprint arXiv:2403.10123_, 2024.\n' +
      '* [182] M. Poli, A. W. Thomas, E. Nguyen, P. Ponnusamy, B. Deiseroth, K. Kersting, T. Suzuki, B. Hie, S. Ermon, C. R\'e, C. Zhang, and S. Massarodi, "Mechanistic design and scaling of hybrid architectures," _arXiv preprint arXiv:2403.17844_, 2024.\n' +
      '* [183] D. LaRocque, W. Guinont-Martin, D.-A. Duclos, P. Giguere, and F. Pomerleau, "Proprioception is all you need: Terrain classification for boreal forests," _arXiv preprint arXiv:2403.16877_, 2024.\n' +
      '* [184] Z. Wang, F. Kong, S. Feng, M. Wang, H. Zhao, D. Wang, and Y. Zhang, "Is mamba effective for time series forecasting?" _arXiv preprint arXiv:2403.11144_, 2024.\n' +
      '* [185] B. N. Patro and V. S. Agneswaran, "Simba: Simplified mambased architecture for vision and multivariate time series," _arXiv preprint arXiv:2403.15360_, 2024.\n' +
      '* [186] Z. Xu, "Rankmamba, benchmarking mamba\'s document ranking performance in the era of transformers," _arXiv preprint arXiv:2403.18276_, 2024.\n' +
      '* [187] A. S. Sharma, D. Atkinson, and D. Bau, "Locating and editing factual associations in mamba," 2024.\n' +
      '* [188] H. Yin, G. Cheng, C. J. Steinmetz, R. Yuan, R. M. Stern, and R. Dannenberg, "Modeling analog dynamic range compressors using deep learning and state-space models," _arXiv preprint arXiv:2403.16331_, 2024.\n' +
      '* [189] M. Forgione, M. Mejari, and D. Piga, "Model order reduction of deep structured state-space models: A system-theoretic approach," _arXiv preprint arXiv:2403.14833_, 2024.\n' +
      '* [190] J. Yang, Y. Li, J. Zhao, H. Wang, M. Ma, J. Ma, Z. Ren, M. Zhang, X. Yin, Z. Chen _et al._, "Uncovering selective state space model" capabilities in lifelong sequential recommendation," _arXiv preprint arXiv:2403.16371_, 2024.\n' +
      '* [191] S. Wang and B. Xue, "State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory," in _Proceedings of Advances in Neural Information Processing Systems_, 2023.\n' +
      '* [192] I. Amos, J. Berant, and A. Gupta, "Never train from scratch: Fair comparison of long-sequence models requires data-driven priors," _arXiv preprint arXiv:2310.02980_, 2023.\n' +
      '* [193] C. A. Alonso, J. Sieber, and M. N. Zeilinger, "State space models as foundation models: A control theoretic overview," _arXiv preprint arXiv:2403.16899_, 2024.\n' +
      '* [194] E. J. Olucha, B. Terzin, A. Das, and R. Toth, "On the reduction of linear parameter-varying state-space models," _arXiv preprint arXiv:2404.01817_, 2024.\n' +
      '* [195] Z. Tan, Y. Yang, J. Wan, G. Guo, and S. Z. Li, "Relation-aware pedestrian attribute recognition with graph convolutional networks," _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 34, no. 7, pp. 12055-12062, 2020.\n' +
      '* [196] J. Wu, H. Liu, J. Jiang, M. Qi, B. Ren, X. Li, and Y. Wang, "Person attribute recognition by sequence contextual relation learning," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 30, no. 10, pp. 3398-3412, 2020.\n' +
      '* [197] J. Jia, X. Chen, and K. Huang, "Spatial and semantic consistency regularizations for pedestrian attribute recognition," _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 942-951, 2021.\n' +
      '* [198] "Inter-attribute awareness for pedestrian attribute recognition," _Pattern Recognition_, vol. 131, p. 108865, 2022.\n' +
      '* [199] L. Chen, J. Song, X. Zhang, and M. Shang, "Mcfi: multi-label contrastive focal loss for deep imbalanced pedestrian attribute recognition," _Neural Computing and Applications_, vol. 34, no. 19, pp. 16 701-16 715, 2022.\n' +
      '* [200] Z. Tang and J. Huang, "Drformer: Learning dual relations using transformer for pedestrian attribute recognition," _Neurocomputing_, vol. 497, pp. 159-169, 2022.\n' +
      '* [201] H. Guo, X. Fan, and S. Wang, "Visual attention consistency for human attribute recognition," _International Journal of Computer Vision_, vol. 130, no. 4, pp. 1088-1106, 2022.\n' +
      '* [202] J. Jia, N. Gao, F. He, X. Chen, and K. Huang, "Learning disentangled attribute representations for robust pedestrian attribute recognition," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022, pp. 1069-1077.\n' +
      '* [203] H. Fan, H.-M. Hu, S. Liu, W. Lu, and S. Pu, "Correlation graph convolutional network for pedestrian attribute recognition," _IEEE Transactions on Multimedia_, vol. 24, pp. 49-60, 2020.\n' +
      '* [204] Y. Yang, Z. Tan, P. Tiwari, H. M. Pandey, J. Wan, Z. Lei, G. Guo, and S. Z. Li, "Cascaded split-and-aggregate learning with feature recombination for pedestrian attribute recognition," _International Journal of Computer Vision_, vol. 129, no. 10, pp. 2731-2744, 2021.\n' +
      '* [205] X. Wang, J. Jin, C. Li, J. Tang, C. Zhang, and W. Wang, "Pedestrian attribute recognition via clip based prompt vision-language fusion," _arXiv preprint arXiv:2312.10692_, 2023.\n' +
      '* [206] J. Jin, X. Wang, C. Li, L. Huang, and J. Tang, "Sequencepar: Understanding pedestrian attributes via a sequence generation paradigm," _arXiv preprint arXiv:2312.01640_, 2023.\n' +
      '* [207] X. Cheng, M. Jia, Q. Wang, and J. Zhang, "A simple visual-textual baseline for pedestrian attribute recognition," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 32, no. 10, pp. 6994-7004, 2022.\n' +
      '* [208] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, J. Yan, and X. Wang, "Hydraplus-net: Attentive deep features for pedestrian analysis," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2017, pp. 350-359.\n' +
      '* [209] Y. Deng, P. Luo, C. C. Loy, and X. Tang, "Pedestrian attribute recognition at far distance," in _Proceedings of the ACM International Conference on Multimedia_, 2014, pp. 789-792.\n' +
      '* [210] B. Ye, H. Chang, B. Ma, S. Shan, and X. Chen, "Joint feature learning and relation modeling for tracking: A one-stream framework," in _Proceedings of European Conference on Computer Vision_, 2022, pp. 341-357.\n' +
      '* [211] N. Wang, W. Zhou, J. Wang, and H. Li, "Transformer meets tracker: Exploiting temporal context for robust visual tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, p. 1571-1580.\n' +
      '* [211] C. Mayer, M. Danelljan, G. Bhat, M. Paul, D. P. Paudel, F. Yu, and L. V. Gool, "Transforming model prediction for tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, p. 8731-8740.\n' +
      '* [212] G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, "Learning discriminative model prediction for tracking," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, p. 6182-6191.\n' +
      '* [213] M. Danelljan, L. V. Gool, and R. Timofte, "Probabilistic regression for visual tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, p. 7183-7192.\n' +
      '* [214] G. Bhat, M. Danelljan, L. Van Gool, and R. Timofte, "Know your surroundings: Exploiting scene information for object tracking,"in _Proceedings of European Conference on Computer Vision_, 2020, p. 205-221.\n' +
      '* [21] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, "Atom: Accurate tracking by overlap maximization," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, p. 4660-4669.\n' +
      '* [22] X. Wang, S. Wang, C. Tang, L. Zhu, B. Jiang, Y. Tian, and J. Tang, "Event stream-based visual object tracking: A high-resolution benchmark dataset and a novel baseline," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2024.\n' +
      '* [23] S. Gao, C. Zhou, C. Ma, X. Wang, and J. Yuan, "Aiatrack: Attention in attention for transformer visual tracking," in _Proceedings of European Conference on Computer Vision_, 2022, p. 146-164.\n' +
      '* [24] B. Ye, H. Chang, B. Ma, and S. Shan, "Joint feature learning and relation modeling for tracking: A one-stream framework," _arXiv preprint arXiv:2203.11991_, 2022.\n' +
      '* [25] X. Chen, J. Yan, Bin Zhu, D. Wang, X. Yang, and H. Lu, "Transformer tracking," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, p. 8126-8135.\n' +
      '* [26] Y. Cui, C. Jiang, L. Wang, and W. Gangshan, "Miformor: End-to-end tracking with iterative mixed attention," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, p. 13608-13618.\n' +
      '* [27] B. Chen, P. Li, L. Bai, L. Qiao, Q. Shen, B. Li, W. Gan, W. Wu, and W. Ouyang, "Backbone is all your need: A simplified architecture for visual object tracking," in _Proceedings of European Conference on Computer Vision_, 2021, p. 375-392.\n' +
      '* [28] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, "Swin-unet: Unet-like pure transformer for medical image segmentation," in _Proceedings of European Conference on Computer Vision_, 2022, pp. 205-218.\n' +
      '* [29] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan, L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, "Preparing a collection of radiology examinations for distribution and retrieval," _Journal of the American Medical Informatics Association_, vol. 23, no. 2, pp. 304-310, 2016.\n' +
      '* [30] H. Touvron, L. Martin, K. R. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlyko, S. Batra, P. Bhargava, S. Bhosale, D. M. Bliek, I. Blekec, C. C. Ferrer, M. Chen, G. Cucurill, D. Esbub, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. S. Hartshorn, S. Hossein, R. Hou, H. Inan, M. Kardas, V. Kerrez, M. Khabsa, I. M. Klourann, A. V. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Rez Rez Rezenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambardu, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Seiolam, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [26] Z. Chen, Y. Song, T.-H. Chang, and X. Wan, "Generating radiology reports via memory-driven transformer," in _Proceedings of the Conference on Empirical Methods in Natural Language Processing_, 2020, pp. 1439-1449.\n' +
      '* [27] C. Y. Li, X. Liang, Z. Hu, and E. P. Xing, "Knowledge-driven encode, retrieve, paraphrase for medical image report generation," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019, pp. 6666-6673.\n' +
      '* [28] Y. Li, X. Liang, Z. Hu, and E. P. Xing, "Hybrid retrieval-generation reinforced agent for medical image report generation," in _Proceedings of Advances in Neural Information Processing Systems_, 2018.\n' +
      '* [29] Y. Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, and D. Xu, "When radiology report generation meets knowledge graph," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020, pp. 12910-12917.\n' +
      '* [30] F. Liu, X. Wu, S. Ge, W. Fan, and Y. Zou, "Exploring and distilling posterior and prior knowledge for radiology report generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 13753-13762.\n' +
      '* [31] S. Yang, X. Wu, S. Ge, S. K. Zhou, and L. Xiao, "Knowledge matters: Chest radiology report generation with general and specific knowledge," _Medical Image Analysis_, vol. 80, p. 102510, 2022.\n' +
      '* [32] F. Liu, C. Yin, X. Wu, S. Ge, P. Zhang, and X. Sun, "Contrastive attention for automatic chest x-ray report generation," in _Proceedings of Findings of the Association for Computational Linguistics_, 2021, pp. 269-280.\n' +
      '* [33] F. Liu, S. Ge, and X. Wu, "Competence-based multimodal curriculum learning for medical report generation," in _Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing_, 2021, pp. 3001-3012.\n' +
      '* [34] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang, "Dynamic graph enhanced contrastive learning for chest x-ray report generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 3334-3343.\n' +
      '* [35] Z. Zhuang, L. Wei, L. Xie, T. Zhang, H. Zhang, H. Wu, H. Ai, and Q. Tian, "Rethinking the distribution gap of person re-identification with camera-based batch normalization," in _Proceedings of European Conference on Computer Vision_. Springer, 2020, pp. 140-157.\n' +
      '* [36] B. He, J. Li, Y. Zhao, and Y. Tian, "Part-regularized near-duplicate vehicle re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 3997-4005.\n' +
      '* [37] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, "Omni-scale feature learning for person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 3702-3712.\n' +
      '* [38] J. Qian, W. Jiang, H. Luo, and H. Yu, "Stripe-based and attribute-aware network: A two-branch deep model for vehicle re-identification," _Measurement Science and Technology_, vol. 31, no. 9, p. 09504, 2020.\n' +
      '* [39] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, "Learning discriminative features with multiple granularities for person re-identification," in _Proceedings of the ACM International Conference on Multimedia_, 2018, pp. 274-282.\n' +
      '* [40] X. Jin, C. Lan, W. Zeng, and Z. Chen, "Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020, pp. 11165-1172.\n' +
      '* [41] Z. Zhang, C. Lan, W. Zeng, X. Jin, and Z. Chen, "Relation-aware global attention for person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 3186-3195.\n' +
      '* [42] R. Chu, Y. Sun, Y. Li, Z. Liu, C. Zhang, and Y. Wei, "Vehicle re-identification with viewpoint-aware metric learning," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 8282-8291.\n' +
      '* [43] X. Jin, C. Lan, W. Zeng, G. Wei, and Z. Chen, "Semantics-aligned representation learning for person re-identification," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2020, pp. 11 173-111 80.\n' +
      '* [44] T.-S. Chen, C.-T. Liu, C.-W. Wu, and S.-Y. Chien, "Orientation-aware vehicle re-identification with semantics-guided part attention network," in _Proceedings of European Conference on Computer Vision_, 2020, pp. 330-346.\n' +
      '* [45] X. Chen, C. Fu, Y. Zhao, F. Zheng, J. Song, R. Ji, and Y. Yang, "Salience-guided cascaded suppression network for person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 3300-3310.\n' +
      '* [46] X. Zhang, R. Zhang, J. Cao, D. Gong, M. You, and C. Shen, "Parti-guided attention learning for vehicle re-identification," _arXiv preprint arXiv:1909.06023_, 2019.\n' +
      '* [47] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang, "Abd-net: Attentive but diverse person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 8351-8361.\n' +
      '* [48] D. Meng, L. Li, X. Liu, Y. Li, S. Yang, Z.-J. Zha, X. Gao, S. Wang, and Q. Huang, "Parsing-based view-aware embedding network for vehicle re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 7103-7112.\n' +
      '* [49] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, "Pose-guided feature alignment for occluded person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 542-551.\n' +
      '* [50] P. Khorramshahi, N. Peri, J.-c. Chen, and R. Chellappa, "The devil is in the details: Self-supervised attention for vehicle re-identification," in _Proceedings of European Conference on Computer Vision_, 2020, pp. 369-386.\n' +
      '\n' +
      '* [251] G. Wang, S. Yang, H. Liu, Z. Wang, Y. Yang, S. Wang, G. Yu, E. Zhou, and J. Sun, "High-order information matters: Learning relation and topology for occluded person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 6449-6458.\n' +
      '* [252] Z. Sun, X. Nie, X. Xi, and Y. Yin, "Cfvmnet: A multi-branch network for vehicle re-identification based on common field of view," in _Proceedings of the ACM International Conference on Multimedia_, 2020, pp. 3523-3531.\n' +
      '* [253] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, "Identity-guided human semantic parsing for person re-identification," in _Proceedings of European Conference on Computer Vision_, 2020, pp. 346-363.\n' +
      '* [254] A. Suprem and C. Pu, "Looking glamorous: Vehicle re-id in heterogeneous cameras networks with global and local attention," _arXiv preprint arXiv:2002.02256_, 2020.\n' +
      '* [255] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, "Transreid: Transformer-based object re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021, pp. 15 013-15 022.\n' +
      '* [256] X. Wang, W. Wu, C. Li, Z. Zhao, Z. Chen, Y. Shi, and J. Tang, "Structural information guided multimodal pre-training for vehicle-centric perception," in _Proceedings of the AAAI Conference on Artificial Intelligence_, 2024, pp. 5624-5632.\n' +
      '* [257] X. Shu, X. Wang, X. Zang, S. Zhang, Y. Chen, G. Li, and Q. Tian, "Large-scale spatio-temporal person re-identification: Algorithms and benchmark," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 32, no. 7, pp. 4390-4403, 2021.\n' +
      '* [258] L. Wei, S. Zhang, W. Gao, and Q. Tian, "Person transfer gan to bridge domain gap for person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2018, pp. 79-88.\n' +
      '* [259] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, "Scalable person re-identification: A benchmark," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2015, pp. 1116-1124.\n' +
      '* [260] Z. Zhang, J. Wu, X. Zhang, and C. Zhang, "Multi-target, multi-camera tracking by hierarchical clustering: Recent progress on dukemtmc project," _arXiv preprint arXiv:1712.09531_, 2017.\n' +
      '* [261] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, "Pose-guided feature alignment for occluded person re-identification," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 542-551.\n' +
      '* [262] X. Liu, W. Liu, H. Ma, and H. Fu, "Large-scale vehicle re-identification in urban surveillance videos," in _Proceedings of the IEEE International Conference on Multimedia and Expo_, 2016, pp. 1-6.\n' +
      '* [263] H. Liu, Y. Tian, Y. Yang, L. Pang, and T. Huang, "Deep relative distance learning: Tell the difference between similar vehicles," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2016, pp. 2167-2175.\n' +
      '* [264] H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang, "Bag of tricks and a strong baseline for deep person re-identification," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, 2019, pp. 1487-1495.\n' +
      '* [265] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, "Training data-efficient image transformers & distillation through attention," in _Proceedings of the International Conference on Machine Learning_, 2021, pp. 10 347-10 357.\n' +
      '* [266] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l Casas, E. B. Hanna, F. Bressand _et al._, "Mirxtal of experts," _arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* [267] C. Tang, X. Wang, J. Huang, B. Jiang, L. Zhu, J. Zhang, Y. Wang, and Y. Tian, "Revisiting color-event based tracking: A unified network, dataset, and metric," _arXiv preprint arXiv:2211.11010_, 2022.\n' +
      '* [268] X. Wang, J. Huang, S. Wang, C. Tang, B. Jiang, Y. Tian, J. Tang, and B. Luo, "Long-term frame-event visual tracking: Benchmark dataset and baseline," _arXiv preprint arXiv:2403.05839_, 2024.\n' +
      '* [269] M. Lin, Q. Chen, and S. Yan, "Network in network," _arXiv preprint arXiv:1312.4400_, 2013.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
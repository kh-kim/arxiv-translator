<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# DeepSeekMoE: 혼합 전문가 언어 모델에서 궁극적인 전문가 전문화를 위한 연구\n' +
      '\n' +
      'Damai Dai\\({}^{1,2}\\)\n' +
      '\n' +
      ' 청기등({}^{1}\\)\n' +
      '\n' +
      ' 천강자오\\({}^{1,3}\\)\n' +
      '\n' +
      ' R.X. Xu\\({}^{1}\\)\n' +
      '\n' +
      ' 화좌가오\\({}^{1}\\)\n' +
      '\n' +
      ' Deli Chen\\({}^{1}\\)\n' +
      '\n' +
      ' 지하시 Li\\({}^{1}\\)\n' +
      '\n' +
      ' Wangding Zeng\\({}^{1}\\)\n' +
      '\n' +
      ' 싱카이유\\({}^{1,4}\\)\n' +
      '\n' +
      ' Y Wu\\({}^{1}\\)\n' +
      '\n' +
      ' Zhenda Xie\\({}^{1}\\)\n' +
      '\n' +
      ' Y.K. Li\\({}^{1}\\)\n' +
      '\n' +
      ' 판판황\\({}^{1}\\)\n' +
      '\n' +
      ' Fuli Luo\\({}^{1}\\)\n' +
      '\n' +
      ' 정루안\\({}^{1}\\)\n' +
      '\n' +
      ' 지팡수이\\({}^{2}\\)\n' +
      '\n' +
      ' 원봉량\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)DeepSeek-AI\n' +
      '\n' +
      '북경대학교 멀티미디어 정보처리 국가핵심실험실\n' +
      '\n' +
      '칭화대학교 학제간정보과학연구소\n' +
      '\n' +
      '난징대학교 소프트웨어 신기술 국가핵심연구소\n' +
      '\n' +
      '{daidamai, szf}@pku.edu.cn,{wenfeng.liang}@deepseek.com\n' +
      '\n' +
      '[https://github.com/deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델 시대에, Mixture-of-Experts (MoE)는 모델 파라미터들을 스케일링할 때 계산 비용들을 관리하기 위한 유망한 아키텍처이다. 그러나, GShard와 같은 기존의 MoE 아키텍쳐는 전문가들 중 상위\\(K\\)를 활성화시켜 전문가 전문화를 보장하는 데 어려움이 있다. 즉, 각 전문가는 중복되지 않고 집중된 지식을 획득하게 된다. 이에 대해 궁극적인 전문가 전문화를 위한 **DeepSeekMoE** 아키텍처를 제안합니다. 이를 위해 첫째, 전문가들을 \\(mN\\)로 세분화하고 \\(mK\\)를 활성화하여 활성화된 전문가들을 보다 융통성 있게 조합할 수 있도록 하는 전략, 둘째, \\(K_{s}\\)의 전문가들을 공유된 전문가들로 분리하는 전략, 셋째, 공통지식을 포착하고 라우팅된 전문가들의 중복성을 완화하기 위한 전략이다. 본 논문에서 제안하는 DeepSeekMoE 2B는 1.5\\(\\times\\)의 전문가 파라미터와 연산량을 갖는 GShard 2.9B와 유사한 성능을 보임을 보인다. 또한, DeepSeekMoE 2B는 MoE 모델의 상한을 설정하는 동일한 수의 총 매개변수로 조밀한 대응물의 성능에 거의 근접한다. 그 후 DeepSeekMoE를 16B 파라미터로 확장하여 LLaMA2 7B와 비교 가능한 성능을 얻었으며, 계산량은 약 40%에 불과하였다. 또한 DeepSeekMoE를 145B 매개변수로 확장하려는 예비 노력은 GShard 아키텍처에 비해 상당한 이점을 일관되게 검증하고 28.5%(심지어 18.2%)의 계산만을 사용하여 DeepSeek 67B와 유사한 성능을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근의 연구 및 관행들은 이용 가능한 충분한 트레이닝 데이터로, 증가된 파라미터들 및 계산 예산들을 갖는 스케일링 언어 모델들이 현저하게 더 강한 모델들을 산출할 수 있다는 것을 경험적으로 입증했다(Brown et al., 2020; Hoffmann et al., 2022; OpenAI, 2023; Touvron et al., 2023a). 그러나 모델을 매우 대규모로 확장하려는 노력은 또한 매우 높은 계산 비용과 관련이 있음을 인정해야 한다. 상당한 비용을 고려하여, Mixture-of-Experts(MoE) 아키텍처(Jacobs et al., 1991; Jordan and Jacobs, 1994; Shazeer et al., 2017)가 대중적인 해결책으로 부상했다. 매개 변수 크기를 조정할 수 있는 동시에 계산 비용을 적당한 수준으로 유지할 수 있습니다. Transformers(Vaswani et al., 2017)에서 MoE 아키텍처의 최근 응용은 언어 모델을 상당한 크기로 스케일링하는 성공적인 시도를 산출했다(Du et al., 2022; Fedus et al., 2021; Lepikhin et al., 2021; Zoph, 2022). 이러한 성과는 MoE 언어 모델의 상당한 잠재력과 가능성을 강조한다.\n' +
      '\n' +
      'MoE 아키텍처의 유망한 잠재력에도 불구하고, 기존의 MoE 아키텍처는 잠재적으로 지식 혼종성 및 지식 중복성의 문제로 고통받으며, 이는 전문가 전문화를 제한하며, 즉 각 전문가가 비중첩 및 집중된 지식을 획득한다. 기존의 MoE 구조는 트랜스포머의 Feed-Forward Networks(FFN)을 MoE 계층으로 대체한다. 각각의 MoE 계층은 다수의 전문가들로 구성되며, 각각은 표준 FFN과 구조적으로 동일하며, 각각의 토큰은 하나의(Fedus et al., 2021) 또는 두 개의(Lepikhin et al., 2021) 전문가들에게 할당된다. 이 아키텍처는 두 가지 잠재적인 문제를 나타냅니다. (1) **지식 하이브리드**: 기존 MoE 관행은 종종 제한된 수의 전문가(예: 8 또는 16)를 사용하므로 특정 전문가에 할당된 토큰은 다양한 지식을 포함할 가능성이 높습니다. 따라서 지정된 전문가는 동시에 활용하기 어려운 매개변수에서 매우 다양한 유형의 지식을 수집할 계획이다. (2) **지식 중복**: 서로 다른 전문가에 할당된 토큰에는 공통 지식이 필요할 수 있습니다. 결과적으로, 다수의 전문가들은 각자의 파라미터들에서 공유된 지식을 획득하는데 수렴할 수 있고, 이에 의해 전문가 파라미터들에서 중복성으로 이어질 수 있다. 이러한 문제는 기존 MoE 관행의 전문가 전문화를 일괄적으로 방해하여 MoE 모델의 이론적 상한 성능에 도달하지 못하게 한다.\n' +
      '\n' +
      '앞서 언급한 문제에 대응하여 궁극적인 전문가 전문화를 위해 특별히 설계된 혁신적인 MoE 아키텍처인 **DeepSeekMoE** 를 소개합니다. 우리의 아키텍처는 두 가지 주요 전략을 포함한다: (1) **Fine-Grained Expert Segmentation:** 매개 변수의 수를 일정하게 유지하면서 전문가를 분할함으로써 더 미세한 그레인으로 분할합니다.\n' +
      '\n' +
      '그림 1: Open LLM 리더보드에서 DeepSeekMoE 16B와 오픈 소스 모델 간의 비교. 빨간색 점선은 DeepSeekMoE 16B를 제외한 모든 모델의 데이터 포인트에서 선형으로 피팅된다. DeepSeekMoE 16B는 유사한 수의 활성화된 파라미터를 갖는 모델보다 큰 마진만큼 일관되게 우수한 성능을 보이며, 활성화된 파라미터의 약 2.5배를 갖는 LLaMA2 7B와 유사한 성능을 달성한다.\n' +
      '\n' +
      'FFN 중간 은닉 차원입니다. 이에 대응하여 일정한 계산 비용을 유지하면서 보다 세분화된 전문가를 활성화하여 활성화된 전문가의 보다 유연하고 적응적인 조합을 가능하게 한다. 세립화된 전문가 세분화는 다양한 지식을 더 세밀하게 분해하고 다른 전문가로 더 정확하게 학습할 수 있도록 하며, 여기서 각 전문가는 더 높은 수준의 전문화를 유지할 것이다. 또한, 활성화된 전문가를 결합하는 유연성의 증가는 보다 정확하고 표적화된 지식 습득에도 기여한다. (2)\n' +
      '\n' +
      '**공유 전문가 격리:** 특정 전문가를 격리하여 다양한 컨텍스트에 걸쳐 공통 지식을 캡처하고 통합하는 것을 목표로 항상 활성화되는 공유 전문가 역할을 합니다. 상식 지식을 이러한 공유 전문가로 압축함으로써, 라우팅된 다른 전문가들 간의 중복성이 완화될 것이다. 이는 파라미터 효율성을 향상시키고 각각의 라우팅된 전문가가 독특한 측면에 초점을 맞추어 전문화된 상태를 유지하도록 할 수 있다. DeepSeeMoE의 이러한 아키텍처 혁신은 각 전문가가 고도로 전문화된 매개변수 효율적인 MoE 언어 모델을 훈련할 수 있는 기회를 제공한다.\n' +
      '\n' +
      '2B 매개 변수를 사용한 적당한 규모에서 시작하여 DeepSeekoMoE 아키텍처의 이점을 검증한다. 다양한 작업에 걸쳐 12개의 제로 샷 또는 소수 샷 벤치마크에 대한 평가를 수행합니다. 실험 결과 DeepSeeMoE 2B는 GShard 2B(Lepikhin et al., 2021)를 상당한 차이로 능가하며, 1.5\\(\\times\\)의 전문가 매개변수와 계산량을 가진 더 큰 MoE 모델인 GShard 2.9B와도 일치함을 알 수 있었다. 놀랍게도, DeepSeekMoE 2B는 MoE 언어 모델의 엄격한 상한을 설정하는 동등한 수의 매개변수로 조밀한 대응물의 성능에 거의 근접한다는 것을 발견했다. 더 깊은 통찰력을 추구하기 위해 DeepSeekMoE에 대한 전문가 전문화에 대한 정교한 절제 연구와 분석을 수행한다. 이러한 연구는 세밀한 전문가 세분화와 공유 전문가 격리의 효과를 검증하고 DeepSeekMoE가 높은 수준의 전문가 전문화를 달성할 수 있다는 주장을 뒷받침하는 실증적 증거를 제공한다.\n' +
      '\n' +
      '우리의 아키텍처를 활용하여 모델 매개변수를 16B로 확장하고 2T 토큰이 있는 대규모 코퍼스에서 DeepSeekMoE 16B를 훈련한다. 평가 결과는 약 40%의 계산만으로 DeepSeekMoE 16B가 동일한 2T 코퍼스에 대해 훈련된 밀집 모델인 DeepSeek 7B(DeepSeek-AI, 2024)와 유사한 성능을 달성한다는 것을 보여준다. 또한 DeepSeekMoE를 오픈소스 모델과 비교하였으며, 성능 평가를 통해 DeepSeekMoE 16B가 유사한 수의 활성화된 파라미터를 갖는 모델보다 큰 마진으로 일관되게 우수한 성능을 보였으며, 활성화된 파라미터의 약 2.5배인 LLaMA2 7B(Touvron et al., 2023b)와 유사한 성능을 보였다. 그림 1은 Open LLM Leaderboard1에 대한 평가 결과를 보여준다. 또한, 정렬을 위한 감독 미세 조정(supervised fine-tuning, SFT)을 수행하여 모델을 채팅 모델로 변환한다. 평가 결과는 DeepSeekMoE Chat 16B도 채팅 설정에서 DeepSeek Chat 7B 및 LLaMA2 SFT 7B와 유사한 성능을 달성함을 보여준다. 이러한 결과에 고무되어 DeepSeekMoE를 145B로 확장하기 위한 예비 노력을 추가로 수행한다. 실험 결과는 여전히 GShard 아키텍처에 비해 상당한 이점을 일관되게 검증한다. 또한, 28.5%(심지어 18.2%)의 계산만을 사용하여 DeepSeek 67B와 유사한 성능을 보여준다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* **아키텍처 혁신.* * 고급 전문가 세분화 및 공유 전문가 격리의 두 가지 주요 전략을 사용하는 궁극적인 전문가 전문화를 달성하는 것을 목표로 하는 혁신적인 MoE 아키텍처인 DeepSeekMoE를 소개합니다.\n' +
      '* **경험적 유효성 검사** DeepSeekMoE 아키텍처의 유효성을 실증적으로 검증하기 위해 광범위한 실험을 수행합니다. 실험 결과는 DeepSeekMoE 2B에서 높은 수준의 전문가 전문화를 검증하고, DeepSeekMoE 2B가 MoE 모델에 대한 상한 성능에 거의 접근할 수 있음을 나타낸다.\n' +
      '* **확장성** DeepSeekMoE를 확장하여 16B 모델을 학습하고 약 40%의 계산만으로 DeepSeekMoE 16B가 DeepSeek 7B 및 LLaMA2 7B와 유사한 성능을 달성한다는 것을 보여줍니다. 또한 DeepSeekMoE를 145B로 확장하기 위한 예비 시도를 수행하여 GShard 아키텍처에 비해 일관된 이점을 강조하고 DeepSeek 67B와 유사한 성능을 보여준다.\n' +
      '* **MoE에 대한 정렬** DeepSeekMoE 16B에서 감독 미세 조정을 성공적으로 수행하여 정렬된 채팅 모델을 만들어 DeepSeekMoE 16B의 적응성과 다양성을 보여줍니다.\n' +
      '* **퍼블릭 릴리스** 개방형 연구 정신에서 DeepSeekMoE 16B의 모델 체크포인트를 대중에게 릴리스합니다. 특히, 이 모델은 양자화할 필요 없이 40GB의 메모리를 갖는 단일 GPU에 배치될 수 있다.\n' +
      '\n' +
      '## 2 Preiminaries: Transformers용 Mixture-of-Experts\n' +
      '\n' +
      '먼저 트랜스포머 언어 모델에서 일반적으로 사용되는 일반적인 MoE 아키텍처를 소개한다. 표준 트랜스포머 언어 모델은 표준 트랜스포머 블록의 \\(L\\) 레이어를 쌓아 구축되며, 여기서 각 블록은 다음과 같이 나타낼 수 있다:\n' +
      '\n' +
      '\\[\\mathbf{u}_{1:T}^{l} =\\text{Self-Att}\\left(\\mathbf{h}_{1:T}^{l-1}\\right)+\\mathbf{h}_{1:T}^{l-1}, \\tag{1}\\] \\[\\mathbf{h}_{t}^{l} =\\text{FFN}\\left(\\mathbf{u}_{t}^{l}\\right)+\\mathbf{u}_{t}^{l}, \\tag{2}\\]\n' +
      '\n' +
      '여기서, \\(T\\)는 시퀀스 길이를 나타내고, \\(\\text{Self-Att}(\\cdot)\\)는 셀프 어텐션 모듈을 나타내고, \\(\\text{FFN}(\\cdot)\\)는 Feed-Forward Network(FFN), \\(\\mathbf{u}_{1:T}^{l}\\in\\mathbb{R}^{T\\times d}\\)는 \\(l\\)번째 어텐션 모듈 이후의 모든 토큰의 은닉 상태이고, \\(\\mathbf{h}_{t}^{l}\\in\\mathbb{R}^{d}\\)는 \\(l\\)번째 트랜스포머 블록 이후의 모든 토큰의 출력 은닉 상태이다. 간결함을 위해, 우리는 위의 공식들에서 층 정규화를 생략한다.\n' +
      '\n' +
      'MoE 언어 모델을 구성하기 위한 전형적인 관행은 일반적으로 지정된 간격으로 MoE 층들을 갖는 트랜스포머 내의 FFN들을 대체한다(Du et al., 2022; Fedus et al., 2021; Lepikhin et al., 2021; Zoph, 2022). MoE 층은 다수의 전문가들로 구성되며, 여기서 각각의 전문가는 표준 FFN과 구조적으로 동일하다. 그런 다음 각 토큰은 전문가 1명(페더스 등, 2021) 또는 2명(레피킨 등, 2021)에게 할당됩니다. \\(l\\)-th FFN을 MoE 층으로 치환하면 출력 은닉 상태 \\(\\mathbf{h}_{t}^{l}\\)에 대한 계산은 다음과 같이 표현된다:\n' +
      '\n' +
      '\\tag{3}\\] \\[g_{i,t} =\\begin{cases}s_{i,t},&s_{i,t}\\in\\text{Topk}(\\{s_{j,t}|1\\leqslant j \\leqslant N\\},K),\\\\0,&\\text{otherwise},\\end{cases}\\] (4) \\[s_{i,t} =\\text{Softmax}_{i}\\left(\\mathbf{u}_{t}^{lT}\\mathbf{e}_{i}^{l} \\right), \\tag{5}\\]\n' +
      '\n' +
      '여기서, \\(N\\)는 총 전문가 수를 나타내고, \\(\\text{FFN}_{i}(\\cdot)\\)는 \\(i\\)번째 전문가 FFN, \\(g_{i,t}\\)는 \\(i\\)번째 전문가에 대한 게이트 값을 나타내고, \\(s_{i,t}\\)는 토큰 대 전문가 친화도를 나타내며, \\(\\text{Topk}(\\cdot,K)\\)는 \\(t\\)번째 토큰과 모든 \\(N\\) 전문가에 대해 계산된 가장 높은 친화도 점수를 포함하는 집합을 나타내며, \\(\\mathbf{e}_{i}^{l}\\)는 \\(l\\)번째 레이어의 \\(i\\)번째 전문가의 중심이다. \\(g_{i,t}\\)는 희소하여 \\(N\\) 게이트 값 중 \\(K\\)만 0이 아님을 나타냅니다. 이 희소성 속성은 MoE 계층 내에서 계산 효율성을 보장한다. 즉, 각 토큰은 오직 \\(K\\) 전문가에게 할당되고 계산될 것이다. 또한, 상기 공식들에서, 우리는 간결함을 위해 레이어 정규화 동작을 생략한다.\n' +
      '\n' +
      '## 3 DeepSeeMoE Architecture\n' +
      '\n' +
      '섹션 2에 요약된 일반적인 MoE 아키텍처 외에도 전문가 전문화의 잠재력을 활용하기 위해 특별히 설계된 DeepSeeMoE를 소개한다. 그림 2에서 볼 수 있듯이, 우리의 아키텍처는 세밀한 전문가 분할과 공유 전문가 격리의 두 가지 주요 전략을 통합한다. 이 두 가지 전략은 모두 전문가 전문화의 수준을 높이기 위해 고안되었다.\n' +
      '\n' +
      '### 세분화된 전문가 세분화\n' +
      '\n' +
      '전문가 수가 제한된 시나리오에서 특정 전문가에게 할당된 토큰은 다양한 유형의 지식을 다룰 가능성이 더 높다. 이에 따라 지정 전문가는 매개 변수에서 매우 다양한 유형의 지식을 학습할 계획이며 동시에 활용되기 어렵다. 그러나 각 토큰이 더 많은 전문가에게 라우팅될 수 있다면 다양한 지식은 각각 다른 전문가에서 분해되고 학습될 잠재력을 얻을 것이다. 이러한 맥락에서 각 전문가는 여전히 높은 수준의 전문가 전문화를 유지할 수 있어 전문가 전반에 걸쳐 보다 집중된 지식 분배에 기여할 수 있다.\n' +
      '\n' +
      '목표 달성을 위해 일관된 수의 전문가 매개변수와 계산 비용을 유지하면서 더 미세한 그레인으로 전문가를 분할한다. 더 미세한 전문가 세분화는 활성화된 전문가의 보다 유연하고 적응 가능한 조합을 가능하게 한다. 구체적으로, 그림 2(a)에 나타난 전형적인 MoE 아키텍처 위에, FFN 중간 은닉 차원을 원래 크기의 \\(\\frac{1}{m}\\)배로 줄임으로써 각 전문가 FFN을 \\(m\\) 더 작은 전문가로 분할한다. 각 전문가가 작아지기 때문에 이에 대응하여 그림 2(b)와 같이 동일한 계산 비용을 유지하기 위해 활성화된 전문가의 수를 \\(m\\)배로 늘린다. 세립으로\n' +
      '\n' +
      '그림 2: DeepSeeMoE의 일러스트레이션. 도표 (a)는 종래의 top-2 라우팅 전략을 갖는 MoE 층을 도시한다. 도식 (b)는 세분화된 전문가 세분화 전략을 예시한다. 이어서, 하위 그림 (c)는 완전한 DeepSeeMoE 아키텍처를 구성하는 공유 전문가 격리 전략의 통합을 보여준다. 이 세 가지 아키텍처에 걸쳐 전문가 매개변수의 수와 계산 비용이 일정하게 유지된다는 점은 주목할 만하다.\n' +
      '\n' +
      '전문가 세그먼테이션, MoE 레이어의 출력은 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      '\\{s_{j,t}|1 \\leqslant j\\leqslant mN\\},mK),\\\\0,&\\text{otherwise},\\end{cases}\\] (7) \\[s_{i,t} =\\operatorname{Softmax}_{i}\\left(\\mathbf{u}_{t}^{lT}\\mathbf{e}_{ i}^{l}\\right), \\tag{8}\\]\n' +
      '\n' +
      '여기서 전문가 매개변수의 총 수는 표준 FFN의 매개변수 수의 \\(N\\) 배와 동일하고, \\(mN\\)는 세립 전문가의 총 수를 나타낸다. 세립화된 전문가 분할 전략을 사용하면 0이 아닌 게이트의 수도 \\(mK\\)로 증가한다.\n' +
      '\n' +
      '조합적 관점에서 세분화된 전문가 세분화 전략은 활성화된 전문가의 조합적 유연성을 실질적으로 향상시킨다. 예로서, \\(N=16\\)인 경우를 고려한다. 전형적인 top-2 라우팅 전략은 \\(\\binom{16}{2}=120\\) 가능한 조합을 산출할 수 있다. 이와는 대조적으로, 각 전문가가 4명의 작은 전문가로 분할될 경우, 세밀한 라우팅 전략은 \\(\\binom{64}{8}=4,426,165,368\\) 잠재적인 조합을 산출할 수 있다. 조합 유연성의 급증은 보다 정확하고 목표된 지식 획득을 달성할 수 있는 잠재력을 향상시킨다.\n' +
      '\n' +
      '### 공유 Expert Isolation\n' +
      '\n' +
      '기존의 라우팅 전략을 사용하면 서로 다른 전문가에게 할당된 토큰이 몇 가지 일반적인 지식이나 정보를 필요로 할 수 있습니다. 그 결과, 다수의 전문가가 각자의 파라미터에서 공유된 지식을 획득하는 데 수렴할 수 있고, 그에 따라 전문가 파라미터에서 중복이 발생할 수 있다. 그러나 다양한 컨텍스트에 걸쳐 공통 지식을 캡처하고 통합하는 데 전념하는 공유 전문가가 있는 경우 라우팅된 다른 전문가 간의 매개변수 중복성이 완화된다. 이러한 중복성 완화는 보다 전문화된 전문가와 함께 보다 모수 효율적인 모델에 기여할 것이다.\n' +
      '\n' +
      '이 목적을 위해 세밀한 전문가 세분화 전략 외에도 \\(K_{s}\\) 전문가를 분리하여 공유 전문가 역할을 수행한다. 라우터 모듈에 관계없이 각 토큰은 이러한 공유 전문가에게 결정적으로 할당됩니다. 일정한 계산 비용을 유지하기 위해, 다른 라우팅된 전문가들 중 활성화된 전문가의 수는 그림 2(c)와 같이 \\(K_{s}\\)만큼 감소될 것이다. 공유된 전문가 격리 전략이 통합된 상태에서, 완전한 DeepSeeMoE 아키텍처 내의 MoE 계층은 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\{s_{j,t}|K_ {s}+1\\leqslant j\\leqslant mN\\},mK-K_{s}),\\\\0,&\\text{otherwise},\\end{cases}\\] (10) \\[s_{i,t} =\\operatorname{Softmax}_{i}\\left(\\mathbf{u}_{t}^{lT}\\mathbff{e}_{ i}^{l}\\right) \\tag{11}\\]\n' +
      '\n' +
      '마지막으로 DeepSeeMoE에서 공유 전문가 수는 \\(K_{s}\\), 라우팅된 총 전문가 수는 \\(mN-K_{s}\\), 논제로 게이트 수는 \\(mK-K_{s}\\)이다.\n' +
      '\n' +
      '공유된 전문가 격리의 원형은 Rajbhandari 등(2022)에게 인정될 수 있다는 점에 주목할 필요가 있다. 핵심적인 구별은 그들이 이 전략을 공학적 관점에서 도출하는 반면 우리는 알고리즘적 관점에서 접근한다는 사실에 있다.\n' +
      '\n' +
      '### Load Balance 고려 사항\n' +
      '\n' +
      '자동으로 학습된 라우팅 전략은 두 가지 주목할 만한 결함을 나타내는 부하 불균형 문제에 직면할 수 있다. 첫째, 라우팅 붕괴의 위험이 있다(Shazeer et al., 2017). 즉, 모델은 항상 소수의 전문가만을 선택하여 다른 전문가가 충분한 훈련을 받지 못하게 한다. 둘째, 전문가가 여러 장치에 분산되어 있는 경우 부하 불균형이 계산 병목 현상을 악화시킬 수 있습니다.\n' +
      '\n' +
      '전문가 수준의 균형 손실. 라우팅 붕괴 위험을 완화하기 위해 전문가 수준의 균형 손실도 사용합니다. 잔액 손실의 계산은 다음과 같다:\n' +
      '\n' +
      '[\\mathcal{L}_{\\mathrm{ExpBal}} =\\alpha_{1}\\sum_{i=1}^{N^{\\prime}}f_{i}P_{i}, \\tag{12}\\] \\[f_{i} =\\frac{N^{\\prime}}{K^{\\prime}T}\\sum_{t=1}^{T}\\mathds{1}(\\text{ Token $t$ select Expert $i$),\\] (13) \\[P_{i} =\\frac{1}{T}\\sum_{t=1}^{T}s_{i,t}, \\tag{14}\\]\n' +
      '\n' +
      '여기서 \\(\\alpha_{1}\\)는 전문가 수준의 균형 인자라고 하는 하이퍼-파라미터이고, \\(N^{\\prime}\\)는 \\((mN-K_{s})\\)와 같고, \\(K^{\\prime}\\)는 간결성을 위해 \\((mK-K_{s})\\)와 같다. \\ (\\mathds{1}(\\cdot)\\)는 지표 함수를 나타낸다.\n' +
      '\n' +
      '장치 수준 균형 손실.전문가 수준 균형 손실 외에도 장치 수준 균형 손실을 소개합니다. 부하 균형에 대한 과도한 제약은 모델 성능을 손상시키기 때문에, 계산 병목 현상을 완화하기 위해 전문가 수준에서 엄격한 균형 제약을 적용하는 것은 불필요해진다. 대신, 우리의 주요 목표는 장치 전반에 걸쳐 균형 잡힌 계산을 보장하는 것입니다. 라우팅된 모든 전문가를 \\(D\\) 그룹 \\(\\{\\mathcal{E}_{1},\\mathcal{E}_{2},...,\\mathcal{E}_{D}\\}\\)으로 분할하고 각 그룹을 단일 장치에 배포하면 장치 수준 균형 손실이 다음과 같이 계산됩니다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{DevBal}} =\\alpha_{2}\\sum_{i=1}^{D}f_{i}^{\\prime}P_{i}^{\\prime}, \\tag{15}\\] \\[f_{i}^{\\prime} =\\frac{1}{|\\mathcal{E}_{i}|}\\sum_{j\\in\\mathcal{E}_{i}}f_{j},\\] (16) \\[P_{i}^{\\prime} =\\sum_{j\\in\\mathcal{E}_{i}}P_{j}, \\tag{17}\\]\n' +
      '\n' +
      '여기서 \\(\\alpha_{2}\\)는 디바이스 레벨 밸런스 팩터라고 하는 하이퍼-파라미터이다. 실제로, 라우팅 붕괴의 위험을 완화하기 위해 작은 전문가 수준의 균형 요소를 설정하고, 장치 전반에 걸쳐 균형 계산을 촉진하기 위해 더 큰 장치 수준의 균형 요소를 설정한다.\n' +
      '\n' +
      '## 4 유효성 검사 실험\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 4.1.1 Training Data and Tokenization\n' +
      '\n' +
      '우리의 훈련 데이터는 DeepSeek-AI가 만든 대규모 다국어 말뭉치에서 샘플링된다. 코퍼스는 주로 영어와 중국어에 초점을 맞추지만 다른 언어들도 포함한다. 웹 텍스트, 수학 자료, 코딩 스크립트, 출판 문헌 및 기타 다양한 텍스트 자료를 포함한 다양한 출처에서 파생된다. 검증 실험을 위해 코퍼스에서 100B 토큰을 포함하는 하위 집합을 샘플링하여 모델을 학습한다. 토큰화를 위해 HuggingFace Tokenizer2 도구를 사용하여 훈련 코퍼스의 더 작은 부분 집합에 바이트 쌍 인코딩(BPE)(Sennrich et al., 2016) 토큰라이저를 훈련한다. 검증 실험에서는 어휘 크기가 8K인 토큰화기를 준비하였으며, 더 큰 모델을 훈련할 때 어휘 크기가 확장될 것이다.\n' +
      '\n' +
      '각주 2: [https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)\n' +
      '\n' +
      '#### 4.1.2 Infrastructures\n' +
      '\n' +
      '본 논문에서는 텐서 병렬성(Korthikanti et al., 2023; Narayanan et al., 2021; Shoeybi et al., 2019), ZeRO 데이터 병렬성(Rajbhandari et al., 2020), PipeDream 파이프라인 병렬성(Harlap et al., 2018), 보다 구체적으로는 데이터와 텐서 병렬성을 결합하여 전문가 병렬성(Lepikhin et al., 2021)을 통합하는 효율적이고 경량화된 훈련 프레임워크인 HAI-LLM(High-Flyer, 2023)을 기반으로 실험을 수행한다. 성능을 최적화하기 위해, 우리는 알고리즘을 게이팅하고 서로 다른 전문가에서 선형 계층에 걸쳐 계산을 융합하기 위해 CUDA 및 트리톤(Tillet et al., 2019)을 갖는 GPU 커널을 개발한다.\n' +
      '\n' +
      '모든 실험은 NVIDIA A100 또는 H800 GPU가 장착된 클러스터에서 수행된다. A100 클러스터의 각 노드는 NVLink 브리지를 통해 쌍으로 연결된 8개의 GPU를 포함한다. H800 클러스터는 또한 노드 내에서 NVLink 및 NVSwitch를 사용하여 상호 연결된 노드당 8개의 GPU를 특징으로 한다. A100 및 H800 클러스터 모두에 대해, InfiniBand 상호연결은 노드들 간의 통신을 용이하게 하기 위해 활용된다.\n' +
      '\n' +
      '#### 4.1.3 Hyper-Parameters\n' +
      '\n' +
      '모델 설정.검증 실험에서 트랜스포머 레이어의 수를 9개로, 히든 차원을 1280개로 설정하였다. 총 10개의 어텐션 헤드를 갖는 다중 헤드 어텐션 메커니즘을 사용하였으며, 각 헤드의 차원은 128이다. 초기화를 위해 학습 가능한 모든 파라미터는 표준편차 0.006으로 랜덤하게 초기화된다. 모든 FFN을 MoE 레이어로 대체하고, 전문가 파라미터의 총 개수가 표준 FFN의 16배인지 확인한다. 또한, 공유된 전문가 파라미터와 활성화된 라우팅된 전문가 파라미터를 포함한 활성화된 전문가 파라미터를 표준 FFN의 2배로 유지한다. 이러한 구성 하에서, 각각의 MoE 모델은 대략 2B 총 파라미터를 가지며, 활성화된 파라미터의 수는 약 0.3B이다.\n' +
      '\n' +
      '학습 설정 하이퍼 파라미터로 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), weight_decay = 0.1로 설정된 AdamW 최적화기(Loshchilov and Hutter, 2019)를 사용한다. 학습률은 웜업 및 스텝 디케이 전략을 사용하여 스케줄링된다. 처음에, 학습률은 처음 2K 단계 동안 0에서 최대값까지 선형적으로 증가한다. 이어서, 학습 속도는 훈련 단계의 80%에서 0.316을 곱하고, 다시 훈련 단계의 90%에서 0.316을 곱한다. 검증 실험을 위한 최대 학습률은 \\(1.08\\times 10^{-3}\\)로 설정되었고, 기울기 클리핑 규범은 1.0으로 설정되었다. 배치 크기는 2K로 설정되었고, 최대 시퀀스 길이가 2K인 각 훈련 배치에는 4M 토큰이 포함되어 있다. 상응하게, 100B 트레이닝 토큰들을 달성하기 위해 트레이닝 단계들의 총 수는 25,000으로 설정된다. 훈련 데이터가 풍부하기 때문에 훈련 중 중도 탈락은 사용하지 않습니다. 상대적으로 작은 모델 크기를 감안할 때, 전문가 파라미터를 포함한 모든 파라미터는 불균형한 계산을 피하기 위해 단일 GPU 장치에 배치된다. 이에 따라 교육 중에 토큰을 떨어뜨리지 않으며 장치 수준 균형 손실을 사용하지 않습니다. 라우팅 붕괴를 방지하기 위해 전문가 수준의 균형 계수를 0.01로 설정하였다.\n' +
      '\n' +
      '가독성을 위해 부록 A의 다양한 크기에 걸쳐 DeepSeekMoE에 대한 하이퍼 매개 변수의 개요 표도 제시한다.\n' +
      '\n' +
      '#### 4.1.4 평가 벤치마크\n' +
      '\n' +
      '다양한 유형의 작업을 포괄하는 광범위한 벤치마크에 대한 평가를 수행합니다. 벤치마크를 다음과 같이 나열합니다.\n' +
      '\n' +
      '언어 모델링.언어 모델링을 위해, 우리는 Pile(Gao et al., 2020)의 테스트 세트에 대한 모델을 평가하며, 평가 메트릭은 교차 엔트로피 손실이다.\n' +
      '\n' +
      '언어 이해 및 추론.언어 이해 및 추론을 위해, 우리는 HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-challenge and ARC-easy (Clark et al., 2018)를 고려한다. 이러한 작업에 대한 평가 메트릭은 정확성입니다.\n' +
      '\n' +
      '읽기 이해.읽기 이해는 RACE-high와 RACE-middle Lai 등(2017)을 사용하며, 평가 척도는 정확도이다.\n' +
      '\n' +
      'Code Generation.For code generation, we evaluate the models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). 평가 메트릭은 하나의 세대 시도에 대한 패스 레이트를 나타내는 Pass@1이다.\n' +
      '\n' +
      'Closed-Book Question Answering.Close-Book Question answer는 TriviaQA (Joshi et al., 2017)와 NaturalQuestions (Kwiatkowski et al., 2019)를 고려한다. 평가 메트릭은 정확 일치율(EM)입니다.\n' +
      '\n' +
      '### Evaluations\n' +
      '\n' +
      '기준선. DeepSeekMoE를 포함하여 5가지 모델을 검증 실험을 위해 비교한다. **밀도** 는 0.2B 총 매개 변수를 사용 하는 표준 밀도 변환기 언어 모델을 나타냅니다. **해시 계층**(Roller 등, 2021)은 조밀한 기준선과 정렬된 2.0B 총 매개 변수 및 0.2B 활성화된 매개 변수를 사용 하는 상위 1 해시 라우팅을 기반으로 하는 MoE 아키텍처입니다. **Switch Transformer**(Fedus et al., 2021)는 상위 1 학습 가능한 라우팅을 기반으로 하는 또 다른 잘 알려진 MoE 아키텍처로, 총 매개 변수와 활성화된 매개 변수는 해시 계층과 동일합니다. **GShard**(Lepikhin 등, 2021)는 Top-1 라우팅 방법에 비해 한 명의 전문가가 더 활성화되기 때문에 2.0B의 총 매개 변수와 0.3B의 활성화된 매개 변수를 사용하여 Top-2 학습 가능한 라우팅 전략을 사용합니다. **DeepSeekMoE**에는 공유 전문가 1명과 라우팅된 전문가 63명이 있으며, 각 전문가는 표준 FFN의 0.25배 크기입니다. DeepSeekMoE를 포함하여, 비교된 모든 모델들은 동일한 트레이닝 코퍼스와 트레이닝 하이퍼-파라미터를 공유한다. 비교된 모든 MoE 모델은 동일한 수의 총 파라미터를 가지며, GShard는 DeepSeekMoE와 동일한 수의 활성화된 파라미터를 갖는다.\n' +
      '\n' +
      '결과.우리는 표 1에 평가 결과를 제시한다. 입증된 모든 모델에 대해, 우리는 100B 토큰에 대한 트레이닝 후 최종 평가 결과를 보고한다. 표로부터, 우리는 다음과 같은 관찰을 한다: (1) 희박한 아키텍처와 더 많은 총 파라미터를 사용하여, Hash Layerand Switch Transformer는 동일한 수의 활성화된 파라미터를 갖는 조밀한 베이스라인보다 훨씬 더 강한 성능을 달성한다. (2) 해시 레이어 및 스위치 트랜스포머와 비교하여, GShard는 활성화된 파라미터가 더 많고 스위치 트랜스포머보다 약간 더 나은 성능을 달성한다. (3) 동일한 수의 총 파라미터 및 활성화된 파라미터로, DeepSeekMoE는 GShard에 비해 압도적인 이점을 보여준다. 이러한 결과는 MoE 아키텍처의 기존 풍경 내에서 우리의 DeepSeekMoE 아키텍처의 우수성을 보여준다.\n' +
      '\n' +
      '### DeepSeekMoE 정렬 MoE 모델의 상한과 가깝게 정렬\n' +
      '\n' +
      '우리는 DeepSeekMoE가 조밀한 기준선 및 다른 MoE 아키텍처보다 우수하다는 것을 입증했다. DeepSeekMoE의 성능을 보다 정확하게 이해하기 위해 더 많은 총 매개변수 또는 활성화된 매개변수와 더 큰 기준선과 비교한다. 비교를 통해 DeepSeekMoE와 동등한 성능을 얻기 위해 GShard 또는 조밀한 기준선의 필요한 모델 크기를 추정할 수 있다.\n' +
      '\n' +
      'GShard\\(\\times\\)1.5와의 비교 표 2는 DeepSeekMoE와 전문가 크기의 1.5배의 더 큰 GShard 모델을 비교하여 전문가 매개변수와 전문가 계산 모두 1.5배의 결과를 나타낸다. 전반적으로, DeepSeekMoE는 GShard\\(\\times\\)1.5와 유사한 성능을 달성하며, 이는 DeepSeekMoE 아키텍처에 내재된 상당한 이점을 강조한다. 또한 GShard\\(\\times\\)1.5와의 비교뿐만 아니라 부록 B의 GShard\\(\\times\\)1.2와의 비교도 보인다.\n' +
      '\n' +
      '또한, DeepSeekMoE의 총 파라미터 수를 13.3B로 증가시켰으며, GShard\\(\\times\\)1.2와 GShard\\(\\times\\)1.5와 각각 15.9B와 19.8B의 총 파라미터 수를 비교하였다. 우리는 더 큰 규모에서 DeepSeekMoE가 GShard\\(\\times\\)1.5를 뚜렷하게 능가할 수 있음을 발견했다. 이거.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c|c c} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **Dense** & **Hash Layer** & **Switch** & **GShard** & **DeepSeekMoE** \\\\ \\hline \\# Total Params & N/A & 0.2B & 2.0B & 2.0B & 2.0B & 2.0B \\\\ \\# Activated Params & N/A & 0.2B & 0.2B & 0.2B & 0.3B & 0.3B \\\\ FLOPs per 2K Tokens & N/A & 2.9T & 2.9T & 2.9T & 4.3T & 4.3T \\\\ \\# Training Tokens & N/A & 100B & 100B & 100B & 100B & 100B \\\\ \\hline Pile (Loss) & N/A & 2.060 & 1.932 & 1.881 & 1.867 & **1.808** \\\\ \\hline HellaSwag (Acc.) & 0-shot & 38.8 & 46.2 & 49.1 & 50.5 & **54.8** \\\\ PIQA (Acc.) & 0-shot & 66.8 & 68.4 & 70.5 & 70.6 & **72.3** \\\\ ARC-easy (Acc.) & 0-shot & 41.0 & 45.3 & 45.9 & 43.9 & **49.4** \\\\ ARC-challenge (Acc.) & 0-shot & 26.0 & 28.2 & 30.2 & 31.6 & **34.3** \\\\ \\hline RACE-middle (Acc.) & 5-shot & 38.8 & 38.8 & 43.6 & 42.1 & **44.0** \\\\ RACE-high (Acc.) & 5-shot & 29.0 & 30.0 & 30.9 & 30.4 & **31.7** \\\\ \\hline HumanEval (Pass@1) & 0-shot & 0.0 & 1.2 & 2.4 & 3.7 & **4.9** \\\\ MBPP (Pass@1) & 3-shot & 0.2 & 0.6 & 0.4 & 0.2 & **2.2** \\\\ \\hline TriviaQA (EM) & 5-shot & 4.9 & 6.5 & 8.9 & 10.2 & **16.6** \\\\ NaturalQuestions (EM) & 5-shot & 1.4 & 1.4 & 2.5 & 3.2 & **5.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: | 유효성 검사 실험에 대한 평가 결과입니다. **굵게 표시된** 글꼴이 가장 좋습니다. 다른 MoE 아키텍처와 비교하여 DeepSeekMoE는 상당한 성능 이점을 나타낸다.\n' +
      '\n' +
      '결과는 부록 B에도 나와 있습니다.\n' +
      '\n' +
      'Dense\\(\\times16\\)와의 비교 표 2는 DeepSeeMoE와 더 큰 밀집 모델 간의 비교도 보여준다. 공정한 비교를 위해 주의력과 FFN 매개변수 간의 널리 사용되는 비율(1:2)을 사용하지 않는다. 대신, 각 전문가가 표준 FFN과 동일한 수의 파라미터를 갖는 16개의 공유 전문가를 구성한다. 이 아키텍처는 16배 표준 FFN 매개변수를 가진 밀집 모델을 모방한다. 표로부터, DeepSeeMoE는 모델 용량 측면에서 MoE 모델의 엄격한 상한을 설정하는 Dense\\(\\times16\\)의 성능에 거의 근접함을 알 수 있다. 이러한 결과는 _적어도 약 2B 매개변수 및 100B 훈련 토큰의 규모에서 DeepSeeMoE의 성능이 MoE 모델의 이론적 상한과 밀접하게 일치함을 시사한다_. 또한 부록 B의 Dense\\(\\times4\\)와 추가적인 비교를 제공한다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '세분화된 전문가 분할 및 공유 전문가 분리 전략의 효과를 입증하기 위해 DeepSeeMoE에 대한 절제 연구를 수행하고 결과를 그림 3에 제시한다. 공정한 비교를 위해 비교에 포함된 모든 모델이 일치하는지를 확인한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **GShard\\(\\times 1.5\\)** & **Dense\\(\\times 16\\)** & **DeepSeeMoE** \\\\ \\hline Relative Expert Size & N/A & 1.5 & 1 & 0.25 \\\\ \\# Experts & N/A & \\(0+16\\) & \\(16+0\\) & \\(1+63\\) \\\\ \\# Activated Experts & N/A & \\(0+2\\) & \\(16+0\\) & \\(1+7\\) \\\\ \\# Total Expert Params & N/A & 2.83B & 1.89B & 1.89B \\\\ \\# Activated Expert Params & N/A & 0.35B & 1.89B & 0.24B \\\\ FLOPs per 2K Tokens & N/A & 5.8T & 24.6T & 4.3T \\\\ \\# Training Tokens & N/A & 100B & 100B & 100B \\\\ \\hline Pile (Loss) & N/A & 1.808 & 1.806 & 1.808 \\\\ \\hline HellaSwag (Acc.) & 0-shot & 54.4 & 55.1 & 54.8 \\\\ PIQA (Acc.) & 0-shot & 71.1 & 71.9 & 72.3 \\\\ ARC-easy (Acc.) & 0-shot & 47.3 & 51.9 & 49.4 \\\\ ARC-challenge (Acc.) & 0-shot & 34.1 & 33.8 & 34.3 \\\\ \\hline RACE-middle (Acc.) & 5-shot & 46.4 & 46.3 & 44.0 \\\\ RACE-high (Acc.) & 5-shot & 32.4 & 33.0 & 31.7 \\\\ \\hline HumanEval (Pass@1) & 0-shot & 3.0 & 4.3 & 4.9 \\\\ MBPP (Pass@1) & 3-shot & 2.6 & 2.2 & 2.2 \\\\ \\hline TriviaQA (EM) & 5-shot & 15.7 & 16.5 & 16.6 \\\\ NaturalQuestions (EM) & 5-shot & 4.7 & 6.3 & 5.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: DeepSeeMoE, 더 큰 GShard 모델 및 더 큰 밀집 모델 간의 비교. # 전문가 줄에서 \\(a+b\\)는 \\(a\\) 공유 전문가 및 \\(b\\) 라우팅된 전문가를 나타냅니다. # 활성화된 전문가 줄에서 \\(a+b\\)는 활성화된 공유 전문가 및 \\(b\\) 활성화된 라우팅 전문가를 나타냅니다. DeepSeeMoE는 전문가 파라미터와 연산이 1.5배 포함된 GShard 모델과 비교 가능한 성능을 달성한다. 또한 DeepSeeMoE는 모델 용량 측면에서 MoE 모델의 상한을 설정하는 16배 FFN 매개변수로 밀집 모델의 성능에 거의 근접한다.\n' +
      '\n' +
      '동일한 개수의 총 매개 변수 및 활성화된 매개 변수입니다.\n' +
      '\n' +
      '공유 전문가 격리. 공유 전문가 격리 전략의 영향을 평가하기 위해 GShard를 기반으로 한 전문가를 공유 전문가로 격리한다. 그림 3에서 우리는 GShard와 비교하여 공유 전문가의 의도적인 격리가 대부분의 벤치마크에서 향상된 성능을 산출한다는 것을 관찰한다. 이러한 결과는 공유 전문가 격리 전략이 더 강력한 모델 성능에 기여한다는 명제를 뒷받침한다.\n' +
      '\n' +
      '세립화된 전문가 분할.세립화된 전문가 분할 전략의 효과를 평가하기 위해 전문가를 더 세립화된 그레인으로 세분화하여 보다 상세한 비교를 수행한다. 구체적으로, 우리는 각 전문가를 2명 또는 4명의 작은 전문가로 세분화하여 총 32명(1개의 공유 + 31개의 라우팅) 또는 64명(1개의 공유 + 63개의 라우팅)의 전문가를 생성한다. 그림 3은 전문가 세분화 세분화의 연속적 개선이 전체 모델 성능의 연속적 향상에 해당한다는 일관된 경향을 보여준다. 이러한 결과는 세분화된 전문가 세분화 전략의 효과성에 대한 실증적 검증을 제공한다.\n' +
      '\n' +
      '공유 전문가와 라우팅 전문가 간의 비율.또한 공유 전문가와 라우팅 전문가의 최상의 비율을 조사합니다. 총 64명의 전문가와 활성화된 전문가의 수를 일정하게 유지하면서 최고의 세분성을 바탕으로 1, 2, 4명의 전문가를 공유 전문가로 분리하고자 한다. 연구 결과, 공유전문가와 라우팅된 전문가의 비율이 서로 다른 경우 성능에 큰 영향을 미치지 않는 것으로 나타났으며, 1, 2, 4명의 공유전문가는 각각 1.808, 1.806, 1.811의 파일손실을 달성하였다. 1:3의 비율이 약간 더 나은 파일 손실을 산출한다는 점을 고려하여 DeepSeekMoE를 확장할 때 공유 전문가와 활성화된 라우팅 전문가 간의 비율을 1:3으로 유지한다.\n' +
      '\n' +
      '그림 3: DeepSeekMoE에 대한 절제 연구. 성능은 프레젠테이션에서 명확성을 위해 최상의 성능에 의해 정규화된다. 비교된 모든 모델은 동일한 수의 파라미터 및 활성화된 파라미터를 갖는다. 세분화된 전문가 분할과 공유된 전문가 격리가 모두 더 강력한 전체 성능에 기여한다는 것을 발견할 수 있다.\n' +
      '\n' +
      '### 전문가 전문화 분석\n' +
      '\n' +
      '본 절에서는 DeepSeekMoE 2B의 전문가 전문화에 대한 실증분석을 실시한다. 이 섹션의 DeepSeekMoE 2B는 표 1에 보고된 모델, 즉 2.0B 총 매개변수를 포함하며 공유 전문가 1명과 라우팅된 전문가 63명 중 7명이 활성화된다.\n' +
      '\n' +
      'DeepSeekMoE는 라우팅된 전문가들 사이의 낮은 중복성을 나타낸다. 라우팅된 전문가들 사이의 중복성을 평가하기 위해, 라우팅된 전문가들 사이의 다양한 비율을 디스에이블하고 파일 손실을 평가한다. 구체적으로 각 토큰에 대해 라우팅 확률이 가장 높은 특정 비율의 전문가를 마스킹한 후 나머지 라우팅된 전문가 중에서 상위 K명의 전문가를 선택한다. 공정성을 위해 DeepSeekMoE와 GShard\\(\\times\\)1.5를 비교하였는데, 이는 전문가가 장애인이 없을 때 동일한 파일손실을 갖기 때문이다. 그림 4에서 볼 수 있듯이 DeepSeekMoE는 GShard\\(\\times\\)1.5에 비해 상위 라우팅 전문가의 디스에이블에 더 민감하다. 이 민감도는 각 라우팅된 전문가가 더 대체할 수 없기 때문에 DeepSeekMoE에서 더 낮은 수준의 매개변수 중복성을 시사한다. 반면 GShard\\(\\times\\)1.5는 전문가 매개 변수 중 더 큰 중복성을 보여 상위 라우팅 전문가가 비활성화되었을 때 성능 저하를 완충할 수 있다.\n' +
      '\n' +
      '공유된 전문가는 라우팅된 전문가에 의해 대체 불가능합니다. DeepSeekMoE에서 공유된 전문가의 역할을 조사하기 위해, 이를 비활성화하고 라우팅된 전문가를 한 명 더 활성화합니다. Pile에 대한 평가는 동일한 계산 비용을 유지함에도 불구하고 파일 손실이 1.808에서 2.414로 크게 증가하는 것으로 나타났다. 이 결과는 공유 전문가의 중요한 기능을 강조하며 공유 전문가가 라우팅된 전문가와 공유되지 않은 기본적이고 필수적인 지식을 포착하여 라우팅된 전문가로 대체할 수 없음을 나타낸다.\n' +
      '\n' +
      'DeepSeekMoE는 보다 정확하게 지식을 습득한다. 활성화된 전문가들을 결합할 때 더 높은 유연성이 보다 정확하고 표적화된 지식 습득에 기여한다는 주장을 검증하기 위해, 우리는 더 적은 수의 활성화된 전문가들로 DeepSeekMoE가 필요한 지식을 습득할 수 있는지 여부를 조사한다. 구체적으로, 우리는 활성화된 라우팅 전문가의 수를 3명에서 7명으로 변경하고 결과적인 파일 손실을 평가한다. 도 5에서 설명한 바와 같이, 단지\n' +
      '\n' +
      '그림 4: 장애가 있는 상위 경로 전문가의 다른 비율과 관련하여 파일 손실. 특히, DeepSeekMoE는 장애가 있는 상위 라우팅 전문가의 비율에 대해 더 큰 민감도를 나타내며, 이는 DeepSeekMoE의 라우팅 전문가들 사이에서 더 낮은 중복성을 나타낸다.\n' +
      '\n' +
      '4명의 라우팅 전문가가 활성화되어 DeepSeekMoE는 GShard와 유사한 파일 손실을 달성합니다. 이러한 관찰은 DeepSeekMoE가 보다 정확하고 효율적으로 필요한 지식을 획득할 수 있다는 명제를 뒷받침한다.\n' +
      '\n' +
      '이러한 결과를 통해 DeepSeekMoE의 전문가 전문화와 정확한 지식 습득을 보다 엄격하게 검증하기 위해 새로운 모델을 처음부터 훈련한다. 이 모델은 공유 전문가 1명과 라우팅 전문가 63명으로 구성되며, 라우팅 전문가 3명만 활성화된다. 그림 6에 나타난 평가 결과는 동일한 총 전문가 매개변수와 활성화된 전문가 매개변수의 절반만 있어도 DeepSeekMoE가 여전히 GShard보다 우수하다는 것을 보여준다. 이것은 전문가 매개변수를 활용하는 DeepSeekMoE의 능력을 강조한다.\n' +
      '\n' +
      '그림 5: \\(|\\) DeepSeekMoE에서 활성화된 라우팅된 전문가의 서로 다른 수에 대한 파일 손실입니다. 4명의 라우팅된 전문가만 활성화된 상태에서 DeepSeekMoE는 GShard에 필적하는 파일 손실을 달성한다.\n' +
      '\n' +
      '그림 6: \\(|\\) GShard와 DeepSeekMoE를 활성화된 전문가 절반(처음부터 훈련됨)과 비교합니다. 동일한 총 전문가 매개변수와 활성화된 전문가 매개변수의 절반만 사용하여 DeepSeekMoE는 여전히 GShard보다 우수합니다.\n' +
      '\n' +
      '더 효율적으로, 즉 활성화된 전문가에서 효과적인 매개변수의 비율이 GShard에 비해 훨씬 높다.\n' +
      '\n' +
      '## 5 Scaling up to DeepSeekMoE 16B\n' +
      '\n' +
      'DeepSeekMoE 아키텍처를 사용하여 MoE 모델을 16B 총 매개변수로 더 큰 규모로 확장하고 2T 토큰에서 훈련합니다. 실험 결과, DeepSeekMoE 16B는 LLaMA2 7B와 비교하여 약 40%의 계산량으로 우수한 성능을 보였다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 5.1.1 Training Data and Tokenization\n' +
      '\n' +
      '섹션 4.1.1에서 설명한 것과 동일한 코퍼스에서 훈련 데이터를 샘플링한다. 검증 실험과 달리 LLaMA2 7B의 훈련 토큰 수와 정렬하여 2T 토큰으로 더 많은 양의 데이터를 샘플링한다. 또한 HuggingFace 토큰나이저 도구를 사용하여 BPE 토큰나이저를 훈련시키지만, DeepSeekMoE 16B의 경우 어휘 크기가 100K로 설정된다.\n' +
      '\n' +
      '#### 5.1.2 Hyper-Parameters\n' +
      '\n' +
      'Model Settings.DeepSeekMoE 16B의 경우 Transformer Layer의 수를 28, hidden dimension을 2048로 설정하였다. 총 16개의 attention head를 가지는 multi-head attention 메커니즘을 사용하였으며, 각 head는 128이다. 초기화는 모든 학습 가능한 파라미터들이 표준편차 0.006으로 랜덤하게 초기화된다. 첫 번째 레이어에서 부하 균형 상태가 특히 느리게 수렴하는 것을 관찰하기 때문에 첫 번째 레이어를 제외한 모든 FFN을 MoE 레이어로 대체한다. 각 MoE 계층은 공유 전문가 2명과 라우팅 전문가 64명으로 구성되며, 각 전문가는 표준 FFN의 0.25배 크기이다. 각 토큰은 이 두 명의 공유 전문가와 64명의 라우팅된 전문가 중 6명에게 라우팅됩니다. 너무 작은 전문가 크기와 관련된 계산 효율의 잠재적인 감소로 인해 더 미세한 전문가 세분화 세분화는 사용되지 않는다. 16B에 걸쳐 더 큰 스케일에서, 더 미세한 입도가 여전히 채용될 수 있다. 우리의 구성에서 DeepSeekMoE 16B는 약 16.4B의 총 매개변수를 가지고 있으며 활성화된 매개변수의 수는 약 2.8B이다.\n' +
      '\n' +
      '학습 설정 하이퍼파라미터를 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), weight_decay = 0.1로 설정한 AdamW Optimizer(Loshchilov and Hutter, 2019)를 사용한다. 학습률 또한 웜업 및 스텝 디케이 전략을 사용하여 스케줄링한다. 처음에, 학습률은 처음 2K 단계 동안 0에서 최대값까지 선형적으로 증가한다. 이어서, 학습 속도는 훈련 단계의 80%에서 0.316을 곱하고, 다시 훈련 단계의 90%에서 0.316을 곱한다. DeepSeekMoE 16B의 최대 학습률은 \\(4.2\\times 10^{-4}\\)로 설정되고, 기울기 클리핑 규범은 1.0으로 설정된다. 배치 크기는 4.5K로 설정되고, 최대 시퀀스 길이가 4K인 각 훈련 배치에는 18M 토큰이 포함된다. 상응하게, 트레이닝 단계들의 총 수는 2T 트레이닝 토큰들을 달성하기 위해 106,449로 설정된다. 훈련 데이터가 풍부하기 때문에 훈련 중 중도 탈락은 사용하지 않습니다. 파이프라인 병렬성을 활용하여 모델의 다른 계층을 다른 장치에 배치합니다. 각 계층에 대해 모든 전문가가 동일한 장치에 배치됩니다. 따라서 우리는 또한 훈련 중에 토큰을 떨어뜨리지 않으며 장치 수준 잔액 손실을 사용하지 않습니다. 라우팅 붕괴를 방지하기 위해 0.001의 상당히 작은 전문가 수준의 균형 계수를 설정했는데, 이는 병렬화 전략 하에서 전문가 수준의 균형 계수가 높을수록 계산 효율이 증가할 수 없지만 모델 성능이 손상된다는 것을 발견했기 때문이다.\n' +
      '\n' +
      '#### 5.1.3 평가 벤치마크\n' +
      '\n' +
      '검증 실험에 사용된 벤치마크 외에도 보다 포괄적인 평가를 위해 추가 벤치마크를 통합한다. 검증 실험에 사용된 벤치마크와의 차이점을 다음과 같이 소개한다.\n' +
      '\n' +
      'Language Modeling.For language modeling, we also evaluate the models on the test set of Pile (Gao et al., 2020). DeepSeekMoE 16B에서 사용되는 토큰화기는 LLaMA2 7B에서 사용되는 토큰화기와 다르기 때문이다. 공정한 비교를 위해 우리는 평가 메트릭으로 바이트당 비트(BPB)를 사용한다.\n' +
      '\n' +
      '읽기 이해.읽기 이해를 위해 DROP(Dua et al., 2019)를 추가적으로 고려한다. 평가 메트릭은 정확 일치율(EM)입니다.\n' +
      '\n' +
      'Math Reasoning. 수학 추론을 위해 EM을 평가 메트릭으로 사용하여 GSM8K(Cobbe et al., 2021) 및 MATH(Hendrycks et al., 2021)를 추가로 통합한다.\n' +
      '\n' +
      'Multi-Subject Multiple-Choice. Multi-subject multiple-choice에 대해, 우리는 MMLU(Hendrycks et al., 2020)에 대한 모델들을 추가적으로 평가한다. 평가 메트릭은 정확성입니다.\n' +
      '\n' +
      '중복성 해소를 위해 WinoGrande (Sakaguchi et al., 2019)를 추가적으로 고려하며, 평가 메트릭은 정확도이다.\n' +
      '\n' +
      '중국 벤치마크. DeepSeekMoE 16B는 이중 언어 코퍼스에서 사전 훈련되기 때문에 4개의 중국 벤치마크에서도 평가한다. CLUEWSC(Xu et al., 2020)는 중국어 중의성 해소 벤치마크이다. CEval(Huang et al., 2023) 및 CMMLU(Li et al., 2023)는 MMLU와 유사한 형태를 갖는 두 개의 중국어 다중주제 객관식 벤치마크이다. CHID(Zheng et al., 2019)는 중국문화에 대한 이해를 평가하는 것을 목표로 하는 중국어 관용어 완성 벤치마크이다. 앞서 언급한 중국 벤치마크에 대한 평가 지표는 정확성 또는 EM이다.\n' +
      '\n' +
      '오픈 LLM 리더보드.우리는 내부 평가 프레임워크를 기반으로 앞서 언급한 모든 벤치마크를 평가한다. DeepSeekMoE 16B를 오픈소스 모델과 공정하고 편리하게 비교하기 위해 Open LLM 리더보드 상에서 DeepSeekMoE 16B를 추가적으로 평가한다. Open LLM Leaderboard는 HuggingFace가 지원하는 공개 리더보드로, ARC(Clark et al., 2018), HellaSwag(Zellers et al., 2019), MMLU(Hendrycks et al., 2020), TruthfulQA(Lin et al., 2022), Winogrande(Sakaguchi et al., 2019), GSM8K(Cobbe et al., 2021)의 6개의 태스크로 구성된다.\n' +
      '\n' +
      '### Evaluations\n' +
      '\n' +
      '#### 5.2.1 Internal Comparison with DeepSeek 7B\n' +
      '\n' +
      '먼저 6.9B 파라미터를 갖는 밀집 언어 모델인 DeepSeekMoE 16B와 DeepSeek 7B(DeepSeekAI, 2024)의 내부 비교를 수행한다. 공정성을 보장하기 위해 두 모델은 2T 토큰이 있는 동일한 코퍼스에서 훈련된다. 이를 통해 훈련 데이터의 영향과 무관하게 MoE 아키텍처의 유효성을 정확하게 평가할 수 있다.\n' +
      '\n' +
      '평가 결과는 표 3에 제시되어 다음과 같은 관찰을 산출한다. (1) 전체적으로 계산의 약 40%만으로 DeepSeeMoE 16B는 DeepSeek 7B와 비교 가능한 성능을 달성한다. (2) DeepSeekMoE 16B는 Pile, HellaSwag, TriviaQA 및 NaturalQuestions와 같은 언어 모델링 및 지식 집약적 태스크에서 주목할 만한 강점을 나타낸다. MoE 모델에서 FFN 파라미터가 주의 파라미터보다 훨씬 무겁다는 점을 감안할 때, 이러한 결과는 트랜스포머의 FFN이 지식 암기 능력을 나타낸다는 명제와 일치한다(Dai et al., 2022). (3) 다른 태스크에 대한 우수한 성능과 비교하여, DeepSeekMoE는 객관식 태스크를 다루는데 한계를 나타낸다. 이러한 부적절함은 DeepSeekMoE 16B에서의 제한된 주의 파라미터로부터 기인한다(DeepSeekMoE 16B는 단지 약 0.5B 주의 파라미터를 갖는 반면, DeepSeek 7B는 2.5B 주의 파라미터를 갖는다). 딥씽 7B에 대한 우리의 초기 조사는 객관식 과제에 대한 주의력과 수행 사이의 양의 상관관계를 보여준다. 예를 들어 다중 질의 어텐션 메커니즘(Shazeer, 2019)을 탑재한 DeepSeek 7B MQA도 MMLU와 같은 작업에서 고전했다. 또한, 훈련과정에 대한 보다 포괄적인 이해를 위하여\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|r r} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **DeepSeek 7B (Dense)** & **DeepSeekMoE 16B** \\\\ \\hline \\# Total Params & N/A & 6.9B & 16.4B \\\\ \\# Activated Params & N/A & 6.9B & 2.8B \\\\ FLOPs per 4K Tokens & N/A & 183.5T & 74.4T \\\\ \\# Training Tokens & N/A & 2T & 2T \\\\ \\hline Pile (BPB) & N/A & 0.75 & **0.74** \\\\ \\hline HellaSwag (Acc.) & 0-shot & 75.4 & **77.1** \\\\ PIQA (Acc.) & 0-shot & 79.2 & **80.2** \\\\ ARC-easy (Acc.) & 0-shot & **67.9** & **68.1** \\\\ ARC-challenge (Acc.) & 0-shot & 48.1 & **49.8** \\\\ \\hline RACE-middle (Acc.) & 5-shot & **63.2** & 61.9 \\\\ RACE-high (Acc.) & 5-shot & **46.5** & **46.4** \\\\ DROP (EM) & 1-shot & **34.9** & 32.9 \\\\ \\hline GSM8K (EM) & 8-shot & 17.4 & **18.8** \\\\ MATH (EM) & 4-shot & 3.3 & **4.3** \\\\ \\hline HumanEval (Pass@1) & 0-shot & 26.2 & **26.8** \\\\ MBPP (Pass@1) & 3-shot & **39.0** & **39.2** \\\\ \\hline TriviaQA (EM) & 5-shot & 59.7 & **64.8** \\\\ NaturalQuestions (EM) & 5-shot & 22.2 & **25.5** \\\\ \\hline MMLU (Acc.) & 5-shot & **48.2** & 45.0 \\\\ \\hline WinoGrande (Acc.) & 0-shot & **70.5** & **70.2** \\\\ \\hline \\hline CLUEWSC (EM) & 5-shot & **73.1** & 72.1 \\\\ CEval (Acc.) & 5-shot & **45.0** & 40.6 \\\\ CMMLU (Acc.) & 5-shot & **47.2** & 42.5 \\\\ CHID (Acc.) & 0-shot & **89.3** & **89.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: DeepSeek 7B와 DeepSeekMoE 16B의 비교. **굵게 표시된** 글꼴은 최고 또는 최고 근처를 나타냅니다. 40.5%의 계산만으로 DeepSeekMoE 16B는 DeepSeek 7B와 비교 가능한 성능을 달성한다.\n' +
      '\n' +
      'DeepSeekMoE 16B, 또한 참조를 위해 부록 C에서 훈련하는 동안 DeepSeekMoE 16B 및 DeepSeek 7B(Dense)의 벤치마크 곡선을 제공한다.\n' +
      '\n' +
      '비판적으로 DeepSeekMoE 16B의 적당한 수의 파라미터로 인해 40GB의 메모리를 가진 GPU에서 단일 장치 배포를 가능하게 한다. 적절한 연산자 최적화를 통해 7B 밀집 모델의 추론 속도의 거의 2.5배를 달성할 수 있다.\n' +
      '\n' +
      '#### 5.2.2 Open Source Model과의 비교\n' +
      '\n' +
      'LLaMA2 7B와의 내부 비교 오픈 소스 모델의 영역에서는 주로 DeepSeekMoE 16B와 6.7B 파라미터를 가진 잘 알려져 있고 강력한 오픈 소스 언어 모델인 LLaMA2 7B(Touvron et al., 2023b)를 비교한다. DeepSeekMoE 16B와 LLaMA2 7B는 모두 2T 토큰에 대해 사전 훈련된다. LLaMA2 7B와 비교하여 DeepSeekMoE는 전체 매개변수의 245%를 갖지만 계산의 39.6%만 필요하다. 내부 벤치마크에 대한 결과는 표 4에 나와 있으며 다음과 같은 관찰로 이어진다. (1) 평가된 벤치마크 중에서, 단지 약 40%의 계산으로, DeepSeekMoE 16B는 대부분의 벤치마크에서 LLaMA2 7B를 능가한다. (2) DeepSeekMoE 16B의 수학 추론 및 코드 생성 능력\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **LLaMA2 7B** & **DeepSeekMoE 16B** \\\\ \\hline \\# Total Params & N/A & 6.7B & 16.4B \\\\ \\# Activated Params & N/A & 6.7B & 2.8B \\\\ FLOPs per 4K Tokens & N/A & 187.9T & 74.4T \\\\ \\# Training Tokens & N/A & 2T & 2T \\\\ \\hline Pile (BPB) & N/A & 0.76 & **0.74** \\\\ \\hline HellaSwag (Acc.) & 0-shot & 75.6 & **77.1** \\\\ PIQA (Acc.) & 0-shot & 78.0 & **80.2** \\\\ ARC-easy (Acc.) & 0-shot & **69.1** & 68.1 \\\\ ARC-challenge (Acc.) & 0-shot & 49.0 & **49.8** \\\\ \\hline RACE-middle (Acc.) & 5-shot & 60.7 & **61.9** \\\\ RACE-high (Acc.) & 5-shot & 45.8 & **46.4** \\\\ DROP (EM) & 1-shot & **34.0** & 32.9 \\\\ \\hline GSM8K (EM) & 8-shot & 15.5 & **18.8** \\\\ MATH (EM) & 4-shot & 2.6 & **4.3** \\\\ \\hline HumanEval (Pass@1) & 0-shot & 14.6 & **26.8** \\\\ MBPP (Pass@1) & 3-shot & 21.8 & **39.2** \\\\ \\hline TriviaQA (EM) & 5-shot & 63.8 & **64.8** \\\\ NaturalQuestions (EM) & 5-shot & **25.5** & **25.5** \\\\ \\hline MMLU (Acc.) & 5-shot & **45.8** & 45.0 \\\\ \\hline WinoGrande (Acc.) & 0-shot & 69.6 & **70.2** \\\\ \\hline \\hline CLUEWSC (EM) & 5-shot & 64.0 & **72.1** \\\\ CEval (Acc.) & 5-shot & 33.9 & **40.6** \\\\ CMMLU (Acc.) & 5-shot & 32.6 & **42.5** \\\\ CHID (Acc.) & 0-shot & 37.9 & **89.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: | LLaMA2 7B와 DeepSeekMoE 16B의 비교입니다. 계산량이 39.6%에 불과하여 DeepSeekMoE 16B는 대부분의 벤치마크에서 LLaMA2 7B를 능가한다.\n' +
      '\n' +
      'LLaMA2 7B보다 강하며, 이는 사전 훈련 말뭉치에 수학적 및 코드 관련 텍스트가 풍부하게 존재하기 때문이다. (3) 사전 훈련 코퍼스에 중국어 텍스트가 존재하는 경우, DeepSeekMoE 16B는 중국 벤치마크에서 LLaMA2 7B에 비해 상당한 성능 이점을 나타낸다. (4) 더 적은 수의 영어 텍스트들에 대해 트레이닝되었음에도 불구하고, DeepSeekMoE 16B는 영어 이해 또는 지식 집약적 벤치마크들에 대해 LLaMA2 7B와 비교되거나 더 나은 성능을 달성하며, 이는 DeepSeekMoE 16B의 탁월한 능력을 입증한다.\n' +
      '\n' +
      'Open LLM Leaderboard에 대한 평가. 내부 평가 외에도 Open LLM Leaderboard에서 DeepSeekMoE 16B를 평가하고 다른 오픈 소스 모델과 비교한다. LLaMA2 7B 외에도, LLaMA 7B(Touvron et al., 2023a), Falcon 7B(Almazrouei et al., 2023), GPT-J 6B(Wang and Komatsuzaki, 2021), RedPajama-INCITE 7B 및 3B(Together-AI, 2023), Open LLaMA 7B 및 3B(Geng and Liu, 2023), OPT 2.7B(Zhang et al., 2022), Pythia 2.8B(Biderman et al., 2023), GPT-neo 2.7B(Black et al., 2021), 및 BLOOM 3B(Scao et al., 2022)를 포함하는 광범위한 오픈 소스 모델 세트를 고려한다. 그림 1에 제시된 평가 결과는 DeepSeekMoE 16B가 유사한 활성화된 매개변수를 가진 모델보다 큰 마진만큼 일관되게 우수하다는 것을 보여준다. 또한, 약 2.5배의 활성화된 파라미터를 갖는 LLaMA2 7B와 유사한 성능을 달성한다.\n' +
      '\n' +
      '## 6 Alignment for DeepSeekMoE 16B\n' +
      '\n' +
      '이전 연구에 따르면 MoE 모델은 일반적으로 미세 조정에서 유의미한 이득을 나타내지 않는다(Artetxe et al., 2022; Fedus et al., 2021). 그러나 Shen et al.(2023)은 MoE 모델이 실제로 명령어 튜닝의 이점을 얻을 수 있음을 시사하는 결과를 제시한다. DeepSeekMoE 16B가 미세조정의 이점을 얻을 수 있는지 평가하기 위해 DeepSeekMoE 16B를 기반으로 채팅 모델을 구성하기 위해 감독 미세조정을 수행한다. 실험 결과는 DeepSeekMoE Chat 16B도 LLaMA2 SFT 7B 및 DeepSeek Chat 7B와 유사한 성능을 달성함을 보여준다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '훈련 데이터.채팅 모델을 훈련하기 위해 1.4M 훈련 예제로 구성된 자체 선별된 데이터에 대해 감독 미세 조정(SFT)을 수행한다. 이 데이터 세트는 수학, 코드, 쓰기, 질문 답변, 추론, 요약 등을 포함한 광범위한 범주에 걸쳐 있습니다. 우리의 SFT 훈련 데이터의 대부분은 영어와 중국어로 되어 있어 채팅 모델을 다재다능하고 이중 언어 시나리오에서 적용할 수 있다.\n' +
      '\n' +
      'Hyper-Parameters.감독 미세 조정 동안 배치 크기를 1024개의 예로 설정하고 AdamW 최적화기(Loshchilov and Hutter, 2019)를 사용하여 8개의 에폭에 걸쳐 훈련을 수행한다. 우리는 4K의 최대 시퀀스 길이를 사용하고, 시퀀스 길이 한계에 도달할 때까지 훈련 예제를 최대한 조밀하게 패킹한다. 우리는 감독된 미세 조정을 위해 드롭아웃을 사용하지 않고 학습률 스케줄링 전략을 통합하지 않고 단순히 \\(10^{-5}\\)의 일정한 학습률을 설정한다.\n' +
      '\n' +
      '평가 벤치마크.채팅 모델의 평가를 위해 섹션 5.1.3에서 사용된 것과 유사한 벤치마크를 사용하며 다음과 같이 조정한다. (1) 채팅 모델은 순수 언어 모델링에 거의 사용되지 않기 때문에 파일(Gao 등, 2020)을 제외한다. (2) 관찰된 결과의 불안정성으로 인해 CHID(Zheng et al., 2019)를 제외하여 견고한 결론 도출을 방해한다. (3) 채팅 모델들의 추론 능력에 대한 보다 포괄적인 평가를 제공하기 위해 BBH(Suzgun et al., 2022)를 추가로 포함한다.\n' +
      '\n' +
      '### Evaluations\n' +
      '\n' +
      '기준.정렬 후 DeepSeekMoE 16B의 잠재력을 검증하기 위해 LLaMA2 7B, DeepSeek 7B 및 DeepSeekMoE 16B에 대해 감독 미세 조정을 수행하며, 공정성을 보장하기 위해 완전히 동일한 미세 조정 데이터를 활용한다. 이에 대응하여 LLaMA2 SFT 7B3, DeepSeek Chat 7B, DeepSeekMoE Chat 16B 등 3개의 채팅 모델을 구축한다. 그 후, 우리는 DeepSeekMoE Chat 16B를 광범위한 다운스트림 태스크에 걸쳐 다른 두 개의 밀집 채팅 모델(FLOP의 약 2.5배)과 비교한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **LLaMA2** & **DeepSeek** & **DeepSeekMoE** \\\\  & & **SFT 7B** & **Chat 7B** & **Chat 16B** \\\\ \\hline \\# Total Params & N/A & 6.7B & 6.9B & 16.4B \\\\ \\# Activated Params & N/A & 6.7B & 6.9B & 2.8B \\\\ FLOPs per 4K Tokens & N/A & 187.9T & 183.5T & 74.4T \\\\ \\hline HellaSwag (Acc.) & 0-shot & 67.9 & 71.0 & **72.2** \\\\ PIQA (Acc.) & 0-shot & 76.9 & 78.4 & **79.7** \\\\ ARC-easy (Acc.) & 0-shot & 69.7 & **70.2** & **69.9** \\\\ ARC-challenge (Acc.) & 0-shot & **50.8** & 50.2 & 50.0 \\\\ BBH (EM) & 3-shot & 39.3 & **43.1** & 42.2 \\\\ \\hline RACE-middle (Acc.) & 5-shot & 63.9 & **66.1** & 64.8 \\\\ RACE-high (Acc.) & 5-shot & 49.6 & **50.8** & **50.6** \\\\ DROP (EM) & 1-shot & 40.0 & **41.7** & 33.8 \\\\ \\hline GSM8K (EM) & 0-shot & **63.4** & 62.6 & 62.2 \\\\ MATH (EM) & 4-shot & 13.5 & 14.7 & **15.2** \\\\ \\hline HumanEval (Pass@1) & 0-shot & 35.4 & 45.1 & **45.7** \\\\ MBPP (Pass@1) & 3-shot & 27.8 & 39.0 & **46.2** \\\\ \\hline TriviaQA (EM) & 5-shot & 60.1 & 59.5 & **63.3** \\\\ NaturalQuestions (EM) & 0-shot & **35.2** & 32.7 & **35.1** \\\\ \\hline MMLU (Acc.) & 0-shot & **50.0** & 49.7 & 47.2 \\\\ \\hline WinoGrande (Acc.) & 0-shot & 65.1 & 68.4 & **69.0** \\\\ \\hline \\hline CLUEWSC (EM) & 5-shot & 48.4 & 66.2 & **68.2** \\\\ CEval (Acc.) & 0-shot & 35.1 & **44.7** & 40.0 \\\\ CMMLU (Acc.) & 0-shot & 36.9 & **51.2** & 49.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: LLaMA2 SFT 7B, DeepSeek Chat 7B 및 DeepSeekMoE Chat 16B 간의 비교이며, 이 세 모델 모두 동일한 SFT 데이터에서 미세 조정된다. 두 7B 밀집 모델과 비교하여 DeepSeekMoE Chat 16B는 여전히 40%의 계산만으로 대부분의 벤치마크에서 비슷하거나 더 나은 성능을 달성한다.\n' +
      '\n' +
      '평가 결과는 표 5에 나와 있다. 우리의 주요 관찰은 다음과 같다. (1) DeepSeekMoE Chat 16B는 계산의 거의 40%를 소비하지만 언어 이해 및 추론(PIQA, ARC, BBH), 기계 독해(RACE), 수학적(GSM8K, MATH), 지식 집약적 작업(TriviaQA, NaturalQuestions)에 걸쳐 7B 밀집 모델과 비교 가능한 성능을 달성한다. (2) 코드 생성 작업에서 DeepSeekMoE Chat 16B는 LLaMA2 SFT 7B를 상당히 능가하여 HumanEval 및 MBPP에서 주목할만한 개선을 보여준다. 또한 딥씽 채팅 7B를 능가합니다. (3) MMLU, CEval 및 CMMLU를 포함하는 객관식 질의 응답 벤치마크에서, DeepSeekMoE Chat 16B는 여전히 기본 모델에 대한 관찰과 일치하는 DeepSeek Chat 7B에 뒤처진다(섹션 5.2.1). 그러나 감독 미세 조정 후 DeepSeekMoE 16B와 DeepSeek 7B 사이의 성능 격차가 좁혀진다는 점에 주목할 필요가 있다. (4) 이중 언어 코퍼스에 대한 사전 훈련으로부터 이득을 얻은 DeepSeekMoE Chat 16B는 모든 중국 벤치마크에서 LLaMA2 SFT 7B를 현저하게 능가한다. 이러한 결과는 중국어와 영어 모두에서 DeepSeekMoE 16B의 균형 잡힌 기능을 보여주며 다양한 시나리오에서 다양성과 적용 가능성을 향상시킨다. 결론적으로 채팅 모델에 대한 평가는 정렬의 이점을 얻을 수 있는 DeepSeekMoE 16B의 잠재력을 강조하고 계산의 약 40%만 사용하면서 밀집 모델과 비교할 수 있는 성능을 달성하는 일관된 이점을 검증한다.\n' +
      '\n' +
      '## 7 DeepSeekMoE 145B Ongoing\n' +
      '\n' +
      'DeepSeekMoE 16B의 뛰어난 성능에 고무되어 DeepSeekMoE를 145B로 확장하기 위한 예비 노력을 더 수행한다. 이 초기 연구에서 DeepSeekMoE 145B는 245B 토큰에 대해 훈련되지만 GShard 아키텍처에 비해 일관된 이점을 보여주었고 DeepSeek 67B(Dense)의 성능과 일치하거나 초과할 가능성을 보여주었다. 나아가 DeepSeekMoE 145B의 최종 버전과 전수 교육이 완료되면 공개도 할 계획이다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '훈련 데이터 및 토큰화. DeepSeekMoE 145B의 경우 DeepSeekMoE 16B와 정확히 동일한 훈련 코퍼스 및 토큰화기를 사용하며, 유일한 차이점은 초기 연구를 위해 DeepSeekMoE 145B가 245B 토큰에 대해 훈련된다는 것이다.\n' +
      '\n' +
      '모델 설정 DeepSeekMoE 145B의 경우 트랜스포머 층의 수를 62개, 은닉 차원을 4096개로 설정하였다. 총 32개의 어텐션 헤드가 있는 다중 헤드 어텐션 메커니즘을 사용하며, 각 헤드의 차원은 128이다. 초기화는 학습 가능한 모든 파라미터가 표준편차 0.006으로 랜덤하게 초기화된다. DeepSeekMoE 16B에서와 마찬가지로 첫 번째 레이어를 제외한 FFN도 모두 MoE 레이어로 대체한다. 각 MoE 레이어는 4명의 공유 전문가와 128명의 라우팅 전문가로 구성되며, 각 전문가는 표준 FFN의 0.125배 크기이다. 각 토큰은 이 4명의 공유 전문가와 128명의 라우팅된 전문가 중 12명에게 라우팅됩니다. 이러한 구성 하에서, DeepSeekMoE 145는 약 144.6B의 총 파라미터를 가지며, 활성화된 파라미터의 수는 약 22.2B이다.\n' +
      '\n' +
      '훈련 설정. 하이퍼 파라미터를 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), weight_decay = 0.1로 설정한 AdamW 최적화기(Loshchilov and Hutter, 2019)를 사용한다. DeepSeekMoE 145B의 예비 연구를 위해 웜업 및 상수 학습률 스케줄러를 사용한다. 처음에, 학습률은 처음 2K 단계 동안 0에서 최대값까지 선형적으로 증가한다.\n' +
      '\n' +
      '이어서, 나머지 훈련 과정 동안 학습률은 일정하게 유지된다. DeepSeekMoE 145B의 최대 학습률은 \\(3.0\\times 10^{-4}\\)로 설정되고, 기울기 클리핑 규범은 1.0으로 설정된다. 배치 크기는 4.5K로 설정되고, 최대 시퀀스 길이가 4K인 각 훈련 배치에는 18M 토큰이 포함된다. 우리는 DeepSeekMoE 145B를 13,000단계 동안 훈련하여 245B 훈련 토큰을 달성합니다. 또한, 우리는 훈련 중에 중퇴를 사용하지 않습니다. 파이프라인 병렬성을 활용하여 모델의 다른 계층을 다른 장치에 배치하며, 각 계층에 대해 라우팅된 모든 전문가가 4개의 장치(즉, 데이터 병렬성과 결합된 전문가 병렬성)에 균일하게 배치됩니다. DeepSeekMoE 145B에 대해 전문가 병렬성을 사용하기 때문에 계산 병목 현상을 줄이기 위해 장치 수준의 부하 균형을 고려해야 한다. 이에 대해 장치 수준 균형 계수를 0.05로 설정하여 장치 간 균형 계산을 권장합니다. 또한 라우팅 붕괴를 방지하기 위해 여전히 0.003의 작은 전문가 수준의 균형 계수를 설정했습니다.\n' +
      '\n' +
      '평가 벤치마크.우리는 DeepSeekMoE 145B에 사용된 것과 정확히 동일한 내부 벤치마크에 대해 DeepSeekMoE 145B를 평가한다(섹션 5.1.3 참조).\n' +
      '\n' +
      '### Evaluations\n' +
      '\n' +
      '기준. **DeepSeekMoE 145B** 외에도 비교를 위해 세 가지 추가 모델을 고려 합니다. **DeepSeek 67B(밀도)** 는 67.4B 총 매개 변수(모델 및 훈련 세부 정보는 DeepSeek-AI(2024) 참조)가 있는 밀집 모델입니다. **GShard 137B** 는 DeepSeekMoE 145B와 동일한 숨겨진 차원 및 레이어 수를 공유하지만 GShard 아키텍처를 따릅니다. DeepSeekMoE 145B는 계산 효율성을 위해 각 전문가의 중간 은닉 차원을 64의 배수로 정렬하므로 모델 크기는 GShard 137B보다 6% 더 크다. **DeepSeekMoE 142B(Half Activated)**는 DeepSeekMoE 145B와 유사한 아키텍처를 가지고 있지만 공유 전문가 2명만 포함하고 라우팅된 전문가 128명 중 6명만 활성화됩니다. DeepSeekMoE 145B를 포함한 비교된 모든 모델이 동일한 훈련 코퍼스를 공유한다는 점은 주목할 만하다. 또한, 비교의 모든 MoE 모델은 처음부터 훈련되고 동일한 훈련 하이퍼-파라미터를 공유한다.\n' +
      '\n' +
      '결과.표 6에 제시된 평가 결과로부터, 우리는 다음과 같은 관찰들을 갖는다: (1) 유사한 총 파라미터들 및 계산들을 가짐에도 불구하고, DeepSeekMoE 145B는 GShard 137B를 상당히 능가하여, DeepSeekMoE 아키텍처의 이점을 다시 강조한다. (2) 전체적으로, 28.5%의 계산만으로, DeepSeekMoE 145B는 DeepSeek 67B(Dense)와 비교 가능한 성능을 달성한다. DeepSeekMoE 16B의 결과와 일치하게, DeepSeekMoE 145B는 언어 모델링 및 지식 집약적 작업에서 현저한 강점을 나타내지만 객관식 작업에서는 한계가 있다. (3) 더 큰 규모에서, DeepSeekMoE 142B(Half Activated)의 성능은 DeepSeekMoE 145B로부터 너무 많이 뒤처지지 않는다. 또한, 활성화된 전문가 매개변수의 절반만을 가지고 있음에도 불구하고 DeepSeekMoE 142B(Half Activated)는 여전히 DeepSeek 67B(Dense)의 성능과 18.2%의 계산만으로 일치한다. 또한 섹션 4.5의 결론과 일치하는 GShard 137B를 능가한다.\n' +
      '\n' +
      '## 8 관련 작업\n' +
      '\n' +
      'Mixture of Experts (MoE) 기법은 Jacobs et al. (1991); Jordan and Jacobs (1994)에 의해 독립 전문가 모듈로 서로 다른 샘플을 다루기 위해 처음 제안되었다. Shazeer et al.(2017)은 언어 모델 훈련에 MoE를 도입하고 대규모 LSTM 기반(Hochreiter and Schmidhuber, 1997) MoE 모델을 구축한다. 트랜스포머가 NLP의 가장 인기 있는 아키텍처가 되면서 많은 시도가 트랜스포머에서 FFN을 MoE 계층으로 확장하여 MoE 언어 모델을 구축한다. GShard (Lepikhin et al., 2021)와 Switch Transformer (Fedus et al., 2021)는 MoE 언어 모델을 매우 큰 규모로 확장하기 위해 학습 가능한 상위 2 또는 상위 1 라우팅 전략을 사용하는 선구자이다. Hash Layer (Roller et al., 2021) 및 StableMoE (Dai et al., 2022b)는 보다 안정적인 라우팅 및 트레이닝을 위해 고정 라우팅 전략을 사용한다. Zhou et al.(2022)은 전문가-선택 라우팅 전략을 제안하며, 여기서 각 토큰은 서로 다른 수의 전문가에게 할당될 수 있다. Zoph(2022)는 MoE 모델의 훈련 불안정성 및 미세 조정 어려움 문제에 초점을 맞추고,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c|c} \\hline \\hline \\multirow{2}{*}{**Metric**} & \\multirow{2}{*}{**\\# Shot**} & \\multirow{2}{*}{\\begin{tabular}{c} **DeepSeek** \\\\ **67B (Dense)** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **GShard** \\\\ **137B** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **DeepSeekMoE** \\\\ **145B** \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} **DeepSeekMoE** \\\\ **(Half Activated)** \\\\ \\end{tabular} } \\\\ \\hline \\# Total Params & N/A & 67.4B & 136.5B & 144.6B & 142.3B \\\\ \\# Activated Params & N/A & 67.4B & 21.6B & 22.2B & 12.2B \\\\ Relative Expert Size & N/A & N/A & 1 & 0.125 & 0.125 \\\\ \\# Experts & N/A & N/A & \\(0+16\\) & \\(4+128\\) & \\(2+128\\) \\\\ \\# Activated Experts & N/A & N/A & \\(0+2\\) & \\(4+12\\) & \\(2+6\\) \\\\ FLOPs per 4K Tokens & N/A & 2057.5T & 572.7T & 585.6T & 374.6T \\\\ \\# Training Tokens & N/A & 245B & 245B & 245B & 245B \\\\ \\hline Pile (Loss.) & N/A & 1.905 & 1.961 & **1.876** & 1.888 \\\\ \\hline HellaSwag (Acc.) & 0-shot & 74.8 & 72.0 & **75.8** & 74.9 \\\\ PIQA (Acc.) & 0-shot & 79.8 & 77.6 & **80.7** & 80.2 \\\\ ARC-easy (Acc.) & 0-shot & 69.0 & 64.0 & **69.7** & 67.9 \\\\ ARC-challenge (Acc.) & 0-shot & **50.4** & 45.8 & 48.8 & 49.0 \\\\ \\hline RACE-middle (Acc.) & 5-shot & **63.2** & 59.2 & 62.1 & 59.5 \\\\ RACE-high (Acc.) & 5-shot & **46.9** & 43.5 & 45.5 & 42.6 \\\\ DROP (EM) & 1-shot & **27.5** & 21.6 & **27.8** & 28.9 \\\\ GSM8K (EM) & 8-shot & **11.8** & 6.4 & **12.2** & 13.8 \\\\ MATH (EM) & 4-shot & 2.1 & 1.6 & **3.1** & 2.8 \\\\ \\hline HumanEval (Pass@1) & 0-shot & **23.8** & 17.7 & 19.5 & 23.2 \\\\ MBPP (Pass@1) & 3-shot & **33.6** & 27.6 & **33.2** & 32.0 \\\\ \\hline TriviaQA (EM) & 5-shot & 57.2 & 52.5 & **61.1** & 59.8 \\\\ NaturalQuestions (EM) & 5-shot & 22.6 & 19.0 & **25.0** & 23.5 \\\\ \\hline MMLU (Acc.) & 5-shot & **45.1** & 26.3 & 39.4 & 37.5 \\\\ \\hline WinoGrande (Acc.) & 0-shot & 70.7 & 67.6 & **71.9** & 70.8 \\\\ \\hline \\hline CLUEWSC (EM) & 5-shot & 69.1 & 65.7 & **71.9** & 72.6 \\\\ CEval (Acc.) & 5-shot & **40.3** & 26.2 & 37.1 & 32.8 \\\\ CMMLU (Acc.) & 5-shot & **40.6** & 25.4 & 35.9 & 31.9 \\\\ CHID (Acc.) & 0-shot & 88.5 & 86.9 & **90.3** & 88.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: \\(|\\) 약 140B 총 파라미터의 스케일에서 DeepSeek 67B (Dense) 및 MoE 모델 간의 비교. "# 전문가" 및 "# 활성화된 전문가"의 줄에서 \\(a+b\\)는 각각 \\(a\\) 공유 전문가 및 \\(b\\) 라우팅된 전문가를 나타낸다. **굵게 표시된** 글꼴은 마지막 열을 제외하고 가장 좋은 성능을 나타내거나 가장 가까운 성능을 나타냅니다. DeepSeekMoE 145B, 심지어 활성화된 전문가 매개변수의 절반만 있는 DeepSeekMoE 142B(Half Activated)는 GShard 137B를 큰 마진 차이로 능가한다. 또한, 28.5%의 계산량으로 DeepSeekMoE 145B는 DeepSeek 67B와 비슷한 성능을 보인다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n' +
      'S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. 오브라이언, E. 한라한, M. A. 칸, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, O. 반 더 월 피티아: 훈련과 스케일링 전반에 걸쳐 대규모 언어 모델을 분석하기 위한 제품군입니다. A. Krause, E. Brunskill, K. 조병하트 Sabato and J. Scarlett, editors, _International Conference on Machine Learning_, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, Volume 202 of _Proceedings of Machine Learning Research_, pages 2397-2430. PMLR, 2023. URL [https://proceedings.mlr.press/v202/biderman23a.html](https://proceedings.mlr.press/v202/biderman23a.html)\n' +
      '* Bisk et al.(2020) Y. 비스크 젤러스, R. L. Bras, J. Gao, 그리고 Y. 최 PIQA: 자연어의 물리적 상식에 대한 추론. _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 7432-7439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL [https://doi.org/10.1609/aaai.v34i05.6239](https://doi.org/10.1609/aaai.v34i05.6239)\n' +
      '* Black et al.(2021) S. Black L. Gao, P. Wang, C. Leahy, S. 바이더맨 GPT-Neo: Mesh-Tensorflow, Mar. 2021. URL [https://doi.org/10.5281/zenodo.5297715](https://doi.org/10.5281/zenodo.5297715). 이 misc를 사용하는 경우 이러한 메타데이터를 사용하여 인용하십시오.\n' +
      '* Brown et al.(2020) T. B. Brown, B. Mann, N. 라이더 Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. 헤니간 Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. 천은실러 리트윈 Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei. 언어 모델은 샷이 적은 학습자입니다. _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)\n' +
      '* Chen et al.(2021) M. 천진투렉 Wuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. 버다 조지프, G. 브록맨, A. 레이, R 푸리 G. 크루거 페트로프, H. 클라프, G. 사스트리, P. 미슈킨, B. 찬, S. 그레이 라이더 파블로프 A.파워 L. 카이저 Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. 테작 J. Tang, I. Babuschkin, S. 발라지 장욱 손더스, C. 헤세, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. 나이트미 M.Brundage 무라티 Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, 그리고 W. 자렘바 코드에 대해 학습된 대규모 언어 모델을 평가하는 중입니다. _ CoRR_, abs/2107.03374, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).\n' +
      '* Clark et al.(2018) P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. 타피오르드 질문을 풀었다고 생각해? AI2 추론 챌린지를 시도합니다. _ CoRR_, abs/1803.05457, 2018. URL [http://arxiv.org/abs/1803.05457](http://arxiv.org/abs/1803.05457).\n' +
      '* Cobbe et al.(2021) K. 코베 고사라주 바이에른 천현준 카이저 플래퍼트, J. 투렉, J. 힐튼, R. Nakano, et al. Training verifiers to solve math word problem. _ arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* Dai et al.(2022) D. Dai, L. 동영 하정 수이, 보창, 에프웨이 사전 훈련된 변압기의 지식 뉴런. In S. Muresan, P. Nakov, and A. Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, _ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 8493-8502. Association for Computational Linguistics, 2022a. doi: 10.18653/V1/2022.ACL-LONG.581. URL [https://doi.org/10.18653/v1/2022.acl-long.581](https://doi.org/10.18653/v1/2022.acl-long.581).\n' +
      '* Chen et al.(2020)D. 대락 동성 마병정 수이, 보창, 에프웨이 안정모: 전문가의 혼합을 위한 안정적인 라우팅 전략. In S. Muresan, P. Nakov, and A. Villavicencio, 편집자, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 7085-7095. Association for Computational Linguistics, 2022b. doi: 10.18653/V1/2022.ACL-LONG.489. URL [https://doi.org/10.18653/v1/2022.acl-long.489](https://doi.org/10.18653/v1/2022.acl-long.489).\n' +
      '* DeepSeek-AI (2024) DeepSeek-AI. 딥섹 llm: 장기주의를 가진 오픈 소스 언어 모델을 확장합니다. _ arXiv preprint arXiv:2401.02954_, 2024.\n' +
      '* Du et al.(2022) N. 두영 황암만대 통동래핀 서민 크리쿤 주아원유 피라트 B.조프 L. Fedus, M. P. Bosma, Z. 주태 왕윤은 웹스터 펠라트 T. K. S. Meier-Hellstern 로빈슨 듀크 딕슨 장규범 우정 Chen, C. Cui. 글램: 전문가 혼합을 사용한 언어 모델의 효율적인 크기 조정 International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, Volume 162 of _Proceedings of Machine Learning Research_, pages 5547-5569. PMLR, 2022. URL [https://proceedings.mlr.press/v162/du22c.html](https://proceedings.mlr.press/v162/du22c.html)\n' +
      '* Dua et al.(2019) D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. 싱, 엠. 가드너 DROP: 단락에 대한 이산 추론이 필요한 읽기 이해 벤치마크. J. Burstein, C. Doran, T. Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, June 2-7, 2019, Volume 1(Long and Short Papers)_, pages 2368-2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL [https://doi.org/10.18653/v1/n19-1246](https://doi.org/10.18653/v1/n19-1246)\n' +
      '* Fedus et al.(2021) W. Fedus, B. Zoph, N. 셰이저 트랜스포머 전환: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장합니다. _ CoRR_, abs/2101.03961, 2021. URL [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961).\n' +
      '*Gao et al.(2020) L. 고성훈 비더만 Black L. 골딩, T Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트 _ arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '*Geng and Liu (2023) X. 겅과 류현 Openllama: 2023년 5월 llama의 열린 복제본. URL [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama)\n' +
      '* Harlap et al.(2018) A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons. 피드림: 빠르고 효율적인 파이프라인 병렬 DNN 훈련입니다. _ CoRR_, abs/1806.03377, 2018. URL [http://arxiv.org/abs/1806.03377](http://arxiv.org/abs/1806.03377).\n' +
      '* Hendrycks et al.(2020) D. Hendrycks, C. Burns, S. 바사르트 아조 마제이카, D. 송, J. 스타인하르트 대규모 멀티태스킹 언어 이해를 측정하는 중입니다. _ arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* Hendrycks et al.(2021) D. Hendrycks, C. Burns, S. 카다바스 아로라 바사르트, E. Tang, D. Song, J. 스타인하르트. 수학 데이터 세트를 사용하여 수학 문제 해결 측정, 2021.\n' +
      '* High-Flyer (2023) High-Flyer. 하이-llm: 2023 대 모델 교육을 위한 효율적이고 가벼운 도구입니다. URL [https://www.high-flyer.cn/en/blog/hai-llm](https://www.high-flyer.cn/en/blog/hai-llm).\n' +
      '* Hochreiter and Schmidhuber (1997) S. Hochreiter와 J. Schmidhuber 장기간 단기 기억. _ Neural Computing_, 9(8):1735-1780, 1997. URL [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).\n' +
      '* Hochreiter et al. (2019)J. 호프만 Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. 헤니건 이놀랜드 Millican, G. van den Driessche, B. Damoc, A. Guy, S. 오신데로 Simonyan, E. Elsen, J. W. Rae, O. Vinyals, L. 시프르 컴퓨팅 최적화 대용량 언어 모델 교육 _ CoRR_, abs/2203.15556, 2022. doi: 10.48550 / arXiv.2203.15556. URL [https://doi.org/10.48550/arXiv.2203.15556](https://doi.org/10.48550/arXiv.2203.15556).\n' +
      '* Huang et al.(2023) Y. 황영 배진호 주장장 수진유 Zhang, J. Lei, et al. C-Eval: A multi-level multi-disc discipline chinese evaluation suite for foundation models. _ arXiv preprint arXiv:2305.08322_, 2023.\n' +
      '* Jacobs 등(1991) R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. 지역 전문가의 적응형 혼합물입니다. _ Neural Computing_, 3(1):79-87, 1991. URL [https://doi.org/10.1162/neco.1991.3.1.79](https://doi.org/10.1162/neco.1991.3.1.79).\n' +
      '* Jordan and Jacobs (1994) M. I. Jordan and R. A. Jacobs. 전문가와 EM 알고리즘의 계층적 혼합물입니다. _ Neural Computing_, 6(2):181-214, 1994. URL [https://doi.org/10.1162/neco.1994.6.2.181](https://doi.org/10.1162/neco.1994.6.2.181).\n' +
      '* Joshi et al.(2017) M. Joshi, E. Choi, D. Weld, L. 제틀모이어 triviaqa: 읽기 이해를 위한 대규모 원격 감독 챌린지 데이터 세트 _ arXiv e-prints_, art. arXiv:1705.03551, 2017.\n' +
      '* Korthikanti 등 (2023) V. A. Korthikanti, J. Casper, S. 임락 맥아피 안데르시 쇼이비, B. 카탄자로 대형 변압기 모델에서 활성화 재계산 감소 _ Proceedings of Machine Learning and Systems_, 5, 2023.\n' +
      '* Kwiatkowski et al. (2019) T. Kwiatkowski J. Palomaki, O. 레드필드 콜린스, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. 켈시 J. 데블린 이경남 토타노바 존스 -W. 장아대, J.우즈코릿, Q. Le, S. 페트로프 자연스러운 질문: 질의 응답 연구를 위한 벤치마크입니다. _ Computational Linguistics 협회의 트랜잭션_, 2019.\n' +
      '* Lai et al.(2017) G. Lai, Q. 허현유 양은호 RACE: 시험의 대규모 읽기 이해 데이터 세트. In M. 파머 화, S. Riedel, editors, _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017_, pages 785-794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL [https://doi.org/10.18653/v1/d17-1082](https://doi.org/10.18653/v1/d17-1082).\n' +
      '* Lepikhin et al.(2021) D. Lepikhin, H. Lee, Y. 서덕천 피라트 황민 크리쿤 Shazeer, Z. 첸 Gshard: 조건부 계산 및 자동 샤딩으로 거대 모델을 스케일링합니다. 제9회 International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021. URL [https://openreview.net/forum?id=qrwe7XHTmYb](https://openreview.net/forum?id=qrwe7XHTmYb)\n' +
      '*Li et al.(2023) H. Li, Y. 장필고토 양호조 공남 Duan, T. 볼드윈 CMMLU: 중국어로 방대한 멀티태스킹 언어 이해도를 측정합니다. _ arXiv preprint arXiv:2306.09212_, 2023.\n' +
      '* Lin et al.(2021) J. Lin, R. 남아양주 딩영 장필왕 장상욱 자장장 주주 이희석 Deng, J. Liu, J. Xue, H. Zhou, J. Ma, J. Yu, Y. 이원 린종주, 탕종주, 양종주 M6: 중국 멀티모달 프리트레이너. _ CoRR_, abs/2103.00823, 2021. URL [https://arxiv.org/abs/2103.00823](https://arxiv.org/abs/2103.00823).\n' +
      '* Lin et al.(2022) S. Lin, J. Hilton, and O. 에반스 진실성: 모델들이 인간의 거짓을 어떻게 모방하는지 측정하는 것. In S. Muresan, P. Nakov, and A. Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 3214-3252. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.229. URL [https://doi.org/10.18653/v1/2022.a](https://doi.org/10.18653/v1/2022.a) cl-long.229.\n' +
      '* Loshchilov and Hutter [2019] I. Loshchilov and F. Hutter. 분리된 중량 감쇠 규칙화. 2019년 5월 6일부터 9일까지 _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)입니다.\n' +
      '* Narayanan et al. [2021] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15, 2021.\n' +
      '* OpenAI [2023] OpenAI. GPT-4 기술 보고서입니다. _ CoRR_, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)\n' +
      '* Rajbhandari et al. [2020] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In C. Cuicchi, I. Qualters, and W. T. Kramer, editors, _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL [https://doi.org/10.1109/SC41405.2020.00024](https://doi.org/10.1109/SC41405.2020.00024).\n' +
      '* Rajbhandari et al. [2022] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 18332-18346. PMLR, 2022. URL [https://proceedings.mlr.press/v162/rajbhandari22a.html](https://proceedings.mlr.press/v162/rajbhandari22a.html).\n' +
      '* Ren et al. [2023] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, A. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao. Pangu-E: Towards trillion parameter language model with sparse heterogeneous computing. _CoRR_, abs/2303.10845, 2023. URL [https://doi.org/10.48550/arXiv.2303.10845](https://doi.org/10.48550/arXiv.2303.10845).\n' +
      '* Roller et al. [2021] S. Roller, S. Sukhbaatar, A. Szlam, and J. Weston. Hash layers for large sparse models. _CoRR_, abs/2106.04426, 2021. URL [https://arxiv.org/abs/2106.04426](https://arxiv.org/abs/2106.04426).\n' +
      '* Sakaguchi et al. [2019] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n' +
      '* Scao et al. [2022] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamachi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurencon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. _CoRR_, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL [https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100).\n' +
      '* Sukhbaatar et al. [2021]R. 센리히, B. 해도, A. 버치. 하위 단어 단위로 희귀 단어를 기계 번역합니다. "제54차 컴퓨터 언어학 협회 연례 회의"에서, ACL 2016, 2016년 8월 7일부터 12일까지, 독일 베를린, 제1권: 장문. 컴퓨터 언어학 협회, 2016. doi: 10.18653/V1/P16-1162. URL [https://doi.org/10.18653/v1/p16-1162](https://doi.org/10.18653/v1/p16-1162).\n' +
      '* Shazeer (2019) N. 셰이저 빠른 변환기 디코딩: 하나의 쓰기 헤드만 있으면 됩니다. _ CoRR_, abs/1911.02150, 2019. URL [http://arxiv.org/abs/1911.02150](http://arxiv.org/abs/1911.02150).\n' +
      '* Shazeer et al.(2017) N. A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. 엄청나게 큰 신경망들: 희박하게 게이팅된 전문가 혼합 계층. 제5차 국제 학습 표상 회의에서 ICLR 2017_. OpenReview.net, 2017. URL [https://openreview.net/forum?id=B1ckMDqlg](https://openreview.net/forum?id=B1ckMDqlg)\n' +
      '* Shen et al. (2023) S. 신락 허영 주남 두성 Longpre, J. Wei, H. W. Chung, B. Zoph, W. 페더스, 엑스 천태환 부영 우원 천애범 이병호 조현우 케우처 대럴과 주 플란뫼: 전문가 혼합이 드문드문 있는 명령어 세분화 언어 모델 크기 조정. _ CoRR_, abs/2305.14705, 2023. doi: 10.48550/ARXIV.2305.14705. URL [https://doi.org/10.48550/arXiv.2305.14705](https://doi.org/10.48550/arXiv.2305.14705).\n' +
      '* Shoeybi et al.(2019) M. 소이비 팻워리 푸리, P. 레그레슬리, J. 캐스퍼, B. 카탄자로 메가트론-lm: 모델 병렬성을 사용하여 수십억 개의 매개 변수 언어 모델을 학습합니다. _ arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* Suzgun et al. (2022) M. 수즈건 비늘, N. Scharli 게르만 Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challengeing big-bench tasks and whether chain-of-thought can solve them. _ arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '* Tillet et al.(2019) P. Tillet, H. T. Kung, and D. Cox. 트리톤: 타일형 신경망 계산을 위한 중간 언어 및 컴파일러입니다. "Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages_, MAPL 2019, page 10-19, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367196. doi: 10.1145/331550 8.3329973. URL [https://doi.org/10.1145/3315508.3329973](https://doi.org/10.1145/3315508.3329973).\n' +
      '* Together-AI (2023) Together-AI. Redpajama-data: 2023년 4월 llama 학습 데이터 세트를 재현하는 오픈 소스 레시피. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)\n' +
      '* Touvron et al.(2023) H. Touvron, T. 라브릴, G. 이자카드, X. 마틴 라쇼 라크루아 B. 로지에르 고얄, E. 함브로, F. 아즈하르, A. 로드리게스, A. 줄린, E. 그레이브 및 G. 램플. 라마: 개방적이고 효율적인 기초 언어 모델입니다. _ CoRR_, abs/2302.13971, 2023a. doi: 10.48550/arXiv.2302.13971. URL [https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971).\n' +
      '* Touvron et al.(2017) H. Touvron, L. 마틴 스톤, P. 알버트, A. 알마하이리, Y. 바배이 바슐리코프 P. Bhargava, S. Bhosale D. Bikel C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. 고스와미 A. Hartshorn, S. 호세이니 허현인 카다스 케르케즈 Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. 라쇼 라브릴 J. Lee D. Liskovich, Y. 류영 마오성 마틴 미하일로프, P. 미슈라, I. 몰리복, Y. Nie, A. Poulton, J. Reizenstein, R. 룽타 살라디, A. 셸텐 실바 E. M. 스미스, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. 얀인자로프 장아판 캄바두르 나랑 로드리게스 스토닉 에듀노프, T. 시알롬 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. _ CoRR_, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).\n' +
      '\n' +
      '아바스와니 노셰이저 파르마, J. Uszkoreit, L. 존스 A. N. 고메즈 Kaiser와 I. Polosukhin (2017)의 관심만 있으면 됩니다. In Advances in Neural Information Processing Systems 30, pp. 5998-6008. External Links: Link, Document Cited by: SS1.\n' +
      '* B. Wang and A. Komatsuzaki (2021)GPT-J-6B: a six Billion Parameter Autoregressive Language Model. 참고: SS1로 인용 된 [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).\n' +
      '* L. 서현호 장락 이창조 이영 서경훈 선동유 천규 동원 류병식 최재리 왕우 시영 이영 패터슨, Z 천영 장현주 류종호 자오치 자오씨유 장중 양광 Richardson, Z. Lan (2020)CLUE: 중국어 이해 평가 벤치마크. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 4762-4772. External Links: Link, Document Cited by: SS1.\n' +
      '* F. Xue, Z. 정영 Fu, J. Ni, Z. 정우 주, Y You (2023)Openmoe: 개방형 전문가 혼합 언어 모델. 참고: [https://github.com/XueFuzhao/OpenMoE](https://github.com/XueFuzhao/OpenMoE) 외부 링크: 연결 기준: SS1입니다.\n' +
      '* R. 젤러스 A. 홀츠만 Bisk, A. Farhadi, Y. 최(2019)헬라 스웨그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-8 2, 2019, Volume 1: Long Papers, pp. 4791-4800. External Links: Link, Document Cited by: SS1.\n' +
      '* S. 장성 롤러 고얄 아르테텍세 천성호 천창대완 Diab, X 이석빈 미하일로프 오석 클라이퍼 Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, L. Zettlemoyer (2022)Opt: open pre-trained transformer language models. 참고: [https://github.com/XueFuzhao/OpenMoE](https://github.com/XueFuzhao/OpenMoE) 외부 링크: 링크, 문서 인용: SS1입니다.\n' +
      '* C. Zheng, M. Huang, and A. Sun (2019)Chid: a large-scale Chinese idiom dataset for cloze test. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-8 2, 2019, Volume 1: Long Papers, pp. 778-787. External Links: Link, Document Cited by: SS1.\n' +
      '* Y. 주태 유남래 두영 황병호 자오아엠다이 Chen, Q. V. Le, and J. Laudon (2022)Mixture-of-experts with expert choice routing. NeurIPS에서 외부 링크: 링크, SS1에 의해 인용된 문서.\n' +
      '* June 3, 2022, pp. 1044. External Links: Link, Document Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:31]\n' +
      '\n' +
      '## 부록 C Training Benchmark Curves DeepSeeMoE 16B\n' +
      '\n' +
      '우리는 참고하기 위해 그림 7에서 DeepSeeMoE 16B와 DeepSeek 7B(Dense)의 훈련 중 벤치마크 곡선을 제시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **GShard\\(\\times 1.2\\)** & **GShard\\(\\times 1.5\\)** & **DeepSeekMoE** \\\\ \\hline Relative Expert Size & N/A & 1.2 & 1.5 & 0.25 \\\\ \\# Experts & N/A & 0 + 16 & 0 + 16 & 1 + 63 \\\\ \\# Activated Experts & N/A & 15.9B & 19.8B & 13.3B \\\\ \\# Activated Expert Params & N/A & 2.37B & 2.82B & 2.05B \\\\ \\# Training Tokens & N/A & 100B & 100B & 100B \\\\ \\hline HellaSwag (Acc.) & 0-shot & 66.6 & 67.7 & **69.1** \\\\ PIQA (Acc.) & 0-shot & 75.6 & **76.0** & **75.7** \\\\ ARC-easy (Acc.) & 0-shot & 56.8 & 56.8 & **58.8** \\\\ ARC-challenge (Acc.) & 0-shot & **39.9** & 37.6 & 38.5 \\\\ \\hline RACE-middle (Acc.) & 5-shot & 51.6 & 50.6 & **52.4** \\\\ RACE-high (Acc.) & 5-shot & 37.4 & 36.3 & **38.5** \\\\ \\hline HumanEval (Pass@1) & 0-shot & 6.1 & 6.1 & **9.8** \\\\ MBPP (Pass@1) & 3-shot & 7.0 & **11.6** & 10.6 \\\\ \\hline TriviaQA (EM) & 5-shot & 36.5 & 36.7 & **38.2** \\\\ NaturalQuestions (EM) & 5-shot & 12.6 & 12.1 & **13.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 더 큰 스케일에서 DeepSeekMoE와 더 큰 GShard 모델 간의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c} \\hline \\hline\n' +
      '**Metric** & **\\# Shot** & **Dense\\(\\times 4\\)** & **Dense\\(\\times 16\\)** & **DeepSeekMoE** \\\\ \\hline Relative Expert Size & N/A & 1 & 1 & 0.25 \\\\ \\# Experts & N/A & 4 + 0 & 16 + 0 & 1 + 63 \\\\ \\# Activated Experts & N/A & 4 + 0 & 16 + 0 & 1 + 7 \\\\ \\# Total Expert Params & N/A & 0.47B & 1.89B & 1.89B \\\\ \\# Activated Expert Params & N/A & 0.47B & 1.89B & 0.24B \\\\ \\# Training Tokens & N/A & 100B & 100B & 100B \\\\ \\hline Pile (Loss) & N/A & 1.908 & **1.806** & **1.808** \\\\ \\hline HellaSwag (Acc.) & 0-shot & 47.6 & **55.1** & **54.8** \\\\ PIQA (Acc.) & 0-shot & 70.0 & 71.9 & **72.3** \\\\ ARC-easy (Acc.) & 0-shot & 43.9 & **51.9** & 49.4 \\\\ ARC-challenge (Acc.) & 0-shot & 30.5 & 33.8 & **34.3** \\\\ \\hline RACE-middle (Acc.) & 5-shot & 42.4 & **46.3** & 44.0 \\\\ RACE-high (Acc.) & 5-shot & 30.7 & **33.0** & 31.7 \\\\ \\hline HumanEval (Pass@1) & 0-shot & 1.8 & 4.3 & **4.9** \\\\ MBPP (Pass@1) & 3-shot & 0.2 & **2.2** & **2.2** \\\\ \\hline TriviaQA (EM) & 5-shot & 9.9 & **16.5** & **16.6** \\\\ NaturalQuestions (EM) & 5-shot & 3.0 & **6.3** & 5.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: DeepSeekMoE와 더 큰 조밀한 기준선 사이의 비교.\n' +
      '\n' +
      '도 7 \\(|\\) DeepSeekMoE 16B 및 DeepSeek 7B (Dense)의 트레이닝 동안의 벤치마크 곡선.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
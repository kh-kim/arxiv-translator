{
    "2402.18815v1": {
        "paper_id": "2402.18815v1",
        "abs_url": "https://arxiv.org/abs/2402.18815v1",
        "pdf_url": "https://arxiv.org/pdf/2402.18815v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2402.18815v1_How_do_Large_Language_Models_Handle_Multilingualism?.pdf",
        "title": "How do Large Language Models Handle Multilingualism?",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Yiran Zhao",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Kenji Kawaguchi",
            "Lidong Bing"
        ],
        "abstract": "Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/how-do-large-language-models-handle",
        "bibtex": "@misc{zhao2024large,\n      title={How do Large Language Models Handle Multilingualism?}, \n      author={Yiran Zhao and Wenxuan Zhang and Guizhen Chen and Kenji Kawaguchi and Lidong Bing},\n      year={2024},\n      eprint={2402.18815},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
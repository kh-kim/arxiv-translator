<html lang="en"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</title>
<!--Generated on Thu Jul 13 18:39:44 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="css/ar5iv.min.css" rel="stylesheet" type="text/css">
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" rel="stylesheet">
<script crossorigin="anonymous" integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz" src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script defer="" src="https://services.dev.arxiv.org/html/addons.js"></script>
<script defer="" src="https://services.dev.arxiv.org/html/feedbackOverlay.js"></script>
<link href="https://services.dev.arxiv.org/html/styles.css" rel="stylesheet" type="text/css"></head>
<body>
<div class="ltx_page_main" id="main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\newtcolorbox</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">boxEnv
colback=mythmback,coltitle=blue,colframe=mythmback,
center,
width=boxrule=0.5pt,
left=5pt,right=0pt,
top=2pt,bottom=2pt,
before skip=10pt, after skip=10pt





</p>
</div>
<h1 class="ltx_title ltx_title_document">
<em class="ltx_emph ltx_font_italic" id="id13.id1">WizardCoder</em>: Empowering Code Large Language Models with Evol-Instruct</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">  Ziyang Luo<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mn id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><cn id="id1.1.m1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Can Xu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mn id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><cn id="id2.2.m2.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>  Pu Zhao<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mn id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><cn id="id3.3.m3.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Qingfeng Sun<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mn id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><cn id="id4.4.m4.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Xiubo Geng<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mn id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><cn id="id5.5.m5.1.1.1.cmml" type="integer" xref="id5.5.m5.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id6.6.1">Wenxiang Hu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id6.6.1.m1.1"><semantics id="id6.6.1.m1.1a"><msup id="id6.6.1.m1.1.1" xref="id6.6.1.m1.1.1.cmml"><mi id="id6.6.1.m1.1.1a" xref="id6.6.1.m1.1.1.cmml"></mi><mn id="id6.6.1.m1.1.1.1" mathvariant="normal" xref="id6.6.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id6.6.1.m1.1b"><apply id="id6.6.1.m1.1.1.cmml" xref="id6.6.1.m1.1.1"><cn id="id6.6.1.m1.1.1.1.cmml" type="integer" xref="id6.6.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id6.6.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>  <span class="ltx_text ltx_font_bold" id="id7.7.2">Chongyang Tao<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id7.7.2.m1.1"><semantics id="id7.7.2.m1.1a"><msup id="id7.7.2.m1.1.1" xref="id7.7.2.m1.1.1.cmml"><mi id="id7.7.2.m1.1.1a" xref="id7.7.2.m1.1.1.cmml"></mi><mn id="id7.7.2.m1.1.1.1" mathvariant="normal" xref="id7.7.2.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.2.m1.1b"><apply id="id7.7.2.m1.1.1.cmml" xref="id7.7.2.m1.1.1"><cn id="id7.7.2.m1.1.1.1.cmml" type="integer" xref="id7.7.2.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.2.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id7.7.2.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>  <span class="ltx_text ltx_font_bold" id="id8.8.3">Jing Ma<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id8.8.3.m1.1"><semantics id="id8.8.3.m1.1a"><msup id="id8.8.3.m1.1.1" xref="id8.8.3.m1.1.1.cmml"><mi id="id8.8.3.m1.1.1a" xref="id8.8.3.m1.1.1.cmml"></mi><mn id="id8.8.3.m1.1.1.1" mathvariant="normal" xref="id8.8.3.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id8.8.3.m1.1b"><apply id="id8.8.3.m1.1.1.cmml" xref="id8.8.3.m1.1.1"><cn id="id8.8.3.m1.1.1.1.cmml" type="integer" xref="id8.8.3.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.3.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id8.8.3.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>  <span class="ltx_text ltx_font_bold" id="id9.9.4">Qingwei Lin<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id9.9.4.m1.1"><semantics id="id9.9.4.m1.1a"><msup id="id9.9.4.m1.1.1" xref="id9.9.4.m1.1.1.cmml"><mi id="id9.9.4.m1.1.1a" xref="id9.9.4.m1.1.1.cmml"></mi><mn id="id9.9.4.m1.1.1.1" mathvariant="normal" xref="id9.9.4.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.4.m1.1b"><apply id="id9.9.4.m1.1.1.cmml" xref="id9.9.4.m1.1.1"><cn id="id9.9.4.m1.1.1.1.cmml" type="integer" xref="id9.9.4.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.4.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id9.9.4.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>  <span class="ltx_text ltx_font_bold" id="id11.11.6">Daxin Jiang<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id10.10.5.m1.1"><semantics id="id10.10.5.m1.1a"><msup id="id10.10.5.m1.1.1" xref="id10.10.5.m1.1.1.cmml"><mi id="id10.10.5.m1.1.1a" xref="id10.10.5.m1.1.1.cmml"></mi><mn id="id10.10.5.m1.1.1.1" mathvariant="normal" xref="id10.10.5.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.5.m1.1b"><apply id="id10.10.5.m1.1.1.cmml" xref="id10.10.5.m1.1.1"><cn id="id10.10.5.m1.1.1.1.cmml" type="integer" xref="id10.10.5.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.5.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id10.10.5.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id11.11.6.m2.1"><semantics id="id11.11.6.m2.1a"><msup id="id11.11.6.m2.1.1" xref="id11.11.6.m2.1.1.cmml"><mi id="id11.11.6.m2.1.1a" xref="id11.11.6.m2.1.1.cmml"></mi><mn id="id11.11.6.m2.1.1.1" mathvariant="normal" xref="id11.11.6.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id11.11.6.m2.1b"><apply id="id11.11.6.m2.1.1.cmml" xref="id11.11.6.m2.1.1"><cn id="id11.11.6.m2.1.1.1.cmml" type="integer" xref="id11.11.6.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.6.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id11.11.6.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>Microsoft
<br class="ltx_break"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="id12.12.m6.1"><semantics id="id12.12.m6.1a"><msup id="id12.12.m6.1.1" xref="id12.12.m6.1.1.cmml"><mi id="id12.12.m6.1.1a" xref="id12.12.m6.1.1.cmml"></mi><mn id="id12.12.m6.1.1.1" xref="id12.12.m6.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id12.12.m6.1b"><apply id="id12.12.m6.1.1.cmml" xref="id12.12.m6.1.1"><cn id="id12.12.m6.1.1.1.cmml" type="integer" xref="id12.12.m6.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m6.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id12.12.m6.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Hong Kong Baptist University 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id14.13.id1">{caxu,puzhao,qins,xigeng,wenxh,chongyang.tao,qlin,djiang}@microsoft.com
<br class="ltx_break">{cszyluo, majing}@comp.hkbu.edu.hk</span>
</span><span class="ltx_author_notes"> Equal contribution. Work done during the internship at Microsoft. Corresponding author.
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id15.id1">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce <em class="ltx_emph ltx_font_italic" id="id15.id1.1">WizardCoder</em>, which empowers Code LLMs with complex instruction fine-tuning, by adapting the <em class="ltx_emph ltx_font_italic" id="id15.id1.2">Evol-Instruct</em> method to the domain of code.
Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic’s Claude and Google’s Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nlpxucan/WizardLM" title="">https://github.com/nlpxucan/WizardLM</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recently, Large Language Models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">GPT3</span> </a>; <a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">GPT4</span> </a>; <a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">PaLM</span> </a>; <a class="ltx_ref" href="#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">palm2</span> </a>; <a class="ltx_ref" href="#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">Chinchilla</span> </a>; <a class="ltx_ref" href="#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">gopher</span> </a>; <a class="ltx_ref" href="#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">GLM-130B</span> </a>; <a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">llama</span> </a>; <a class="ltx_ref" href="#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">opt</span> </a></cite> have garnered significant attention and demonstrated impressive success. Notably, OpenAI’s ChatGPT stands out as a prominent example. Leveraging extensive pre-training on vast amounts of internet data and further fine-tuning with detailed instruction data&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">DBLP:conf/nips/Ouyang0JAWMZASR22</span> </a></cite>, these models have achieved state-of-the-art (SOTA) zero-shot performance across diverse tasks. This trend is also observed in the domain of code understanding and generation. Numerous Code LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a>; <a class="ltx_ref" href="#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">AlphaCode</span> </a>; <a class="ltx_ref" href="#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">codegen</span> </a>; <a class="ltx_ref" href="#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">CodeGeeX</span> </a>; <a class="ltx_ref" href="#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">incoder</span> </a>; <a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">codex</span> </a>; <a class="ltx_ref" href="#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">codet5</span> </a>; <a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">CodeT5+</span> </a></cite> have been proposed to tackle the challenges associated with code-related tasks. These Code LLMs undergo pre-training using substantial amounts of code data, enabling them to excel in various code-related tasks, showcasing impressive performance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In contrast to most previous Code LLMs that primarily emphasize the pre-training process, there has been limited exploration of fine-grained instruction tuning in the Code domain. The introduction of instruction tuning initially aimed to enhance the generalization capabilities of LMs across different tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">t5</span> </a>; <a class="ltx_ref" href="#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">DBLP:conf/iclr/WeiBZGYLDDL22</span> </a>; <a class="ltx_ref" href="#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">flan-t5</span> </a>; <a class="ltx_ref" href="#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">ExT5</span> </a>; <a class="ltx_ref" href="#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">T0</span> </a>; <a class="ltx_ref" href="#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ZeroPrompt</span> </a>; <a class="ltx_ref" href="#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">UnifiedQA</span> </a></cite>. OpenAI’s InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">DBLP:conf/nips/Ouyang0JAWMZASR22</span> </a></cite>, for instance, involved soliciting human annotators to provide explicit instructions to ensure alignment with users’ intentions. Similarly, recent works such as Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">alpaca</span> </a></cite> employed the self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">wang2022self</span> </a></cite> method, where ChatGPT generated the instruction data. Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">vicuna2023</span> </a></cite> utilized user-shared conversations collected from ShareGPT.com. WizardLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">xu2023wizardlm</span> </a></cite> introduced the <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">Evol-Instruct</em> method, which involved evolving existing instruction data to generate more complex and diverse datasets. However, it is worth noting that all these approaches primarily focused on the general domain and lacked specific design considerations for the code domain.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Motivated by the <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">Evol-Instruct</em> method, this study aims to enhance the capabilities of the SOTA open-source Code LLM, StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite>, by generating intricate code instruction data through code-specific <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">Evol-Instruct</em>. To achieve this, we have made several adaptations to the evolutionary prompt process tailored specifically for code-related tasks. These modifications include refining the evolutionary instructions, simplifying the form of evolutionary prompts, and incorporating code debugging and time-space complexity constraints. Initially, our method is applied to evolve the basic code instruction data, Code Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">codealpaca</span> </a></cite>. Subsequently, we conduct fine-tuning of StarCoder using our newly created code instruction-following training set and obtain our <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">WizardCoder</em>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The experimental results obtained from four code generation benchmarks, namely HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite>, HumanEval+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">humanevalp</span> </a></cite>, MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">MBPP</span> </a></cite>, and DS-100&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">DS1000</span> </a></cite>, demonstrate that our <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">WizardCoder</em> outperforms all other open-source Code LLMs, achieving state-of-the-art (SOTA) performance. Specifically, we observe a substantial improvement in pass@1 scores, with an increase of +22.3 (57.3 vs. 35.0) in HumanEval and +8.2 (51.8 vs. 43.6) in MBPP. Remarkably, despite its much smaller size, our <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">WizardCoder</em> even surpasses Anthropic’s Claude and Google’s Bard in terms of pass rates on HumanEval and HumanEval+.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The contributions of this work can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">WizardCoder</em>, which enhances the performance of the open-source Code LLM, StarCoder, through the application of Code <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.2">Evol-Instruct</em>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.1">WizardCoder</em> surpasses all other open-source Code LLMs by a substantial margin in terms of code generation, including StarCoder, CodeGen, CodeGee, CodeT5+, InstructCodeT5+, StarCoder-GPTeacher, and Instruct-Codegen-16B.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.1">WizardCoder</em> achieves superior results in code generation compared to the largest closed-source LLMs, such as Claude, Bard, PaLM, PaLM-2, and LaMDA, despite being considerably smaller in size.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Large Language Models.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Recently, LLMs have demonstrated remarkable achievements across a broad spectrum of tasks. Prominent tech companies have made significant strides in developing highly proficient LLMs. These include OpenAI’s GPT3&amp;4&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">GPT3</span> </a>; <a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">GPT4</span> </a></cite>, Google’s PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">PaLM</span> </a>; <a class="ltx_ref" href="#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">palm2</span> </a></cite>, and Bard<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bard.google.com/" title="">https://bard.google.com/</a></span></span></span>, DeepMind’s Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">Chinchilla</span> </a></cite>, and Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">gopher</span> </a></cite>, as well as Anthropic’s Claude<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/index/introducing-claude" title="">https://www.anthropic.com/index/introducing-claude</a></span></span></span>. However, it is important to note that these models are closed-source and can only be accessed through specific APIs or may not be accessible at all.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">The AI community has witnessed the release of several open-source LLMs, where the model weights are made publicly available. EleutherAI has contributed GPT-NeoX-20B&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">GPT-NeoX-20B</span> </a></cite> and GPT-J-6B&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">gpt-j</span> </a></cite>. Google has released UL2-20B&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">UL2</span> </a></cite>. Tsinghua University has introduced GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">GLM-130B</span> </a></cite>. Meta has released OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">opt</span> </a></cite> and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">llama</span> </a></cite>. It is worth noting that while these open-source models have made valuable contributions, they generally do not exhibit the same level of performance as their closed-source counterparts.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Large Language Models for Code.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Recent studies have introduced a significant number of LLMs for code-related tasks to address the challenges of code understanding and generation. OpenAI has unveiled Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">codex</span> </a></cite> and Code-Davinci&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">Azure</span> </a></cite>. Google has proposed PaLM-Coder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">PaLM</span> </a></cite>. They perform outstandingly on the popular code completion benchmarks, like HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite> and MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">MBPP</span> </a></cite>. However, these models are closed-source.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">On the other hand, there are several open-source Code LLMs available. Salesforce has introduced CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">codegen</span> </a></cite>, CodeT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">codet5</span> </a></cite>, and CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">CodeT5+</span> </a></cite>. Tsinghua University has contributed CodeGeeX&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">CodeGeeX</span> </a></cite>, and the BigCode Project has developed StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite>. These models have demonstrated notable advancements in code-related tasks. However, when compared to the SOTA closed-source models, they still lag behind significantly. In contrast to the aforementioned models without instruction fine-tuning, our work demonstrates that further training Code LLMs with Code <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p2.1.1">Evol-Instruct</em> can substantially enhance performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Instruction Fine-Tuning.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">The primary objective of instruction fine-tuning in its early stages was to enhance the cross-task generalization capabilities of LMs. This was achieved by fine-tuning LMs with a substantial corpus of public NLP tasks. T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">t5</span> </a></cite> was among the first models to explore this approach, training on a multitude of supervised text-to-text tasks. Subsequent works such as FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">DBLP:conf/iclr/WeiBZGYLDDL22</span> </a></cite>, ExT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">ExT5</span> </a></cite>, T0&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">T0</span> </a></cite>, and UnifiedQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">UnifiedQA</span> </a></cite> further expanded the range of tasks to bolster the overall generalization ability of LMs. Notably, ZeroPrompt&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ZeroPrompt</span> </a></cite> and FLAN-T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">flan-t5</span> </a></cite> pushed the envelope by incorporating thousands of tasks in their training pipelines. Across these studies, a consistent finding emerges: fine-tuning LMs with diverse NLP task instructions yields significant performance improvements when applied to new tasks.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">While fine-tuning LMs with diverse NLP tasks has shown promising results, it often falls short in aligning with the intentions of real-world users. OpenAI has pursued a different approach by soliciting human annotators to provide a large corpus of human instructions, encompassing diverse forms and a wide range of task types. Building upon this dataset, OpenAI trained its GPT3&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">GPT3</span> </a></cite> model to create InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">DBLP:conf/nips/Ouyang0JAWMZASR22</span> </a></cite>, which better aligns with users’ inputs. This line of development has even led to the impressive work known as ChatGPT. However, it is important to note that the dataset and model weights associated with these advancements are not publicly available. Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">alpaca</span> </a></cite> takes a different route by adopting the self-instruct method&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">wang2022self</span> </a></cite>, leveraging ChatGPT to generate data for training. Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">vicuna2023</span> </a></cite> utilizes user-shared conversations collected from ShareGPT.com to train its models. WizardLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">xu2023wizardlm</span> </a></cite> introduces the <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px3.p2.1.1">Evol-Instruct</em> method, which involves evolving existing instruction data to generate more complex and diverse datasets. In contrast to these general instruction fine-tuning approaches, our <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px3.p2.1.2">WizardCoder</em> successfully applies the <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px3.p2.1.3">Evol-Instruct</em> method specifically in the domain of Code LLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we elaborate on the methodological details of <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">WizardCoder</em>. Following WizardLM, we apply the <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">Evol-Instruct</em> method to evolve Code Alpaca generated using self-instruct and fine-tune the pre-trained Code LLM StarCoder with the evolved data.
</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evol-Instruct Prompts for Code</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.5">Inspired by the Evol-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">xu2023wizardlm</span> </a></cite> method proposed by WizardLM, this work also attempts to make code instructions more complex to enhance the fine-tuning effectiveness of code pre-trained large models. To adapt Evol-Instruct to the realm of code, we made the following modifications to the evolutionary prompt:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Streamlined the evolutionary instructions by removing deepening, complicating input, and In-Breadth Evolving.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Simplified the form of evolutionary prompts by unifying the evolutionary prompt template.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Addressing the specific characteristics of the code domain, we added two evolutionary instructions: code debugging and code time-space complexity constraints.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS1.p1.4">The unified code evolutionary prompt template is as follows:
<span class="ltx_ERROR undefined" id="S3.SS1.p1.4.1">{boxEnv}</span>
<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.4.2">Please increase the difficulty of the given programming test question a bit. 
<br class="ltx_break">
<br class="ltx_break">You can increase the difficulty using, but not limited to, the following methods:
<br class="ltx_break">{method}
<br class="ltx_break">
<br class="ltx_break">{question}</span>

Here, <math alttext="\{" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mo id="S3.SS1.p1.1.m1.1.1" stretchy="false" xref="S3.SS1.p1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">{</annotation></semantics></math>question<math alttext="\}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mo id="S3.SS1.p1.2.m2.1.1" stretchy="false" xref="S3.SS1.p1.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">}</annotation></semantics></math> represents the current code instruction awaiting evolution, and <math alttext="\{" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mo id="S3.SS1.p1.3.m3.1.1" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">{</annotation></semantics></math>method<math alttext="\}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mo id="S3.SS1.p1.4.m4.1.1" stretchy="false" xref="S3.SS1.p1.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">}</annotation></semantics></math> is the type of evolution. The five types we used are listed as follows:
<span class="ltx_ERROR undefined" id="S3.SS1.p1.4.3">{boxEnv}</span>
<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.4.4">Add new constraints and requirements to the original problem, adding approximately 10 additional words.
<br class="ltx_break">
<br class="ltx_break">Replace a commonly used requirement in the programming task with a less common and more specific one.
<br class="ltx_break">
<br class="ltx_break">If the original problem can be solved with only a few logical steps, please add more reasoning steps.
<br class="ltx_break">
<br class="ltx_break">Provide a piece of erroneous code as a reference to increase misdirection.
<br class="ltx_break">
<br class="ltx_break">Propose higher time or space complexity requirements, but please refrain from doing so frequently.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training <em class="ltx_emph ltx_font_italic" id="S3.SS2.1.1">WizardCoder</em>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We employ the following procedure to train <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">WizardCoder</em>. Initially, we utilize StarCoder 15B&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite> as the foundation and proceed to fine-tune it using the code instruction-following training set, which was evolved through <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.2">Evol-Instruct</em>. The prompt format for fine-tuning is outlined as follows:
<span class="ltx_ERROR undefined" id="S3.SS2.p1.1.3">{boxEnv}</span>
<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.4">Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. 
<br class="ltx_break">
<br class="ltx_break">### Instruction:
<br class="ltx_break">{instruction}
<br class="ltx_break">
<br class="ltx_break">### Response:</span>

To construct the training dataset, we initialized it with the 20K instruction-following dataset called Code Alpaca<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sahil280114/codealpaca" title="">https://github.com/sahil280114/codealpaca</a></span></span></span>. We iteratively employ the Evol-Instruct technique on this dataset consisting of 20,000 samples to produce evolved data. After each round of data evolution, we merge the evolved data from all previous rounds with the original dataset to finetune StarCoder and assess the pass@1 metric on HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite>. Once we observe a decline in the pass@1 metric, we will discontinue the usage of Evol-Instruct and choose the model with the highest pass@1 as the ultimate model.

</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S3.F1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.4.2" style="font-size:90%;">The percentage of pass rates on the HumanEval (164 problems) with a single attempt. All baseline scores are retrieved from the LLM-Humaneval-Benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">llm-humaneval-benchmarks</span> </a></cite>. Our <em class="ltx_emph ltx_font_italic" id="S3.F1.4.2.1">WizardCoder</em> generates an answer with greedy decoding.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section begins by providing a comprehensive overview of the baseline models in our experiments. Subsequently, we present the performance of our models on four code generation benchmarks: HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite>, HumanEval+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">humanevalp</span> </a></cite>, MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">MBPP</span> </a></cite>, and DS-1000&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">DS1000</span> </a></cite>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baselines</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Closed-Source Models.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">Multiple technology companies have successfully developed highly proficient LLMs while choosing not to publicly release them. These models are referred to as closed-source models. For our research, we incorporate a substantial number of these models as our baselines. Specifically, our baselines encompass the following: (i) OpenAI’s GPT3.5&amp;4&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">GPT4</span> </a></cite>, Code-Davinci-002&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">Azure</span> </a></cite>, Code-Cushman-001&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">Azure</span> </a></cite>, and Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">codex</span> </a></cite>; (ii) Google’s Bard, PaLM 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">palm2</span> </a></cite>, PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">PaLM</span> </a></cite>, and LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">LaMDA</span> </a></cite>; (iii) Google DeepMind’s AlphaCode&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">AlphaCode</span> </a></cite>; and (iv) Anthropic’s Claude.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Open-Source Models.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">Several open-source LLMs have been made available to the AI community, although their performance generally lags behind the closed-source models a lot. As part of our research, we incorporate a significant number of these open-source models as our baselines. Our baselines encompass the following models: StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite>, LLaMa&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">llama</span> </a></cite>, CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">codegen</span> </a></cite>, CodeGeeX&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">CodeGeeX</span> </a></cite>, CodeT5+<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">CodeT5+</span> </a></cite>, and InCoder<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">incoder</span> </a></cite>. In addition, we also include several models with instructions fine-tuning, including StarCoder-GPTeacher,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct" title="">https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct</a></span></span></span> Instruct-Codegen-16B,<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sahil2801/instruct-codegen-16B" title="">https://huggingface.co/sahil2801/instruct-codegen-16B</a></span></span></span> Guanaco-65B,<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/TheBloke/guanaco-65B-HF" title="">https://huggingface.co/TheBloke/guanaco-65B-HF</a></span></span></span> and Falcon-40B-Instruct.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tiiuae/falcon-40b-instruct" title="">https://huggingface.co/tiiuae/falcon-40b-instruct</a></span></span></span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite> serves as our basic foundation model. The evolved dataset consists of approximately 78k samples. To fine-tune the basic models, we employ specific configurations, including a batch size of 512, a sequence length of 2048, 200 fine-tuning steps, 30 warmup steps, a learning rate of 2e-5, a Cosine learning rate scheduler, and fp16 mixed precision.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation on HumanEval, HumanEval+, and MBPP</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite>, HumanEval+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">humanevalp</span> </a></cite> and MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">MBPP</span> </a></cite> are extensively utilized benchmarks within the field of Code LLMs. These benchmarks encompass a vast collection of Python programming problems, employing test cases to validate the code generated by Code LLMs. HumanEval consists of 164 original programming problems, with an average of 9.6 test cases allocated to each problem. To ensure a thorough assessment of the functional correctness of LLM-synthesized code, HumanEval+ extends the number of test cases significantly, averaging at 774.8 test cases per problem. On the other hand, MBPP offers a set of 500 test programming problems, accompanied by three automated test cases per problem. The prompt format for these tasks is as follows:
<span class="ltx_ERROR undefined" id="S4.SS3.p1.1.1">{boxEnv}</span>
<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.2">Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. 
<br class="ltx_break">
<br class="ltx_break">### Instruction:
<br class="ltx_break">Create a Python script for this problem:
<br class="ltx_break">{Question}
<br class="ltx_break">
<br class="ltx_break">### Response:</span></p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Comparing with the Closed-Source Models.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">The SOTA LLMs for code generation, such as GPT4, Claude, and Bard, are predominantly closed-source. Acquiring access to the APIs of these models proves challenging. In this study, we adopt an alternative approach by retrieving the scores for HumanEval and HumanEval+ from the LLM-Humaneval-Benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">llm-humaneval-benchmarks</span> </a></cite>. Notably, all the mentioned models generate code solutions for each problem utilizing a single attempt, and the resulting pass rate percentage is reported. To maintain consistency, we employ the same experimental setup by generating answers using greedy decoding and evaluate our <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px1.p1.1.1">WizardCoder</em> using the provided evaluation codes. By adhering to these standardized procedures, we aim to ensure fair and comparable evaluations of our model against existing benchmarks.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p2.1">As depicted in Figure&nbsp;<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.2 Training WizardCoder ‣ 3 Approach ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a>, our <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px1.p2.1.1">WizardCoder</em> attains the third position in this benchmark, surpassing Claude-Plus (59.8 vs. 53.0) and Bard (59.8 vs. 44.5). Notably, our model exhibits a substantially smaller size compared to these models. Furthermore, our <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px1.p2.1.2">WizardCoder</em> demonstrates a remarkable superiority over other open-source LLMs that undergo instruction fine-tuning, showcasing a significant performance margin.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Results of pass@1(%) on HumanEval and MBPP. Most scores are retrieved from the papers of StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite> and CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">CodeT5+</span> </a></cite>. We follow the previous works&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite> to generate n samples to estimate the pass@1 score with the same set of hyper-parameters: temperate=0.2, and top_p=0.95. *: we evaluate this model by ourselves.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tbody><tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.1">Params</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.3.1">HumanEval</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.4.1">MBPP</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T1.1.3.1">Closed-source models</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.1">LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">LaMDA</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.2">137B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.3">14.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.4.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.1">AlphaCode&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">AlphaCode</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.2">1.1B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3">17.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6">
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.1">PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">PaLM</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.2">540B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.3">26.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4">36.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.1">PaLM-Coder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">PaLM</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.2">540B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.3">36.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.4">47.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8">
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.1">PaLM 2-S&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">palm2</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.3">37.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.4">50.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.1">Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">codex</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.2">2.5B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.3">21.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10">
<td class="ltx_td ltx_align_left" id="S4.T1.1.10.1">Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">codex</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.2">12B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.3">28.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11">
<td class="ltx_td ltx_align_left" id="S4.T1.1.11.1">Code-Cushman-001&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">Azure</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.3">33.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.4">45.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12">
<td class="ltx_td ltx_align_left" id="S4.T1.1.12.1">Code-Davinci-002&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">Azure</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.3">47.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.4">58.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13">
<td class="ltx_td ltx_align_left" id="S4.T1.1.13.1">GPT-3.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">GPT4</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.3">48.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14">
<td class="ltx_td ltx_align_left" id="S4.T1.1.14.1">GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">GPT4</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.14.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.14.3">67.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.14.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T1.1.15.1">Open-source models</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.16.1">LLaMa&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">llama</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.2">33B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.3">21.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.16.4">30.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.17">
<td class="ltx_td ltx_align_left" id="S4.T1.1.17.1">LLaMa&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">llama</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.2">65B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.3">23.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.4">37.7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.18">
<td class="ltx_td ltx_align_left" id="S4.T1.1.18.1">CodeGen-Multi&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">codegen</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.18.2">16B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.18.3">18.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.18.4">20.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.19">
<td class="ltx_td ltx_align_left" id="S4.T1.1.19.1">CodeGen-Mono&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">codegen</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.2">16B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.3">29.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.4">35.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.20">
<td class="ltx_td ltx_align_left" id="S4.T1.1.20.1">CodeGeeX&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">CodeGeeX</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.20.2">13B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.20.3">22.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.20.4">24.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.2">StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">li2023starcoder</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3">15B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4">33.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.1">43.6<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><msup id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.m1.1.1a" xref="S4.T1.1.1.1.m1.1.1.cmml"></mi><mo id="S4.T1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><times id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.21">
<td class="ltx_td ltx_align_left" id="S4.T1.1.21.1">CodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">CodeT5+</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.21.2">16B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.21.3">30.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.21.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.22">
<td class="ltx_td ltx_align_left" id="S4.T1.1.22.1">InstructCodeT5+&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">CodeT5+</span> </a></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.22.2">16B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.22.3">35.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.22.4">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.23">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.23.1"><em class="ltx_emph ltx_font_italic" id="S4.T1.1.23.1.1">WizardCoder</em></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.23.2">15B</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.23.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.23.3.1">57.3</span> (<span class="ltx_text" id="S4.T1.1.23.3.2" style="color:#FF0000;">+22.3</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.1.23.4">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.23.4.1">51.8</span> (<span class="ltx_text" id="S4.T1.1.23.4.2" style="color:#FF0000;">+8.2</span>)</td>
</tr>
</tbody></table>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Comparing with the Open-Source Models.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ Comparing with the Closed-Source Models. ‣ 4.3 Evaluation on HumanEval, HumanEval+, and MBPP ‣ 4 Experiment ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a>, we conduct a comprehensive comparison of our <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px2.p1.1.1">WizardCoder</em> with other open-source models on the HumanEval and MBPP benchmarks. In contrast to the results presented in Figure&nbsp;<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.2 Training WizardCoder ‣ 3 Approach ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a>, we adhere to the approach outlined in previous studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">humeval</span> </a></cite> by generating n samples for each problem to estimate the pass@1 score. The findings presented in Table&nbsp;<a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ Comparing with the Closed-Source Models. ‣ 4.3 Evaluation on HumanEval, HumanEval+, and MBPP ‣ 4 Experiment ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a> clearly demonstrate that our <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS0.Px2.p1.1.2">WizardCoder</em> exhibits a substantial performance advantage over all the open-source models.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p2.1">From the experimental results in Figure&nbsp;<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.2 Training WizardCoder ‣ 3 Approach ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a> and Table&nbsp;<a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ Comparing with the Closed-Source Models. ‣ 4.3 Evaluation on HumanEval, HumanEval+, and MBPP ‣ 4 Experiment ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a>, we have the following conclusions:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.i1.p1.1.1">WizardCoder</em> outperforms the largest closed-source LLMs, including Claude, Bard, PaLM, PaLM-2, and LaMDA, despite being significantly smaller.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.i2.p1.1.1">WizardCoder</em> outperforms all the open-source Code LLMs by a large margin (+22.3 on HumanEval), including StarCoder, CodeGen, CodeGee, and CodeT5+.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.i3.p1.1.1">WizardCoder</em> significantly outperforms all the open-source Code LLMs with instructions fine-tuning, including InstructCodeT5+, StarCoder-GPTeacher, and Instruct-Codegen-16B.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation on DS-1000</h3>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Performance of <em class="ltx_emph ltx_font_italic" id="S4.T2.4.2.1">WizardCoder</em> and baseline models on DS-1000. All models are evaluated with the same set of hyper-parameters: temperature=0.2, top_p=0.5, max_length=1024. Scores are average pass@1 accuracy over 40 samples. Matplotlib (plt) task does not have the right context, so insertion and completion scores are identical.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.5">
<tbody><tr class="ltx_tr" id="S4.T2.5.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.1.1">Format</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.3.1">plt</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.4.1">np</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.5.1">pd</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.6.1">py</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.7.1">scp</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.8"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.8.1">sk</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.9"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.9.1">tf</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.10"><span class="ltx_text ltx_font_bold" id="S4.T2.5.1.10.1">All</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.2">
<td class="ltx_td ltx_border_t" id="S4.T2.5.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.2"># of problems:</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.3">155</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.4">220</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.5">291</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.6">68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.7">106</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.8">115</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.9">45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.10">1,000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.5.3.1">Completion</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.2">InCoder-6B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3">28.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.4">4.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.5">3.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.6">4.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.7">2.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.8">2.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.9">3.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.10">7.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.4">
<td class="ltx_td ltx_align_left" id="S4.T2.5.4.1">Completion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.2">CodeGen-mono</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.3">31.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4">10.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.5">3.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.6">7.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.7">9.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.8">10.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.9">15.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.10">11.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5">
<td class="ltx_td ltx_align_left" id="S4.T2.5.5.1">Completion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.2">Code-Cushman-001</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.3">40.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.4">21.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5">7.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.6">12.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.7">11.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.8">18.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.9">12.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.10">18.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.6">
<td class="ltx_td ltx_align_left" id="S4.T2.5.6.1">Completion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.2">StarCoder</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.3">51.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.4">29.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.5">11.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6">21.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.7">20.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.8"><span class="ltx_text ltx_font_bold" id="S4.T2.5.6.8.1">29.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.9">24.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.10">26.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.7">
<td class="ltx_td ltx_align_left" id="S4.T2.5.7.1">Completion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.2"><em class="ltx_emph ltx_font_italic" id="S4.T2.5.7.2.1">WizardCoder</em></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.3"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.3.1">55.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.4"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.4.1">33.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.5.1">16.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.6"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.6.1">26.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.7.1">24.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.8">24.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.9"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.9.1">26.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.10"><span class="ltx_text ltx_font_bold" id="S4.T2.5.7.10.1">29.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.5.8.1">Insertion</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.2">InCoder-6B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.3">28.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.4">4.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.5">2.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.6">4.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.7">2.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.8">3.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.9">7.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.8.10">7.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.9">
<td class="ltx_td ltx_align_left" id="S4.T2.5.9.1">Insertion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.2">StarCoder</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.3">51.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.4">30.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.5">10.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.6">21.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.7">20.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.8">27.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9">20.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.10">25.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.5.10.1">Insertion</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.2"><em class="ltx_emph ltx_font_italic" id="S4.T2.5.10.2.1">WizardCoder</em></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.3"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.3.1">55.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.4"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.4.1">35.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.5"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.5.1">20.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.6"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.6.1">30.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.7"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.7.1">28.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.8"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.8.1">32.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.9"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.9.1">37.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.10.10"><span class="ltx_text ltx_font_bold" id="S4.T2.5.10.10.1">32.8</span></td>
</tr>
</tbody></table>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The DS-1000 benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">DS1000</span> </a></cite> comprises 1,000 distinct data science workflows spanning seven libraries. It assesses the performance of code generations against test cases and supports two evaluation modes: completion and insertion. In our experiments, we only report insertion scores for models that support. The DS-1000 benchmark further classifies problems based on the libraries employed, including Matplotlib (plt), NumPy (np), Pandas (pd), SciPy (scp), Scikit-Learn (sk), PyTorch (py), and TensorFlow (tf). We follow the same prompt format as StarCoder. In Table&nbsp;<a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.4 Evaluation on DS-1000 ‣ 4 Experiment ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">2</span></a>, we present pass@1 (n=40) results for each library, along with an overall score. Based on these results, our conclusion is that <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.1">WizardCoder</em> demonstrates a significant superiority over all other models when tackling data science problems on the DS-1000 benchmark. This observation holds true across nearly all data science libraries.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S4.F2.g1" src="x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Ablation study on the number of data evolution rounds.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Figure&nbsp;<a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4.5 Ablation Study ‣ 4 Experiment ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">2</span></a> presents an ablation study investigating the impact of the number of data evolution rounds. The first round of evolved data contains 38k samples. The second round contains 58k. The third round contains 78k. The fourth round contains 98k. For consistency, all models undergo fine-tuning with 200 steps. The results reveal that the highest pass@1 score on humaneval is achieved after three rounds of data evolution. Based on this observation, we select the data that evolved during the third round as the ultimate dataset.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.9.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.10.2" style="font-size:90%;">Examples of interaction with our <em class="ltx_emph ltx_font_italic" id="S4.T3.10.2.1">WizardCoder</em>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.6">
<tbody><tr class="ltx_tr" id="S4.T3.6.7">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S4.T3.6.7.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.6.7.1.1">Instruction</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T3.6.7.2" style="width:284.5pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S4.T3.6.7.2.1">Response</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.2.2.3" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.2.2.3.1">Write a Python function to tell me what the date is today.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T3.2.2.2" style="width:284.5pt;">
<div class="ltx_block ltx_align_top" id="S4.T3.2.2.2.2">
<p class="ltx_p" id="S4.T3.2.2.2.2.2"><span class="ltx_inline-para ltx_minipage ltx_align_middle" id="S4.T3.2.2.2.2.2.2" style="width:290.0pt;">
<span class="ltx_para" id="S4.T3.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.T3.1.1.1.1.1.1.p1.1">Here’s a Python function that tells you what the date is today:</span>
</span>
<span class="ltx_para" id="S4.T3.2.2.2.2.2.2.p2">
<span class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing" id="S4.T3.2.2.2.2.2.2.p2.1" style="background-color:#F2F2EB;"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,aW1wb3J0IGRhdGV0aW1lCgpkZWYgZ2V0X2RhdGUoKToKICAgIG5vdyA9IGRhdGV0aW1lLmRhdGV0aW1lLm5vdygpCiAgICByZXR1cm4gbm93LnN0cmZ0aW1lKCIlQSwgJUIgJWQsICVZIikKCnByaW50KGdldF9kYXRlKCkp">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx1"><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx1.1" style="font-size:50%;color:#FF00FF;">import</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3" style="font-size:50%;">datetime</span>
</span>
<span class="ltx_listingline" id="lstnumberx2">
</span>
<span class="ltx_listingline" id="lstnumberx3"><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx3.1" style="font-size:50%;color:#FF00FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.3" style="font-size:50%;">get_date</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.4" style="font-size:50%;">():</span>
</span>
<span class="ltx_listingline" id="lstnumberx4"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.2" style="font-size:50%;">now</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.4" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.6" style="font-size:50%;">datetime</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.7" style="font-size:50%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.8" style="font-size:50%;">datetime</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.9" style="font-size:50%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.10" style="font-size:50%;">now</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.11" style="font-size:50%;">()</span>
</span>
<span class="ltx_listingline" id="lstnumberx5"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx5.2" style="font-size:50%;color:#FF00FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.4" style="font-size:50%;">now</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.5" style="font-size:50%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.6" style="font-size:50%;">strftime</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.7" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx5.8" style="font-size:50%;color:#9400D1;">"%A,<span class="ltx_text ltx_lst_space" id="lstnumberx5.8.1">&nbsp;</span>%B<span class="ltx_text ltx_lst_space" id="lstnumberx5.8.2">&nbsp;</span>%d,<span class="ltx_text ltx_lst_space" id="lstnumberx5.8.3">&nbsp;</span>%Y"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.9" style="font-size:50%;">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx6">
</span>
<span class="ltx_listingline" id="lstnumberx7"><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.1" style="font-size:50%;color:#FF00FF;">print</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.2" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.3" style="font-size:50%;">get_date</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.4" style="font-size:50%;">())</span>
</span>
</span>
</span></span></p>
<p class="ltx_p" id="S4.T3.2.2.2.2.3">This function uses the datetime module to get the current date and time. It then uses the strftime() method to format the date as a string in the format "Day, Month Date, Year". Finally, it returns the formatted string.</p>
<p class="ltx_p" id="S4.T3.2.2.2.2.4">You can call this function from your Python code to get the current date and time.
</p>
</div>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S4.T3.4.4.3" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.4.4.3.1">I do not know how to integrate a Python function numerically. Please write a code for me.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T3.4.4.2" style="width:284.5pt;">
<div class="ltx_block ltx_align_top" id="S4.T3.4.4.2.2">
<p class="ltx_p" id="S4.T3.4.4.2.2.2"><span class="ltx_inline-para ltx_minipage ltx_align_middle" id="S4.T3.4.4.2.2.2.2" style="width:290.0pt;">
<span class="ltx_para" id="S4.T3.3.3.1.1.1.1.p1">
<span class="ltx_p" id="S4.T3.3.3.1.1.1.1.p1.1">Sure, here’s an example code for integrating a Python function numerically using the trapezoidal rule:</span>
</span>
<span class="ltx_para" id="S4.T3.4.4.2.2.2.2.p2">
<span class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing" id="S4.T3.4.4.2.2.2.2.p2.1" style="background-color:#F2F2EB;"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,ZGVmIGludGVncmF0ZShmLCBhLCBiLCBuKToKICAgIGggPSAoYiAtIGEpIC8gbgogICAgcyA9IChmKGEpICsgZihiKSkgLyAyCiAgICBmb3IgaSBpbiByYW5nZSgxLCBuKToKICAgICAgICBzICs9IGYoYSArIGkgKiBoKQogICAgcmV0dXJuIGggKiBz">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx8"><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx8.1" style="font-size:50%;color:#FF00FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.3" style="font-size:50%;">integrate</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.4" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.5" style="font-size:50%;">f</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.6" style="font-size:50%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.7" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.8" style="font-size:50%;">a</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.9" style="font-size:50%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.10" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.11" style="font-size:50%;">b</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.12" style="font-size:50%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.13" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.14" style="font-size:50%;">n</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.15" style="font-size:50%;">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx9"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.2" style="font-size:50%;">h</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.6" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.7" style="font-size:50%;">b</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.8" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.9" style="font-size:50%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.10" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.11" style="font-size:50%;">a</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.12" style="font-size:50%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.13" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.14" style="font-size:50%;">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.15" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.16" style="font-size:50%;">n</span>
</span>
<span class="ltx_listingline" id="lstnumberx10"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.2" style="font-size:50%;">s</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.4" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.6" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.7" style="font-size:50%;">f</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.8" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.9" style="font-size:50%;">a</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.10" style="font-size:50%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.11" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.12" style="font-size:50%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.13" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.14" style="font-size:50%;">f</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.15" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.16" style="font-size:50%;">b</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.17" style="font-size:50%;">))</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.18" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.19" style="font-size:50%;">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.20" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.21" style="font-size:50%;">2</span>
</span>
<span class="ltx_listingline" id="lstnumberx11"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx11.2" style="font-size:50%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.4" style="font-size:50%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx11.6" style="font-size:50%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.7" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx11.8" style="font-size:50%;color:#FF00FF;">range</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.9" style="font-size:50%;">(1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.10" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.11" style="font-size:50%;">n</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.12" style="font-size:50%;">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx12"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.2" style="font-size:50%;">s</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.4" style="font-size:50%;">+=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.6" style="font-size:50%;">f</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.7" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.8" style="font-size:50%;">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.9" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.10" style="font-size:50%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.11" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.12" style="font-size:50%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.13" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.14" style="font-size:50%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.15" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.16" style="font-size:50%;">h</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.17" style="font-size:50%;">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx13"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx13.2" style="font-size:50%;color:#FF00FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.4" style="font-size:50%;">h</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.6" style="font-size:50%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.7" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.8" style="font-size:50%;">s</span>
</span>
</span>
</span></span></p>
<p class="ltx_p" id="S4.T3.4.4.2.2.3">This code takes in a function f, the lower bound a, the upper bound b, and the number of subintervals n. It then calculates the width of each subinterval h, and uses the trapezoidal rule to approximate the integral of f between a and b. The result is returned as a float.</p>
<p class="ltx_p" id="S4.T3.4.4.2.2.4">To use this function, you would call it with the function you want to integrate, the lower bound, the upper bound, and the number of subintervals you want to use. For example:</p>
<div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing" id="S4.T3.4.4.2.2.5" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ZGVmIGYoeCk6CiAgICByZXR1cm4geCoqMgoKcmVzdWx0ID0gaW50ZWdyYXRlKGYsIDAsIDEsIDEwMDApCnByaW50KHJlc3VsdCk=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx14.1" style="font-size:50%;color:#FF00FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3" style="font-size:50%;">f</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.4" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5" style="font-size:50%;">x</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.6" style="font-size:50%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx15.2" style="font-size:50%;color:#FF00FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.4" style="font-size:50%;">x</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.5" style="font-size:50%;">**2</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.1" style="font-size:50%;">result</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.3" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.4" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.5" style="font-size:50%;">integrate</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.6" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.7" style="font-size:50%;">f</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.8" style="font-size:50%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.9" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.10" style="font-size:50%;">0,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.11" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.12" style="font-size:50%;">1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.13" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.14" style="font-size:50%;">1000)</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx18.1" style="font-size:50%;color:#FF00FF;">print</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.2" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.3" style="font-size:50%;">result</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4" style="font-size:50%;">)</span>
</div>
</div>
<p class="ltx_p" id="S4.T3.4.4.2.2.6">This would integrate the function f(x) = x**2 between 0 and 1 using 1000 subintervals, and print the result.</p>
</div>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.6.6.3" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T3.6.6.3.1">List the prime numbers between 20 and 30 with Java.</p>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S4.T3.6.6.2" style="width:284.5pt;">
<div class="ltx_block ltx_align_top" id="S4.T3.6.6.2.2">
<p class="ltx_p" id="S4.T3.6.6.2.2.2"><span class="ltx_inline-para ltx_minipage ltx_align_middle" id="S4.T3.6.6.2.2.2.2" style="width:290.0pt;">
<span class="ltx_para" id="S4.T3.5.5.1.1.1.1.p1">
<span class="ltx_p" id="S4.T3.5.5.1.1.1.1.p1.1">Here’s the Java code to list the prime numbers between 20 and 30:</span>
</span>
<span class="ltx_para" id="S4.T3.6.6.2.2.2.2.p2">
<span class="ltx_listing ltx_lst_language_Java ltx_lstlisting ltx_listing" id="S4.T3.6.6.2.2.2.2.p2.1" style="background-color:#F2F2EB;"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,cHVibGljIGNsYXNzIFByaW1lTnVtYmVycyB7CiAgICBwdWJsaWMgc3RhdGljIHZvaWQgbWFpbihTdHJpbmdbXSBhcmdzKSB7CiAgICAgICAgZm9yIChpbnQgaSA9IDIwOyBpIDw9IDMwOyBpKyspIHsKICAgICAgICAgICAgYm9vbGVhbiBpc1ByaW1lID0gdHJ1ZTsKICAgICAgICAgICAgZm9yIChpbnQgaiA9IDI7IGogPCBpOyBqKyspIHsKICAgICAgICAgICAgICAgIGlmIChpICUgaiA9PSAwKSB7CiAgICAgICAgICAgICAgICAgICAgaXNQcmltZSA9IGZhbHNlOwogICAgICAgICAgICAgICAgICAgIGJyZWFrOwogICAgICAgICAgICAgICAgfQogICAgICAgICAgICB9CiAgICAgICAgICAgIGlmIChpc1ByaW1lKSB7CiAgICAgICAgICAgICAgICBTeXN0ZW0ub3V0LnByaW50KGkgKyAiICIpOwogICAgICAgICAgICB9CiAgICAgICAgfQogICAgfQp9">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx19"><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx19.1" style="font-size:50%;color:#FF00FF;">public</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx19.3" style="font-size:50%;color:#FF00FF;">class</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.4" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.5" style="font-size:50%;">PrimeNumbers</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.6" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.7" style="font-size:50%;">{</span>
</span>
<span class="ltx_listingline" id="lstnumberx20"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx20.2" style="font-size:50%;color:#FF00FF;">public</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx20.4" style="font-size:50%;color:#FF00FF;">static</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx20.6" style="font-size:50%;color:#FF00FF;">void</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.7" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.8" style="font-size:50%;">main</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.9" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.10" style="font-size:50%;">String</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.11" style="font-size:50%;">[]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.12" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.13" style="font-size:50%;">args</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.14" style="font-size:50%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.15" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.16" style="font-size:50%;">{</span>
</span>
<span class="ltx_listingline" id="lstnumberx21"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx21.2" style="font-size:50%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.4" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx21.5" style="font-size:50%;color:#FF00FF;">int</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.6" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.7" style="font-size:50%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.8" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.9" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.10" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.11" style="font-size:50%;">20;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.12" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.13" style="font-size:50%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.14" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.15" style="font-size:50%;">&lt;=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.16" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.17" style="font-size:50%;">30;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.18" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.19" style="font-size:50%;">i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.20" style="font-size:50%;">++)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.21" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.22" style="font-size:50%;">{</span>
</span>
<span class="ltx_listingline" id="lstnumberx22"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx22.2" style="font-size:50%;color:#FF00FF;">boolean</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.4" style="font-size:50%;">isPrime</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.6" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.7" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx22.8" style="font-size:50%;color:#FF00FF;">true</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.9" style="font-size:50%;">;</span>
</span>
<span class="ltx_listingline" id="lstnumberx23"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx23.2" style="font-size:50%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.4" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx23.5" style="font-size:50%;color:#FF00FF;">int</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.6" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.7" style="font-size:50%;">j</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.8" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.9" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.10" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.11" style="font-size:50%;">2;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.12" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.13" style="font-size:50%;">j</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.14" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.15" style="font-size:50%;">&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.16" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.17" style="font-size:50%;">i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.18" style="font-size:50%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.19" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.20" style="font-size:50%;">j</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.21" style="font-size:50%;">++)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.22" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.23" style="font-size:50%;">{</span>
</span>
<span class="ltx_listingline" id="lstnumberx24"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx24.2" style="font-size:50%;color:#FF00FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.4" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.5" style="font-size:50%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.6" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.7" style="font-size:50%;">%</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.8" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.9" style="font-size:50%;">j</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.10" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.11" style="font-size:50%;">==</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.12" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.13" style="font-size:50%;">0)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.14" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.15" style="font-size:50%;">{</span>
</span>
<span class="ltx_listingline" id="lstnumberx25"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.2" style="font-size:50%;">isPrime</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.4" style="font-size:50%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.5" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx25.6" style="font-size:50%;color:#FF00FF;">false</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.7" style="font-size:50%;">;</span>
</span>
<span class="ltx_listingline" id="lstnumberx26"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx26.2" style="font-size:50%;color:#FF00FF;">break</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.3" style="font-size:50%;">;</span>
</span>
<span class="ltx_listingline" id="lstnumberx27"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.2" style="font-size:50%;">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx28"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.2" style="font-size:50%;">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx29"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx29.2" style="font-size:50%;color:#FF00FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.3" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.4" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.5" style="font-size:50%;">isPrime</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.6" style="font-size:50%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.7" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.8" style="font-size:50%;">{</span>
</span>
<span class="ltx_listingline" id="lstnumberx30"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.2" style="font-size:50%;">System</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.3" style="font-size:50%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.4" style="font-size:50%;">out</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.5" style="font-size:50%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.6" style="font-size:50%;">print</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.7" style="font-size:50%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.8" style="font-size:50%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.9" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.10" style="font-size:50%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.11" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx30.12" style="font-size:50%;color:#9400D1;">"<span class="ltx_text ltx_lst_space" id="lstnumberx30.12.1">&nbsp;</span>"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.13" style="font-size:50%;">);</span>
</span>
<span class="ltx_listingline" id="lstnumberx31"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.2" style="font-size:50%;">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx32"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.2" style="font-size:50%;">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx33"><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.1" style="font-size:50%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.2" style="font-size:50%;">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx34"><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.1" style="font-size:50%;">}</span>
</span>
</span>
</span></span></p>
<p class="ltx_p" id="S4.T3.6.6.2.2.3">Output:</p>
<div class="ltx_listing ltx_lst_language_Java ltx_lstlisting ltx_listing" id="S4.T3.6.6.2.2.4" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,MjMgMjk=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx35">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx35.1" style="font-size:50%;">23</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.2" style="font-size:50%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.3" style="font-size:50%;">29</span>
</div>
</div>
<p class="ltx_p" id="S4.T3.6.6.2.2.5">Explanation:</p>
<p class="ltx_p" id="S4.T3.6.6.2.2.6">- We use a nested for loop to iterate through all the numbers between 20 and 30.</p>
<p class="ltx_p" id="S4.T3.6.6.2.2.7">- For each number, we use another for loop to check if it is prime or not.</p>
<p class="ltx_p" id="S4.T3.6.6.2.2.8">- If a number is divisible by any number other than 1 and itself, then it is not prime.</p>
<p class="ltx_p" id="S4.T3.6.6.2.2.9">- If a number is not divisible by any number other than 1 and itself, then it is prime.</p>
<p class="ltx_p" id="S4.T3.6.6.2.2.10">- We print the prime numbers to the console.</p>
</div>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Examples</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">Table&nbsp;<a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.5 Ablation Study ‣ 4 Experiment ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">3</span></a> showcases examples of interactions with our <em class="ltx_emph ltx_font_italic" id="S4.SS6.p1.1.1">WizardCoder</em>. The examples demonstrate that our model consistently generates accurate responses accompanied by clear explanations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper introduces <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">WizardCoder</em>, a Code <em class="ltx_emph ltx_font_italic" id="S5.p1.1.2">Evol-Instruct</em> fine-tuned Code LLM. The experimental results demonstrate that <em class="ltx_emph ltx_font_italic" id="S5.p1.1.3">WizardCoder</em> achieves SOTA performance surpassing all existing open-source Code LLMs on four widely recognized code generation benchmarks: HumanEval, HumanEval+, MBPP, and DS-1000. Furthermore, <em class="ltx_emph ltx_font_italic" id="S5.p1.1.4">WizardCoder</em> exhibits superior performance compared to the largest closed LLMs, including Anthropic’s Claude and Google’s Bard.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Future Work.</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Although our <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.1.1">WizardCoder</em> demonstrates impressive coding performance, as depicted in Figure&nbsp;<a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.2 Training WizardCoder ‣ 3 Approach ‣ WizardCoder: Empowering Code Large Language Models with Evol-Instruct"><span class="ltx_text ltx_ref_tag">1</span></a>, our model still falls significantly behind the SOTA LLM, GPT4. Therefore, future work will prioritize the enhancement of the Code <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.1.2">Evol-Instruct</em> method to further augment the performance of our model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Broader Impact.</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Similar to the other LLMs, our <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">WizardCoder</em> could also generate unethical, harmful, or misleading information. Therefore, future research to address the ethical and societal implications is needed.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.2.2.1" style="font-size:90%;">(1)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.4.1" style="font-size:90%;">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.5.1" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.6.1" style="font-size:90%;">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib1.7.2" style="font-size:90%;">Advances in
Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual</span><span class="ltx_text" id="bib.bib1.8.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.2.2.1" style="font-size:90%;">(2)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.4.1" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.5.1" style="font-size:90%;">GPT-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib2.7.2" style="font-size:90%;">, abs/2303.08774, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.2.2.1" style="font-size:90%;">(3)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.4.1" style="font-size:90%;">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
</span><span class="ltx_text" id="bib.bib3.5.2" style="font-size:90%;">Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai,
Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah
Fiedel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.6.1" style="font-size:90%;">Palm: Scaling language modeling with pathways.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib3.8.2" style="font-size:90%;">, abs/2204.02311, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.2.2.1" style="font-size:90%;">(4)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.4.1" style="font-size:90%;">
Rohan Anil, Andrew&nbsp;M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
Eric Chu, Jonathan&nbsp;H. Clark, Laurent&nbsp;El Shafey, Yanping Huang, Kathy
Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi&nbsp;Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo&nbsp;Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan&nbsp;A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele
</span><span class="ltx_text" id="bib.bib4.5.2" style="font-size:90%;">Catasta, Yong Cheng, Colin Cherry, Christopher&nbsp;A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg,
Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian
Gehrmann, Lucas Gonzalez, and et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.6.1" style="font-size:90%;">Palm 2 technical report.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib4.8.2" style="font-size:90%;">, abs/2305.10403, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.2.2.1" style="font-size:90%;">(5)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.4.1" style="font-size:90%;">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van&nbsp;den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and Laurent Sifre.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.5.1" style="font-size:90%;">Training compute-optimal large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib5.7.2" style="font-size:90%;">, abs/2203.15556, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.2.2.1" style="font-size:90%;">(6)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.4.1" style="font-size:90%;">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
H.&nbsp;Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant&nbsp;M. Jayakumar, Elena Buchatskaya, David Budden, Esme
</span><span class="ltx_text" id="bib.bib6.5.2" style="font-size:90%;">Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones, James
Bradbury, Matthew&nbsp;J. Johnson, Blake&nbsp;A. Hechtman, Laura Weidinger, Iason
Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris
Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis
Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.6.1" style="font-size:90%;">Scaling language models: Methods, analysis &amp; insights from
training gopher.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib6.8.2" style="font-size:90%;">, abs/2112.11446, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.2.2.1" style="font-size:90%;">(7)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.4.1" style="font-size:90%;">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng&nbsp;Lam Tam, Zixuan Ma, Yufei Xue,
Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.5.1" style="font-size:90%;">GLM-130B: an open bilingual pre-trained model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib7.7.2" style="font-size:90%;">, abs/2210.02414, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.2.2.1" style="font-size:90%;">(8)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.4.1" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
</span><span class="ltx_text" id="bib.bib8.5.2" style="font-size:90%;">Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.6.1" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib8.8.2" style="font-size:90%;">, abs/2302.13971, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.2.2.1" style="font-size:90%;">(9)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.4.1" style="font-size:90%;">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona&nbsp;T. Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor
Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh
Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.5.1" style="font-size:90%;">OPT: open pre-trained transformer language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib9.7.2" style="font-size:90%;">, abs/2205.01068, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.2.2.1" style="font-size:90%;">(10)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.4.1" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul&nbsp;F. Christiano, Jan Leike, and Ryan Lowe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.5.1" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.7.2" style="font-size:90%;">NeurIPS</span><span class="ltx_text" id="bib.bib10.8.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.2.2.1" style="font-size:90%;">(11)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.4.1" style="font-size:90%;">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.5.1" style="font-size:90%;">Starcoder: may the source be with you!
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.6.1" style="font-size:90%;">arXiv preprint arXiv:2305.06161</span><span class="ltx_text" id="bib.bib11.7.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.2.2.1" style="font-size:90%;">(12)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.4.1" style="font-size:90%;">
Yujia Li, David&nbsp;H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin&nbsp;Dal
Lago, Thomas Hubert, Peter Choy, Cyprien de&nbsp;Masson&nbsp;d’Autume, Igor Babuschkin,
Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,
James Molloy, Daniel&nbsp;J. Mankowitz, Esme&nbsp;Sutherland Robson, Pushmeet Kohli,
Nando de&nbsp;Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.5.1" style="font-size:90%;">Competition-level code generation with alphacode.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib12.7.2" style="font-size:90%;">, abs/2203.07814, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.2.2.1" style="font-size:90%;">(13)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.4.1" style="font-size:90%;">
Erik Nijkamp, Bo&nbsp;Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio
Savarese, and Caiming Xiong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.5.1" style="font-size:90%;">Codegen: An open large language model for code with multi-turn
program synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.7.2" style="font-size:90%;">The Eleventh International Conference on Learning
Representations</span><span class="ltx_text" id="bib.bib13.8.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.2.2.1" style="font-size:90%;">(14)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.4.1" style="font-size:90%;">
Qinkai Zheng, Xiao Xia, Xu&nbsp;Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang,
Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.5.1" style="font-size:90%;">Codegeex: A pre-trained model for code generation with multilingual
evaluations on humaneval-x.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib14.7.2" style="font-size:90%;">, abs/2303.17568, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.2.2.1" style="font-size:90%;">(15)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.4.1" style="font-size:90%;">
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.5.1" style="font-size:90%;">Incoder: A generative model for code infilling and synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib15.7.2" style="font-size:90%;">, abs/2204.05999, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.2.2.1" style="font-size:90%;">(16)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.4.1" style="font-size:90%;">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Pondé
de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings,
Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,
William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
Hesse, Andrew&nbsp;N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
Wojciech Zaremba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.5.1" style="font-size:90%;">Evaluating large language models trained on code.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib16.7.2" style="font-size:90%;">, abs/2107.03374, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.2.2.1" style="font-size:90%;">(17)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.4.1" style="font-size:90%;">
Yue Wang, Weishi Wang, Shafiq&nbsp;R. Joty, and Steven C.&nbsp;H. Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.5.1" style="font-size:90%;">Codet5: Identifier-aware unified pre-trained encoder-decoder models
for code understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.6.1" style="font-size:90%;">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and
Scott&nbsp;Wen-tau Yih, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib17.7.2" style="font-size:90%;">Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11 November, 2021</span><span class="ltx_text" id="bib.bib17.8.3" style="font-size:90%;">, pages 8696–8708.
Association for Computational Linguistics, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.2.2.1" style="font-size:90%;">(18)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.4.1" style="font-size:90%;">
Yue Wang, Hung Le, Akhilesh&nbsp;Deepak Gotmare, Nghi D.&nbsp;Q. Bui, Junnan Li, and
Steven C.&nbsp;H. Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.5.1" style="font-size:90%;">Codet5+: Open code large language models for code understanding and
generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib18.7.2" style="font-size:90%;">, abs/2305.07922, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.2.2.1" style="font-size:90%;">(19)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.4.1" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.5.1" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text
transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.6.1" style="font-size:90%;">J. Mach. Learn. Res.</span><span class="ltx_text" id="bib.bib19.7.2" style="font-size:90%;">, 21:140:1–140:67, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.2.2.1" style="font-size:90%;">(20)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.4.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib20.5.2" style="font-size:90%;">Jason Wei, Maarten Bosma, Vincent&nbsp;Y. Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian
Lester, Nan Du, Andrew&nbsp;M. Dai, and Quoc&nbsp;V. Le.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.6.1" style="font-size:90%;">Finetuned language models are zero-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.8.2" style="font-size:90%;">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</span><span class="ltx_text" id="bib.bib20.9.3" style="font-size:90%;">.
OpenReview.net, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.2.2.1" style="font-size:90%;">(21)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.4.1" style="font-size:90%;">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
Shixiang&nbsp;Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent&nbsp;Y. Zhao, Yanping
Huang, Andrew&nbsp;M. Dai, Hongkun Yu, Slav Petrov, Ed&nbsp;H. Chi, Jeff Dean, Jacob
Devlin, Adam Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.5.1" style="font-size:90%;">Scaling instruction-finetuned language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib21.7.2" style="font-size:90%;">, abs/2210.11416, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.2.2.1" style="font-size:90%;">(22)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.4.1" style="font-size:90%;">
Vamsi Aribandi, Yi&nbsp;Tay, Tal Schuster, Jinfeng Rao, Huaixiu&nbsp;Steven Zheng,
Sanket&nbsp;Vaibhav Mehta, Honglei Zhuang, Vinh&nbsp;Q. Tran, Dara Bahri, Jianmo Ni,
Jai&nbsp;Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.5.1" style="font-size:90%;">Ext5: Towards extreme multi-task scaling for transfer learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.7.2" style="font-size:90%;">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</span><span class="ltx_text" id="bib.bib22.8.3" style="font-size:90%;">.
OpenReview.net, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.2.2.1" style="font-size:90%;">(23)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.4.1" style="font-size:90%;">
Victor Sanh, Albert Webson, Colin Raffel, Stephen&nbsp;H. Bach, Lintang Sutawika,
Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M&nbsp;Saiful Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza
Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal&nbsp;V. Nayak, Debajyoti Datta,
Jonathan Chang, Mike&nbsp;Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng&nbsp;Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason&nbsp;Alan
Fries, Ryan Teehan, Teven&nbsp;Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and
Alexander&nbsp;M. Rush.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.5.1" style="font-size:90%;">Multitask prompted training enables zero-shot task generalization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.7.2" style="font-size:90%;">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</span><span class="ltx_text" id="bib.bib23.8.3" style="font-size:90%;">.
OpenReview.net, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.2.2.1" style="font-size:90%;">(24)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.4.1" style="font-size:90%;">
Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin
Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.5.1" style="font-size:90%;">Zeroprompt: Scaling prompt-based pretraining to 1, 000 tasks improves
zero-shot generalization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.6.1" style="font-size:90%;">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib24.7.2" style="font-size:90%;">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu
Dhabi, United Arab Emirates, December 7-11, 2022</span><span class="ltx_text" id="bib.bib24.8.3" style="font-size:90%;">, pages 4235–4252.
Association for Computational Linguistics, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.2.2.1" style="font-size:90%;">(25)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.4.1" style="font-size:90%;">
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,
Peter Clark, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.5.1" style="font-size:90%;">Unifiedqa: Crossing format boundaries with a single QA system.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.6.1" style="font-size:90%;">In Trevor Cohn, Yulan He, and Yang Liu, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib25.7.2" style="font-size:90%;">Findings of the
Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20
November 2020</span><span class="ltx_text" id="bib.bib25.8.3" style="font-size:90%;">, volume EMNLP 2020 of </span><span class="ltx_text ltx_font_italic" id="bib.bib25.9.4" style="font-size:90%;">Findings of ACL</span><span class="ltx_text" id="bib.bib25.10.5" style="font-size:90%;">, pages
1896–1907. Association for Computational Linguistics, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.2.2.1" style="font-size:90%;">(26)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.4.1" style="font-size:90%;">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.5.1" style="font-size:90%;">Stanford alpaca: An instruction-following llama model.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" style="font-size:90%;" title="">https://github.com/tatsu-lab/stanford_alpaca</a><span class="ltx_text" id="bib.bib26.6.1" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.2.2.1" style="font-size:90%;">(27)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.4.1" style="font-size:90%;">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A Smith, Daniel
Khashabi, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.5.1" style="font-size:90%;">Self-instruct: Aligning language model with self generated
instructions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.6.1" style="font-size:90%;">arXiv preprint arXiv:2212.10560</span><span class="ltx_text" id="bib.bib27.7.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.2.2.1" style="font-size:90%;">(28)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.4.1" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
</span><span class="ltx_text" id="bib.bib28.5.2" style="font-size:90%;">Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and
Eric&nbsp;P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.6.1" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.2.2.1" style="font-size:90%;">(29)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.4.1" style="font-size:90%;">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang
Tao, and Daxin Jiang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.5.1" style="font-size:90%;">Wizardlm: Empowering large language models to follow complex
instructions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.6.1" style="font-size:90%;">arXiv preprint arXiv:2304.12244</span><span class="ltx_text" id="bib.bib29.7.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.2.2.1" style="font-size:90%;">(30)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.4.1" style="font-size:90%;">
Sahil Chaudhary.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.5.1" style="font-size:90%;">Code alpaca: An instruction-following llama model for code
generation.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sahil280114/codealpaca" style="font-size:90%;" title="">https://github.com/sahil280114/codealpaca</a><span class="ltx_text" id="bib.bib30.6.1" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.2.2.1" style="font-size:90%;">(31)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.4.1" style="font-size:90%;">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Pondé
de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings,
</span><span class="ltx_text" id="bib.bib31.5.2" style="font-size:90%;">Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,
William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
Hesse, Andrew&nbsp;N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
Wojciech Zaremba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.6.1" style="font-size:90%;">Evaluating large language models trained on code.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib31.8.2" style="font-size:90%;">, abs/2107.03374, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.2.2.1" style="font-size:90%;">(32)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.4.1" style="font-size:90%;">
Jiawei Liu, Chunqiu&nbsp;Steven Xia, Yuyao Wang, and Lingming Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.5.1" style="font-size:90%;">Is your code generated by chatgpt really correct? rigorous evaluation
of large language models for code generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib32.7.2" style="font-size:90%;">, abs/2305.01210, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.2.2.1" style="font-size:90%;">(33)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.4.1" style="font-size:90%;">
Jacob Austin, Augustus Odena, Maxwell&nbsp;I. Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie&nbsp;J. Cai, Michael Terry, Quoc&nbsp;V.
Le, and Charles Sutton.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.5.1" style="font-size:90%;">Program synthesis with large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib33.7.2" style="font-size:90%;">, abs/2108.07732, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.2.2.1" style="font-size:90%;">(34)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.4.1" style="font-size:90%;">
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke
</span><span class="ltx_text" id="bib.bib34.5.2" style="font-size:90%;">Zettlemoyer, Scott&nbsp;Wen-tau Yih, Daniel Fried, Sida&nbsp;I. Wang, and Tao Yu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.6.1" style="font-size:90%;">DS-1000: A natural and reliable benchmark for data science code
generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib34.8.2" style="font-size:90%;">, abs/2211.11501, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.2.2.1" style="font-size:90%;">(35)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.4.1" style="font-size:90%;">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
USVSN&nbsp;Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
Wang, and Samuel Weinbach.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.5.1" style="font-size:90%;">Gpt-neox-20b: An open-source autoregressive language model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib35.7.2" style="font-size:90%;">, abs/2204.06745, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.2.2.1" style="font-size:90%;">(36)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.4.1" style="font-size:90%;">
Ben Wang and Aran Komatsuzaki.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.5.1" style="font-size:90%;">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kingoflolz/mesh-transformer-jax" style="font-size:90%;" title="">https://github.com/kingoflolz/mesh-transformer-jax</a><span class="ltx_text" id="bib.bib36.6.1" style="font-size:90%;">, May 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.2.2.1" style="font-size:90%;">(37)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.4.1" style="font-size:90%;">
Yi&nbsp;Tay, Mostafa Dehghani, Vinh&nbsp;Q. Tran, Xavier Garcia, Dara Bahri, Tal
Schuster, Huaixiu&nbsp;Steven Zheng, Neil Houlsby, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.5.1" style="font-size:90%;">Unifying language learning paradigms.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.6.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib37.7.2" style="font-size:90%;">, abs/2205.05131, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.2.2.1" style="font-size:90%;">(38)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.4.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib38.5.2" style="font-size:90%;">Microsoft.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.6.1" style="font-size:90%;">Azure openai service models.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models" style="font-size:90%;" title="">https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models</a><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.2.2.1" style="font-size:90%;">(39)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.4.1" style="font-size:90%;">
Llm humaneval benchmarks.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/my-other-github-account/llm-humaneval-benchmarks" style="font-size:90%;" title="">https://github.com/my-other-github-account/llm-humaneval-benchmarks</a><span class="ltx_text" id="bib.bib39.5.1" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.2.2.1" style="font-size:90%;">(40)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.4.1" style="font-size:90%;">
Romal Thoppilan, Daniel&nbsp;De Freitas, Jamie Hall, Noam Shazeer, Apoorv
Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu&nbsp;Du,
YaGuang Li, Hongrae Lee, Huaixiu&nbsp;Steven Zheng, Amin Ghafouri, Marcelo
Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou,
Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen&nbsp;S.
Meier-Hellstern, Meredith&nbsp;Ringel Morris, Tulsee Doshi, Renelito&nbsp;Delos
Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,
Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin
Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew
Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray
Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed&nbsp;H. Chi, and
</span><span class="ltx_text" id="bib.bib40.5.2" style="font-size:90%;">Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.6.1" style="font-size:90%;">Lamda: Language models for dialog applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.7.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib40.8.2" style="font-size:90%;">, abs/2201.08239, 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 13 18:39:44 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="></a>
</div></footer>
</div>


</body></html>
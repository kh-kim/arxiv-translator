# _WizardCoder_: Evol-Instruct를 사용하여 코드 대용량 언어 모델 임파워링

Ziyang Luo\({}^{2}\)1 Can Xu\({}^{1}\)1 Pu Zhao\({}^{1}\) Qingfeng Sun\({}^{1}\) Xiubo Geng\({}^{1}\)

**원샹후\({}^{1}\) 청양도\({}^{1}\) 징마\({}^{2}\) 칭웨이린\({}^{1}\) 다신강\({}^{1}\)2 \({}^{1}\)Microsoft**

홍콩침례대학

{caxu,puzhao,qins,xigeng,wenxh,chongyang.tao,qlin,djiang}@microsoft.com

{cszyluo, majing}@comp.hkbu.edu.hk

동등한 기여. 마이크로소프트 교신저자의 인턴십 기간 동안 일했다.

각주 1: 교신저자.

###### Abstract

StarCoder와 같은 코드 대용량 언어 모델(Code LLMs)은 코드 관련 작업에서 탁월한 성능을 보여주었다. 그러나 대부분의 기존 모델은 명령어 미세 조정 없이 광범위한 원시 코드 데이터에 대해서만 사전 훈련된다. 본 논문에서는 코드 영역에 Evol-Instruct 방법을 적용하여 복잡한 명령어 미세 조정을 통해 코드 LLM을 가능하게 하는 WizardCoder를 소개한다. 4개의 주요 코드 생성 벤치마크인 HumanEval, HumanEval+, MBPP 및 DS-1000에 대한 포괄적인 실험을 통해 모델의 탁월한 성능을 공개합니다. 다른 모든 오픈 소스 코드 LLM을 상당한 차이로 능가합니다. 게다가, 우리 모델은 휴먼에벌과 휴먼에벌+에서 가장 큰 폐쇄형 LLM인 앤트로픽의 클로드와 구글의 바드를 능가한다. 코드, 모델 가중치 및 데이터는 [https://github.com/nlpxucan/WizardLM](https://github.com/nlpxucan/WizardLM)에 공개됩니다.

## 1 Introduction

최근, 큰 언어 모델들[1; 2; 3; 4; 5; 6; 7; 8; 9]이 상당한 관심을 끌었고 인상적인 성공을 보여주었다. 특히 OpenAI의 ChatGPT가 두드러진 사례로 눈에 띈다. 광범위한 양의 인터넷 데이터에 대한 광범위한 사전 훈련과 상세한 명령 데이터로 추가 미세 조정을 활용하여 [10] 이러한 모델은 다양한 작업에 걸쳐 최첨단(SOTA) 제로 샷 성능을 달성했다. 이러한 경향은 코드 이해와 생성의 영역에서도 관찰된다. 수많은 코드 LLM[11; 12; 13; 14; 15; 16; 17; 18]이 코드 관련 작업과 관련된 과제를 해결하기 위해 제안되었다. 이러한 코드 LLM은 상당한 양의 코드 데이터를 사용하여 사전 훈련을 거쳐 다양한 코드 관련 작업을 탁월하게 수행할 수 있어 인상적인 성능을 보여줍니다.

사전 훈련 과정을 주로 강조하는 대부분의 이전 코드 LLM과 달리 코드 도메인에서 세밀한 명령어 튜닝에 대한 탐색은 제한적이었다. 명령어 튜닝의 도입은 처음에 상이한 태스크들에 걸쳐 LMs들의 일반화 능력들을 향상시키는 것을 목표로 하였다[19; 20; 21; 22; 23; 24; 25]. 예를 들어 OpenAI의 InstructGPT [10]은 사용자의 의도와 정렬을 보장하기 위해 명시적인 지침을 제공하기 위해 인간 주석자를 요청하는 것을 포함했다. 마찬가지로 Alpaca[26]과 같은 최근 연구에서는 ChatGPT가 명령 데이터를 생성하는 자체 명령 [27] 방법을 사용했다. Vicuna [28]은 ShareGPT.com에서 수집된 사용자 공유 대화를 활용했다. WizardLM [29]는 보다 복잡하고 다양한 데이터 세트를 생성하기 위해 기존 명령 데이터를 진화시키는 _Evol-Instruct_ 방법을 도입했다. 그러나 이러한 모든 접근법은 주로 일반 도메인에 초점을 맞추고 코드 도메인에 대한 구체적인 설계 고려 사항이 부족하다는 점에 주목할 필요가 있다.

본 연구는 _Evol-Instruct_ 방법에 의해 동기화된 코드별 _Evol-Instruct_를 통해 복잡한 코드 명령어 데이터를 생성함으로써 SOTA 오픈소스 코드 LLM인 StarCoder[11]의 성능을 향상시키는 것을 목적으로 한다. 이를 위해 코드 관련 작업을 위해 특별히 조정된 진화적 프롬프트 프로세스에 몇 가지 적응을 수행했다. 이러한 수정은 진화 명령어를 정제하는 것, 진화 프롬프트의 형태를 단순화하는 것, 코드 디버깅 및 시공간 복잡성 제약들을 통합하는 것을 포함한다. 초기에는 기본 코드 명령 데이터인 코드 알파카[30]를 진화시키기 위해 이 방법을 적용하였다. 그런 다음 새로 만든 코드 명령 후속 훈련 세트를 사용하여 StarCoder의 미세 조정을 수행하고 _WizardCoder_ 를 가져옵니다.

4개의 코드 생성 벤치마크인 HumanEval [31], HumanEval+ [32], MBPP [33] 및 DS-100 [34]에서 얻은 실험 결과는 우리의 _WizardCoder_가 다른 모든 오픈 소스 코드 LLMs보다 우수함으로써 최첨단(SOTA) 성능을 달성함을 보여준다. 특히, +22.3(57.3 vs. 35.0)의 증가로 패스@1 점수의 상당한 개선을 관찰한다. In HumanEval and +8.2 (51.8 vs. 43.6) (p<0.05). 놀랍게도, 훨씬 작은 규모에도 불구하고, 우리의 위저드코더는 휴먼에벌과 휴먼에벌+의 합격률에서 인류학의 클로드와 구글의 바드를 능가한다.

이 작품의 공헌은 다음과 같이 요약할 수 있다.

* Code _Evol-Instruct_ 적용을 통해 오픈 소스 Code LLM인 StarCoder의 성능을 향상시키는 _WizardCoder_ 를 소개합니다.
* _WizardCoder_ 는 StarCoder, CodeGen, CodeGee, CodeT5+, InstructCodeT5+, StarCoder-GPTeacher 및 Instruct-Codegen-16B를 포함 하 여 코드 생성 측면에서 다른 모든 오픈 소스 코드 LLM을 상당히 능가 합니다.
* _WizardCoder_ 는 크기가 상당히 작음에도 불구하고 Claude, Bard, PaLM, PaLM-2 및 LaMDA와 같은 가장 큰 폐쇄 소스 LLM에 비해 코드 생성에서 우수한 결과를 달성합니다.

## 2 관련 작업

대규모 언어 모델.최근 LLM은 광범위한 작업에서 놀라운 성과를 보여주었다. 저명한 기술 회사들은 매우 능숙한 LLM을 개발하는 데 상당한 진전을 이루었다. 여기에는 OpenAI의 GPT3&4[1; 2], 구글의 PaLM[3; 4], Bard3, DeepMind의 친칠라[5], 고퍼[6], Anthropic의 Claude4 등이 포함된다. 그러나 이러한 모델은 폐쇄 소스이며 특정 API를 통해서만 액세스할 수 있거나 전혀 액세스할 수 없을 수 있다는 점에 유의하는 것이 중요하다.

각주 3: [https://bard.google.com/](https://bard.google.com/)

각주 4: [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude)

AI 커뮤니티는 모델 가중치가 공개적으로 공개되는 여러 오픈 소스 LLM이 출시되는 것을 목격했다. EleutherAI는 GPT-NeoX-20B[35]와 GPT-J-GB[36]에 기여했다. 구글은 UL2-20B를 출시했다[37]. 칭화대학은 GLM-130B[7]을 도입했다. 메타가 OPT[9]와 LLaMA[8]를 출시했다. 이러한 오픈 소스 모델은 귀중한 기여를 했지만 일반적으로 폐쇄 소스 모델과 동일한 수준의 성능을 나타내지 않는다는 점에 주목할 필요가 있다.

코드에 대한 대규모 언어 모델 최근 연구는 코드 이해와 생성의 어려움을 해결하기 위해 코드 관련 작업에 대한 상당수의 LLM을 도입했다. OpenAI는 코덱스[16]와 코드다빈치[38]를 공개했다. 구글은 PaLM-Coder[3]를 제안했다. 그들은 HumanEval [31] 및 MBPP [33]과 같은 인기 있는 코드 완성 벤치마크에서 뛰어난 성능을 발휘합니다. 그러나 이러한 모델은 폐쇄 소스입니다.

한편, 이용 가능한 오픈 소스 코드 LLM은 여러 가지가 있다. Salesforce는 CodeGen[13], CodeT5[17], CodeT5+[18]을 도입하였다. 칭화대는 CodeGeeX[14]를, 빅코드 프로젝트는 스타코더[11]를 개발했다. 이러한 모델은 코드 관련 작업에서 주목할 만한 발전을 보여주었다. 그러나 SOTA 폐쇄 소스 모델과 비교할 때 여전히 크게 뒤처진다. 명령어 미세 조정 없이 앞서 언급한 모델과 대조적으로, 본 연구는 코드 _Evol-Instruct_ 를 사용하여 추가 훈련 코드 LLM이 성능을 크게 향상시킬 수 있음을 보여준다.

수업 미세 조정 초기 수업의 주요 목표는 LMs의 교차 작업 일반화 능력을 향상시키는 것이었다. 이것은 공개 NLP 작업의 상당한 코퍼스로 LMs를 미세 조정함으로써 달성되었다. T5[19]는 많은 감독된 텍스트 대 텍스트 작업에 대한 훈련으로 이 접근법을 탐구한 첫 번째 모델 중 하나였다. FLAN[20], ExT5[22], T0[23], UnifiedQA[25]와 같은 후속 작업은 LMs의 전반적인 일반화 능력을 강화하기 위해 작업 범위를 더욱 확장했다. 특히, ZeroPrompt [24]와 FLAN-T5 [21]은 훈련 파이프라인에 수천 개의 작업을 통합하여 한계를 넘어섰다. 이러한 연구 전반에 걸쳐 일관된 발견이 나타났는데, 다양한 NLP 작업 지침으로 LM을 미세 조정하면 새로운 작업에 적용할 때 상당한 성능 개선이 나타난다.

다양한 NLP 작업으로 LM을 미세 조정하면 유망한 결과를 보여주었지만 실제 사용자의 의도에 부합하지 못하는 경우가 많다. OpenAI는 다양한 형태와 광범위한 작업 유형을 포괄하는 인간 명령어의 대규모 코퍼스를 제공하기 위해 인간 주석자를 요청함으로써 다른 접근법을 추구했다. 이 데이터 세트를 기반으로 OpenAI는 GPT3 [1] 모델을 훈련하여 사용자의 입력과 더 잘 일치하는 InstructGPT [10]을 생성했다. 이 발전 라인은 ChatGPT라고 알려진 인상적인 작업으로 이어졌습니다. 그러나 이러한 발전과 관련된 데이터 세트 및 모델 가중치는 공개적으로 사용할 수 없다는 점에 유의하는 것이 중요하다. 알파카[26]는 자가 지시 방법[27]을 채택하여 ChatGPT를 활용하여 훈련을 위한 데이터를 생성함으로써 다른 경로를 취한다. Vicuna[28]는 ShareGPT.com에서 수집된 사용자 공유 대화를 활용하여 모델을 학습시킨다. WizardLM [29]는 기존 명령 데이터를 진화하여 더 복잡하고 다양한 데이터 세트를 생성하는 _Evol-Instruct_ 방법을 소개합니다. 이러한 일반적인 명령 미세 조정 접근법과 달리, 본 논문에서 제안한 WizardCoder는 코드 LLM 도메인에서 _Evol-Instruct_ 메서드를 성공적으로 적용하였다.

## 3 Approach

이 섹션에서는 _WizardCoder_ 의 방법론적 세부 사항에 대해 자세히 설명합니다. WizardLM에 이어 _Evol-Instruct_ 방법을 적용하여 자체 명령을 사용하여 생성된 코드 알파카를 진화시키고 진화된 데이터로 사전 훈련된 코드 LLM StarCoder를 미세 조정합니다.

### Code에 대한 Evol-Instruct Prompt

WizardLM이 제안한 Evol-Instruct [29] 방법에서 영감을 받은 이 작업은 또한 코드 사전 훈련된 대형 모델의 미세 조정 효과를 향상시키기 위해 코드 명령을 더 복잡하게 만들려고 시도한다. Evol-Instruct를 코드의 영역에 적용하기 위해 진화 프롬프트를 다음과 같이 수정했습니다.

1. 심화, 복잡한 입력 및 In-Breadth 진화를 제거하여 진화 지침을 간소화했습니다.
2. 진화 프롬프트 템플릿을 통일하여 진화 프롬프트의 형태를 단순화한다.
3. 코드 도메인의 특정 특성을 해결하기 위해 코드 디버깅과 코드 시공간 복잡도 제약의 두 가지 진화 명령어를 추가했다.

통합된 코드 진화 프롬프트 템플릿은 다음과 같습니다.

주어진 프로그래밍 시험 문제의 난이도를 조금 높여주세요.

다음 방법을 사용하여 난이도를 높일 수 있지만 이에 제한되지 않습니다. {method}

{question}

여기서, {질문}은 진화를 기다리는 현재의 코드 명령을 나타내고, {방법}은 진화의 유형이다. 우리가 사용한 다섯 가지 유형은 다음과 같이 나열된다: 원래 문제에 새로운 제약 조건과 요구 사항을 추가하고 약 10개의 추가 단어를 추가한다.

프로그래밍 작업에서 일반적으로 사용되는 요구 사항을 덜 일반적이고 더 구체적인 요구 사항으로 대체합니다.

논리적 단계 몇 개만으로 원래 문제를 해결할 수 있다면 추론 단계를 더 추가해 주세요.

잘못된 코드를 참조로 제공하여 오방향성을 높입니다.

더 높은 시간 또는 공간 복잡성 요구 사항을 제안하지만 자주 하는 것을 자제하십시오.

### Training _WizardCoder_

다음 절차를 사용하여 _WizardCoder_ 를 교육합니다. 처음에는 스타코더 15B [11]을 기반으로 하고 _Evol-Instruct_를 통해 진화한 코드 명령어 후속 훈련 세트를 사용하여 미세 조정을 진행합니다. 미세 조정을 위한 프롬프트 형식은 다음과 같이 요약됩니다.

아래는 태스크를 설명하는 명령어로, 추가 컨텍스트를 제공하는 입력과 쌍을 이룬다. 요청을 적절하게 완료하는 응답을 작성합니다.

### Instruction: {instruction}

### Response:

훈련 데이터 세트를 구축하기 위해 코드 알파카5라는 20K 명령어 후속 데이터 세트로 초기화했다. 진화된 데이터를 생성하기 위해 20,000개의 샘플로 구성된 이 데이터 세트에 Evol-Instruct 기법을 반복적으로 사용한다. 데이터 진화의 각 라운드 후에, 우리는 모든 이전 라운드로부터 진화된 데이터를 원래 데이터세트와 병합하여 StarCoder를 미세조정하고 HumanEval에서 pass@1 메트릭을 평가한다[31]. 통과@1 메트릭의 감소를 관찰하면, 우리는 Evol-Instruct의 사용을 중단하고 가장 높은 통과@1을 갖는 모델을 궁극적인 모델로 선택할 것이다.

각주 5: [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)

## 4 Experiment

이 섹션은 실험에서 기본 모델에 대한 포괄적인 개요를 제공하는 것으로 시작한다. 다음으로 4개의 코드 생성 벤치마크인 HumanEval[31], HumanEval+[32], MBPP[33], DS-1000[34]에 대한 모델의 성능을 제시한다.

### Baselines

폐쇄형 출처 모델.여러 기술 회사가 공개적으로 출시하지 않기로 선택하면서 매우 능숙한 LLM을 성공적으로 개발했습니다. 이러한 모델을 폐쇄 소스 모델이라고 합니다. 연구를 위해 이러한 모델의 상당수를 기준선으로 통합합니다. 구체적으로, 우리의 기준들은 다음과 같다: (i) OpenAI의 GPT3.5&4 [2], Code-Davinci-002 [38], Code-Cushman-001 [38], 및 Codex [16]; (ii) 구글의 바드, PaLM 2 [4], PaLM [3], 및 LaMDA [40]; (iii) 구글 딥마인드의 알파코드 [12]; 및 (iv) 인류학의 클로드.

오픈 소스 모델. AI 커뮤니티에는 여러 오픈 소스 LLM이 제공되었지만 일반적으로 성능이 폐쇄 소스 모델에 비해 많이 뒤처진다. 연구의 일환으로, 우리는 이러한 오픈 소스 모델의 상당수를 기준선으로 통합합니다. 기준선은 StarCoder [11], LLaMa [8], CodeGen [13], CodeGeeX [14], CodeT5+[18] 및 InCoder [15] 모델을 포함합니다. 또한 StarCoder-GPTeacher,6 Instruct-Codegen-16B,7 Guanaco-65B,8 및 Falcon-40B-Instruct-9를 포함하여 명령어 미세 조정이 있는 여러 모델도 포함한다.

각주 6: [https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct](https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct)

각주 7: [https://huggingface.co/sahil2801/instruct-codegen-16B](https://huggingface.co/sahil2801/instruct-codegen-16B)

각주 8: [https://huggingface.co/TheBloke/guanaco-65B-HF](https://huggingface.co/TheBloke/guanaco-65B-HF)

각주 9: [https://huggingface.co/tiuae/falcon-40b-instruct](https://huggingface.co/tiuae/falcon-40b-instruct)

### Implementation Details

스타코더[11]는 우리의 기본 기초 모델 역할을 한다. 진화된 데이터 세트는 약 78k개의 샘플로 구성된다. 기본 모델을 미세 조정하기 위해 배치 크기 512, 시퀀스 길이 2048, 미세 조정 단계 200, 워밍업 단계 30, 학습 속도 2e-5, 코사인 학습 속도 스케줄러 및 fp16 혼합 정밀도를 포함한 특정 구성을 사용한다.

### HumanEval, HumanEval+, MBPP 평가

HumanEval [31], HumanEval+ [32] 및 MBPP [33]은 코드 LLM 분야에서 광범위하게 활용되는 벤치마크이다. 이러한 벤치마크는 코드 LLM에서 생성된 코드를 검증하기 위해 테스트 케이스를 사용하는 방대한 파이썬 프로그래밍 문제를 포함한다. HumanEval은 164개의 독창적인 프로그래밍 문제로 구성되어 있으며, 각 문제에 평균 9.6개의 테스트 케이스가 할당된다. LLM 합성 코드의 기능적 정확성에 대한 철저한 평가를 보장하기 위해 HumanEval+는 테스트 케이스 수를 크게 확장하여 문제당 평균 774.8개의 테스트 케이스로 확장한다. 반면에 MBPP는 500개의 테스트 프로그래밍 문제 세트를 제공하며 문제당 3개의 자동화된 테스트 케이스를 제공한다. 이러한 작업에 대한 프롬프트 형식은 다음과 같습니다.

아래는 태스크를 설명하는 명령어로, 추가 컨텍스트를 제공하는 입력과 쌍을 이룬다. 요청을 적절하게 완료하는 응답을 작성합니다.

### 명령: 이 문제에 대 한 Python 스크립트 만들기 {Question}

### Response:

그림 1: 단일 시도로 HumanEval(164개의 문제)에 대한 합격률의 백분율. 모든 기준 점수는 LLM-인간-벤치마크로부터 검색된다[39]. 우리의 _WizardCoder_ 는 탐욕스러운 디코딩으로 답변을 생성한다.

GPT4, Claude, Bard와 같은 코드 생성을 위한 SOTA LLM은 주로 폐쇄 소스 모델과 비교된다. 이러한 모델의 API에 대한 액세스 획득은 어려운 것으로 판명되었습니다. 본 연구에서는 LLM-Humananeval-Benchmark에서 HumanEval과 HumanEval+에 대한 점수를 검색하여 대안적인 접근법을 채택한다[39]. 특히, 언급된 모든 모델은 단일 시도를 사용하여 각 문제에 대한 코드 솔루션을 생성하고 결과 통과율 백분율이 보고된다. 일관성을 유지하기 위해 그리디 디코딩을 사용하여 답변을 생성하여 동일한 실험 설정을 사용하고 제공된 평가 코드를 사용하여 _위잔Loder_를 평가한다. 이러한 표준화된 절차를 준수함으로써 기존 벤치마크에 대한 모델의 공정하고 유사한 평가를 보장하는 것을 목표로 한다.

그림 1에서 볼 수 있듯이 우리의 _WizardCoder_는 Claude-Plus (59.8 대 53.0)를 능가하여 이 벤치마크에서 세 번째 위치에 도달합니다. (59.8 vs. 44.5). 특히, 우리 모델은 이러한 모델에 비해 훨씬 작은 크기를 나타낸다. 또한, 우리의 _WizandCoder_ 는 명령어 미세 조정을 거치는 다른 오픈 소스 LLM에 비해 현저한 우월성을 보여 상당한 성능 마진을 보여준다.

Open-Source 모델과 비교.표 1에서는 HumanEval 및 MBPP 벤치마크에서 _WizardCoder_ 를 다른 오픈 소스 모델과 종합적으로 비교 합니다. 그림 1에 제시된 결과와 달리 패스@1 점수를 추정하기 위해 각 문제에 대해 n개의 샘플을 생성하여 이전 연구 [31]에 요약된 접근법을 고수한다. 표 1에 제시된 결과는 우리의 _위저드코더_가 모든 오픈 소스 모델에 비해 상당한 성능 이점을 나타낸다는 것을 분명히 보여준다.

그림 1과 표 1의 실험 결과로부터 다음과 같은 결론을 얻을 수 있다:

1. _WizardCoder_ 는 훨씬 작음에도 불구하고 Claude, Bard, PaLM, PaLM-2 및 LaMDA를 포함한 가장 큰 폐쇄 소스 LLM을 능가합니다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **Params** & **HumanEval** & **MBPP** \\ \hline \multicolumn{4}{c}{Closed-source models} \\ \hline LaMDA [40] & 137B & 14.0 & - \\ AlphaCode [12] & 1.1B & 17.1 & - \\ PaLM [3] & 540B & 26.2 & 36.8 \\ PaLM-Coder [3] & 540B & 36.0 & 47.0 \\ PaLM 2-S [4] & - & 37.6 & 50.0 \\ Codex [16] & 2.5B & 21.4 & - \\ Codex [16] & 12B & 28.8 & - \\ Code-Cushman-001 [38] & - & 33.5 & 45.9 \\ Code-Davinci-002 [38] & - & 47.0 & 58.1 \\ GPT-3.5 [2] & - & 48.1 & - \\ GPT-4 [2] & - & 67.0 & - \\ \hline \multicolumn{4}{c}{Open-source models} \\ \hline LLaMa [8] & 33B & 21.7 & 30.2 \\ LLaMa [8] & 65B & 23.7 & 37.7 \\ CodeGen-Multi [13] & 16B & 18.3 & 20.9 \\ CodeGen-Mono [13] & 16B & 29.3 & 35.3 \\ CodeGeeX [14] & 13B & 22.9 & 24.4 \\ StarCoder [11] & 15B & 33.6 & 43.6\({}^{*}\) \\ CodeT5+ [18] & 16B & 30.9 & - \\ InstructCodeT5+ [18] & 16B & 35.0 & - \\ \hline _WizardCoder_ & 15B & **57.3** (+22.3) & **51.8** (+8.2) \\ \hline \hline \end{tabular}
\end{table}
표 1: HumanEval 및 MBPP에 대한 pass@1(%)의 결과. 대부분의 점수는 StarCoder[11]와 CodeT5+[18]의 논문에서 검색된다. 우리는 이전 작업 [31]을 따라 n개의 샘플을 생성하여 온대=0.2, top_p=0.95의 동일한 하이퍼 매개 변수 집합으로 pass@1 점수를 추정한다. *: 이 모델을 스스로 평가한다.

2. _WizardCoder_ 는 스타코더, CodeGen, CodeGee 및 CodeT5+를 포함 하 여 모든 오픈 소스 코드 LLM을 큰 마진 (HumanEval의 +22.3)으로 능가 합니다.
3. _WizardCoder_ 는 InstructCodeT5+, StarCoder-GPTeacher 및 Instruct-Codegen-16B를 포함 하 여 명령 미세 조정을 사용 하 여 모든 오픈 소스 코드 LLM을 크게 능가 합니다.

### DS-1000에 대한 평가

DS-1000 벤치마크 [34]는 7개의 라이브러리에 걸쳐 있는 1,000개의 별개의 데이터 과학 워크플로로 구성된다. 테스트 케이스에 대한 코드 생성의 성능을 평가하고 완료 및 삽입의 두 가지 평가 모드를 지원합니다. 실험에서는 지원하는 모델에 대한 삽입 점수만 보고한다. DS-1000 벤치마크는 Matplotlib (plt), NumPy (np), Pandas (pd), SciPy (scp), Scikit-Learn (sk), PyTorch (py), TensorFlow (tf)와 같은 라이브러리를 기반으로 문제를 더 분류한다. 스타코더와 동일한 프롬프트 형식을 따릅니다. 표 2에서는 전체 점수와 함께 각 라이브러리에 대한 pass@1(n=40) 결과를 제시한다. 이러한 결과를 바탕으로, 우리의 결론은 _WizardCoder_가 DS-1000 벤치마크에서 데이터 과학 문제를 다룰 때 다른 모든 모델에 비해 상당한 우위를 보여준다는 것입니다. 이 관찰은 거의 모든 데이터 과학 라이브러리에서 적용된다.

### Ablation Study

그림 2는 데이터 진화 라운드 수의 영향을 조사하는 절제 연구를 제시한다. 진화된 데이터의 첫 번째 라운드에는 38k개의 샘플이 포함되어 있다. 두 번째 라운드에는 58k가 들어 있습니다. 세 번째 라운드는 78k입니다. 네 번째 라운드는 98k입니다. 일관성을 위해 모든 모델은 200단계로 미세 조정을 거칩니다. 결과는 인간성에 대한 가장 높은 패스@1 점수가 3회의 데이터 진화 후에 달성된다는 것을 보여준다. 이 관찰을 기반으로 3라운드 동안 진화한 데이터를 최종 데이터 세트로 선택한다.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Format** & **Model** & **plt** & **np** & **pd** & **py** & **scp** & **sk** & **tf** & **All** \\ \hline  & \# of problems: & 155 & 220 & 291 & 68 & 106 & 115 & 45 & 1,000 \\ \hline Completion & InCoder-6B & 28.3 & 4.4 & 3.1 & 4.4 & 2.8 & 2.8 & 3.8 & 7.4 \\ Completion & CodeGen-mono & 31.7 & 10.9 & 3.4 & 7.0 & 9.0 & 10.8 & 15.2 & 11.7 \\ Completion & Code-Cushman-001 & 40.7 & 21.8 & 7.9 & 12.4 & 11.3 & 18.0 & 12.2 & 18.1 \\ Completion & StarCoder & 51.7 & 29.7 & 11.4 & 21.4 & 20.2 & **29.5** & 24.5 & 26.0 \\ Completion & _WizardCoder_ & **55.2** & **33.6** & **16.7** & **26.2** & **24.2** & 24.9 & **26.7** & **29.2** \\ \hline Insertion & InCoder-6B & 28.3 & 4.6 & 2.9 & 4.4 & 2.8 & 3.1 & 7.8 & 7.5 \\ Insertion & StarCoder & 51.7 & 30.8 & 10.3 & 21.0 & 20.2 & 27.4 & 20.0 & 25.4 \\ Insertion & _WizardCoder_ & **55.2** & **35.1** & **20.4** & **30.4** & **28.9** & **32.3** & **37.8** & **32.8** \\ \hline \hline \end{tabular}
\end{table}
표 2: DS-1000에서 _WizardCoder_ 및 베이스라인 모델의 성능. 모든 모델은 동일한 하이퍼 매개 변수 세트: 온도=0.2, top_p=0.5, max_length=1024로 평가됩니다. 점수는 40개 샘플에 대한 평균 pass@1 정확도입니다. Matplotlib(plt) 작업은 올바른 컨텍스트가 없으므로 삽입 및 완료 점수가 동일합니다.

그림 2: 데이터 진화 라운드의 수에 대한 절제 연구.

### Examples

표 3은 _WizardCoder_와의 상호 작용의 예를 보여줍니다. 예제는 우리의 모델이 명확한 설명과 함께 일관된 정확한 응답을 생성한다는 것을 보여준다.

## 5 결론 및 향후 작업

이 문서에서는 Code _Evol-Instruct_ fine-tuned Code LLM인 _WizardCoder_ 를 소개합니다. 실험 결과, 현재 널리 알려진 4개의 코드 생성 벤치마크인 HumanEval, HumanEval+, MBPP, DS-1000에서 WizardCoder가 기존의 모든 오픈소스 코드 LLMs를 능가하는 SOTA 성능을 보였으며, Anthropic의 Claude, Google의 Bard를 포함한 가장 큰 폐쇄형 LLMs에 비해 우수한 성능을 보였다.

Future Work. 우리의 _WizardCoder_는 그림 1과 같이 인상적인 코딩 성능을 보여주지만, 우리의 모델은 여전히 SOTA LLM인 GPT4에 크게 뒤처진다. 따라서 미래의 작업은 우리의 모델의 성능을 더욱 높이기 위해 코드 _Evol-Instruct_ 방법의 향상을 우선시할 것이다.

브로드너 임팩트. 다른 LLM과 유사하게 _위저드코더_ 는 비윤리적이거나 유해하거나 오판의 소지가 있는 정보를 생성할 수도 있습니다. 따라서 윤리적, 사회적 함의를 해결하기 위한 향후 연구가 필요하다.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Instruction** & **Response** \\ \hline \hline \multirow{7}{*}{Write a Python function to tell me what the date is today.} & Here’s a Python function that tells you what the date is today: \\  & import:dattime \\  & dart get_data(): \\  & now = datetime.datetime.now() \\  & return now.ntrfftime(’L1, L1 Kd., T1’) \\  & print(get_data()): \\ \cline{1-1}  & This function uses the datetime module to get the current date and time. It then uses the strftime() method to format the date as a string in the format "Day, Month Date, Year". Finally, it returns the formatted string. \\ \cline{1-1}  & You can call this function from your Python code to get the current date and time. \\ \hline \multirow{7}{*}{This code takes in a function f, the lower bound a, the upper bound b, and the number of subintervals n. It then calculates the width of each subinterval h, and uses the trapezoidal rule to approximate the integral of f between a and b. The result is returned as a float. \\ \cline{1-1}  & To use this function, you would call it with the function you want to integrate, the lower bound, the upper bound, and the number of subintervals you want to use. For example: \\ \cline{1-1}  & dart f(x): \\ \cline{1-1}  & return **2 \\ \cline{1-1}  & result = integrate(f. 0, 1, 1000) \\ \cline{1-1}  & This would integrate the function f(x) = x**2 between 0 and 1 using 1000 subintervals, and print the result. \\ \hline \hline \multirow{7}{*}{List the prime numbers between 20 and 30 with Java.} & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \hline \hline \end{tabular}
\end{table}
표 3: _WizardCoder_와의 상호 작용 예제입니다.

## References

* [1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [2] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023.
* [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Guor-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunjna Dev, Henry Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denmy Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Eric Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _CoRR_, abs/2204.02311, 2022.
* [4] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienberg, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. _CoRR_, abs/2305.10403, 2023.
* [5] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. _CoRR_, abs/2203.15556, 2022.
* [6] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Casser, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glasee, Johannes Welbl, Sumanth Dathathathri, Saffron Huang, Jonathan Desato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lepsiau, Maria Tsimpoukelli, Nikola Grigorev, Doug Fritz, Thibault Sotititauv, Manus Palayarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. _CoRR_, abs/2112.11446, 2021.
* [7] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. _CoRR_, abs/2210.02414, 2022.
* [8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, ArmandJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023.
* [9] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models. _CoRR_, abs/2205.01068, 2022.
* [10] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _NeurIPS_, 2022.
* [11] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023.
* [12] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. _CoRR_, abs/2203.07814, 2022.
* [13] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In _The Eleventh International Conference on Learning Representations_, 2023.
* [14] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. _CoRR_, abs/2303.17568, 2023.
* [15] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. _CoRR_, abs/2204.05999, 2022.
* [16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotos Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Piano, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. _CoRR_, abs/2107.03374, 2021.
* [17] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Coded5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Marie-Francine Moens, Xuuning Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, pages 8696-8708. Association for Computational Linguistics, 2021.
* [18] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Coded5+: Open code large language models for code understanding and generation. _CoRR_, abs/2305.07922, 2023.
* [19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020.
* [20] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.

* [21] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirca Suzgun, Xinyun Chen, Aakanska Chowdrey, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. _CoRR_, abs/2210.11416, 2022.
* [22] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jia Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [23] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshif Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jose Rozen, Abbeesh Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Techan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [24] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zeroprompt: Scaling prompt-based pretraining to 1, 000 tasks improves zero-shot generalization. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 4235-4252. Association for Computational Linguistics, 2022.
* [25] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. In Trevor Cohn, Yulan He, and Yang Liu, editors, _Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020_, volume EMNLP 2020 of _Findings of ACL_, pages 1896-1907. Association for Computational Linguistics, 2020.
* [26] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.
* [27] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* [28] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgtp quality, March 2023.
* [29] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_, 2023.
* [30] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca), 2023.
* [31] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotos Chantzis, Elizabeth Barnes, Ariel Herber-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. _CoRR_, abs/2107.03374, 2021.
* [32] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. _CoRR_, abs/2305.01210, 2023.

* [33] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. _CoRR_, abs/2108.07732, 2021.
* [34] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida I. Wang, and Tao Yu. DS-1000: A natural and reliable benchmark for data science code generation. _CoRR_, abs/2211.11501, 2022.
* [35] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model. _CoRR_, abs/2204.06745, 2022.
* [36] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax), May 2021.
* [37] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. _CoRR_, abs/2205.05131, 2022.
* [38] Microsoft. Azure openai service models. [https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models), 2023.
* [39] Lim humaneval benchmarks. [https://github.com/my-other-github-account/llm-humaneval-benchmarks](https://github.com/my-other-github-account/llm-humaneval-benchmarks), 2023.
* [40] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegal, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulse Doshi, Reneilito Delos Santos, Toqju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. _CoRR_, abs/2201.08239, 2022.

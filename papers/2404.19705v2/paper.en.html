<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\n' +
      '\n' +
      'Tiziano Labruna\n' +
      '\n' +
      'Corresponding Author. Email: tlabruna@fbk.euUniversity of Bozen-Bolzano\n' +
      '\n' +
      'Fondazione Bruno Kessler\n' +
      '\n' +
      'Cobere\n' +
      '\n' +
      'HiTZ Center - Ixa, University of the Basque Country UPV/EHU\n' +
      '\n' +
      'ORCID (Tiziano Labruna): [https://orcid.org/0000-0001-7713-7679](https://orcid.org/0000-0001-7713-7679), ORCID (Jon Ander Campos): [https://orcid.org/0000-0002-1447-5870](https://orcid.org/0000-0002-1447-5870), ORCID (Gorka Zakune): [https://orcid.org/0000-0002-2506-7426](https://orcid.org/0000-0002-2506-7426)\n' +
      '\n' +
      'Jon Ander Campos\n' +
      '\n' +
      'HiTZ Center - Ixa, University of the Basque Country UPV/EHU\n' +
      '\n' +
      'ORCID (Tiziano Labruna): [https://orcid.org/0000-0001-7713-7679](https://orcid.org/0000-0001-7713-7679), ORCID (Jon Ander Campos): [https://orcid.org/0000-0002-1447-5870](https://orcid.org/0000-0002-1447-5870), ORCID (Gorka Zakune): [https://orcid.org/0000-0002-2506-7426](https://orcid.org/0000-0002-2506-7426)\n' +
      '\n' +
      'Gorka Zakune\n' +
      '\n' +
      'HiTZ Center - Ixa, University of the Basque Country UPV/EHU\n' +
      '\n' +
      'ORCID (Tiziano Labruna): [https://orcid.org/0000-0001-7713-7679](https://orcid.org/0000-0001-7713-7679), ORCID (Jon Ander Campos): [https://orcid.org/0000-0002-1447-5870](https://orcid.org/0000-0002-1447-5870), ORCID (Gorka Zakune): [https://orcid.org/0000-0002-2506-7426](https://orcid.org/0000-0002-2506-7426)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question. Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM\'s parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, \\(\\langle\\text{RET}\\rangle\\), when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that Adapt-LLM is able to generate the \\(\\langle\\text{RET}\\rangle\\) token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The task of question answering (QA) remains a focal point in Natural Language Understanding research. There are many different datasets serving as benchmarks for evaluating QA models, such as Natural Questions (NQ) [18], SQuAD [25] or QuAC [7], just to mention a few. Nowadays, Large Language Models (LLMs) consistently outperform traditional methods on these benchmarks, showcasing remarkable performance.\n' +
      '\n' +
      'Typically, there are two primary approaches to utilize LLMs for question answering:\n' +
      '\n' +
      '(i) **Closed Book Question Answering**: This approach involves strategies like instruction tuning [32] or few-shot prompting [6] to enhance performance. Here, the LLM relies solely on its parametric memory to answer questions. However, these parametric memories have inherent limitations as they are based entirely on the training corpus, meaning for example that they could be outdated regarding events occurring after the training process.\n' +
      '\n' +
      '(ii) **Open Book Question Answering**: In this approach, the LLM is coupled with an Information Retriever (IR) system [13, 36]. By leveraging the IR system, the LLM can retrieve relevant context to supplement its understanding and provide more accurate answers.\n' +
      '\n' +
      'However, the research conducted by Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] sheds light on the complexity of question-answering strategies, challenging the notion that the optimal approach always involves the utilization of an IR system. Through the introduction of the PopQA dataset, comprising 14 thousand questions annotated with popularity scores, they demonstrated that while LLMs relying solely on their parametric memories excel in addressing high-popularity questions, the efficacy diminishes for low-popularity questions, where using IR becomes crucial.\n' +
      '\n' +
      'Their findings underscore the importance of a hybrid approach, where LLMs utilize parametric memory for high-popularity questions, but use an off-the-shelf IR system to retrieve relevant context to answer low-popularity questions. Central to their methodology is the establishment of a fixed popularity score threshold, which they use to decide whether an IR system has to be employed.\n' +
      '\n' +
      'In many cases, however, question answering datasets do not include popularity scores, so relying on such scores is not a generalizable approach. Motivated by this limitation, our study aims to address whether LLMs can autonomously determine when to employ an IR system for improved question answering. To investigate this, we conduct an evaluation of an LLM using an open-domain question answering dataset to identify the questions for which the LLM provides accurate responses and those where its answers are incorrect.\n' +
      '\n' +
      'Specifically, for questions where the LLM\'s response is incorrect, we annotate them with a special token, \\(\\langle\\text{RET}\\rangle\\), indicating the need for additional context. Subsequently, we utilize these annotations to construct a new dataset tailored for training purposes, where we teach an LLM to answer directly if it is confident about the answer or to require context it believes is useful for answering the question (see Figure 1). Our hypothesis is that through this training process, the LLM learns to use an IR system when it needs extra context to answera question, thus we name it Adapt-LLM.\n' +
      '\n' +
      'To validate our hypothesis, we conducted several experiments on the PopQA dataset [22], as it provides a suitable platform for benchmarking hybrid retrieval strategies. As a result of these experiments we find that:\n' +
      '\n' +
      '* Adapt-LLM consistently outperforms typical fixed strategies for question answering, such as (i) using the IR system for all questions and (ii) relying solely on the parametric memory of the LLM.\n' +
      '* Adapt-LLM demonstrates performance comparable to strategies that rely on popularity scores to determine when to use an IR system, even without utilizing any popularity score or similar metric. It\'s worth noting that popularity scores are a unique feature of the PopQA dataset, rendering them inapplicable to other open-domain question answering datasets.\n' +
      '* When Adapt-LLM decides to retrieve additional information, the results obtained with the context are significantly better than those without it. Similarly, when Adapt-LLM directly answers questions relying on its parametric memory, it achieves high accuracies. These observations indicate that the model effectively discerns when to retrieve information and when it can answer a question without further context.\n' +
      '* The primary bottleneck for the performance of Adapt-LLM lies in the IR system. Adapt-LLM achieves much higher performance with gold passages compared to passages retrieved by the IR system.\n' +
      '\n' +
      'Our findings underscore the significance of adaptive retrieval strategies in enhancing the performance of LLMs for question answering tasks. By training Adapt-LLM to dynamically determine when to retrieve additional context, we demonstrate the feasibility of teaching an LLM how to effectively leverage external information sources only when necessary.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Retrieval-Augmented Generation (RAG) [19] has shown improvements on a wide variety of NLP areas, such as question answering [17, 13, 31, 23], truthfulness [14, 21] and language modelling [12, 5, 26] among others. The ability to ground model generations on retrieved text chunks has also enabled smaller models to match the performance of larger ones [2]. Moreover, due to the extremely high cost of training LLMs, RAG has become the standard way to maintain them updated with new information, not having to re-train the models periodically to incorporate new facts [10].\n' +
      '\n' +
      'Even if augmenting LLMs with retrieval is an essential step for the current generation of LLMs [15, 27] it also comes with a cost. Traditional retrieval methods as TF-IDF or BM-25 [29] are only able to retrieve documents with keyword overlap and suffer from lexical gap [4]. In order to try to solve this issue, many pre-trained Transformer encoder based dense models have been proposed [9, 28, 17, 11]. Trained neural models have shown good performance over a variety of retrieval benchmarks but they still struggle in the zero-shot setup for new domains [33]. The quality of the retrieval engine is essential for retrieval-augmented models as this will set the upper bound of the model performance. Moreover, the usage of a retrieval engine, especially when the target document index is huge, can significantly increase the latency of the model and hurt real time applications user experience [3].\n' +
      '\n' +
      'On the other hand, as models keep scaling, the world knowledge encoded in their parameters does too [16]. Many previous efforts have shown that language models are able to memorize a significant amount of world knowledge and achieve competitive performance on tasks such as open-domain question answering when they just use their parametric knowledge for solving the task [20, 1, 34, 35].\n' +
      '\n' +
      'Motivated by all this, the adaptive approach has been proposed as a new solution [30, 22]. In this approach, if the solution to the task is encoded in the parameters of the model, the model will be directly used for generating a solution. Conversely, if the answer is not encoded in the knowledge of the model, the answer generation will be augmented with external knowledge.\n' +
      '\n' +
      'Recently, Schick et al. [30] proposed the Toolformer, a model that can self teach how and when to use external tools via simple API calls including a calculator, search engines, a calendar and so on. The self learning process is based on a synthetic text only corpus that is enriched by prompting an LLM. The LLM first adds inline API calls on top of the unsupervised corpus. These API calls are\n' +
      '\n' +
      'Figure 1: The inference process of Adapt-LLM step-by-step: given a question (step 1), an LLM decides (step 2) whether to answer the question directly (step 3) or to ask for additional contextual information, generating the special \\(\\langle\\)RET\\(\\rangle\\) token; for the later, an off-the-shelf IR system is used to retrieve relevant context (step 4), which is used alongside the question to prompt again the LLM for the final answer (step 5).\n' +
      '\n' +
      'then validated by evaluating whether the execution of the API calls is helpful for predicting the future tokens. This unsupervised method significantly boosts model performance in a variety of tasks when compared against non augmented LLMs, but it also makes the model over use tools. As an example, for the QA task the model uses the search engine 99.3% of the cases. On our work, we try to take advantage of the parametric knowledge of LLMs and just perform retrieval when needed. Adapt-LLM decreases the usage of IR down to 83.99% while improving performance over vanilla retrieval.\n' +
      '\n' +
      'More similar to our work, Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] propose a dataset and method for measuring when non-parametric information needs to be retrieved. They present the PopQA dataset that contains 14K questions about a set of entities with varying popularity. The popularity of an entity is measured by the page views of its Wikipedia page. In order to solve this QA task, they use a popularity score threshold calculated on the PopQA dataset. If the popularity score of an individual entity is below the threshold they perform a retrieval step. On the contrary, if the score is greater than the threshold they directly answer the question. This method yields better results than vanilla retrieval but it requires the calculation of a popularity score that is not available in realistic QA scenarios.\n' +
      '\n' +
      'Another relevant contribution in this field, contemporaneous with our research, is the work by Erbacher et al. [8], where they trained an LLM to determine when to utilize external knowledge. They particularly focused on finding the optimal trade-off between the risk of hallucination and the cost of information retrieval, given the potentially high expense associated with IR. Our Adapt-LLM method adopts a similar approach, training an LLM to learn when to retrieve information. However, we extend this by comparing our method\'s performance against some baselines, and assess the effectiveness of retrieving information in an adaptive manner against the strategies of never retrieving or always retrieving.1\n' +
      '\n' +
      'Footnote 1: All resources are publicly available at [https://github.com/tLabruna/Adapt-LLM](https://github.com/tLabruna/Adapt-LLM).\n' +
      '\n' +
      '## 3 Adaptive Retrieval LLM (Adapt-LLM)\n' +
      '\n' +
      'Adaptive retrieval refers to the model\'s capability to dynamically determine whether to retrieve additional context information for generating answers in question answering tasks. Unlike traditional models that either always incorporate context or never consider it, adaptive retrieval allows the model to selectively retrieve context based on the specific requirements of each question. This adaptive approach aims to optimize performance by leveraging context only when necessary, thereby enhancing the model\'s ability to generate accurate answers.\n' +
      '\n' +
      'As depicted in Figure 1, the process of the Adapt-LLM unfolds in the following sequence:\n' +
      '\n' +
      '1. The first prompt containing the question is sent to the model (step 1 of Figure 1).\n' +
      '2. The Adapt-LLM evaluates the prompt to determine whether additional context is necessary to answer the question effectively (step 2).\n' +
      '3. If the model determines that context is not required, it directly produces a response to the question by leveraging its parametric memory (step 3).\n' +
      '4. If context is deemed necessary, the Adapt-LLM model returns a special token, represented as \\(\\langle\\text{RET}\\rangle\\), and an off-the-shelf IR system is used to retrieve pertinent context based on the question (step 4); the context is then combined with the original question prompt to form a comprehensive representation for answer generation (step 5).\n' +
      '\n' +
      'The decision-making process of Adapt-LLM enables the model to determine the necessity of context for answering questions through dynamic assessment of each prompt. This flexible behavior allows the model to strike a balance between utilizing context for enhanced understanding and delivering direct answers when sufficient.\n' +
      '\n' +
      '### Training Adapt-LLM\n' +
      '\n' +
      'Here, we delineate the methodology employed to train our Adapt-LLM model. The process of crafting the training data, denoted as \\(DS_{Adapt}\\), is presented in Algorithm 1.\n' +
      '\n' +
      '```\n' +
      'Input: Q: questions, A: answers, P: passages, LLM Output:\\(DS_{Adapt}\\): A training dataset for Adaptive Retrieval\n' +
      '1\\(DS_{Adapt}\\) = init_empty() forq, gold_ans, pass in (Q, A, P)do\n' +
      '2 ans = LLM(q) ifans = gold_ans then\n' +
      '3ist = build_instance(\'parametric_prompt\', q, gold_ans) \\(DS_{Adapt}\\).add(inst)\n' +
      '4 end if\n' +
      '5else\n' +
      '6inst1 = build_instance(\'parametric_prompt\', q, "<RET>") \\(DS_{Adapt}\\).add(inst1)\n' +
      '7ist2 = build_instance(\'context_prompt\', q, gold_ans, pass) \\(DS_{Adapt}\\).add(inst2)\n' +
      '8 end if\n' +
      '9\n' +
      '10 end for\n' +
      '11\n' +
      '12 end for return \\(DS_{Adapt}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1**Training data creation\n' +
      '\n' +
      'We begin by selecting an open-domain question answering dataset containing questions \\(Q\\), associated context passages \\(P\\), and corresponding answers \\(A\\). We initialize \\(DS_{Adapt}\\) to an empty set (line 1 of the algorithm). For each question in \\(Q\\), we leverage the base LLM without any retrieval mechanism to perform a zero-shot inference (line 3). This step allows us to differentiate questions for which the model generates correct answers from those where its responses are inaccurate. This process can be understood as a way to discover what the base LLM _knows_ due to its parametric memory. For questions where the model\'s response is accurate (line 4), we build a training set instance incorporating the following prompt, which we call _parametric_prompt_:\n' +
      '\n' +
      '```\n' +
      'Input: Q: questions, A: answers, P: passages, LLM Output:\\(DS_{Adapt}\\): A training dataset for Adaptive Retrieval\n' +
      '1\\(DS_{Adapt}\\) = init_empty() forq, gold_ans, pass in (Q, A, P)do\n' +
      '2 ans = LLM(q) ifans = gold_ans then\n' +
      '3ist = build_instance(\'parametric_prompt\', q, gold_ans) \\(DS_{Adapt}\\).add(inst)\n' +
      '4 end if\n' +
      '5else\n' +
      '6inst1 = build_instance(\'parametric_prompt\', q, "<RET>") \\(DS_{Adapt}\\).add(inst1)\n' +
      '7ist2 = build_instance(\'context_prompt\', q, gold_ans, pass) \\(DS_{Adapt}\\).add(inst2)\n' +
      '8 end if\n' +
      '9\n' +
      '10 end for return \\(DS_{Adapt}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 2**Training data creation\n' +
      '\n' +
      'Alongside this prompt, we include the corresponding question from \\(Q\\) and the golden answer from \\(A\\), collectively forming the instance (line 5), which is subsequently appended to the \\(DS_{Adapt}\\) dataset (line 6).\n' +
      '\n' +
      'In contrast, if the LLM fails to produce a correct response to the question (line 8), we build two different instances. The first employsthe same _parametric_prompt_ as previously described, with \\(\\langle\\)RET\\(\\rangle\\) designated as the answer (line 9), indicating the necessity for additional context. The second prompt, termed _context_prompt_, encompasses contextual information alongside the question:\n' +
      '\n' +
      'Prompt: Answer the question Q given the context C. Q: {...}, C: {...}\n' +
      '\n' +
      'For this instance, we include the prompt, the question from \\(Q\\), the golden answer from \\(A\\), and the corresponding context passage from \\(P\\) (line 11).\n' +
      '\n' +
      'After populating the dataset with both types of prompts for questions where the LLM could not respond accurately and only the _parametric_prompt_ with golden answers for all other questions, our training set \\(D_{Adapt}\\) is prepared for the subsequent fine-tuning phase. The fine-tuning process entails training the base LLM on our dataset, resulting in the Adapt-LLM model.\n' +
      '\n' +
      'This approach ensures that the model effectively learns to discern when context is necessary for answering questions, or to provide a direct response when it suffices, as well as answer directly when provided with context.\n' +
      '\n' +
      '### Inference\n' +
      '\n' +
      'In the inference phase, we utilize the fine-tuned model to generate responses to unseen questions. We employ the same prompts used during the training phase, as outlined in Section 3.1.\n' +
      '\n' +
      'Initially, the model is prompted to either provide a direct response or return \\(\\langle\\)RET\\(\\rangle\\) if it is unsure of the answer. If the model returns \\(\\langle\\)RET\\(\\rangle\\), we proceed with information retrieval to acquire relevant context by means of an off-the-shelf IR system. Subsequently, we augment the question with the retrieved context and prompt the model again using the second type of prompt introduced during the training phase.\n' +
      '\n' +
      '## 4 Experiments and Results\n' +
      '\n' +
      'In this section, we outline the experimental framework aimed at assessing the performance of the proposed adaptive retrieval approach, Adapt-LLM. We begin by describing the datasets utilized (Section 4.1), followed by an overview of our base model (Section 4.2), the different configurations of the base model (Section 4.3), and the training details (Section 4.4). Subsequently, we introduce the three primary experiments:\n' +
      '\n' +
      '1. Evaluation of Adapt-LLM performance compared to the following baseline models: (i) an LLM that retrieves contextual information for all questions, and (ii) an LLM that exclusively relies on its parametric memory without using an IR system for any question (Section 4.5).\n' +
      '2. Analysis of Adapt-LLM\'s ability to determine when extra context is necessary to answer a question (Section 4.6).\n' +
      '3. Comparison with the state-of-the-art approach for PopQA (Section 4.7).\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'To ensure comprehensive training and evaluation of our models, we specifically selected three diverse question answering datasets. For training, we chose NQ [18] and SQuAD [25], as they are widely recognized datasets that assess factual knowledge and are based on Wikipedia. For evaluation, we opted for PopQA [22]. Below are brief descriptions of each dataset:\n' +
      '\n' +
      'NqThe Natural Questions dataset [18] is a collection of real-world questions derived from Google search queries, accompanied by long-form text passages obtained from Wikipedia articles and providing a diverse range of topics and natural language variations. We utilize this dataset for **training** our models in the experiments.\n' +
      '\n' +
      'SQuADThe Stanford Question Answering Dataset SQuAD [25] is a widely utilized dataset in the field of natural language processing and comprises questions posed by crowdworkers on a diverse range of Wikipedia articles, along with relevant paragraph passages serving as context. We utilize this dataset for **training** our models in the experiments.\n' +
      '\n' +
      'PopQAThe Popular Questions and Answers dataset [22] consists of curated questions sourced from various online platforms, encompassing a wide range of domains and styles. Given the variability in the effectiveness of context retrieval strategies observed in this dataset, we select PopQA as our test set to **evaluate** the language models\' performance in determining when context is necessary for accurate answer provision.\n' +
      '\n' +
      '### Base Model\n' +
      '\n' +
      'In our experiments, we employ Llama-2 [34] as our base LLM. Llama-2 is an open-source instruction-based LLM, which comes in versions of 7B, 13B, and 70B parameters. The model is pretrained on an expanded corpus sourced from publicly available online data sources. This corpus offers a 40% increase in size compared to its predecessor, contributing to the model\'s enhanced performance and capabilities.\n' +
      '\n' +
      'Additionally, Llama-2 features an extended context length, effectively doubling its capacity to process and comprehend longer sequences of text. These enhancements significantly improve the model\'s effectiveness across various natural language understanding tasks. Specifically, for our experiments, we utilize the Llama-2 model\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l l} \\hline \\hline \\multirow{2}{*}{**Training Set**} & **Model configuration** & **Accuracy** \\\\ \\hline \\multirow{3}{*}{NQ} & Never Retrieve & 21.43\\% \\\\  & Always Retrieve & 35.86\\% \\\\  & Adapt-LLM (ours) & **36.77\\%** \\\\ \\hline \\multirow{3}{*}{SQuAD} & Never Retrieve & 21.22\\% \\\\  & Always Retrieve & 36.59\\% \\\\ \\cline{1-1}  & Adapt-LLM (ours) & **38.15\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Performance comparison of Llama-2 models trained on the NQ and SQuAD datasets using different retrieval configurations (NR-LLM, AR-LLM, and Adapt-LLM), evaluated on the PopQA test set. Exact match accuracy is reported for all models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline  & **NQ** & **SQuAD** & **PopQA** \\\\ \\hline Questions & 58,880 & 87,599 & 14,282 \\\\ Words/question & 9.20 & 10.06 & 6.62 \\\\ Words/answer & 2.26 & 3.16 & 2.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Comparison of the three datasets we use for our experiments, i.e. SQuAD, NQ and PopQA. For each of them we provide the number of questions, and the average number of words per question and answer.\n' +
      '\n' +
      'with 7B parameters, leveraging its robust capabilities for our specific research objectives.\n' +
      '\n' +
      '### Model Configurations\n' +
      '\n' +
      'We conduct the experiments using three different model configurations, corresponding to the three different ways in which an LLM and an IR system can be combined:\n' +
      '\n' +
      '* **Adaptive Retrieval (Adapt-LLM)**. The Adapt-LLM model dynamically decides whether to retrieve context based on the question and its perceived need for contextual information, as explained in Section 3.1. As the IR system, we use Contriever [11], which is an unsupervised model pretrained on a large corpus, followed by fine-tuning on MS MARCO [24]. We only retrieve the most relevant passage according to the IR system to prompt the base LLM for the final answer.\n' +
      '* **Never-Retrieve (NR-LLM)**. This model configuration is trained to answer questions solely based on the question text without considering any contextual information. It serves as the baseline for evaluating the performance of question answering models in the absence of context.\n' +
      '* **Always-Retrieve (AR-LLM)**. In contrast to the NR-LLM model, this configuration always retrieves context passages to assist in answering questions. It is trained to utilize context consistently for generating answers. To ensure a fair comparison with Adapt-LLM, we also use Contriever [11] as the IR system and only retrieve the most relevant passage as context.\n' +
      '\n' +
      '### Training Details\n' +
      '\n' +
      'For all three model configurations (Adapt-LLM, AR-LLM and NR-LLM) and both training sets (SQuAD and NQ), we adhere to the parameter configuration established in Alpaca-Lora [32] which includes a batch size of 128, three epochs, and a fixed learning rate of 3e-4. We incorporated LoRA (Low-Rank Adaptation) regularization, with parameters configured for r=8, alpha=16, and a dropout rate of 0.05. Training was performed on an NVIDIA A40 GPU, for an average training time of approximately 8 hours. We do not perform any model selection and we use the last checkpoint after 3 epochs of training.\n' +
      '\n' +
      '### Validating the Adaptive Retrieval Approach\n' +
      '\n' +
      'In order to assess the effectiveness of our adaptive approach (Adapt-LLM) in comparison to the NR-LLM and AR-LLM configurations, we conducted fine-tuning of the Llama-2 model on both the NQ and SQuAD datasets across all three configurations. For the NR-LLM and AR-LLM configurations, we constructed training samples by extracting question-answer pairs from the datasets and incorporating corresponding instruction prompts.\n' +
      '\n' +
      'Specifically, prompts for the NR-LLM configuration instructed the model to answer questions without additional context, whereas prompts for the AR-LLM configuration included both the question and contextual information. In contrast, the Adapt-LLM training set was constructed following the approach outlined in Section 3.1, employing a two-step process. As a result of this process, the 74.72% of the questions in NQ are marked with the (RET) token, whereas the 87.49% questions are marked for SQuAD.\n' +
      '\n' +
      'The trained models were then tested on the PopQA dataset to evaluate their performance in a real-world question answering scenario. During inference, the NR-LLM and AR-LLM models were utilized as is, with corresponding instruction prompts provided, and outputs expected to be answers to the questions. Conversely, for the Adapt-LLM model, we followed the same prompt procedure as explained in Section 3.2.\n' +
      '\n' +
      'The generated answers are then compared to the set of possible answers for each question, which are already annotated in the PopQA test set. The evaluation metric used is **Exact Match Accuracy**, which measures the percentage of generated outputs that exactly match one of the possible answers for the corresponding question.\n' +
      '\n' +
      'Table 1 presents the results of this experiment, illustrating the performance of the Llama-2 model across the different configurations and datasets. Across both the NQ and SQuAD training datasets, the Adapt-LLM configuration consistently outperforms the Never Retrieve (NR-LLM) and Always Retrieve (AR-LLM) configurations on the PopQA test set. As can be observed, NR-LLM exhibits the lowest performance among the models, with an accuracy difference of approximately 14 absolute points compared to the other configurations. This disparity suggests that the parametric memory of Llama-2 alone is not sufficient for effectively answering PopQA questions.\n' +
      '\n' +
      'The differences between AR-LLM and Adapt-LLM are narrower. Specifically, the Adapt-LLM configuration achieves an accuracy of 36.77% and 38.15% on the PopQA test set when trained on the NQ and SQuAD datasets, respectively, compared to 35.86% and 36.59% for the AR-LLM configuration. Across both training datasets, Adapt-LLM outperforms AR-LLM, with the largest difference observed when trained on SQuAD.\n' +
      '\n' +
      'All in all, these results underscore the efficacy of the adaptive retrieval approach in dynamically determining the necessity of context for accurate question answering, resulting in improved performance compared to fixed strategies of always or never retrieving context.\n' +
      '\n' +
      'Although the disparity between training Adapt-LLM on NQ or SQuAD is relatively minor, we try to determine the suitability of a training set for a given evaluation set. While both training sets (NQ and SQuAD) and the evaluation set (PopQA) are based on Wikipedia, subtle differences may exist.\n' +
      '\n' +
      'Table 2 provides insights into the characteristics of the three datasets involved in our experimental procedure, including the total number of questions and the average number of words per ques\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '**Training** & \\(\\langle\\)**RET**\\(\\rangle\\)** & \\(\\langle\\)**RET**\\(\\rangle\\)** & \\multicolumn{2}{c}{**No \\(\\langle\\)RET**\\(\\rangle\\)**} \\\\  & & **Acc. w/ context** & **Acc. w/o context** & **Acc. w/ context** & **Acc. w/o context** \\\\ \\hline NQ & 82.26\\% & 33.04\\% & 14.65\\% & 55.72\\% & 62.36\\% \\\\ SQuAD & 83.93\\% & 33.40\\% & 9.94\\% & 57.73\\% & 62.92\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Results of the usage of the \\(\\langle\\)RET\\(\\rangle\\) token in the Adapt-LLM model. The first column shows the percentage of PopQA questions for which the model requests additional context. The second column focuses on the questions for which Adapt-LLM asks for context (\\(\\langle\\)RET\\(\\rangle\\)), comparing the performance between answering those questions with and without context. The last column (No \\(\\langle\\)RET\\(\\rangle\\)) is for questions which Adapt-LLM decides to answer directly. We also compare the performance with and without the context retrieved by the IR system.\n' +
      '\n' +
      'tion and answer. While NQ appears to be closer to PopQA in terms of question and answer lengths, the key factor influencing the better results of training Adapt-LLM on SQuAD may be the number of questions in the training dataset (\\(\\sim\\)87K in SQuAD and \\(\\sim\\)58K in NQ). Further analyses are required to elucidate the factors that render a training dataset more suitable for a given target dataset (which is beyond the scope of our study), but these results suggest that scale may play once again a crucial role.\n' +
      '\n' +
      '### Contextual Retrieval Decision Analysis\n' +
      '\n' +
      'In this experiment, our objective is to once again evaluate the effectiveness of the Adapt-LLM model, this time focusing on its ability to accurately determine when additional context is needed. For this purpose, we adhere to the following steps:\n' +
      '\n' +
      '1. We conduct inference on the Adapt-LLM model using the PopQA test set, prompting it to either return an answer directly or indicate the need for additional context by returning \\(\\langle\\)RET\\(\\rangle\\).\n' +
      '2. In the case of receiving a \\(\\langle\\)RET\\(\\rangle\\) response from the Adapt-LLM model, we proceed with the following steps: 1. We conduct inference on the Adapt-LLM model, prompting it to return an answer given the context obtained from the IR system. 2. We also conduct inference on the NR-LLM model with the instruction to provide an answer directly without additional context.\n' +
      '3. If the Adapt-LLM model decides to answer the question directly relying only on its parametric memory: 1. We conduct inference on the Adapt-LLM model, prompting it to return the answer without providing context. 2. We conduct inference on the AR-LLM model with the instruction to provide an answer using the context retrieved by the IR system.\n' +
      '\n' +
      'Table 3 presents the results of this experiment. The first thing to note is that the Adapt-LLM model generates the \\(\\langle\\)RET\\(\\rangle\\) token for approximately 82-83% of the questions in the PopQA dataset, with similar ratios observed across both training datasets. This observation aligns with the low performance of the NR-LLM configuration demonstrated in Table 1.\n' +
      '\n' +
      'However, Adapt-LLM consistently determines when additional context is required to answer a question accurately. Across both the NQ and SQuAD training datasets, Adapt-LLM exhibits significantly higher accuracy when retrieving context compared to the NR-LLM model\'s accuracy without context (as indicated in the \\(\\langle\\)RET\\(\\rangle\\) column of Table 3). Specifically, for the NQ dataset, the accuracy of the Adapt-LLM model when requesting context is 33.04%, whereas the accuracy of the NR-LLM model without context retrieval is notably lower at 14.65%. Similarly, for the SQuAD dataset, Adapt-LLM achieves an accuracy of 33.40% with context retrieval, whereas the NR-LLM model\'s accuracy without context is substantially lower at 9.94%.\n' +
      '\n' +
      'Finally, the last column of Table 3 (No \\(\\langle\\)RET\\(\\rangle\\)) shows the performance of Adapt-LLM when answering questions based solely on its parametric memory. As can be seen, accuracies above 62% are obtained when no context is utilized, providing further evidence that Adapt-LLM effectively discerns between retrieving context and providing direct answers to questions. Additionally, we evaluate the performance of these questions when context is added to the input, revealing significant decreases in accuracy of up to 7 absolute points.\n' +
      '\n' +
      'These findings provide insights into the effectiveness of the decision-making process employed by the Adapt-LLM model in determining the necessity of additional context for accurate response generation and present empirical evidence of the necessity of performing dynamic context retrieval in improving the accuracy of ques\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline \\multirow{2}{*}{**Passages**} & **SQuAD Dev** & **NQ Dev** \\\\  & **Acc.** & **Acc.** \\\\ \\hline Gold & **89.42\\%** & **69.76\\%** \\\\ Contirever & 22.49 & 27.04\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Performance comparison of Adapt-LLM for the SQuAD and NQ dev sets, when using the gold passages provided by the datasets and when using the best passage retrieved by Contriever.\n' +
      '\n' +
      'Figure 2: Histograms depicting the proportion of questions where Adapt-LLM trained on NQ (left) and Adapt-LLM trained on SQuAD (right) ask for extra context for different popularity score intervals.\n' +
      '\n' +
      'tion answering models.\n' +
      '\n' +
      'However, it is notable that the overall performance of the model when answering questions with retrieved context, as observed in Table 3 (approximately 33%), is relatively low. To further explore this observation, we conduct an additional experiment: evaluating Adapt-LLM (both versions trained on NQ and SQuAD) on the NQ and SQuAD development splits, comparing performance when using the gold passages of the dataset and the context retrieved by our IR system, Contriever [11]. Unfortunately, PopQA does not provide the gold passages, so direct evaluation there was not possible.\n' +
      '\n' +
      'Table 4 presents the results of this experiment. A significant performance difference is observed between using the gold passage and the top passage retrieved by Contriever for both datasets (approximately 67 absolute points for SQuAD and 42 for NQ). This indicates that Contriever, and current IR systems in general, do not consistently retrieve the most relevant passage to answer a given question. This observation underscores the importance of retrieving multiple documents as context, as seen in the most successful open-domain QA systems [13], and highlights its impact on the overall performance of Adapt-LLM in PopQA.\n' +
      '\n' +
      'To further validate the behavior of Adapt-LLM when requesting additional context, Figure 2 illustrates the proportion of questions for which our model generates the \\(\\langle\\text{RET}\\rangle\\) token, aggregated by popularity score intervals (left image for Adapt-LLM trained on NQ and right image for SQuAD). Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] suggest that high-popularity questions can be adequately answered using the parametric memory of the LLM, while lower popularity scores necessitate extra context. In Figure 2, we observe this pattern for both versions of Adapt-LLM, indicating that our model, despite lacking access to popularity scores during training or inference, has learned effective criteria for requesting additional context.\n' +
      '\n' +
      '### Comparison with state-of-the-art methods\n' +
      '\n' +
      'We conducted a comparative analysis between our Adapt-LLM model and the current state-of-the-art approach for PopQA proposed by Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22]. Their methodology relies on the popularity score annotated in the PopQA dataset to determine whether a question requires additional context. To establish the optimal threshold for determining question popularity, Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] split the PopQA dataset into 75% as a development set for threshold determination and 25% as a test set. In the original paper, they apply this methodology to various LLMs available at that moment (Llama-2 was not released yet).\n' +
      '\n' +
      'To ensure a fair comparison between Adapt-LLM and the popularity-based method, we replicated their approach using the Llama-2 TB model to determine the best popularity score threshold (found to be 707,000) using the same PopQA development set. This allowed us to obtain results consistent with their methodology while utilizing our base LLM. Similar to the original results in Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] when using smaller models, the popularity score threshold is almost equivalent to always retrieving contextual information for Llama-2 7B. The IR usage is of 99.86% as presented in Table 5. This clearly shows how the popularity score method struggles with smaller size models, being GPT-3 davinci-003 the only model to get a IR usage below 80% in the original paper when using adaptive retrieval with the Contriever. Subsequently, we evaluated our Adapt-LLM configuration on the same 25% test set split and compared the outcomes with those obtained using the method described by Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] with the Llama-2 LLM as the base model.\n' +
      '\n' +
      'The results of this experiment are presented in Table 5. We observe comparable performance between the replicated approach of Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] and Adapt-LLM when trained on NQ and SQuAD datasets and tested on the 25% subset of PopQA. It\'s worth mentioning that Adapt-LLM does not utilize any information from PopQA, unlike Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22], who directly use the popularity score and a 75% portion of PopQA dataset to find an optimal value for that popularity score. This methodology is not generalizable to other open-domain question answering tasks since the popularity score is a unique feature of PopQA. However, Adapt-LLM can be applied to any similar dataset. Given these characteristics, we believe that the results obtained by Adapt-LLM are even more significant, offering comparable performance to an approach that utilizes dataset-specific information. These findings substantiate the validity of our approach, demonstrating its effectiveness even when trained on datasets different from the one used for testing.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      'In this paper, we introduce Adapt-LLM, a LLM which learns to discern when additional context is necessary for answering a question, rather than relying solely on its parametric memory. Adapt-LLM is the result of fine-tuning a base LLM on an open-domain question answering dataset that has been modified to differentiate between questions answerable with the LLM\'s parametric memory alone and those requiring supplementary context. To construct these training datasets, we initially subject the base LLM to zero-shot evaluation to determine its accuracy in answering questions. For questions where the model\'s response is incorrect, we train the LLM to generate a special token, \\(\\langle\\text{RET}\\rangle\\), indicating the need for additional context.\n' +
      '\n' +
      'Through extensive experiments conducted on the PopQA dataset, we show that Adapt-LLM performs better than its two fixed alternatives: never retrieving and always retrieving relevant context information. Furthermore, our findings highlight Adapt-LLM\'s capability to effectively discern the necessity of additional context, which is the primary objective of this work.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline\n' +
      '**Model Configuration** & **IR usage** & **Accuracy** \\\\ \\hline Popularity Score & 99.86\\% & 36.81\\% \\\\ Adapt-LLM (NQ) & 87.22\\% & 35.30\\% \\\\ Adapt-LLM (SQuAD) & 83.99\\% & **37.29\\%** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Performance comparison of Llama-2 base models trained on the SQuAD and NQ datasets for the Adapt-LLM and Popularity Score configurations. The later mimics the methodology proposed by Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [22] with the Llama-2 LLM as the base model.\n' +
      '\n' +
      'For future investigations, we propose exploring methods to enhance performance when utilizing an IR system, such as incorporating learnable sequential retrieval techniques. Furthermore, we believe it would be valuable to conduct a more in-depth analysis of the interaction between training and testing datasets in the development of Adapt-LLM systems.\n' +
      '\n' +
      '## 6 Acknowledgments\n' +
      '\n' +
      'This work received partial support from the Basque Government through research group funding IT1805-22 and the ICL4LANG project (grant no. KK-2023/00094). Additionally, we acknowledge the support of several MCIN/AEI/10.13039/501100011033 projects: (i) DeepKnowledge (PID2021-127777/DB-C21) and funding from FEDER, EU; (ii) AWARE (TED2021-131617B-I00) and support from the European Union NextGenerationEU/PRTR. We express our gratitude to Carlos Dominguez for his assistance in the experimental setup and to Eneko Agirre for his valuable feedback and guidance.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.\n' +
      '* [2]A. Catav and R. Miara and I. Giloh and N. Cordeiro and A. Ingber (2024) RAG makes LLMs better and equal. External Links: Link Cited by: SS1.\n' +
      '* [3]S. Barnett, S. Kurimwan, S. Thudumu, Z. Brannelly, and M. Abdel-razek (2024) Seven failure points when engineering a retrieval augmented generation system. arXiv preprint arXiv:2401.05856. Cited by: SS1.\n' +
      '* [4]A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mittal (2000) Bridging the lexical chasm: statistical approaches to answer-finding. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 192-199. Cited by: SS1.\n' +
      '* [5]B. Bogaud, S. Mensch, A. Hoffmann, J. Con, C. K. M. S.\n' +
      '\n' +
      'the Association for Computational Linguistics, ACL 2022_, pages 1487-1492. Association for Computational Linguistics, 2022.\n' +
      '* [32] Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B. Stanford alpaca: an instruction-following llama model (2023). _URL https://github. com/tatsu-lab/stanford_alpaca_, 2023.\n' +
      '* [33] N. Thakur, N. Reimers, A. Ruckle, A. Srivastava, and I. Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.\n' +
      '* [34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [35] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [36] F. Zhu, W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua. Retrieving and reading: A comprehensive survey on open-domain question answering. _arXiv preprint arXiv:2101.00774_, 2021.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
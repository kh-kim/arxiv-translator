<html lang="en" data-theme="dark"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</title>
<!--Generated on Mon May  6 19:07:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.19705v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.19705v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="Dark mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.19705v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.19705v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S2" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Adaptive Retrieval LLM (<span class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="In 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Training <span class="ltx_text ltx_font_smallcaps">Adapt-LLM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS2" title="In 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments and Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1.SSS0.Px1" title="In 4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title">NQ</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1.SSS0.Px2" title="In 4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title">SQuAD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1.SSS0.Px3" title="In 4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title">PopQA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS2" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Base Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS3" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Model Configurations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS4" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS5" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Validating the Adaptive Retrieval Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS6" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Contextual Retrieval Decision Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS7" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Comparison with state-of-the-art methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S5" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S6" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert" onclick="closePopup()">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: spverbatim</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2404.19705v2 [cs.CL] 06 May 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiziano&nbsp;Labruna<span class="ltx_ERROR undefined" id="id5.1.id1">\orcid</span>0000-0001-7713-7679
</span><span class="ltx_author_notes">Corresponding Author. Email: tlabruna@fbk.eu.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jon Ander&nbsp;Campos<span class="ltx_ERROR undefined" id="id6.1.id1">\orcid</span>0000-0002-1447-5870
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gorka&nbsp;Azkune<span class="ltx_ERROR undefined" id="id7.1.id1">\orcid</span>0000-0002-2506-7426
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Bozen-Bolzano
</span>
<span class="ltx_contact ltx_role_address">Fondazione Bruno Kessler
</span>
<span class="ltx_contact ltx_role_address">Cohere
</span>
<span class="ltx_contact ltx_role_address">HiTZ Center - Ixa, University of the Basque Country UPV/EHU
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.4">In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question.
Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM’s parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, <math alttext="\langle" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" stretchy="false" xref="id1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" stretchy="false" xref="id2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">⟩</annotation></semantics></math>, when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (<span class="ltx_text ltx_font_smallcaps" id="id4.4.1">Adapt-LLM</span>) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that <span class="ltx_text ltx_font_smallcaps" id="id4.4.2">Adapt-LLM</span> is able to generate the <math alttext="\langle" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" stretchy="false" xref="id3.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" stretchy="false" xref="id4.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><ci id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">⟩</annotation></semantics></math> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\paperid</span>
<p class="ltx_p" id="p1.2">123</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The task of question answering (QA) remains a focal point in Natural Language Understanding research. There are many different datasets serving as benchmarks for evaluating QA models, such as Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib18" title="">18</a>]</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib25" title="">25</a>]</cite> or QuAC <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib7" title="">7</a>]</cite>, just to mention a few. Nowadays, Large Language Models (LLMs) consistently outperform traditional methods on these benchmarks, showcasing remarkable performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Typically, there are two primary approaches to utilize LLMs for question answering:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">(i) <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Closed Book Question Answering</span>: This approach involves strategies like instruction tuning <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib32" title="">32</a>]</cite> or few-shot prompting <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib6" title="">6</a>]</cite> to enhance performance. Here, the LLM relies solely on its parametric memory to answer questions. However, these parametric memories have inherent limitations as they are based entirely on the training corpus, meaning for example that they could be outdated regarding events occurring after the training process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">(ii) <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Open Book Question Answering</span>: In this approach, the LLM is coupled with an Information Retriever (IR) system <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib36" title="">36</a>]</cite>. By leveraging the IR system, the LLM can retrieve relevant context to supplement its understanding and provide more accurate answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">However, the research conducted by <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> sheds light on the complexity of question-answering strategies, challenging the notion that the optimal approach always involves the utilization of an IR system. Through the introduction of the PopQA dataset, comprising 14 thousand questions annotated with popularity scores, they demonstrated that while LLMs relying solely on their parametric memories excel in addressing high-popularity questions, the efficacy diminishes for low-popularity questions, where using IR becomes curcial.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.g1" src="x1.png" width="747">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The inference process of <span class="ltx_text ltx_font_smallcaps" id="S1.F1.6.1">Adapt-LLM</span> step-by-step: given a question (step 1), an LLM decides (step 2) whether to answer the question directly (step 3) or to ask for additional contextual information, generating the special <math alttext="\langle" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mo id="S1.F1.3.m1.1.1" stretchy="false" xref="S1.F1.3.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><mo id="S1.F1.4.m2.1.1" stretchy="false" xref="S1.F1.4.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m2.1e">⟩</annotation></semantics></math> token; for the later, an off-the-shelf IR system is used to retrieve relevant context (step 4), which is used alongside the question to prompt again the LLM for the final answer (step 5).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Their findings underscore the importance of a hybrid approach, where LLMs utilize parametric memory for high-popularity questions, but use an off-the-shelf IR system to retrieve relevant context to answer low-popularity questions. Central to their methodology is the establishment of a fixed popularity score threshold, which they use to decide whether an IR system has to be employed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In many cases, however, question answering datasets do not include popularity scores, so relying on such scores is not a generalizable approach. Motivated by this limitation, our study aims to address whether LLMs can autonomously determine when to employ an IR system for improved question answering. To investigate this, we conduct an evaluation of an LLM using an open-domain question answering dataset to identify the questions for which the LLM provides accurate responses and those where its answers are incorrect.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.2">Specifically, for questions where the LLM’s response is incorrect, we annotate them with a special token, <math alttext="\langle" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" stretchy="false" xref="S1.p8.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S1.p8.2.m2.1"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" stretchy="false" xref="S1.p8.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><ci id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S1.p8.2.m2.1d">⟩</annotation></semantics></math>, indicating the need for additional context. Subsequently, we utilize these annotations to construct a new dataset tailored for training purposes, where we teach an LLM to answer directly if it is confident about the answer or to require context it believes is useful for answering the question (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>). Our hypothesis is that through this training process, the LLM learns to use an IR system when it needs extra context to answer a question, thus we name it <span class="ltx_text ltx_font_smallcaps" id="S1.p8.2.1">Adapt-LLM</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">To validate our hypothesis, we conducted several experiments on the PopQA dataset <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>, as it provides a suitable platform for benchmarking hybrid retrieval strategies.
As a result of these experiments we find that:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p10">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">Adapt-LLM</span> consistently outperforms typical fixed strategies for question answering, such as (i) using the IR system for all questions and (ii) relying solely on the parametric memory of the LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">Adapt-LLM</span> demonstrates performance comparable to strategies that rely on popularity scores to determine when to use an IR system, even without utilizing any popularity score or similar metric. It’s worth noting that popularity scores are a unique feature of the PopQA dataset, rendering them inapplicable to other open-domain question answering datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">When <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.1">Adapt-LLM</span> decides to retrieve additional information, the results obtained with the context are significantly better than those without it. Similarly, when <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.2">Adapt-LLM</span> directly answers questions relying on its parametric memory, it achieves high accuracies. These observations indicate that the model effectively discerns when to retrieve information and when it can answer a question without further context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">The primary bottleneck for the performance of <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i4.p1.1.1">Adapt-LLM</span> lies in the IR system. <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i4.p1.1.2">Adapt-LLM</span> achieves much higher performance with gold passages compared to passages retrieved by the IR system.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">Our findings underscore the significance of adaptive retrieval strategies in enhancing the performance of LLMs for question answering tasks. By training <span class="ltx_text ltx_font_smallcaps" id="S1.p11.1.1">Adapt-LLM</span> to dynamically determine when to retrieve additional context, we demonstrate the feasibility of teaching an LLM how to effectively leverage external information sources only when necessary.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib19" title="">19</a>]</cite> has shown improvements on a wide variety of NLP areas, such as question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib23" title="">23</a>]</cite>, truthfulness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib21" title="">21</a>]</cite> and language modelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib26" title="">26</a>]</cite> among others. The ability to ground model generations on retrieved text chunks has also enabled smaller models to match the performance of larger ones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib2" title="">2</a>]</cite>. Moreover, due to the extremely high cost of training LLMs, RAG has become the standard way to maintain them updated with new information, not having to re-train the models periodically to incorporate new facts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib10" title="">10</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Even if augmenting LLMs with retrieval is an essential step for the current generation of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib27" title="">27</a>]</cite> it also comes with a cost. Traditional retrieval methods as TF-IDF or BM-25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib29" title="">29</a>]</cite> are only able to retrieve documents with keyword overlap and suffer from lexical gap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib4" title="">4</a>]</cite>. In order to try to solve this issue, many pre-trained Transformer encoder based dense models have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>. Trained neural models have shown good performance over a variety of retrieval benchmarks but they still struggle in the zero-shot setup for new domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib33" title="">33</a>]</cite>. The quality of the retrieval engine is essential for retrieval-augmented models as this will set the upper bound of the model performance. Moreover, the usage of a retrieval engine, especially when the target document index is huge, can significantly increase the latency of the model and hurt real time applications user experience <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib3" title="">3</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">On the other hand, as models keep scaling, the world knowledge encoded in their parameters does too <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib16" title="">16</a>]</cite>. Many previous efforts have shown that language models are able to memorize a significant amount of world knowledge and achieve competitive performance on tasks such as open-domain question answering when they just use their parametric knowledge for solving the task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib35" title="">35</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Motivated by all this, the adaptive approach has been proposed as a new solution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>. In this approach, if the solution to the task is encoded in the parameters of the model, the model will be directly used for generating a solution. Conversely, if the answer is not encoded in the knowledge of the model, the answer generation will be augmented with external knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Recently, <cite class="ltx_cite ltx_citemacro_citet">Schick et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib30" title="">30</a>]</cite> proposed the Toolformer, a model that can self teach how and when to use external tools via simple API calls including a calculator, search engines, a calendar and so on. The self learning process is based on a synthetic text only corpus that is enriched by prompting an LLM. The LLM first adds inline API calls on top of the unsupervised corpus. These API calls are then validated by evaluating whether the execution of the API calls is helpful for predicting the future tokens. This unsupervised method significantly boosts model performance in a variety of tasks when compared against non augmented LLMs, but it also makes the model over use tools. As an example, for the QA task the model uses the search engine 99.3% of the cases. On our work, we try to take advantage of the parametric knowledge of LLMs and just perform retrieval when needed. <span class="ltx_text ltx_font_smallcaps" id="S2.p5.1.1">Adapt-LLM</span> decreases the usage of IR down to 83.99% while improving performance over vanilla retrieval.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">More similar to our work, <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> propose a dataset and method for measuring when non-parametric information needs to be retrieved. They present the PopQA dataset that contains 14K questions about a set of entities with varying popularity. The popularity of an entity is measured by the page views of its Wikipedia page. In order to solve this QA task, they use a popularity score threshold calculated on the PopQA dataset. If the popularity score of an individual entity is below the threshold they perform a retrieval step. On the contrary, if the score is greater than the threshold they directly answer the question. This method yields better results than vanilla retrieval but it requires the calculation of a popularity score that is not available in realistic QA scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">Another relevant contribution in this field, contemporaneous with our research, is the work by <cite class="ltx_cite ltx_citemacro_citet">Erbacher et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib8" title="">8</a>]</cite>, where they trained an LLM to determine when to utilize external knowledge. They particularly focused on finding the optimal trade-off between the risk of hallucination and the cost of information retrieval, given the potentially high expense associated with IR.
Our <span class="ltx_text ltx_font_smallcaps" id="S2.p7.1.1">Adapt-LLM</span> method adopts a similar approach, training an LLM to learn when to retrieve information. However, we extend this by comparing our method’s performance against some baselines, and assess the effectiveness of retrieving information in an adaptive manner against the strategies of never retrieving or always retrieving.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>All resources are publicly available at https://github.com/tLabruna/Adapt-LLM.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Adaptive Retrieval LLM (<span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Adapt-LLM</span>)</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Adaptive retrieval refers to the model’s capability to dynamically determine whether to retrieve additional context information for generating answers in question answering tasks. Unlike traditional models that either always incorporate context or never consider it, adaptive retrieval allows the model to selectively retrieve context based on the specific requirements of each question. This adaptive approach aims to optimize performance by leveraging context only when necessary, thereby enhancing the model’s ability to generate accurate answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>, the process of the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.1">Adapt-LLM</span> unfolds in the following sequence:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p3">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The first prompt containing the question is sent to the model (step 1 of Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.p1.1.1">Adapt-LLM</span> evaluates the prompt to determine whether additional context is necessary to answer the question effectively (step 2).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">If the model determines that context is not required, it directly produces a response to the question by leveraging its parametric memory (step 3).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.2">If context is deemed necessary, the <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.2.1">Adapt-LLM</span> model returns a special token, represented as <math alttext="\langle" class="ltx_Math" display="inline" id="S3.I1.i4.p1.1.m1.1"><semantics id="S3.I1.i4.p1.1.m1.1a"><mo id="S3.I1.i4.p1.1.m1.1.1" stretchy="false" xref="S3.I1.i4.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><ci id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.I1.i4.p1.2.m2.1"><semantics id="S3.I1.i4.p1.2.m2.1a"><mo id="S3.I1.i4.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i4.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><ci id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.2.m2.1d">⟩</annotation></semantics></math>, and an off-the-shelf IR system is used to retrieve pertinent context based on the question (step 4); the context is then combined with the original question prompt to form a comprehensive representation for answer generation (step 5).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The decision-making process of <span class="ltx_text ltx_font_smallcaps" id="S3.p4.1.1">Adapt-LLM</span> enables the model to determine the necessity of context for answering questions through dynamic assessment of each prompt. This flexible behavior allows the model to strike a balance between utilizing context for enhanced understanding and delivering direct answers when sufficient.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.1.1">Adapt-LLM</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Here, we delineate the methodology employed to train our <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">Adapt-LLM</span> model. The process of crafting the training data, denoted as <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">D</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1b" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.5" xref="S3.SS1.p1.1.m1.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1c" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.6" xref="S3.SS1.p1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝑆</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.6.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, is presented in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#algorithm1" title="In 3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">We begin by selecting an open-domain question answering dataset containing questions <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_Q</annotation></semantics></math>, associated context passages <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_P</annotation></semantics></math>, and corresponding answers <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_A</annotation></semantics></math>. We initialize <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">D</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.4" xref="S3.SS1.p2.4.m4.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1b" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.5" xref="S3.SS1.p2.4.m4.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1c" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.6" xref="S3.SS1.p2.4.m4.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐷</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">𝑆</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.5.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.6.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to an empty set (line 1 of the algorithm). For each question in <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_Q</annotation></semantics></math>, we leverage the base LLM without any retrieval mechanism to perform a zero-shot inference (line 3). This step allows us to differentiate questions for which the model generates correct answers from those where its responses are inaccurate. This process can be understood as a way to discover what the base LLM <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.1">knows</span> due to its parametric memory. For questions where the model’s response is accurate (line 4), we build a training set instance incorporating the following prompt, which we call <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.2">parametric_prompt</span>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.6">
<div class="ltx_listingline" id="algorithm1.6.7">
<span class="ltx_text" id="algorithm1.6.7.1"><span class="ltx_text ltx_font_bold" id="algorithm1.6.7.1.1">Input:</span> </span>Q: questions, A: answers, P: passages, LLM
</div>
<div class="ltx_listingline" id="algorithm1.1.1"> <span class="ltx_text" id="algorithm1.1.1.1"><span class="ltx_text ltx_font_bold" id="algorithm1.1.1.1.1">Output:</span> </span><math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.1.1.m1.1"><semantics id="algorithm1.1.1.m1.1a"><mrow id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml"><mi id="algorithm1.1.1.m1.1.1.2" xref="algorithm1.1.1.m1.1.1.2.cmml">D</mi><mo id="algorithm1.1.1.m1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.1.1.m1.1.1.3" xref="algorithm1.1.1.m1.1.1.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.2" xref="algorithm1.1.1.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.1.1.m1.1.1.3.3" xref="algorithm1.1.1.m1.1.1.3.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.3.2" xref="algorithm1.1.1.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.3" xref="algorithm1.1.1.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1a" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.4" xref="algorithm1.1.1.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1b" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.5" xref="algorithm1.1.1.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1c" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.6" xref="algorithm1.1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1"><times id="algorithm1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1"></times><ci id="algorithm1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.2">𝐷</ci><apply id="algorithm1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.1.1.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3">subscript</csymbol><ci id="algorithm1.1.1.m1.1.1.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.1.1.m1.1.1.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3"><times id="algorithm1.1.1.m1.1.1.3.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3.3.1"></times><ci id="algorithm1.1.1.m1.1.1.3.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.1.1.m1.1.1.3.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.1.1.m1.1.1.3.3.4.cmml" xref="algorithm1.1.1.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.1.1.m1.1.1.3.3.5.cmml" xref="algorithm1.1.1.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.1.1.m1.1.1.3.3.6.cmml" xref="algorithm1.1.1.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.1.1.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>: A training dataset for Adaptive Retrieval
</div>
<div class="ltx_listingline" id="algorithm1.2.2">
<span class="ltx_tag ltx_tag_listingline">1</span>
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.2.2.m1.1"><semantics id="algorithm1.2.2.m1.1a"><mrow id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml"><mi id="algorithm1.2.2.m1.1.1.2" xref="algorithm1.2.2.m1.1.1.2.cmml">D</mi><mo id="algorithm1.2.2.m1.1.1.1" xref="algorithm1.2.2.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.2.2.m1.1.1.3" xref="algorithm1.2.2.m1.1.1.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.2" xref="algorithm1.2.2.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.2.2.m1.1.1.3.3" xref="algorithm1.2.2.m1.1.1.3.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.3.2" xref="algorithm1.2.2.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.3" xref="algorithm1.2.2.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1a" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.4" xref="algorithm1.2.2.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1b" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.5" xref="algorithm1.2.2.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1c" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.6" xref="algorithm1.2.2.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><apply id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1"><times id="algorithm1.2.2.m1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1"></times><ci id="algorithm1.2.2.m1.1.1.2.cmml" xref="algorithm1.2.2.m1.1.1.2">𝐷</ci><apply id="algorithm1.2.2.m1.1.1.3.cmml" xref="algorithm1.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.1.1.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3">subscript</csymbol><ci id="algorithm1.2.2.m1.1.1.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.2.2.m1.1.1.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3"><times id="algorithm1.2.2.m1.1.1.3.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3.3.1"></times><ci id="algorithm1.2.2.m1.1.1.3.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.2.2.m1.1.1.3.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.2.2.m1.1.1.3.3.4.cmml" xref="algorithm1.2.2.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.2.2.m1.1.1.3.3.5.cmml" xref="algorithm1.2.2.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.2.2.m1.1.1.3.3.6.cmml" xref="algorithm1.2.2.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.2.2.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math> = init_empty()
</div>
<div class="ltx_listingline" id="algorithm1.6.8">
<span class="ltx_tag ltx_tag_listingline">2</span>
<span class="ltx_text ltx_font_bold" id="algorithm1.6.8.1">for</span>&nbsp;<em class="ltx_emph ltx_font_italic" id="algorithm1.6.8.2">q, gold_ans, pass in (Q, A, P)</em>&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.8.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.6.9">
<span class="ltx_tag ltx_tag_listingline">3</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
ans = LLM(q)

</div>
<div class="ltx_listingline" id="algorithm1.6.10">
<span class="ltx_tag ltx_tag_listingline">4</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.10.1">if</span>&nbsp;<em class="ltx_emph ltx_font_italic" id="algorithm1.6.10.2">ans = gold_ans</em>&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.10.3">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.6.11">
<span class="ltx_tag ltx_tag_listingline">5</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst = build_instance(’parametric_prompt’, q, gold_ans)
</div>
<div class="ltx_listingline" id="algorithm1.3.3">
<span class="ltx_tag ltx_tag_listingline">6</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.3.3.m1.1"><semantics id="algorithm1.3.3.m1.1a"><mrow id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml"><mi id="algorithm1.3.3.m1.1.1.2" xref="algorithm1.3.3.m1.1.1.2.cmml">D</mi><mo id="algorithm1.3.3.m1.1.1.1" xref="algorithm1.3.3.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.3.3.m1.1.1.3" xref="algorithm1.3.3.m1.1.1.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.2" xref="algorithm1.3.3.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.3.3.m1.1.1.3.3" xref="algorithm1.3.3.m1.1.1.3.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.3.2" xref="algorithm1.3.3.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.3" xref="algorithm1.3.3.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1a" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.4" xref="algorithm1.3.3.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1b" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.5" xref="algorithm1.3.3.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1c" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.6" xref="algorithm1.3.3.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1"><times id="algorithm1.3.3.m1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1.1"></times><ci id="algorithm1.3.3.m1.1.1.2.cmml" xref="algorithm1.3.3.m1.1.1.2">𝐷</ci><apply id="algorithm1.3.3.m1.1.1.3.cmml" xref="algorithm1.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.1.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3">subscript</csymbol><ci id="algorithm1.3.3.m1.1.1.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.3.3.m1.1.1.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3"><times id="algorithm1.3.3.m1.1.1.3.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3.3.1"></times><ci id="algorithm1.3.3.m1.1.1.3.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.3.3.m1.1.1.3.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.3.3.m1.1.1.3.3.4.cmml" xref="algorithm1.3.3.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.3.3.m1.1.1.3.3.5.cmml" xref="algorithm1.3.3.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.3.3.m1.1.1.3.3.6.cmml" xref="algorithm1.3.3.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.3.3.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.add(inst)

</div>
<div class="ltx_listingline" id="algorithm1.6.12">
<span class="ltx_tag ltx_tag_listingline">7</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div class="ltx_listingline" id="algorithm1.6.13">
<span class="ltx_tag ltx_tag_listingline">8</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div class="ltx_listingline" id="algorithm1.6.14">
<span class="ltx_tag ltx_tag_listingline">9</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.14.1">else</span>
</div>
<div class="ltx_listingline" id="algorithm1.6.15">
<span class="ltx_tag ltx_tag_listingline">10</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst1 = build_instance(’parametric_prompt’, q, "&lt;RET&gt;")
</div>
<div class="ltx_listingline" id="algorithm1.4.4">
<span class="ltx_tag ltx_tag_listingline">11</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.4.4.m1.1"><semantics id="algorithm1.4.4.m1.1a"><mrow id="algorithm1.4.4.m1.1.1" xref="algorithm1.4.4.m1.1.1.cmml"><mi id="algorithm1.4.4.m1.1.1.2" xref="algorithm1.4.4.m1.1.1.2.cmml">D</mi><mo id="algorithm1.4.4.m1.1.1.1" xref="algorithm1.4.4.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.4.4.m1.1.1.3" xref="algorithm1.4.4.m1.1.1.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.2" xref="algorithm1.4.4.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.4.4.m1.1.1.3.3" xref="algorithm1.4.4.m1.1.1.3.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.3.2" xref="algorithm1.4.4.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.3" xref="algorithm1.4.4.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1a" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.4" xref="algorithm1.4.4.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1b" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.5" xref="algorithm1.4.4.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1c" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.6" xref="algorithm1.4.4.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m1.1b"><apply id="algorithm1.4.4.m1.1.1.cmml" xref="algorithm1.4.4.m1.1.1"><times id="algorithm1.4.4.m1.1.1.1.cmml" xref="algorithm1.4.4.m1.1.1.1"></times><ci id="algorithm1.4.4.m1.1.1.2.cmml" xref="algorithm1.4.4.m1.1.1.2">𝐷</ci><apply id="algorithm1.4.4.m1.1.1.3.cmml" xref="algorithm1.4.4.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.4.4.m1.1.1.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3">subscript</csymbol><ci id="algorithm1.4.4.m1.1.1.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.4.4.m1.1.1.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3"><times id="algorithm1.4.4.m1.1.1.3.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3.3.1"></times><ci id="algorithm1.4.4.m1.1.1.3.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.4.4.m1.1.1.3.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.4.4.m1.1.1.3.3.4.cmml" xref="algorithm1.4.4.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.4.4.m1.1.1.3.3.5.cmml" xref="algorithm1.4.4.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.4.4.m1.1.1.3.3.6.cmml" xref="algorithm1.4.4.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.4.4.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.add(inst1)
</div>
<div class="ltx_listingline" id="algorithm1.6.16">
<span class="ltx_tag ltx_tag_listingline">12</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst2 = build_instance(’context_prompt’, q, gold_ans, pass)
</div>
<div class="ltx_listingline" id="algorithm1.5.5">
<span class="ltx_tag ltx_tag_listingline">13</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.5.5.m1.1"><semantics id="algorithm1.5.5.m1.1a"><mrow id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml"><mi id="algorithm1.5.5.m1.1.1.2" xref="algorithm1.5.5.m1.1.1.2.cmml">D</mi><mo id="algorithm1.5.5.m1.1.1.1" xref="algorithm1.5.5.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.5.5.m1.1.1.3" xref="algorithm1.5.5.m1.1.1.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.2" xref="algorithm1.5.5.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.5.5.m1.1.1.3.3" xref="algorithm1.5.5.m1.1.1.3.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.3.2" xref="algorithm1.5.5.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.3" xref="algorithm1.5.5.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1a" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.4" xref="algorithm1.5.5.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1b" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.5" xref="algorithm1.5.5.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1c" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.6" xref="algorithm1.5.5.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><apply id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1"><times id="algorithm1.5.5.m1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1.1"></times><ci id="algorithm1.5.5.m1.1.1.2.cmml" xref="algorithm1.5.5.m1.1.1.2">𝐷</ci><apply id="algorithm1.5.5.m1.1.1.3.cmml" xref="algorithm1.5.5.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3">subscript</csymbol><ci id="algorithm1.5.5.m1.1.1.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.5.5.m1.1.1.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3"><times id="algorithm1.5.5.m1.1.1.3.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3.3.1"></times><ci id="algorithm1.5.5.m1.1.1.3.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.5.5.m1.1.1.3.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.5.5.m1.1.1.3.3.4.cmml" xref="algorithm1.5.5.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.5.5.m1.1.1.3.3.5.cmml" xref="algorithm1.5.5.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.5.5.m1.1.1.3.3.6.cmml" xref="algorithm1.5.5.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.5.5.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.add(inst2)

</div>
<div class="ltx_listingline" id="algorithm1.6.17">
<span class="ltx_tag ltx_tag_listingline">14</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div class="ltx_listingline" id="algorithm1.6.18">
<span class="ltx_tag ltx_tag_listingline">15</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div class="ltx_listingline" id="algorithm1.6.19">
<span class="ltx_tag ltx_tag_listingline">16</span> end for
</div>
<div class="ltx_listingline" id="algorithm1.6.6">return <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.6.6.m1.1"><semantics id="algorithm1.6.6.m1.1a"><mrow id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml"><mi id="algorithm1.6.6.m1.1.1.2" xref="algorithm1.6.6.m1.1.1.2.cmml">D</mi><mo id="algorithm1.6.6.m1.1.1.1" xref="algorithm1.6.6.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.6.6.m1.1.1.3" xref="algorithm1.6.6.m1.1.1.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.2" xref="algorithm1.6.6.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.6.6.m1.1.1.3.3" xref="algorithm1.6.6.m1.1.1.3.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.3.2" xref="algorithm1.6.6.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.3" xref="algorithm1.6.6.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1a" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.4" xref="algorithm1.6.6.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1b" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.5" xref="algorithm1.6.6.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1c" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.6" xref="algorithm1.6.6.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><apply id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1"><times id="algorithm1.6.6.m1.1.1.1.cmml" xref="algorithm1.6.6.m1.1.1.1"></times><ci id="algorithm1.6.6.m1.1.1.2.cmml" xref="algorithm1.6.6.m1.1.1.2">𝐷</ci><apply id="algorithm1.6.6.m1.1.1.3.cmml" xref="algorithm1.6.6.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m1.1.1.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3">subscript</csymbol><ci id="algorithm1.6.6.m1.1.1.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.6.6.m1.1.1.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3"><times id="algorithm1.6.6.m1.1.1.3.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3.3.1"></times><ci id="algorithm1.6.6.m1.1.1.3.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.6.6.m1.1.1.3.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.6.6.m1.1.1.3.3.4.cmml" xref="algorithm1.6.6.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.6.6.m1.1.1.3.3.5.cmml" xref="algorithm1.6.6.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.6.6.m1.1.1.3.3.6.cmml" xref="algorithm1.6.6.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.6.6.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.8.1.1">Algorithm&nbsp;1</span> </span>Training data creation</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p3">
<span class="ltx_ERROR undefined" id="S3.SS1.p3.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p3.2">Prompt: Answer the question Q. If you need help answer &lt;RET&gt; to get the context. Q: …</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.3">Alongside this prompt, we include the corresponding question from <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_Q</annotation></semantics></math> and the golden answer from <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_A</annotation></semantics></math>, collectively forming the instance (line 5), which is subsequently appended to the <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">D</mi><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.3.2" xref="S3.SS1.p4.3.m3.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1a" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.4" xref="S3.SS1.p4.3.m3.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1b" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.5" xref="S3.SS1.p4.3.m3.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1c" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.6" xref="S3.SS1.p4.3.m3.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><times id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></times><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝐷</ci><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">𝑆</ci><apply id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3"><times id="S3.SS1.p4.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.p4.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.4.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.5.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.6.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math> dataset (line 6).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.2">In contrast, if the LLM fails to produce a correct response to the question (line 8), we build two different instances. The first employs the same <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.1">parametric_prompt</span> as previously described, with <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mo id="S3.SS1.p5.1.m1.1.1" stretchy="false" xref="S3.SS1.p5.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mo id="S3.SS1.p5.2.m2.1.1" stretchy="false" xref="S3.SS1.p5.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">⟩</annotation></semantics></math> designated as the answer (line 9), indicating the necessity for additional context. The second prompt, termed <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.2">context_prompt</span>, encompasses contextual information alongside the question:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<span class="ltx_ERROR undefined" id="S3.SS1.p6.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p6.2">Prompt: Answer the question Q given the context C. Q: …, C: …


</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.3">For this instance, we include the prompt, the question from <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><mi id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">italic_Q</annotation></semantics></math>, the golden answer from <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p7.2.m2.1"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.2.m2.1d">italic_A</annotation></semantics></math>, and the corresponding context passage from <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p7.3.m3.1"><semantics id="S3.SS1.p7.3.m3.1a"><mi id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.3.m3.1d">italic_P</annotation></semantics></math> (line 11).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">After populating the dataset with both types of prompts for questions where the LLM could not respond accurately and only the <span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.1">parametric_prompt</span> with golden answers for all other questions, our training set <math alttext="D_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p8.1.m1.1"><semantics id="S3.SS1.p8.1.m1.1a"><msub id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.1.1.2" xref="S3.SS1.p8.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS1.p8.1.m1.1.1.3" xref="S3.SS1.p8.1.m1.1.1.3.cmml"><mi id="S3.SS1.p8.1.m1.1.1.3.2" xref="S3.SS1.p8.1.m1.1.1.3.2.cmml">A</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.3" xref="S3.SS1.p8.1.m1.1.1.3.3.cmml">d</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1a" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.4" xref="S3.SS1.p8.1.m1.1.1.3.4.cmml">a</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1b" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.5" xref="S3.SS1.p8.1.m1.1.1.3.5.cmml">p</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1c" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.6" xref="S3.SS1.p8.1.m1.1.1.3.6.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><apply id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p8.1.m1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3"><times id="S3.SS1.p8.1.m1.1.1.3.1.cmml" xref="S3.SS1.p8.1.m1.1.1.3.1"></times><ci id="S3.SS1.p8.1.m1.1.1.3.2.cmml" xref="S3.SS1.p8.1.m1.1.1.3.2">𝐴</ci><ci id="S3.SS1.p8.1.m1.1.1.3.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3.3">𝑑</ci><ci id="S3.SS1.p8.1.m1.1.1.3.4.cmml" xref="S3.SS1.p8.1.m1.1.1.3.4">𝑎</ci><ci id="S3.SS1.p8.1.m1.1.1.3.5.cmml" xref="S3.SS1.p8.1.m1.1.1.3.5">𝑝</ci><ci id="S3.SS1.p8.1.m1.1.1.3.6.cmml" xref="S3.SS1.p8.1.m1.1.1.3.6">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">D_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is prepared for the subsequent fine-tuning phase. The fine-tuning process entails training the base LLM on our dataset, resulting in the <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p8.1.2">Adapt-LLM</span> model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">This approach ensures that the model effectively learns to discern when context is necessary for answering questions, or to provide a direct response when it suffices, as well as answer directly when provided with context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In the inference phase, we utilize the fine-tuned model to generate responses to unseen questions. We employ the same prompts used during the training phase, as outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.4">Initially, the model is prompted to either provide a direct response or return <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mo id="S3.SS2.p2.2.m2.1.1" stretchy="false" xref="S3.SS2.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">⟩</annotation></semantics></math> if it is unsure of the answer. If the model returns <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" stretchy="false" xref="S3.SS2.p2.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mo id="S3.SS2.p2.4.m4.1.1" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">⟩</annotation></semantics></math>, we proceed with information retrieval to acquire relevant context by means of an off-the-shelf IR system. Subsequently, we augment the question with the retrieved context and prompt the model again using the second type of prompt introduced during the training phase.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Training Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Model configuration</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.2.1.1.1">NQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.2.1.2.1">Never Retrieve</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.3">21.43%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.3.2.1.1">Always Retrieve</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.2">35.86%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.4.3.1.1">Adapt-LLM</span> (ours)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.2.1">36.77%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.5.4.1.1">SQuAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.5.4.2"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.5.4.2.1">Never Retrieve</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.5.4.3">21.22%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.6.5.1.1">Always Retrieve</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5.2">36.59%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.6.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.7.6.1.1">Adapt-LLM</span> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.2.1">38.15%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison of Llama-2 models trained on the NQ and SQuAD datasets using different retrieval configurations (NR-LLM, AR-LLM, and <span class="ltx_text ltx_font_smallcaps" id="S3.T1.3.1">Adapt-LLM</span>), evaluated on the PopQA test set. Exact match accuracy is reported for all models.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we outline the experimental framework aimed at assessing the performance of the proposed adaptive retrieval approach, <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">Adapt-LLM</span>. We begin by describing the datasets utilized (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1" title="4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.1</span></a>), followed by an overview of our base model (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS2" title="4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.2</span></a>), the different configurations of the base model (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS3" title="4.3 Model Configurations ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.3</span></a>), and the training details (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS4" title="4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Subsequently, we introduce the three primary experiments:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Evaluation of <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i1.p1.1.1">Adapt-LLM</span> performance compared to the following baseline models: (i) an LLM that retrieves contextual information for all questions, and (ii) an LLM that exclusively relies on its parametric memory without using an IR system for any question (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS5" title="4.5 Validating the Adaptive Retrieval Approach ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Analysis of <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i2.p1.1.1">Adapt-LLM</span>’s ability to determine when extra context is necessary to answer a question (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS6" title="4.6 Contextual Retrieval Decision Analysis ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.6</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Comparison with the state-of-the-art approach for PopQA (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS7" title="4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.7</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">NQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">SQuAD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">PopQA</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.1">Questions</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.2">58,880</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.3">87,599</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.4">14,282</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.1">Words/question</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.2">9.20</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.3">10.06</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.4">6.62</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.1">Words/answer</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.2">2.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.3">3.16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.4">2.04</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of the three datasets we use for our experiments, i.e. SQuAD, NQ and PopQA. For each of them we provide the number of questions, and the average number of words per question and answer.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.6.6.7"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.7.1">Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.2">
<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.1">RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo id="S4.T3.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">⟩</annotation></semantics></math> Usage</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.4.4.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.1">RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo id="S4.T3.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T3.4.4.4.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">⟩</annotation></semantics></math></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.2">No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.1.m1.1a"><mo id="S4.T3.5.5.5.1.m1.1.1" stretchy="false" xref="S4.T3.5.5.5.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><ci id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.6.6.6.2.m2.1"><semantics id="S4.T3.6.6.6.2.m2.1a"><mo id="S4.T3.6.6.6.2.m2.1.1" stretchy="false" xref="S4.T3.6.6.6.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.2.m2.1b"><ci id="S4.T3.6.6.6.2.m2.1.1.cmml" xref="S4.T3.6.6.6.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.2.m2.1d">⟩</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.6.7.1">
<td class="ltx_td" id="S4.T3.6.7.1.1"></td>
<td class="ltx_td" id="S4.T3.6.7.1.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.3.1">Acc. w/ context</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.4.1">Acc. w/o context</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.5.1">Acc. w/ context</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.6.1">Acc. w/o context</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.8.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.1">NQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.2">82.26%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.3">33.04%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.4">14.65%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.5">55.72%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.6">62.36%</th>
</tr>
<tr class="ltx_tr" id="S4.T3.6.9.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.1">SQuAD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.2">83.93%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.3">33.40%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.4">9.94%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.5">57.73%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.6">62.92%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of the usage of the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.13.m1.1"><semantics id="S4.T3.13.m1.1b"><mo id="S4.T3.13.m1.1.1" stretchy="false" xref="S4.T3.13.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.m1.1c"><ci id="S4.T3.13.m1.1.1.cmml" xref="S4.T3.13.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.m1.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.m1.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.14.m2.1"><semantics id="S4.T3.14.m2.1b"><mo id="S4.T3.14.m2.1.1" stretchy="false" xref="S4.T3.14.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.m2.1c"><ci id="S4.T3.14.m2.1.1.cmml" xref="S4.T3.14.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.m2.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.m2.1e">⟩</annotation></semantics></math> token in the <span class="ltx_text ltx_font_smallcaps" id="S4.T3.22.1">Adapt-LLM</span> model. The first column shows the percentage of PopQA questions for which the model requests additional context. The second column focuses on the questions for which <span class="ltx_text ltx_font_smallcaps" id="S4.T3.23.2">Adapt-LLM</span> asks for context (<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.15.m3.1"><semantics id="S4.T3.15.m3.1b"><mo id="S4.T3.15.m3.1.1" stretchy="false" xref="S4.T3.15.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.m3.1c"><ci id="S4.T3.15.m3.1.1.cmml" xref="S4.T3.15.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.m3.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.15.m3.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.16.m4.1"><semantics id="S4.T3.16.m4.1b"><mo id="S4.T3.16.m4.1.1" stretchy="false" xref="S4.T3.16.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.m4.1c"><ci id="S4.T3.16.m4.1.1.cmml" xref="S4.T3.16.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.m4.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.16.m4.1e">⟩</annotation></semantics></math>), comparing the performance between answering those questions with and without context. The last column (No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.17.m5.1"><semantics id="S4.T3.17.m5.1b"><mo id="S4.T3.17.m5.1.1" stretchy="false" xref="S4.T3.17.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.m5.1c"><ci id="S4.T3.17.m5.1.1.cmml" xref="S4.T3.17.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.m5.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.17.m5.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.18.m6.1"><semantics id="S4.T3.18.m6.1b"><mo id="S4.T3.18.m6.1.1" stretchy="false" xref="S4.T3.18.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.m6.1c"><ci id="S4.T3.18.m6.1.1.cmml" xref="S4.T3.18.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.m6.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.18.m6.1e">⟩</annotation></semantics></math>) is for questions which <span class="ltx_text ltx_font_smallcaps" id="S4.T3.24.3">Adapt-LLM</span> decides to answer directly. We also compare the performance with and without the context retrieved by the IR system.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To ensure comprehensive training and evaluation of our models, we specifically selected three diverse question answering datasets. For training, we chose NQ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib18" title="">18</a>]</cite> and SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib25" title="">25</a>]</cite>, as they are widely recognized datasets that assess factual knowledge and are based on Wikipedia. For evaluation, we opted for PopQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>. Below are brief descriptions of each dataset:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">NQ</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">The Natural Questions dataset <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib18" title="">18</a>]</cite> is a collection of real-world questions derived from Google search queries, accompanied by long-form text passages obtained from Wikipedia articles and providing a diverse range of topics and natural language variations. We utilize this dataset for <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">training</span> our models in the experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">SQuAD</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">The Stanford Question Answering Dataset SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib25" title="">25</a>]</cite> is a widely utilized dataset in the field of natural language processing and comprises questions posed by crowdworkers on a diverse range of Wikipedia articles, along with relevant paragraph passages serving as context. We utilize this dataset for <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.1">training</span> our models in the experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">PopQA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">The Popular Questions and Answers dataset <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> consists of curated questions sourced from various online platforms, encompassing a wide range of domains and styles. Given the variability in the effectiveness of context retrieval strategies observed in this dataset, we select PopQA as our test set to <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px3.p1.1.1">evaluate</span> the language models’ performance in determining when context is necessary for accurate answer provision.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Base Model</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In our experiments, we employ Llama-2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib34" title="">34</a>]</cite> as our base LLM. Llama-2 is an open-source instruction-based LLM, which comes in versions of 7B, 13B, and 70B parameters. The model is pretrained on an expanded corpus sourced from publicly available online data sources. This corpus offers a 40% increase in size compared to its predecessor, contributing to the model’s enhanced performance and capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Additionally, Llama-2 features an extended context length, effectively doubling its capacity to process and comprehend longer sequences of text. These enhancements significantly improve the model’s effectiveness across various natural language understanding tasks. Specifically, for our experiments, we utilize the Llama-2 model with 7B parameters, leveraging its robust capabilities for our specific research objectives.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Configurations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We conduct the experiments using three different model configurations, corresponding to the three different ways in which an LLM and an IR system can be combined:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Adaptive Retrieval (<span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.1.1">Adapt-LLM</span>)</span>. The <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.2">Adapt-LLM</span> model dynamically decides whether to retrieve context based on the question and its perceived need for contextual information, as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>. As the IR system, we use Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>, which is an unsupervised model pretrained on a large corpus, followed by fine-tuning on MS MARCO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib24" title="">24</a>]</cite>. We only retrieve the most relevant passage according to the IR system to prompt the base LLM for the final answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Never-Retrieve (NR-LLM)</span>. This model configuration is trained to answer questions solely based on the question text without considering any contextual information. It serves as the baseline for evaluating the performance of question answering models in the absence of context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Always-Retrieve (AR-LLM)</span>. In contrast to the NR-LLM model, this configuration always retrieves context passages to assist in answering questions. It is trained to utilize context consistently for generating answers. To ensure a fair comparison with <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i3.p1.1.2">Adapt-LLM</span>, we also use Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite> as the IR system and only retrieve the most relevant passage as context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Training Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">For all three model configurations (<span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.1">Adapt-LLM</span>, AR-LLM and NR-LLM) and both training sets (SQuAD and NQ), we adhere to the parameter configuration established in Alpaca-Lora <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib32" title="">32</a>]</cite> which includes a batch size of 128, three epochs, and a fixed learning rate of 3e-4. We incorporated LoRA (Low-Rank Adaptation) regularization, with parameters configured for r=8, alpha=16, and a dropout rate of 0.05. Training was performed on an NVIDIA A40 GPU, for an average training time of approximately 8 hours. We do not perform any model selection and we use the last checkpoint after 3 epochs of training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F2.1" style="width:195.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S4.F2.1.g1" src="extracted/5573676/img/istogramma_pop_score_use_ret_nq.png" width="598">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F2.2" style="width:195.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S4.F2.2.g1" src="extracted/5573676/img/istogramma_pop_score_use_ret_squad.png" width="598">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Histograms depicting the proportion of questions where <span class="ltx_text ltx_font_smallcaps" id="S4.F2.5.1">Adapt-LLM</span> trained on NQ (left) and <span class="ltx_text ltx_font_smallcaps" id="S4.F2.6.2">Adapt-LLM</span> trained on SQuAD (right) ask for extra context for different popularity score intervals.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Validating the Adaptive Retrieval Approach</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In order to assess the effectiveness of our adaptive approach (<span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p1.1.1">Adapt-LLM</span>) in comparison to the NR-LLM and AR-LLM configurations, we conducted fine-tuning of the Llama-2 model on both the NQ and SQuAD datasets across all three configurations. For the NR-LLM and AR-LLM configurations, we constructed training samples by extracting question-answer pairs from the datasets and incorporating corresponding instruction prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.2">Specifically, prompts for the NR-LLM configuration instructed the model to answer questions without additional context, whereas prompts for the AR-LLM configuration included both the question and contextual information. In contrast, the <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p2.2.1">Adapt-LLM</span> training set was constructed following the approach outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>, employing a two-step process. As a result of this process, the 74.72% of the questions in NQ are marked with the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mo id="S4.SS5.p2.1.m1.1.1" stretchy="false" xref="S4.SS5.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mo id="S4.SS5.p2.2.m2.1.1" stretchy="false" xref="S4.SS5.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.2.m2.1d">⟩</annotation></semantics></math> token, whereas the 87.49% questions are marked for SQuAD.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">The trained models were then tested on the PopQA dataset to evaluate their performance in a real-world question answering scenario. During inference, the NR-LLM and AR-LLM models were utilized as is, with corresponding instruction prompts provided, and outputs expected to be answers to the questions. Conversely, for the <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p3.1.1">Adapt-LLM</span> model, we followed the same prompt procedure as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS2" title="3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">The generated answers are then compared to the set of possible answers for each question, which are already annotated in the PopQA test set. The evaluation metric used is <span class="ltx_text ltx_font_bold" id="S4.SS5.p4.1.1">Exact Match Accuracy</span>, which measures the percentage of generated outputs that exactly match one of the possible answers for the corresponding question.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.T1" title="Table 1 ‣ 3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a> presents the results of this experiment, illustrating the performance of the Llama-2 model across the different configurations and datasets. Across both the NQ and SQuAD training datasets, the <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p5.1.1">Adapt-LLM</span> configuration consistently outperforms the Never Retrieve (NR-LLM) and Always Retrieve (AR-LLM) configurations on the PopQA test set. As can be observed, NR-LLM exhibits the lowest performance among the models, with an accuracy difference of approximately 14 absolute points compared to the other configurations. This disparity suggests that the parametric memory of Llama-2 alone is not sufficient for effectively answering PopQA questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p6">
<p class="ltx_p" id="S4.SS5.p6.1">The differences between AR-LLM and <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.1">Adapt-LLM</span> are narrower. Specifically, the <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.2">Adapt-LLM</span> configuration achieves an accuracy of 36.77% and 38.15% on the PopQA test set when trained on the NQ and SQuAD datasets, respectively, compared to 35.86% and 36.59% for the AR-LLM configuration. Across both training datasets, <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.3">Adapt-LLM</span> outperforms AR-LLM, with the largest difference observed when trained on SQuAD.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p7">
<p class="ltx_p" id="S4.SS5.p7.1">All in all, these results underscore the efficacy of the adaptive retrieval approach in dynamically determining the necessity of context for accurate question answering, resulting in improved performance compared to fixed strategies of always or never retrieving context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p8">
<p class="ltx_p" id="S4.SS5.p8.1">Although the disparity between training <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p8.1.1">Adapt-LLM</span> on NQ or SQuAD is relatively minor, we try to determine the suitability of a training set for a given evaluation set. While both training sets (NQ and SQuAD) and the evaluation set (PopQA) are based on Wikipedia, subtle differences may exist.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p9">
<p class="ltx_p" id="S4.SS5.p9.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a> provides insights into the characteristics of the three datasets involved in our experimental procedure, including the total number of questions and the average number of words per question and answer. While NQ appears to be closer to PopQA in terms of question and answer lengths, the key factor influencing the better results of training <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p9.2.1">Adapt-LLM</span> on SQuAD may be the number of questions in the training dataset (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p9.1.m1.1"><semantics id="S4.SS5.p9.1.m1.1a"><mo id="S4.SS5.p9.1.m1.1.1" xref="S4.SS5.p9.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p9.1.m1.1.1.cmml" xref="S4.SS5.p9.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p9.1.m1.1d">∼</annotation></semantics></math>87K in SQuAD and <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p9.2.m2.1"><semantics id="S4.SS5.p9.2.m2.1a"><mo id="S4.SS5.p9.2.m2.1.1" xref="S4.SS5.p9.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.2.m2.1b"><csymbol cd="latexml" id="S4.SS5.p9.2.m2.1.1.cmml" xref="S4.SS5.p9.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p9.2.m2.1d">∼</annotation></semantics></math>58K in NQ). Further analyses are required to elucidate the factors that render a training dataset more suitable for a given target dataset (which is beyond the scope of our study), but these results suggest that scale may play once again a crucial role.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Contextual Retrieval Decision Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">In this experiment, our objective is to once again evaluate the effectiveness of the <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p1.1.1">Adapt-LLM</span> model, this time focusing on its ability to accurately determine when additional context is needed. For this purpose, we adhere to the following steps:
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.2">We conduct inference on the <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i1.p1.2.1">Adapt-LLM</span> model using the PopQA test set, prompting it to either return an answer directly or indicate the need for additional context by returning <math alttext="\langle" class="ltx_Math" display="inline" id="S4.I3.i1.p1.1.m1.1"><semantics id="S4.I3.i1.p1.1.m1.1a"><mo id="S4.I3.i1.p1.1.m1.1.1" stretchy="false" xref="S4.I3.i1.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><ci id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.I3.i1.p1.2.m2.1"><semantics id="S4.I3.i1.p1.2.m2.1a"><mo id="S4.I3.i1.p1.2.m2.1.1" stretchy="false" xref="S4.I3.i1.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><ci id="S4.I3.i1.p1.2.m2.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.2.m2.1d">⟩</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.2">In the case of receiving a <math alttext="\langle" class="ltx_Math" display="inline" id="S4.I3.i2.p1.1.m1.1"><semantics id="S4.I3.i2.p1.1.m1.1a"><mo id="S4.I3.i2.p1.1.m1.1.1" stretchy="false" xref="S4.I3.i2.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><ci id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.I3.i2.p1.2.m2.1"><semantics id="S4.I3.i2.p1.2.m2.1a"><mo id="S4.I3.i2.p1.2.m2.1.1" stretchy="false" xref="S4.I3.i2.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><ci id="S4.I3.i2.p1.2.m2.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.2.m2.1d">⟩</annotation></semantics></math> response from the <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i2.p1.2.1">Adapt-LLM</span> model, we proceed with the following steps:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S4.I3.i2.I1">
<li class="ltx_item" id="S4.I3.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1.</span>
<div class="ltx_para" id="S4.I3.i2.I1.i1.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i1.p1.1">We conduct inference on the <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i2.I1.i1.p1.1.1">Adapt-LLM</span> model, prompting it to return an answer given the context obtained from the IR system.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2.</span>
<div class="ltx_para" id="S4.I3.i2.I1.i2.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i2.p1.1">We also conduct inference on the NR-LLM model with the instruction to provide an answer directly without additional context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1">If the <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i3.p1.1.1">Adapt-LLM</span> model decides to answer the question directly relying only on its parametric memory:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S4.I3.i3.I1">
<li class="ltx_item" id="S4.I3.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.1.</span>
<div class="ltx_para" id="S4.I3.i3.I1.i1.p1">
<p class="ltx_p" id="S4.I3.i3.I1.i1.p1.1">We conduct inference on the <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i3.I1.i1.p1.1.1">Adapt-LLM</span> model, prompting it to return the answer without providing context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.2.</span>
<div class="ltx_para" id="S4.I3.i3.I1.i2.p1">
<p class="ltx_p" id="S4.I3.i3.I1.i2.p1.1">We conduct inference on the AR-LLM model with the instruction to provide an answer using the context retrieved by the IR system.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results of this experiment. The first thing to note is that the <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p2.2.1">Adapt-LLM</span> model generates the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p2.1.m1.1"><semantics id="S4.SS6.p2.1.m1.1a"><mo id="S4.SS6.p2.1.m1.1.1" stretchy="false" xref="S4.SS6.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><ci id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p2.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p2.2.m2.1"><semantics id="S4.SS6.p2.2.m2.1a"><mo id="S4.SS6.p2.2.m2.1.1" stretchy="false" xref="S4.SS6.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.1b"><ci id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p2.2.m2.1d">⟩</annotation></semantics></math> token for approximately 82-83% of the questions in the PopQA dataset, with similar ratios observed across both training datasets. This observation aligns with the low performance of the NR-LLM configuration demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.T1" title="Table 1 ‣ 3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS6.p3">
<p class="ltx_p" id="S4.SS6.p3.2">However, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.1">Adapt-LLM</span> consistently determines when additional context is required to answer a question accurately. Across both the NQ and SQuAD training datasets, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.2">Adapt-LLM</span> exhibits significantly higher accuracy when retrieving context compared to the NR-LLM model’s accuracy without context (as indicated in the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p3.1.m1.1"><semantics id="S4.SS6.p3.1.m1.1a"><mo id="S4.SS6.p3.1.m1.1.1" stretchy="false" xref="S4.SS6.p3.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p3.1.m1.1b"><ci id="S4.SS6.p3.1.m1.1.1.cmml" xref="S4.SS6.p3.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p3.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p3.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p3.2.m2.1"><semantics id="S4.SS6.p3.2.m2.1a"><mo id="S4.SS6.p3.2.m2.1.1" stretchy="false" xref="S4.SS6.p3.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p3.2.m2.1b"><ci id="S4.SS6.p3.2.m2.1.1.cmml" xref="S4.SS6.p3.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p3.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p3.2.m2.1d">⟩</annotation></semantics></math> column of Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>). Specifically, for the NQ dataset, the accuracy of the <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.3">Adapt-LLM</span> model when requesting context is 33.04%, whereas the accuracy of the NR-LLM model without context retrieval is notably lower at 14.65%. Similarly, for the SQuAD dataset, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.4">Adapt-LLM</span> achieves an accuracy of 33.40% with context retrieval, whereas the NR-LLM model’s accuracy without context is substantially lower at 9.94%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Passages</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">SQuAD Dev</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">NQ Dev</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.1.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.2.1">Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.1">Gold</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.1.2.1">89.42%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.1.3.1">69.76%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.2.1">Contriever</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.2.2">22.49</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.2.3">27.04%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison of <span class="ltx_text ltx_font_smallcaps" id="S4.T4.3.1">Adapt-LLM</span> for the SQuAD and NQ dev sets, when using the gold passages provided by the datasets and when using the best passage retrieved by Contriever.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS6.p4">
<p class="ltx_p" id="S4.SS6.p4.2">Finally, the last column of Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a> (No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p4.1.m1.1"><semantics id="S4.SS6.p4.1.m1.1a"><mo id="S4.SS6.p4.1.m1.1.1" stretchy="false" xref="S4.SS6.p4.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.1.m1.1b"><ci id="S4.SS6.p4.1.m1.1.1.cmml" xref="S4.SS6.p4.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p4.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p4.2.m2.1"><semantics id="S4.SS6.p4.2.m2.1a"><mo id="S4.SS6.p4.2.m2.1.1" stretchy="false" xref="S4.SS6.p4.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.2.m2.1b"><ci id="S4.SS6.p4.2.m2.1.1.cmml" xref="S4.SS6.p4.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p4.2.m2.1d">⟩</annotation></semantics></math>) shows the performance of <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p4.2.1">Adapt-LLM</span> when answering questions based solely on its parametric memory. As can be seen, accuracies above 62% are obtained when no context is utilized, providing further evidence that <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p4.2.2">Adapt-LLM</span> effectively discerns between retrieving context and providing direct answers to questions. Additionally, we evaluate the performance of these questions when context is added to the input, revealing significant decreases in accuracy of up to 7 absolute points.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS6.p5">
<p class="ltx_p" id="S4.SS6.p5.1">These findings provide insights into the effectiveness of the decision-making process employed by the <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p5.1.1">Adapt-LLM</span> model in determining the necessity of additional context for accurate response generation and present empirical evidence of the necessity of performing dynamic context retrieval in improving the accuracy of question answering models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS6.p6">
<p class="ltx_p" id="S4.SS6.p6.1">However, it is notable that the overall performance of the model when answering questions with retrieved context, as observed in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a> (approximately 33%), is relatively low. To further explore this observation, we conduct an additional experiment: evaluating <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p6.1.1">Adapt-LLM</span> (both versions trained on NQ and SQuAD) on the NQ and SQuAD development splits, comparing performance when using the gold passages of the dataset and the context retrieved by our IR system, Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>. Unfortunately, PopQA does not provide the gold passages, so direct evaluation there was not possible.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS6.p7">
<p class="ltx_p" id="S4.SS6.p7.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T4" title="Table 4 ‣ 4.6 Contextual Retrieval Decision Analysis ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results of this experiment. A significant performance difference is observed between using the gold passage and the top passage retrieved by Contriever for both datasets (approximately 67 absolute points for SQuAD and 42 for NQ). This indicates that Contriever, and current IR systems in general, do not consistently retrieve the most relevant passage to answer a given question. This observation underscores the importance of retrieving multiple documents as context, as seen in the most successful open-domain QA systems <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib13" title="">13</a>]</cite>, and highlights its impact on the overall performance of <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p7.1.1">Adapt-LLM</span> in PopQA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS6.p8">
<p class="ltx_p" id="S4.SS6.p8.2">To further validate the behavior of <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.1">Adapt-LLM</span> when requesting additional context, Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.F2" title="Figure 2 ‣ 4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the proportion of questions for which our model generates the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p8.1.m1.1"><semantics id="S4.SS6.p8.1.m1.1a"><mo id="S4.SS6.p8.1.m1.1.1" stretchy="false" xref="S4.SS6.p8.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.1.m1.1b"><ci id="S4.SS6.p8.1.m1.1.1.cmml" xref="S4.SS6.p8.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p8.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p8.2.m2.1"><semantics id="S4.SS6.p8.2.m2.1a"><mo id="S4.SS6.p8.2.m2.1.1" stretchy="false" xref="S4.SS6.p8.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.2.m2.1b"><ci id="S4.SS6.p8.2.m2.1.1.cmml" xref="S4.SS6.p8.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p8.2.m2.1d">⟩</annotation></semantics></math> token, aggregated by popularity score intervals (left image for <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.2">Adapt-LLM</span> trained on NQ and right image for SQuAD). <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> suggest that high-popularity questions can be adequately answered using the parametric memory of the LLM, while lower popularity scores necessitate extra context. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.F2" title="Figure 2 ‣ 4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe this pattern for both versions of <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.3">Adapt-LLM</span>, indicating that our model, despite lacking access to popularity scores during training or inference, has learned effective criteria for requesting additional context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Comparison with state-of-the-art methods</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">We conducted a comparative analysis between our <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p1.1.1">Adapt-LLM</span> model and the current state-of-the-art approach for PopQA proposed by <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>. Their methodology relies on the popularity score annotated in the PopQA dataset to determine whether a question requires additional context. To establish the optimal threshold for determining question popularity, <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> split the PopQA dataset into 75% as a development set for threshold determination and 25% as a test set. In the original paper, they apply this methodology to various LLMs available at that moment (Llama-2 was not released yet).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1">To ensure a fair comparison between <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.1">Adapt-LLM</span> and the popularity-based method, we replicated their approach using the Llama-2 7B model to determine the best popularity score threshold (found to be 707,000) using the same PopQA development set. This allowed us to obtain results consistent with their methodology while utilizing our base LLM. Similar to the original results in <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> when using smaller models, the popularity score threshold is almost equivalent to always retrieving contextual information for Llama-2 7B. The IR usage is of 99.86% as presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T5" title="Table 5 ‣ 4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">5</span></a>. This clearly shows how the popularity score method struggles with smaller size models, being <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.2">GPT-3 davinci-003</span> the only model to get a IR usage below 80% in the original paper when using adaptive retrieval with the Contriever. Subsequently, we evaluated our <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.3">Adapt-LLM</span> configuration on the same 25% test set split and compared the outcomes with those obtained using the method described by <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>. This systematic comparison enabled us to assess the efficacy of our <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.4">Adapt-LLM</span> model in relation to the current state of the art.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS7.p3">
<p class="ltx_p" id="S4.SS7.p3.1">The results of this experiment are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T5" title="Table 5 ‣ 4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">5</span></a>. We observe comparable performance between the replicated approach of <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.1">Adapt-LLM</span> when trained on NQ and SQuAD datasets and tested on the 25% subset of PopQA. It’s worth mentioning that <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.2">Adapt-LLM</span> does not utilize any information from PopQA, unlike <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>, who directly use the popularity score and a 75% portion of PopQA dataset to find an optimal value for that popularity score. This methodology is not generalizable to other open-domain question answering tasks since the popularity score is a unique feature of PopQA. However, <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.3">Adapt-LLM</span> can be applied to any similar dataset. Given these characteristics, we believe that the results obtained by <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.4">Adapt-LLM</span> are even more significant, offering comparable performance to an approach that utilizes dataset-specific information. These findings substantiate the validity of our approach, demonstrating its effectiveness even when trained on datasets different from the one used for testing.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Model Configuration</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">IR usage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.3.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.2.1.1.1">Popularity Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.2">99.86%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3">36.81%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.3.2.1.1">Adapt-LLM (NQ)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.2">87.22%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3">35.30%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.4.3.1.1">Adapt-LLM (SQuAD)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.4.3.2">83.99%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.4.3.3.1">37.29%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparison of Llama-2 base models trained on the SQuAD and NQ datasets for the <span class="ltx_text ltx_font_smallcaps" id="S4.T5.4.1">Adapt-LLM</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.T5.5.2">Popularity Score</span> configurations. The later mimics the methodology proposed by <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> with the Llama-2 LLM as the base model.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">In this paper, we introduce <span class="ltx_text ltx_font_smallcaps" id="S5.p1.2.1">Adapt-LLM</span>, a LLM which learns to discern when additional context is necessary for answering a question, rather than relying solely on its parametric memory. <span class="ltx_text ltx_font_smallcaps" id="S5.p1.2.2">Adapt-LLM</span> is the result of fine-tuning a base LLM on an open-domain question answering dataset that has been modified to differentiate between questions answerable with the LLM’s parametric memory alone and those requiring supplementary context. To construct these training datasets, we initially subject the base LLM to zero-shot evaluation to determine its accuracy in answering questions. For questions where the model’s response is incorrect, we train the LLM to generate a special token, <math alttext="\langle" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" stretchy="false" xref="S5.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mo id="S5.p1.2.m2.1.1" stretchy="false" xref="S5.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">⟩</annotation></semantics></math>, indicating the need for additional context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Through extensive experiments conducted on the PopQA dataset, we show that <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">Adapt-LLM</span> performs better than its two fixed alternatives: never retrieving and always retrieving relevant context information. Furthermore, our findings highlight <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">Adapt-LLM</span>’s capability to effectively discern the necessity of additional context, which is the primary objective of this work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">For future investigations, we propose exploring methods to enhance performance when utilizing an IR system, such as incorporating learnable sequential retrieval techniques. Furthermore, we believe it would be valuable to conduct a more in-depth analysis of the interaction between training and testing datasets in the development of <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.1">Adapt-LLM</span> systems.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work received partial support from the Basque Government through research group funding IT1805-22 and the ICL4LANG project (grant no. KK-2023/00094). Additionally, we acknowledge the support of several MCIN/AEI/10.13039/501100011033 projects: (i) DeepKnowledge (PID2021-127777OB-C21) and funding from FEDER, EU; (ii) AWARE (TED2021-131617B-I00) and support from the European Union NextGenerationEU/PRTR. We express our gratitude to Carlos Domínguez for his assistance in the experimental setup and to Eneko Agirre for his valuable feedback and guidance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Achiam, S.&nbsp;Adler, S.&nbsp;Agarwal, L.&nbsp;Ahmad, I.&nbsp;Akkaya, F.&nbsp;L. Aleman, D.&nbsp;Almeida, J.&nbsp;Altenschmidt, S.&nbsp;Altman, S.&nbsp;Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber.

</span>
<span class="ltx_bibblock">RAG makes LLMs better and equal.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.pinecone.io/blog/rag-study/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Barnett, S.&nbsp;Kurniawan, S.&nbsp;Thudumu, Z.&nbsp;Brannelly, and M.&nbsp;Abdelrazek.

</span>
<span class="ltx_bibblock">Seven failure points when engineering a retrieval augmented generation system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2401.05856</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berger et&nbsp;al. [2000]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Berger, R.&nbsp;Caruana, D.&nbsp;Cohn, D.&nbsp;Freitag, and V.&nbsp;Mittal.

</span>
<span class="ltx_bibblock">Bridging the lexical chasm: statistical approaches to answer-finding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</em>, pages 192–199, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">International conference on machine learning</em>, pages 2206–2240. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke.

</span>
<span class="ltx_bibblock">QuAC: Question Answering in Context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2174–2184, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erbacher et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Erbacher, L.&nbsp;Falissar, V.&nbsp;Guigue, and L.&nbsp;Soulier.

</span>
<span class="ltx_bibblock">Navigating uncertainty: Optimizing api dependency for hallucination reduction in closed-book question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2401.01780</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Gao, X.&nbsp;Yao, and D.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021</em>, pages 6894–6910. Association for Computational Linguistics (ACL), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei.

</span>
<span class="ltx_bibblock">REALM: retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 37th International Conference on Machine Learning</em>. JMLR.org, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard, Gautier and Grave, Edouard [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, Gautier and Grave, Edouard.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">EACL 2021-16th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 874–880. Association for Computational Linguistics, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACM Computing Surveys</em>, 55(12):1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Roux, A.&nbsp;Mensch, B.&nbsp;Savary, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l. Casas, E.&nbsp;B. Hanna, F.&nbsp;Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. Association for Computational Linguistics, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Liang, R.&nbsp;Bommasani, T.&nbsp;Lee, D.&nbsp;Tsipras, D.&nbsp;Soylu, M.&nbsp;Yasunaga, Y.&nbsp;Zhang, D.&nbsp;Narayanan, Y.&nbsp;Wu, A.&nbsp;Kumar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin, Stephanie and Hilton, Jacob and Evans, Owain [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, Stephanie and Hilton, Jacob and Evans, Owain.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring How Models Mimic Human Falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh.

</span>
<span class="ltx_bibblock">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">The 61st Annual Meeting Of The Association For Computational Linguistics</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2112.09332</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li.

</span>
<span class="ltx_bibblock">Ms marco: A human-generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, pages 2383–2392, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Transactions of the Association for Computational Linguistics</em>, 11:1316–1331, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap, J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Reimers and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et&nbsp;al. [2009]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Robertson, H.&nbsp;Zaragoza, et&nbsp;al.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Foundations and Trends® in Information Retrieval</em>, 3(4):333–389, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro, L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun.

</span>
<span class="ltx_bibblock">Two-Step Question Retrieval for Open-Domain QA.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">60th Annual Meeting of the Association for Computational Linguistics, ACL 2022</em>, pages 1487–1492. Association for Computational Linguistics, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.

</span>
<span class="ltx_bibblock">Stanford alpaca: an instruction-following llama model (2023).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">URL https://github. com/tatsu-lab/stanford_alpaca</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Thakur, N.&nbsp;Reimers, A.&nbsp;Rücklé, A.&nbsp;Srivastava, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix, B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei, N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Zhu, W.&nbsp;Lei, C.&nbsp;Wang, J.&nbsp;Zheng, S.&nbsp;Poria, and T.-S. Chua.

</span>
<span class="ltx_bibblock">Retrieving and reading: A comprehensive survey on open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2101.00774</em>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header" data-bs-theme="dark"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.19705] When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</title><meta property="og:description" content="In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.19705">

<!--Generated on Sun May  5 16:40:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiziano&nbsp;Labruna<span id="id5.1.id1" class="ltx_ERROR undefined">\orcid</span>0000-0001-7713-7679
</span><span class="ltx_author_notes">Corresponding Author. Email: tlabruna@fbk.eu.</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jon Ander&nbsp;Campos<span id="id6.1.id1" class="ltx_ERROR undefined">\orcid</span>0000-0002-1447-5870
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gorka&nbsp;Azkune<span id="id7.1.id1" class="ltx_ERROR undefined">\orcid</span>0000-0002-2506-7426
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Bozen-Bolzano
</span>
<span class="ltx_contact ltx_role_address">Fondazione Bruno Kessler
</span>
<span class="ltx_contact ltx_role_address">HiTZ Center - Ixa, University of the Basque Country UPV/EHU
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.4" class="ltx_p">In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question.
Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM‚Äôs parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, <math id="id1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="id1.1.m1.1a"><mo stretchy="false" id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\langle</annotation></semantics></math>RET<math id="id2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="id2.2.m2.1a"><mo stretchy="false" id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\rangle</annotation></semantics></math>, when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (<span id="id4.4.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that <span id="id4.4.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> is able to generate the <math id="id3.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="id3.3.m3.1a"><mo stretchy="false" id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\langle</annotation></semantics></math>RET<math id="id4.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="id4.4.m4.1a"><mo stretchy="false" id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><ci id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\rangle</annotation></semantics></math> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\paperid</span>
<p id="p1.2" class="ltx_p">123</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The task of question answering (QA) remains a focal point in Natural Language Understanding research. There are many different datasets serving as benchmarks for evaluating QA models, such as Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> or QuAC <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, just to mention a few. Nowadays, Large Language Models (LLMs) consistently outperform traditional methods on these benchmarks, showcasing remarkable performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Typically, there are two primary approaches to utilize LLMs for question answering:</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">(i) <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Closed Book Question Answering</span>: This approach involves strategies like instruction tuning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> or few-shot prompting <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to enhance performance. Here, the LLM relies solely on its parametric memory to answer questions. However, these parametric memories have inherent limitations as they are based entirely on the training corpus, meaning for example that they could be outdated regarding events occurring after the training process.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">(ii) <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Open Book Question Answering</span>: In this approach, the LLM is coupled with an Information Retriever (IR) system <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. By leveraging the IR system, the LLM can retrieve relevant context to supplement its understanding and provide more accurate answers.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">However, the research conducted by <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> sheds light on the complexity of question-answering strategies, challenging the notion that the optimal approach always involves the utilization of an IR system. Through the introduction of the PopQA dataset, comprising 14 thousand questions annotated with popularity scores, they demonstrated that while LLMs relying solely on their parametric memories excel in addressing high-popularity questions, the efficacy diminishes for low-popularity questions, where using IR becomes curcial.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.19705/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The inference process of <span id="S1.F1.6.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> step-by-step: given a question (step 1), an LLM decides (step 2) whether to answer the question directly (step 3) or to ask for additional contextual information, generating the special <math id="S1.F1.3.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S1.F1.3.m1.1b"><mo stretchy="false" id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\langle</annotation></semantics></math>RET<math id="S1.F1.4.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S1.F1.4.m2.1b"><mo stretchy="false" id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\rangle</annotation></semantics></math> token; for the later, an off-the-shelf IR system is used to retrieve relevant context (step 4), which is used alongside the question to prompt again the LLM for the final answer (step 5).</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Their findings underscore the importance of a hybrid approach, where LLMs utilize parametric memory for high-popularity questions, but use an off-the-shelf IR system to retrieve relevant context to answer low-popularity questions. Central to their methodology is the establishment of a fixed popularity score threshold, which they use to decide whether an IR system has to be employed.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In many cases, however, question answering datasets do not include popularity scores, so relying on such scores is not a generalizable approach. Motivated by this limitation, our study aims to address whether LLMs can autonomously determine when to employ an IR system for improved question answering. To investigate this, we conduct an evaluation of an LLM using an open-domain question answering dataset to identify the questions for which the LLM provides accurate responses and those where its answers are incorrect.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.2" class="ltx_p">Specifically, for questions where the LLM‚Äôs response is incorrect, we annotate them with a special token, <math id="S1.p8.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S1.p8.1.m1.1a"><mo stretchy="false" id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S1.p8.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S1.p8.2.m2.1a"><mo stretchy="false" id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><ci id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\rangle</annotation></semantics></math>, indicating the need for additional context. Subsequently, we utilize these annotations to construct a new dataset tailored for training purposes, where we teach an LLM to answer directly if it is confident about the answer or to require context it believes is useful for answering the question (see Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Our hypothesis is that through this training process, the LLM learns to use an IR system when it needs extra context to answer a question, thus we name it <span id="S1.p8.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">To validate our hypothesis, we conducted several experiments on the PopQA dataset <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, as it provides a suitable platform for benchmarking hybrid retrieval strategies.
As a result of these experiments we find that:</p>
</div>
<div id="S1.p10" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> consistently outperforms typical fixed strategies for question answering, such as (i) using the IR system for all questions and (ii) relying solely on the parametric memory of the LLM.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> demonstrates performance comparable to strategies that rely on popularity scores to determine when to use an IR system, even without utilizing any popularity score or similar metric. It‚Äôs worth noting that popularity scores are a unique feature of the PopQA dataset, rendering them inapplicable to other open-domain question answering datasets.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">When <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> decides to retrieve additional information, the results obtained with the context are significantly better than those without it. Similarly, when <span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> directly answers questions relying on its parametric memory, it achieves high accuracies. These observations indicate that the model effectively discerns when to retrieve information and when it can answer a question without further context.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">The primary bottleneck for the performance of <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> lies in the IR system. <span id="S1.I1.i4.p1.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> achieves much higher performance with gold passages compared to passages retrieved by the IR system.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">Our findings underscore the significance of adaptive retrieval strategies in enhancing the performance of LLMs for question answering tasks. By training <span id="S1.p11.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> to dynamically determine when to retrieve additional context, we demonstrate the feasibility of teaching an LLM how to effectively leverage external information sources only when necessary.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> has shown improvements on a wide variety of NLP areas, such as question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, truthfulness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and language modelling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> among others. The ability to ground model generations on retrieved text chunks has also enabled smaller models to match the performance of larger ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Moreover, due to the extremely high cost of training LLMs, RAG has become the standard way to maintain them updated with new information, not having to re-train the models periodically to incorporate new facts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Even if augmenting LLMs with retrieval is an essential step for the current generation of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> it also comes with a cost. Traditional retrieval methods as TF-IDF or BM-25 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> are only able to retrieve documents with keyword overlap and suffer from lexical gap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In order to try to solve this issue, many pre-trained Transformer encoder based dense models have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Trained neural models have shown good performance over a variety of retrieval benchmarks but they still struggle in the zero-shot setup for new domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The quality of the retrieval engine is essential for retrieval-augmented models as this will set the upper bound of the model performance. Moreover, the usage of a retrieval engine, especially when the target document index is huge, can significantly increase the latency of the model and hurt real time applications user experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">On the other hand, as models keep scaling, the world knowledge encoded in their parameters does too <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Many previous efforts have shown that language models are able to memorize a significant amount of world knowledge and achieve competitive performance on tasks such as open-domain question answering when they just use their parametric knowledge for solving the task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Motivated by all this, the adaptive approach has been proposed as a new solution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. In this approach, if the solution to the task is encoded in the parameters of the model, the model will be directly used for generating a solution. Conversely, if the answer is not encoded in the knowledge of the model, the answer generation will be augmented with external knowledge.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Recently, <cite class="ltx_cite ltx_citemacro_citet">Schick et&nbsp;al. [<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposed the Toolformer, a model that can self teach how and when to use external tools via simple API calls including a calculator, search engines, a calendar and so on. The self learning process is based on a synthetic text only corpus that is enriched by prompting an LLM. The LLM first adds inline API calls on top of the unsupervised corpus. These API calls are then validated by evaluating whether the execution of the API calls is helpful for predicting the future tokens. This unsupervised method significantly boosts model performance in a variety of tasks when compared against non augmented LLMs, but it also makes the model over use tools. As an example, for the QA task the model uses the search engine 99.3% of the cases. On our work, we try to take advantage of the parametric knowledge of LLMs and just perform retrieval when needed. <span id="S2.p5.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> decreases the usage of IR down to 83.99% while improving performance over vanilla retrieval.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">More similar to our work, <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> propose a dataset and method for measuring when non-parametric information needs to be retrieved. They present the PopQA dataset that contains 14K questions about a set of entities with varying popularity. The popularity of an entity is measured by the page views of its Wikipedia page. In order to solve this QA task, they use a popularity score threshold calculated on the PopQA dataset. If the popularity score of an individual entity is below the threshold they perform a retrieval step. On the contrary, if the score is greater than the threshold they directly answer the question. This method yields better results than vanilla retrieval but it requires the calculation of a popularity score that is not available in realistic QA scenarios. Our <span id="S2.p6.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> method extends the idea of adaptive retrieval by generating a synthetic dataset that teaches the LLM whether to call the IR engine.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Adaptive Retrieval LLM (<span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>)</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Adaptive retrieval refers to the model‚Äôs capability to dynamically determine whether to retrieve additional context information for generating answers in question answering tasks. Unlike traditional models that either always incorporate context or never consider it, adaptive retrieval allows the model to selectively retrieve context based on the specific requirements of each question. This adaptive approach aims to optimize performance by leveraging context only when necessary, thereby enhancing the model‚Äôs ability to generate accurate answers.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">As depicted in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the process of the <span id="S3.p2.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> unfolds in the following sequence:</p>
</div>
<div id="S3.p3" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The first prompt containing the question is sent to the model (step 1 of Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">The <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> evaluates the prompt to determine whether additional context is necessary to answer the question effectively (step 2).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">If the model determines that context is not required, it directly produces a response to the question by leveraging its parametric memory (step 3).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.2" class="ltx_p">If context is deemed necessary, the <span id="S3.I1.i4.p1.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model returns a special token, represented as <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mo stretchy="false" id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><ci id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S3.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.I1.i4.p1.2.m2.1a"><mo stretchy="false" id="S3.I1.i4.p1.2.m2.1.1" xref="S3.I1.i4.p1.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><ci id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">\rangle</annotation></semantics></math>, and an off-the-shelf IR system is used to retrieve pertinent context based on the question (step 4); the context is then combined with the original question prompt to form a comprehensive representation for answer generation (step 5).</p>
</div>
</li>
</ol>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The decision-making process of <span id="S3.p4.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> enables the model to determine the necessity of context for answering questions through dynamic assessment of each prompt. This flexible behavior allows the model to strike a balance between utilizing context for enhanced understanding and delivering direct answers when sufficient.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training <span id="S3.SS1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Here, we delineate the methodology employed to train our <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model. The process of crafting the training data, denoted as <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1b" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.5" xref="S3.SS1.p1.1.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1c" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.6" xref="S3.SS1.p1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ùê∑</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ùëÜ</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">ùê¥</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">ùëë</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">ùëé</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.5">ùëù</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.6.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">DS_{Adapt}</annotation></semantics></math>, is presented in Algorithm <a href="#algorithm1" title="In 3.1 Training Adapt-LLM ‚Ä£ 3 Adaptive Retrieval LLM (Adapt-LLM) ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">We begin by selecting an open-domain question answering dataset containing questions <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">Q</annotation></semantics></math>, associated context passages <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">P</annotation></semantics></math>, and corresponding answers <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">A</annotation></semantics></math>. We initialize <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.3.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.4" xref="S3.SS1.p2.4.m4.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.3.3.1b" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.5" xref="S3.SS1.p2.4.m4.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.3.3.1c" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.6" xref="S3.SS1.p2.4.m4.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ùê∑</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">ùëÜ</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">ùê¥</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">ùëë</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4">ùëé</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.5.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.5">ùëù</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.6.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">DS_{Adapt}</annotation></semantics></math> to an empty set (line 1 of the algorithm). For each question in <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">Q</annotation></semantics></math>, we leverage the base LLM without any retrieval mechanism to perform a zero-shot inference (line 3). This step allows us to differentiate questions for which the model generates correct answers from those where its responses are inaccurate. This process can be understood as a way to discover what the base LLM <span id="S3.SS1.p2.5.1" class="ltx_text ltx_font_italic">knows</span> due to its parametric memory. For questions where the model‚Äôs response is accurate (line 4), we build a training set instance incorporating the following prompt, which we call <span id="S3.SS1.p2.5.2" class="ltx_text ltx_font_italic">parametric_prompt</span>:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<span id="S3.SS1.p3.1" class="ltx_ERROR undefined">{spverbatim}</span>
<p id="S3.SS1.p3.2" class="ltx_p">Prompt: Answer the question Q. If you need help answer &lt;RET&gt; to get the context. Q: ‚Ä¶</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.3" class="ltx_p">Alongside this prompt, we include the corresponding question from <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">Q</annotation></semantics></math> and the golden answer from <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">A</annotation></semantics></math>, collectively forming the instance (line 5), which is subsequently appended to the <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.3.2" xref="S3.SS1.p4.3.m3.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1a" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.4" xref="S3.SS1.p4.3.m3.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1b" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.5" xref="S3.SS1.p4.3.m3.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1c" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.6" xref="S3.SS1.p4.3.m3.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><times id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></times><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">ùê∑</ci><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">ùëÜ</ci><apply id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3"><times id="S3.SS1.p4.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.p4.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.2">ùê¥</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.3">ùëë</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.4.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.4">ùëé</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.5.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.5">ùëù</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.6.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">DS_{Adapt}</annotation></semantics></math> dataset (line 6).</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.2" class="ltx_p">In contrast, if the LLM fails to produce a correct response to the question (line 8), we build two different instances. The first employs the same <span id="S3.SS1.p5.2.1" class="ltx_text ltx_font_italic">parametric_prompt</span> as previously described, with <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mo stretchy="false" id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mo stretchy="false" id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\rangle</annotation></semantics></math> designated as the answer (line 9), indicating the necessity for additional context. The second prompt, termed <span id="S3.SS1.p5.2.2" class="ltx_text ltx_font_italic">context_prompt</span>, encompasses contextual information alongside the question:</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.6" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.6.7" class="ltx_listingline">

<span id="algorithm1.6.7.1" class="ltx_text"><span id="algorithm1.6.7.1.1" class="ltx_text ltx_font_bold">Input:</span> </span>Q: questions, A: answers, P: passages, LLM
</div>
<div id="algorithm1.1.1" class="ltx_listingline"> <span id="algorithm1.1.1.1" class="ltx_text"><span id="algorithm1.1.1.1.1" class="ltx_text ltx_font_bold">Output:</span> </span><math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mrow id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml"><mi id="algorithm1.1.1.m1.1.1.2" xref="algorithm1.1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="algorithm1.1.1.m1.1.1.3" xref="algorithm1.1.1.m1.1.1.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.2" xref="algorithm1.1.1.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.1.1.m1.1.1.3.3" xref="algorithm1.1.1.m1.1.1.3.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.3.2" xref="algorithm1.1.1.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.1.1.m1.1.1.3.3.3" xref="algorithm1.1.1.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1a" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.1.1.m1.1.1.3.3.4" xref="algorithm1.1.1.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1b" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.1.1.m1.1.1.3.3.5" xref="algorithm1.1.1.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1c" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.1.1.m1.1.1.3.3.6" xref="algorithm1.1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1"><times id="algorithm1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1"></times><ci id="algorithm1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.2">ùê∑</ci><apply id="algorithm1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.1.1.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3">subscript</csymbol><ci id="algorithm1.1.1.m1.1.1.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.2">ùëÜ</ci><apply id="algorithm1.1.1.m1.1.1.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3"><times id="algorithm1.1.1.m1.1.1.3.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3.3.1"></times><ci id="algorithm1.1.1.m1.1.1.3.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.3.2">ùê¥</ci><ci id="algorithm1.1.1.m1.1.1.3.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3.3">ùëë</ci><ci id="algorithm1.1.1.m1.1.1.3.3.4.cmml" xref="algorithm1.1.1.m1.1.1.3.3.4">ùëé</ci><ci id="algorithm1.1.1.m1.1.1.3.3.5.cmml" xref="algorithm1.1.1.m1.1.1.3.3.5">ùëù</ci><ci id="algorithm1.1.1.m1.1.1.3.3.6.cmml" xref="algorithm1.1.1.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">DS_{Adapt}</annotation></semantics></math>: A training dataset for Adaptive Retrieval
</div>
<div id="algorithm1.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>


<math id="algorithm1.2.2.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.2.2.m1.1a"><mrow id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml"><mi id="algorithm1.2.2.m1.1.1.2" xref="algorithm1.2.2.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1" xref="algorithm1.2.2.m1.1.1.1.cmml">‚Äã</mo><msub id="algorithm1.2.2.m1.1.1.3" xref="algorithm1.2.2.m1.1.1.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.2" xref="algorithm1.2.2.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.2.2.m1.1.1.3.3" xref="algorithm1.2.2.m1.1.1.3.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.3.2" xref="algorithm1.2.2.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.2.2.m1.1.1.3.3.3" xref="algorithm1.2.2.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1a" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.2.2.m1.1.1.3.3.4" xref="algorithm1.2.2.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1b" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.2.2.m1.1.1.3.3.5" xref="algorithm1.2.2.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1c" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.2.2.m1.1.1.3.3.6" xref="algorithm1.2.2.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><apply id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1"><times id="algorithm1.2.2.m1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1"></times><ci id="algorithm1.2.2.m1.1.1.2.cmml" xref="algorithm1.2.2.m1.1.1.2">ùê∑</ci><apply id="algorithm1.2.2.m1.1.1.3.cmml" xref="algorithm1.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.1.1.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3">subscript</csymbol><ci id="algorithm1.2.2.m1.1.1.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.2">ùëÜ</ci><apply id="algorithm1.2.2.m1.1.1.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3"><times id="algorithm1.2.2.m1.1.1.3.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3.3.1"></times><ci id="algorithm1.2.2.m1.1.1.3.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.3.2">ùê¥</ci><ci id="algorithm1.2.2.m1.1.1.3.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3.3">ùëë</ci><ci id="algorithm1.2.2.m1.1.1.3.3.4.cmml" xref="algorithm1.2.2.m1.1.1.3.3.4">ùëé</ci><ci id="algorithm1.2.2.m1.1.1.3.3.5.cmml" xref="algorithm1.2.2.m1.1.1.3.3.5">ùëù</ci><ci id="algorithm1.2.2.m1.1.1.3.3.6.cmml" xref="algorithm1.2.2.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">DS_{Adapt}</annotation></semantics></math> = init_empty()
</div>
<div id="algorithm1.6.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>
<span id="algorithm1.6.8.1" class="ltx_text ltx_font_bold">for</span>&nbsp;<em id="algorithm1.6.8.2" class="ltx_emph ltx_font_italic">q, gold_ans, pass in (Q, A, P)</em>&nbsp;<span id="algorithm1.6.8.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.6.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
ans = LLM(q)

</div>
<div id="algorithm1.6.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span id="algorithm1.6.10.1" class="ltx_text ltx_font_bold">if</span>&nbsp;<em id="algorithm1.6.10.2" class="ltx_emph ltx_font_italic">ans = gold_ans</em>&nbsp;<span id="algorithm1.6.10.3" class="ltx_text ltx_font_bold">then</span> 
</div>
<div id="algorithm1.6.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst = build_instance(‚Äôparametric_prompt‚Äô, q, gold_ans)
</div>
<div id="algorithm1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.3.3.m1.1a"><mrow id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml"><mi id="algorithm1.3.3.m1.1.1.2" xref="algorithm1.3.3.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.1" xref="algorithm1.3.3.m1.1.1.1.cmml">‚Äã</mo><msub id="algorithm1.3.3.m1.1.1.3" xref="algorithm1.3.3.m1.1.1.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.2" xref="algorithm1.3.3.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.3.3.m1.1.1.3.3" xref="algorithm1.3.3.m1.1.1.3.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.3.2" xref="algorithm1.3.3.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.3.3.m1.1.1.3.3.3" xref="algorithm1.3.3.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1a" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.3.3.m1.1.1.3.3.4" xref="algorithm1.3.3.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1b" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.3.3.m1.1.1.3.3.5" xref="algorithm1.3.3.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1c" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.3.3.m1.1.1.3.3.6" xref="algorithm1.3.3.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1"><times id="algorithm1.3.3.m1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1.1"></times><ci id="algorithm1.3.3.m1.1.1.2.cmml" xref="algorithm1.3.3.m1.1.1.2">ùê∑</ci><apply id="algorithm1.3.3.m1.1.1.3.cmml" xref="algorithm1.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.1.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3">subscript</csymbol><ci id="algorithm1.3.3.m1.1.1.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.2">ùëÜ</ci><apply id="algorithm1.3.3.m1.1.1.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3"><times id="algorithm1.3.3.m1.1.1.3.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3.3.1"></times><ci id="algorithm1.3.3.m1.1.1.3.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.3.2">ùê¥</ci><ci id="algorithm1.3.3.m1.1.1.3.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3.3">ùëë</ci><ci id="algorithm1.3.3.m1.1.1.3.3.4.cmml" xref="algorithm1.3.3.m1.1.1.3.3.4">ùëé</ci><ci id="algorithm1.3.3.m1.1.1.3.3.5.cmml" xref="algorithm1.3.3.m1.1.1.3.3.5">ùëù</ci><ci id="algorithm1.3.3.m1.1.1.3.3.6.cmml" xref="algorithm1.3.3.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">DS_{Adapt}</annotation></semantics></math>.add(inst)

</div>
<div id="algorithm1.6.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div id="algorithm1.6.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div id="algorithm1.6.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span id="algorithm1.6.14.1" class="ltx_text ltx_font_bold">else</span> 
</div>
<div id="algorithm1.6.15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst1 = build_instance(‚Äôparametric_prompt‚Äô, q, "&lt;RET&gt;")
</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math id="algorithm1.4.4.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.4.4.m1.1a"><mrow id="algorithm1.4.4.m1.1.1" xref="algorithm1.4.4.m1.1.1.cmml"><mi id="algorithm1.4.4.m1.1.1.2" xref="algorithm1.4.4.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.1" xref="algorithm1.4.4.m1.1.1.1.cmml">‚Äã</mo><msub id="algorithm1.4.4.m1.1.1.3" xref="algorithm1.4.4.m1.1.1.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.2" xref="algorithm1.4.4.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.4.4.m1.1.1.3.3" xref="algorithm1.4.4.m1.1.1.3.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.3.2" xref="algorithm1.4.4.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.4.4.m1.1.1.3.3.3" xref="algorithm1.4.4.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1a" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.4.4.m1.1.1.3.3.4" xref="algorithm1.4.4.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1b" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.4.4.m1.1.1.3.3.5" xref="algorithm1.4.4.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1c" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.4.4.m1.1.1.3.3.6" xref="algorithm1.4.4.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m1.1b"><apply id="algorithm1.4.4.m1.1.1.cmml" xref="algorithm1.4.4.m1.1.1"><times id="algorithm1.4.4.m1.1.1.1.cmml" xref="algorithm1.4.4.m1.1.1.1"></times><ci id="algorithm1.4.4.m1.1.1.2.cmml" xref="algorithm1.4.4.m1.1.1.2">ùê∑</ci><apply id="algorithm1.4.4.m1.1.1.3.cmml" xref="algorithm1.4.4.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.4.4.m1.1.1.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3">subscript</csymbol><ci id="algorithm1.4.4.m1.1.1.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.2">ùëÜ</ci><apply id="algorithm1.4.4.m1.1.1.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3"><times id="algorithm1.4.4.m1.1.1.3.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3.3.1"></times><ci id="algorithm1.4.4.m1.1.1.3.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.3.2">ùê¥</ci><ci id="algorithm1.4.4.m1.1.1.3.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3.3">ùëë</ci><ci id="algorithm1.4.4.m1.1.1.3.3.4.cmml" xref="algorithm1.4.4.m1.1.1.3.3.4">ùëé</ci><ci id="algorithm1.4.4.m1.1.1.3.3.5.cmml" xref="algorithm1.4.4.m1.1.1.3.3.5">ùëù</ci><ci id="algorithm1.4.4.m1.1.1.3.3.6.cmml" xref="algorithm1.4.4.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m1.1c">DS_{Adapt}</annotation></semantics></math>.add(inst1)
</div>
<div id="algorithm1.6.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst2 = build_instance(‚Äôcontext_prompt‚Äô, q, gold_ans, pass)
</div>
<div id="algorithm1.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math id="algorithm1.5.5.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.5.5.m1.1a"><mrow id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml"><mi id="algorithm1.5.5.m1.1.1.2" xref="algorithm1.5.5.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.1" xref="algorithm1.5.5.m1.1.1.1.cmml">‚Äã</mo><msub id="algorithm1.5.5.m1.1.1.3" xref="algorithm1.5.5.m1.1.1.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.2" xref="algorithm1.5.5.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.5.5.m1.1.1.3.3" xref="algorithm1.5.5.m1.1.1.3.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.3.2" xref="algorithm1.5.5.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.5.5.m1.1.1.3.3.3" xref="algorithm1.5.5.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1a" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.5.5.m1.1.1.3.3.4" xref="algorithm1.5.5.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1b" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.5.5.m1.1.1.3.3.5" xref="algorithm1.5.5.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1c" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.5.5.m1.1.1.3.3.6" xref="algorithm1.5.5.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><apply id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1"><times id="algorithm1.5.5.m1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1.1"></times><ci id="algorithm1.5.5.m1.1.1.2.cmml" xref="algorithm1.5.5.m1.1.1.2">ùê∑</ci><apply id="algorithm1.5.5.m1.1.1.3.cmml" xref="algorithm1.5.5.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3">subscript</csymbol><ci id="algorithm1.5.5.m1.1.1.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.2">ùëÜ</ci><apply id="algorithm1.5.5.m1.1.1.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3"><times id="algorithm1.5.5.m1.1.1.3.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3.3.1"></times><ci id="algorithm1.5.5.m1.1.1.3.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.3.2">ùê¥</ci><ci id="algorithm1.5.5.m1.1.1.3.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3.3">ùëë</ci><ci id="algorithm1.5.5.m1.1.1.3.3.4.cmml" xref="algorithm1.5.5.m1.1.1.3.3.4">ùëé</ci><ci id="algorithm1.5.5.m1.1.1.3.3.5.cmml" xref="algorithm1.5.5.m1.1.1.3.3.5">ùëù</ci><ci id="algorithm1.5.5.m1.1.1.3.3.6.cmml" xref="algorithm1.5.5.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">DS_{Adapt}</annotation></semantics></math>.add(inst2)

</div>
<div id="algorithm1.6.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div id="algorithm1.6.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div id="algorithm1.6.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span> end for
</div>
<div id="algorithm1.6.6" class="ltx_listingline">return <math id="algorithm1.6.6.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.6.6.m1.1a"><mrow id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml"><mi id="algorithm1.6.6.m1.1.1.2" xref="algorithm1.6.6.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.1" xref="algorithm1.6.6.m1.1.1.1.cmml">‚Äã</mo><msub id="algorithm1.6.6.m1.1.1.3" xref="algorithm1.6.6.m1.1.1.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.2" xref="algorithm1.6.6.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.6.6.m1.1.1.3.3" xref="algorithm1.6.6.m1.1.1.3.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.3.2" xref="algorithm1.6.6.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.6.6.m1.1.1.3.3.3" xref="algorithm1.6.6.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1a" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.6.6.m1.1.1.3.3.4" xref="algorithm1.6.6.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1b" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.6.6.m1.1.1.3.3.5" xref="algorithm1.6.6.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1c" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="algorithm1.6.6.m1.1.1.3.3.6" xref="algorithm1.6.6.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><apply id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1"><times id="algorithm1.6.6.m1.1.1.1.cmml" xref="algorithm1.6.6.m1.1.1.1"></times><ci id="algorithm1.6.6.m1.1.1.2.cmml" xref="algorithm1.6.6.m1.1.1.2">ùê∑</ci><apply id="algorithm1.6.6.m1.1.1.3.cmml" xref="algorithm1.6.6.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m1.1.1.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3">subscript</csymbol><ci id="algorithm1.6.6.m1.1.1.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.2">ùëÜ</ci><apply id="algorithm1.6.6.m1.1.1.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3"><times id="algorithm1.6.6.m1.1.1.3.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3.3.1"></times><ci id="algorithm1.6.6.m1.1.1.3.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.3.2">ùê¥</ci><ci id="algorithm1.6.6.m1.1.1.3.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3.3">ùëë</ci><ci id="algorithm1.6.6.m1.1.1.3.3.4.cmml" xref="algorithm1.6.6.m1.1.1.3.3.4">ùëé</ci><ci id="algorithm1.6.6.m1.1.1.3.3.5.cmml" xref="algorithm1.6.6.m1.1.1.3.3.5">ùëù</ci><ci id="algorithm1.6.6.m1.1.1.3.3.6.cmml" xref="algorithm1.6.6.m1.1.1.3.3.6">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">DS_{Adapt}</annotation></semantics></math>

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.8.1.1" class="ltx_text ltx_font_bold">Algorithm&nbsp;1</span> </span>Training data creation</figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<span id="S3.SS1.p6.1" class="ltx_ERROR undefined">{spverbatim}</span>
<p id="S3.SS1.p6.2" class="ltx_p">Prompt: Answer the question Q given the context C. Q: ‚Ä¶, C: ‚Ä¶


</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.3" class="ltx_p">For this instance, we include the prompt, the question from <math id="S3.SS1.p7.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p7.1.m1.1a"><mi id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">Q</annotation></semantics></math>, the golden answer from <math id="S3.SS1.p7.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">A</annotation></semantics></math>, and the corresponding context passage from <math id="S3.SS1.p7.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.p7.3.m3.1a"><mi id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">P</annotation></semantics></math> (line 11).</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p">After populating the dataset with both types of prompts for questions where the LLM could not respond accurately and only the <span id="S3.SS1.p8.1.1" class="ltx_text ltx_font_italic">parametric_prompt</span> with golden answers for all other questions, our training set <math id="S3.SS1.p8.1.m1.1" class="ltx_Math" alttext="D_{Adapt}" display="inline"><semantics id="S3.SS1.p8.1.m1.1a"><msub id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.1.1.2" xref="S3.SS1.p8.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS1.p8.1.m1.1.1.3" xref="S3.SS1.p8.1.m1.1.1.3.cmml"><mi id="S3.SS1.p8.1.m1.1.1.3.2" xref="S3.SS1.p8.1.m1.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.1.m1.1.1.3.1" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.1.m1.1.1.3.3" xref="S3.SS1.p8.1.m1.1.1.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.1.m1.1.1.3.1a" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.1.m1.1.1.3.4" xref="S3.SS1.p8.1.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.1.m1.1.1.3.1b" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.1.m1.1.1.3.5" xref="S3.SS1.p8.1.m1.1.1.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.1.m1.1.1.3.1c" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.1.m1.1.1.3.6" xref="S3.SS1.p8.1.m1.1.1.3.6.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><apply id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.1.1.2">ùê∑</ci><apply id="S3.SS1.p8.1.m1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3"><times id="S3.SS1.p8.1.m1.1.1.3.1.cmml" xref="S3.SS1.p8.1.m1.1.1.3.1"></times><ci id="S3.SS1.p8.1.m1.1.1.3.2.cmml" xref="S3.SS1.p8.1.m1.1.1.3.2">ùê¥</ci><ci id="S3.SS1.p8.1.m1.1.1.3.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3.3">ùëë</ci><ci id="S3.SS1.p8.1.m1.1.1.3.4.cmml" xref="S3.SS1.p8.1.m1.1.1.3.4">ùëé</ci><ci id="S3.SS1.p8.1.m1.1.1.3.5.cmml" xref="S3.SS1.p8.1.m1.1.1.3.5">ùëù</ci><ci id="S3.SS1.p8.1.m1.1.1.3.6.cmml" xref="S3.SS1.p8.1.m1.1.1.3.6">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">D_{Adapt}</annotation></semantics></math> is prepared for the subsequent fine-tuning phase. The fine-tuning process entails training the base LLM on our dataset, resulting in the <span id="S3.SS1.p8.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model.</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">This approach ensures that the model effectively learns to discern when context is necessary for answering questions, or to provide a direct response when it suffices, as well as answer directly when provided with context.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the inference phase, we utilize the fine-tuned model to generate responses to unseen questions. We employ the same prompts used during the training phase, as outlined in Section <a href="#S3.SS1" title="3.1 Training Adapt-LLM ‚Ä£ 3 Adaptive Retrieval LLM (Adapt-LLM) ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">Initially, the model is prompted to either provide a direct response or return <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\rangle</annotation></semantics></math> if it is unsure of the answer. If the model returns <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mo stretchy="false" id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\langle</annotation></semantics></math>RET<math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mo stretchy="false" id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\rangle</annotation></semantics></math>, we proceed with information retrieval to acquire relevant context by means of an off-the-shelf IR system. Subsequently, we augment the question with the retrieved context and prompt the model again using the second type of prompt introduced during the training phase.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we outline the experimental framework aimed at assessing the performance of the proposed adaptive retrieval approach, <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>. We begin by describing the datasets utilized (Section <a href="#S4.SS1" title="4.1 Datasets ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), followed by an overview of our base model (Section <a href="#S4.SS2" title="4.2 Base Model ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), the different configurations of the base model (Section <a href="#S4.SS3" title="4.3 Model Configurations ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), and the training details (Section <a href="#S4.SS4" title="4.4 Training Details ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Subsequently, we introduce the three primary experiments:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Evaluation of <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> performance compared to the following baseline models: (i) an LLM that retrieves contextual information for all questions, and (ii) an LLM that exclusively relies on its parametric memory without using an IR system for any question (Section <a href="#S4.SS5" title="4.5 Validating the Adaptive Retrieval Approach ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Analysis of <span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>‚Äôs ability to determine when extra context is necessary to answer a question (Section <a href="#S4.SS6" title="4.6 Contextual Retrieval Decision Analysis ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.6</span></a>).</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Comparison with the state-of-the-art approach for PopQA (Section <a href="#S4.SS7" title="4.7 Comparison with state-of-the-art methods ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.7</span></a>).</p>
</div>
</li>
</ol>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training Set</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model configuration</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">NQ</span></td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.2.1" class="ltx_text ltx_font_smallcaps">Never Retrieve</span></td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">21.43%</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">Always Retrieve</span></td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">35.86%</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center">
<span id="S4.T1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> (ours)</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.3.2.1" class="ltx_text ltx_font_bold">36.77%</span></td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">SQuAD</span></td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.5.4.2.1" class="ltx_text ltx_font_smallcaps">Never Retrieve</span></td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">21.22%</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.5.1.1" class="ltx_text ltx_font_smallcaps">Always Retrieve</span></td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center">36.59%</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T1.1.7.6.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> (ours)</td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.7.6.2.1" class="ltx_text ltx_font_bold">38.15%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison of Llama-2 models trained on the NQ and SQuAD datasets using different retrieval configurations (NR-LLM, AR-LLM, and <span id="S4.T1.3.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>), evaluated on the PopQA test set. Exact match accuracy is reported for all models.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To ensure comprehensive training and evaluation of our models, we specifically selected three diverse question answering datasets. For training, we chose NQ <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, as they are widely recognized datasets that assess factual knowledge and are based on Wikipedia. For evaluation, we opted for PopQA <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Below are brief descriptions of each dataset:</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">NQ</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">The Natural Questions dataset <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is a collection of real-world questions derived from Google search queries, accompanied by long-form text passages obtained from Wikipedia articles and providing a diverse range of topics and natural language variations. We utilize this dataset for <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">training</span> our models in the experiments.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">SQuAD</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">The Stanford Question Answering Dataset SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is a widely utilized dataset in the field of natural language processing and comprises questions posed by crowdworkers on a diverse range of Wikipedia articles, along with relevant paragraph passages serving as context. We utilize this dataset for <span id="S4.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">training</span> our models in the experiments.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">PopQA</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The Popular Questions and Answers dataset <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> consists of curated questions sourced from various online platforms, encompassing a wide range of domains and styles. Given the variability in the effectiveness of context retrieval strategies observed in this dataset, we select PopQA as our test set to <span id="S4.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">evaluate</span> the language models‚Äô performance in determining when context is necessary for accurate answer provision.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Base Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In our experiments, we employ Llama-2 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as our base LLM. Llama-2 is an open-source instruction-based LLM, which comes in versions of 7B, 13B, and 70B parameters. The model is pretrained on an expanded corpus sourced from publicly available online data sources. This corpus offers a 40% increase in size compared to its predecessor, contributing to the model‚Äôs enhanced performance and capabilities.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Additionally, Llama-2 features an extended context length, effectively doubling its capacity to process and comprehend longer sequences of text. These enhancements significantly improve the model‚Äôs effectiveness across various natural language understanding tasks. Specifically, for our experiments, we utilize the Llama-2 model with 7B parameters, leveraging its robust capabilities for our specific research objectives.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">NQ</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">SQuAD</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">PopQA</span></th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Questions</td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">58,880</td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">87,599</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">14,282</td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<td id="S4.T2.1.3.3.1" class="ltx_td ltx_align_center">Words/question</td>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center">9.20</td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center">10.06</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center">6.62</td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<td id="S4.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_bb">Words/answer</td>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">2.26</td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">3.16</td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">2.04</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of the three datasets we use for our experiments, i.e. SQuAD, NQ and PopQA. For each of them we provide the number of questions, and the average number of words per question and answer.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.6" class="ltx_tr">
<th id="S4.T3.6.6.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.6.6.7.1" class="ltx_text ltx_font_bold">Training</span></th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\langle</annotation></semantics></math><span id="S4.T3.2.2.2.1" class="ltx_text ltx_font_bold">RET<math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\rangle</annotation></semantics></math> Usage</span>
</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">
<math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\langle</annotation></semantics></math><span id="S4.T3.4.4.4.1" class="ltx_text ltx_font_bold">RET<math id="S4.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\rangle</annotation></semantics></math></span>
</th>
<th id="S4.T3.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.6.6.6.2" class="ltx_text ltx_font_bold">No <math id="S4.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.5.5.5.1.m1.1a"><mo stretchy="false" id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><ci id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.T3.6.6.6.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.6.6.6.2.m2.1a"><mo stretchy="false" id="S4.T3.6.6.6.2.m2.1.1" xref="S4.T3.6.6.6.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.2.m2.1b"><ci id="S4.T3.6.6.6.2.m2.1.1.cmml" xref="S4.T3.6.6.6.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.2.m2.1c">\rangle</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.7.1" class="ltx_tr">
<td id="S4.T3.6.7.1.1" class="ltx_td"></td>
<td id="S4.T3.6.7.1.2" class="ltx_td"></td>
<td id="S4.T3.6.7.1.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.3.1" class="ltx_text ltx_font_bold">Acc. w/ context</span></td>
<td id="S4.T3.6.7.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.4.1" class="ltx_text ltx_font_bold">Acc. w/o context</span></td>
<td id="S4.T3.6.7.1.5" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.5.1" class="ltx_text ltx_font_bold">Acc. w/ context</span></td>
<td id="S4.T3.6.7.1.6" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.6.1" class="ltx_text ltx_font_bold">Acc. w/o context</span></td>
</tr>
<tr id="S4.T3.6.8.2" class="ltx_tr">
<th id="S4.T3.6.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NQ</th>
<th id="S4.T3.6.8.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">82.26%</th>
<th id="S4.T3.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">33.04%</th>
<th id="S4.T3.6.8.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">14.65%</th>
<th id="S4.T3.6.8.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">55.72%</th>
<th id="S4.T3.6.8.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">62.36%</th>
</tr>
<tr id="S4.T3.6.9.3" class="ltx_tr">
<td id="S4.T3.6.9.3.1" class="ltx_td ltx_align_center ltx_border_bb">SQuAD</td>
<td id="S4.T3.6.9.3.2" class="ltx_td ltx_align_center ltx_border_bb">83.93%</td>
<td id="S4.T3.6.9.3.3" class="ltx_td ltx_align_center ltx_border_bb">33.40%</td>
<td id="S4.T3.6.9.3.4" class="ltx_td ltx_align_center ltx_border_bb">9.94%</td>
<td id="S4.T3.6.9.3.5" class="ltx_td ltx_align_center ltx_border_bb">57.73%</td>
<td id="S4.T3.6.9.3.6" class="ltx_td ltx_align_center ltx_border_bb">62.92%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of the usage of the <math id="S4.T3.13.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.13.m1.1b"><mo stretchy="false" id="S4.T3.13.m1.1.1" xref="S4.T3.13.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.m1.1c"><ci id="S4.T3.13.m1.1.1.cmml" xref="S4.T3.13.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.m1.1d">\langle</annotation></semantics></math>RET<math id="S4.T3.14.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.14.m2.1b"><mo stretchy="false" id="S4.T3.14.m2.1.1" xref="S4.T3.14.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.m2.1c"><ci id="S4.T3.14.m2.1.1.cmml" xref="S4.T3.14.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.m2.1d">\rangle</annotation></semantics></math> token in the <span id="S4.T3.22.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model. The first column shows the percentage of PopQA questions for which the model requests additional context. The second column focuses on the questions for which <span id="S4.T3.23.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> asks for context (<math id="S4.T3.15.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.15.m3.1b"><mo stretchy="false" id="S4.T3.15.m3.1.1" xref="S4.T3.15.m3.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.m3.1c"><ci id="S4.T3.15.m3.1.1.cmml" xref="S4.T3.15.m3.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.m3.1d">\langle</annotation></semantics></math>RET<math id="S4.T3.16.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.16.m4.1b"><mo stretchy="false" id="S4.T3.16.m4.1.1" xref="S4.T3.16.m4.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.m4.1c"><ci id="S4.T3.16.m4.1.1.cmml" xref="S4.T3.16.m4.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.m4.1d">\rangle</annotation></semantics></math>), comparing the performance between answering those questions with and without context. The last column (No <math id="S4.T3.17.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.17.m5.1b"><mo stretchy="false" id="S4.T3.17.m5.1.1" xref="S4.T3.17.m5.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.m5.1c"><ci id="S4.T3.17.m5.1.1.cmml" xref="S4.T3.17.m5.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.m5.1d">\langle</annotation></semantics></math>RET<math id="S4.T3.18.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.18.m6.1b"><mo stretchy="false" id="S4.T3.18.m6.1.1" xref="S4.T3.18.m6.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.m6.1c"><ci id="S4.T3.18.m6.1.1.cmml" xref="S4.T3.18.m6.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.m6.1d">\rangle</annotation></semantics></math>) is for questions which <span id="S4.T3.24.3" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> decides to answer directly. We also compare the performance with and without the context retrieved by the IR system.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Configurations</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We conduct the experiments using three different model configurations, corresponding to the three different ways in which an LLM and an IR system can be combined:</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Adaptive Retrieval (<span id="S4.I2.i1.p1.1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>)</span>. The <span id="S4.I2.i1.p1.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model dynamically decides whether to retrieve context based on the question and its perceived need for contextual information, as explained in Section <a href="#S3.SS1" title="3.1 Training Adapt-LLM ‚Ä£ 3 Adaptive Retrieval LLM (Adapt-LLM) ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. As the IR system, we use Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which is an unsupervised model pretrained on a large corpus, followed by fine-tuning on MS MARCO <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. We only retrieve the most relevant passage according to the IR system to prompt the base LLM for the final answer.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Never-Retrieve (NR-LLM)</span>. This model configuration is trained to answer questions solely based on the question text without considering any contextual information. It serves as the baseline for evaluating the performance of question answering models in the absence of context.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Always-Retrieve (AR-LLM)</span>. In contrast to the NR-LLM model, this configuration always retrieves context passages to assist in answering questions. It is trained to utilize context consistently for generating answers. To ensure a fair comparison with <span id="S4.I2.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>, we also use Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as the IR system and only retrieve the most relevant passage as context.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Training Details</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">For all three model configurations (<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>, AR-LLM and NR-LLM) and both training sets (SQuAD and NQ), we adhere to the parameter configuration established in Alpaca-Lora <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> which includes a batch size of 128, three epochs, and a fixed learning rate of 3e-4. We incorporated LoRA (Low-Rank Adaptation) regularization, with parameters configured for r=8, alpha=16, and a dropout rate of 0.05. Training was performed on an NVIDIA A40 GPU, for an average training time of approximately 8 hours. We do not perform any model selection and we use the last checkpoint after 3 epochs of training.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Validating the Adaptive Retrieval Approach</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In order to assess the effectiveness of our adaptive approach (<span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>) in comparison to the NR-LLM and AR-LLM configurations, we conducted fine-tuning of the Llama-2 model on both the NQ and SQuAD datasets across all three configurations. For the NR-LLM and AR-LLM configurations, we constructed training samples by extracting question-answer pairs from the datasets and incorporating corresponding instruction prompts.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.2" class="ltx_p">Specifically, prompts for the NR-LLM configuration instructed the model to answer questions without additional context, whereas prompts for the AR-LLM configuration included both the question and contextual information. In contrast, the <span id="S4.SS5.p2.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> training set was constructed following the approach outlined in Section <a href="#S3.SS1" title="3.1 Training Adapt-LLM ‚Ä£ 3 Adaptive Retrieval LLM (Adapt-LLM) ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, employing a two-step process. As a result of this process, the 74.72% of the questions in NQ are marked with the <math id="S4.SS5.p2.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mo stretchy="false" id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.SS5.p2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.SS5.p2.2.m2.1a"><mo stretchy="false" id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\rangle</annotation></semantics></math> token, whereas the 87.49% questions are marked for SQuAD.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">The trained models were then tested on the PopQA dataset to evaluate their performance in a real-world question answering scenario. During inference, the NR-LLM and AR-LLM models were utilized as is, with corresponding instruction prompts provided, and outputs expected to be answers to the questions. Conversely, for the <span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model, we followed the same prompt procedure as explained in Section <a href="#S3.SS2" title="3.2 Inference ‚Ä£ 3 Adaptive Retrieval LLM (Adapt-LLM) ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">The generated answers are then compared to the set of possible answers for each question, which are already annotated in the PopQA test set. The evaluation metric used is <span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">Exact Match Accuracy</span>, which measures the percentage of generated outputs that exactly match one of the possible answers for the corresponding question.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the results of this experiment, illustrating the performance of the Llama-2 model across the different configurations and datasets. Across both the NQ and SQuAD training datasets, the <span id="S4.SS5.p5.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> configuration consistently outperforms the Never Retrieve (NR-LLM) and Always Retrieve (AR-LLM) configurations on the PopQA test set. As can be observed, NR-LLM exhibits the lowest performance among the models, with an accuracy difference of approximately 14 absolute points compared to the other configurations. This disparity suggests that the parametric memory of Llama-2 alone is not sufficient for effectively answering PopQA questions.</p>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<p id="S4.SS5.p6.1" class="ltx_p">The differences between AR-LLM and <span id="S4.SS5.p6.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> are narrower. Specifically, the <span id="S4.SS5.p6.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> configuration achieves an accuracy of 36.77% and 38.15% on the PopQA test set when trained on the NQ and SQuAD datasets, respectively, compared to 35.86% and 36.59% for the AR-LLM configuration. Across both training datasets, <span id="S4.SS5.p6.1.3" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> outperforms AR-LLM, with the largest difference observed when trained on SQuAD.</p>
</div>
<div id="S4.SS5.p7" class="ltx_para">
<p id="S4.SS5.p7.1" class="ltx_p">All in all, these results underscore the efficacy of the adaptive retrieval approach in dynamically determining the necessity of context for accurate question answering, resulting in improved performance compared to fixed strategies of always or never retrieving context.</p>
</div>
<div id="S4.SS5.p8" class="ltx_para">
<p id="S4.SS5.p8.1" class="ltx_p">Although the disparity between training <span id="S4.SS5.p8.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> on NQ or SQuAD is relatively minor, we try to determine the suitability of a training set for a given evaluation set. While both training sets (NQ and SQuAD) and the evaluation set (PopQA) are based on Wikipedia, subtle differences may exist.</p>
</div>
<div id="S4.SS5.p9" class="ltx_para">
<p id="S4.SS5.p9.2" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.2 Base Model ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides insights into the characteristics of the three datasets involved in our experimental procedure, including the total number of questions and the average number of words per question and answer. While NQ appears to be closer to PopQA in terms of question and answer lengths, the key factor influencing the better results of training <span id="S4.SS5.p9.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> on SQuAD may be the number of questions in the training dataset (<math id="S4.SS5.p9.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS5.p9.1.m1.1a"><mo id="S4.SS5.p9.1.m1.1.1" xref="S4.SS5.p9.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p9.1.m1.1.1.cmml" xref="S4.SS5.p9.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.1.m1.1c">\sim</annotation></semantics></math>87K in SQuAD and <math id="S4.SS5.p9.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS5.p9.2.m2.1a"><mo id="S4.SS5.p9.2.m2.1.1" xref="S4.SS5.p9.2.m2.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.2.m2.1b"><csymbol cd="latexml" id="S4.SS5.p9.2.m2.1.1.cmml" xref="S4.SS5.p9.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.2.m2.1c">\sim</annotation></semantics></math>58K in NQ). Further analyses are required to elucidate the factors that render a training dataset more suitable for a given target dataset (which is beyond the scope of our study), but these results suggest that scale may play once again a crucial role.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<img src="/html/2404.19705/assets/img/istogramma_pop_score_use_ret_nq.png" id="S4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<img src="/html/2404.19705/assets/img/istogramma_pop_score_use_ret_squad.png" id="S4.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Histograms depicting the proportion of questions where <span id="S4.F2.5.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> trained on NQ (left) and <span id="S4.F2.6.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> trained on SQuAD (right) ask for extra context for different popularity score intervals.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Contextual Retrieval Decision Analysis</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">In this experiment, our objective is to once again evaluate the effectiveness of the <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model, this time focusing on its ability to accurately determine when additional context is needed. For this purpose, we adhere to the following steps:</p>
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.2" class="ltx_p">We conduct inference on the <span id="S4.I3.i1.p1.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model using the PopQA test set, prompting it to either return an answer directly or indicate the need for additional context by returning <math id="S4.I3.i1.p1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.I3.i1.p1.1.m1.1a"><mo stretchy="false" id="S4.I3.i1.p1.1.m1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><ci id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.I3.i1.p1.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.I3.i1.p1.2.m2.1a"><mo stretchy="false" id="S4.I3.i1.p1.2.m2.1.1" xref="S4.I3.i1.p1.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><ci id="S4.I3.i1.p1.2.m2.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">\rangle</annotation></semantics></math>.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.2" class="ltx_p">In the case of receiving a <math id="S4.I3.i2.p1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.I3.i2.p1.1.m1.1a"><mo stretchy="false" id="S4.I3.i2.p1.1.m1.1.1" xref="S4.I3.i2.p1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><ci id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.I3.i2.p1.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.I3.i2.p1.2.m2.1a"><mo stretchy="false" id="S4.I3.i2.p1.2.m2.1.1" xref="S4.I3.i2.p1.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><ci id="S4.I3.i2.p1.2.m2.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">\rangle</annotation></semantics></math> response from the <span id="S4.I3.i2.p1.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model, we proceed with the following steps:</p>
<ol id="S4.I3.i2.I1" class="ltx_enumerate">
<li id="S4.I3.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1.</span> 
<div id="S4.I3.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I3.i2.I1.i1.p1.1" class="ltx_p">We conduct inference on the <span id="S4.I3.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model, prompting it to return an answer given the context obtained from the IR system.</p>
</div>
</li>
<li id="S4.I3.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2.</span> 
<div id="S4.I3.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I3.i2.I1.i2.p1.1" class="ltx_p">We also conduct inference on the NR-LLM model with the instruction to provide an answer directly without additional context.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">If the <span id="S4.I3.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model decides to answer the question directly relying only on its parametric memory:</p>
<ol id="S4.I3.i3.I1" class="ltx_enumerate">
<li id="S4.I3.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.1.</span> 
<div id="S4.I3.i3.I1.i1.p1" class="ltx_para">
<p id="S4.I3.i3.I1.i1.p1.1" class="ltx_p">We conduct inference on the <span id="S4.I3.i3.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model, prompting it to return the answer without providing context.</p>
</div>
</li>
<li id="S4.I3.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.2.</span> 
<div id="S4.I3.i3.I1.i2.p1" class="ltx_para">
<p id="S4.I3.i3.I1.i2.p1.1" class="ltx_p">We conduct inference on the AR-LLM model with the instruction to provide an answer using the context retrieved by the IR system.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.2" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Base Model ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results of this experiment. The first thing to note is that the <span id="S4.SS6.p2.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model generates the <math id="S4.SS6.p2.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.SS6.p2.1.m1.1a"><mo stretchy="false" id="S4.SS6.p2.1.m1.1.1" xref="S4.SS6.p2.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><ci id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.SS6.p2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.SS6.p2.2.m2.1a"><mo stretchy="false" id="S4.SS6.p2.2.m2.1.1" xref="S4.SS6.p2.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.1b"><ci id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.1c">\rangle</annotation></semantics></math> token for approximately 82-83% of the questions in the PopQA dataset, with similar ratios observed across both training datasets. This observation aligns with the low performance of the NR-LLM configuration demonstrated in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.2" class="ltx_p">However, <span id="S4.SS6.p3.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> consistently determines when additional context is required to answer a question accurately. Across both the NQ and SQuAD training datasets, <span id="S4.SS6.p3.2.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> exhibits significantly higher accuracy when retrieving context compared to the NR-LLM model‚Äôs accuracy without context (as indicated in the <math id="S4.SS6.p3.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.SS6.p3.1.m1.1a"><mo stretchy="false" id="S4.SS6.p3.1.m1.1.1" xref="S4.SS6.p3.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p3.1.m1.1b"><ci id="S4.SS6.p3.1.m1.1.1.cmml" xref="S4.SS6.p3.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p3.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.SS6.p3.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.SS6.p3.2.m2.1a"><mo stretchy="false" id="S4.SS6.p3.2.m2.1.1" xref="S4.SS6.p3.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p3.2.m2.1b"><ci id="S4.SS6.p3.2.m2.1.1.cmml" xref="S4.SS6.p3.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p3.2.m2.1c">\rangle</annotation></semantics></math> column of Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Base Model ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Specifically, for the NQ dataset, the accuracy of the <span id="S4.SS6.p3.2.3" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model when requesting context is 33.04%, whereas the accuracy of the NR-LLM model without context retrieval is notably lower at 14.65%. Similarly, for the SQuAD dataset, <span id="S4.SS6.p3.2.4" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> achieves an accuracy of 33.40% with context retrieval, whereas the NR-LLM model‚Äôs accuracy without context is substantially lower at 9.94%.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Passages</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">SQuAD Dev</span></th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">NQ Dev</span></th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.2.2.1.1" class="ltx_text ltx_font_bold">Acc.</span></th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.2.2.2.1" class="ltx_text ltx_font_bold">Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<td id="S4.T4.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">Gold</td>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.2.1" class="ltx_text ltx_font_bold">89.42%</span></td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.3.1" class="ltx_text ltx_font_bold">69.76%</span></td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<td id="S4.T4.1.4.2.1" class="ltx_td ltx_align_center ltx_border_bb">Contriever</td>
<td id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">22.49</td>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">27.04%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison of <span id="S4.T4.3.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> for the SQuAD and NQ dev sets, when using the gold passages provided by the datasets and when using the best passage retrieved by Contriever.</figcaption>
</figure>
<div id="S4.SS6.p4" class="ltx_para">
<p id="S4.SS6.p4.2" class="ltx_p">Finally, the last column of Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Base Model ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (No <math id="S4.SS6.p4.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.SS6.p4.1.m1.1a"><mo stretchy="false" id="S4.SS6.p4.1.m1.1.1" xref="S4.SS6.p4.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.1.m1.1b"><ci id="S4.SS6.p4.1.m1.1.1.cmml" xref="S4.SS6.p4.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.SS6.p4.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.SS6.p4.2.m2.1a"><mo stretchy="false" id="S4.SS6.p4.2.m2.1.1" xref="S4.SS6.p4.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.2.m2.1b"><ci id="S4.SS6.p4.2.m2.1.1.cmml" xref="S4.SS6.p4.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.2.m2.1c">\rangle</annotation></semantics></math>) shows the performance of <span id="S4.SS6.p4.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> when answering questions based solely on its parametric memory. As can be seen, accuracies above 62% are obtained when no context is utilized, providing further evidence that <span id="S4.SS6.p4.2.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> effectively discerns between retrieving context and providing direct answers to questions. Additionally, we evaluate the performance of these questions when context is added to the input, revealing significant decreases in accuracy of up to 7 absolute points.</p>
</div>
<div id="S4.SS6.p5" class="ltx_para">
<p id="S4.SS6.p5.1" class="ltx_p">These findings provide insights into the effectiveness of the decision-making process employed by the <span id="S4.SS6.p5.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model in determining the necessity of additional context for accurate response generation and present empirical evidence of the necessity of performing dynamic context retrieval in improving the accuracy of question answering models.</p>
</div>
<div id="S4.SS6.p6" class="ltx_para">
<p id="S4.SS6.p6.1" class="ltx_p">However, it is notable that the overall performance of the model when answering questions with retrieved context, as observed in Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Base Model ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (approximately 33%), is relatively low. To further explore this observation, we conduct an additional experiment: evaluating <span id="S4.SS6.p6.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> (both versions trained on NQ and SQuAD) on the NQ and SQuAD development splits, comparing performance when using the gold passages of the dataset and the context retrieved by our IR system, Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Unfortunately, PopQA does not provide the gold passages, so direct evaluation there was not possible.</p>
</div>
<div id="S4.SS6.p7" class="ltx_para">
<p id="S4.SS6.p7.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.6 Contextual Retrieval Decision Analysis ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results of this experiment. A significant performance difference is observed between using the gold passage and the top passage retrieved by Contriever for both datasets (approximately 67 absolute points for SQuAD and 42 for NQ). This indicates that Contriever, and current IR systems in general, do not consistently retrieve the most relevant passage to answer a given question. This observation underscores the importance of retrieving multiple documents as context, as seen in the most successful open-domain QA systems <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and highlights its impact on the overall performance of <span id="S4.SS6.p7.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> in PopQA.</p>
</div>
<div id="S4.SS6.p8" class="ltx_para">
<p id="S4.SS6.p8.2" class="ltx_p">To further validate the behavior of <span id="S4.SS6.p8.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> when requesting additional context, Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.5 Validating the Adaptive Retrieval Approach ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the proportion of questions for which our model generates the <math id="S4.SS6.p8.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.SS6.p8.1.m1.1a"><mo stretchy="false" id="S4.SS6.p8.1.m1.1.1" xref="S4.SS6.p8.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.1.m1.1b"><ci id="S4.SS6.p8.1.m1.1.1.cmml" xref="S4.SS6.p8.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.SS6.p8.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.SS6.p8.2.m2.1a"><mo stretchy="false" id="S4.SS6.p8.2.m2.1.1" xref="S4.SS6.p8.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.2.m2.1b"><ci id="S4.SS6.p8.2.m2.1.1.cmml" xref="S4.SS6.p8.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.2.m2.1c">\rangle</annotation></semantics></math> token, aggregated by popularity score intervals (left image for <span id="S4.SS6.p8.2.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> trained on NQ and right image for SQuAD). <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> suggest that high-popularity questions can be adequately answered using the parametric memory of the LLM, while lower popularity scores necessitate extra context. In Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.5 Validating the Adaptive Retrieval Approach ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe this pattern for both versions of <span id="S4.SS6.p8.2.3" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>, indicating that our model, despite lacking access to popularity scores during training or inference, has learned effective criteria for requesting additional context.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Comparison with state-of-the-art methods</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">We conducted a comparative analysis between our <span id="S4.SS7.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model and the current state-of-the-art approach for PopQA proposed by <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Their methodology relies on the popularity score annotated in the PopQA dataset to determine whether a question requires additional context. To establish the optimal threshold for determining question popularity, <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> split the PopQA dataset into 75% as a development set for threshold determination and 25% as a test set. In the original paper, they apply this methodology to various LLMs available at that moment (Llama-2 was not released yet).</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">To ensure a fair comparison between <span id="S4.SS7.p2.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> and the popularity-based method, we replicated their approach using the Llama-2 7B model to determine the best popularity score threshold (found to be 707,000) using the same PopQA development set. This allowed us to obtain results consistent with their methodology while utilizing our base LLM. Similar to the original results in <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> when using smaller models, the popularity score threshold is almost equivalent to always retrieving contextual information for Llama-2 7B. The IR usage is of 99.86% as presented in Table <a href="#S4.T5" title="Table 5 ‚Ä£ 4.7 Comparison with state-of-the-art methods ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This clearly shows how the popularity score method struggles with smaller size models, being <span id="S4.SS7.p2.1.2" class="ltx_text ltx_font_smallcaps">GPT-3 davinci-003</span> the only model to get a IR usage below 80% in the original paper when using adaptive retrieval with the Contriever. Subsequently, we evaluated our <span id="S4.SS7.p2.1.3" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> configuration on the same 25% test set split and compared the outcomes with those obtained using the method described by <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This systematic comparison enabled us to assess the efficacy of our <span id="S4.SS7.p2.1.4" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model in relation to the current state of the art.</p>
</div>
<div id="S4.SS7.p3" class="ltx_para">
<p id="S4.SS7.p3.1" class="ltx_p">The results of this experiment are presented in Table <a href="#S4.T5" title="Table 5 ‚Ä£ 4.7 Comparison with state-of-the-art methods ‚Ä£ 4 Experiments and Results ‚Ä£ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We observe comparable performance between the replicated approach of <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and <span id="S4.SS7.p3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> when trained on NQ and SQuAD datasets and tested on the 25% subset of PopQA. It‚Äôs worth mentioning that <span id="S4.SS7.p3.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> does not utilize any information from PopQA, unlike <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, who directly use the popularity score and a 75% portion of PopQA dataset to find an optimal value for that popularity score. This methodology is not generalizable to other open-domain question answering tasks since the popularity score is a unique feature of PopQA. However, <span id="S4.SS7.p3.1.3" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> can be applied to any similar dataset. Given these characteristics, we believe that the results obtained by <span id="S4.SS7.p3.1.4" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> are even more significant, offering comparable performance to an approach that utilizes dataset-specific information. These findings substantiate the validity of our approach, demonstrating its effectiveness even when trained on datasets different from the one used for testing.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Configuration</span></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">IR usage</span></th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">Popularity Score</span></td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">99.86%</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">36.81%</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM (NQ)</span></td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center">87.22%</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center">35.30%</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM (SQuAD)</span></td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">83.99%</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.4.3.3.1" class="ltx_text ltx_font_bold">37.29%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparison of Llama-2 base models trained on the SQuAD and NQ datasets for the <span id="S4.T5.4.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> and <span id="S4.T5.5.2" class="ltx_text ltx_font_smallcaps">Popularity Score</span> configurations. The later mimics the methodology proposed by <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. [<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> with the Llama-2 LLM as the base model.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">In this paper, we introduce <span id="S5.p1.2.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>, a LLM which learns to discern when additional context is necessary for answering a question, rather than relying solely on its parametric memory. <span id="S5.p1.2.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> is the result of fine-tuning a base LLM on an open-domain question answering dataset that has been modified to differentiate between questions answerable with the LLM‚Äôs parametric memory alone and those requiring supplementary context. To construct these training datasets, we initially subject the base LLM to zero-shot evaluation to determine its accuracy in answering questions. For questions where the model‚Äôs response is incorrect, we train the LLM to generate a special token, <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S5.p1.1.m1.1a"><mo stretchy="false" id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S5.p1.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S5.p1.2.m2.1a"><mo stretchy="false" id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\rangle</annotation></semantics></math>, indicating the need for additional context.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Through extensive experiments conducted on the PopQA dataset, we show that <span id="S5.p2.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> performs better than its two fixed alternatives: never retrieving and always retrieving relevant context information. Furthermore, our findings highlight <span id="S5.p2.1.2" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>‚Äôs capability to effectively discern the necessity of additional context, which is the primary objective of this work.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">For future investigations, we propose exploring methods to enhance performance when utilizing an IR system, such as incorporating learnable sequential retrieval techniques. Furthermore, we believe it would be valuable to conduct a more in-depth analysis of the interaction between training and testing datasets in the development of <span id="S5.p3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> systems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
J.&nbsp;Achiam, S.&nbsp;Adler, S.&nbsp;Agarwal, L.&nbsp;Ahmad, I.&nbsp;Akkaya, F.&nbsp;L. Aleman, D.&nbsp;Almeida, J.&nbsp;Altenschmidt, S.&nbsp;Altman, S.&nbsp;Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber [2024]</span>
<span class="ltx_bibblock">
Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber.

</span>
<span class="ltx_bibblock">RAG makes LLMs better and equal.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.pinecone.io/blog/rag-study/</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
S.&nbsp;Barnett, S.&nbsp;Kurniawan, S.&nbsp;Thudumu, Z.&nbsp;Brannelly, and M.&nbsp;Abdelrazek.

</span>
<span class="ltx_bibblock">Seven failure points when engineering a retrieval augmented generation system.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.05856</em>, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berger et&nbsp;al. [2000]</span>
<span class="ltx_bibblock">
A.&nbsp;Berger, R.&nbsp;Caruana, D.&nbsp;Cohn, D.&nbsp;Freitag, and V.&nbsp;Mittal.

</span>
<span class="ltx_bibblock">Bridging the lexical chasm: statistical approaches to answer-finding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</em>, pages 192‚Äì199, 2000.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others [2022]</span>
<span class="ltx_bibblock">
Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 2206‚Äì2240. PMLR, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others [2020]</span>
<span class="ltx_bibblock">
Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877‚Äì1901, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke [2018]</span>
<span class="ltx_bibblock">
Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke.

</span>
<span class="ltx_bibblock">QuAC: Question Answering in Context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2174‚Äì2184, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
T.&nbsp;Gao, X.&nbsp;Yao, and D.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021</em>, pages 6894‚Äì6910. Association for Computational Linguistics (ACL), 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen [2023]</span>
<span class="ltx_bibblock">
Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave [2022]</span>
<span class="ltx_bibblock">
Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei [2020]</span>
<span class="ltx_bibblock">
Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei.

</span>
<span class="ltx_bibblock">REALM: retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine Learning</em>. JMLR.org, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard, Gautier and Grave, Edouard [2021]</span>
<span class="ltx_bibblock">
Izacard, Gautier and Grave, Edouard.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">EACL 2021-16th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 874‚Äì880. Association for Computational Linguistics, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale [2023]</span>
<span class="ltx_bibblock">
Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1‚Äì38, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Roux, A.&nbsp;Mensch, B.&nbsp;Savary, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l. Casas, E.&nbsp;B. Hanna, F.&nbsp;Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
V.&nbsp;Karpukhin, B.&nbsp;Oguz, S.&nbsp;Min, P.&nbsp;Lewis, L.&nbsp;Wu, S.&nbsp;Edunov, D.&nbsp;Chen, and W.-t. Yih.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769‚Äì6781, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others [2019]</span>
<span class="ltx_bibblock">
Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:453‚Äì466, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K√ºttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt√§schel, Tim and others [2020]</span>
<span class="ltx_bibblock">
Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K√ºttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt√§schel, Tim and others.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:9459‚Äì9474, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
P.&nbsp;Liang, R.&nbsp;Bommasani, T.&nbsp;Lee, D.&nbsp;Tsipras, D.&nbsp;Soylu, M.&nbsp;Yasunaga, Y.&nbsp;Zhang, D.&nbsp;Narayanan, Y.&nbsp;Wu, A.&nbsp;Kumar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin, Stephanie and Hilton, Jacob and Evans, Owain [2022]</span>
<span class="ltx_bibblock">
Lin, Stephanie and Hilton, Jacob and Evans, Owain.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring How Models Mimic Human Falsehoods.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214‚Äì3252, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
A.&nbsp;T. Mallen, A.&nbsp;Asai, V.&nbsp;Zhong, R.&nbsp;Das, D.&nbsp;Khashabi, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">The 61st Annual Meeting Of The Association For Computational Linguistics</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others [2021]</span>
<span class="ltx_bibblock">
Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.09332</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li [2016]</span>
<span class="ltx_bibblock">
Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li.

</span>
<span class="ltx_bibblock">Ms marco: A human-generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy [2016]</span>
<span class="ltx_bibblock">
Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, pages 2383‚Äì2392, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav [2023]</span>
<span class="ltx_bibblock">
Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1316‚Äì1331, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap, J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych [2019]</span>
<span class="ltx_bibblock">
N.&nbsp;Reimers and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982‚Äì3992, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et&nbsp;al. [2009]</span>
<span class="ltx_bibblock">
S.&nbsp;Robertson, H.&nbsp;Zaragoza, et&nbsp;al.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends¬Æ in Information Retrieval</em>, 3(4):333‚Äì389, 2009.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dess√¨, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro, L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun [2022]</span>
<span class="ltx_bibblock">
Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun.

</span>
<span class="ltx_bibblock">Two-Step Question Retrieval for Open-Domain QA.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">60th Annual Meeting of the Association for Computational Linguistics, ACL 2022</em>, pages 1487‚Äì1492. Association for Computational Linguistics, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B [2023]</span>
<span class="ltx_bibblock">
Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.

</span>
<span class="ltx_bibblock">Stanford alpaca: an instruction-following llama model (2023).

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">URL https://github. com/tatsu-lab/stanford_alpaca</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
N.&nbsp;Thakur, N.&nbsp;Reimers, A.&nbsp;R√ºckl√©, A.&nbsp;Srivastava, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix, B.&nbsp;Rozi√®re, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei, N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
F.&nbsp;Zhu, W.&nbsp;Lei, C.&nbsp;Wang, J.&nbsp;Zheng, S.&nbsp;Poria, and T.-S. Chua.

</span>
<span class="ltx_bibblock">Retrieving and reading: A comprehensive survey on open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00774</em>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.19704" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.19705" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.19705">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.19705" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.19706" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 16:40:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
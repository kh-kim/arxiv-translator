<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.19705] When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</title><meta property="og:description" content="In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.19705">

<!--Generated on Sun May  5 16:40:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiziano&nbsp;Labruna<span id="id5.1.id1" class="ltx_ERROR undefined">\orcid</span>0000-0001-7713-7679
</span><span class="ltx_author_notes">Corresponding Author. Email: tlabruna@fbk.eu.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jon Ander&nbsp;Campos<span id="id6.1.id1" class="ltx_ERROR undefined">\orcid</span>0000-0002-1447-5870
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gorka&nbsp;Azkune<span id="id7.1.id1" class="ltx_ERROR undefined">\orcid</span>0000-0002-2506-7426
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Bozen-Bolzano
</span>
<span class="ltx_contact ltx_role_address">Fondazione Bruno Kessler
</span>
<span class="ltx_contact ltx_role_address">HiTZ Center - Ixa, University of the Basque Country UPV/EHU
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.4">본 논문에서는 LLM(Large Language Models)이 주어진 질문에 답하기 위해 추가적인 컨텍스트가 필요할 때 특히 기성 정보 검색(Off-the-Shelf Information Retrieval, IR) 시스템을 사용하는 방법을 효과적으로 학습할 수 있음을 보인다. IR 시스템의 성능을 고려할 때, 질문 응답을 위한 최적 전략은 항상 외부 정보 검색을 수반하는 것은 아니며, 오히려 LLM 자체의 매개변수 메모리를 활용하는 것을 종종 수반한다. 이전 연구에서는 PopQA 데이터 세트에서 이러한 현상을 확인했는데, 가장 인기 있는 질문은 LLM의 매개변수 메모리를 사용하여 효과적으로 해결되는 반면 덜 인기 있는 질문은 IR 시스템 사용을 필요로 한다. 다음으로, 우리는 기존의 오픈 도메인 질의 응답 데이터 세트를 활용하여 LLMs에 대한 맞춤형 훈련 접근법을 제안한다. 여기서, LLMs는 질문에 대한 답변을 알지 못하는 경우 특수 토큰인 <math alttext="\langle" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" stretchy="false" xref="id1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" stretchy="false" xref="id2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\rangle</annotation></semantics></math>를 생성하도록 훈련된다. PopQA 데이터 세트에 대한 적응형 검색 LLM(<span class="ltx_text ltx_font_smallcaps" id="id4.4.1">Adapt-LLM</span>)의 평가는 (i) 모든 질문에 대한 정보를 검색하는 것, (ii) 항상 LLM의 파라메트릭 메모리를 사용하는 것, (iii) 검색기를 사용할 시기를 결정하기 위해 인기 임계값을 사용하는 것의 세 가지 구성에서 동일한 LLM에 대한 개선을 보여준다. 분석을 통해 <span class="ltx_text ltx_font_smallcaps" id="id4.4.2">Adapt-LLM</span>은 IR에 대한 필요성을 나타내는 질문에 답하는 방법을 모른다고 결정할 때 <math alttext="\langle" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" stretchy="false" xref="id3.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" stretchy="false" xref="id4.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><ci id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\rangle</annotation></semantics></math> 토큰을 생성할 수 있는 반면, 파라메트릭 메모리에만 의존하도록 선택할 때 현저하게 높은 정확도 수준을 달성한다는 것을 보여준다.</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\paperid</span>
<p class="ltx_p" id="p1.2">123</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">질의응답(QA)의 과제는 자연어 이해 연구의 초점으로 남아 있다. QA 모델을 평가하기 위한 벤치마크 역할을 하는 많은 다른 데이터 세트가 있는데, 예를 들어 NQ(Natural Questions) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite> 또는 QuAC <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite> 등이 있다. 오늘날, 대형 언어 모델(LLM)은 이러한 벤치마크에서 전통적인 방법을 일관되게 능가하여 놀라운 성능을 보여준다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">전형적으로, 질의 응답을 위해 LLMs를 활용하는 두 가지 주요 접근법이 있다:</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">(i) <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Closed Book Question Answering</span>: 이 접근법은 성능을 향상시키기 위해 명령어 튜닝 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite> 또는 소수의shot 프롬프트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite>와 같은 전략을 포함한다. 여기서 LLM은 질문에 답하기 위해 매개변수 기억에만 의존한다. 그러나 이러한 모수적 기억은 훈련 코퍼스에 전적으로 기반을 두고 있기 때문에 내재적 한계를 가지고 있으며, 이는 예를 들어 훈련 과정 후에 발생하는 사건에 대해 구식일 수 있다는 것을 의미한다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">(ii) <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Open Book Question Answering</span>: 이 접근법에서 LLM은 IR(Information Retriever) 시스템 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>, <a class="ltx_ref" href="#bib.bib35" title="">35</a>]</cite>와 결합된다. IR 시스템을 활용함으로써 LLM은 관련 컨텍스트를 검색하여 이해도를 보완하고 보다 정확한 답변을 제공할 수 있다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">그러나 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>에 의해 수행된 연구는 질문 응답 전략의 복잡성을 조명하여 최적의 접근 방식이 항상 IR 시스템의 활용을 포함한다는 개념에 도전한다. 인기 점수로 주석이 달린 14,000개의 질문으로 구성된 PopQA 데이터 세트의 도입을 통해, 그들은 모수적 기억에만 의존하는 LLM이 인기 높은 질문을 다루는 데 탁월하지만 IR을 사용하면 인기 낮은 질문에 대한 효능이 감소한다는 것을 보여주었다.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19705/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span><span class="ltx_text ltx_font_smallcaps" id="S1.F1.6.1">Adapt-LLM</span>step-by-step: 주어진 질문(단계 1), LLM이 질문에 직접 답할지(단계 3) 또는 추가 컨텍스트 정보를 요청할지를 결정(단계 2), 특수 <math alttext="\langle" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mo id="S1.F1.3.m1.1.1" stretchy="false" xref="S1.F1.3.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><mo id="S1.F1.4.m2.1.1" stretchy="false" xref="S1.F1.4.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\rangle</annotation></semantics></math> 토큰을 생성, 나중에, off-the-shelf IR 시스템이 관련 컨텍스트를 검색(단계 4)하는데 사용되며, 이는 질문과 함께 사용되어 최종 답변에 대한 LLM을 다시 프롬프트한다(단계 5).</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">그들의 연구 결과는 LLM이 인기 높은 질문에 모수 메모리를 활용하지만 인기 낮은 질문에 답하기 위해 관련 컨텍스트를 검색하기 위해 기성 IR 시스템을 사용하는 하이브리드 접근법의 중요성을 강조한다. 그들의 방법론의 핵심은 IR 시스템을 사용해야 하는지 여부를 결정하는 데 사용되는 고정된 인기 점수 임계값을 설정하는 것이다.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p" id="S1.p7.1">그러나 많은 경우에, 질의 응답 데이터 세트는 인기 점수를 포함하지 않으므로, 그러한 점수에 의존하는 것은 일반화할 수 있는 접근법이 아니다. 이러한 제한으로 인해, 본 연구는 LLMs가 개선된 질문 답변을 위해 IR 시스템을 사용할 시기를 자율적으로 결정할 수 있는지 여부를 다루는 것을 목표로 한다. 이를 조사하기 위해 개방형 질문 응답 데이터 세트를 사용하여 LLM에 대한 평가를 수행하여 LLM이 정확한 응답을 제공하는 질문과 응답이 잘못된 질문을 식별한다.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p class="ltx_p" id="S1.p8.2">구체적으로, LLM 응답이 잘못된 질문의 경우, 추가 컨텍스트의 필요성을 나타내는 특수 토큰인 <math alttext="\langle" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" stretchy="false" xref="S1.p8.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S1.p8.2.m2.1"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" stretchy="false" xref="S1.p8.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><ci id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\rangle</annotation></semantics></math>로 주석을 달게 된다. 이어서, 우리는 이러한 주석을 사용하여 훈련 목적을 위해 맞춤화된 새로운 데이터 세트를 구성하며, 여기서 LLM이 답변에 대해 확신하거나 질문에 답하는 데 유용하다고 믿는 컨텍스트를 요구하는 경우 직접 답하도록 가르친다(도 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a> 참조). 우리의 가설은 이 훈련 과정을 통해 LLM이 질문에 답하기 위해 추가 컨텍스트가 필요할 때 IR 시스템을 사용하도록 학습하므로 <span class="ltx_text ltx_font_smallcaps" id="S1.p8.2.1">Adapt-LLM</span>이라는 이름을 붙인다.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p class="ltx_p" id="S1.p9.1">가설을 검증하기 위해 하이브리드 검색 전략을 벤치마킹하는 데 적합한 플랫폼을 제공하기 때문에 PopQA 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>에 대해 여러 실험을 수행했다. 이러한 실험들의 결과로서 우리는 다음과 같은 것을 발견한다:</p>
</div>
<div id="S1.p10" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">Adapt-LLM</span>은 (i) 모든 질문에 대해 IR 시스템을 사용하고 (ii) LLM의 파라메트릭 메모리에만 의존하는 것과 같은 질문 응답을 위한 전형적인 고정 전략을 일관되게 능가한다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">Adapt-LLM</span>은 인기 점수 또는 유사한 메트릭을 활용하지 않더라도 IR 시스템을 사용할 시기를 결정하기 위해 인기 점수에 의존하는 전략에 필적하는 성능을 보여줍니다. 인기 점수는 PopQA 데이터 세트의 고유한 기능으로, 다른 오픈 도메인 질문 응답 데이터 세트에 적용할 수 없게 만든다는 점에 주목할 필요가 있다.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.1">Adapt-LLM</span>이 추가 정보를 검색하기로 결정하면 컨텍스트와 함께 얻은 결과가 그렇지 않은 결과보다 훨씬 좋습니다. 마찬가지로 <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.2">Adapt-LLM</span>이 파라메트릭 메모리에 의존하는 질문에 직접 응답할 때 높은 정확도를 달성합니다. 이러한 관찰은 모델이 정보를 검색할 때와 더 이상의 컨텍스트 없이 질문에 답할 수 있는 때를 효과적으로 식별한다는 것을 나타낸다.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i4.p1.1.1">Adapt-LLM</span>의 성능에 대한 기본 병목 현상은 IR 시스템에 있습니다. <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i4.p1.1.2">Adapt-LLM</span>은 IR 시스템에 의해 검색된 패시지에 비해 골드 패시지로 훨씬 더 높은 성능을 달성한다.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p11" class="ltx_para">
<p class="ltx_p" id="S1.p11.1">본 연구의 결과는 질의응답을 위한 LLM의 성능을 향상시키기 위한 적응적 검색 전략의 중요성을 강조한다. 추가 컨텍스트를 검색할 시기를 동적으로 결정하기 위해 <span class="ltx_text ltx_font_smallcaps" id="S1.p11.1.1">Adapt-LLM</span>을 학습하여 필요한 경우에만 외부 정보 소스를 효과적으로 활용하는 방법을 LLM에 가르치는 가능성을 보여 줍니다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>는 질문 응답 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>, <a class="ltx_ref" href="#bib.bib12" title="">12</a>, <a class="ltx_ref" href="#bib.bib30" title="">30</a>, <a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite>, 진실성 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>, <a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite> 및 언어 모델링 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib5" title="">5</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite> 등 매우 다양한 NLP 영역에서 개선점을 보였다. 검색된 텍스트 청크에 모델 생성을 접지하는 기능은 또한 더 작은 모델이 더 큰 모델 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>의 성능과 일치하도록 했다. 더욱이, LLM을 훈련하는 데 드는 매우 높은 비용으로 인해, RAG는 새로운 사실 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>를 통합하기 위해 모델을 주기적으로 재훈련할 필요가 없이 새로운 정보로 업데이트된 상태를 유지하는 표준 방법이 되었다.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p" id="S2.p2.1">검색과 함께 LLMs를 증가시키는 것이 LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>, <a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>의 현재 생성을 위한 필수 단계라고 하더라도 비용이 수반된다. TF-IDF 또는 BM-25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>와 같은 전통적인 검색 방법은 키워드가 겹치는 문서만 검색할 수 있고 어휘 격차 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>를 겪는다. 이러한 문제를 해결하기 위해, 많은 사전 훈련된 트랜스포머 인코더 기반 밀집 모델들이 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>, <a class="ltx_ref" href="#bib.bib27" title="">27</a>, <a class="ltx_ref" href="#bib.bib16" title="">16</a>, <a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>로 제안되었다. 훈련된 신경망 모델은 다양한 검색 벤치마크에서 좋은 성능을 보였지만 새로운 도메인 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>에 대한 제로 샷 설정에서는 여전히 어려움을 겪고 있다. 검색 엔진의 품질은 검색 강화 모델에 필수적인데, 이는 모델 성능의 상한을 설정할 것이기 때문이다. 또한, 특히 대상 문서 인덱스가 큰 경우 검색 엔진의 사용은 모델의 대기 시간을 크게 증가시키고 실시간 응용 프로그램 사용자 경험 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite>를 손상시킬 수 있다.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p" id="S2.p3.1">반면에, 모델들이 스케일링을 계속함에 따라, 그들의 파라미터들에 인코딩된 세계 지식은 너무 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>를 수행한다. 이전의 많은 노력들은 언어 모델들이 태스크 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>, <a class="ltx_ref" href="#bib.bib1" title="">1</a>, <a class="ltx_ref" href="#bib.bib33" title="">33</a>, <a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>를 풀기 위해 그들의 파라메트릭 지식을 사용할 때, 오픈 도메인 질의 응답과 같은 태스크들에 대해 상당한 양의 세계 지식을 암기할 수 있고 경쟁적 성능을 달성할 수 있다는 것을 보여주었다.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p" id="S2.p4.1">이 모든 것에 의해 동기화된 적응형 접근법은 새로운 솔루션 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>, <a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>로 제안되었다. 이 접근법에서, 태스크에 대한 솔루션이 모델의 파라미터들에서 인코딩되면, 모델은 솔루션을 생성하기 위해 직접 사용될 것이다. 반대로 모델의 지식에 답안이 인코딩되어 있지 않다면, 답안 생성은 외부 지식으로 증강될 것이다.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p" id="S2.p5.1">최근 <cite class="ltx_cite ltx_citemacro_citet">Schick et al. [<a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>는 계산기, 검색 엔진, 캘린더 등을 포함한 간단한 API 호출을 통해 외부 도구를 사용하는 방법과 시기를 스스로 가르칠 수 있는 모델인 Toolformer를 제안하였다. 자기 학습 과정은 LLM을 프롬프트하여 풍부한 합성 텍스트 전용 말뭉치를 기반으로 한다. LLM은 먼저 감독되지 않은 코퍼스 위에 인라인 API 호출을 추가합니다. 그런 다음 API 호출의 실행이 향후 토큰을 예측하는 데 도움이 되는지 여부를 평가 하 여 이러한 API 호출의 유효성을 검사 합니다. 이 비감독 방법은 비증강 LLM과 비교할 때 다양한 작업에서 모델 성능을 크게 향상시키지만 모델을 도구보다 더 많이 사용하게 만든다. 예로서, QA 태스크에 대해 모델은 사례들의 99.3%의 검색 엔진을 사용한다. 본 연구에서는 LLMs에 대한 파라메트릭 지식을 활용하여 필요할 때 검색을 수행하고자 한다. <span class="ltx_text ltx_font_smallcaps" id="S2.p5.1.1">Adapt-LLM</span>은 IR의 사용량을 83.99%로 감소시키면서 바닐라 검색보다 성능을 향상시킨다.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p" id="S2.p6.1">우리의 작업과 더 유사하게 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>는 비모수 정보를 검색해야 할 때 데이터세트와 측정 방법을 제안한다. 그들은 다양한 인기를 가진 엔터티 세트에 대한 14K 질문을 포함하는 PopQA 데이터 세트를 제시한다. 개체 인기는 위키피디아 페이지의 페이지 뷰에 의해 측정되며, 이러한 QA 태스크를 해결하기 위해 PopQA 데이터 세트에서 계산된 인기도 점수 임계값을 사용한다. 개별 개체의 인기 점수가 임계값 미만이면 검색 단계를 수행합니다. 반대로 점수가 임계값보다 크면 직접 질문에 답한다. 이 방법은 바닐라 검색보다 더 나은 결과를 얻지만 현실적인 QA 시나리오에서는 사용할 수 없는 인기 점수를 계산해야 한다. 우리의 <span class="ltx_text ltx_font_smallcaps" id="S2.p6.1.1">Adapt-LLM</span> 방법은 IR 엔진을 호출할지 여부를 LLM에 가르치는 합성 데이터 세트를 생성함으로써 적응형 검색의 아이디어를 확장한다.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Adaptive Retrieval LLM (<span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>)</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">적응적 검색은 질의 응답 태스크에서 답변을 생성하기 위한 추가 컨텍스트 정보를 검색할지 여부를 동적으로 결정하는 모델의 능력을 의미한다. 적응적 검색은 항상 컨텍스트를 통합하거나 고려하지 않는 기존 모델과 달리 모델이 각 질문의 특정 요구 사항에 따라 컨텍스트를 선택적으로 검색할 수 있도록 한다. 이 적응형 접근법은 필요한 경우에만 컨텍스트를 활용하여 성능을 최적화하여 모델의 정확한 답변 생성 능력을 향상시키는 것을 목표로 한다.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p" id="S3.p2.1">도 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>에 도시된 바와 같이, <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.1">Adapt-LLM</span>의 프로세스는 다음 시퀀스로 전개된다:</p>
</div>
<div id="S3.p3" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1">질문을 포함하는 첫 번째 프롬프트는 모델로 전송된다(도 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>의 단계 1).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.p1.1.1">Adapt-LLM</span>은 질문을 효과적으로 답변하기 위해 추가적인 컨텍스트가 필요한지 여부를 결정하기 위해 프롬프트를 평가한다(단계 2).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i3.p1.1">모델이 컨텍스트가 필요하지 않다고 결정하면, 그것은 자신의 파라메트릭 메모리를 활용하여 질문에 대한 응답을 직접 생성한다(단계 3).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i4.p1.2">컨텍스트가 필요하다고 간주되면, <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.2.1">Adapt-LLM</span> 모델은 <math alttext="\langle" class="ltx_Math" display="inline" id="S3.I1.i4.p1.1.m1.1"><semantics id="S3.I1.i4.p1.1.m1.1a"><mo id="S3.I1.i4.p1.1.m1.1.1" stretchy="false" xref="S3.I1.i4.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><ci id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.I1.i4.p1.2.m2.1"><semantics id="S3.I1.i4.p1.2.m2.1a"><mo id="S3.I1.i4.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i4.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><ci id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">\rangle</annotation></semantics></math>로 표현되는 특수 토큰을 반환하고, off-the-shelf IR 시스템은 질문에 기초하여 관련 컨텍스트를 검색하는데 사용된다(단계 4); 이어서 컨텍스트는 답변 생성을 위한 포괄적인 표현을 형성하기 위해 원래의 질문 프롬프트와 결합된다(단계 5).</p>
</div>
</li>
</ol>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p4.1.1">Adapt-LLM</span>의 의사 결정 프로세스는 모델이 각 프롬프트의 동적 평가를 통해 질문에 답하기 위한 컨텍스트의 필요성을 결정할 수 있게 한다. 이 유연한 동작을 통해 모델은 향상된 이해를 위해 컨텍스트를 활용하는 것과 충분한 경우 직접적인 답변을 전달하는 것 사이의 균형을 맞출 수 있다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training <span id="S3.SS1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Here, we delineate the methodology employed to train our <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> model. The process of crafting the training data, denoted as <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">​</mo><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1b" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.5" xref="S3.SS1.p1.1.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1c" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.6" xref="S3.SS1.p1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝑆</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.6.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">DS_{Adapt}</annotation></semantics></math>, is presented in Algorithm <a href="#algorithm1" title="In 3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p2.5">질문들 <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">Q</annotation></semantics></math>, 연관된 컨텍스트 통로들 <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">P</annotation></semantics></math>, 및 대응하는 답변들 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">A</annotation></semantics></math>를 포함하는 오픈 도메인 질문 응답 데이터세트를 선택하는 것으로 시작한다. <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">D</mi><mo id="S3.SS1.p2.4.m4.1.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p2.4.m4.1.1.1.cmml">​</mo><msub id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1" lspace="0em" rspace="0em" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1a" lspace="0em" rspace="0em" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.4" xref="S3.SS1.p2.4.m4.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1b" lspace="0em" rspace="0em" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.5" xref="S3.SS1.p2.4.m4.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1c" lspace="0em" rspace="0em" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.6" xref="S3.SS1.p2.4.m4.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐷</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">𝑆</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.5.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.6.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">DS_{Adapt}</annotation></semantics></math>를 빈 집합(알고리즘의 라인 1)으로 초기화한다. <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">Q</annotation></semantics></math>의 각 질문에 대해 검색 메커니즘 없이 기본 LLM을 활용하여 제로샷 추론(라인 3)을 수행한다. 이 단계를 통해 모델이 정답을 생성하는 질문과 응답이 부정확한 질문을 구별할 수 있습니다. 이 프로세스는 기본 LLM <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.1">knows</span>이 파라메트릭 메모리로 인해 무엇인지 발견하는 방법으로 이해될 수 있다. 모델의 응답이 정확한 질문(라인 4)의 경우 <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.2">parametric_prompt</span>이라고 하는 다음 프롬프트를 포함하는 훈련 세트 인스턴스를 빌드합니다.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<span id="S3.SS1.p3.1" class="ltx_ERROR undefined">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p3.2">Prompt: Answer the question Q. If you need help answer &lt;RET&gt; to get the context. Q: …</p>질문 Q에 답하세요. 컨텍스트를 얻기 위해 도움말 대답 <RET>가 필요한 경우. Q: …</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.3" class="ltx_p">Alongside this prompt, we include the corresponding question from <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">Q</annotation></semantics></math> and the golden answer from <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">A</annotation></semantics></math>, collectively forming the instance (line 5), which is subsequently appended to the <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.3.2" xref="S3.SS1.p4.3.m3.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1a" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.4" xref="S3.SS1.p4.3.m3.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1b" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.5" xref="S3.SS1.p4.3.m3.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1.3.3.1c" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.6" xref="S3.SS1.p4.3.m3.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><times id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></times><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝐷</ci><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">𝑆</ci><apply id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3"><times id="S3.SS1.p4.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.p4.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.4.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.5.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.6.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">DS_{Adapt}</annotation></semantics></math> dataset (line 6).</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p5.2">대조적으로, LLM이 질문에 대한 올바른 응답을 생성하지 못하면(라인 8), 우리는 두 개의 다른 인스턴스를 구축한다. 첫 번째는 앞서 설명한 것과 동일한 <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.1">parametric_prompt</span>을 채용하며, <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mo id="S3.SS1.p5.1.m1.1.1" stretchy="false" xref="S3.SS1.p5.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mo id="S3.SS1.p5.2.m2.1.1" stretchy="false" xref="S3.SS1.p5.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\rangle</annotation></semantics></math>가 답변(line 9)으로 지정되어 추가 컨텍스트에 대한 필요성을 나타낸다. <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.2">context_prompt</span>이라고 하는 두 번째 프롬프트는 질문과 함께 컨텍스트 정보를 포함합니다.</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.6" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.6.7" class="ltx_listingline">

<span id="algorithm1.6.7.1" class="ltx_text"><span id="algorithm1.6.7.1.1" class="ltx_text ltx_font_bold">Input:</span> </span>Q: questions, A: answers, P: passages, LLM
</div>
<div id="algorithm1.1.1" class="ltx_listingline"> <span id="algorithm1.1.1.1" class="ltx_text"><span id="algorithm1.1.1.1.1" class="ltx_text ltx_font_bold">Output:</span> </span><math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mrow id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml"><mi id="algorithm1.1.1.m1.1.1.2" xref="algorithm1.1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.cmml">​</mo><msub id="algorithm1.1.1.m1.1.1.3" xref="algorithm1.1.1.m1.1.1.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.2" xref="algorithm1.1.1.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.1.1.m1.1.1.3.3" xref="algorithm1.1.1.m1.1.1.3.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.3.2" xref="algorithm1.1.1.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.1.1.m1.1.1.3.3.3" xref="algorithm1.1.1.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1a" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.1.1.m1.1.1.3.3.4" xref="algorithm1.1.1.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1b" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.1.1.m1.1.1.3.3.5" xref="algorithm1.1.1.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.3.3.1c" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.1.1.m1.1.1.3.3.6" xref="algorithm1.1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1"><times id="algorithm1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1"></times><ci id="algorithm1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.2">𝐷</ci><apply id="algorithm1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.1.1.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3">subscript</csymbol><ci id="algorithm1.1.1.m1.1.1.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.1.1.m1.1.1.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3"><times id="algorithm1.1.1.m1.1.1.3.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3.3.1"></times><ci id="algorithm1.1.1.m1.1.1.3.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.1.1.m1.1.1.3.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.1.1.m1.1.1.3.3.4.cmml" xref="algorithm1.1.1.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.1.1.m1.1.1.3.3.5.cmml" xref="algorithm1.1.1.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.1.1.m1.1.1.3.3.6.cmml" xref="algorithm1.1.1.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">DS_{Adapt}</annotation></semantics></math>: A training dataset for Adaptive Retrieval
</div>
<div id="algorithm1.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>


<math id="algorithm1.2.2.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.2.2.m1.1a"><mrow id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml"><mi id="algorithm1.2.2.m1.1.1.2" xref="algorithm1.2.2.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><msub id="algorithm1.2.2.m1.1.1.3" xref="algorithm1.2.2.m1.1.1.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.2" xref="algorithm1.2.2.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.2.2.m1.1.1.3.3" xref="algorithm1.2.2.m1.1.1.3.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.3.2" xref="algorithm1.2.2.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.2.2.m1.1.1.3.3.3" xref="algorithm1.2.2.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1a" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.2.2.m1.1.1.3.3.4" xref="algorithm1.2.2.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1b" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.2.2.m1.1.1.3.3.5" xref="algorithm1.2.2.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.3.3.1c" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.2.2.m1.1.1.3.3.6" xref="algorithm1.2.2.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><apply id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1"><times id="algorithm1.2.2.m1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1"></times><ci id="algorithm1.2.2.m1.1.1.2.cmml" xref="algorithm1.2.2.m1.1.1.2">𝐷</ci><apply id="algorithm1.2.2.m1.1.1.3.cmml" xref="algorithm1.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.1.1.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3">subscript</csymbol><ci id="algorithm1.2.2.m1.1.1.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.2.2.m1.1.1.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3"><times id="algorithm1.2.2.m1.1.1.3.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3.3.1"></times><ci id="algorithm1.2.2.m1.1.1.3.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.2.2.m1.1.1.3.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.2.2.m1.1.1.3.3.4.cmml" xref="algorithm1.2.2.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.2.2.m1.1.1.3.3.5.cmml" xref="algorithm1.2.2.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.2.2.m1.1.1.3.3.6.cmml" xref="algorithm1.2.2.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">DS_{Adapt}</annotation></semantics></math> = init_empty()
</div>
<div id="algorithm1.6.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>
<span id="algorithm1.6.8.1" class="ltx_text ltx_font_bold">for</span>&nbsp;<em id="algorithm1.6.8.2" class="ltx_emph ltx_font_italic">q, gold_ans, pass in (Q, A, P)</em>&nbsp;<span id="algorithm1.6.8.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.6.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
ans = LLM(q)

</div>
<div id="algorithm1.6.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span id="algorithm1.6.10.1" class="ltx_text ltx_font_bold">if</span>&nbsp;<em id="algorithm1.6.10.2" class="ltx_emph ltx_font_italic">ans = gold_ans</em>&nbsp;<span id="algorithm1.6.10.3" class="ltx_text ltx_font_bold">then</span> 
</div>
<div id="algorithm1.6.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst = build_instance(’parametric_prompt’, q, gold_ans)
</div>
<div id="algorithm1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.3.3.m1.1a"><mrow id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml"><mi id="algorithm1.3.3.m1.1.1.2" xref="algorithm1.3.3.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.1" xref="algorithm1.3.3.m1.1.1.1.cmml">​</mo><msub id="algorithm1.3.3.m1.1.1.3" xref="algorithm1.3.3.m1.1.1.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.2" xref="algorithm1.3.3.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.3.3.m1.1.1.3.3" xref="algorithm1.3.3.m1.1.1.3.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.3.2" xref="algorithm1.3.3.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.3.3.m1.1.1.3.3.3" xref="algorithm1.3.3.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1a" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.3.3.m1.1.1.3.3.4" xref="algorithm1.3.3.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1b" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.3.3.m1.1.1.3.3.5" xref="algorithm1.3.3.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.m1.1.1.3.3.1c" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.3.3.m1.1.1.3.3.6" xref="algorithm1.3.3.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1"><times id="algorithm1.3.3.m1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1.1"></times><ci id="algorithm1.3.3.m1.1.1.2.cmml" xref="algorithm1.3.3.m1.1.1.2">𝐷</ci><apply id="algorithm1.3.3.m1.1.1.3.cmml" xref="algorithm1.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.1.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3">subscript</csymbol><ci id="algorithm1.3.3.m1.1.1.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.3.3.m1.1.1.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3"><times id="algorithm1.3.3.m1.1.1.3.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3.3.1"></times><ci id="algorithm1.3.3.m1.1.1.3.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.3.3.m1.1.1.3.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.3.3.m1.1.1.3.3.4.cmml" xref="algorithm1.3.3.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.3.3.m1.1.1.3.3.5.cmml" xref="algorithm1.3.3.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.3.3.m1.1.1.3.3.6.cmml" xref="algorithm1.3.3.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">DS_{Adapt}</annotation></semantics></math>.add(inst)

</div>
<div id="algorithm1.6.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div id="algorithm1.6.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div id="algorithm1.6.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span id="algorithm1.6.14.1" class="ltx_text ltx_font_bold">else</span> 
</div>
<div id="algorithm1.6.15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst1 = build_instance(’parametric_prompt’, q, "&lt;RET&gt;")
</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math id="algorithm1.4.4.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.4.4.m1.1a"><mrow id="algorithm1.4.4.m1.1.1" xref="algorithm1.4.4.m1.1.1.cmml"><mi id="algorithm1.4.4.m1.1.1.2" xref="algorithm1.4.4.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.1" xref="algorithm1.4.4.m1.1.1.1.cmml">​</mo><msub id="algorithm1.4.4.m1.1.1.3" xref="algorithm1.4.4.m1.1.1.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.2" xref="algorithm1.4.4.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.4.4.m1.1.1.3.3" xref="algorithm1.4.4.m1.1.1.3.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.3.2" xref="algorithm1.4.4.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.4.4.m1.1.1.3.3.3" xref="algorithm1.4.4.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1a" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.4.4.m1.1.1.3.3.4" xref="algorithm1.4.4.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1b" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.4.4.m1.1.1.3.3.5" xref="algorithm1.4.4.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.m1.1.1.3.3.1c" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.4.4.m1.1.1.3.3.6" xref="algorithm1.4.4.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m1.1b"><apply id="algorithm1.4.4.m1.1.1.cmml" xref="algorithm1.4.4.m1.1.1"><times id="algorithm1.4.4.m1.1.1.1.cmml" xref="algorithm1.4.4.m1.1.1.1"></times><ci id="algorithm1.4.4.m1.1.1.2.cmml" xref="algorithm1.4.4.m1.1.1.2">𝐷</ci><apply id="algorithm1.4.4.m1.1.1.3.cmml" xref="algorithm1.4.4.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.4.4.m1.1.1.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3">subscript</csymbol><ci id="algorithm1.4.4.m1.1.1.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.4.4.m1.1.1.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3"><times id="algorithm1.4.4.m1.1.1.3.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3.3.1"></times><ci id="algorithm1.4.4.m1.1.1.3.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.4.4.m1.1.1.3.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.4.4.m1.1.1.3.3.4.cmml" xref="algorithm1.4.4.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.4.4.m1.1.1.3.3.5.cmml" xref="algorithm1.4.4.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.4.4.m1.1.1.3.3.6.cmml" xref="algorithm1.4.4.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m1.1c">DS_{Adapt}</annotation></semantics></math>.add(inst1)
</div>
<div id="algorithm1.6.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst2 = build_instance(’context_prompt’, q, gold_ans, pass)
</div>
<div id="algorithm1.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math id="algorithm1.5.5.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.5.5.m1.1a"><mrow id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml"><mi id="algorithm1.5.5.m1.1.1.2" xref="algorithm1.5.5.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.1" xref="algorithm1.5.5.m1.1.1.1.cmml">​</mo><msub id="algorithm1.5.5.m1.1.1.3" xref="algorithm1.5.5.m1.1.1.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.2" xref="algorithm1.5.5.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.5.5.m1.1.1.3.3" xref="algorithm1.5.5.m1.1.1.3.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.3.2" xref="algorithm1.5.5.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.5.5.m1.1.1.3.3.3" xref="algorithm1.5.5.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1a" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.5.5.m1.1.1.3.3.4" xref="algorithm1.5.5.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1b" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.5.5.m1.1.1.3.3.5" xref="algorithm1.5.5.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.5.5.m1.1.1.3.3.1c" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.5.5.m1.1.1.3.3.6" xref="algorithm1.5.5.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><apply id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1"><times id="algorithm1.5.5.m1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1.1"></times><ci id="algorithm1.5.5.m1.1.1.2.cmml" xref="algorithm1.5.5.m1.1.1.2">𝐷</ci><apply id="algorithm1.5.5.m1.1.1.3.cmml" xref="algorithm1.5.5.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3">subscript</csymbol><ci id="algorithm1.5.5.m1.1.1.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.5.5.m1.1.1.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3"><times id="algorithm1.5.5.m1.1.1.3.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3.3.1"></times><ci id="algorithm1.5.5.m1.1.1.3.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.5.5.m1.1.1.3.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.5.5.m1.1.1.3.3.4.cmml" xref="algorithm1.5.5.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.5.5.m1.1.1.3.3.5.cmml" xref="algorithm1.5.5.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.5.5.m1.1.1.3.3.6.cmml" xref="algorithm1.5.5.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">DS_{Adapt}</annotation></semantics></math>.add(inst2)

</div>
<div id="algorithm1.6.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div id="algorithm1.6.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div id="algorithm1.6.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span> end for
</div>
<div id="algorithm1.6.6" class="ltx_listingline">return <math id="algorithm1.6.6.m1.1" class="ltx_Math" alttext="DS_{Adapt}" display="inline"><semantics id="algorithm1.6.6.m1.1a"><mrow id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml"><mi id="algorithm1.6.6.m1.1.1.2" xref="algorithm1.6.6.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.1" xref="algorithm1.6.6.m1.1.1.1.cmml">​</mo><msub id="algorithm1.6.6.m1.1.1.3" xref="algorithm1.6.6.m1.1.1.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.2" xref="algorithm1.6.6.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.6.6.m1.1.1.3.3" xref="algorithm1.6.6.m1.1.1.3.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.3.2" xref="algorithm1.6.6.m1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.6.6.m1.1.1.3.3.3" xref="algorithm1.6.6.m1.1.1.3.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1a" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.6.6.m1.1.1.3.3.4" xref="algorithm1.6.6.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1b" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.6.6.m1.1.1.3.3.5" xref="algorithm1.6.6.m1.1.1.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="algorithm1.6.6.m1.1.1.3.3.1c" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">​</mo><mi id="algorithm1.6.6.m1.1.1.3.3.6" xref="algorithm1.6.6.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><apply id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1"><times id="algorithm1.6.6.m1.1.1.1.cmml" xref="algorithm1.6.6.m1.1.1.1"></times><ci id="algorithm1.6.6.m1.1.1.2.cmml" xref="algorithm1.6.6.m1.1.1.2">𝐷</ci><apply id="algorithm1.6.6.m1.1.1.3.cmml" xref="algorithm1.6.6.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m1.1.1.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3">subscript</csymbol><ci id="algorithm1.6.6.m1.1.1.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.6.6.m1.1.1.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3"><times id="algorithm1.6.6.m1.1.1.3.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3.3.1"></times><ci id="algorithm1.6.6.m1.1.1.3.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.6.6.m1.1.1.3.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.6.6.m1.1.1.3.3.4.cmml" xref="algorithm1.6.6.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.6.6.m1.1.1.3.3.5.cmml" xref="algorithm1.6.6.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.6.6.m1.1.1.3.3.6.cmml" xref="algorithm1.6.6.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">DS_{Adapt}</annotation></semantics></math>

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.8.1.1">Algorithm 1</span></span>Training data creation</figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<span id="S3.SS1.p6.1" class="ltx_ERROR undefined">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p6.2">프롬프트: 문맥 C가 주어진 질문 Q에 답하라. Q: …, C: …,</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p7.3">이 경우 프롬프트, <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><mi id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">Q</annotation></semantics></math>의 질문, <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p7.2.m2.1"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">A</annotation></semantics></math>의 황금 답변, <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p7.3.m3.1"><semantics id="S3.SS1.p7.3.m3.1a"><mi id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">P</annotation></semantics></math>의 해당 컨텍스트 패시지를 포함한다(행 11).</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p8.1">LLM이 정확하게 응답할 수 없는 질문과 다른 모든 질문에 대한 황금 답변이 있는 <span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.1">parametric_prompt</span>에 대해 두 가지 유형의 프롬프트로 데이터 세트를 채운 후, 후속 미세 조정 단계를 위해 훈련 세트 <math alttext="D_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p8.1.m1.1"><semantics id="S3.SS1.p8.1.m1.1a"><msub id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.1.1.2" xref="S3.SS1.p8.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS1.p8.1.m1.1.1.3" xref="S3.SS1.p8.1.m1.1.1.3.cmml"><mi id="S3.SS1.p8.1.m1.1.1.3.2" xref="S3.SS1.p8.1.m1.1.1.3.2.cmml">A</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p8.1.m1.1.1.3.3" xref="S3.SS1.p8.1.m1.1.1.3.3.cmml">d</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1a" lspace="0em" rspace="0em" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p8.1.m1.1.1.3.4" xref="S3.SS1.p8.1.m1.1.1.3.4.cmml">a</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1b" lspace="0em" rspace="0em" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p8.1.m1.1.1.3.5" xref="S3.SS1.p8.1.m1.1.1.3.5.cmml">p</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1c" lspace="0em" rspace="0em" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p8.1.m1.1.1.3.6" xref="S3.SS1.p8.1.m1.1.1.3.6.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><apply id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p8.1.m1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3"><times id="S3.SS1.p8.1.m1.1.1.3.1.cmml" xref="S3.SS1.p8.1.m1.1.1.3.1"></times><ci id="S3.SS1.p8.1.m1.1.1.3.2.cmml" xref="S3.SS1.p8.1.m1.1.1.3.2">𝐴</ci><ci id="S3.SS1.p8.1.m1.1.1.3.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3.3">𝑑</ci><ci id="S3.SS1.p8.1.m1.1.1.3.4.cmml" xref="S3.SS1.p8.1.m1.1.1.3.4">𝑎</ci><ci id="S3.SS1.p8.1.m1.1.1.3.5.cmml" xref="S3.SS1.p8.1.m1.1.1.3.5">𝑝</ci><ci id="S3.SS1.p8.1.m1.1.1.3.6.cmml" xref="S3.SS1.p8.1.m1.1.1.3.6">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">D_{Adapt}</annotation></semantics></math>가 준비된다. 미세 조정 프로세스는 데이터 세트에서 기본 LLM을 훈련하는 것을 수반하여 <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p8.1.2">Adapt-LLM</span> 모델을 생성한다.</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p9.1">이 접근법은 모델이 질문에 답하기 위해 컨텍스트가 필요할 때 식별하거나, 컨텍스트가 제공될 때 직접 답변뿐만 아니라 충분할 때 직접 답변을 제공하도록 효과적으로 학습하도록 보장한다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">추론 단계에서는 미세 조정된 모델을 사용하여 보이지 않는 질문에 대한 응답을 생성한다. 섹션 <a class="ltx_ref" href="#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>에 설명된 것과 같이 훈련 단계에서 사용된 동일한 프롬프트를 사용한다.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p2.4">처음에, 모델은 직접 응답을 제공하거나 대답이 확실하지 않은 경우 <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mo id="S3.SS2.p2.2.m2.1.1" stretchy="false" xref="S3.SS2.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\rangle</annotation></semantics></math>를 반환하도록 프롬프트된다. 모델이 <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" stretchy="false" xref="S3.SS2.p2.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mo id="S3.SS2.p2.4.m4.1.1" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\rangle</annotation></semantics></math>를 반환하는 경우, 우리는 off-the-shelf IR 시스템을 사용하여 관련 컨텍스트를 획득하기 위해 정보 검색을 진행한다. 그 후, 검색된 컨텍스트로 질문을 증강하고 훈련 단계에서 도입된 두 번째 유형의 프롬프트를 사용하여 모델을 다시 프롬프트한다.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">이 섹션에서는 제안된 적응형 검색 접근 방식인 <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">Adapt-LLM</span>의 성능을 평가하기 위한 실험 프레임워크를 설명한다. 사용된 데이터 세트(섹션 <a class="ltx_ref" href="#S4.SS1" title="4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.1</span></a>)를 설명한 다음 기본 모델의 개요(섹션 <a class="ltx_ref" href="#S4.SS2" title="4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.2</span></a>), 기본 모델의 다른 구성(섹션 <a class="ltx_ref" href="#S4.SS3" title="4.3 Model Configurations ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.3</span></a>) 및 훈련 세부 정보(섹션 <a class="ltx_ref" href="#S4.SS4" title="4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.4</span></a>)를 설명하는 것으로 시작합니다. 이어서, 우리는 세 가지 주요 실험을 소개한다:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I1.i1.p1.1.1">Adapt-LLM</span> 성능 비교: (i) 모든 질문에 대한 컨텍스트 정보를 검색하는 LLM, (ii) 임의의 질문에 대해 IR 시스템을 사용하지 않고 자신의 파라메트릭 메모리에 독점적으로 의존하는 LLM(섹션 <a class="ltx_ref" href="#S4.SS5" title="4.5 Validating the Adaptive Retrieval Approach ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I1.i2.p1.1.1">Adapt-LLM</span>’s ability to determine when extra context is necessary to answer a question (Section <a class="ltx_ref" href="#S4.SS6" title="4.6 Contextual Retrieval Decision Analysis ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.6</span></a>).</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i3.p1.1">PopQA에 대한 최신 접근법과의 비교(섹션 <a class="ltx_ref" href="#S4.SS7" title="4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.7</span></a>).</p>
</div>
</li>
</ol>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training Set</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model configuration</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">NQ</span></td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.2.1" class="ltx_text ltx_font_smallcaps">Never Retrieve</span></td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">21.43%</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">Always Retrieve</span></td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">35.86%</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center">
<span id="S4.T1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> (ours)</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.3.2.1" class="ltx_text ltx_font_bold">36.77%</span></td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">SQuAD</span></td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.5.4.2.1" class="ltx_text ltx_font_smallcaps">Never Retrieve</span></td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">21.22%</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.5.1.1" class="ltx_text ltx_font_smallcaps">Always Retrieve</span></td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center">36.59%</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T1.1.7.6.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM</span> (ours)</td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.7.6.2.1" class="ltx_text ltx_font_bold">38.15%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span> 상이한 검색 구성(NR-LLM, AR-LLM, 및 <span class="ltx_text ltx_font_smallcaps" id="S4.T1.3.1">Adapt-LLM</span>)을 사용하여 NQ 및 SQuAD 데이터 세트에 대해 트레이닝된 Llama-2 모델의 성능 비교 PopQA 테스트 세트에 대해 평가되었다. 모든 모델에 대해 정확한 일치 정확도가 보고됩니다.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">모델의 포괄적인 훈련과 평가를 보장하기 위해 세 가지 다양한 질문 응답 데이터 세트를 구체적으로 선택했다. 학습은 사실 지식을 평가하고 위키피디아를 기반으로 하는 널리 알려져 있는 데이터 세트이기 때문에 NQ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>와 SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>를 선택했다. 평가를 위해 PopQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>를 선택했다. 다음은 각 데이터 세트에 대한 간략한 설명입니다.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">NQ</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">자연 질문 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>는 구글 검색 질의에서 파생된 실제 질문의 집합으로, 위키피디아 기사로부터 얻은 긴 형식의 텍스트 지문을 동반하고 다양한 범위의 주제 및 자연 언어 변형을 제공한다. 실험에서는 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">training</span> 모델에 이 데이터 세트를 활용한다.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">SQuAD</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">스탠포드 질의 응답 데이터 세트 SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>는 자연어 처리 분야에서 널리 사용되는 데이터 세트이며 컨텍스트 역할을 하는 관련 단락 구절과 함께 다양한 범위의 위키피디아 기사에 크라우드 워커가 제기하는 질문으로 구성된다. 실험에서는 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.1">training</span> 모델에 이 데이터 세트를 활용한다.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">PopQA</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">Popular Questions and Answers 데이터셋 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>는 광범위한 도메인과 스타일을 포괄하는 다양한 온라인 플랫폼에서 제공되는 선별된 질문으로 구성된다. 이 데이터 세트에서 관찰된 컨텍스트 검색 전략의 효과의 가변성을 감안할 때, 정확한 답변 제공을 위해 컨텍스트가 필요한 시기를 결정하는 언어 모델의 성능인 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px3.p1.1.1">evaluate</span>에 대한 테스트 세트로 PopQA를 선택한다.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Base Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">실험에서는 Llama-2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>]</cite>를 기본 LLM으로 사용한다. Lama-2는 7B, 13B 및 70B 매개변수의 버전으로 제공되는 오픈 소스 명령어 기반 LLM이다. 이 모델은 공개적으로 사용 가능한 온라인 데이터 소스에서 제공되는 확장된 코퍼스에서 사전 훈련된다. 이 코퍼스는 이전 코퍼스에 비해 크기가 40% 증가하여 모델의 향상된 성능과 기능에 기여합니다.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p2.1">또한, Llama-2는 확장된 컨텍스트 길이를 특징으로 하여 더 긴 텍스트 시퀀스를 처리하고 이해하는 능력을 효과적으로 배가한다. 이러한 향상은 다양한 자연어 이해 과제에 걸쳐 모델의 효과를 크게 향상시킨다. 특히, 실험을 위해 7B 매개변수가 있는 라마-2 모델을 활용하여 특정 연구 목표에 대한 강력한 기능을 활용한다.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">NQ</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">SQuAD</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">PopQA</span></th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Questions</td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">58,880</td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">87,599</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">14,282</td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<td id="S4.T2.1.3.3.1" class="ltx_td ltx_align_center">Words/question</td>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center">9.20</td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center">10.06</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center">6.62</td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<td id="S4.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_bb">Words/answer</td>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">2.26</td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">3.16</td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">2.04</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>우리가 실험에 사용하는 세 데이터 세트의 비교, 즉 SQuAD, NQ 및 PopQA. 각각에 대해 질문 수와 질문 및 답변당 평균 단어 수를 제공합니다.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.6" class="ltx_tr">
<th id="S4.T3.6.6.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.6.6.7.1" class="ltx_text ltx_font_bold">Training</span></th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\langle</annotation></semantics></math><span id="S4.T3.2.2.2.1" class="ltx_text ltx_font_bold">RET<math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\rangle</annotation></semantics></math> Usage</span>
</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">
<math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\langle</annotation></semantics></math><span id="S4.T3.4.4.4.1" class="ltx_text ltx_font_bold">RET<math id="S4.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\rangle</annotation></semantics></math></span>
</th>
<th id="S4.T3.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.6.6.6.2" class="ltx_text ltx_font_bold">No <math id="S4.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.T3.5.5.5.1.m1.1a"><mo stretchy="false" id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><ci id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\langle</annotation></semantics></math>RET<math id="S4.T3.6.6.6.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.T3.6.6.6.2.m2.1a"><mo stretchy="false" id="S4.T3.6.6.6.2.m2.1.1" xref="S4.T3.6.6.6.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.2.m2.1b"><ci id="S4.T3.6.6.6.2.m2.1.1.cmml" xref="S4.T3.6.6.6.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.2.m2.1c">\rangle</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.7.1" class="ltx_tr">
<td id="S4.T3.6.7.1.1" class="ltx_td"></td>
<td id="S4.T3.6.7.1.2" class="ltx_td"></td>
<td id="S4.T3.6.7.1.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.3.1" class="ltx_text ltx_font_bold">Acc. w/ context</span></td>
<td id="S4.T3.6.7.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.4.1" class="ltx_text ltx_font_bold">Acc. w/o context</span></td>
<td id="S4.T3.6.7.1.5" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.5.1" class="ltx_text ltx_font_bold">Acc. w/ context</span></td>
<td id="S4.T3.6.7.1.6" class="ltx_td ltx_align_center"><span id="S4.T3.6.7.1.6.1" class="ltx_text ltx_font_bold">Acc. w/o context</span></td>
</tr>
<tr id="S4.T3.6.8.2" class="ltx_tr">
<th id="S4.T3.6.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NQ</th>
<th id="S4.T3.6.8.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">82.26%</th>
<th id="S4.T3.6.8.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">33.04%</th>
<th id="S4.T3.6.8.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">14.65%</th>
<th id="S4.T3.6.8.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">55.72%</th>
<th id="S4.T3.6.8.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">62.36%</th>
</tr>
<tr id="S4.T3.6.9.3" class="ltx_tr">
<td id="S4.T3.6.9.3.1" class="ltx_td ltx_align_center ltx_border_bb">SQuAD</td>
<td id="S4.T3.6.9.3.2" class="ltx_td ltx_align_center ltx_border_bb">83.93%</td>
<td id="S4.T3.6.9.3.3" class="ltx_td ltx_align_center ltx_border_bb">33.40%</td>
<td id="S4.T3.6.9.3.4" class="ltx_td ltx_align_center ltx_border_bb">9.94%</td>
<td id="S4.T3.6.9.3.5" class="ltx_td ltx_align_center ltx_border_bb">57.73%</td>
<td id="S4.T3.6.9.3.6" class="ltx_td ltx_align_center ltx_border_bb">62.92%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Results of the usage of the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.13.m1.1"><semantics id="S4.T3.13.m1.1b"><mo id="S4.T3.13.m1.1.1" stretchy="false" xref="S4.T3.13.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.m1.1c"><ci id="S4.T3.13.m1.1.1.cmml" xref="S4.T3.13.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.m1.1d">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.14.m2.1"><semantics id="S4.T3.14.m2.1b"><mo id="S4.T3.14.m2.1.1" stretchy="false" xref="S4.T3.14.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.m2.1c"><ci id="S4.T3.14.m2.1.1.cmml" xref="S4.T3.14.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.m2.1d">\rangle</annotation></semantics></math> token in the <span class="ltx_text ltx_font_smallcaps" id="S4.T3.22.1">Adapt-LLM</span> model. 첫 번째 열에는 모델이 추가 컨텍스트를 요청하는 PopQA 질문의 백분율이 표시됩니다. 두 번째 열은 <span class="ltx_text ltx_font_smallcaps" id="S4.T3.23.2">Adapt-LLM</span>이 컨텍스트를 묻는 질문(<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.15.m3.1"><semantics id="S4.T3.15.m3.1b"><mo id="S4.T3.15.m3.1.1" stretchy="false" xref="S4.T3.15.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.m3.1c"><ci id="S4.T3.15.m3.1.1.cmml" xref="S4.T3.15.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.m3.1d">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.16.m4.1"><semantics id="S4.T3.16.m4.1b"><mo id="S4.T3.16.m4.1.1" stretchy="false" xref="S4.T3.16.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.m4.1c"><ci id="S4.T3.16.m4.1.1.cmml" xref="S4.T3.16.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.m4.1d">\rangle</annotation></semantics></math>)에 초점을 맞추어 컨텍스트가 있는 질문과 없는 질문 간의 성능을 비교합니다. 마지막 열(No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.17.m5.1"><semantics id="S4.T3.17.m5.1b"><mo id="S4.T3.17.m5.1.1" stretchy="false" xref="S4.T3.17.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.m5.1c"><ci id="S4.T3.17.m5.1.1.cmml" xref="S4.T3.17.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.m5.1d">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.18.m6.1"><semantics id="S4.T3.18.m6.1b"><mo id="S4.T3.18.m6.1.1" stretchy="false" xref="S4.T3.18.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.m6.1c"><ci id="S4.T3.18.m6.1.1.cmml" xref="S4.T3.18.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.m6.1d">\rangle</annotation></semantics></math>)은 <span class="ltx_text ltx_font_smallcaps" id="S4.T3.24.3">Adapt-LLM</span>이 직접 답하기로 결정한 질문에 대한 것이다. 또한, IR 시스템에 의해 검색된 컨텍스트의 유무에 따른 성능을 비교한다.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Configurations</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">LLM과 IR 시스템을 결합할 수 있는 세 가지 다른 방식에 해당하는 세 가지 다른 모델 구성을 사용하여 실험을 수행한다.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Adaptive Retrieval (<span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.1.1">Adapt-LLM</span>)</span>. <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.2">Adapt-LLM</span> 모델은 섹션 <a class="ltx_ref" href="#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>에서 설명한 바와 같이 컨텍스트 정보에 대한 질문 및 인식된 필요성에 기초하여 컨텍스트 검색 여부를 동적으로 결정한다. IR 시스템은 대규모 말뭉치에서 사전 훈련된 비지도 모델인 Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>를 사용하고 MS MARCO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>에서 미세 조정한다. 우리는 최종 답변을 위해 기본 LLM을 프롬프트하기 위해 IR 시스템에 따라 가장 관련성이 높은 통로만 검색한다.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Never-Retrieve (NR-LLM)</span>. 이 모델 구성은 컨텍스트 정보를 고려하지 않고 질문 텍스트만을 기반으로 질문에 답하도록 훈련된다. 문맥이 없는 상황에서 질의 응답 모델의 성능을 평가하는 기준선 역할을 한다.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Always-Retrieve (AR-LLM)</span>. NR-LLM 모델과 대조적으로, 이 구성은 항상 질문에 답하는 것을 돕기 위해 컨텍스트 구절을 검색한다. 이는 답변을 생성하기 위해 컨텍스트를 일관되게 활용하도록 훈련된다. <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i3.p1.1.2">Adapt-LLM</span>과 공정한 비교를 보장하기 위해 IR 시스템으로 Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>를 사용하고 컨텍스트로 가장 관련성이 높은 통로만 검색합니다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Training Details</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p1.1">세 가지 모델 구성(<span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.1">Adapt-LLM</span>, AR-LLM 및 NR-LLM) 및 두 훈련 세트(SQuAD 및 NQ) 모두에 대해 128의 배치 크기, 세 개의 epoch 및 3e-4의 고정 학습 속도를 포함하는 Alpaca-Lora <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite>에서 설정된 매개변수 구성을 준수한다. 우리는 r=8, 알파=16 및 드롭아웃 속도 0.05에 대해 구성된 매개변수와 함께 LoRA(Low-Rank Adaptation) 정규화를 통합했다. 훈련은 NVIDIA A40 GPU에서 약 8시간의 평균 훈련 시간 동안 수행되었다. 우리는 어떤 모델 선택도 수행하지 않고 3번의 훈련 후 마지막 체크포인트를 사용한다.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Validating the Adaptive Retrieval Approach</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p1.1">적응적 접근법(<span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p1.1.1">Adapt-LLM</span>)의 효율성을 평가하기 위해 NR-LLM 및 AR-LLM 구성과 비교하여 세 가지 구성 모두에 걸쳐 NQ 및 SQuAD 데이터 세트 모두에서 Llama-2 모델의 미세 조정을 수행했다. NR-LLM 및 AR-LLM 구성의 경우 데이터 세트에서 질문-응답 쌍을 추출하고 해당 명령 프롬프트를 통합하여 훈련 샘플을 구성했다.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p2.2">구체적으로, NR-LLM 구성에 대한 프롬프트는 추가 컨텍스트 없이 모델에 질문에 답하도록 지시한 반면, AR-LLM 구성에 대한 프롬프트는 질문 및 컨텍스트 정보를 모두 포함했다. 대조적으로, <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p2.2.1">Adapt-LLM</span> 훈련 세트는 2단계 프로세스를 사용하여 섹션 <a class="ltx_ref" href="#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>에 설명된 접근법에 따라 구성되었다. 이 과정의 결과, NQ의 74.72%의 질문이 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mo id="S4.SS5.p2.1.m1.1.1" stretchy="false" xref="S4.SS5.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mo id="S4.SS5.p2.2.m2.1.1" stretchy="false" xref="S4.SS5.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\rangle</annotation></semantics></math> 토큰으로 표시된 반면, SQuAD의 경우 87.49%의 질문이 표시된다.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p3.1">그런 다음 훈련된 모델을 PopQA 데이터 세트에서 테스트하여 실제 질문 응답 시나리오에서 성능을 평가했다. 추론하는 동안 NR-LLM 및 AR-LLM 모델이 그대로 활용되었으며 해당 지시 프롬프트가 제공되었으며 출력은 질문에 대한 답변으로 예상된다. 반대로 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p3.1.1">Adapt-LLM</span> 모델의 경우 섹션 <a class="ltx_ref" href="#S3.SS2" title="3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.2</span></a>에서 설명한 것과 동일한 프롬프트 절차를 따랐다.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p4.1">그런 다음 생성된 답변은 PopQA 테스트 세트에서 이미 주석이 달린 각 질문에 대한 가능한 답변 세트와 비교된다. 사용된 평가 메트릭은 해당 질문에 대한 가능한 답변 중 하나와 정확히 일치하는 생성된 출력의 백분율을 측정하는 <span class="ltx_text ltx_font_bold" id="S4.SS5.p4.1.1">Exact Match Accuracy</span>입니다.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p5.1">표 <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>는 다른 구성 및 데이터 세트에 걸친 Llama-2 모델의 성능을 설명하는 이 실험의 결과를 나타낸다. NQ 및 SQuAD 훈련 데이터 세트 모두에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p5.1.1">Adapt-LLM</span> 구성은 PopQA 테스트 세트에서 NR-LLM(Never Retrieve) 및 AR-LLM(Always Retrieve) 구성을 일관되게 능가합니다. 관찰할 수 있는 바와 같이 NR-LLM은 다른 구성에 비해 약 14개의 절대점의 정확도 차이로 모델 중 가장 낮은 성능을 나타낸다. 이러한 차이는 라마-2의 매개변수 기억만으로는 PopQA 질문에 효과적으로 답하기 충분하지 않음을 시사한다.</p>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p6.1">AR-LLM과 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.1">Adapt-LLM</span>의 차이는 더 좁습니다. 구체적으로, <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.2">Adapt-LLM</span> 구성은 AR-LLM 구성의 35.86% 및 36.59%에 비해 NQ 및 SQuAD 데이터 세트에 대해 훈련될 때 PopQA 테스트 세트에 대해 각각 36.77% 및 38.15%의 정확도를 달성한다. 두 훈련 데이터 세트 모두에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.3">Adapt-LLM</span>은 AR-LLM보다 우수하며 SQuAD에서 훈련할 때 가장 큰 차이가 관찰됩니다.</p>
</div>
<div id="S4.SS5.p7" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p7.1">이러한 결과는 정확한 질의응답을 위해 컨텍스트의 필요성을 동적으로 결정하는 적응적 검색 방법의 효율성을 강조하며, 이는 항상 또는 결코 컨텍스트를 검색하지 않는 고정된 전략에 비해 향상된 성능을 가져온다.</p>
</div>
<div id="S4.SS5.p8" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p8.1">훈련 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p8.1.1">Adapt-LLM</span> on NQ 또는 SQuAD는 상대적으로 미미하지만 주어진 평가 세트에 대한 훈련 세트의 적합성을 결정하려고 한다. 훈련 세트(NQ 및 SQuAD) 및 평가 세트(PopQA)가 모두 위키피디아를 기반으로 하는 반면, 미묘한 차이가 존재할 수 있다.</p>
</div>
<div id="S4.SS5.p9" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p9.2">표 <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>는 총 질문 수와 질문 및 답변당 평균 단어 수를 포함하여 실험 절차에 포함된 세 데이터 세트의 특성에 대한 통찰력을 제공한다. NQ가 질문 및 답변 길이 측면에서 PopQA에 더 가까운 것으로 보이지만, SQuAD 상의 트레이닝 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p9.2.1">Adapt-LLM</span>의 더 나은 결과에 영향을 미치는 핵심 요소는 트레이닝 데이터세트의 질문 수일 수 있다(<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p9.1.m1.1"><semantics id="S4.SS5.p9.1.m1.1a"><mo id="S4.SS5.p9.1.m1.1.1" xref="S4.SS5.p9.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p9.1.m1.1.1.cmml" xref="S4.SS5.p9.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.1.m1.1c">\sim</annotation></semantics></math>87K in SQuAD and <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p9.2.m2.1"><semantics id="S4.SS5.p9.2.m2.1a"><mo id="S4.SS5.p9.2.m2.1.1" xref="S4.SS5.p9.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.2.m2.1b"><csymbol cd="latexml" id="S4.SS5.p9.2.m2.1.1.cmml" xref="S4.SS5.p9.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.2.m2.1c">\sim</annotation></semantics></math>58K in NQ). 훈련 데이터 세트를 주어진 대상 데이터 세트에 더 적합하게 만드는 요인(우리 연구의 범위를 벗어남)을 설명하기 위해서는 추가 분석이 필요하지만 이러한 결과는 규모가 다시 한 번 중요한 역할을 할 수 있음을 시사한다.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<img src="https://ar5iv.labs.arxiv.org/html/2404.19705/assets/img/istogramma_pop_score_use_ret_nq.png" id="S4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<img src="https://ar5iv.labs.arxiv.org/html/2404.19705/assets/img/istogramma_pop_score_use_ret_squad.png" id="S4.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span>Histograms in the proportion of questions where <span class="ltx_text ltx_font_smallcaps" id="S4.F2.5.1">Adapt-LLM</span> trained on NQ (left) and <span class="ltx_text ltx_font_smallcaps" id="S4.F2.6.2">Adapt-LLM</span> trained on SQuAD (right) ask for extra context for different popularity score intervals.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Contextual Retrieval Decision Analysis</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p1.1">이 실험에서 우리의 목표는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p1.1.1">Adapt-LLM</span> 모델의 효과를 다시 한 번 평가하는 것이며, 이번에는 추가 컨텍스트가 필요한 시기를 정확하게 결정하는 능력에 초점을 맞춘다. 이를 위해 다음 단계를 준수합니다.</p>
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i1.p1.2">우리는 PopQA 테스트 세트를 사용하여 <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i1.p1.2.1">Adapt-LLM</span> 모델에 대해 추론을 수행하여 답변을 직접 반환하거나 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.I3.i1.p1.1.m1.1"><semantics id="S4.I3.i1.p1.1.m1.1a"><mo id="S4.I3.i1.p1.1.m1.1.1" stretchy="false" xref="S4.I3.i1.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><ci id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.I3.i1.p1.2.m2.1"><semantics id="S4.I3.i1.p1.2.m2.1a"><mo id="S4.I3.i1.p1.2.m2.1.1" stretchy="false" xref="S4.I3.i1.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><ci id="S4.I3.i1.p1.2.m2.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">\rangle</annotation></semantics></math>를 반환하여 추가 컨텍스트의 필요성을 나타낸다.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i2.p1.2"><span class="ltx_text ltx_font_smallcaps" id="S4.I3.i2.p1.2.1">Adapt-LLM</span> 모델로부터 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.I3.i2.p1.1.m1.1"><semantics id="S4.I3.i2.p1.1.m1.1a"><mo id="S4.I3.i2.p1.1.m1.1.1" stretchy="false" xref="S4.I3.i2.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><ci id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.I3.i2.p1.2.m2.1"><semantics id="S4.I3.i2.p1.2.m2.1a"><mo id="S4.I3.i2.p1.2.m2.1.1" stretchy="false" xref="S4.I3.i2.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><ci id="S4.I3.i2.p1.2.m2.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">\rangle</annotation></semantics></math> 응답을 수신하는 경우, 다음 단계를 진행한다:</p>
<ol id="S4.I3.i2.I1" class="ltx_enumerate">
<li id="S4.I3.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1.</span> 
<div id="S4.I3.i2.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i2.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I3.i2.I1.i1.p1.1.1">Adapt-LLM</span> 모델에 대해 추론을 수행하여 IR 시스템에서 얻은 컨텍스트가 주어진 답변을 반환하도록 프롬프트한다.</p>
</div>
</li>
<li id="S4.I3.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2.</span> 
<div id="S4.I3.i2.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i2.I1.i2.p1.1">또한 추가 컨텍스트 없이 직접 답변을 제공하는 명령어를 사용하여 NR-LLM 모델에 대한 추론을 수행한다.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i3.p1.1">If <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i3.p1.1.1">Adapt-LLM</span> model decides to answer the question directly relying on its parametric memory:</p>
<ol id="S4.I3.i3.I1" class="ltx_enumerate">
<li id="S4.I3.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.1.</span> 
<div id="S4.I3.i3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i3.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I3.i3.I1.i1.p1.1.1">Adapt-LLM</span> 모델에 대해 추론을 수행하여 컨텍스트를 제공하지 않고 답변을 반환하도록 프롬프트한다.</p>
</div>
</li>
<li id="S4.I3.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.2.</span> 
<div id="S4.I3.i3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I3.i3.I1.i2.p1.1">우리는 AR-LLM 모델에 대해 IR 시스템에서 검색된 컨텍스트를 사용하여 답변을 제공하는 지침을 사용하여 추론을 수행한다.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p2.2">표 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>는 본 실험의 결과를 제시한다. 첫 번째로 주목할 점은 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p2.2.1">Adapt-LLM</span> 모델은 PopQA 데이터 세트에서 질문의 약 82-83%에 대해 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p2.1.m1.1"><semantics id="S4.SS6.p2.1.m1.1a"><mo id="S4.SS6.p2.1.m1.1.1" stretchy="false" xref="S4.SS6.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><ci id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p2.2.m2.1"><semantics id="S4.SS6.p2.2.m2.1a"><mo id="S4.SS6.p2.2.m2.1.1" stretchy="false" xref="S4.SS6.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.1b"><ci id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.1c">\rangle</annotation></semantics></math> 토큰을 생성하며, 두 훈련 데이터 세트 모두에서 유사한 비율이 관찰된다는 것이다. 이 관찰은 표 <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>에서 입증된 NR-LLM 구성의 낮은 성능과 일치한다.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p3.2">그러나 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.1">Adapt-LLM</span>은 질문에 정확하게 대답하기 위해 추가 컨텍스트가 필요한 시기를 일관되게 결정합니다. NQ 및 SQuAD 트레이닝 데이터 세트 모두에 걸쳐, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.2">Adapt-LLM</span>은 컨텍스트가 없는 NR-LLM 모델의 정확도에 비해 컨텍스트를 검색할 때 훨씬 더 높은 정확도를 나타낸다(표 <a idx=0></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p3.2.m2.1"><semantics id="S4.SS6.p3.2.m2.1a"><mo id="S4.SS6.p3.2.m2.1.1" stretchy="false" xref="S4.SS6.p3.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p3.2.m2.1b"><ci id="S4.SS6.p3.2.m2.1.1.cmml" xref="S4.SS6.p3.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p3.2.m2.1c">\rangle</annotation></semantics></math> 열의 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>에 표시된 바와 같이). 구체적으로, NQ 데이터 세트의 경우, 컨텍스트 요청 시 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.3">Adapt-LLM</span> 모델의 정확도는 33.04%인 반면, 컨텍스트 검색이 없는 NR-LLM 모델의 정확도는 14.65%로 현저하게 낮다. 유사하게, SQuAD 데이터 세트의 경우, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.4">Adapt-LLM</span>은 컨텍스트 검색에서 33.40%의 정확도를 달성하는 반면, 컨텍스트가 없는 NR-LLM 모델의 정확도는 9.94%로 실질적으로 더 낮다.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Passages</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">SQuAD Dev</span></th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">NQ Dev</span></th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.2.2.1.1" class="ltx_text ltx_font_bold">Acc.</span></th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.2.2.2.1" class="ltx_text ltx_font_bold">Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<td id="S4.T4.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">Gold</td>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.2.1" class="ltx_text ltx_font_bold">89.42%</span></td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.3.1" class="ltx_text ltx_font_bold">69.76%</span></td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<td id="S4.T4.1.4.2.1" class="ltx_td ltx_align_center ltx_border_bb">Contriever</td>
<td id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">22.49</td>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">27.04%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span> <span class="ltx_text ltx_font_smallcaps" id="S4.T4.3.1">Adapt-LLM</span> for the SQuAD and NQ dev sets, when use the gold passages provided by the datasets and use the best passage retrieved by Contriever.</figcaption>
</figure>
<div id="S4.SS6.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p4.2">마지막으로, 표 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>의 마지막 열(No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p4.1.m1.1"><semantics id="S4.SS6.p4.1.m1.1a"><mo id="S4.SS6.p4.1.m1.1.1" stretchy="false" xref="S4.SS6.p4.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.1.m1.1b"><ci id="S4.SS6.p4.1.m1.1.1.cmml" xref="S4.SS6.p4.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p4.2.m2.1"><semantics id="S4.SS6.p4.2.m2.1a"><mo id="S4.SS6.p4.2.m2.1.1" stretchy="false" xref="S4.SS6.p4.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.2.m2.1b"><ci id="S4.SS6.p4.2.m2.1.1.cmml" xref="S4.SS6.p4.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.2.m2.1c">\rangle</annotation></semantics></math>)은 그 파라메트릭 메모리만을 기반으로 질문에 답할 때 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p4.2.1">Adapt-LLM</span>의 성능을 보여준다. 알 수 있듯이 컨텍스트를 활용하지 않을 때 62% 이상의 정확도가 얻어져 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p4.2.2">Adapt-LLM</span>이 컨텍스트 검색과 질문에 대한 직접 답변 제공 사이를 효과적으로 구분한다는 추가 증거를 제공한다. 또한 문맥이 입력에 추가될 때 이러한 질문의 성능을 평가하여 최대 7개의 절대점의 정확도가 크게 감소함을 보여준다.</p>
</div>
<div id="S4.SS6.p5" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p5.1">이러한 연구 결과는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p5.1.1">Adapt-LLM</span> 모델이 정확한 응답 생성을 위한 추가 컨텍스트의 필요성을 결정하는 데 사용되는 의사 결정 프로세스의 효과에 대한 통찰력을 제공하고 질문 응답 모델의 정확도를 향상시키는 데 동적 컨텍스트 검색 수행의 필요성에 대한 경험적 증거를 제시한다.</p>
</div>
<div id="S4.SS6.p6" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p6.1">그러나 표 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>(약 33%)에서 관찰된 바와 같이 검색된 컨텍스트로 질문에 응답할 때 모델의 전체 성능이 상대적으로 낮다는 것이 주목할 만하다. 이 관찰을 더 탐구하기 위해 우리는 추가 실험을 수행한다: NQ 및 SQuAD 개발 분할에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p6.1.1">Adapt-LLM</span> (NQ 및 SQuAD에서 훈련된 버전 모두) 데이터 세트의 골드 패시지와 IR 시스템인 Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>에서 검색된 컨텍스트를 사용할 때의 성능을 비교한다. 불행히도 PopQA는 금 구절을 제공하지 않기 때문에 직접적인 평가는 불가능했다.</p>
</div>
<div id="S4.SS6.p7" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p7.1">표 <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4.6 Contextual Retrieval Decision Analysis ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4</span></a>는 본 실험의 결과를 제시한다. 두 데이터 세트(SQuAD의 경우 약 67개의 절대점 및 NQ의 경우 42개)에 대해 Contriever에서 검색한 골드 통로와 상단 통로를 사용하는 것 사이에 상당한 성능 차이가 관찰된다. 이것은 Contriever 및 현재 IR 시스템이 일반적으로 주어진 질문에 답하기 위해 가장 적절한 구절을 일관되게 검색하지 않는다는 것을 나타낸다. 이 관찰은 가장 성공적인 오픈 도메인 QA 시스템 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>에서 볼 수 있듯이 컨텍스트로 여러 문서를 검색하는 것의 중요성을 강조하고 PopQA에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p7.1.1">Adapt-LLM</span>의 전체 성능에 미치는 영향을 강조한다.</p>
</div>
<div id="S4.SS6.p8" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p8.2">추가 컨텍스트 요청 시 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.1">Adapt-LLM</span>의 동작을 추가로 검증하기 위해, 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4.5 Validating the Adaptive Retrieval Approach ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>는 우리 모델이 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p8.1.m1.1"><semantics id="S4.SS6.p8.1.m1.1a"><mo id="S4.SS6.p8.1.m1.1.1" stretchy="false" xref="S4.SS6.p8.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.1.m1.1b"><ci id="S4.SS6.p8.1.m1.1.1.cmml" xref="S4.SS6.p8.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p8.2.m2.1"><semantics id="S4.SS6.p8.2.m2.1a"><mo id="S4.SS6.p8.2.m2.1.1" stretchy="false" xref="S4.SS6.p8.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.2.m2.1b"><ci id="S4.SS6.p8.2.m2.1.1.cmml" xref="S4.SS6.p8.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.2.m2.1c">\rangle</annotation></semantics></math> 토큰을 생성하는 질문의 비율을 보여줍니다. 인기 점수 간격으로 집계됩니다(<span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.2">Adapt-LLM</span> trained on NQ and right image for SQuAD). <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>는 인기도가 높은 질문은 LLM의 매개변수 기억을 사용하여 적절하게 대답할 수 있는 반면 인기 점수가 낮으면 추가 컨텍스트가 필요함을 시사한다. 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4.5 Validating the Adaptive Retrieval Approach ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.3">Adapt-LLM</span>의 두 버전 모두에 대해 이 패턴을 관찰하며, 이는 훈련 또는 추론 중 인기 점수에 대한 액세스가 부족함에도 불구하고 우리 모델이 추가 컨텍스트를 요청하기 위한 효과적인 기준을 학습했음을 나타낸다.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Comparison with state-of-the-art methods</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS7.p1.1">우리는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p1.1.1">Adapt-LLM</span> 모델과 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>에 의해 제안된 PopQA에 대한 현재 최첨단 접근법 간의 비교 분석을 수행했다. 그들의 방법론은 질문이 추가 컨텍스트를 필요로 하는지 여부를 결정하기 위해 PopQA 데이터 세트에 주석이 달린 인기 점수에 의존한다. 문제 인기도를 결정하는 최적의 임계값을 설정하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite> PopQA 데이터 세트를 임계값 결정을 위한 개발 세트로 75%, 테스트 세트로 25%로 분할했다. 원본 논문에서, 그들은 이 방법론을 그 순간에 이용 가능한 다양한 LLM에 적용한다(Llama-2는 아직 출시되지 않았다).</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS7.p2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.1">Adapt-LLM</span>과 인기 기반 방법 간의 공정한 비교를 보장하기 위해 동일한 PopQA 개발 세트를 사용하여 최고의 인기 점수 임계값(707,000으로 발견됨)을 결정하기 위해 Llama-2 7B 모델을 사용하여 접근법을 복제했다. 이를 통해 기본 LLM을 활용하면서 그들의 방법론과 일치하는 결과를 얻을 수 있었다. 더 작은 모델을 사용할 때 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>의 원래 결과와 유사하게, 인기 점수 임계값은 항상 Llama-2 7B에 대한 컨텍스트 정보를 검색하는 것과 거의 동등하다. IR 사용량은 표 <a class="ltx_ref" href="#S4.T5" title="Table 5 ‣ 4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">5</span></a>에 제시된 바와 같이 99.86%이다. 이는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.2">GPT-3 davinci-003</span>이 Contriever를 사용하여 적응 검색을 사용할 때 원본 논문에서 IR 사용량을 80% 미만으로 얻는 유일한 모델인 더 작은 크기의 모델에 대해 인기 점수 방법이 어떻게 어려움을 겪고 있는지 명확하게 보여준다. 이후 동일한 25% 테스트 세트 분할에 대해 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.3">Adapt-LLM</span> 구성을 평가하고 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>에서 설명한 방법을 사용하여 얻은 결과와 비교했다. 이 체계적인 비교를 통해 현재 기술과 관련하여 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.4">Adapt-LLM</span> 모델의 효능을 평가할 수 있었다.</p>
</div>
<div id="S4.SS7.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS7.p3.1">이 실험의 결과는 표 <a class="ltx_ref" href="#S4.T5" title="Table 5 ‣ 4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">5</span></a>에 제시되어 있다. 우리는 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>의 복제 접근법과 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.1">Adapt-LLM</span>이 NQ 및 SQuAD 데이터 세트에 대해 훈련되고 PopQA의 25% 하위 집합에서 테스트될 때 유사한 성능을 관찰한다. <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.2">Adapt-LLM</span>은 인기 점수 및 PopQA 데이터 세트의 75% 부분을 직접 사용하여 해당 인기 점수에 대한 최적의 값을 찾는 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>와 달리 PopQA의 정보를 사용하지 않는다는 점을 언급할 가치가 있습니다. 이 방법론은 인기 점수가 PopQA의 고유한 기능이기 때문에 다른 개방형 질문 응답 작업에 일반화할 수 없다. 그러나 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.3">Adapt-LLM</span>은 유사한 데이터 세트에 적용할 수 있습니다. 이러한 특성을 감안할 때 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.4">Adapt-LLM</span>에 의해 얻은 결과는 데이터 세트 특정 정보를 활용하는 접근법과 비교할 수 있는 성능을 제공하는 훨씬 더 중요하다고 믿는다. 이러한 발견은 테스트에 사용된 데이터 세트와 다른 데이터 세트로 훈련된 경우에도 우리의 접근법의 유효성을 입증한다.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Configuration</span></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">IR usage</span></th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">Popularity Score</span></td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">99.86%</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">36.81%</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM (NQ)</span></td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center">87.22%</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center">35.30%</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">Adapt-LLM (SQuAD)</span></td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">83.99%</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.4.3.3.1" class="ltx_text ltx_font_bold">37.29%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span> <span class="ltx_text ltx_font_smallcaps" id="S4.T5.4.1">Adapt-LLM</span> 및 <span class="ltx_text ltx_font_smallcaps" id="S4.T5.5.2">Popularity Score</span> 구성에 대한 SQuAD 및 NQ 데이터 세트에 대해 학습된 Llama-2 기본 모델의 성능 비교. 이후에는 Llama-2 LLM을 기본 모델로 하여 <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. [<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>에서 제안한 방법론을 모방한다.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.2">본 논문에서는 파라메트릭 메모리에만 의존하지 않고 질문에 응답하기 위해 추가 컨텍스트가 필요할 때 식별하도록 학습하는 LLM인 <span class="ltx_text ltx_font_smallcaps" id="S5.p1.2.1">Adapt-LLM</span>을 소개한다. <span class="ltx_text ltx_font_smallcaps" id="S5.p1.2.2">Adapt-LLM</span>은 LLM의 파라메트릭 메모리만으로 답변 가능한 질문과 보충 컨텍스트가 필요한 질문을 구별하기 위해 수정된 개방형 도메인 질문 응답 데이터 세트에서 기본 LLM을 미세 조정한 결과이다. 이러한 학습 데이터 세트를 구성하기 위해 먼저 기본 LLM을 제로 샷 평가에 적용하여 질문에 대한 응답 정확도를 결정한다. 모델의 응답이 잘못된 질문의 경우 추가 컨텍스트의 필요성을 나타내는 특수 토큰인 <math alttext="\langle" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" stretchy="false" xref="S5.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\langle</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mo id="S5.p1.2.m2.1.1" stretchy="false" xref="S5.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\rangle</annotation></semantics></math>를 생성하도록 LLM을 훈련한다.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p" id="S5.p2.1">PopQA 데이터 세트에 대해 수행된 광범위한 실험을 통해 <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">Adapt-LLM</span>이 관련 컨텍스트 정보를 절대 검색하지 않고 항상 검색하는 두 가지 고정된 대안보다 더 나은 성능을 발휘한다는 것을 보여준다. 또한, 연구 결과는 이 작업의 주요 목적인 추가 컨텍스트의 필요성을 효과적으로 식별하는 <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">Adapt-LLM</span>의 기능을 강조한다.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p" id="S5.p3.1">추후 조사를 위해 학습 가능한 순차 검색 기술을 통합하는 것과 같은 IR 시스템을 사용할 때 성능을 향상시키기 위한 탐색 방법을 제안한다. 또한 <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.1">Adapt-LLM</span> 시스템 개발에서 훈련 및 테스트 데이터 세트 간의 상호 작용에 대한 보다 심층적인 분석을 수행하는 것이 유용할 것이라고 믿는다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
J.&nbsp;Achiam, S.&nbsp;Adler, S.&nbsp;Agarwal, L.&nbsp;Ahmad, I.&nbsp;Akkaya, F.&nbsp;L. Aleman, D.&nbsp;Almeida, J.&nbsp;Altenschmidt, S.&nbsp;Altman, S.&nbsp;Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber [2024]</span>
<span class="ltx_bibblock">
Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber.

</span>
<span class="ltx_bibblock">RAG makes LLMs better and equal.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.pinecone.io/blog/rag-study/</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
S.&nbsp;Barnett, S.&nbsp;Kurniawan, S.&nbsp;Thudumu, Z.&nbsp;Brannelly, and M.&nbsp;Abdelrazek.

</span>
<span class="ltx_bibblock">Seven failure points when engineering a retrieval augmented generation system.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.05856</em>, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berger et&nbsp;al. [2000]</span>
<span class="ltx_bibblock">
A.&nbsp;Berger, R.&nbsp;Caruana, D.&nbsp;Cohn, D.&nbsp;Freitag, and V.&nbsp;Mittal.

</span>
<span class="ltx_bibblock">Bridging the lexical chasm: statistical approaches to answer-finding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</em>, pages 192–199, 2000.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others [2022]</span>
<span class="ltx_bibblock">
Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 2206–2240. PMLR, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others [2020]</span>
<span class="ltx_bibblock">
Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke [2018]</span>
<span class="ltx_bibblock">
Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke.

</span>
<span class="ltx_bibblock">QuAC: Question Answering in Context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2174–2184, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
T.&nbsp;Gao, X.&nbsp;Yao, and D.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021</em>, pages 6894–6910. Association for Computational Linguistics (ACL), 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen [2023]</span>
<span class="ltx_bibblock">
Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave [2022]</span>
<span class="ltx_bibblock">
Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei [2020]</span>
<span class="ltx_bibblock">
Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei.

</span>
<span class="ltx_bibblock">REALM: retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine Learning</em>. JMLR.org, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard, Gautier and Grave, Edouard [2021]</span>
<span class="ltx_bibblock">
Izacard, Gautier and Grave, Edouard.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">EACL 2021-16th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 874–880. Association for Computational Linguistics, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale [2023]</span>
<span class="ltx_bibblock">
Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1–38, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Roux, A.&nbsp;Mensch, B.&nbsp;Savary, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l. Casas, E.&nbsp;B. Hanna, F.&nbsp;Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
V.&nbsp;Karpukhin, B.&nbsp;Oguz, S.&nbsp;Min, P.&nbsp;Lewis, L.&nbsp;Wu, S.&nbsp;Edunov, D.&nbsp;Chen, and W.-t. Yih.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769–6781, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others [2019]</span>
<span class="ltx_bibblock">
Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others [2020]</span>
<span class="ltx_bibblock">
Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
P.&nbsp;Liang, R.&nbsp;Bommasani, T.&nbsp;Lee, D.&nbsp;Tsipras, D.&nbsp;Soylu, M.&nbsp;Yasunaga, Y.&nbsp;Zhang, D.&nbsp;Narayanan, Y.&nbsp;Wu, A.&nbsp;Kumar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin, Stephanie and Hilton, Jacob and Evans, Owain [2022]</span>
<span class="ltx_bibblock">
Lin, Stephanie and Hilton, Jacob and Evans, Owain.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring How Models Mimic Human Falsehoods.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
A.&nbsp;T. Mallen, A.&nbsp;Asai, V.&nbsp;Zhong, R.&nbsp;Das, D.&nbsp;Khashabi, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">The 61st Annual Meeting Of The Association For Computational Linguistics</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others [2021]</span>
<span class="ltx_bibblock">
Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.09332</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li [2016]</span>
<span class="ltx_bibblock">
Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li.

</span>
<span class="ltx_bibblock">Ms marco: A human-generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy [2016]</span>
<span class="ltx_bibblock">
Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, pages 2383–2392, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav [2023]</span>
<span class="ltx_bibblock">
Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1316–1331, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap, J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych [2019]</span>
<span class="ltx_bibblock">
N.&nbsp;Reimers and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et&nbsp;al. [2009]</span>
<span class="ltx_bibblock">
S.&nbsp;Robertson, H.&nbsp;Zaragoza, et&nbsp;al.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Information Retrieval</em>, 3(4):333–389, 2009.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro, L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun [2022]</span>
<span class="ltx_bibblock">
Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun.

</span>
<span class="ltx_bibblock">Two-Step Question Retrieval for Open-Domain QA.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">60th Annual Meeting of the Association for Computational Linguistics, ACL 2022</em>, pages 1487–1492. Association for Computational Linguistics, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B [2023]</span>
<span class="ltx_bibblock">
Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.

</span>
<span class="ltx_bibblock">Stanford alpaca: an instruction-following llama model (2023).

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">URL https://github. com/tatsu-lab/stanford_alpaca</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
N.&nbsp;Thakur, N.&nbsp;Reimers, A.&nbsp;Rücklé, A.&nbsp;Srivastava, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix, B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei, N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
F.&nbsp;Zhu, W.&nbsp;Lei, C.&nbsp;Wang, J.&nbsp;Zheng, S.&nbsp;Poria, and T.-S. Chua.

</span>
<span class="ltx_bibblock">Retrieving and reading: A comprehensive survey on open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00774</em>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2404.19704" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2404.19705" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.19705">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.19705" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2404.19706" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 16:40:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
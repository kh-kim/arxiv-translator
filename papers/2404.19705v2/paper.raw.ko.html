<html lang="en" data-theme="dark"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</title>
<!--Generated on Mon May  6 19:07:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons_new.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2404.19705v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.19705v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="Dark mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.19705v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.19705v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S2" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Adaptive Retrieval LLM (<span class="ltx_text ltx_font_smallcaps">Adapt-LLM</span>)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="In 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Training <span class="ltx_text ltx_font_smallcaps">Adapt-LLM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS2" title="In 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments and Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1.SSS0.Px1" title="In 4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title">NQ</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1.SSS0.Px2" title="In 4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title">SQuAD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1.SSS0.Px3" title="In 4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title">PopQA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS2" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Base Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS3" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Model Configurations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS4" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS5" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Validating the Adaptive Retrieval Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS6" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Contextual Retrieval Decision Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS7" title="In 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Comparison with state-of-the-art methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S5" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S6" title="In When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert" onclick="closePopup()">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: spverbatim</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2404.19705v2 [cs.CL] 06 May 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiziano&nbsp;Labruna<span class="ltx_ERROR undefined" id="id5.1.id1">\orcid</span>0000-0001-7713-7679
</span><span class="ltx_author_notes">Corresponding Author. Email: tlabruna@fbk.eu.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jon Ander&nbsp;Campos<span class="ltx_ERROR undefined" id="id6.1.id1">\orcid</span>0000-0002-1447-5870
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gorka&nbsp;Azkune<span class="ltx_ERROR undefined" id="id7.1.id1">\orcid</span>0000-0002-2506-7426
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Bozen-Bolzano
</span>
<span class="ltx_contact ltx_role_address">Fondazione Bruno Kessler
</span>
<span class="ltx_contact ltx_role_address">Cohere
</span>
<span class="ltx_contact ltx_role_address">HiTZ Center - Ixa, University of the Basque Country UPV/EHU
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.4">본 논문에서는 LLM(Large Language Models)이 주어진 질문에 답하기 위해 추가적인 컨텍스트가 필요할 때 특히 기성 정보 검색(Off-the-Shelf Information Retrieval, IR) 시스템을 사용하는 방법을 효과적으로 학습할 수 있음을 보인다. IR 시스템의 성능을 고려할 때, 질문 응답을 위한 최적 전략은 항상 외부 정보 검색을 수반하는 것은 아니며, 오히려 LLM 자체의 매개변수 메모리를 활용하는 것을 종종 수반한다. 이전 연구에서는 PopQA 데이터 세트에서 이러한 현상을 확인했는데, 가장 인기 있는 질문은 LLM의 매개변수 메모리를 사용하여 효과적으로 해결되는 반면 덜 인기 있는 질문은 IR 시스템 사용을 필요로 한다. 다음으로, 우리는 기존의 오픈 도메인 질의 응답 데이터 세트를 활용하여 LLMs에 대한 맞춤형 훈련 접근법을 제안한다. 여기서, LLMs는 질문에 대한 답변을 알지 못하는 경우 특수 토큰인 <math alttext="\langle" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" stretchy="false" xref="id1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" stretchy="false" xref="id2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">⟩</annotation></semantics></math>를 생성하도록 훈련된다. PopQA 데이터 세트에 대한 적응형 검색 LLM(<span class="ltx_text ltx_font_smallcaps" id="id4.4.1">Adapt-LLM</span>)의 평가는 (i) 모든 질문에 대한 정보를 검색하는 것, (ii) 항상 LLM의 파라메트릭 메모리를 사용하는 것, (iii) 검색기를 사용할 시기를 결정하기 위해 인기 임계값을 사용하는 것의 세 가지 구성에서 동일한 LLM에 대한 개선을 보여준다. 분석을 통해 <span class="ltx_text ltx_font_smallcaps" id="id4.4.2">Adapt-LLM</span>은 IR에 대한 필요성을 나타내는 질문에 답하는 방법을 모른다고 결정할 때 <math alttext="\langle" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" stretchy="false" xref="id3.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" stretchy="false" xref="id4.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><ci id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">⟩</annotation></semantics></math> 토큰을 생성할 수 있는 반면, 파라메트릭 메모리에만 의존하도록 선택할 때 현저하게 높은 정확도 수준을 달성한다는 것을 보여준다.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\paperid</span>
<p class="ltx_p" id="p1.2">123</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">질의응답(QA)의 과제는 자연어 이해 연구의 초점으로 남아 있다. QA 모델을 평가하기 위한 벤치마크 역할을 하는 많은 다른 데이터 세트가 있는데, 예를 들어 NQ(Natural Questions) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib18" title="">18</a>]</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib25" title="">25</a>]</cite> 또는 QuAC <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib7" title="">7</a>]</cite> 등이 있다. 오늘날, 대형 언어 모델(LLM)은 이러한 벤치마크에서 전통적인 방법을 일관되게 능가하여 놀라운 성능을 보여준다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">전형적으로, 질의 응답을 위해 LLMs를 활용하는 두 가지 주요 접근법이 있다:</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">(i) <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Closed Book Question Answering</span>: 이 접근법은 성능을 향상시키기 위해 명령어 튜닝 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib32" title="">32</a>]</cite> 또는 소수의shot 프롬프트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib6" title="">6</a>]</cite>와 같은 전략을 포함한다. 여기서 LLM은 질문에 답하기 위해 매개변수 기억에만 의존한다. 그러나 이러한 모수적 기억은 훈련 코퍼스에 전적으로 기반을 두고 있기 때문에 내재적 한계를 가지고 있으며, 이는 예를 들어 훈련 과정 후에 발생하는 사건에 대해 구식일 수 있다는 것을 의미한다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">(ii) <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Open Book Question Answering</span>: 이 접근법에서 LLM은 IR(Information Retriever) 시스템 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib36" title="">36</a>]</cite>와 결합된다. IR 시스템을 활용함으로써 LLM은 관련 컨텍스트를 검색하여 이해도를 보완하고 보다 정확한 답변을 제공할 수 있다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">그러나 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>에 의해 수행된 연구는 질문 응답 전략의 복잡성을 조명하여 최적의 접근 방식이 항상 IR 시스템의 활용을 포함한다는 개념에 도전한다. 인기 점수로 주석이 달린 14,000개의 질문으로 구성된 PopQA 데이터 세트의 도입을 통해, 그들은 모수적 기억에만 의존하는 LLM이 인기 높은 질문을 다루는 데 탁월하지만 IR을 사용하면 인기 낮은 질문에 대한 효능이 감소한다는 것을 보여주었다.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.g1" src="https://arxiv.org/html/2404.19705v2/x1.png" width="747">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span><span class="ltx_text ltx_font_smallcaps" id="S1.F1.6.1">Adapt-LLM</span>step-by-step: 주어진 질문(단계 1), LLM이 질문에 직접 답할지(단계 3) 또는 추가 컨텍스트 정보를 요청할지를 결정(단계 2), 특수 <math alttext="\langle" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mo id="S1.F1.3.m1.1.1" stretchy="false" xref="S1.F1.3.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><mo id="S1.F1.4.m2.1.1" stretchy="false" xref="S1.F1.4.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m2.1e">⟩</annotation></semantics></math> 토큰을 생성, 나중에, off-the-shelf IR 시스템이 관련 컨텍스트를 검색(단계 4)하는데 사용되며, 이는 질문과 함께 사용되어 최종 답변에 대한 LLM을 다시 프롬프트한다(단계 5).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">그들의 연구 결과는 LLM이 인기 높은 질문에 모수 메모리를 활용하지만 인기 낮은 질문에 답하기 위해 관련 컨텍스트를 검색하기 위해 기성 IR 시스템을 사용하는 하이브리드 접근법의 중요성을 강조한다. 그들의 방법론의 핵심은 IR 시스템을 사용해야 하는지 여부를 결정하는 데 사용되는 고정된 인기 점수 임계값을 설정하는 것이다.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">그러나 많은 경우에, 질의 응답 데이터 세트는 인기 점수를 포함하지 않으므로, 그러한 점수에 의존하는 것은 일반화할 수 있는 접근법이 아니다. 이러한 제한으로 인해, 본 연구는 LLMs가 개선된 질문 답변을 위해 IR 시스템을 사용할 시기를 자율적으로 결정할 수 있는지 여부를 다루는 것을 목표로 한다. 이를 조사하기 위해 개방형 질문 응답 데이터 세트를 사용하여 LLM에 대한 평가를 수행하여 LLM이 정확한 응답을 제공하는 질문과 응답이 잘못된 질문을 식별한다.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.2">구체적으로, LLM 응답이 잘못된 질문의 경우, 추가 컨텍스트의 필요성을 나타내는 특수 토큰인 <math alttext="\langle" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" stretchy="false" xref="S1.p8.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S1.p8.2.m2.1"><semantics id="S1.p8.2.m2.1a"><mo id="S1.p8.2.m2.1.1" stretchy="false" xref="S1.p8.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><ci id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S1.p8.2.m2.1d">⟩</annotation></semantics></math>로 주석을 달게 된다. 이어서, 우리는 이러한 주석을 사용하여 훈련 목적을 위해 맞춤화된 새로운 데이터 세트를 구성하며, 여기서 LLM이 답변에 대해 확신하거나 질문에 답하는 데 유용하다고 믿는 컨텍스트를 요구하는 경우 직접 답하도록 가르친다(도 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a> 참조). 우리의 가설은 이 훈련 과정을 통해 LLM이 질문에 답하기 위해 추가 컨텍스트가 필요할 때 IR 시스템을 사용하도록 학습하므로 <span class="ltx_text ltx_font_smallcaps" id="S1.p8.2.1">Adapt-LLM</span>이라는 이름을 붙인다.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">가설을 검증하기 위해 하이브리드 검색 전략을 벤치마킹하는 데 적합한 플랫폼을 제공하기 때문에 PopQA 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>에 대해 여러 실험을 수행했다. 이러한 실험들의 결과로서 우리는 다음과 같은 것을 발견한다:</p>
</div>
<div class="ltx_para" id="S1.p10">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">Adapt-LLM</span>은 (i) 모든 질문에 대해 IR 시스템을 사용하고 (ii) LLM의 파라메트릭 메모리에만 의존하는 것과 같은 질문 응답을 위한 전형적인 고정 전략을 일관되게 능가한다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">Adapt-LLM</span>은 인기 점수 또는 유사한 메트릭을 활용하지 않더라도 IR 시스템을 사용할 시기를 결정하기 위해 인기 점수에 의존하는 전략에 필적하는 성능을 보여줍니다. 인기 점수는 PopQA 데이터 세트의 고유한 기능으로, 다른 오픈 도메인 질문 응답 데이터 세트에 적용할 수 없게 만든다는 점에 주목할 필요가 있다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.1">Adapt-LLM</span>이 추가 정보를 검색하기로 결정하면 컨텍스트와 함께 얻은 결과가 그렇지 않은 결과보다 훨씬 좋습니다. 마찬가지로 <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.2">Adapt-LLM</span>이 파라메트릭 메모리에 의존하는 질문에 직접 응답할 때 높은 정확도를 달성합니다. 이러한 관찰은 모델이 정보를 검색할 때와 더 이상의 컨텍스트 없이 질문에 답할 수 있는 때를 효과적으로 식별한다는 것을 나타낸다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.I1.i4.p1.1.1">Adapt-LLM</span>의 성능에 대한 기본 병목 현상은 IR 시스템에 있습니다. <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i4.p1.1.2">Adapt-LLM</span>은 IR 시스템에 의해 검색된 패시지에 비해 골드 패시지로 훨씬 더 높은 성능을 달성한다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">본 연구의 결과는 질의응답을 위한 LLM의 성능을 향상시키기 위한 적응적 검색 전략의 중요성을 강조한다. 추가 컨텍스트를 검색할 시기를 동적으로 결정하기 위해 <span class="ltx_text ltx_font_smallcaps" id="S1.p11.1.1">Adapt-LLM</span>을 학습하여 필요한 경우에만 외부 정보 소스를 효과적으로 활용하는 방법을 LLM에 가르치는 가능성을 보여 줍니다.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib19" title="">19</a>]</cite>는 질문 응답 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib23" title="">23</a>]</cite>, 진실성 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib21" title="">21</a>]</cite> 및 언어 모델링 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib26" title="">26</a>]</cite> 등 매우 다양한 NLP 영역에서 개선점을 보였다. 검색된 텍스트 청크에 모델 생성을 접지하는 기능은 또한 더 작은 모델이 더 큰 모델 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib2" title="">2</a>]</cite>의 성능과 일치하도록 했다. 더욱이, LLM을 훈련하는 데 드는 매우 높은 비용으로 인해, RAG는 새로운 사실 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib10" title="">10</a>]</cite>를 통합하기 위해 모델을 주기적으로 재훈련할 필요가 없이 새로운 정보로 업데이트된 상태를 유지하는 표준 방법이 되었다.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">검색과 함께 LLMs를 증가시키는 것이 LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib27" title="">27</a>]</cite>의 현재 생성을 위한 필수 단계라고 하더라도 비용이 수반된다. TF-IDF 또는 BM-25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib29" title="">29</a>]</cite>와 같은 전통적인 검색 방법은 키워드가 겹치는 문서만 검색할 수 있고 어휘 격차 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib4" title="">4</a>]</cite>를 겪는다. 이러한 문제를 해결하기 위해, 많은 사전 훈련된 트랜스포머 인코더 기반 밀집 모델들이 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>로 제안되었다. 훈련된 신경망 모델은 다양한 검색 벤치마크에서 좋은 성능을 보였지만 새로운 도메인 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib33" title="">33</a>]</cite>에 대한 제로 샷 설정에서는 여전히 어려움을 겪고 있다. 검색 엔진의 품질은 검색 강화 모델에 필수적인데, 이는 모델 성능의 상한을 설정할 것이기 때문이다. 또한, 특히 대상 문서 인덱스가 큰 경우 검색 엔진의 사용은 모델의 대기 시간을 크게 증가시키고 실시간 응용 프로그램 사용자 경험 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib3" title="">3</a>]</cite>를 손상시킬 수 있다.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">반면에, 모델들이 스케일링을 계속함에 따라, 그들의 파라미터들에 인코딩된 세계 지식은 너무 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib16" title="">16</a>]</cite>를 수행한다. 이전의 많은 노력들은 언어 모델들이 태스크 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib35" title="">35</a>]</cite>를 풀기 위해 그들의 파라메트릭 지식을 사용할 때, 오픈 도메인 질의 응답과 같은 태스크들에 대해 상당한 양의 세계 지식을 암기할 수 있고 경쟁적 성능을 달성할 수 있다는 것을 보여주었다.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">이 모든 것에 의해 동기화된 적응형 접근법은 새로운 솔루션 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>로 제안되었다. 이 접근법에서, 태스크에 대한 솔루션이 모델의 파라미터들에서 인코딩되면, 모델은 솔루션을 생성하기 위해 직접 사용될 것이다. 반대로 모델의 지식에 답안이 인코딩되어 있지 않다면, 답안 생성은 외부 지식으로 증강될 것이다.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">최근 <cite class="ltx_cite ltx_citemacro_citet">Schick et al. [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib30" title="">30</a>]</cite>는 계산기, 검색 엔진, 캘린더 등을 포함한 간단한 API 호출을 통해 외부 도구를 사용하는 방법과 시기를 스스로 가르칠 수 있는 모델인 Toolformer를 제안하였다. 자기 학습 과정은 LLM을 프롬프트하여 풍부한 합성 텍스트 전용 말뭉치를 기반으로 한다. LLM은 먼저 감독되지 않은 코퍼스 위에 인라인 API 호출을 추가합니다. 그런 다음 API 호출의 실행이 향후 토큰을 예측하는 데 도움이 되는지 여부를 평가 하 여 이러한 API 호출의 유효성을 검사 합니다. 이 비감독 방법은 비증강 LLM과 비교할 때 다양한 작업에서 모델 성능을 크게 향상시키지만 모델을 도구보다 더 많이 사용하게 만든다. 예로서, QA 태스크에 대해 모델은 사례들의 99.3%의 검색 엔진을 사용한다. 본 연구에서는 LLMs에 대한 파라메트릭 지식을 활용하여 필요할 때 검색을 수행하고자 한다. <span class="ltx_text ltx_font_smallcaps" id="S2.p5.1.1">Adapt-LLM</span>은 IR의 사용량을 83.99%로 감소시키면서 바닐라 검색보다 성능을 향상시킨다.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">우리의 작업과 더 유사하게 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>는 비모수 정보를 검색해야 할 때 데이터세트와 측정 방법을 제안한다. 그들은 다양한 인기를 가진 엔터티 세트에 대한 14K 질문을 포함하는 PopQA 데이터 세트를 제시한다. 개체 인기는 위키피디아 페이지의 페이지 뷰에 의해 측정되며, 이러한 QA 태스크를 해결하기 위해 PopQA 데이터 세트에서 계산된 인기도 점수 임계값을 사용한다. 개별 개체의 인기 점수가 임계값 미만이면 검색 단계를 수행합니다. 반대로 점수가 임계값보다 크면 직접 질문에 답한다. 이 방법은 바닐라 검색보다 더 나은 결과를 얻지만 현실적인 QA 시나리오에서는 사용할 수 없는 인기 점수를 계산해야 한다.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">이 분야의 또 다른 관련 기여는 연구와 동시에 <cite class="ltx_cite ltx_citemacro_citet">Erbacher et al. [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib8" title="">8</a>]</cite>에 의한 작업으로, 외부 지식을 활용할 시기를 결정하기 위해 LLM을 훈련했다. 그들은 특히 IR과 관련된 잠재적으로 높은 비용을 감안할 때 환각의 위험과 정보 검색 비용 사이의 최적의 트레이드오프를 찾는 데 중점을 두었다. 우리의 <span class="ltx_text ltx_font_smallcaps" id="S2.p7.1.1">Adapt-LLM</span> 방법은 유사한 접근법을 채택하여 정보를 검색할 때 LLM을 학습합니다. 그러나 본 논문에서 제안한 방법의 성능을 일부 기준선과 비교하여 확장하고, 절대 검색하지 않거나 항상 검색하지 않는 전략에 대해 적응적으로 정보 검색의 효율성을 평가한다. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>All resources are publicly available at https://github.com/tLabruna/Adapt-LLM.</span></span></span></p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Adaptive Retrieval LLM (<span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Adapt-LLM</span>)</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">적응적 검색은 질의 응답 태스크에서 답변을 생성하기 위한 추가 컨텍스트 정보를 검색할지 여부를 동적으로 결정하는 모델의 능력을 의미한다. 적응적 검색은 항상 컨텍스트를 통합하거나 고려하지 않는 기존 모델과 달리 모델이 각 질문의 특정 요구 사항에 따라 컨텍스트를 선택적으로 검색할 수 있도록 한다. 이 적응형 접근법은 필요한 경우에만 컨텍스트를 활용하여 성능을 최적화하여 모델의 정확한 답변 생성 능력을 향상시키는 것을 목표로 한다.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">도 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>에 도시된 바와 같이, <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.1">Adapt-LLM</span>의 프로세스는 다음 시퀀스로 전개된다:</p>
</div>
<div class="ltx_para" id="S3.p3">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">질문을 포함하는 첫 번째 프롬프트는 모델로 전송된다(도 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>의 단계 1).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.p1.1.1">Adapt-LLM</span>은 질문을 효과적으로 답변하기 위해 추가적인 컨텍스트가 필요한지 여부를 결정하기 위해 프롬프트를 평가한다(단계 2).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">모델이 컨텍스트가 필요하지 않다고 결정하면, 그것은 자신의 파라메트릭 메모리를 활용하여 질문에 대한 응답을 직접 생성한다(단계 3).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.2">컨텍스트가 필요하다고 간주되면, <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.2.1">Adapt-LLM</span> 모델은 <math alttext="\langle" class="ltx_Math" display="inline" id="S3.I1.i4.p1.1.m1.1"><semantics id="S3.I1.i4.p1.1.m1.1a"><mo id="S3.I1.i4.p1.1.m1.1.1" stretchy="false" xref="S3.I1.i4.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><ci id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.I1.i4.p1.2.m2.1"><semantics id="S3.I1.i4.p1.2.m2.1a"><mo id="S3.I1.i4.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i4.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><ci id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.2.m2.1d">⟩</annotation></semantics></math>로 표현되는 특수 토큰을 반환하고, off-the-shelf IR 시스템은 질문에 기초하여 관련 컨텍스트를 검색하는데 사용된다(단계 4); 이어서 컨텍스트는 답변 생성을 위한 포괄적인 표현을 형성하기 위해 원래의 질문 프롬프트와 결합된다(단계 5).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p4.1.1">Adapt-LLM</span>의 의사 결정 프로세스는 모델이 각 프롬프트의 동적 평가를 통해 질문에 답하기 위한 컨텍스트의 필요성을 결정할 수 있게 한다. 이 유연한 동작을 통해 모델은 향상된 이해를 위해 컨텍스트를 활용하는 것과 충분한 경우 직접적인 답변을 전달하는 것 사이의 균형을 맞출 수 있다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.1.1">Adapt-LLM</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">여기서는 <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">Adapt-LLM</span> 모델을 훈련하는 데 사용된 방법론을 설명합니다. <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">D</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1b" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.5" xref="S3.SS1.p1.1.m1.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p1.1.m1.1.1.3.3.1c" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.6" xref="S3.SS1.p1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝑆</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.6.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>로 표기된 트레이닝 데이터를 크래프팅하는 과정은 알고리즘 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#algorithm1" title="In 3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>에 제시되어 있다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">질문들 <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_Q</annotation></semantics></math>, 연관된 컨텍스트 통로들 <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_P</annotation></semantics></math>, 및 대응하는 답변들 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_A</annotation></semantics></math>를 포함하는 오픈 도메인 질문 응답 데이터세트를 선택하는 것으로 시작한다. <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">D</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.4" xref="S3.SS1.p2.4.m4.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1b" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.5" xref="S3.SS1.p2.4.m4.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.1c" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.6" xref="S3.SS1.p2.4.m4.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐷</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">𝑆</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.5.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.6.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>를 빈 집합(알고리즘의 라인 1)으로 초기화한다. <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_Q</annotation></semantics></math>의 각 질문에 대해 검색 메커니즘 없이 기본 LLM을 활용하여 제로샷 추론(라인 3)을 수행한다. 이 단계를 통해 모델이 정답을 생성하는 질문과 응답이 부정확한 질문을 구별할 수 있습니다. 이 프로세스는 기본 LLM <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.1">knows</span>이 파라메트릭 메모리로 인해 무엇인지 발견하는 방법으로 이해될 수 있다. 모델의 응답이 정확한 질문(라인 4)의 경우 <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.2">parametric_prompt</span>이라고 하는 다음 프롬프트를 포함하는 훈련 세트 인스턴스를 빌드합니다.</p>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.6">
<div class="ltx_listingline" id="algorithm1.6.7">
<span class="ltx_text" id="algorithm1.6.7.1"><span class="ltx_text ltx_font_bold" id="algorithm1.6.7.1.1">Input:</span> </span>Q: questions, A: answers, P: passages, LLM
</div>
<div class="ltx_listingline" id="algorithm1.1.1"> <span class="ltx_text" id="algorithm1.1.1.1"><span class="ltx_text ltx_font_bold" id="algorithm1.1.1.1.1">Output:</span> </span><math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.1.1.m1.1"><semantics id="algorithm1.1.1.m1.1a"><mrow id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml"><mi id="algorithm1.1.1.m1.1.1.2" xref="algorithm1.1.1.m1.1.1.2.cmml">D</mi><mo id="algorithm1.1.1.m1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.1.1.m1.1.1.3" xref="algorithm1.1.1.m1.1.1.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.2" xref="algorithm1.1.1.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.1.1.m1.1.1.3.3" xref="algorithm1.1.1.m1.1.1.3.3.cmml"><mi id="algorithm1.1.1.m1.1.1.3.3.2" xref="algorithm1.1.1.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.3" xref="algorithm1.1.1.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1a" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.4" xref="algorithm1.1.1.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1b" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.5" xref="algorithm1.1.1.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.1.1.m1.1.1.3.3.1c" xref="algorithm1.1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.1.1.m1.1.1.3.3.6" xref="algorithm1.1.1.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1"><times id="algorithm1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1"></times><ci id="algorithm1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.2">𝐷</ci><apply id="algorithm1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.1.1.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3">subscript</csymbol><ci id="algorithm1.1.1.m1.1.1.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.1.1.m1.1.1.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3"><times id="algorithm1.1.1.m1.1.1.3.3.1.cmml" xref="algorithm1.1.1.m1.1.1.3.3.1"></times><ci id="algorithm1.1.1.m1.1.1.3.3.2.cmml" xref="algorithm1.1.1.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.1.1.m1.1.1.3.3.3.cmml" xref="algorithm1.1.1.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.1.1.m1.1.1.3.3.4.cmml" xref="algorithm1.1.1.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.1.1.m1.1.1.3.3.5.cmml" xref="algorithm1.1.1.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.1.1.m1.1.1.3.3.6.cmml" xref="algorithm1.1.1.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.1.1.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>: A training dataset for Adaptive Retrieval
</div>
<div class="ltx_listingline" id="algorithm1.2.2">
<span class="ltx_tag ltx_tag_listingline">1</span>
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.2.2.m1.1"><semantics id="algorithm1.2.2.m1.1a"><mrow id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml"><mi id="algorithm1.2.2.m1.1.1.2" xref="algorithm1.2.2.m1.1.1.2.cmml">D</mi><mo id="algorithm1.2.2.m1.1.1.1" xref="algorithm1.2.2.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.2.2.m1.1.1.3" xref="algorithm1.2.2.m1.1.1.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.2" xref="algorithm1.2.2.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.2.2.m1.1.1.3.3" xref="algorithm1.2.2.m1.1.1.3.3.cmml"><mi id="algorithm1.2.2.m1.1.1.3.3.2" xref="algorithm1.2.2.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.3" xref="algorithm1.2.2.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1a" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.4" xref="algorithm1.2.2.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1b" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.5" xref="algorithm1.2.2.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.2.2.m1.1.1.3.3.1c" xref="algorithm1.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.2.2.m1.1.1.3.3.6" xref="algorithm1.2.2.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><apply id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1"><times id="algorithm1.2.2.m1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1"></times><ci id="algorithm1.2.2.m1.1.1.2.cmml" xref="algorithm1.2.2.m1.1.1.2">𝐷</ci><apply id="algorithm1.2.2.m1.1.1.3.cmml" xref="algorithm1.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.1.1.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3">subscript</csymbol><ci id="algorithm1.2.2.m1.1.1.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.2.2.m1.1.1.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3"><times id="algorithm1.2.2.m1.1.1.3.3.1.cmml" xref="algorithm1.2.2.m1.1.1.3.3.1"></times><ci id="algorithm1.2.2.m1.1.1.3.3.2.cmml" xref="algorithm1.2.2.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.2.2.m1.1.1.3.3.3.cmml" xref="algorithm1.2.2.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.2.2.m1.1.1.3.3.4.cmml" xref="algorithm1.2.2.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.2.2.m1.1.1.3.3.5.cmml" xref="algorithm1.2.2.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.2.2.m1.1.1.3.3.6.cmml" xref="algorithm1.2.2.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.2.2.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math> = init_empty()
</div>
<div class="ltx_listingline" id="algorithm1.6.8">
<span class="ltx_tag ltx_tag_listingline">2</span>
<span class="ltx_text ltx_font_bold" id="algorithm1.6.8.1">for</span>&nbsp;<em class="ltx_emph ltx_font_italic" id="algorithm1.6.8.2">q, gold_ans, pass in (Q, A, P)</em>&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.8.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.6.9">
<span class="ltx_tag ltx_tag_listingline">3</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
ans = LLM(q)

</div>
<div class="ltx_listingline" id="algorithm1.6.10">
<span class="ltx_tag ltx_tag_listingline">4</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.10.1">if</span>&nbsp;<em class="ltx_emph ltx_font_italic" id="algorithm1.6.10.2">ans = gold_ans</em>&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.10.3">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.6.11">
<span class="ltx_tag ltx_tag_listingline">5</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst = build_instance(’parametric_prompt’, q, gold_ans)
</div>
<div class="ltx_listingline" id="algorithm1.3.3">
<span class="ltx_tag ltx_tag_listingline">6</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.3.3.m1.1"><semantics id="algorithm1.3.3.m1.1a"><mrow id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml"><mi id="algorithm1.3.3.m1.1.1.2" xref="algorithm1.3.3.m1.1.1.2.cmml">D</mi><mo id="algorithm1.3.3.m1.1.1.1" xref="algorithm1.3.3.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.3.3.m1.1.1.3" xref="algorithm1.3.3.m1.1.1.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.2" xref="algorithm1.3.3.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.3.3.m1.1.1.3.3" xref="algorithm1.3.3.m1.1.1.3.3.cmml"><mi id="algorithm1.3.3.m1.1.1.3.3.2" xref="algorithm1.3.3.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.3" xref="algorithm1.3.3.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1a" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.4" xref="algorithm1.3.3.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1b" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.5" xref="algorithm1.3.3.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.3.3.m1.1.1.3.3.1c" xref="algorithm1.3.3.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.3.3.m1.1.1.3.3.6" xref="algorithm1.3.3.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1"><times id="algorithm1.3.3.m1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1.1"></times><ci id="algorithm1.3.3.m1.1.1.2.cmml" xref="algorithm1.3.3.m1.1.1.2">𝐷</ci><apply id="algorithm1.3.3.m1.1.1.3.cmml" xref="algorithm1.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.1.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3">subscript</csymbol><ci id="algorithm1.3.3.m1.1.1.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.3.3.m1.1.1.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3"><times id="algorithm1.3.3.m1.1.1.3.3.1.cmml" xref="algorithm1.3.3.m1.1.1.3.3.1"></times><ci id="algorithm1.3.3.m1.1.1.3.3.2.cmml" xref="algorithm1.3.3.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.3.3.m1.1.1.3.3.3.cmml" xref="algorithm1.3.3.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.3.3.m1.1.1.3.3.4.cmml" xref="algorithm1.3.3.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.3.3.m1.1.1.3.3.5.cmml" xref="algorithm1.3.3.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.3.3.m1.1.1.3.3.6.cmml" xref="algorithm1.3.3.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.3.3.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.add(inst)

</div>
<div class="ltx_listingline" id="algorithm1.6.12">
<span class="ltx_tag ltx_tag_listingline">7</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div class="ltx_listingline" id="algorithm1.6.13">
<span class="ltx_tag ltx_tag_listingline">8</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div class="ltx_listingline" id="algorithm1.6.14">
<span class="ltx_tag ltx_tag_listingline">9</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="algorithm1.6.14.1">else</span>
</div>
<div class="ltx_listingline" id="algorithm1.6.15">
<span class="ltx_tag ltx_tag_listingline">10</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst1 = build_instance(’parametric_prompt’, q, "&lt;RET&gt;")
</div>
<div class="ltx_listingline" id="algorithm1.4.4">
<span class="ltx_tag ltx_tag_listingline">11</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.4.4.m1.1"><semantics id="algorithm1.4.4.m1.1a"><mrow id="algorithm1.4.4.m1.1.1" xref="algorithm1.4.4.m1.1.1.cmml"><mi id="algorithm1.4.4.m1.1.1.2" xref="algorithm1.4.4.m1.1.1.2.cmml">D</mi><mo id="algorithm1.4.4.m1.1.1.1" xref="algorithm1.4.4.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.4.4.m1.1.1.3" xref="algorithm1.4.4.m1.1.1.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.2" xref="algorithm1.4.4.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.4.4.m1.1.1.3.3" xref="algorithm1.4.4.m1.1.1.3.3.cmml"><mi id="algorithm1.4.4.m1.1.1.3.3.2" xref="algorithm1.4.4.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.3" xref="algorithm1.4.4.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1a" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.4" xref="algorithm1.4.4.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1b" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.5" xref="algorithm1.4.4.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.4.4.m1.1.1.3.3.1c" xref="algorithm1.4.4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.4.4.m1.1.1.3.3.6" xref="algorithm1.4.4.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m1.1b"><apply id="algorithm1.4.4.m1.1.1.cmml" xref="algorithm1.4.4.m1.1.1"><times id="algorithm1.4.4.m1.1.1.1.cmml" xref="algorithm1.4.4.m1.1.1.1"></times><ci id="algorithm1.4.4.m1.1.1.2.cmml" xref="algorithm1.4.4.m1.1.1.2">𝐷</ci><apply id="algorithm1.4.4.m1.1.1.3.cmml" xref="algorithm1.4.4.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.4.4.m1.1.1.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3">subscript</csymbol><ci id="algorithm1.4.4.m1.1.1.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.4.4.m1.1.1.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3"><times id="algorithm1.4.4.m1.1.1.3.3.1.cmml" xref="algorithm1.4.4.m1.1.1.3.3.1"></times><ci id="algorithm1.4.4.m1.1.1.3.3.2.cmml" xref="algorithm1.4.4.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.4.4.m1.1.1.3.3.3.cmml" xref="algorithm1.4.4.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.4.4.m1.1.1.3.3.4.cmml" xref="algorithm1.4.4.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.4.4.m1.1.1.3.3.5.cmml" xref="algorithm1.4.4.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.4.4.m1.1.1.3.3.6.cmml" xref="algorithm1.4.4.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.4.4.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.add(inst1)
</div>
<div class="ltx_listingline" id="algorithm1.6.16">
<span class="ltx_tag ltx_tag_listingline">12</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
inst2 = build_instance(’context_prompt’, q, gold_ans, pass)
</div>
<div class="ltx_listingline" id="algorithm1.5.5">
<span class="ltx_tag ltx_tag_listingline">13</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
<math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.5.5.m1.1"><semantics id="algorithm1.5.5.m1.1a"><mrow id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml"><mi id="algorithm1.5.5.m1.1.1.2" xref="algorithm1.5.5.m1.1.1.2.cmml">D</mi><mo id="algorithm1.5.5.m1.1.1.1" xref="algorithm1.5.5.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.5.5.m1.1.1.3" xref="algorithm1.5.5.m1.1.1.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.2" xref="algorithm1.5.5.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.5.5.m1.1.1.3.3" xref="algorithm1.5.5.m1.1.1.3.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3.3.2" xref="algorithm1.5.5.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.3" xref="algorithm1.5.5.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1a" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.4" xref="algorithm1.5.5.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1b" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.5" xref="algorithm1.5.5.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.5.5.m1.1.1.3.3.1c" xref="algorithm1.5.5.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.5.5.m1.1.1.3.3.6" xref="algorithm1.5.5.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><apply id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1"><times id="algorithm1.5.5.m1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1.1"></times><ci id="algorithm1.5.5.m1.1.1.2.cmml" xref="algorithm1.5.5.m1.1.1.2">𝐷</ci><apply id="algorithm1.5.5.m1.1.1.3.cmml" xref="algorithm1.5.5.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3">subscript</csymbol><ci id="algorithm1.5.5.m1.1.1.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.5.5.m1.1.1.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3"><times id="algorithm1.5.5.m1.1.1.3.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3.3.1"></times><ci id="algorithm1.5.5.m1.1.1.3.3.2.cmml" xref="algorithm1.5.5.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.5.5.m1.1.1.3.3.3.cmml" xref="algorithm1.5.5.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.5.5.m1.1.1.3.3.4.cmml" xref="algorithm1.5.5.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.5.5.m1.1.1.3.3.5.cmml" xref="algorithm1.5.5.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.5.5.m1.1.1.3.3.6.cmml" xref="algorithm1.5.5.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.5.5.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.add(inst2)

</div>
<div class="ltx_listingline" id="algorithm1.6.17">
<span class="ltx_tag ltx_tag_listingline">14</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp; end if
</div>
<div class="ltx_listingline" id="algorithm1.6.18">
<span class="ltx_tag ltx_tag_listingline">15</span>&nbsp;&nbsp;<span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">&nbsp;</span>&nbsp;&nbsp;&nbsp;
</div>
<div class="ltx_listingline" id="algorithm1.6.19">
<span class="ltx_tag ltx_tag_listingline">16</span> end for
</div>
<div class="ltx_listingline" id="algorithm1.6.6">return <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="algorithm1.6.6.m1.1"><semantics id="algorithm1.6.6.m1.1a"><mrow id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml"><mi id="algorithm1.6.6.m1.1.1.2" xref="algorithm1.6.6.m1.1.1.2.cmml">D</mi><mo id="algorithm1.6.6.m1.1.1.1" xref="algorithm1.6.6.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.6.6.m1.1.1.3" xref="algorithm1.6.6.m1.1.1.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.2" xref="algorithm1.6.6.m1.1.1.3.2.cmml">S</mi><mrow id="algorithm1.6.6.m1.1.1.3.3" xref="algorithm1.6.6.m1.1.1.3.3.cmml"><mi id="algorithm1.6.6.m1.1.1.3.3.2" xref="algorithm1.6.6.m1.1.1.3.3.2.cmml">A</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.3" xref="algorithm1.6.6.m1.1.1.3.3.3.cmml">d</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1a" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.4" xref="algorithm1.6.6.m1.1.1.3.3.4.cmml">a</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1b" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.5" xref="algorithm1.6.6.m1.1.1.3.3.5.cmml">p</mi><mo id="algorithm1.6.6.m1.1.1.3.3.1c" xref="algorithm1.6.6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="algorithm1.6.6.m1.1.1.3.3.6" xref="algorithm1.6.6.m1.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><apply id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1"><times id="algorithm1.6.6.m1.1.1.1.cmml" xref="algorithm1.6.6.m1.1.1.1"></times><ci id="algorithm1.6.6.m1.1.1.2.cmml" xref="algorithm1.6.6.m1.1.1.2">𝐷</ci><apply id="algorithm1.6.6.m1.1.1.3.cmml" xref="algorithm1.6.6.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m1.1.1.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3">subscript</csymbol><ci id="algorithm1.6.6.m1.1.1.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.2">𝑆</ci><apply id="algorithm1.6.6.m1.1.1.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3"><times id="algorithm1.6.6.m1.1.1.3.3.1.cmml" xref="algorithm1.6.6.m1.1.1.3.3.1"></times><ci id="algorithm1.6.6.m1.1.1.3.3.2.cmml" xref="algorithm1.6.6.m1.1.1.3.3.2">𝐴</ci><ci id="algorithm1.6.6.m1.1.1.3.3.3.cmml" xref="algorithm1.6.6.m1.1.1.3.3.3">𝑑</ci><ci id="algorithm1.6.6.m1.1.1.3.3.4.cmml" xref="algorithm1.6.6.m1.1.1.3.3.4">𝑎</ci><ci id="algorithm1.6.6.m1.1.1.3.3.5.cmml" xref="algorithm1.6.6.m1.1.1.3.3.5">𝑝</ci><ci id="algorithm1.6.6.m1.1.1.3.3.6.cmml" xref="algorithm1.6.6.m1.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.6.6.m1.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.8.1.1">Algorithm 1</span></span>Training data creation</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p3">
<span class="ltx_ERROR undefined" id="S3.SS1.p3.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p3.2">Prompt: Answer the question Q. If you need help answer &lt;RET&gt; to get the context. Q: …</p>질문 Q에 답하세요. 컨텍스트를 얻기 위해 도움말 대답 <RET>가 필요한 경우. Q: …</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.3">이 프롬프트와 함께, 우리는 <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_Q</annotation></semantics></math>의 대응하는 질문과 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_A</annotation></semantics></math>의 황금 답변을 포함하여, 인스턴스(라인 5)를 집합적으로 형성하고, 이어서 <math alttext="DS_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">D</mi><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">S</mi><mrow id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.3.2" xref="S3.SS1.p4.3.m3.1.1.3.3.2.cmml">A</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.3.cmml">d</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1a" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.4" xref="S3.SS1.p4.3.m3.1.1.3.3.4.cmml">a</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1b" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.5" xref="S3.SS1.p4.3.m3.1.1.3.3.5.cmml">p</mi><mo id="S3.SS1.p4.3.m3.1.1.3.3.1c" xref="S3.SS1.p4.3.m3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.3.m3.1.1.3.3.6" xref="S3.SS1.p4.3.m3.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><times id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></times><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝐷</ci><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">𝑆</ci><apply id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3"><times id="S3.SS1.p4.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.1"></times><ci id="S3.SS1.p4.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.2">𝐴</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.3">𝑑</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.4.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.4">𝑎</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.5.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.5">𝑝</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.6.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">DS_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_D italic_S start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math> 데이터세트(라인 6)에 첨부된다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.2">대조적으로, LLM이 질문에 대한 올바른 응답을 생성하지 못하면(라인 8), 우리는 두 개의 다른 인스턴스를 구축한다. 첫 번째는 앞서 설명한 것과 동일한 <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.1">parametric_prompt</span>을 채용하며, <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mo id="S3.SS1.p5.1.m1.1.1" stretchy="false" xref="S3.SS1.p5.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mo id="S3.SS1.p5.2.m2.1.1" stretchy="false" xref="S3.SS1.p5.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">⟩</annotation></semantics></math>가 답변(line 9)으로 지정되어 추가 컨텍스트에 대한 필요성을 나타낸다. <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.2">context_prompt</span>이라고 하는 두 번째 프롬프트는 질문과 함께 컨텍스트 정보를 포함합니다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<span class="ltx_ERROR undefined" id="S3.SS1.p6.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p6.2">프롬프트: 문맥 C가 주어진 질문 Q에 답하라. Q: …, C: …,</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.3">이 경우 프롬프트, <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><mi id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">italic_Q</annotation></semantics></math>의 질문, <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p7.2.m2.1"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.2.m2.1d">italic_A</annotation></semantics></math>의 황금 답변, <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p7.3.m3.1"><semantics id="S3.SS1.p7.3.m3.1a"><mi id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><ci id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.3.m3.1d">italic_P</annotation></semantics></math>의 해당 컨텍스트 패시지를 포함한다(행 11).</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">LLM이 정확하게 응답할 수 없는 질문과 다른 모든 질문에 대한 황금 답변이 있는 <span class="ltx_text ltx_font_italic" id="S3.SS1.p8.1.1">parametric_prompt</span>에 대해 두 가지 유형의 프롬프트로 데이터 세트를 채운 후, 후속 미세 조정 단계를 위해 훈련 세트 <math alttext="D_{Adapt}" class="ltx_Math" display="inline" id="S3.SS1.p8.1.m1.1"><semantics id="S3.SS1.p8.1.m1.1a"><msub id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.1.1.2" xref="S3.SS1.p8.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS1.p8.1.m1.1.1.3" xref="S3.SS1.p8.1.m1.1.1.3.cmml"><mi id="S3.SS1.p8.1.m1.1.1.3.2" xref="S3.SS1.p8.1.m1.1.1.3.2.cmml">A</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.3" xref="S3.SS1.p8.1.m1.1.1.3.3.cmml">d</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1a" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.4" xref="S3.SS1.p8.1.m1.1.1.3.4.cmml">a</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1b" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.5" xref="S3.SS1.p8.1.m1.1.1.3.5.cmml">p</mi><mo id="S3.SS1.p8.1.m1.1.1.3.1c" xref="S3.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p8.1.m1.1.1.3.6" xref="S3.SS1.p8.1.m1.1.1.3.6.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><apply id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p8.1.m1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3"><times id="S3.SS1.p8.1.m1.1.1.3.1.cmml" xref="S3.SS1.p8.1.m1.1.1.3.1"></times><ci id="S3.SS1.p8.1.m1.1.1.3.2.cmml" xref="S3.SS1.p8.1.m1.1.1.3.2">𝐴</ci><ci id="S3.SS1.p8.1.m1.1.1.3.3.cmml" xref="S3.SS1.p8.1.m1.1.1.3.3">𝑑</ci><ci id="S3.SS1.p8.1.m1.1.1.3.4.cmml" xref="S3.SS1.p8.1.m1.1.1.3.4">𝑎</ci><ci id="S3.SS1.p8.1.m1.1.1.3.5.cmml" xref="S3.SS1.p8.1.m1.1.1.3.5">𝑝</ci><ci id="S3.SS1.p8.1.m1.1.1.3.6.cmml" xref="S3.SS1.p8.1.m1.1.1.3.6">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">D_{Adapt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_A italic_d italic_a italic_p italic_t end_POSTSUBSCRIPT</annotation></semantics></math>가 준비된다. 미세 조정 프로세스는 데이터 세트에서 기본 LLM을 훈련하는 것을 수반하여 <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p8.1.2">Adapt-LLM</span> 모델을 생성한다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">이 접근법은 모델이 질문에 답하기 위해 컨텍스트가 필요할 때 식별하거나, 컨텍스트가 제공될 때 직접 답변뿐만 아니라 충분할 때 직접 답변을 제공하도록 효과적으로 학습하도록 보장한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">추론 단계에서는 미세 조정된 모델을 사용하여 보이지 않는 질문에 대한 응답을 생성한다. 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>에 설명된 것과 같이 훈련 단계에서 사용된 동일한 프롬프트를 사용한다.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.4">처음에, 모델은 직접 응답을 제공하거나 대답이 확실하지 않은 경우 <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" stretchy="false" xref="S3.SS2.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mo id="S3.SS2.p2.2.m2.1.1" stretchy="false" xref="S3.SS2.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">⟩</annotation></semantics></math>를 반환하도록 프롬프트된다. 모델이 <math alttext="\langle" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" stretchy="false" xref="S3.SS2.p2.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mo id="S3.SS2.p2.4.m4.1.1" stretchy="false" xref="S3.SS2.p2.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">⟩</annotation></semantics></math>를 반환하는 경우, 우리는 off-the-shelf IR 시스템을 사용하여 관련 컨텍스트를 획득하기 위해 정보 검색을 진행한다. 그 후, 검색된 컨텍스트로 질문을 증강하고 훈련 단계에서 도입된 두 번째 유형의 프롬프트를 사용하여 모델을 다시 프롬프트한다.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Training Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Model configuration</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.2.1.1.1">NQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.2.1.2.1">Never Retrieve</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.3">21.43%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.3.2.1.1">Always Retrieve</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.2">35.86%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.4.3.1.1">Adapt-LLM</span> (ours)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.2.1">36.77%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.5.4.1.1">SQuAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.5.4.2"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.5.4.2.1">Never Retrieve</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.5.4.3">21.22%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.6.5.1.1">Always Retrieve</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5.2">36.59%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.6.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.1.7.6.1.1">Adapt-LLM</span> (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.2.1">38.15%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span> 상이한 검색 구성(NR-LLM, AR-LLM, 및 <span class="ltx_text ltx_font_smallcaps" id="S3.T1.3.1">Adapt-LLM</span>)을 사용하여 NQ 및 SQuAD 데이터 세트에 대해 트레이닝된 Llama-2 모델의 성능 비교 PopQA 테스트 세트에 대해 평가되었다. 모든 모델에 대해 정확한 일치 정확도가 보고됩니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">이 섹션에서는 제안된 적응형 검색 접근 방식인 <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">Adapt-LLM</span>의 성능을 평가하기 위한 실험 프레임워크를 설명한다. 사용된 데이터 세트(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS1" title="4.1 Datasets ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.1</span></a>)를 설명한 다음 기본 모델의 개요(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS2" title="4.2 Base Model ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.2</span></a>), 기본 모델의 다른 구성(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS3" title="4.3 Model Configurations ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.3</span></a>) 및 훈련 세부 정보(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS4" title="4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.4</span></a>)를 설명하는 것으로 시작합니다. 이어서, 우리는 세 가지 주요 실험을 소개한다:</p>
</div>
<div class="ltx_para" id="S4.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I1.i1.p1.1.1">Adapt-LLM</span> 성능 비교: (i) 모든 질문에 대한 컨텍스트 정보를 검색하는 LLM, (ii) 임의의 질문에 대해 IR 시스템을 사용하지 않고 자신의 파라메트릭 메모리에 독점적으로 의존하는 LLM(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS5" title="4.5 Validating the Adaptive Retrieval Approach ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I1.i2.p1.1.1">Adapt-LLM</span>’s ability to determine when extra context is necessary to answer a question (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS6" title="4.6 Contextual Retrieval Decision Analysis ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.6</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">PopQA에 대한 최신 접근법과의 비교(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.SS7" title="4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4.7</span></a>).</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">NQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">SQuAD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">PopQA</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.1">Questions</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.2">58,880</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.3">87,599</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.4">14,282</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.1">Words/question</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.2">9.20</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.3">10.06</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.4">6.62</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.1">Words/answer</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.2">2.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.3">3.16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.4.4">2.04</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>우리가 실험에 사용하는 세 데이터 세트의 비교, 즉 SQuAD, NQ 및 PopQA. 각각에 대해 질문 수와 질문 및 답변당 평균 단어 수를 제공합니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.6.6.7"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.7.1">Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.2">
<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.1">RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo id="S4.T3.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">⟩</annotation></semantics></math> Usage</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.4.4.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.1">RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.1.m1.1a"><mo id="S4.T3.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T3.4.4.4.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">⟩</annotation></semantics></math></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.2">No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.1.m1.1a"><mo id="S4.T3.5.5.5.1.m1.1.1" stretchy="false" xref="S4.T3.5.5.5.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><ci id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.6.6.6.2.m2.1"><semantics id="S4.T3.6.6.6.2.m2.1a"><mo id="S4.T3.6.6.6.2.m2.1.1" stretchy="false" xref="S4.T3.6.6.6.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.2.m2.1b"><ci id="S4.T3.6.6.6.2.m2.1.1.cmml" xref="S4.T3.6.6.6.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.2.m2.1d">⟩</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.6.7.1">
<td class="ltx_td" id="S4.T3.6.7.1.1"></td>
<td class="ltx_td" id="S4.T3.6.7.1.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.3.1">Acc. w/ context</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.4.1">Acc. w/o context</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.5.1">Acc. w/ context</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.7.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.6.7.1.6.1">Acc. w/o context</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.8.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.1">NQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.2">82.26%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.3">33.04%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.4">14.65%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.5">55.72%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.6.8.2.6">62.36%</th>
</tr>
<tr class="ltx_tr" id="S4.T3.6.9.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.1">SQuAD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.2">83.93%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.3">33.40%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.4">9.94%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.5">57.73%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.9.3.6">62.92%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Results of the usage of the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.13.m1.1"><semantics id="S4.T3.13.m1.1b"><mo id="S4.T3.13.m1.1.1" stretchy="false" xref="S4.T3.13.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.m1.1c"><ci id="S4.T3.13.m1.1.1.cmml" xref="S4.T3.13.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.m1.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.m1.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.14.m2.1"><semantics id="S4.T3.14.m2.1b"><mo id="S4.T3.14.m2.1.1" stretchy="false" xref="S4.T3.14.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.m2.1c"><ci id="S4.T3.14.m2.1.1.cmml" xref="S4.T3.14.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.m2.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.m2.1e">⟩</annotation></semantics></math> token in the <span class="ltx_text ltx_font_smallcaps" id="S4.T3.22.1">Adapt-LLM</span> model. 첫 번째 열에는 모델이 추가 컨텍스트를 요청하는 PopQA 질문의 백분율이 표시됩니다. 두 번째 열은 <span class="ltx_text ltx_font_smallcaps" id="S4.T3.23.2">Adapt-LLM</span>이 컨텍스트를 묻는 질문(<math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.15.m3.1"><semantics id="S4.T3.15.m3.1b"><mo id="S4.T3.15.m3.1.1" stretchy="false" xref="S4.T3.15.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.m3.1c"><ci id="S4.T3.15.m3.1.1.cmml" xref="S4.T3.15.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.m3.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.15.m3.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.16.m4.1"><semantics id="S4.T3.16.m4.1b"><mo id="S4.T3.16.m4.1.1" stretchy="false" xref="S4.T3.16.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.m4.1c"><ci id="S4.T3.16.m4.1.1.cmml" xref="S4.T3.16.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.m4.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.16.m4.1e">⟩</annotation></semantics></math>)에 초점을 맞추어 컨텍스트가 있는 질문과 없는 질문 간의 성능을 비교합니다. 마지막 열(No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.T3.17.m5.1"><semantics id="S4.T3.17.m5.1b"><mo id="S4.T3.17.m5.1.1" stretchy="false" xref="S4.T3.17.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.m5.1c"><ci id="S4.T3.17.m5.1.1.cmml" xref="S4.T3.17.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.m5.1d">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.17.m5.1e">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.T3.18.m6.1"><semantics id="S4.T3.18.m6.1b"><mo id="S4.T3.18.m6.1.1" stretchy="false" xref="S4.T3.18.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.m6.1c"><ci id="S4.T3.18.m6.1.1.cmml" xref="S4.T3.18.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.m6.1d">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.T3.18.m6.1e">⟩</annotation></semantics></math>)은 <span class="ltx_text ltx_font_smallcaps" id="S4.T3.24.3">Adapt-LLM</span>이 직접 답하기로 결정한 질문에 대한 것이다. 또한, IR 시스템에 의해 검색된 컨텍스트의 유무에 따른 성능을 비교한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">모델의 포괄적인 훈련과 평가를 보장하기 위해 세 가지 다양한 질문 응답 데이터 세트를 구체적으로 선택했다. 학습은 사실 지식을 평가하고 위키피디아를 기반으로 하는 널리 알려져 있는 데이터 세트이기 때문에 NQ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib18" title="">18</a>]</cite>와 SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib25" title="">25</a>]</cite>를 선택했다. 평가를 위해 PopQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>를 선택했다. 다음은 각 데이터 세트에 대한 간략한 설명입니다.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">NQ</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">자연 질문 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib18" title="">18</a>]</cite>는 구글 검색 질의에서 파생된 실제 질문의 집합으로, 위키피디아 기사로부터 얻은 긴 형식의 텍스트 지문을 동반하고 다양한 범위의 주제 및 자연 언어 변형을 제공한다. 실험에서는 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">training</span> 모델에 이 데이터 세트를 활용한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">SQuAD</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">스탠포드 질의 응답 데이터 세트 SQuAD <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib25" title="">25</a>]</cite>는 자연어 처리 분야에서 널리 사용되는 데이터 세트이며 컨텍스트 역할을 하는 관련 단락 구절과 함께 다양한 범위의 위키피디아 기사에 크라우드 워커가 제기하는 질문으로 구성된다. 실험에서는 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.1">training</span> 모델에 이 데이터 세트를 활용한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">PopQA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">Popular Questions and Answers 데이터셋 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>는 광범위한 도메인과 스타일을 포괄하는 다양한 온라인 플랫폼에서 제공되는 선별된 질문으로 구성된다. 이 데이터 세트에서 관찰된 컨텍스트 검색 전략의 효과의 가변성을 감안할 때, 정확한 답변 제공을 위해 컨텍스트가 필요한 시기를 결정하는 언어 모델의 성능인 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px3.p1.1.1">evaluate</span>에 대한 테스트 세트로 PopQA를 선택한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Base Model</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">실험에서는 Llama-2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib34" title="">34</a>]</cite>를 기본 LLM으로 사용한다. Lama-2는 7B, 13B 및 70B 매개변수의 버전으로 제공되는 오픈 소스 명령어 기반 LLM이다. 이 모델은 공개적으로 사용 가능한 온라인 데이터 소스에서 제공되는 확장된 코퍼스에서 사전 훈련된다. 이 코퍼스는 이전 코퍼스에 비해 크기가 40% 증가하여 모델의 향상된 성능과 기능에 기여합니다.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">또한, Llama-2는 확장된 컨텍스트 길이를 특징으로 하여 더 긴 텍스트 시퀀스를 처리하고 이해하는 능력을 효과적으로 배가한다. 이러한 향상은 다양한 자연어 이해 과제에 걸쳐 모델의 효과를 크게 향상시킨다. 특히, 실험을 위해 7B 매개변수가 있는 라마-2 모델을 활용하여 특정 연구 목표에 대한 강력한 기능을 활용한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Configurations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">LLM과 IR 시스템을 결합할 수 있는 세 가지 다른 방식에 해당하는 세 가지 다른 모델 구성을 사용하여 실험을 수행한다.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Adaptive Retrieval (<span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.1.1">Adapt-LLM</span>)</span>. <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.2">Adapt-LLM</span> 모델은 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>에서 설명한 바와 같이 컨텍스트 정보에 대한 질문 및 인식된 필요성에 기초하여 컨텍스트 검색 여부를 동적으로 결정한다. IR 시스템은 대규모 말뭉치에서 사전 훈련된 비지도 모델인 Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>를 사용하고 MS MARCO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib24" title="">24</a>]</cite>에서 미세 조정한다. 우리는 최종 답변을 위해 기본 LLM을 프롬프트하기 위해 IR 시스템에 따라 가장 관련성이 높은 통로만 검색한다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Never-Retrieve (NR-LLM)</span>. 이 모델 구성은 컨텍스트 정보를 고려하지 않고 질문 텍스트만을 기반으로 질문에 답하도록 훈련된다. 문맥이 없는 상황에서 질의 응답 모델의 성능을 평가하는 기준선 역할을 한다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Always-Retrieve (AR-LLM)</span>. NR-LLM 모델과 대조적으로, 이 구성은 항상 질문에 답하는 것을 돕기 위해 컨텍스트 구절을 검색한다. 이는 답변을 생성하기 위해 컨텍스트를 일관되게 활용하도록 훈련된다. <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i3.p1.1.2">Adapt-LLM</span>과 공정한 비교를 보장하기 위해 IR 시스템으로 Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>를 사용하고 컨텍스트로 가장 관련성이 높은 통로만 검색합니다.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Training Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">세 가지 모델 구성(<span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.1">Adapt-LLM</span>, AR-LLM 및 NR-LLM) 및 두 훈련 세트(SQuAD 및 NQ) 모두에 대해 128의 배치 크기, 세 개의 epoch 및 3e-4의 고정 학습 속도를 포함하는 Alpaca-Lora <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib32" title="">32</a>]</cite>에서 설정된 매개변수 구성을 준수한다. 우리는 r=8, 알파=16 및 드롭아웃 속도 0.05에 대해 구성된 매개변수와 함께 LoRA(Low-Rank Adaptation) 정규화를 통합했다. 훈련은 NVIDIA A40 GPU에서 약 8시간의 평균 훈련 시간 동안 수행되었다. 우리는 어떤 모델 선택도 수행하지 않고 3번의 훈련 후 마지막 체크포인트를 사용한다.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F2.1" style="width:195.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S4.F2.1.g1" src="https://arxiv.org/html/2404.19705v2/extracted/5573676/img/istogramma_pop_score_use_ret_nq.png" width="598">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F2.2" style="width:195.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S4.F2.2.g1" src="https://arxiv.org/html/2404.19705v2/extracted/5573676/img/istogramma_pop_score_use_ret_squad.png" width="598">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span>Histograms in the proportion of questions where <span class="ltx_text ltx_font_smallcaps" id="S4.F2.5.1">Adapt-LLM</span> trained on NQ (left) and <span class="ltx_text ltx_font_smallcaps" id="S4.F2.6.2">Adapt-LLM</span> trained on SQuAD (right) ask for extra context for different popularity score intervals.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Validating the Adaptive Retrieval Approach</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">적응적 접근법(<span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p1.1.1">Adapt-LLM</span>)의 효율성을 평가하기 위해 NR-LLM 및 AR-LLM 구성과 비교하여 세 가지 구성 모두에 걸쳐 NQ 및 SQuAD 데이터 세트 모두에서 Llama-2 모델의 미세 조정을 수행했다. NR-LLM 및 AR-LLM 구성의 경우 데이터 세트에서 질문-응답 쌍을 추출하고 해당 명령 프롬프트를 통합하여 훈련 샘플을 구성했다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.2">구체적으로, NR-LLM 구성에 대한 프롬프트는 추가 컨텍스트 없이 모델에 질문에 답하도록 지시한 반면, AR-LLM 구성에 대한 프롬프트는 질문 및 컨텍스트 정보를 모두 포함했다. 대조적으로, <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p2.2.1">Adapt-LLM</span> 훈련 세트는 2단계 프로세스를 사용하여 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS1" title="3.1 Training Adapt-LLM ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.1</span></a>에 설명된 접근법에 따라 구성되었다. 이 과정의 결과, NQ의 74.72%의 질문이 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mo id="S4.SS5.p2.1.m1.1.1" stretchy="false" xref="S4.SS5.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mo id="S4.SS5.p2.2.m2.1.1" stretchy="false" xref="S4.SS5.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.2.m2.1d">⟩</annotation></semantics></math> 토큰으로 표시된 반면, SQuAD의 경우 87.49%의 질문이 표시된다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">그런 다음 훈련된 모델을 PopQA 데이터 세트에서 테스트하여 실제 질문 응답 시나리오에서 성능을 평가했다. 추론하는 동안 NR-LLM 및 AR-LLM 모델이 그대로 활용되었으며 해당 지시 프롬프트가 제공되었으며 출력은 질문에 대한 답변으로 예상된다. 반대로 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p3.1.1">Adapt-LLM</span> 모델의 경우 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.SS2" title="3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3.2</span></a>에서 설명한 것과 동일한 프롬프트 절차를 따랐다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">그런 다음 생성된 답변은 PopQA 테스트 세트에서 이미 주석이 달린 각 질문에 대한 가능한 답변 세트와 비교된다. 사용된 평가 메트릭은 해당 질문에 대한 가능한 답변 중 하나와 정확히 일치하는 생성된 출력의 백분율을 측정하는 <span class="ltx_text ltx_font_bold" id="S4.SS5.p4.1.1">Exact Match Accuracy</span>입니다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.T1" title="Table 1 ‣ 3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>는 다른 구성 및 데이터 세트에 걸친 Llama-2 모델의 성능을 설명하는 이 실험의 결과를 나타낸다. NQ 및 SQuAD 훈련 데이터 세트 모두에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p5.1.1">Adapt-LLM</span> 구성은 PopQA 테스트 세트에서 NR-LLM(Never Retrieve) 및 AR-LLM(Always Retrieve) 구성을 일관되게 능가합니다. 관찰할 수 있는 바와 같이 NR-LLM은 다른 구성에 비해 약 14개의 절대점의 정확도 차이로 모델 중 가장 낮은 성능을 나타낸다. 이러한 차이는 라마-2의 매개변수 기억만으로는 PopQA 질문에 효과적으로 답하기 충분하지 않음을 시사한다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p6">
<p class="ltx_p" id="S4.SS5.p6.1">AR-LLM과 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.1">Adapt-LLM</span>의 차이는 더 좁습니다. 구체적으로, <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.2">Adapt-LLM</span> 구성은 AR-LLM 구성의 35.86% 및 36.59%에 비해 NQ 및 SQuAD 데이터 세트에 대해 훈련될 때 PopQA 테스트 세트에 대해 각각 36.77% 및 38.15%의 정확도를 달성한다. 두 훈련 데이터 세트 모두에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p6.1.3">Adapt-LLM</span>은 AR-LLM보다 우수하며 SQuAD에서 훈련할 때 가장 큰 차이가 관찰됩니다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p7">
<p class="ltx_p" id="S4.SS5.p7.1">이러한 결과는 정확한 질의응답을 위해 컨텍스트의 필요성을 동적으로 결정하는 적응적 검색 방법의 효율성을 강조하며, 이는 항상 또는 결코 컨텍스트를 검색하지 않는 고정된 전략에 비해 향상된 성능을 가져온다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p8">
<p class="ltx_p" id="S4.SS5.p8.1">훈련 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p8.1.1">Adapt-LLM</span> on NQ 또는 SQuAD는 상대적으로 미미하지만 주어진 평가 세트에 대한 훈련 세트의 적합성을 결정하려고 한다. 훈련 세트(NQ 및 SQuAD) 및 평가 세트(PopQA)가 모두 위키피디아를 기반으로 하는 반면, 미묘한 차이가 존재할 수 있다.</p>
</div>
<div class="ltx_para" id="S4.SS5.p9">
<p class="ltx_p" id="S4.SS5.p9.2">표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>는 총 질문 수와 질문 및 답변당 평균 단어 수를 포함하여 실험 절차에 포함된 세 데이터 세트의 특성에 대한 통찰력을 제공한다. NQ가 질문 및 답변 길이 측면에서 PopQA에 더 가까운 것으로 보이지만, SQuAD 상의 트레이닝 <span class="ltx_text ltx_font_smallcaps" id="S4.SS5.p9.2.1">Adapt-LLM</span>의 더 나은 결과에 영향을 미치는 핵심 요소는 트레이닝 데이터세트의 질문 수일 수 있다(<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p9.1.m1.1"><semantics id="S4.SS5.p9.1.m1.1a"><mo id="S4.SS5.p9.1.m1.1.1" xref="S4.SS5.p9.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p9.1.m1.1.1.cmml" xref="S4.SS5.p9.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p9.1.m1.1d">∼</annotation></semantics></math>87K in SQuAD and <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p9.2.m2.1"><semantics id="S4.SS5.p9.2.m2.1a"><mo id="S4.SS5.p9.2.m2.1.1" xref="S4.SS5.p9.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p9.2.m2.1b"><csymbol cd="latexml" id="S4.SS5.p9.2.m2.1.1.cmml" xref="S4.SS5.p9.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p9.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p9.2.m2.1d">∼</annotation></semantics></math>58K in NQ). 훈련 데이터 세트를 주어진 대상 데이터 세트에 더 적합하게 만드는 요인(우리 연구의 범위를 벗어남)을 설명하기 위해서는 추가 분석이 필요하지만 이러한 결과는 규모가 다시 한 번 중요한 역할을 할 수 있음을 시사한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Contextual Retrieval Decision Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">이 실험에서 우리의 목표는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p1.1.1">Adapt-LLM</span> 모델의 효과를 다시 한 번 평가하는 것이며, 이번에는 추가 컨텍스트가 필요한 시기를 정확하게 결정하는 능력에 초점을 맞춘다. 이를 위해 다음 단계를 준수합니다.</p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.2">우리는 PopQA 테스트 세트를 사용하여 <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i1.p1.2.1">Adapt-LLM</span> 모델에 대해 추론을 수행하여 답변을 직접 반환하거나 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.I3.i1.p1.1.m1.1"><semantics id="S4.I3.i1.p1.1.m1.1a"><mo id="S4.I3.i1.p1.1.m1.1.1" stretchy="false" xref="S4.I3.i1.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><ci id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.I3.i1.p1.2.m2.1"><semantics id="S4.I3.i1.p1.2.m2.1a"><mo id="S4.I3.i1.p1.2.m2.1.1" stretchy="false" xref="S4.I3.i1.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><ci id="S4.I3.i1.p1.2.m2.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.2.m2.1d">⟩</annotation></semantics></math>를 반환하여 추가 컨텍스트의 필요성을 나타낸다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.2"><span class="ltx_text ltx_font_smallcaps" id="S4.I3.i2.p1.2.1">Adapt-LLM</span> 모델로부터 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.I3.i2.p1.1.m1.1"><semantics id="S4.I3.i2.p1.1.m1.1a"><mo id="S4.I3.i2.p1.1.m1.1.1" stretchy="false" xref="S4.I3.i2.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><ci id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.I3.i2.p1.2.m2.1"><semantics id="S4.I3.i2.p1.2.m2.1a"><mo id="S4.I3.i2.p1.2.m2.1.1" stretchy="false" xref="S4.I3.i2.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><ci id="S4.I3.i2.p1.2.m2.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.2.m2.1d">⟩</annotation></semantics></math> 응답을 수신하는 경우, 다음 단계를 진행한다:</p>
<ol class="ltx_enumerate" id="S4.I3.i2.I1">
<li class="ltx_item" id="S4.I3.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1.</span>
<div class="ltx_para" id="S4.I3.i2.I1.i1.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I3.i2.I1.i1.p1.1.1">Adapt-LLM</span> 모델에 대해 추론을 수행하여 IR 시스템에서 얻은 컨텍스트가 주어진 답변을 반환하도록 프롬프트한다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2.</span>
<div class="ltx_para" id="S4.I3.i2.I1.i2.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i2.p1.1">또한 추가 컨텍스트 없이 직접 답변을 제공하는 명령어를 사용하여 NR-LLM 모델에 대한 추론을 수행한다.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1">If <span class="ltx_text ltx_font_smallcaps" id="S4.I3.i3.p1.1.1">Adapt-LLM</span> model decides to answer the question directly relying on its parametric memory:</p>
<ol class="ltx_enumerate" id="S4.I3.i3.I1">
<li class="ltx_item" id="S4.I3.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.1.</span>
<div class="ltx_para" id="S4.I3.i3.I1.i1.p1">
<p class="ltx_p" id="S4.I3.i3.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I3.i3.I1.i1.p1.1.1">Adapt-LLM</span> 모델에 대해 추론을 수행하여 컨텍스트를 제공하지 않고 답변을 반환하도록 프롬프트한다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.2.</span>
<div class="ltx_para" id="S4.I3.i3.I1.i2.p1">
<p class="ltx_p" id="S4.I3.i3.I1.i2.p1.1">우리는 AR-LLM 모델에 대해 IR 시스템에서 검색된 컨텍스트를 사용하여 답변을 제공하는 지침을 사용하여 추론을 수행한다.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.2">표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>는 본 실험의 결과를 제시한다. 첫 번째로 주목할 점은 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p2.2.1">Adapt-LLM</span> 모델은 PopQA 데이터 세트에서 질문의 약 82-83%에 대해 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p2.1.m1.1"><semantics id="S4.SS6.p2.1.m1.1a"><mo id="S4.SS6.p2.1.m1.1.1" stretchy="false" xref="S4.SS6.p2.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><ci id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p2.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p2.2.m2.1"><semantics id="S4.SS6.p2.2.m2.1a"><mo id="S4.SS6.p2.2.m2.1.1" stretchy="false" xref="S4.SS6.p2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.1b"><ci id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p2.2.m2.1d">⟩</annotation></semantics></math> 토큰을 생성하며, 두 훈련 데이터 세트 모두에서 유사한 비율이 관찰된다는 것이다. 이 관찰은 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S3.T1" title="Table 1 ‣ 3.2 Inference ‣ 3 Adaptive Retrieval LLM (Adapt-LLM) ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">1</span></a>에서 입증된 NR-LLM 구성의 낮은 성능과 일치한다.</p>
</div>
<div class="ltx_para" id="S4.SS6.p3">
<p class="ltx_p" id="S4.SS6.p3.2">그러나 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.1">Adapt-LLM</span>은 질문에 정확하게 대답하기 위해 추가 컨텍스트가 필요한 시기를 일관되게 결정합니다. NQ 및 SQuAD 트레이닝 데이터 세트 모두에 걸쳐, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.2">Adapt-LLM</span>은 컨텍스트가 없는 NR-LLM 모델의 정확도에 비해 컨텍스트를 검색할 때 훨씬 더 높은 정확도를 나타낸다(표 <a idx=0></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p3.2.m2.1"><semantics id="S4.SS6.p3.2.m2.1a"><mo id="S4.SS6.p3.2.m2.1.1" stretchy="false" xref="S4.SS6.p3.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p3.2.m2.1b"><ci id="S4.SS6.p3.2.m2.1.1.cmml" xref="S4.SS6.p3.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p3.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p3.2.m2.1d">⟩</annotation></semantics></math> 열의 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>에 표시된 바와 같이). 구체적으로, NQ 데이터 세트의 경우, 컨텍스트 요청 시 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.3">Adapt-LLM</span> 모델의 정확도는 33.04%인 반면, 컨텍스트 검색이 없는 NR-LLM 모델의 정확도는 14.65%로 현저하게 낮다. 유사하게, SQuAD 데이터 세트의 경우, <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p3.2.4">Adapt-LLM</span>은 컨텍스트 검색에서 33.40%의 정확도를 달성하는 반면, 컨텍스트가 없는 NR-LLM 모델의 정확도는 9.94%로 실질적으로 더 낮다.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Passages</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">SQuAD Dev</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">NQ Dev</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.1.1">Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.2.1">Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.1">Gold</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.1.2.1">89.42%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.1.3.1">69.76%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.2.1">Contriever</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.2.2">22.49</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.2.3">27.04%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span> <span class="ltx_text ltx_font_smallcaps" id="S4.T4.3.1">Adapt-LLM</span> for the SQuAD and NQ dev sets, when use the gold passages provided by the datasets and use the best passage retrieved by Contriever.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS6.p4">
<p class="ltx_p" id="S4.SS6.p4.2">마지막으로, 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>의 마지막 열(No <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p4.1.m1.1"><semantics id="S4.SS6.p4.1.m1.1a"><mo id="S4.SS6.p4.1.m1.1.1" stretchy="false" xref="S4.SS6.p4.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.1.m1.1b"><ci id="S4.SS6.p4.1.m1.1.1.cmml" xref="S4.SS6.p4.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p4.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p4.2.m2.1"><semantics id="S4.SS6.p4.2.m2.1a"><mo id="S4.SS6.p4.2.m2.1.1" stretchy="false" xref="S4.SS6.p4.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p4.2.m2.1b"><ci id="S4.SS6.p4.2.m2.1.1.cmml" xref="S4.SS6.p4.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p4.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p4.2.m2.1d">⟩</annotation></semantics></math>)은 그 파라메트릭 메모리만을 기반으로 질문에 답할 때 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p4.2.1">Adapt-LLM</span>의 성능을 보여준다. 알 수 있듯이 컨텍스트를 활용하지 않을 때 62% 이상의 정확도가 얻어져 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p4.2.2">Adapt-LLM</span>이 컨텍스트 검색과 질문에 대한 직접 답변 제공 사이를 효과적으로 구분한다는 추가 증거를 제공한다. 또한 문맥이 입력에 추가될 때 이러한 질문의 성능을 평가하여 최대 7개의 절대점의 정확도가 크게 감소함을 보여준다.</p>
</div>
<div class="ltx_para" id="S4.SS6.p5">
<p class="ltx_p" id="S4.SS6.p5.1">이러한 연구 결과는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p5.1.1">Adapt-LLM</span> 모델이 정확한 응답 생성을 위한 추가 컨텍스트의 필요성을 결정하는 데 사용되는 의사 결정 프로세스의 효과에 대한 통찰력을 제공하고 질문 응답 모델의 정확도를 향상시키는 데 동적 컨텍스트 검색 수행의 필요성에 대한 경험적 증거를 제시한다.</p>
</div>
<div class="ltx_para" id="S4.SS6.p6">
<p class="ltx_p" id="S4.SS6.p6.1">그러나 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">3</span></a>(약 33%)에서 관찰된 바와 같이 검색된 컨텍스트로 질문에 응답할 때 모델의 전체 성능이 상대적으로 낮다는 것이 주목할 만하다. 이 관찰을 더 탐구하기 위해 우리는 추가 실험을 수행한다: NQ 및 SQuAD 개발 분할에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p6.1.1">Adapt-LLM</span> (NQ 및 SQuAD에서 훈련된 버전 모두) 데이터 세트의 골드 패시지와 IR 시스템인 Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib11" title="">11</a>]</cite>에서 검색된 컨텍스트를 사용할 때의 성능을 비교한다. 불행히도 PopQA는 금 구절을 제공하지 않기 때문에 직접적인 평가는 불가능했다.</p>
</div>
<div class="ltx_para" id="S4.SS6.p7">
<p class="ltx_p" id="S4.SS6.p7.1">표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T4" title="Table 4 ‣ 4.6 Contextual Retrieval Decision Analysis ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">4</span></a>는 본 실험의 결과를 제시한다. 두 데이터 세트(SQuAD의 경우 약 67개의 절대점 및 NQ의 경우 42개)에 대해 Contriever에서 검색한 골드 통로와 상단 통로를 사용하는 것 사이에 상당한 성능 차이가 관찰된다. 이것은 Contriever 및 현재 IR 시스템이 일반적으로 주어진 질문에 답하기 위해 가장 적절한 구절을 일관되게 검색하지 않는다는 것을 나타낸다. 이 관찰은 가장 성공적인 오픈 도메인 QA 시스템 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib13" title="">13</a>]</cite>에서 볼 수 있듯이 컨텍스트로 여러 문서를 검색하는 것의 중요성을 강조하고 PopQA에서 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p7.1.1">Adapt-LLM</span>의 전체 성능에 미치는 영향을 강조한다.</p>
</div>
<div class="ltx_para" id="S4.SS6.p8">
<p class="ltx_p" id="S4.SS6.p8.2">추가 컨텍스트 요청 시 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.1">Adapt-LLM</span>의 동작을 추가로 검증하기 위해, 그림 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.F2" title="Figure 2 ‣ 4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>는 우리 모델이 <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS6.p8.1.m1.1"><semantics id="S4.SS6.p8.1.m1.1a"><mo id="S4.SS6.p8.1.m1.1.1" stretchy="false" xref="S4.SS6.p8.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.1.m1.1b"><ci id="S4.SS6.p8.1.m1.1.1.cmml" xref="S4.SS6.p8.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p8.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS6.p8.2.m2.1"><semantics id="S4.SS6.p8.2.m2.1a"><mo id="S4.SS6.p8.2.m2.1.1" stretchy="false" xref="S4.SS6.p8.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p8.2.m2.1b"><ci id="S4.SS6.p8.2.m2.1.1.cmml" xref="S4.SS6.p8.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p8.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p8.2.m2.1d">⟩</annotation></semantics></math> 토큰을 생성하는 질문의 비율을 보여줍니다. 인기 점수 간격으로 집계됩니다(<span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.2">Adapt-LLM</span> trained on NQ and right image for SQuAD). <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>는 인기도가 높은 질문은 LLM의 매개변수 기억을 사용하여 적절하게 대답할 수 있는 반면 인기 점수가 낮으면 추가 컨텍스트가 필요함을 시사한다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.F2" title="Figure 2 ‣ 4.4 Training Details ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS6.p8.2.3">Adapt-LLM</span>의 두 버전 모두에 대해 이 패턴을 관찰하며, 이는 훈련 또는 추론 중 인기 점수에 대한 액세스가 부족함에도 불구하고 우리 모델이 추가 컨텍스트를 요청하기 위한 효과적인 기준을 학습했음을 나타낸다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Comparison with state-of-the-art methods</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">우리는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p1.1.1">Adapt-LLM</span> 모델과 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>에 의해 제안된 PopQA에 대한 현재 최첨단 접근법 간의 비교 분석을 수행했다. 그들의 방법론은 질문이 추가 컨텍스트를 필요로 하는지 여부를 결정하기 위해 PopQA 데이터 세트에 주석이 달린 인기 점수에 의존한다. 문제 인기도를 결정하는 최적의 임계값을 설정하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite> PopQA 데이터 세트를 임계값 결정을 위한 개발 세트로 75%, 테스트 세트로 25%로 분할했다. 원본 논문에서, 그들은 이 방법론을 그 순간에 이용 가능한 다양한 LLM에 적용한다(Llama-2는 아직 출시되지 않았다).</p>
</div>
<div class="ltx_para" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.1">Adapt-LLM</span>과 인기 기반 방법 간의 공정한 비교를 보장하기 위해 동일한 PopQA 개발 세트를 사용하여 최고의 인기 점수 임계값(707,000으로 발견됨)을 결정하기 위해 Llama-2 7B 모델을 사용하여 접근법을 복제했다. 이를 통해 기본 LLM을 활용하면서 그들의 방법론과 일치하는 결과를 얻을 수 있었다. 더 작은 모델을 사용할 때 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>의 원래 결과와 유사하게, 인기 점수 임계값은 항상 Llama-2 7B에 대한 컨텍스트 정보를 검색하는 것과 거의 동등하다. IR 사용량은 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T5" title="Table 5 ‣ 4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">5</span></a>에 제시된 바와 같이 99.86%이다. 이는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.2">GPT-3 davinci-003</span>이 Contriever를 사용하여 적응 검색을 사용할 때 원본 논문에서 IR 사용량을 80% 미만으로 얻는 유일한 모델인 더 작은 크기의 모델에 대해 인기 점수 방법이 어떻게 어려움을 겪고 있는지 명확하게 보여준다. 이후 동일한 25% 테스트 세트 분할에 대해 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.3">Adapt-LLM</span> 구성을 평가하고 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>에서 설명한 방법을 사용하여 얻은 결과와 비교했다. 이 체계적인 비교를 통해 현재 기술과 관련하여 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p2.1.4">Adapt-LLM</span> 모델의 효능을 평가할 수 있었다.</p>
</div>
<div class="ltx_para" id="S4.SS7.p3">
<p class="ltx_p" id="S4.SS7.p3.1">이 실험의 결과는 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#S4.T5" title="Table 5 ‣ 4.7 Comparison with state-of-the-art methods ‣ 4 Experiments and Results ‣ When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively"><span class="ltx_text ltx_ref_tag">5</span></a>에 제시되어 있다. 우리는 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>의 복제 접근법과 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.1">Adapt-LLM</span>이 NQ 및 SQuAD 데이터 세트에 대해 훈련되고 PopQA의 25% 하위 집합에서 테스트될 때 유사한 성능을 관찰한다. <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.2">Adapt-LLM</span>은 인기 점수 및 PopQA 데이터 세트의 75% 부분을 직접 사용하여 해당 인기 점수에 대한 최적의 값을 찾는 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>와 달리 PopQA의 정보를 사용하지 않는다는 점을 언급할 가치가 있습니다. 이 방법론은 인기 점수가 PopQA의 고유한 기능이기 때문에 다른 개방형 질문 응답 작업에 일반화할 수 없다. 그러나 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.3">Adapt-LLM</span>은 유사한 데이터 세트에 적용할 수 있습니다. 이러한 특성을 감안할 때 <span class="ltx_text ltx_font_smallcaps" id="S4.SS7.p3.1.4">Adapt-LLM</span>에 의해 얻은 결과는 데이터 세트 특정 정보를 활용하는 접근법과 비교할 수 있는 성능을 제공하는 훨씬 더 중요하다고 믿는다. 이러한 발견은 테스트에 사용된 데이터 세트와 다른 데이터 세트로 훈련된 경우에도 우리의 접근법의 유효성을 입증한다.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Model Configuration</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">IR usage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.3.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.2.1.1.1">Popularity Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.2">99.86%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3">36.81%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.3.2.1.1">Adapt-LLM (NQ)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.2">87.22%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3">35.30%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T5.1.4.3.1.1">Adapt-LLM (SQuAD)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.4.3.2">83.99%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.4.3.3.1">37.29%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span> <span class="ltx_text ltx_font_smallcaps" id="S4.T5.4.1">Adapt-LLM</span> 및 <span class="ltx_text ltx_font_smallcaps" id="S4.T5.5.2">Popularity Score</span> 구성에 대한 SQuAD 및 NQ 데이터 세트에 대해 학습된 Llama-2 기본 모델의 성능 비교. 이후에는 Llama-2 LLM을 기본 모델로 하여 <cite class="ltx_cite ltx_citemacro_citet">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [<a class="ltx_ref" href="https://arxiv.org/html/2404.19705v2#bib.bib22" title="">22</a>]</cite>에서 제안한 방법론을 모방한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">본 논문에서는 파라메트릭 메모리에만 의존하지 않고 질문에 응답하기 위해 추가 컨텍스트가 필요할 때 식별하도록 학습하는 LLM인 <span class="ltx_text ltx_font_smallcaps" id="S5.p1.2.1">Adapt-LLM</span>을 소개한다. <span class="ltx_text ltx_font_smallcaps" id="S5.p1.2.2">Adapt-LLM</span>은 LLM의 파라메트릭 메모리만으로 답변 가능한 질문과 보충 컨텍스트가 필요한 질문을 구별하기 위해 수정된 개방형 도메인 질문 응답 데이터 세트에서 기본 LLM을 미세 조정한 결과이다. 이러한 학습 데이터 세트를 구성하기 위해 먼저 기본 LLM을 제로 샷 평가에 적용하여 질문에 대한 응답 정확도를 결정한다. 모델의 응답이 잘못된 질문의 경우 추가 컨텍스트의 필요성을 나타내는 특수 토큰인 <math alttext="\langle" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" stretchy="false" xref="S5.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">⟨</annotation></semantics></math>RET<math alttext="\rangle" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mo id="S5.p1.2.m2.1.1" stretchy="false" xref="S5.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">⟩</annotation></semantics></math>를 생성하도록 LLM을 훈련한다.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">PopQA 데이터 세트에 대해 수행된 광범위한 실험을 통해 <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">Adapt-LLM</span>이 관련 컨텍스트 정보를 절대 검색하지 않고 항상 검색하는 두 가지 고정된 대안보다 더 나은 성능을 발휘한다는 것을 보여준다. 또한, 연구 결과는 이 작업의 주요 목적인 추가 컨텍스트의 필요성을 효과적으로 식별하는 <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">Adapt-LLM</span>의 기능을 강조한다.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">추후 조사를 위해 학습 가능한 순차 검색 기술을 통합하는 것과 같은 IR 시스템을 사용할 때 성능을 향상시키기 위한 탐색 방법을 제안한다. 또한 <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.1">Adapt-LLM</span> 시스템 개발에서 훈련 및 테스트 데이터 세트 간의 상호 작용에 대한 보다 심층적인 분석을 수행하는 것이 유용할 것이라고 믿는다.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">이 작업은 연구 그룹 자금 IT1805-22와 ICL4LANG 프로젝트(그랜트 번호 KK-2023/00094)를 통해 바스크 정부로부터 부분적인 지원을 받았다. 또한, 여러 MCIN/AEI/10.13039/501100011033 프로젝트의 지원을 인정한다: (i) DeepKnowledge (PID2021-127777OB-C21) 및 EU FEDER로부터의 자금 지원; (ii) AWARE (TED2021-131617B-I00) 및 유럽 연합 NextGenerationEU/PRTR로부터의 지원. 실험 환경에서 도움을 주신 카를로스 도밍게스와 소중한 피드백과 안내를 주신 에네코 아기에르에게 감사를 표합니다</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Achiam, S.&nbsp;Adler, S.&nbsp;Agarwal, L.&nbsp;Ahmad, I.&nbsp;Akkaya, F.&nbsp;L. Aleman, D.&nbsp;Almeida, J.&nbsp;Altenschmidt, S.&nbsp;Altman, S.&nbsp;Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amnon Catav and Roy Miara and Ilai Giloh and Nathan Cordeiro and Amir Ingber.

</span>
<span class="ltx_bibblock">RAG makes LLMs better and equal.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.pinecone.io/blog/rag-study/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Barnett, S.&nbsp;Kurniawan, S.&nbsp;Thudumu, Z.&nbsp;Brannelly, and M.&nbsp;Abdelrazek.

</span>
<span class="ltx_bibblock">Seven failure points when engineering a retrieval augmented generation system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2401.05856</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berger et&nbsp;al. [2000]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Berger, R.&nbsp;Caruana, D.&nbsp;Cohn, D.&nbsp;Freitag, and V.&nbsp;Mittal.

</span>
<span class="ltx_bibblock">Bridging the lexical chasm: statistical approaches to answer-finding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</em>, pages 192–199, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">International conference on machine learning</em>, pages 2206–2240. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke.

</span>
<span class="ltx_bibblock">QuAC: Question Answering in Context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2174–2184, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erbacher et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Erbacher, L.&nbsp;Falissar, V.&nbsp;Guigue, and L.&nbsp;Soulier.

</span>
<span class="ltx_bibblock">Navigating uncertainty: Optimizing api dependency for hallucination reduction in closed-book question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2401.01780</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Gao, X.&nbsp;Yao, and D.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021</em>, pages 6894–6910. Association for Computational Linguistics (ACL), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier, Izacard and Mathilde, Caron and Lucas, Hosseini and Sebastian, Riedel and Piotr, Bojanowski and Armand, Joulin and Edouard, Grave.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei.

</span>
<span class="ltx_bibblock">REALM: retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 37th International Conference on Machine Learning</em>. JMLR.org, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard, Gautier and Grave, Edouard [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, Gautier and Grave, Edouard.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">EACL 2021-16th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 874–880. Association for Computational Linguistics, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACM Computing Surveys</em>, 55(12):1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Roux, A.&nbsp;Mensch, B.&nbsp;Savary, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l. Casas, E.&nbsp;B. Hanna, F.&nbsp;Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. Association for Computational Linguistics, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Liang, R.&nbsp;Bommasani, T.&nbsp;Lee, D.&nbsp;Tsipras, D.&nbsp;Soylu, M.&nbsp;Yasunaga, Y.&nbsp;Zhang, D.&nbsp;Narayanan, Y.&nbsp;Wu, A.&nbsp;Kumar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin, Stephanie and Hilton, Jacob and Evans, Owain [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, Stephanie and Hilton, Jacob and Evans, Owain.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring How Models Mimic Human Falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mallen, Alex Troy and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh.

</span>
<span class="ltx_bibblock">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">The 61st Annual Meeting Of The Association For Computational Linguistics</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2112.09332</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li.

</span>
<span class="ltx_bibblock">Ms marco: A human-generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, pages 2383–2392, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Transactions of the Association for Computational Linguistics</em>, 11:1316–1331, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap, J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Reimers and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et&nbsp;al. [2009]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Robertson, H.&nbsp;Zaragoza, et&nbsp;al.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Foundations and Trends® in Information Retrieval</em>, 3(4):333–389, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro, L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Seonwoo, Yeon and Son, Juhee and Jin, Jiho and Lee, Sang-Woo and Kim, Ji-Hoon and Ha, Jung-Woo and Oh, Alice Haeyun.

</span>
<span class="ltx_bibblock">Two-Step Question Retrieval for Open-Domain QA.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">60th Annual Meeting of the Association for Computational Linguistics, ACL 2022</em>, pages 1487–1492. Association for Computational Linguistics, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.

</span>
<span class="ltx_bibblock">Stanford alpaca: an instruction-following llama model (2023).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">URL https://github. com/tatsu-lab/stanford_alpaca</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Thakur, N.&nbsp;Reimers, A.&nbsp;Rücklé, A.&nbsp;Srivastava, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix, B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei, N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Zhu, W.&nbsp;Lei, C.&nbsp;Wang, J.&nbsp;Zheng, S.&nbsp;Poria, and T.-S. Chua.

</span>
<span class="ltx_bibblock">Retrieving and reading: A comprehensive survey on open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2101.00774</em>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header" data-bs-theme="dark"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
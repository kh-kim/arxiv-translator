# Large Language Models: Survey

셰르빈 미나에, 토마스 미콜로프, 나르제스 니차드, 메이삼 첸그루

리처드 사셔, 자비에 아마트리아인, 지안펑 가오

###### Abstract

대형 언어 모델(Large Language Models, LMM)은 2022년 11월 ChatGPT의 출시 이후 광범위한 자연어 작업에 대한 강력한 성능으로 많은 주목을 받고 있다. LMM의 범용 언어 이해 및 생성 능력은 스케일링 법칙에 의해 예측된 바와 같이 방대한 양의 텍스트 데이터에 대한 수십억 모델의 매개변수를 훈련함으로써 획득된다[1, 2]. LLM의 연구 분야는 매우 최근이지만 다양한 방식으로 빠르게 발전하고 있다. 이 논문에서는 세 가지 인기 LLM 패밀리(GPT, LLaMA, PaLM)를 포함하여 가장 두드러진 LLM 중 일부를 검토하고 특성, 기여도 및 한계에 대해 논의한다. 또한 LLM을 구축하고 증강하기 위해 개발된 기술에 대한 개요를 제공한다. 그런 다음 LLM 훈련, 미세 조정 및 평가를 위해 준비된 인기 데이터 세트를 조사하고 널리 사용되는 LLM 평가 메트릭을 검토하고 대표적인 벤치마크 세트에서 여러 인기 LLM의 성능을 비교한다. 마지막으로, 개방형 과제 및 향후 연구 방향에 대해 논의함으로써 논문을 마무리한다.

## I Introduction

언어 모델링은 1950년대 섀넌의 정보 이론을 인간 언어에 적용한 오랜 연구 주제이며, 여기서 그는 간단한 n-그램 언어 모델이 자연 언어 텍스트를 얼마나 잘 예측하거나 압축하는지 측정했다[3]. 그 이후로 통계적 언어 모델링은 음성 인식, 기계 번역에서 정보 검색에 이르기까지 많은 자연어 이해 및 생성 작업에 기본이 되었다[4, 5, 6].

최근 웹 규모의 텍스트 코퍼스에서 사전 훈련된 변압기 기반 대용량 언어 모델(LLM)의 발전은 언어 모델(LLM)의 기능을 크게 확장했다. 예를 들어, OpenAI의 ChatGPT 및 GPT-4는 자연어 처리뿐만 아니라 마이크로소프트의 Co-Pilot 시스템에 전력을 공급하기 위한 일반적인 작업 해결기로 사용될 수 있으며, 예를 들어, 필요할 때 다단계 추론을 수행하는 복잡한 새로운 작업의 인간 지시를 따를 수 있다. 따라서 LMM은 범용 AI 에이전트 또는 인공지능(AGI) 개발을 위한 기본 구성 요소가 되고 있다.

LLM 분야가 빠르게 진행됨에 따라 몇 달 또는 몇 주 만에 새로운 발견, 모델 및 기술이 발표됨에 따라 AI 연구자와 실무자는 종종 작업에 적합한 LLM 기반 AI 시스템을 구축하기 위한 최상의 레시피를 찾는 것이 어렵다는 것을 알게 된다. 이 논문은 LLM의 최근 발전에 대한 시의적절한 조사를 제공한다. 이 조사가 학생, 연구원 및 개발자에게 가치 있고 접근 가능한 자원이 되기를 바란다.

LLM은 신경망에 기반한 대규모 사전 훈련된 통계적 언어 모델이다. 최근 LLM의 성공은 수십 년 동안 축적된 언어 모델의 연구 개발이며, 통계 언어 모델, 신경 언어 모델, 사전 훈련 언어 모델 및 LLM의 시작점과 속도가 다른 4개의 파동으로 분류할 수 있다.

통계 언어 모델(SLM)은 텍스트를 단어의 시퀀스로 보고, 텍스트의 확률을 그들의 단어 확률의 곱으로 추정한다. SLM의 지배적인 형태는 n-gram 모델이라고 알려진 마르코프 체인 모델이며, 이 모델은 바로 진행되는 \(n-1\)개의 단어에 조건화된 단어의 확률을 계산한다. 단어 확률은 텍스트 말뭉치로부터 수집된 단어 및 n-그램 카운트를 사용하여 추정되기 때문에, 모델은 _smoothing_을 사용하여 데이터 희소성(즉, 보이지 않는 단어 또는 n-그램에 제로 확률을 할당함)을 다룰 필요가 있으며, 여기서 모델의 일부 확률 질량은 보이지 않는 n-그램에 대해 예약된다[12]. N-그램 모델은 많은 NLP 시스템에서 널리 사용된다. 그러나 이러한 모델은 데이터 희소성으로 인해 자연어의 다양성과 가변성을 완전히 포착할 수 없다는 점에서 불완전하다.

초기 신경망 모델(NLMs) [13, 14, 15, 16]은 단어를 저차원 연속 벡터(임베딩 벡터)에 매핑하여 데이터 희소성을 다루고, 신경망을 사용하여 진행 단어의 임베딩 벡터의 집계를 기반으로 다음 단어를 예측한다. NLM들에 의해 학습된 임베딩 벡터들은 벡터들 사이의 의미적 유사성이 그들의 거리로서 쉽게 계산될 수 있는 숨겨진 공간을 정의한다. 이것은 그들의 형태(예를 들어, 쿼리 대 쿼리)에 관계없이 임의의 두 입력의 의미론적 유사성을 컴퓨팅하는 문을 연다. 웹 검색에서의 문서[17, 18], 기계 번역에서의 상이한 언어의 문장[19, 20]) 또는 모달리티(예를 들어, 이미지 캡셔닝에서의 이미지 및 텍스트[21, 22])를 포함한다. 초기 NLM은 태스크 특정 데이터에 대해 훈련되고 학습된 숨겨진 공간이 태스크 특정적이라는 점에서 태스크 특정 모델이다.

사전 훈련된 언어 모델(PLM)은 초기 NLM과 달리 작업 불가지론적이다. 이러한 일반성은 또한 학습된 숨겨진 임베딩 공간으로 확장된다. PLM의 트레이닝 및 추론은 _사전-트레이닝 및 미세-튜닝_ 패러다임을 따르며, 여기서 순환 신경망[23] 또는 변압기[24, 25, 26]를 갖는 언어 모델들은 단어 예측과 같은 일반적인 태스크들을 위해 웹-스케일 비라벨 텍스트 코퍼라 상에서 사전 트레이닝된 후, 소량의 (라벨링된) 태스크-특정 데이터를 사용하여 특정 태스크들로 미세 조정된다. PLM에 대한 최근의 조사는 [8, 27, 28]을 포함한다.

LLM(Large Language Model)은 주로 표 Ⅲ에 요약된 바와 같이 PaLM[31], LLaMA[32], GPT-4[33]와 같이 방대한 텍스트 데이터에 대해 사전 훈련된 수십 내지 수천억 개의 파라미터를 포함하는 변압기 기반 신경망 언어 모델 1을 지칭한다. PLM에 비해 LLM은 모델 크기가 훨씬 클 뿐만 아니라 더 강력한 언어 이해 및 생성 능력, 더 중요한 것은 더 작은 규모의 언어 모델에는 존재하지 않는 창발적 능력을 나타낸다. 에 도시된 바와 같이. 도 1에서, 이러한 창발 능력들은 (1) 추론 시간에 프롬프트에 제시된 작은 예들의 세트로부터 LLM들이 새로운 태스크를 학습하는 맥락내 학습, (2) 명령어 튜닝 후에 LLM들이 명시적인 예들을 사용하지 않고 새로운 타입들의 태스크들에 대한 명령어들을 따를 수 있는 명령어 추종, 및 (3) LLM들이 연쇄-생각 프롬프트에서 입증된 바와 같이 중간 추론 단계들로 그 태스크를 분해함으로써 복잡한 태스크를 해결할 수 있는 다중 단계 추론을 포함한다[34]. LLM은 또한 외부 지식 및 도구를 사용하여 증강될 수 있어서 사용자 및 환경과 효과적으로 상호 작용할 수 있고[37], 상호 작용을 통해 수집된 피드백 데이터를 사용하여(예를 들어, 인간 피드백(RLHF)으로 강화 학습을 통해) 지속적으로 자신을 개선할 수 있다.

고급 사용 및 증강 기술을 통해 LLM은 환경을 감지하고 결정을 내리고 조치를 취하는 인공 개체라는 소위 AI 에이전트로 배치될 수 있다. 이전 연구는 특정 작업 및 도메인에 대한 에이전트 개발에 초점을 맞추었다. LLM이 보여주는 비상 능력은 LLM을 기반으로 범용 AI 에이전트를 구축할 수 있게 한다. LLM이 정적 설정에서 응답을 생성하도록 훈련되는 동안 AI 에이전트는 동적 환경과 상호 작용하기 위한 조치를 취해야 한다. 따라서 LLM 기반 에이전트는 외부 지식 베이스에서 업데이트된 정보를 얻고 시스템 액션이 예상 결과를 생성하는지 확인하고 예상대로 일이 진행되지 않을 때 대처하는 등 LLM을 보강해야 하는 경우가 많다. 섹션 IV에서 LLM 기반 에이전트에 대해 자세히 논의할 것이다.

이 논문의 나머지 부분에서 섹션 II는 3개의 LLM 패밀리(GPT, LLaMA 및 PaLM) 및 기타 대표적인 모델을 중심으로 LLM의 최신 기술에 대한 개요를 제시한다. 섹션 III에서는 LLM이 어떻게 구축되는지에 대해 논의한다. 섹션 IV에서는 LLM이 어떻게 사용되는지 논의하고 실제 애플리케이션에 대해 증강된 섹션 V 및 VI는 LLM을 평가하기 위한 인기 있는 데이터 세트 및 벤치마크를 검토하고 보고된 LLM 평가 결과를 요약한다. 마지막으로 제Ⅶ절은 과제 및 향후 연구 방향을 요약하여 논문을 마무리한다.

## II Large Language Models

이 섹션에서는 초기 사전 훈련된 신경 언어 모델이 LLM의 기본이기 때문에 검토를 시작으로 GPT, LlaMA 및 PaLM의 세 가지 LLM 계열에 대한 논의를 집중한다. 표 I은 이러한 모델 중 일부와 그 특성에 대한 개요를 제공한다.

### _초기 사전 훈련된 신경망 모델_

신경망을 이용한 언어 모델링은 [38, 39, 40]에 의해 개척되었다. Bengio et al. [13]은 n-gram 모델에 필적하는 최초의 신경 언어 모델(NLMs) 중 하나를 개발했다. 그런 다음 [14]는 기계 번역에 NLM을 성공적으로 적용했다. 미콜로프[41, 42]에 의한 RNNLM(오픈 소스 NLM 툴킷)의 출시는 NLM의 상당한 대중화에 도움이 되었다. 그 후, 순환 신경망(RNN)을 기반으로 하는 NLM과 LSTM[19] 및 GRU[20]와 같은 변형은 기계 번역, 텍스트 생성 및 텍스트 분류를 포함한 많은 자연 언어 응용 프로그램에 널리 사용되었다[43].

그런 다음, 트랜스포머 아키텍처의 발명[44]은 NLM의 발전에 또 다른 이정표를 표시한다. 문장이나 문서 내의 모든 단어에 대해 자기 주의를 적용하여 각 단어가 다른 단어에 미치는 영향을 모델링하기 위해 "주의 점수"를 병렬로 계산함으로써, 트랜스포머는 RNN보다 훨씬 더 많은 병렬화를 허용하여 GPU의 대용량 데이터에 대해 매우 큰 언어 모델을 효율적으로 사전 훈련할 수 있다. 이러한 사전 훈련된 언어 모델(PLM)은 많은 다운스트림 작업에 대해 미세 조정될 수 있다.

도. 1: LLM Capabilities.

초기 트랜스포머 기반 PLM은 신경망 구조를 기반으로 인코더 전용, 디코더 전용 및 인코더-디코더 모델의 세 가지 주요 범주로 분류된다. 초기 PLM에 대한 종합적인 조사는 [43, 28]에 제공된다.

#### Iii-A1 Encoder-only PLMs

이름에서 알 수 있듯이 인코더 전용 모델은 인코더 네트워크로만 구성됩니다. 이러한 모델들은 원래 텍스트 분류와 같은 언어 이해 작업을 위해 개발되었으며, 여기서 모델들은 입력 텍스트에 대한 클래스 라벨을 예측할 필요가 있다. 대표적인 인코더 전용 모델들은 BERT 및 그 변형들, 예를 들어, 아래에서 설명될 RoBERTa, ALBERT, DeBERTa, XLM, XLNet, UNILM을 포함한다.

BERT(Birectional Encoder Representations from Transformers)[24]는 가장 널리 사용되는 인코더 전용 언어 모델 중 하나이다. BERT는 세 개의 모듈로 구성된다 : (1) 입력 텍스트를 임베딩 벡터의 시퀀스로 변환하는 임베딩 모듈, (2) 임베딩 벡터를 문맥 표현 벡터로 변환하는 트랜스포머 인코더의 스택, (3) 표현 벡터(최종 계층에서)를 원-핫 벡터로 변환하는 완전 연결 계층. BERT는 MMLM(Masked Language Modeling)과 다음 문장 예측의 두 가지 목적을 사용하여 사전 학습된다. 사전 훈련된 BERT 모델은 텍스트에서 다양한 언어 이해 작업을 위한 분류기 계층을 추가하여 미세 조정될 수 있다.

도. 2 : 종이 구조.

분류, 언어 추론에 대한 질문 응답. BERT 프레임워크의 높은 수준의 개요는 그림 3에 나와 있다. BERT가 발표되었을 때 광범위한 언어 이해 작업에서 최신 기술을 크게 향상시켰기 때문에 AI 커뮤니티는 BERT를 기반으로 하는 많은 유사한 인코더 전용 언어 모델을 개발하도록 영감을 받았다.

RoBERTa[25]는 몇 가지 주요 하이퍼파라미터 수정, 다음 문장 사전 훈련 목표 제거 및 훨씬 더 큰 미니 배치 및 학습 속도로 훈련과 같은 일련의 모델 설계 선택 및 훈련 전략을 사용하여 BERT의 견고성을 크게 향상시킨다. ALBERT[45]는 메모리 소비를 낮추고 BERT의 트레이닝 속도를 증가시키기 위해 두 개의 파라미터 감소 기술을 사용한다: (1) 임베딩 매트릭스를 두 개의 더 작은 매트릭스로 분할하는 것, (2) 그룹들 사이에서 분할된 반복 계층을 사용하는 것. DeBERTa(Decoding-enhanced BERT with disentangled attention)[26]는 두 가지 새로운 기술을 사용하여 BERT 및 RoBERTa 모델을 개선한다. 첫 번째는 어텐션 메커니즘으로, 각 단어는 각각 내용과 위치를 인코딩하는 두 개의 벡터를 사용하여 표현되며 단어 간의 어텐션 가중치가 표시된다.

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline
**Type** & **Model Name** & **\#Parameters** & **Relase** & **Base Models** & **Open Source** & **\#Tokens** & **Training dataset** \\ \hline \multirow{6}{*}{**Encoder-Only**} & BERT & 110M, 340M & 2018 & - & ✓ & 137B & BooksCorpus, English Wikipedia \\  & RoBERTa & 355M & 2019 & - & ✓ & 2.2T & BooksCorpus, English Wikipedia, CC-NEWS, STORIES (a subset of Common Crawl), Reddit BooksCorpus, English Wikipedia \\  & ALBERT & 12M, 18M, 235M & 60M, 2019 & - & ✓ & 137B & BooksCorpus, English Wikipedia \\  & DeBERTa & - & 2020 & - & ✓ & - & BooksCorpus, English Wikipedia, STORIES, Reddit content \\  & XLNet & 110M, 340M & 2019 & - & ✓ & 32.89B & BooksCorpus, English Wikipedia, Gig5, Common Crawl, Chewe 2012-B \\ \hline \multirow{2}{*}{**Decoder-only**} & GPT-1 & 120M & 2018 & - & ✓ & 1.3B & BooksCorpus \\  & GPT-2 & 1.5B & 2019 & - & ✓ & 10B & Reddit outbound \\ \cline{2-7}  & T5 (Base) & 223M & 2019 & - & ✓ & 156B & Common Crawl \\  & MTS (Base) & 300M & 2020 & - & ✓ & - & New Common Crawl-based dataset in 101 languages (m Common Crawl) & \\  & BART (Base) & 139M & 2019 & - & ✓ & - & Computing text \\ \hline \multirow{6}{*}{**GPT Family**} & GPT-3 & 125M, 350M, 760M, 1, 13B, 2.7B, 6.7B, 6.7B, 13B, 175B & & & & & \\  & CODEX & 12B & 2021 & GPT & ✓ & - & Public GitHub software repositories \\  & WebGPT & 760M, 13B, 175B & 2021 & GPT-3 & \(\times\) & - & ELI5 \\  & GPT-4 & 1.767 & 2023 & - & \(\times\) & 13T & - \\ \hline \multirow{6}{*}{**LLAMA Family**} & LLMA1 & 78, 13B, 33B, 65B & 2023 & - & ✓ & 1T, 1.4T & Online sources \\  & LLMA2 & 78, 13B, 34B, 70B & 2023 & - & ✓ & 2T & Online sources \\  & Alpaca & 7B & 2023 & LLMA1 & ✓ & - & GPT-3 \\  & Vienna-13B & 13B & 2023 & LLMA1 & ✓ & - & GPT-3 \\  & Koula & 13B & 2023 & LLMA & ✓ & - & Dialogue data \\  & Mixtal-7B & 7.3B & 2023 & & ✓ & - & - \\  & Code Lima & 34 & 2023 & LLMA2 & ✓ & 500B & Publicly available code \\  & LongLLAMA & 3B, 7B & 2023 & OpenLLAMA & ✓ & 1T & - \\  & LLMA-Pro-8B & 8.3B & 2024 & LLMA-7B & ✓ & 80B & Code and math corpora \\  & TinyLlama-1.1B & 1.1B & 2024 & LLMA1.1B & ✓ & 3T & Simnjajtana, Staverderdata \\ \hline \multirow{6}{*}{**PaLM Family**} & PatLM & 8B, 62B, 540B & 2022 & - & \(\times\) & 780B & Web documents, books, Wikipedia, conversations, GitHub code \\  & U-PalM & 8B, 62B, 540B & 2022 & - & \(\times\) & 1.3B & Web documents, books, Wikipedia, conversations, GitHub code \\  & PuLM-2 & 340B & 2023 & - & ✓ & 3.6T & Web documents, books, code, mathematics, conversational data \\  & Med-PulM & 540B & 2022 & PuLM & \(\times\) & 780B & HealthSearchQA, MedicationQA, LiveQA \\  & Med-PulM 2 & - & 2023 & PuLM 2 & \(\times\) & - & ModQA, MedMCQA, HealthSearchQA, LiveQA \\  & MedicationQA & & & & & MedicationQA \\ \hline \multirow{6}{*}{**Other Popular LLA**} & PLAN & 137B & 2021 & LaMDA-PT & ✓ & - & Web documents, code, dialog data, Wikipedia \\  & Gober & 280B & 2021 & - & \(\times\) & 300B & MassiveText \\  & ERNE 4.0 & 10B & 2023 & - & \(\times\) & 4TB & Chinese text \\  & Retro & 7.5B & 2021 & - & \(\times\) & 600B & MassiveText \\  & LMDA & 137B & 2022 & - & \(\times\) & 168B & public dialog data and web documents \\  & ChinChilla & 70B & 2022 & - & \(\times\) & 1.4T & MassiveText \\  & Glacitci-120B & 120B & 2022 & - & \(\times\) & 450B & \\  & CodGen & 16.1B & 2022 & - & ✓ & - & THE PLE, BIGQUERY, BIGPYTHON \\  & BLOOM & 176B & 2022 & - & ✓ & 366B & ROOTS \\  & Zaghyr & 7.2Bk & 2023 & Mistral-7B & ✓ & 800B & Synthetic data \\  & Grok-0 & 33B & 2023 & - & \(\times\) & - & Online source \\  & ORCA-2 & 13B & 2023 & LLMA2 & - & 200B & - \\  & StartColor & 15.5B & 2023 & - & ✓ & 35B & GitHub \\  & MPT & 7B & 2023 & - & ✓ & 1T & RedPajama, m Common Crawl, S2ORC, Common Crawl \\  & Mixxtal-8x7B & 46.7B & 2023 & - & ✓ & - & Instruction dataset \\  & Falcon 180B & 180B & 2023 & - & ✓ & 3.5T & RefinedWeb \\  & Gemini & 1.8B, 3.25B & 2023 & - & ✓ & - & Web documents, books, and code, image data, audio data, video data \\  & DeepSee-Coder & 1.3B, 6.7B, 33B & 2024 & - & ✓ & 2T & GitHub’s Markdown and StackExchange \\  & DocLLM & 1B,7B & 2024 & - & \(\times\) & 2T & IIT-CDIP Test Collection 1.0, DocBank \\ \hline \hline \end{tabular}
\end{table} TABLE I: High-level Overview of Popular Language Modelsare computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a novel virtual adversarial training method is used for fine-tuning to improve models' generalization. ELECTRA [46] uses a new pre-training task, known as replaced token detection (RTD), which is empirically proven to be more sample-efficient than MLM. Instead of masking the input, RTD corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not. RTD is more sample-efficient than MLM because the former is defined over all input tokens rather than just the small subset being masked out, as illustrated in Fig 4.

XLMs [47]은 두 가지 방법을 사용하여 BERT를 교차 언어 모델로 확장했다. (1) 단일 언어 데이터에만 의존하는 비감독 방법과 (2) 도 5에 예시된 바와 같이 새로운 교차 언어 모델 목표로 병렬 데이터를 활용하는 감독 방법. XLMs는 제안된 시점에서 교차 언어 분류, 비감독 및 감독 기계 번역에 대한 최신 결과를 얻었다.

모델 학습 및 추론을 위해 자동 회귀(디코더) 모델의 장점을 활용하는 인코더 전용 언어 모델도 있다. 두 가지 예는 XLNet과 UNILM입니다. XLNet[48]은 트랜스포머-XL을 기반으로 하며, 인수분해 차수의 모든 순열에 대해 예상 우도를 최대화하여 양방향 컨텍스트를 학습할 수 있도록 하는 일반화된 자기회귀 방법을 사용하여 사전 훈련된다. UNILM(UNIfied pre-trained Language Model)[49]은 단방향, 양방향, 시퀀스-투-시퀀스 예측의 세 가지 유형의 언어 모델링 작업을 사용하여 사전 트레이닝된다. 이는 공유 트랜스포머 네트워크를 사용하고 그림 6에 예시된 바와 같이 예측이 조건화되는 컨텍스트를 제어하기 위해 특정 자기 주의 마스크를 활용함으로써 달성된다. 사전 훈련된 모델은 자연어 이해 및 생성 작업 모두에 대해 미세 조정될 수 있다.

#### Iii-C2 Decoder-only PLMs

가장 널리 사용되는 디코더 전용 PLM 중 두 가지는 OpenAI에서 개발한 GPT-1과 GPT-2이다. 이러한 모델은 이후에 GPT-3 및 GPT-4와 같은 더 강력한 LLM의 기초를 마련한다.

GPT-1[50]은 자율 지도 학습 방식(즉, 다음 단어/토큰 프리딕)으로 레이블이 지정되지 않은 텍스트의 다양한 말뭉치 상에서 디코더 전용 트랜스포머 모델의 GPT(Generative Pre-Training)에 의해 광범위한 자연어 작업에 대한 우수한 성능을 얻을 수 있음을 처음으로 보여준다.

도. 4: 대체된 토큰 검출과 마스킹된 언어 모델링 간의 비교. [46]의 예의.

도. 5: 교차-언어 언어 모델 사전 트레이닝. MLM 목표는 BERT와 유사하지만 문장 쌍과 대조적으로 텍스트의 연속적인 스트림을 가지고 있다. TLM 목표는 MLM을 병렬 문장 쌍으로 확장한다. 마스크된 영어 단어를 예측하기 위해, 모델은 영어 문장과 그것의 프랑스어 번역 모두에 참석할 수 있고, 영어와 프랑스어 표현을 정렬하도록 권장된다. [47]의 예절.

도. 3: 전반적으로 BERT에 대한 사전 훈련 및 미세 조정 절차. [24]의 예의.

도. 6: 통합 LM 사전 훈련의 개요. 모델 매개변수는 LM 목표(즉, 양방향 LM, 단방향 LM 및 시퀀스 간 LM)에 걸쳐 공유된다. [49]의 예절.

그림 7에 도시된 바와 같이, GPT-1은 각 버전이 아키텍처를 개선하고 다양한 언어 작업에서 더 나은 성능을 달성하면서 후속 GPT 모델을 위한 길을 열어준다.

GPT-2[51]은 언어 모델들이 수백만 개의 웹 페이지들로 구성된 대형 WebText 데이터세트에서 트레이닝될 때 명시적인 감독 없이 특정 자연어 태스크들을 수행하는 것을 학습할 수 있음을 보여준다. GPT-2 모델은 GPT-1의 모델 설계를 몇 가지 수정으로 따른다: 계층 정규화는 각 서브-블록의 입력으로 이동되고, 최종 셀프-어텐션 블록 후에 추가 계층 정규화가 추가되며, 초기화는 잔여 경로 상의 축적을 고려하도록 수정되고 잔여 계층들의 가중치들을 스케일링하고, 어휘 크기는 50,25로 확장되고, 컨텍스트 크기는 512에서 1024 토큰으로 증가된다.

#### Iv-A3 Encoder-Decoder PLMs

[52]에서, Raffle 등은 거의 모든 NLP 태스크들이 시퀀스 투 시퀀스 생성 태스크로서 캐스팅될 수 있음을 보여준다. 따라서, 인코더-디코더 언어 모델은, 설계에 의해, 모든 자연 언어 이해 및 생성 작업을 수행할 수 있다는 점에서 통일된 모델이다. 아래에서 살펴볼 대표적인 인코더-디코더 PLM은 T5, mT5, MASS, BART이다.

T5[52]는 텍스트 대 텍스트 트랜스퍼 트랜스포머(Text-to-Text Transfer Transformer; T5) 모델로서, 모든 NLP 태스크가 텍스트 대 텍스트 생성 태스크로서 캐스팅되는 통합 프레임워크의 도입을 통해 전이 학습이 NLP에 효과적으로 악용된다. mT5 [53]은 101개 언어의 텍스트로 구성된 새로운 Common Crawl 기반 데이터 세트에 사전 훈련된 T5의 다국어 변형이다.

MASS(MASked Sequence to Sequence pre-training)[54]는 인코더-디코더 프레임워크를 채택하여 문장의 나머지 부분이 주어진 문장 조각을 재구성한다. 인코더는 랜덤하게 마스킹된 프래그먼트(몇 개의 연속적인 토큰들)를 입력으로 하는 문장을 취하고, 디코더는 마스킹된 프래그먼트를 예측한다. 이러한 방식으로, MASS는 언어 임베딩 및 생성을 위한 인코더 및 디코더를 각각 공동으로 트레이닝한다.

BART[55]는 표준 시퀀스 대 시퀀스 변환 모델 아키텍처를 사용한다. 임의의 잡음함수로 텍스트를 손상시킨 후 원문을 재구성하는 학습을 통해 사전 학습한다.

### _Large Language Model Families_

LOM(Large Language Model)은 주로 수십에서 수천억 개의 파라미터를 포함하는 트랜스포머 기반의 PLM을 의미한다. 위에서 검토한 PLM과 비교하여 LLM은 모델 크기가 훨씬 클 뿐만 아니라 더 작은 규모의 모델에서는 존재하지 않는 더 강력한 언어 이해 및 생성 및 창발 능력을 나타낸다. 다음에서 그림 8과 같이 GPT, LLaMA 및 PaLM의 세 가지 LLM 패밀리를 검토한다.

#### Iv-B1 **GPT 패밀리**

GPT(Generative Pre-trained Transformers)는 OpenAI에서 개발한 디코더 전용 Transformer 기반 언어 모델이다. 이 패밀리는 GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4, CODEX 및 WebGPT로 구성된다. GPT-1 및 GPT-2와 같은 초기 GPT 모델은 오픈 소스이지만, GPT-3 및 GPT-4와 같은 최근 모델은 클로즈 소스이며 API를 통해서만 액세스할 수 있다. GPT-1 및 GPT-2 모델은 초기 PLM 하위 섹션에서 논의되었다. 아래 GPT-3부터 시작합니다.

GPT-3 [56]은 1,750억 개의 매개변수를 가진 사전 훈련된 자기 회귀 언어 모델이다. GPT-3는 이전의 PLM보다 훨씬 클 뿐만 아니라 이전의 더 작은 PLM에서 관찰되지 않는 비상 능력을 처음으로 보여준다는 점에서 최초의 LLM으로 널리 간주된다. GPT-3은 그래프 업데이트나 미세 조정 없이 모든 다운스트림 작업에 GPT-3을 적용할 수 있음을 의미하며, 모델과의 텍스트 상호 작용을 통해 태스크와 몇 개의 샷 데모가 순전히 지정된다. GPT-3는 번역, 질문 응답 및 클로즈 작업을 포함한 많은 NLP 작업뿐만 아니라 문장에서 새로운 단어를 사용하여 스크램블링 해제 또는 도메인 적응과 같은 즉각적인 추론 또는 도메인 적응이 필요한 여러 작업에서 강력한 성능을 달성했다. 그림 9는 문맥 내 프롬프트에서 예제 수의 함수로 GPT-3의 성능을 보여준다.

오픈AI가 2023년 3월 출시한 코덱스[57]는 자연어를 파싱해 대응해 코드를 생성할 수 있는 범용 프로그래밍 모델이다. CODEX는 GPT-3의 후손이며, GitHub에서 수집된 코드 코퍼라에서 프로그래밍 애플리케이션을 위해 미세 조정된다. CODEX는 Microsoft의 GitHub 코파일롯에 전원을 공급합니다.

WebGPT[58]은 GPT-3의 또 다른 후손이며, 텍스트 기반 웹 브라우저를 사용하여 개방형 질문에 답하도록 미세 조정되어 사용자가 웹을 검색하고 탐색할 수 있게 한다. 구체적으로, WebGPT는 세 단계로 훈련된다. 첫 번째는 WebGPT가 인간의 시연 데이터를 이용하여 인간의 브라우징 행위를 모방하는 것을 학습하는 것이다. 그리고, 보상 함수를 학습하여 인간의 선호도를 예측한다. 마지막으로, 강화 학습 및 거부 샘플링을 통해 보상 함수를 최적화하기 위해 WebGPT를 정제한다.

LLM들이 예상된 인간 지시들을 따를 수 있게 하기 위해, InstructGPT[59]는 인간 피드백으로 미세 조정함으로써 광범위한 태스크들에 대한 사용자 의도에 언어 모델들을 정렬하도록 제안된다. OpenAI API를 통해 제출된 라벨러 작성 프롬프트 및 프롬프트 세트로 시작하여, 원하는 모델 동작의 라벨러 시연 데이터세트가 수집된다. 그런 다음 GPT-3가 이 데이터 집합에서 미세 조정됩니다. 그런 다음 강화 학습을 사용하여 모델을 추가로 미세 조정하기 위해 인간 순위 모델 출력의 데이터 세트를 수집한다. 이 방법은 인간의 피드백으로부터 강화 학습으로 알려져 있다.

도. 7: GPT 사전 훈련의 상위 레벨 개요, 및 미세 조정 단계. 오픈아이의 예절

결과적으로 InstructGPT 모델은 공개 NLP 데이터 세트에 대해 최소한의 성능 회귀를 가지면서 진실성의 개선과 독성 출력 생성의 감소를 보여주었다.

LLM 개발의 가장 중요한 이정표는 2022년 11월 30일 ChatGPT(Chat Generative Pre-trained Transformer)[60]의 출시이다. ChatGPT는 사용자가 대화를 조종하여 질의 응답, 정보 탐색, 텍스트 요약 등과 같은 광범위한 작업을 완료할 수 있도록 하는 챗봇이다. ChatGPT는 프롬프트에서 명령을 따르고 상세한 응답을 제공하도록 트레이닝되는 InstructGPT에 대한 형제 모델인 GPT-3.5(및 나중에 GPT-4에 의해)에 의해 구동된다.

GPT-4 [33]은 GPT 계열에서 가장 최신의 가장 강력한 LLM이다. 2023년 3월 출시된 GPT-4는 이미지와 텍스트를 입력으로 받아 텍스트 출력을 낼 수 있다는 점에서 멀티모달 LLM이다. 가장 도전적인 실세계 시나리오들 중 일부에서 인간보다 여전히 능력이 덜하지만, GPT-4는 도 11에 도시된 바와 같이, 수험생들의 상위 10% 정도의 점수를 갖는 시뮬레이션된 변호사 시험을 통과하는 것을 포함하는 다양한 전문적 및 학문적 벤치마크에서 인간 수준의 성능을 나타낸다. 초기 GPT 모델들과 마찬가지로, GPT-4는 먼저 큰 텍스트 말뭉치에서 다음 토큰을 예측하도록 사전 훈련된 다음, RLHF로 미세 조정되어 모델 거동을 인간이 원하는 것과 정렬한다.

#### V-A2 **LLaMA 패밀리**

LLaMA는 메타가 발표한 기초 언어 모델 모음이다. GPT 모델과 달리 LLaMA 모델은 오픈 소스이며, 즉 모델 가중치는 비상업적 라이선스에 따라 연구 커뮤니티에 공개된다. 따라서 LLaMA 계열은 폐쇄 소스와 경쟁하기 위해 더 나은 오픈 소스 LLM을 개발하거나 임무가 중요한 응용 프로그램을 위한 태스크별 LLM을 개발하기 위해 많은 연구 그룹에서 널리 사용됨에 따라 빠르게 성장한다.

LLaMA 모델[32]의 첫 번째 세트는 2023년 2월에 7B에서 65B 매개변수에 이르기까지 출시되었다. 이러한 모델은 공개적으로 사용 가능한 데이터 세트에서 수집 된 수 조 개의 토큰에 대해 사전 훈련 됩니다. LLaMA는 GPT-3의 트랜스포머 아키텍처를 사용하는데, 여기에는 (1) ReLU 대신 SwiGLU 활성화 함수를 사용하고, (2) 절대 위치 임베딩 대신 회전 위치 임베딩을 사용하고, (3) 표준 계층 정규화 대신 루트 평균 제곱 계층 정규화를 사용하는 등 몇 가지 사소한 아키텍처 수정이 있다. 오픈 소스 LLaMA-13B 모델은 대부분의 벤치마크에서 독점 GPT-3(175B) 모델보다 성능이 우수하여 LLM 연구에 좋은 기준선이 되었다.

도. 10: RLHF의 상위 레벨 개요. [59]의 예절.

도. 도 9: GPT-3은 더 큰 모델들이 인-컨텍스트 정보를 점점 더 효율적으로 사용하는 것을 보여준다. 이것은 자연어 태스크 설명이 있거나 없는 단어에서 랜덤 기호를 제거하기 위해 모델이 필요한 간단한 태스크에서 문맥 내 학습 성능을 보여준다. [56]의 예절.

도. 8: 인기있는 LLM 가족.

Meta는 2023년 7월 마이크로소프트와 제휴하여 LLaMA-2 채팅으로 알려진 대화용으로 피니튜닝된 기본 언어 모델과 Chat 모델을 모두 포함하는 LLaMA-2 컬렉션[61]을 출시했다. LLaMA-2 Chat 모델은 많은 공개 벤치마크에서 다른 오픈 소스 모델을 능가하는 것으로 보고되었다. 도 12는 LLaMA-2 Chat의 트레이닝 과정을 나타낸다. 이 과정은 공개적으로 이용 가능한 온라인 데이터를 사용하여 LLaMA-2를 사전 훈련하는 것으로 시작한다. 그런 다음, 감독된 미세 조정을 통해 LLaMA-2 Chat의 초기 버전이 구축된다. 이후 RLHF, 거부 샘플링 및 근접 정책 최적화를 사용하여 모델을 반복적으로 정제한다. RLHF 단계에서 보상 모델을 수정하기 위한 인간 피드백의 축적은 보상 모델이 너무 많이 변경되는 것을 방지하여 LLaMA 모델 훈련의 안정성을 해칠 수 있다.

알파카[62]는 GPT-3.5(text-davinci-003)를 사용하여 자체 지시 방식으로 생성된 52K 지시-추종 시연을 사용하여 LLaMA-7B 모델로부터 미세 조정된다. 알파카는 훈련, 특히 학술 연구에 매우 비용 효율적입니다. 자기 지시 평가 세트에서 알파카는 알파카가 훨씬 작음에도 불구하고 GPT-3.5와 유사하게 수행한다.

비쿠나 팀은 ShareGPT에서 수집한 사용자 공유 대화에서 LLaMA를 미세 조정하여 13B 채팅 모델인 비쿠나-13B를 개발했다. GPT-4를 평가자로 사용한 예비평가 결과, 비쿠나-13B는 OpenAI의 ChatGPT, 구글의 Bard의 90% 이상의 품질을 달성하면서도 LLaMA, 스탠포드 알파카와 같은 다른 모델을 90% 이상 능가하는 것으로 나타났다. 도 13은 GPT-4에 의한 Vicuna 및 몇몇 다른 잘 알려진 모델의 상대적 응답 품질을 도시한다. Vicuna-13B의 또 다른 이점은 모델 훈련에 대한 상대적으로 제한된 계산 수요이다. 비쿠나-13B의 훈련 비용은 단지 300달러이다.

알파카 및 비쿠나와 마찬가지로 구아나코 모델 [63]도 명령어 추적 데이터를 사용하여 미세 조정된 LLaMA 모델이다. 그러나 단일 48GB GPU에서 65B 파라미터 모델을 finetuning 할 수 있도록 QLoRA를 사용하여 finetuning을 매우 효율적으로 수행한다. QLoRA는 동결된 4비트 양자화된 사전 훈련된 언어 모델을 통해 구배를 LoRA(Low Rank Adapters)로 역전파한다. 최고의 구아나코 모델은 이전에 출시된 모든 모델을 비쿠나 벤치마크에서 능가하여 챗GPT의 성능 수준의 99.3%에 도달하는 동시에 단일 GPU에서 24시간의 미세 조정만 필요합니다.

코알라[64]는 LLaMA에 구축된 또 다른 명령어 추종 언어 모델이지만, ChatGPT와 같은 고도로 유능한 폐쇄 소스 채팅 모델에 의해 생성된 사용자 입력 및 응답을 포함하는 상호작용 데이터에 특정 초점을 맞춘다. Koala-13B 모델은 실제 사용자 프롬프트에 기반한 인간 평가에 따라 최첨단 채팅 모델과 경쟁적으로 수행한다.

Mistral-7B[65]는 우수한 성능과 효율성을 위해 설계된 7B-파라미터 언어 모델이다. 미스트랄-7B는 평가된 모든 벤치마크에서 최고의 오픈 소스 13B 모델(LLaMA-2-13B)과 추론, 수학 및 코드 생성에서 최고의 오픈 소스 34B 모델(LLaMA-34B)보다 성능이 우수하다. 이 모델은 더 빠른 추론을 위해 그룹화된 질의 어텐션을 활용하며, 슬라이딩 윈도우 어텐션과 결합하여 추론 비용을 줄이고 임의 길이의 시퀀스를 효과적으로 처리한다.

LLaMA 계열은 코드 LLaMA[66], 고릴라[67], 기라페[68], 비고그네[69], 툴루 65B[70], 롱 LLaMA[71], 안정 벨루가2[72]를 포함하여 LLaMA 또는 LLaMA-2에 더 많은 명령 추종 모델이 구축됨에 따라 빠르게 성장하고 있다.

#### V-B3 **PaLM 패밀리**

PaLM(Pathways Language Model) 패밀리는 구글에 의해 개발된다. 첫 번째 PaLM 모델[31]은 2022년 4월에 발표되어 2023년 3월까지 비공개로 유지되었으며, 540B 파라미터 트랜스포머 기반 LLM이다. 이 모델은 광범위한 자연어 작업과 사용 사례를 구성하는 7,800억 개의 토큰으로 구성된 고품질 텍스트 코퍼스에 대해 사전 훈련된다. PaLM은 사전 트레이닝된

도. 11: GPT-4 성능, GPT 3.5와 비교, [33]의 예.

도. 12: LLaMA-2 Chat의 트레이닝. [61]의 예절.

도. 13: 비쿠나의 상대 응답 품질 및 GPT-4에 의해 몇몇 다른 잘 알려진 모델. 비쿠나 팀의 제공.

경로 시스템을 사용하여 6144 TPU v4 칩에서 여러 TPU 포드에 걸쳐 매우 효율적인 교육을 수행할 수 있습니다. PaLM은 수백 개의 언어 이해 및 생성 벤치마크에 대해 최첨단의 소수의 학습 결과를 달성함으로써 스케일링의 지속적인 이점을 보여준다. PaLM-540B는 일련의 다단계 추론 작업에서 최첨단 미세 조정 모델뿐만 아니라 최근에 출시된 BIG 벤치마크에서 인간과 동등한 성능을 보인다.

8B, 62B 및 540B 스케일의 U-PaLM 모델은 UL2R이 있는 PaLM에서 지속적으로 훈련되며, UL2의 디노이저 혼합물 대물렌즈를 사용하여 몇 단계로 LLM을 계속 훈련하는 방법이다[73]. 약 2배 계산 절감율이 보고된다.

U-PaLM은 나중에 Flan-PaLM으로 지시-조정된다[74]. 위에서 언급한 다른 명령어 피니튜닝 작업에 비해 Flan-PaLM의 피니튜닝은 훨씬 더 많은 수의 작업, 더 큰 모델 크기 및 체인-오브-사상 데이터를 사용하여 수행된다. 결과적으로, Flan-PaLM은 이전의 명령어-추종 모델들보다 실질적으로 더 우수하다. 예를 들어, 1.8K 태스크에 대한 명령어 조정인 Flan-PaLM-540B는 큰 마진(평균 +9.4%)으로 PaLM-540B보다 성능이 우수하다. 피니튜닝 데이터는 그림 14와 같이 473개의 데이터 세트, 146개의 작업 범주 및 1,836개의 총 작업으로 구성된다.

PaLM-2 [75]는 이전 PaLM에 비해 더 나은 다국어 및 추론 기능을 가진 더 계산 효율적인 LLM이다. PaLM-2는 목적들의 혼합물을 사용하여 트레이닝된다. 영어, 다국어 및 추론 작업에 대한 광범위한 평가를 통해 PaLM-2는 다양한 모델 크기에 걸쳐 다운스트림 작업에 대한 모델 성능을 크게 향상시키는 동시에 PaLM보다 빠르고 효율적인 추론을 나타낸다.

Med-PaLM[76]은 도메인-특정 PaLM이며, 의료 질문에 대한 고품질 답변을 제공하도록 설계된다. Med-PaLM은 몇 가지 예를 사용하여 LLM을 새로운 도메인에 정렬하기 위한 매개변수 효율적인 방법인 지시 프롬프트 튜닝을 사용하여 PaLM에서 미세 조정된다. Med-PaLM은 여전히 인간 임상의보다 열등하지만 많은 의료 작업에서 매우 고무적인 결과를 얻는다. Med-PaLM 2는 Med-도메인 피니튜닝 및 앙상블 프롬프팅[77]을 통해 Med-PaLM을 개선한다. Med-PaLM 2는 MedQA 데이터 세트(즉, 전문 의료 검사, 연구 및 소비자 쿼리에 걸쳐 있는 6개의 기존 개방형 질문 응답 데이터 세트를 결합한 벤치마크)에서 최대 86.5%의 점수를 얻었으며, Med-PaLM을 19% 이상 개선하고 새로운 최신 기술을 설정했다.

### _Other Representative LLMs_

이전 하위 섹션에서 논의된 모델 외에도 세 모델 패밀리에 속하지 않는 다른 인기 있는 LLM이 있지만 우수한 성능을 달성했으며 LLM 필드를 앞으로 밀어냈다. 우리는 이 하위 섹션에서 이러한 LLM을 간략하게 설명한다.

**FLAN:** [78]에서 Wei 등은 언어 모델의 제로 샷 학습 능력을 개선하기 위한 간단한 방법을 탐색했습니다. 그들은 명령어를 통해 설명된 데이터 세트의 컬렉션에 대한 명령어 튜닝 언어 모델이 보이지 않는 작업에 대한 제로 샷 성능을 실질적으로 향상시킨다는 것을 보여주었다. 이들은 사전 훈련된 137B 파라미터 언어 모델을 취하고 자연어 명령어 템플릿을 통해 언어화된 60개 이상의 NLP 데이터 세트에 대해 명령어를 튜닝한다. 그들은 이 명령어 조정 모델 FLAN이라고 부른다. 그림 15는 프리트레인-피네이션과 프롬프팅과의 명령어 튜닝의 비교를 제공한다.

**Gopher:** [79]에서 Rae 등은 Gopher라고 하는 수천만 개의 매개 변수를 사용 하는 모델에서 최대 2,800억 개의 매개 변수 모델을 사용 하는 광범위한 모델 규모에 걸쳐 Transformer 기반 언어 모델 성능 분석을 제시 했습니다. 이 모델은 152개의 다양한 작업에 대해 평가되어 대다수에 걸쳐 최첨단 성능을 달성했다. 레이어의 수, 키/값 크기 및 다른 모델 크기의 기타 하이퍼 매개 변수는 그림 16에 나와 있다.

**T0:** [80]에서 Sanh 등은 모든 자연 언어 작업을 사람이 읽을 수 있는 프롬프트 형식으로 쉽게 매핑하는 시스템인 T0를 개발했습니다. 그들은 각각 다양한 문구를 가진 여러 프롬프트가 있는 많은 감독 데이터 세트 세트를 변환했다.

도. 14: Flan-PaLM 피니튜닝은 위의 태스크 카테고리에 있는 473개의 데이터 세트로 구성된다. [74]의 예절.

도. 15: 사전 훈련-피네튠과 지시 튜닝의 비교 및 프롬프트. [78]의 예의.

이러한 프롬프트 데이터 세트는 모델이 완전히 보류된 작업을 수행할 수 있는 능력을 벤치마킹할 수 있도록 한다. 그리고, 텍스트 입력을 소비하고 타겟 응답을 생성하기 위해 T0 인코더-디코더 모델이 개발된다. 이 모델은 서로 다른 작업으로 분할된 NLP 데이터 세트의 다중 작업 혼합에 대해 학습됩니다.

**ERNIE 3.0:** [81]에서 Sun 등은 대규모 지식 강화 모델을 사전 훈련하기 위해 ERNIE 3.0이라는 통합 프레임워크를 제안했습니다. 자동-회귀 네트워크와 자동-인코딩 네트워크를 융합하여, 훈련된 모델이 제로-샷 학습, 적은-샷 학습 또는 미세조정을 사용하여 자연어 이해 및 생성 작업 모두에 대해 쉽게 맞춤화될 수 있다. 그들은 일반 텍스트와 대규모 지식 그래프로 구성된 4TB 코퍼스에서 100억 개의 매개변수로 ERNIE 3.0을 훈련했다. 그림 17은 어니 3.0의 모델 아키텍처를 보여준다.

**RETRO:** [82]에서 Borgeaud 등은 이전 토큰과의 로컬 유사성을 기반으로 대규모 코퍼스에서 검색된 문서 청크를 컨디셔닝하여 자동 회귀 언어 모델을 향상시켰습니다. 2조 톤-토큰 데이터베이스를 사용하여, 검색-향상된 트랜스포머(Retro)는 25% 더 적은 파라미터들을 사용함에도 불구하고, 파일 상의 GPT-3 및 쥬라기-1[83]에 필적하는 성능을 얻는다. 도 18에 도시된 바와 같이, Retro는 동결된 Bert 리트리버, 미분가능한 인코더 및 청크된 크로스-어텐션 메커니즘을 결합하여 트레이닝 동안 통상적으로 소비되는 것보다 10배 더 많은 데이터에 기초하여 토큰을 예측한다.

**GLaM:** [84]에서 Du 등은 조밀한 변형에 비해 상당히 적은 훈련 비용을 발생시키면서 모델 용량을 확장하기 위해 희박하게 활성화된 전문가 혼합물 아키텍처를 사용하는 GLaM(Generalist Language Model)이라는 LLM 패밀리를 제안했습니다. 가장 큰 GLaM은 1.2조 개의 파라미터를 가지며, 이는 GPT-3보다 약 7배 크다. GPT-3을 훈련하는 데 사용되는 에너지의 1/3만을 소모하고 추론을 위해 계산 플롭의 절반을 필요로 하는 반면, 29개의 NLP 태스크에서 여전히 더 나은 전체 0, 1 및 소수의 샷 성능을 달성한다. 도 19는 GLAM의 하이 레벨 아키텍처를 도시한다.

**LaMDA:** [85]에서 Thoppilan 등은 최대 137B 매개 변수를 가지며 공용 대화 데이터 및 웹 텍스트의 1.56T 단어에 대해 사전 훈련된 대화체에 특화된 Transformer 기반 신경 언어 모델 계열인 LaMDA를 제시했습니다. 그들은 주석이 달린 데이터로 미세 조정하고 모델이 외부 지식 소스를 상담할 수 있도록 하는 것이 안전 및 사실적 접지의 두 가지 주요 문제에 대한 상당한 개선으로 이어질 수 있음을 보여주었다.

**OPT:** [86]에서 Zhang 등은 연구자와 공유 하는 125M에서 175B 매개 변수 범위의 디코더 전용 사전 훈련 된 변압기의 집합인 Open Pre-trained Transformers (OPT)를 제시 했습니다. OPT 모델의 매개변수는 20으로 표시된다.

도. 19: GLaM 모델 아키텍처. 각각의 MoE 층(하부 블록)은 Transformer 층(상부 블록)과 인터리빙된다. [84]의 예절.

도. 20: 상이한 OPT 모델의 아키텍처 세부사항. [86]의 예절.

도. 17: High-level model architecture of ERNIE 3.0. Courtesy of [81].

**Chinchilla:** [2]에서 Hoffmann 등은 주어진 컴퓨팅 예산에서 변압기 언어 모델을 훈련하기 위한 최적의 모델 크기 및 토큰 수를 조사했습니다. 5-5천억 개의 토큰에 대해 7천만에서 160억 개 이상의 파라미터에 이르는 400개 이상의 언어 모델을 훈련함으로써, 컴퓨팅 최적 훈련을 위해 모델 크기와 훈련 토큰의 수를 동일하게 조정해야 한다는 것을 발견했다. 모델 크기가 두 배로 증가할 때마다 훈련 토큰의 수도 두 배로 증가해야 한다. 그들은 고퍼와 동일한 컴퓨팅 예산을 사용하지만 70B 매개변수와 4% 더 많은 데이터를 사용하는 예측된 컴퓨팅 최적 모델인 친칠라를 훈련시켜 이 가설을 테스트했다.

**Galactica:** [87]에서 Taylor 등은 과학적 지식에 대해 저장, 결합 및 추론할 수 있는 대규모 언어 모델인 Galactica를 소개했습니다. 그들은 논문, 참고 자료, 지식 기반 및 기타 많은 출처의 대규모 과학 코퍼스에 대해 훈련했다. Galactica는 추론에서 잘 수행되어 수학적 MMLU에서 Chinchilla를 41.3%~35.7%, MATH에서 PaLM 540B를 20.4% 대 8.8%의 점수로 능가했다.

**CodeGen:** [88]에서 Nijkamp 등은 자연어 및 프로그래밍 언어 데이터에 대해 CODEGEN이라고 하는 최대 16.1B 매개 변수 패밀리를 교육 하 고 릴리스 하 고 교육 라이브러리 JAXFORMER를 공개 했습니다. 그들은 휴먼에벌에서 제로샷 파이썬 코드 생성에 대한 기존 최신 기술과 경쟁력이 있음을 입증함으로써 훈련된 모델의 유용성을 보여주었다. 그들은 또한 단일 프로그램이 하위 문제를 지정하는 여러 프롬프트로 인수분해되는 프로그램 합성을 위한 다단계 패러다임을 조사했다. 또한 멀티턴 프롬프트로 인수분해된 115개의 다양한 문제집합으로 구성된 개방형 벤치마크인 멀티턴 프로그래밍 벤치마크(Multi-Turn Programming Benchmark, MTPB)를 구축하였다.

**AlexaTM:** [89]에서 Soltan 등은 디노이징과 인과 언어 모델링(CLM) 태스크들의 혼합에 대해 사전 트레이닝된 다국어 대규모 시퀀스 투 시퀀스(seq2seq) 모델들이 다양한 태스크들에 대한 디코더-전용 모델들보다 더 효율적인 소수의 샷 학습자들이라는 것을 입증하였다. 그들은 알렉사 교사 모델(AlexaTM 20B)이라고 불리는 200억 개의 파라미터 다국어 seq2seq 모델을 훈련시켰고, 1-샷 요약 작업에서 최첨단(SOTA) 성능을 달성하여 훨씬 더 큰 540B PaLM 디코더 모델을 능가한다는 것을 보여주었다. AlexaTM은 46개의 인코더 레이어, 32개의 디코더 레이어, 32개의 어텐션 헤드, \(d_{model}=4096\)으로 구성된다.

**참새:** [90]에서 Glaese 등은 프롬프트된 언어 모델 기준선에 비해 더 유용하고 정확하며 무해하도록 훈련된 정보 추구 대화 에이전트인 Sparrow를 제시했습니다. 그들은 인간 피드백에서 강화 학습을 사용하여 인간 평가자가 에이전트 행동을 판단하는 데 도움이 되도록 두 가지 새로운 추가 기능으로 모델을 훈련했다. Sparrow 모델의 상위 파이프라인은 그림 21에 나와 있다.

**Minerva:** [91]에서 Lewkowycz 등은 일반 자연어 데이터에 대해 사전 훈련되고 기술 콘텐츠에 대해 추가로 훈련된 대규모 언어 모델인 Minerva를 도입하여 정량적 추론(예: 수학, 과학 및 공학 문제 해결)과의 이전 LLM 어려움을 해결했습니다.

**MoD:** [92]에서 Tay 등은 NLP에서 자체 감독을 위한 일반화되고 통합된 관점을 제시했으며 서로 다른 사전 훈련 목표가 서로 캐스팅될 수 있는 방법과 서로 다른 목표 간에 보간하는 방법이 효과적일 수 있음을 보여줍니다. 그들은 다양한 사전 훈련 패러다임을 함께 결합하는 사전 훈련 목표인 Mixture-of-Denoisers(MoD)를 제안했다. 이 프레임워크는 통일 언어 학습(UL2)으로 알려져 있습니다. UL2 사전 훈련 패러다임의 개요는 그림 21에 나와 있다.

**BLOOM:** [93]에서 Scao 등은 수백 명의 연구자의 협력 덕분에 설계 및 빌드된 176B 매개 변수 오픈 액세스 언어 모델인 BLOOM을 제시했습니다. BLOOM은 ROOTS 코퍼스에서 훈련된 디코더 전용 트랜스포머 언어 모델로서, 46개의 자연어 및 13개의 프로그래밍 언어(총 59개)로 수백 개의 소스를 포함하는 데이터세트이다. BLOOM 아키텍처의 개요는 그림 23에 나와 있다.

**GLM:** [94]에서 Zeng 등은 GLM-130B를 소개했습니다.

도. 21: 참새 파이프라인은 훈련 세트를 지속적으로 확장하기 위해 인간 참여에 의존한다. [90]의 예절.

도. 22: UL2 사전 트레이닝 패러다임의 개요. [92]의 예절.

도. 23: BLOOM 아키텍처의 개요. [93]의 예의.

1,300억 개의 매개변수를 가진 이중언어(영어 및 중국어) 사전 훈련 언어 모델. 적어도 GPT-3(다빈치)만큼 좋은 100B 규모 모델을 오픈소스화하고, 이러한 규모의 모델이 어떻게 성공적으로 사전 훈련될 수 있는지를 밝히려는 시도였다.

**Pythia:** [95]에서 Biderman 등은 모두 70M에서 12B 매개 변수 크기 범위의 동일한 순서로 볼 수 있는 공용 데이터에 대해 훈련된 16개의 LLM 제품군인 Pythia를 소개했습니다. 우리는 추가 연구를 위해 정확한 교육 데이터 로더를 다운로드하고 재구성하는 도구와 함께 16개 모델 중 하나에 대해 154개의 체크포인트에 대한 공개 액세스를 제공한다.

**Orca:** [96]에서 Mukherjee 등은 큰 기초 모델의 추론 프로세스를 모방 하는 130억 매개 변수 모델인 Orca를 개발 합니다. Orca는 GPT-4의 풍부한 신호로부터 설명 흔적, 단계별 사고 과정, 그리고 ChatGPT의 교사 도움에 의해 안내된 다른 복잡한 지시를 배운다.

**StarCoder:** [97]에서 Li 등은 StarCoder와 StarCoderBase를 소개하였다. 이 모델은 8K 컨텍스트 길이, 채우기 기능 및 다중 쿼리 주의에 의해 가능한 빠른 대용량 추론을 가진 15.5B 매개변수 모델이다. StarCoderBase는 검사 도구 및 옵트 아웃 프로세스를 사용 하 여 허용 가능한 라이선스 GitHub 리포지토리의 대규모 컬렉션인 더 스택에서 공급 되는 1 조 토큰에 대해 훈련 됩니다. 그들은 35B 파이썬 토큰에 스타코더베이스를 미세 조정하여 스타코더를 만들었다. 그들은 현재까지 코드 LLM에 대한 가장 포괄적인 평가를 수행했으며 스타코더베이스가 여러 프로그래밍 언어를 지원하고 OpenAI 코드-쿠슈만-001 모델과 일치하거나 성능이 우수한 모든 개방형 코드 LLM보다 우수함을 보여주었다.

**Kosmos:** [98]에서 Huang 등은 일반 모달리티를 인식하고, 컨텍스트에서 학습(즉, few-shot)하고, 지침을 따를 수 있는 MLLM(Multimodal Large Language Model)인 KOSMOS-1을 소개했습니다. 구체적으로, 그들은 임의로 인터리브된 텍스트 및 이미지, 이미지-캡션 쌍 및 텍스트 데이터를 포함하는 웹 규모 멀티 모달 말뭉치에서 KOSMOS-1을 처음부터 훈련했다. 실험 결과 KOSMOS-1은 (i) 언어 이해, 생성, 심지어 OCR이 없는 NLP(문서 이미지를 직접 공급), (ii) 멀티모달 대화, 이미지 캡셔닝, 시각적 질의 응답, (iii) 기술(텍스트 명령어를 통한 분류 지정)을 포함하는 이미지 인식과 같은 인식 언어 태스크에서 인상적인 성능을 달성했다.

**Gemini:** [99]에서 Gemini 팀은 이미지, 오디오, 비디오 및 텍스트 이해에 걸쳐 유망한 기능을 나타내는 새로운 멀티모달 모델 패밀리를 도입했습니다. 제미니 패밀리는 매우 복잡한 작업을 위한 울트라, 규모에서 향상된 성능과 배치 가능성을 위한 프로, 온 디바이스 애플리케이션을 위한 나노의 세 가지 버전이 있습니다. 제미니 아키텍처는 트랜스포머 디코더의 상부에 구축되며, (효율적인 주의 메커니즘을 사용하여) 32k 컨텍스트 길이를 지원하도록 훈련된다.

다른 인기 있는 LLM 프레임워크(또는 LLM의 효율적인 개발을 위해 사용되는 기술) 중 일부는 Inner-Monologue[100], Megatron-Turing NLG[101], LongFormer[102], OPT-IML[103], MeTaLM[104], Dromedary[105], Palmyra[106], Camel[107], Yalm[108], MPT[109], ORCA-2[110], Gorilla[67], PAL[111], Claude[112], CodeGen2[113], Zephyr[114], Groke[115], Qwen[116], Mamba[30], Mistral-8x7B[117], DocLLM[118], DeepSeek-Coder[119], FuseLLM-7B[120], TinyLlama-1.1B[121], LLaMA-Pro-8B[122]를 포함한다.

그림 24는 가장 대표적인 LLM 프레임워크 중 일부와 LLM의 성공에 기여하고 LLM의 한계를 극복하는 데 도움이 된 관련 작업에 대한 개요를 제공한다.

## III LLMs is built

이 섹션에서는 먼저 LLM에 사용되는 인기 있는 아키텍처를 검토하고 데이터 준비, 토큰화, 사전 훈련, 명령어 튜닝 및 정렬에 이르는 데이터 및 모델링 기술에 대해 논의한다.

모델 아키텍처가 선택되면, LLM을 트레이닝하는 것과 관련된 주요 단계는 데이터 준비(수집, 클리닝, 디도핑 등), 토큰화, 모델 사전 트레이닝(자기 지도 학습 방식으로), 명령어 튜닝 및 정렬을 포함한다. 우리는 아래 별도의 세부 섹션에서 그들 각각에 대해 설명할 것입니다. 이들 단계들은 또한 도 25에 예시되어 있다.

### _Dominant LLM Architectures_

가장 널리 사용되는 LLM 아키텍처는 인코더 전용, 디코더 전용, 및 인코더-디코더이다. 대부분 트랜스포머를 기반으로 합니다. 따라서 본 논문에서는 트랜스포머 구조를 살펴본다.

#### Iii-A1 **Transformer**

[44] 획기적인 작업에서 Vaswani 등은 원래 GPU를 사용하여 효과적인 병렬 컴퓨팅을 위해 설계된 트랜스포머 프레임워크를 제안했다. 트랜스포머의 핵심은 (self-attention) 메커니즘으로, GPU를 사용하여 반복 및 컨볼루션 메커니즘보다 훨씬 더 효과적으로 장기 컨텍스트 정보를 캡처할 수 있다. 그림 26은 변압기 작업에 대한 고수준의 개요를 제공한다. 이 섹션에서는 주요 요소 및 변형에 대한 개요를 제공하고 자세한 내용은 [44, 123]을 참조하십시오.

원래 기계 번역을 위해 제안된 Transformer 언어 모델 아키텍처는 인코더와 디코더로 구성된다. 인코더는 N=6개의 동일한 트랜스포머 층들의 스택으로 구성된다. 각 레이어에는 두 개의 하위 레이어가 있습니다. 첫 번째는 다중 헤드 자기 주의 계층이고, 다른 하나는 간단한 위치-와이즈 완전 연결 피드-포워드 네트워크이다. 디코더는 6개의 동일한 층들의 스택으로 구성된다. 각각의 인코더 계층 내의 2개의 서브-계층들에 더하여, 디코더는 제3 서브-계층을 가지며, 이는 인코더 스택의 출력에 걸쳐 멀티-헤드 어텐션을 수행한다. 어텐션 함수는 쿼리 및 키-값 쌍의 세트를 출력에 매핑하는 것으로 설명될 수 있으며, 여기서 쿼리, 키, 값 및 출력은 모두 벡터이다. 출력은 값들의 가중 합으로서 계산되며, 여기서 각각의 값에 할당된 가중치는 대응하는 키와의 질의의 호환성 함수에 의해 계산된다. \(d_{model}\) 차원 키, 값 및 쿼리로 단일 주의 함수를 수행하는 대신, \(d_{k}\), \(d_{k}\) 및 \(d_{v}\) 차원으로 각각 다른 학습 선형 프로젝션을 가진 쿼리, 키 및 값 \(h\)을 선형 프로젝션하는 것이 유익한 것으로 나타났다. 포지션 인코딩은 시퀀스에서 토큰들의 상대적 또는 절대적 위치에 관한 정보를 융합하기 위해 통합된다.

#### Vi-A2 **Encoder-Only**

이 가족의 경우, 각 단계에서, 주의 계층은 초기 문장의 모든 단어에 액세스할 수 있다. 이러한 모델들의 사전 트레이닝은 보통 주어진 문장을 어떻게든 손상시키는 것(예를 들어, 그 안에 랜덤 단어들을 마스킹하는 것) 및 초기 문장을 찾거나 재구성하는 것으로 모델을 태스크화하는 것으로 구성된다. 인코더 모델은 문장 분류, 명명된 개체 인식 및 추출 질문 응답과 같은 전체 시퀀스에 대한 이해가 필요한 작업에 적합하다. 하나의 두드러진 인코더 전용 모델은 [24]에서 제안된 BERT(Bidirectional Encoder Representations from Transformers)이다.

#### Vi-A3 **Decoder-Only**

이러한 모델들의 경우, 각 단계에서, 임의의 단어에 대해, 주의 계층들은 단지 문장에서 그 이전에 위치된 단어들에 액세스할 수 있다. 이러한 모델은 자동 회귀 모델이라고도 합니다. 이러한 모델의 사전 훈련은 일반적으로 시퀀스에서 다음 단어(또는 토큰)를 예측하는 것으로 공식화된다. 디코더 전용 모델은 텍스트 생성과 관련된 작업에 가장 적합합니다. GPT 모델은 이 모델 범주의 두드러진 예이다.

#### Vi-A4 **Encoder-Decoder**

이러한 모델들은 인코더와 디코더를 모두 사용하며, 때때로 시퀀스-투-시퀀스 모델이라고 불린다. 각각의 스테이지에서, 인코더의 어텐션 계층들은 초기 문장 내의 모든 단어들에 액세스할 수 있는 반면, 디코더의 어텐션 계층들은 입력 내의 주어진 단어 앞에 위치된 단어들에만 액세스한다. 이러한 모델들은 일반적으로 인코더 또는 디코더 모델들의 목적들을 사용하여 사전 트레이닝되지만, 일반적으로 약간 더 복잡한 것을 수반한다. 예를 들어, 일부 모델들은 (여러 단어들을 포함할 수 있는) 텍스트의 랜덤 스팬들을 단일 마스크 특수 단어로 대체함으로써 사전 트레이닝되고, 그 다음 목적은 이 마스크 단어가 대체하는 텍스트를 예측하는 것이다. 인코더-디코더 모델은 요약, 번역 또는 생성 질문 응답과 같은 주어진 입력에 조건화된 새로운 문장을 생성하는 작업에 가장 적합하다.

### _Data Cleaning_

데이터 품질은 학습한 언어 모델의 성능에 매우 중요합니다. 필터링, 중복제거와 같은 데이터 클리닝 기법은 모델 성능에 큰 영향을 미치는 것으로 나타났다.

예를 들어, **Falcon40B**[124]에서 Penedo 등은 적절하게 필터링되고 중복 제거된 웹 데이터만으로도 강력한 모델로 이어질 수 있음을 보여주었으며, The Pile에서 훈련된 최신 모델에서 모델을 훨씬 능가합니다. 광범위한 필터링에도 불구하고, 그들은 커먼크롤로부터 5조 개의 토큰을 얻을 수 있었다. 그들은 또한 REFINEDWEB 데이터 세트에서 6,000억 개의 토큰 추출과 이에 대해 훈련된 1.3/7.5B 매개변수 언어 모델을 출시했다. 도 27은 이 작업에 의한 CommonCrawl 데이터의 정제 프로세스를 나타낸다.

#### Vi-B1 Data Filtering

데이터 필터링은 훈련 데이터의 품질과 훈련된 LLM의 효율성을 향상시키는 것을 목표로 한다. 공통 데이터 필터링 기술은 다음을 포함한다:

**잡음 제거:** 모델의 일반화 능력에 영향을 줄 수 있는 관련 없거나 노이즈가 있는 데이터를 제거하는 것을 말합니다. 예를 들어, 모델이 잘못된 응답을 생성할 가능성을 낮추기 위해 훈련 데이터에서 잘못된 정보를 제거하는 것을 생각할 수 있다. 품질 필터링을 위한 두 가지 주류 접근법은 분류기 기반 프레임워크와 휴리스틱 기반 프레임워크를 포함한다.

도. 24: 지금까지의 가장 대표적인 LLM 프레임워크들 중 일부의 타임라인. 우리의 #파라미터 임계치를 갖는 큰 언어 모델 외에도, 우리는 언어 모델의 한계를 밀어붙이고 그들의 성공을 위한 길을 열어준 몇 가지 대표적인 작업들(예: 바닐라 트랜스포머, BERT, GPT-1)과 몇몇 작은 언어 모델들)을 포함시켰다. \ (\clubsuit\)는 모델뿐만 아니라 접근 방식으로도 역할을 하는 엔터티를 보여 줍니다. \ (\clubsuit\)는 접근 방식만 보여 줍니다.

[MISSING_PAGE_POST]

garwal

**이상값 처리:** 데이터의 이상값 또는 이상값을 식별 하 고 처리 하 여 모델에 불균형적으로 영향을 주지 않도록 합니다.

**불균형 해결:** 편향을 방지하고 공정한 표현을 보장하기 위해 데이터 집합에서 클래스 또는 범주의 분포를 균등화합니다. 책임 있는 모델 교육 및 평가에 특히 유용합니다.

**텍스트 전처리:** 모델의 학습에 크게 기여 하지 않을 수 있는 불용어, 구두점 또는 기타 요소를 제거 하 여 텍스트 데이터를 정리 하 고 표준화 합니다.

**모호성 처리:** 학습 중에 모델을 혼동할 수 있는 모호하거나 모순되는 데이터를 해결하거나 제외합니다. 이는 모델이 보다 확실하고 신뢰할 수 있는 답변을 제공하는 데 도움이 될 수 있다.

#### V-B2 Deduplication

중복 제거는 데이터셋에서 중복된 인스턴스 또는 동일한 데이터가 반복적으로 발생하는 것을 제거하는 과정을 의미한다. 중복 데이터 포인트는 모델이 동일한 예로부터 여러 번 학습하여 잠재적으로 그러한 특정 인스턴스에 과적합으로 이어질 수 있기 때문에 모델 학습 프로세스에서 편향을 도입하고 다양성을 감소시킬 수 있다. 일부 작업[125]은 중복 제거가 새로운 보이지 않는 데이터로 일반화하는 모델의 능력을 향상시킨다는 것을 보여주었다.

중복은 의도하지 않게 특정 패턴 또는 특성의 중요성을 부풀릴 수 있기 때문에, 대규모 데이터 세트를 다룰 때 중복 제거 프로세스가 특히 중요하다. 이것은 특히 다양하고 대표적인 훈련 데이터가 강력한 언어 모델을 구축하는 데 중요한 NLP 작업에서 관련이 있다.

특정 중복 제거 방법은 데이터의 특성 및 트레이닝되는 특정 언어 모델의 요구 사항에 따라 달라질 수 있다. 중복을 식별하고 제거하기 위해 전체 데이터 포인트 또는 특정 특징을 비교하는 것을 포함할 수 있다. 문서 수준에서 기존 작업은 중복 샘플을 탐지하기 위해 문서 간의 높은 수준의 특징(예: n-grams overlap)의 중첩 비율에 주로 의존한다.

### _Tokenizations_

토큰화는 텍스트의 시퀀스를 토큰으로 알려진 더 작은 부분으로 변환하는 과정을 참조한다. 가장 간단한 토큰화 툴은 단순히 텍스트를 화이트 스페이스를 기반으로 토큰으로 자르는 반면, 대부분의 토큰화 툴은 단어 사전(word dictionary)에 의존한다. 그러나, OOV(out-of-vocabulary)는 토큰라이저가 자신의 사전에 있는 단어만을 알기 때문에 이 경우에 문제가 된다. 사전의 커버리지를 증가시키기 위해, LLM에 사용되는 인기 있는 토큰라이저는 서브-단어를 기반으로 하며, 이는 트레이닝 데이터에서 보이지 않는 단어 또는 상이한 언어의 단어를 포함하여 많은 수의 단어를 형성하기 위해 결합될 수 있다. 다음에서는 세 가지 인기 있는 토큰라이저를 설명한다.

#### V-C1 **BytePairEncoding**

_BytePairEncoding_은 원래 바이트 레벨에서 빈발 패턴을 사용하여 데이터를 압축하는 데이터 압축 알고리즘의 한 종류이다. 정의에 따르면, 이 알고리즘은 주로 빈발 단어를 원래 형태로 유지하고 일반적이지 않은 단어를 분해하려고 한다. 이 단순한 패러다임은 어휘를 매우 크게 유지하지 않을 뿐만 아니라 공통 단어를 동시에 나타낼 수 있을 만큼 훌륭하다. 또한, 알고리즘의 학습 데이터에 접미사 또는 접두사가 공통으로 제시된다면 빈발 단어의 형태소 형태가 매우 잘 표현될 수 있다.

#### V-C2 **WordPieceEncoding**

이 알고리즘은 주로 BERT 및 Electra와 같은 매우 잘 알려진 모델에 사용된다. 학습 시작 시 알고리즘은 학습 데이터에서 모든 알파벳을 가져와서 학습 데이터 집합에서 UNK 또는 _unknown_로 남아 있는 것이 없는지 확인합니다. 이 경우는 모델에 토큰화기에서 토큰화할 수 없는 입력이 주어졌을 때 발생한다. 대부분 일부 문자가 토큰화할 수 없는 경우에 발생합니다. BytePairEncoding과 유사하게 빈도에 따라 모든 토큰을 어휘에 넣을 가능성을 최대화하려고 한다.

#### V-C3 **SentencePieceEncoding**

앞서 설명된 두 토큰라이저는 모두 강력하고 화이트-스페이스 토큰화에 비해 많은 장점을 갖지만, 여전히 워드가 항상 화이트-스페이스에 의해 분리된다는 가정은 부여된 바와 같다. 이러한 가정이 항상 사실인 것은 아니며, 실제로 일부 언어에서, 단어는 원치 않는 공간 또는 심지어 발명된 단어와 같은 많은 시끄러운 요소에 의해 손상될 수 있다. SentencePieceEncoding은 이 문제를 해결하기 위해 노력합니다.

도. 27: Macrodata Refinement의 후속 단계들은 원래 CommonCrawl에 있는 문서들의 거의 90%를 제거한다. [124]의 예의.

도. 26: 변압기 작업에 대한 상위 레벨 개요. [44]의 예절.

### **Positional Encoding**

#### Iii-D1 **절대 위치 임베딩**

(APE)[44]는 시퀀스 순서의 정보를 보존하기 위해 원래의 Transformer 모델에서 사용되어 왔다. 그러므로, 단어들의 위치 정보는 인코더 및 디코더 스택들 모두의 하단의 입력 임베딩들에 부가된다. 위치 인코딩에는 학습되거나 고정된 다양한 옵션이 있습니다. 바닐라 트랜스포머에서는 사인 및 코사인 함수를 사용하였다. 트랜스포머에서 APE를 사용하는 주요 단점은 특정 수의 토큰에 대한 제한이다. 또한 APE는 토큰 간의 상대적 거리를 설명하지 못합니다.

#### Iii-D2 **상대 위치 임베딩**

(RPE)[126]은 입력 요소들 사이의 쌍별 링크들을 고려하기 위해 자기-주의를 확장하는 것을 포함한다. RPE는 먼저 키에 대한 추가 구성요소로, 이후 값 행렬의 하위 구성요소로 두 가지 수준에서 모델에 추가된다. 이 접근법은 입력을 라벨 및 방향 에지를 갖는 완전-연결된 그래프로서 본다. 선형 시퀀스들의 경우, 에지들은 입력 엘리먼트들 간의 상대적인 위치 차이들에 관한 정보를 캡처할 수 있다. k \(2\leq k\leq n-4\)으로 표현되는 클리핑 거리는 상대 위치에 대한 최대 제한을 지정한다. 이를 통해 모델은 훈련 데이터의 일부가 아닌 시퀀스 길이에 대해 합리적인 예측을 할 수 있다.

#### Iii-D3 **회전 위치 임베딩**

로터리 포지셔널 임베딩(RoPE)[127]은 기존의 접근법들의 문제들을 다룬다. 학습된 절대적 위치 부호화는 특히 문장이 짧을 때 일반화 가능성과 의미성이 결여될 수 있다. 더욱이, T5의 위치 임베딩과 같은 현재의 방법들은 위치들 사이에 완전한 주의 매트릭스를 구성하는 것에 도전한다. RoPE는 단어의 절대 위치를 인코딩하기 위해 회전 행렬을 사용하고 동시에 자기 주의에서 명시적인 상대 위치 세부 사항을 포함한다. RoPE는 문장 길이의 유연성, 상대 거리가 증가함에 따라 단어 의존성의 감소, 상대 위치 인코딩으로 선형 자기 주의력을 향상시키는 능력과 같은 유용한 특징을 가져온다. GPT-NeoX-20B, PaLM, CODEGEN 및 LLaMA는 아키텍처에서 RoPE를 활용하는 모델 중 하나이다.

#### Iii-D4 **상대 위치 편향**

이러한 유형의 위치 임베딩 이면의 개념은 훈련에서 마주치는 것보다 긴 시퀀스에 대한 추론 동안 외삽을 용이하게 하기 위한 것이다. [128] Press 등에서는 Linear Biases(ALiBi)를 이용한 Attention을 제안하였다. 단순히 단어 임베딩에 위치 임베딩을 추가하는 대신 질의-키 쌍의 주의 점수에 편향을 도입하여 거리에 비례하는 페널티를 부과했다. BLOOM 모델에서는 ALiBi를 활용합니다.

### _Model Pre-training_

사전 훈련은 대규모 언어 모델 훈련 파이프라인의 첫 단계이며 LLM이 기본적인 언어 이해 능력을 습득하는 데 도움이 되며, 이는 광범위한 언어 관련 작업에 유용할 수 있다. 사전 훈련 동안, LLM은 일반적으로 자체 감독 방식으로 대량의 (통상적으로) 라벨링되지 않은 텍스트에 대해 훈련된다. 다음 문장 예측[24]과 같이 사전 훈련에 사용되는 다른 접근법이 있으며, 가장 일반적인 두 가지 방법은 다음 토큰 예측(자동 회귀 언어 모델링) 및 마스킹 언어 모델링이다.

**자기 회귀 언어 모델링** 프레임워크에서 \(n\) 토큰 \(x_{1}\),..., \(x_{n}\)의 시퀀스가 주어지면 모델은 자동 회귀 방식으로 다음 토큰 \(x_{n+1}\)(그리고 때로는 다음 토큰 시퀀스)을 예측하려고 합니다. 이 경우의 하나의 인기 있는 손실 함수는 Eq2에 도시된 바와 같이 예측된 토큰의 로그-우도이다.

\[\mathscr{L}_{ALM}(x)=\sum_{i=1}^{N}p(x_{i+n}|x_{i},...,x_{i+n-1}) \tag{1}\]

이 프레임워크의 자동 회귀 특성을 감안할 때 디코더 전용 모델은 자연스럽게 이러한 작업을 수행하는 방법을 배우는 데 더 적합하다.

**마스크된 언어 모델링** 에서 일부 단어는 시퀀스로 마스킹되고 모델은 주변 컨텍스트를 기반으로 마스킹된 단어를 예측하도록 학습됩니다. 때때로 사람들은 이 접근 방식을 노이즈 제거 자동 인코딩이라고도 합니다. 순서 \(x\), \(\tilde{x}\)에서 마스킹 된/파손 된 샘플을 표시 하는 경우이 접근 방식의 훈련 목표는 다음과 같이 작성 될 수 있습니다.

\[\mathscr{L}_{MLM}(x)=\sum_{i=1}^{N}p(\tilde{x}|x\backslash\tilde{x}) \tag{2}\]

그리고 최근에는 **MoE(Mixture of Experts)**[130, 131]가 LLM 공간에서도 매우 유명해졌습니다. MoE는 훨씬 적은 계산으로 모델을 사전 훈련할 수 있도록 하며, 이는 밀도가 높은 모델과 동일한 계산 예산으로 모델 또는 데이터 세트 크기를 극적으로 확장할 수 있음을 의미합니다. MoE는 두 가지 주요 요소로 구성 됩니다. **Sparse MoE 계층** 은 고밀도 피드 포워드 네트워크 (FFN) 계층 대신 사용 되며 각 전문가가 신경망인 특정 수의 "전문가" (예: 8)를 포함 합니다. 실제로 전문가들은 FFN이지만 더 복잡한 네트워크일 수도 있다. **게이트 네트워크 또는 라우터** 는 어떤 토큰이 어떤 전문가에게 전송되는지 결정합니다. 한 명 이상의 전문가에게 토큰을 보낼 수 있다는 점은 주목할 가치가 있습니다. 토큰을 전문가에게 라우팅하는 방법은 MoE를 사용하여 작업할 때 큰 결정 중 하나입니다. 라우터는 학습된 매개 변수로 구성되고 네트워크의 나머지 부분과 동시에 사전 훈련됩니다. 도 29는 MoE에서 사용되는 스위치 트랜스포머 인코더 블록의 예시를 제공한다.

### _Fine-tuning and Instruction Tuning_

섹션 III-E에서 설명한 대로 자체 감독을 사용하여 훈련된 BERT와 같은 초기 언어 모델은 특정 작업을 수행할 수 없었다. 기초 모델이 유용하기 위해서는 레이블이 지정된 데이터(이른바 감독 미세 조정 또는 짧은 경우 SFT)를 사용하여 특정 작업에 미세 조정해야 했다. 예를 들어, 원래 BERT 논문[24]에서 모델은 11개의 다른 작업으로 미세 조정되었다. 보다 최근의 LLM은 더 이상 미세 조정을 사용할 필요가 없지만 작업 또는 데이터별 미세 조정으로부터 여전히 이익을 얻을 수 있다. 예를 들어, OpenAI는 태스크 특정 데이터 2로 미세 조정될 때 훨씬 더 작은 GPT-3.5 터보 모델이 GPT-4를 능가할 수 있다고 보고한다.

각주 2: [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)

파인-튜닝은 단일 태스크에 대해 수행될 필요는 없지만, 멀티-태스크 파인-튜닝에 대한 상이한 접근법들이 존재한다(예를 들어, Mahabi 등 [132] 참조). 하나 이상의 작업에 대한 미세 조정은 결과를 개선하고 신속한 엔지니어링의 복잡성을 줄이는 것으로 알려져 있으며 검색 증강 생성에 대한 분석 역할을 할 수 있다. 또한 미세 조정하는 것이 바람직할 수 있는 다른 이유가 있습니다. 예를 들어 사전 교육 중에 노출되지 않은 새 데이터 또는 독점 데이터에 모델을 노출하도록 미세 조정하고 싶을 수 있습니다.

LLM을 미세 조정하는 중요한 이유는 프롬프트를 통해 지침을 제공할 때 인간이 가질 기대에 대한 응답을 정렬하기 때문이다. 이것은 소위 **명령 조정**[133]입니다. 섹션 IV-B에서 프롬프트를 설계하고 설계하는 방법에 대한 세부 사항을 살펴보지만, 명령어 튜닝의 맥락에서 명령어가 LLM이 수행해야 하는 작업을 지정하는 프롬프트라는 것을 이해하는 것이 중요하다. 자연 지침 [134]와 같은 지침 조정 데이터 세트에는 작업 정의뿐만 아니라 긍정/부정 예제와 같은 다른 구성 요소 또는 피해야 할 사항이 포함 됩니다.

LLM을 명령어 튜닝하는 데 사용되는 특정 접근법 및 명령어 데이터 세트는 다양하지만, 일반적으로 말하면, 명령어 튜닝 모델은 그들이 기반으로 하는 원래 기초 모델보다 성능이 우수하다. 예를 들어, InstructGPT[59]는 대부분의 벤치마크에서 GPT-3보다 성능이 우수하다. LLMA와 비교할 때 알파카[62]에 대해서도 마찬가지이다.

W Wang 등이 제안한 **Self-Instruct**[135]는 또한 자신의 세대를 부트스트랩하여 사전 훈련된 언어 모델의 명령어 추적 기능을 개선하기 위한 프레임워크를 도입한 이 라인을 따라 널리 사용되는 접근 방식입니다. 그들의 파이프라인은 언어 모델에서 명령, 입력 및 출력 샘플을 생성한 다음 잘못된 또는 유사한 샘플을 필터링한 다음 원래 모델을 미세 조정하기 위해 사용한다.

### _Alignment_

AI 정렬은 AI 시스템을 인간의 목표, 선호도 및 원칙으로 조정하는 프로세스입니다. 단어 예측을 위해 미리 훈련된 LMM은 종종 의도하지 않은 행동을 보인다. 예를 들어, 독성, 유해성, 오해의 소지가 있고 편향된 콘텐츠를 생성할 수 있습니다.

위에서 논의된 명령어 튜닝은 LLM들이 정렬되는 것에 한 걸음 더 가까워지게 한다. 그러나 많은 경우에 모델의 정렬을 개선하고 의도하지 않은 동작을 피하기 위해 추가 단계를 포함하는 것이 중요하다. 3. 가장 인기 있는 방법을 검토한다.

도. 28: LLM에는 다양한 위치 인코딩이 채용된다.

도. 29 : 스위치 트랜스포머 인코더 블록의 예시. 이들은 트랜스포머에 존재하는 조밀한 피드 포워드 네트워크(FFN) 층을 희박한 스위치 FFN 층(가벼운 청색)으로 대체하였다. [131]의 예절.

이 하위 섹션의 정렬에 대한 접근입니다.

**RLHF** (인간 피드백에서 강화 학습) 및 **RLAIF** (AI 피드백에서 강화 학습)는 두 가지 인기 있는 접근 방식입니다. RLHF는 보상 모델을 사용하여 인간의 피드백으로부터 정렬을 학습한다. 이 보상 모델은, 튜닝된 후에, 상이한 출력들을 평가할 수 있고, 인간에 의해 주어진 그들의 정렬 선호도에 따라 점수를 매길 수 있다. 보상 모델은 원래의 LLM에 피드백을 제공하고, 이 피드백은 LLM을 추가로 튜닝하는 데 사용된다[137]. 반면, AI 피드백으로부터의 강화 학습은 사전 훈련되고 잘 정렬된 모델을 LLM에 직접 연결하고 더 크고 더 정렬된 모델로부터 학습하도록 돕는다[138].

또 다른 최근 작업(**DPO**로 알려져 있음)[139]에서 Rafailov 등은 RLHF가 복잡하고 종종 불안정한 절차라고 논의했으며 새로운 접근법으로 이를 해결하려고 했다. 그들은 보상 함수와 최적 정책 간의 매핑을 활용하여 이 제한된 보상 최대화 문제가 단일 단계의 정책 훈련으로 정확하게 최적화될 수 있음을 보여주었으며, 본질적으로 인간 선호도 데이터에 대한 분류 문제를 해결했다. 직접 선호도 최적화(DPO)라고 하는 결과 알고리즘은 안정적이고 수행적이며 계산량이 가벼워 보상 모델을 피팅하거나 미세 조정 동안 LM에서 샘플링하거나 상당한 하이퍼파라미터 조정을 수행할 필요가 없다. 그들은 DPO를 사용한 미세 조정이 세대의 감성을 제어하는 RLHF의 능력을 초과하고 요약에서 응답 품질을 향상시킨다는 것을 관찰했다. 그림 30은 DPO와 RLHF 간의 고수준의 비교를 보여준다.

더욱 최근에 Ethayarajh 등은 Kahneman-Tversky Optimization(KTO)이라는 새로운 정렬 접근법을 제안했다[136]. KTO는 기존의 최신 접근 방식과 달리 쌍방향 선호도 데이터(\(x\), \(y_{w}\), \(y_{l}\))를 필요로 하지 않으며, \(y\)이 바람직한지 바람직하지 않은지에 대한 (x,y)와 지식만을 필요로 한다. KTO 정렬 모델은 쌍을 이루는 선호도를 사용하지 않았음에도 불구하고 1B에서 30B까지의 규모에서 DPO 정렬 모델보다 우수하거나 더 나은 것으로 나타났다. KTO는 또한 필요한 데이터의 종류가 훨씬 풍부하기 때문에 선호도 최적화 방법보다 실제 세계에서 사용하기 훨씬 쉽다. 예로서, 모든 소매 회사는 많은 고객 상호 작용 데이터 및 그 상호 작용이 성공적이었는지(예를 들어, 구매가 이루어졌는지) 또는 성공하지 않았는지(예를 들어, 구매가 이루어지지 않았는지)를 갖는다. 그러나, 고객 상호작용 \(y_{l}\)이 실패한 고객 상호작용 \(y_{w}\)을 성공적인 고객 상호작용 \(y_{w}\)으로 만드는 것과 같은 반사실적 데이터는 거의 또는 전혀 없다. 그림 31은 위에서 논의한 KTO와 다른 정렬 접근법 간의 고수준의 비교를 보여준다.

### _Decoding Strategies_

디코딩은 미리 훈련된 LLM들을 이용하여 텍스트가 생성되는 과정을 의미한다. 입력 프롬프트가 주어지면, 토큰라이저는 입력 텍스트 내의 각각의 토큰을 대응하는 토큰 ID로 변환한다. 그런 다음, 언어 모델은 이러한 토큰 ID들을 입력으로 사용하고 다음으로 가능성이 높은 토큰(또는 토큰의 시퀀스)을 예측한다. 마지막으로, 모델은 로짓들을 생성하고, 이 로짓들은 소프트맥스 함수를 사용하여 확률들로 변환된다. 상이한 디코딩 전략들이 제안되었다. 가장 인기 있는 것들 중 일부는 탐욕 탐색, 빔 탐색뿐만 아니라 top-K, top-P(Nucleus sampling)와 같은 상이한 샘플 기법이다.

#### V-H1 **탐욕 검색**

탐욕 검색은 각 단계에서 가장 가능성이 높은 토큰을 시퀀스의 다음 토큰으로 사용하여 다른 모든 잠재적 옵션을 폐기합니다. 상상할 수 있듯이, 이것은 단순한 접근법이며 많은 시간적 일관성과 일관성을 잃을 수 있다. 순서에 대한 전반적인 영향을 고려하지 않고 각 단계에서 가장 가능성 있는 토큰만을 고려한다. 이 속성은 빠르게 만들지만, 그것은 또한 약간 덜 가능성 있는 다음 토큰으로 나타날 수 있는 더 나은 시퀀스를 놓칠 수 있다는 것을 의미한다.

#### V-H2 **빔 검색**

다음 가장 가능성 있는 토큰만 고려하는 그리디 검색과 달리 빔 검색은 **N** 개의 가장 가능성 있는 토큰을 고려하며, 여기서 **N** 은 빔 수를 나타냅니다. 이 절차는 미리 정의된 최대 시퀀스 길이에 도달하거나 시퀀스 종료 토큰이 나타날 때까지 반복됩니다. 이 시점에서, 가장 높은 전체 스코어를 갖는 토큰들의 시퀀스(AKA "빔")가 출력으로서 선택된다. 예를 들어, 빔 크기가 2이고 최대 길이가 5인 경우 빔 탐색은 \(2^{5}=32\) 가능한 시퀀스를 추적할 필요가 있다. 따라서 탐욕스러운 검색보다 계산 집약적입니다.

#### V-H3 **Top-k 샘플링**

Top-k 샘플링은 언어 모델에 의해 생성된 확률 분포를 사용하여 k개의 가장 가능성 있는 옵션에서 토큰을 무작위로 선택하는 기술이다.

우리가 6개의 토큰(A, B, C, D, E, F)과 k=2이고, P(A)=30%, P(B)=20%, P(C)=P(D)=P(E)=P(F)=

도. 31: LLM 정렬은 감독된 피니튜닝 후 인간 중심 손실(HALO)을 최적화하는 것을 포함한다. 그러나, 기존의 접근법들이 필요로 하는 쌍을 이루는 선호도들은 획득하기 어렵다. 대조적으로, KTO는 훨씬 더 풍부한 종류의 데이터를 사용하여 실제 세계에서 훨씬 더 쉽게 사용할 수 있다. [136]의 예의.

도. 30: DPO는 강화 학습을 피하면서 인간의 선호도에 최적화한다. 인간 피드백을 갖는 언어 모델을 미세 조정하는 기존의 방법은 먼저 응답 쌍에 대한 프롬프트 및 인간 선호도의 데이터 세트에 보상 모델을 적합시킨 다음 RL을 사용하여 학습된 보상을 최대화하는 정책을 찾는다. 대조적으로, DPO는 명시적 보상 함수 또는 RL 없이 간단한 분류 목표로 선호도를 가장 잘 충족하는 정책을 직접 최적화한다. [139]의 예절.

12.5% Top-k 샘플링에서 토큰 C, D, E, F는 무시되고 모델은 A의 60%, B의 40%를 출력한다. 이 접근법은 선택 과정에서 무작위성의 요소를 도입하면서 가장 가능성 있는 토큰의 우선순위를 매기는 것을 보장한다.

무작위성은 보통 온도의 개념을 통해 도입된다. 온도 T는 0에서 1 사이의 파라미터로 소프트맥스 함수에 의해 생성된 확률에 영향을 주어 가장 가능성이 높은 토큰이 더 영향력 있게 된다. 실제로는 단순히 입력 로짓들을 온도 값으로 나누는 것으로 구성된다:

\[softmax(x_{i})=\frac{e^{x_{i}/T}}{\sum_{j}e^{x_{j}/T}} \tag{3}\]

낮은 온도 설정은 확률 분포를 상당히 변경하는 반면(그리고 생성된 출력에서 "창의성"의 레벨을 제어하기 위해 텍스트 생성에서 일반적으로 사용됨), 큰 온도는 더 높은 확률로 토큰을 우선한다. Top-k는 창의적인 샘플링 방식이며 빔 탐색과 함께 사용할 수 있다. 탑-k 샘플링에 의해 선택된 시퀀스는 빔 탐색에서 가장 높은 확률을 갖는 시퀀스가 아닐 수 있다. 그러나 가장 높은 점수가 항상 더 사실적이거나 의미 있는 시퀀스로 이어지는 것은 아니라는 것을 기억하는 것이 중요하다.

#### V-B4 **Top-p 샘플링**

핵 샘플링이라고도 하는 Top-p 샘플링은 Top-k 샘플링과 약간 다른 접근법을 취한다. 상위 k개의 가장 가능성 있는 토큰들을 선택하는 대신에, 핵 샘플링은 선택된 토큰들의 확률들의 합이 p를 초과하도록 컷오프 값 p를 선택한다. 이것은 다음 토큰을 랜덤하게 선택할 토큰들의 "핵"을 형성한다. 다시 말해, top-p 샘플링에서 언어 모델은 내림차순으로 가장 가능성 있는 토큰들을 조사하고 확률들의 합이 임계치 p를 초과할 때까지 그것들을 목록에 계속 추가한다. 당신이 상상할 수 있는 바와 같이, 이것은 특히 top-k 토큰들이 큰 확률 질량을 갖지 않는 시나리오들에 대해 더 좋을 수 있다. Top-k 샘플링과 달리 핵 샘플링에 포함된 토큰의 수는 고정되어 있지 않다. 이러한 가변성은 종종 더 다양하고 창의적인 출력을 초래하여 핵 샘플링을 텍스트 생성 관련 작업에 인기 있게 만든다.

### _Cost-Effective Training/Inference/Adaptation/Compression_

이 부분에서 우리는 LLM의 보다 비용 친화적(및 계산 친화적) 훈련 및 사용에 사용되는 몇 가지 일반적인 접근법을 검토한다.

#### V-I1 **최적화된 교육**

LLM의 최적화된 훈련을 위해 개발된 많은 프레임워크가 있으며, 여기서는 몇 가지 주요 프레임워크를 소개한다.

**ZeRO:** [140]에서 Rajbhandari 등은 메모리를 최적화 하는 새로운 솔루션 ZeRO (Zero Redundancy Optimizer)를 개발 하 여 효율적으로 훈련할 수 있는 모델 크기를 증가시키면서 LLM의 훈련 속도를 크게 향상시켰습니다. ZeRO는 낮은 통신 볼륨과 높은 계산 입도를 유지하면서 데이터 및 모델 병렬 훈련에서 메모리 중복을 제거하여 지속적인 고효율로 장치 수에 비례하는 모델 크기를 확장할 수 있다.

**RWKV:** [141]에서 Peng 등은 Transformers의 효율적인 병렬화 훈련과 RNN의 효율적인 추론을 결합하는 새로운 모델 아키텍처인 RWKV(Recceptance Weighted Key Value)를 제안했습니다. 그들의 접근법은 선형 주의 메커니즘을 활용하여 모델을 트랜스포머 또는 RNN으로 공식화할 수 있으며, 이는 훈련 동안 계산을 병렬화하고 추론 동안 일정한 계산 및 메모리 복잡성을 유지하여 수백억 개의 매개변수로 확장되는 최초의 비변환기 아키텍처로 이어진다. RWKV 아키텍처는 그림 32에 나와 있다.

다른 변압기를 사용한 RWKV의 시간 복잡도 비교는 그림 33에 나와 있다.

#### V-I2 **Low-Rank Adaption(LoRA)**

Low-Rank Adaptation은 훈련 가능한 파라미터의 수를 상당히 감소시키는 대중적이고 가벼운 훈련 기법이며, 전문 작업에 대한 미세 조정된 가중치와 초기 사전 훈련된 가중치 사이의 차이가 종종 "낮은 고유 순위"를 나타낸다는 중요한 통찰에 기초한다 - 이는 낮은 순위 매트릭스에 의해 잘 근사될 수 있다는 것을 의미한다[142].

도. 33: 다른 트랜스포머를 사용한 RWKV의 시간 복잡도 비교. 여기서 T는 시퀀스 길이, d는 특징 차원, c는 MEGA의 2차 주의 청크 크기이다. [141]의 예의.

도. 32: RWKV 아키텍처. [141]의 예의.

LoRA를 사용한 트레이닝은 훨씬 더 빠르고, 메모리 효율적이며, 더 작은 모델 가중치(수백 MB)를 생성하는데, 이는 저장 및 공유가 더 쉽다. 낮은 순위의 행렬의 한 가지 특성은 두 개의 더 작은 행렬의 곱으로 나타낼 수 있다는 것이다. 이 실현은 미세 조정된 가중치와 초기 사전 훈련된 가중치 사이의 이러한 델타가 두 개의 훨씬 더 작은 행렬의 행렬 곱으로 표현될 수 있다는 가설로 이어진다. 전체 원래의 가중치 행렬이 아닌 이들 두 개의 더 작은 행렬을 업데이트하는 것에 초점을 맞추면, 계산 효율이 실질적으로 향상될 수 있다.

구체적으로, 미리 훈련된 가중치 행렬 \(W_{0}\in R^{d\times k}\)의 경우 LoRA는 낮은 순위 분해 \(W_{0}+\Delta W=\hat{W}_{0}+BA\), 여기서 \(B\in R^{d\times r}\,,\,A\in R^{r\times k}\), 그리고 순위 \(r\ll min(d,k)\)으로 후자를 표현함으로써 갱신을 제한한다. 훈련 중에 \(W_{0}\)은 동결되어 기울기 업데이트를 받지 않는 반면 \(A\) 및 \(B\)에는 훈련 가능한 매개변수가 포함되어 있다. \(W_{0}\)과 \(\Delta W=BA\)은 모두 동일한 입력으로 곱해지고 각각의 출력 벡터는 좌표적으로 합산된다는 것을 언급할 가치가 있다. \(h=W_{0}x\)의 경우 수정된 순방향 통과 수율이 \(h=W_{0}x+\Delta Wx=W_{0}x+BAx\)이다. 일반적으로 랜덤 가우시안 초기화는 \(A\)에, 제로 초기화는 \(B\)에 사용되므로 \(\Delta W=BA\)은 훈련 초기에 0이 된다. 그런 다음 \(\Delta Wx\)를 \(\alpha r\)으로 확장하며, 여기서 \(\alpha\)는 r에서 상수이다. 이러한 재파라메트리징은 도 34에 예시되어 있다

LoRA는 훈련 가능한 파라미터의 수를 줄이기 위해 신경망에서 가중치 행렬의 하위 집합에 적용될 수 있다는 점을 언급할 가치가 있다. 트랜스포머 구조에서는 자기 주의 모듈(\(W_{q}\), \(W_{k}\), \(W_{w}\), \(W_{o}\))에 4개의 가중치 행렬이 있고 MLP 모듈에는 2개의 가중치 행렬이 있다. 대부분의 경우 LoRA는 다운스트림 작업에 대해서만 주의 가중치를 적용하는 데 중점을 두고 MLP 모듈을 동결하므로 단순성과 매개변수 효율성을 위해 다운스트림 작업에서 훈련되지 않는다.

#### Iii-B3 **지식 증류**

지식 증류는 더 큰 모델로부터 학습하는 과정이다[143]. 최적의 성능을 보이는 모델 출시의 초기에는 이 접근법이 API 증류 접근법에 사용되더라도 매우 유용하다는 것이 입증되었다. 단일 모델이 아니라 실제로 여러 모델에 대한 지식을 더 작은 모델로 증류하는 접근법이라고도 한다. 이러한 접근법에 의해 더 작은 모델들을 생성하는 것은 에지 디바이스들에서도 사용될 수 있는 더 작은 모델 크기들을 산출한다. 그림 35와 같은 지식 증류는 이 훈련 계획의 일반적인 설정을 보여준다.

지식은 반응 증류, 특징 증류 및 API 증류와 같은 다양한 형태의 학습에 의해 전달될 수 있다. 반응 증류는 교사 모델의 출력에만 관심을 가지며 학생 모델에 교사로서 (예측의 의미에서) 정확히 또는 적어도 유사하게 수행하는 방법을 가르치려고 한다. 특징 증류는 마지막 층뿐만 아니라 중간 층도 사용하여 학생 모델에 대한 더 나은 내부 표현을 생성한다. 이는 작은 모형이 교사 모형과 유사한 표상을 가질 수 있도록 도와준다.

API 증류는 API (일반적으로 OpenAI와 같은 LLM 공급자)를 사용 하 여 더 작은 모델을 훈련 하는 프로세스입니다. LLM의 경우 더 큰 모델의 직접 출력으로부터 모델을 훈련하는 데 사용되므로 반응 증류와 매우 유사하다. 모델 자체를 공개적으로 사용할 수 없는 경우 최종 사용자를 위해 (일반적으로) 유료 API가 노출되기 때문에 이러한 유형의 증류로 인해 많은 우려가 제기됩니다. 반면에 사용자가 각 호출에 대해 비용을 지불하는 동안 예측을 사용하는 방법은 제한적입니다. 예를 들어 OpenAI는 나중에 경쟁하는 데 사용할 LLM을 만들기 위해 API를 사용하는 것을 금지합니다. 그러한 경우의 주요 값은 트레이닝 데이터이다.

#### Iii-B4 **Quantization**

심층 학습은 행렬에 적용되는 수학적 함수의 집합으로 모델 가중치에 대한 특정 정밀도를 갖는다. 가중치의 정밀도를 줄이는 것은 모델의 크기를 줄이고 또한 더 빠르게 만드는 데 사용될 수 있다. 일 예로서, Int-8 연산에 비해 Float-32 연산은 더 느리다. 양자화라고 불리는 이러한 과정은 서로 다른 위상으로 적용될 수 있다. 모델 양자화를 위한 주요 접근법은 사후 학습 양자화 및 양자화 인식 학습으로 분류할 수 있다. 훈련 후 양자화는 잘 알려진 두 가지 방법인 동적 및 정적 방법으로 양자화된 훈련된 모델에 관한 것이다. 동적 후-트레이닝 양자화는 런타임 상에서 양자화의 범위를 계산하며 정적에 비해 느리다. 양자화 인식 훈련은 양자화 기준을 훈련에 추가하고, 양자화된 모델은 훈련 과정에서 훈련되고 최적화된다. 이 접근법은 엔드 모델이 좋은 성능을 가질 것이고 또한 트레이닝 후에 양자화될 필요가 없다는 것을 보장한다.

## IV LLMs 사용 및 확장 방법

LLM이 훈련되면 다양한 작업에 대해 원하는 출력을 생성하는 데 사용할 수 있습니다. 기본 프롬프팅을 통해 LMM을 직접 사용할 수 있습니다. 그러나, 이들의 잠재력을 최대한 활용하거나 몇 가지 단점을 해결하기 위해,

도. 34: LoRA reparametrizan의 예시. 이 과정에서 \(A\)와 \(B\)만 훈련되었다. [142]의 예의.

도. 35: 학생 및 교사와의 일반적인 지식 증류 프레임워크(Courtesy of [144]).

우리는 외부 수단을 통해 모델을 보강해야 합니다. 이 섹션에서는 먼저 환각 문제에 대해 자세히 살펴보면서 LLM의 주요 단점에 대한 간략한 개요를 제공한다. 그런 다음 프롬프트 및 일부 확장 접근법이 이러한 한계를 해결할 수 있을 뿐만 아니라 LLM을 외부 세계와 인터페이스할 수 있는 능력을 갖춘 완전한 AI 에이전트로 만드는 LLM의 기능을 확장하는 데 사용할 수 있는 방법을 설명한다.

### _LLM limitations_

LLM은 토큰을 예측하도록 훈련된다는 것을 기억하는 것이 중요하다. 미세 조정 및 정렬은 성능을 향상시키고 능력에 다른 차원을 추가하지만, 특히 순진하게 사용되는 경우 여전히 몇 가지 중요한 한계가 있다. 그 중 일부는 다음을 포함한다:

* 상태/메모리가 없습니다. LLM은 이전 프롬프트에서 보낸 내용조차 기억할 수 없습니다. 그것은 어떤 형태의 상태를 필요로 하는 많은 사용 사례에 대한 중요한 한계이다.
* 확률적/확률적입니다. 동일한 프롬프트를 여러 번 LLM에 보내는 경우 다른 응답을 받을 수 있습니다. 반응의 변동성을 제한하기 위해 매개변수, 특히 온도가 있지만 이는 문제를 일으킬 수 있는 훈련의 고유한 특성이다.
* 오래된 정보가 있으며 자체적으로 외부 데이터에 액세스할 수 없습니다. LLM은 그 자체로 현재 시간이나 요일에 대해 알지 못하며, 그것의 트레이닝 세트에 존재하지 않았던 어떤 정보도 액세스할 수 없다.
* 일반적으로 매우 큽니다. 이는 교육 및 서비스를 위해 많은 비용이 드는 GPU 기계가 필요하다는 것을 의미합니다. 어떤 경우에는 가장 큰 모델이 특히 지연 시간 측면에서 좋지 않은 SLA를 가지고 있다.
* 환각 상태입니다. LLM은 "진실"이라는 개념을 가지고 있지 않으며 보통 좋은 내용과 나쁜 내용의 혼합에 대해 훈련을 받았다. 그들은 매우 그럴듯하지만 진실되지 않은 대답을 만들어 낼 수 있다.

이전의 제한은 일부 응용 프로그램에 모두 중요해질 수 있지만 지난 몇 달 동안 많은 관심을 모았고 나중에 설명할 많은 신속한 접근법과 LLM 확장 방법을 촉발했기 때문에 마지막 환각으로 약간 뛰어드는 것이 가치가 있다.

**환각:** LMM (대형 언어 모델) 영역에서 "환각" 현상이 크게 주목을 받았습니다. 문헌, 특히 "자연어 생성에서의 환각에 대한 조사" 논문[145]에서 LLM에서의 환각은 "제공된 출처에 대해 터무니없거나 불성실한 내용의 생성"으로 특징지어진다. 이 용어는 심리학에 뿌리를 두고 있지만 인공지능 분야에서 전유되었다.

LLM의 환각은 크게 두 가지 유형으로 분류할 수 있다.

1. **내재적 환각**: 원본 자료와 직접 충돌하여 사실적 부정확성 또는 논리적 불일치를 도입합니다.
2. **외재적 환각**: 이는 모순되지는 않지만 추측 또는 확인할 수 없는 요소를 포함 하 여 원본에 대해 확인할 수 없습니다.

LLM 컨텍스트에서 '소스'의 정의는 태스크에 따라 다르며, 대화 기반 태스크에서는 '세계 지식'을 지칭하는 반면, 텍스트 요약에서는 입력 텍스트 자체를 지칭한다. 이러한 구분은 환각을 평가하고 해석하는 데 중요한 역할을 한다. 환각의 영향은 또한 맥락 의존성이 높다. 예를 들어, 시 쓰기와 같은 창의적인 노력에서 환각은 수용되거나 심지어 유익하다고 여겨질 수 있다.

인터넷, 서적, 위키피디아를 포함한 다양한 데이터셋에 대해 학습된 LMM은 진실이나 거짓에 대한 고유한 이해 없이 확률 모델을 기반으로 텍스트를 생성한다. 최근 인간 피드백(RLHF: Human Feedback)으로부터 지시 튜닝 및 강화 학습과 같은 발전은 LLM을 보다 사실적인 출력으로 조향하려고 시도했지만 근본적인 확률적 특성과 고유한 한계는 남아 있다. 최근 연구인 "추론 작업에 대한 대규모 언어 모델에 의한 환각의 원천" [146]은 LLM에서 환각에 기여하는 두 가지 주요 측면을 강조하며, LLM 훈련 및 출력 생성에 내재된 복잡성을 강조한다.

LLM에서 환각의 효과적인 자동화된 측정은 통계 및 모델 기반 메트릭의 조합을 필요로 한다.

_Statistical Metrics_:

* ROUGE [147] 및 BLEU [148]과 같은 메트릭은 고유한 환각에 중점을 두고 텍스트 유사성을 평가하는 데 일반적입니다.
* 구조화된 지식 소스를 사용할 수 있는 경우 PARENT [149], PARENT-T [150], Knowledge F1 [151]과 같은 고급 메트릭이 활용됩니다. 이러한 메트릭은 효과적이지만 구문 및 의미적 뉘앙스를 포착하는 데 한계가 있다.

_Model-Based Metrics_:

* **IE 기반 메트릭**: 정보 추출 모델을 활용 하 여 지식을 관계형 튜플로 간소화 한 다음 원본과 비교 합니다.
* **QA 기반 메트릭**: 질문 응답 프레임워크를 통해 생성된 콘텐츠와 원본 간의 중복을 평가합니다 ([152] 참조).
* **NLI 기반 메트릭**: 자연 언어 추론 데이터 세트를 사용 하 여 지정 된 전제에 따라 생성 된 가설의 진실성을 평가 합니다 ([153] 참조).
* **충실도 분류 메트릭**: 미묘한 평가를 위해 작업별 데이터 세트를 만들어 정제된 평가를 제공합니다 ([154] 참조).

자동화된 측정 기준의 발전에도 불구하고 인간의 판단은 여전히 중요한 부분으로 남아 있다. 일반적으로 두 가지 방법론이 포함됩니다. 1. **점수 매기기**: 인간 평가자는 미리 정의된 척도 내에서 환각 수준을 평가합니다.
2. **비교 분석**: 평가자는 생성 된 콘텐츠를 기준 또는 지상 진실 참조와 비교 하 여 주관적 평가의 필수 계층을 추가 합니다.

FactScore[155]는 인간 및 모델 기반 평가 모두에 사용될 수 있는 메트릭의 최근 예이다. 이 미터법은 LLM 세대를 "원자 사실"로 나눈다. 최종 점수는 각 원자 사실에 대한 정확도의 합으로 계산되어 각 원자 사실에 동일한 가중치를 부여한다. 정확도는 원자적 사실이 소스에 의해 뒷받침되는지 여부를 단순히 나타내는 이진수이다. 저자는 LLM을 사용하여 이 메트릭을 추정하는 다양한 자동화 전략을 구현한다.

마지막으로 LLM에서 환각을 완화하는 것은 다양한 응용 프로그램에 맞는 맞춤형 전략이 필요한 다면적인 도전이다. 포함한다.

* 제품 디자인 및 사용자 상호 작용 전략 예를 들어 유스케이스 디자인, 입/출력 구조화 또는 사용자 피드백을 위한 메커니즘 제공입니다.
* 데이터 관리 및 지속적인 개선입니다. 환각 추적 세트를 유지하고 분석하는 것은 지속적인 모델 개선을 위해 필수적이다.
* 프롬프트 엔지니어링 및 메타 프롬프트 디자인입니다. 검색 증강 생성과 같은 IV-B에 설명된 많은 고급 프롬프트 기술은 환각 위험을 직접 다룬다.
* 환각 완화를 위한 모델 선택 및 구성입니다. 충분한 경우 낮은 온도 설정을 가진 더 큰 모델이 일반적으로 더 나은 성능을 발휘한다. 또한, RLHF 또는 도메인별 미세 조정과 같은 기술은 환각 위험을 완화할 수 있다.

### _LLMs 사용** **Prompt Design and Engineering**_

생성 AI 모델의 프롬프트는 모델의 출력을 안내하기 위해 사용자가 제공하는 텍스트 입력이다. 간단한 질문부터 자세한 설명 또는 특정 작업에 이르기까지 다양할 수 있습니다. 프롬프트는 일반적으로 지침, 질문, 입력 데이터 및 예로 구성된다. 실제로, AI 모델로부터 원하는 응답을 이끌어내기 위해 프롬프트에는 명령 또는 질문이 포함되어야 하며, 다른 요소는 선택 사항이다. 첨단 프롬프트는, 예를 들어, 보다 복잡한 구조를 포함한다.

도. 36: LLMs의 사용 및 증강 방법.

"사고의 사슬" 프롬프트는 모델이 답을 얻기 위해 논리적 추론 과정을 따르도록 안내된다.

신속한 공학은 LLM 및 기타 생성 AI 모델의 상호 작용 및 출력을 형성하는 빠르게 진화하는 학문이다. 신속공학의 본질은 생성적 모델로 특정 목표를 달성하기 위한 최적의 신속성을 만드는 데 있다. 이 과정은 모델을 지시하는 것뿐만 아니라 모델의 능력과 한계, 그리고 그것이 작동하는 맥락에 대한 약간의 이해도 포함한다.

프롬프트 공학은 프롬프트의 단순한 구성을 초월하며, 도메인 지식의 혼합, AI 모델에 대한 이해 및 다양한 컨텍스트에 대한 프롬프트를 맞춤화하기 위한 체계적인 접근이 필요하다. 여기에는 지정된 데이터 세트 또는 컨텍스트를 기반으로 프로그래밍 방식으로 수정할 수 있는 템플릿을 만드는 작업이 포함될 수 있습니다. 예를 들어, 사용자 데이터에 기초하여 개인화된 응답을 생성하는 것은 관련 사용자 정보로 동적으로 채워진 템플릿을 사용할 수 있다.

또한, 신속한 공학은 모델 평가 또는 하이퍼파라미터 튜닝과 같은 전통적인 기계 학습 관행과 유사한 반복적이고 탐구적인 과정이다. 이 분야의 급속한 성장은 기능 또는 아키텍처 엔지니어링과 같은 전통적인 방법을 넘어 기계 학습의 특정 측면에 혁명을 일으킬 가능성을 시사한다. 반면에, 버전 제어 및 회귀 테스트와 같은 전통적인 엔지니어링 관행은 다른 기계 학습 접근법에 적응된 것처럼 이 새로운 패러다임에 적응될 필요가 있다[156].

다음 단락에서 우리는 가장 흥미롭고 인기 있는 신속한 공학 접근법 중 일부를 자세히 설명한다.

#### Iii-B1 Chain of Thought (CoT)

CoT(Chain of Thought) 기법은 구글 연구자들에 의해 "Chain of Thought Prompting Elicits Reasoning in Large Language Models"[34]에 처음 기술된 것으로, LMM(Large Language Models)의 신속한 공학에서 중추적인 발전을 나타낸다. 이 접근법은 토큰 예측에 능숙하지만 LLM이 명시적 추론을 위해 본질적으로 설계되지 않았다는 이해에 달려 있다. CoT는 필수 추론 단계를 통해 모델을 안내함으로써 이를 해결한다.

CoT는 LLM의 암시적 추론 과정을 명시적으로 만드는 것을 기반으로 한다. 추론에 필요한 단계를 설명함으로써, 모델은 특히 단순한 정보 검색 또는 패턴 인식보다 더 많은 것을 요구하는 시나리오에서 논리적이고 추론된 출력에 더 가깝게 지향된다.

CoT 프롬프트는 두 가지 기본 형태로 표시됩니다.

1. **Zero-Shot CoT:** 이 양식은 LLM에 "단계별로 생각"하도록 지시하여 문제를 해체하고 각 추론 단계를 명확히 하는 것을 포함합니다.
2. **수동 CoT:** 더 복잡한 변형으로 모델에 대한 템플릿으로 단계별 추론 예제를 제공해야 합니다. 보다 효과적인 결과를 얻으면서 확장성 및 유지 관리에 어려움을 제기합니다.

수동 CoT가 제로 샷보다 더 효과적입니다. 그러나 이 예제 기반 CoT의 효과는 다양한 예제의 선택에 달려 있으며, 수작업에 의한 단계별 추론으로 프롬프트를 구성하는 것은 어렵고 오류가 발생하기 쉽다. 그것은 자동 CoT[157]가 작동하는 곳입니다.

#### Iii-B2 Tree of Thought (ToT)

Tree of Thought(ToT)[158] 프롬프트 기법은 가장 그럴듯한 것으로 수렴하기 전에 다양한 대안적 해결책이나 사고 과정을 고려하는 개념에서 영감을 얻는다. ToT는 각 가지가 다른 추론 라인을 나타내는 여러 개의 "생각 트리"로 분기하는 아이디어를 기반으로 한다. 이 방법을 사용하면 LLM이 가장 가능성이 높은 시나리오를 결정하기 전에 여러 시나리오를 고려하는 인간의 인지 과정과 마찬가지로 다양한 가능성과 가설을 탐색할 수 있다.

ToT의 중요한 측면은 이러한 추론 경로에 대한 평가이다. LLM이 서로 다른 사고 분기를 생성함에 따라 각각이 쿼리에 대한 유효성 및 관련성에 대해 평가된다. 이 프로세스는 분기의 실시간 분석 및 비교를 포함하여 가장 일관되고 논리적인 결과를 선택한다.

ToT는 한 줄의 추론이 충분하지 않을 수 있는 복잡한 문제 해결 시나리오에서 특히 유용하다. LLM은 결론에 도달하기 전에 다양한 가능성을 고려하여 인간과 유사한 문제 해결 접근법을 모방할 수 있다. 이 기술은 모호성, 복잡성 및 미묘한 작업을 처리하는 모델의 능력을 향상시켜 고급 AI 응용 프로그램에서 귀중한 도구가 된다.

#### Iii-B3 Self-Consistency

자기 일관성[159]은 앙상블 기반 방법을 이용하는데, 여기서 LLM은 동일한 질의에 대한 다수의 응답을 생성하도록 프롬프트된다. 이러한 응답 간의 일관성은 정확성과 신뢰성을 나타내는 지표 역할을 한다.

자기 일관성 접근법은 LLM이 동일한 프롬프트에 대해 다수의 유사한 응답을 생성하는 경우 응답이 정확할 가능성이 더 높다는 원칙에 근거한다. 이 방법에는 일관성을 위해 응답을 분석할 때마다 LLM에 여러 번 쿼리를 처리하도록 요청하는 작업이 포함됩니다. 이 기술은 사실적 정확도와 정밀도가 가장 중요한 시나리오에서 특히 유용하다.

응답의 일관성은 다양한 방법을 사용하여 측정할 수 있다. 한 가지 일반적인 접근법은 응답 내용의 중복을 분석하는 것이다. 다른 방법들은 응답들의 의미론적 유사성을 비교하는 것 또는 BERT-점수들 또는 n-그램 중첩들과 같은 보다 정교한 기법들을 채용하는 것을 포함할 수 있다. 이러한 측정은 LLM에 의해 생성된 응답 간의 일치 수준을 정량화하는 데 도움이 된다.

자기 일관성은 정보의 진실성이 중요한 분야에서 중요한 응용 분야를 가지고 있다. AI 모델이 제공하는 정보의 정확성을 보장하는 것이 필수적인 팩트 체킹과 같은 시나리오에서 특히 관련이 있다. 이 기법을 사용하면 신속한 엔지니어가 LLM의 신뢰성을 향상시켜 높은 수준의 사실적 정확성을 요구하는 작업에 대해 보다 신뢰할 수 있다.

#### Iii-B4 Reflection

반사 [160]은 LLM이 응답의 정확성과 일관성에 대한 추론을 기반으로 자신의 출력을 평가하고 잠재적으로 수정하도록 유도하는 것을 포함한다. 반성의 개념은 자기 평가의 한 형태에 참여하는 LLM의 능력에 중점을 둔다. 초기 응답을 생성한 후, 모델은 사실적 정확도, 논리적 일관성 및 관련성과 같은 요소를 고려하여 자체 출력을 반영하도록 프롬프트된다. 이러한 내성적 과정은 수정되거나 개선된 반응의 생성으로 이어질 수 있다.

반성의 핵심 측면은 자체 편집을 위한 LLM의 용량이다. 초기 반응을 평가함으로써 모델은 잠재적인 오류 또는 개선 영역을 식별할 수 있다. 생성, 반사 및 개정의 이러한 반복적인 프로세스는 LLM이 출력을 정제할 수 있게 하여 응답의 전반적인 품질과 신뢰성을 향상시킨다.

#### Iii-B5 Expert Prompting

전문가 프롬프팅[161]은 다양한 분야의 전문가들의 응답을 시뮬레이션함으로써 대형 언어 모델(LLM)의 능력을 향상시킨다. 이 방법은 LLM이 전문가의 역할을 맡고 그에 따라 응답하도록 유도하여 고품질의 정보에 입각한 답변을 제공하는 것을 포함한다. 전문가 프롬프팅 내의 핵심 전략은 다중 전문가 접근법이다. LLM은 여러 전문가의 관점에서 응답을 고려하도록 유도되며, 종합되어 종합적이고 균형 잡힌 답변을 형성한다. 이 기술은 반응의 깊이를 향상시킬 뿐만 아니라 주제에 대한 보다 전체적인 이해를 반영하여 다양한 관점을 통합한다.

#### Iii-B6 Chains

체인이란 LMM(Large Language Models)으로 복잡한 작업을 처리하기 위해 여러 컴포넌트를 시퀀스로 연결하는 방법을 말한다. 이 접근법은 각각 최종 결과에 기여하는 일련의 상호 연결된 단계 또는 프로세스를 생성하는 것을 포함한다. 체인(Chains)의 개념은 서로 다른 스테이지 또는 컴포넌트가 순차적으로 배열되는 워크플로우를 구성하는 아이디어를 기반으로 한다. 체인의 각 구성 요소는 특정 기능을 수행하며, 하나의 출력은 다음 것을 위한 입력 역할을 한다. 이러한 엔드-투-엔드 배열은, 각각의 스테이지가 태스크의 특정 양태를 처리하도록 맞춤화될 수 있기 때문에, 보다 복잡하고 미묘한 처리를 허용한다. 체인들은 요구사항에 따라 복잡도 및 구조가 달라질 수 있다. "PromptChainer: Chaining Large Language Model Prompts through Visual Programming" [162]에서, 저자들은 체인 설계의 주요 과제들을 기술할 뿐만 아니라, 그러한 과제들을 지원하기 위한 시각적 도구를 기술한다.

#### Iii-B7 Rails

고급 프롬프트 엔지니어링에서의 레일은 미리 정의된 규칙 또는 템플릿을 통해 LLM(Large Language Models)의 출력을 안내하고 제어하는 방법을 지칭한다. 이 접근법은 모델의 응답이 특정 표준 또는 기준을 준수하도록 보장하여 출력의 관련성, 안전성 및 정확성을 향상시키도록 설계된다. 레일 개념은 응답을 생성하는 동안 LLM이 따라야 하는 프레임워크 또는 지침 세트를 설정하는 것을 포함한다. 이러한 지침은 일반적으로 자연어 문장이 구조화되고 전달되는 방식을 표준화하는 표준 양식(Canonical Forms)으로 알려진 모델링 언어 또는 템플릿을 사용하여 정의된다.

레일은 애플리케이션의 특정 요구에 따라 다양한 목적을 위해 설계될 수 있다:

* **토픽 레일:** LLM이 특정 토픽 또는 도메인에 연결되었는지 확인합니다.
* **팩트 검사 레일:** 잘못된 정보 또는 오판의 소지가 있는 정보의 생성을 최소화하는 것을 목표로 합니다.
* **Jailbreaking Rails:** LLM이 자체 운영 제약 조건 또는 지침을 우회하려는 응답을 생성하지 않도록 합니다.

#### Iii-B8 자동 프롬프트 엔지니어링(APE)

자동 프롬프트 엔지니어링(APE) [163]은 대형 언어 모델(LLM)에 대한 프롬프트 생성 프로세스를 자동화하는 데 중점을 둔다. APE는 프롬프트를 생성하고 평가하기 위해 LLM 자체의 기능을 활용하여 프롬프트 설계 프로세스를 능률화하고 최적화하려고 한다. APE는 모델을 사용하여 프롬프트를 생성하고, 점수를 매기고, 정제하는 자기 참조 방식으로 LLM을 사용하는 것을 포함한다. LLM의 이러한 재귀적 사용은 원하는 응답 또는 결과를 이끌어낼 가능성이 더 높은 고품질 프롬프트를 생성할 수 있게 한다.

APE의 방법론은 몇 가지 주요 단계로 분해될 수 있다:

* **프롬프트 생성:** LLM은 지정된 작업 또는 목표에 따라 다양한 잠재적 프롬프트를 생성합니다.
* **프롬프트 점수:** 생성된 각 프롬프트는 종종 명확성, 특이성 및 원하는 응답을 도출할 가능성과 같은 기준을 사용하여 효과에 대해 평가됩니다.
* **정제 및 반복:** 이러한 평가를 기반으로 프롬프트를 정제하고 반복하여 품질과 효율성을 더욱 높일 수 있습니다.

### 외부 지식을 통해 LLM 확장 - RAG_

사전 훈련된 LLM의 주요 한계 중 하나는 최신 지식 부족 또는 개인 또는 사용 사례별 정보에 대한 액세스이다. 이것이 검색 증강 생성(RAG)이 그림으로 들어오는 곳이다[164]. 도 37에 예시된 RAG는 입력 프롬프트로부터 쿼리를 추출하고 그 쿼리를 사용하여 외부 지식 소스(예를 들어, 검색 엔진 또는 지식 그래프, 도 38 참조)로부터 관련 정보를 검색하는 것을 포함한다. 그런 다음 관련 정보가 원래 프롬프트에 추가되고 모델이 최종 응답을 생성하기 위해 LLM에 공급된다. RAG 시스템은 검색, 생성, 증강의 세 가지 중요한 구성요소를 포함한다[165].

RAG 인식 프롬프트 기술은 고급 LLM 시스템을 구축하기 위해 RAG의 중요성으로 인해 최근 몇 가지 RAG 인식 프롬프트 기술이 개발되었다. 그러한 기술 중 하나는 전향 능동 검색 증강 생성(FLARE)이다.

전향 지향 능동 검색 증강 생성(FLARE, Active Retrieval Augmented Generation) [168]은 예측과 정보 검색을 반복적으로 결합함으로써 대형 언어 모델(LLM, Large Language Models)의 능력을 향상시킨다. FLARE는 LLM 응답의 정확성과 관련성을 개선하기 위한 검색 증강 생성 사용의 진화를 나타낸다.

FLARE는 LLM이 다가오는 콘텐츠를 능동적으로 예측하고 이러한 예측을 관련 정보를 검색하기 위한 쿼리로서 사용하는 반복적인 프로세스를 포함한다. 이 방법은 일반적으로 정보를 한 번 검색한 다음 생성을 진행하는 전통적인 검색 증강 모델과 대조된다. FLARE에서 이 프로세스는 생성 단계 전반에 걸쳐 역동적이고 진행 중이다. FLARE에서, LLM에 의해 생성된 각각의 문장 또는 세그먼트는 신뢰도에 대해 평가된다. 신뢰 레벨이 특정 임계치 미만인 경우, 모델은 생성된 콘텐츠를 질의로서 사용하여 관련 정보를 검색하고, 이어서 문장을 재생성하거나 정제하는 데 사용된다. 이러한 반복 프로세스는 응답의 각 부분이 이용가능한 가장 관련성 있고 현재 정보에 의해 통지되는 것을 보장한다.

RAG 프레임워크 및 관련 작업에 대한 자세한 내용은 검색 증강 세대에 대한 이 조사를 독자에게 참조한다[165].

### **외부 도구 사용**

전술한 바와 같이 외부 지식 소스로부터 정보를 검색하는 것은 LLM을 증강시키는 잠재적인 방법들 중 하나일 뿐이다. 보다 일반적으로, LLM은 그 기능을 보강하기 위해 임의의 수의 외부 도구(예를 들어, 서비스에 대한 API)에 액세스할 수 있다. 그런 점에서 RAG는 소위 "도구"의 광범위한 범주의 특정 사례로 볼 수 있다.

이러한 컨텍스트의 도구는 LLM이 활용할 수 있는 외부 기능 또는 서비스입니다. 이러한 도구는 LLM이 수행할 수 있는 작업의 범위를 기본 정보 검색에서 외부 데이터베이스 또는 API와의 복잡한 상호 작용으로 확장한다.

논문 "도구형성기: 언어 모델은 도구를 사용하도록 스스로 가르칠 수 있다" [169]에서 저자는 LLM을 훈련시켜 언제 사용할 도구, 심지어 API가 필요로 하는 매개변수가 무엇인지 결정함으로써 단순한 도구 사용을 넘어간다. 도구에는 두 개의 서로 다른 검색 엔진 또는 계산기가 포함됩니다. 다음 예제에서 LLM은 외부 Q&A 도구, 계산기 및 위키피디아 검색 엔진을 호출 하기로 결정 합니다. 보다 최근에 버클리의 연구자들은 특정 하지만 상당히 일반적인 도구인 Api를 사용 하 여 GPT-4를 능가 하는 고릴라 [67] 라는 새 LLM을 교육 했습니다.

RAG로 설명한 것과 유사한 도구 인식 프롬프트 기술은 도구를 보다 확장 가능하게 만들기 위해 여러 도구 인식 프롬프트 접근법이 개발되었다. 인기 있는 기술은 소위 자동 다단계 추론 및 도구 사용(ART)입니다.

ART(Automatic Multi-step Reasoning and Tool-use) [170]은 자동화된 사고 사슬 프롬프팅과 외부 도구의 사용을 결합한 프롬프트 엔지니어링 기법이다. ART는 다중 프롬프트 엔지니어링 전략의 융합을 나타내며, 외부 데이터 소스 또는 도구와의 추론 및 상호 작용이 모두 필요한 복잡한 작업을 처리하는 LMM(Large Language Model)의 능력을 향상시킨다.

ART는 태스크와 입력이 주어지면 시스템이 먼저 태스크 라이브러리에서 유사한 태스크를 식별하는 체계적인 접근법을 포함한다. 그런 다음 이러한 작업을 프롬프트에서 예로 사용 하 여 현재 작업에 접근 하 고 실행 하는 방법에 대 한 LLM을 안내 합니다. 이 방법은 태스크들이 내부 추론과 외부 데이터 처리 또는 검색의 조합을 필요로 할 때 특히 효과적이다.

### **LLM Agents**

AI 에이전트에 대한 아이디어는 AI의 역사에서 잘 탐구되었습니다. 에이전트는 전형적으로 자신의 센서들을 사용하여 환경을 인지할 수 있는 자율적인 엔티티이고, 현재 존재하는 상태에 기초하여 판단을 하고, 그에 따라 그것이 이용할 수 있는 동작들에 기초하여 행동한다.

LLM들의 맥락에서, 에이전트는 특정 태스크들을 자율적으로 수행할 수 있는 (증강된) LLM의 전문화된 인스턴스화에 기초한 시스템을 지칭한다. 이러한 에이전트는 사용자 및 환경과 상호작용하여 입력 및 상호작용의 의도된 목표에 기초하여 결정을 내리도록 설계된다. 에이전트는 LLM에 기반을 두고 있습니다.

도. 37 : 질의 응답 어플리케이션용 LLM과 RAG를 합성한 예[166].

도. 38 : LMM과 리트리버로서의 KG를 합성한 일례[167]이다.

도구에 접근하고 사용하고 주어진 입력에 따라 결정을 내릴 수 있는 가능성. 그들은 보통 단순한 대응 생성을 넘어 어느 정도의 자율성과 의사 결정이 필요한 업무를 처리하도록 설계되었다.

일반 LLM-기반 에이전트의 기능들은 다음을 포함한다:

* 도구 액세스 및 활용: 에이전트는 외부 도구 및 서비스에 액세스하고 이러한 리소스를 효과적으로 활용하여 작업을 수행할 수 있습니다.
* 의사 결정: 입력, 컨텍스트 및 사용 가능한 도구를 기반으로 결정을 내릴 수 있으며, 종종 복잡한 추론 프로세스를 사용합니다.

일 예로, 날씨 API와 같은 함수(또는 API)에 접근할 수 있는 LLM은 특정 장소의 날씨와 관련된 임의의 질문에 답할 수 있다. 즉, API를 이용하여 문제를 해결할 수 있다. 나아가, 그 LLM이 구매를 할 수 있게 하는 API에 액세스할 수 있다면, 구매 에이전트가 구축되어 외부 세계로부터 정보를 판독할 수 있는 능력을 가질 뿐만 아니라 그에 따라 행동할 수 있다[171].

도. 도 40은 대화 정보 추구를 위한 LLM 기반 에이전트의 다른 예를 나타낸다[36]. 여기서, LLM은 대화 상태를 추적하는 _워킹 메모리_, 작업에 대한 실행 계획을 만들고 다음 시스템 액션을 선택하는 _정책_, 정책에 의해 선택된 액션을 수행하는 _액션 실행기_(외부 지식으로부터의 증거를 통합하거나 LLM이 응답을 생성하도록 프롬프트함), 및 LLM의 응답과 사용자 기대 또는 특정 비즈니스 요구 사항의 정렬에 액세스하고, 에이전트 성능을 향상시키기 위해 피드백을 생성하는 _유틸리티_를 포함하는 플러그 앤 플레이 모듈 세트로 증강된다.

LLM 기반 AI 에이전트에 대한 자세한 내용은 최근 조사[172, 173, 174]를 참조하십시오.

RAG 및 Tools와 같은 에이전트에 대한 신속한 엔지니어링 기술, LLM 기반 에이전트의 요구를 구체적으로 해결하는 신속한 엔지니어링 기술이 개발되었다. 이러한 세 가지 예는 관찰 없는 추론(ReWOO), 추론 및 행위(ReAct), 대화 가능 해결 에이전트(DERA)이다.

관찰 없는 추론(ReWOO)(175)은 직접적인 관찰로부터 추론을 분리하는 것을 목표로 한다. ReWOO는 LLM이 외부 데이터나 도구에 대한 즉각적인 의존 없이 포괄적인 추론 계획이나 메타 계획을 수립할 수 있도록 함으로써 작동한다. 이 접근법은 에이전트가 필요한 데이터 또는 관찰이 이용가능하면 실행될 수 있는 추론을 위한 구조화된 프레임워크를 생성할 수 있게 한다. ReWOO에서 LLM은 초기에 주어진 문제에 접근하고 해결하는 방법을 설명하는 계획(일련의 단계)을 개발한다. 이 메타 계획 단계는 에이전트가 정보를 사용할 수 있게 되면 처리하는 단계를 설정하기 때문에 중요합니다. 실행 단계는 실제 데이터 또는 관찰을 미리 지정된 계획에 통합하여 일관되고 맥락적으로 관련된 응답을 유도하는 것을 포함한다. ReWOO는 토큰 효율성과 툴 실패에 대한 견고성 측면에서 상당한 이점을 제공한다. LLM은 외부 데이터에 즉시 액세스할 수 없는 작업을 잘 구조화된 추론 프레임워크에 의존하여 처리할 수 있다. 이 방법은 데이터 검색이 비용이 많이 들고, 느리거나, 불확실한 시나리오에서 특히 유리하여 LLM 기반 에이전트가 높은 수준의 성능 및 신뢰성을 유지할 수 있게 한다.

이성과 행위(ReAct)[176]는 LLM이 언어적 추론뿐만 아니라 실행 가능한 단계를 생성하도록 촉구하여 모델의 동적 문제 해결 능력을 향상시킨다. ReAct는 추론과 행동을 통합하는 원칙에 근거한다. 이 접근법에서, LLM은 인터리브 방식으로 추론 트레이스(설명)를 생성하는 것과 액션(스텝 또는 커맨드)을 취하는 것 사이에서 교번하도록 프롬프트된다. 이 접근법은 모델이 문제에 대해 동적으로 추론할 수 있게 하고, 동시에 구체적인 행동을 제안하고 취할 수 있게 한다.

대화 가능 해결 에이전트(Dialog-Enabled Resolving Agent;DERA)[177]는 대화형 교환에 기초하여 대화에 참여하고, 쿼리를 해결하고, 결정을 내릴 수 있는 특화된 AI 에이전트이다. DERA는 각각 특정 역할과 기능을 가진 대화 컨텍스트 내에서 여러 에이전트를 활용하는 아이디어를 기반으로 개발된다. 이러한 에이전트에는 정보를 수집하고 분석하는 연구원과 제공된 정보를 기반으로 최종 판단을 내리는 결정자가 포함될 수 있습니다. 이러한 역할 분담은 문제 해결과 의사 결정에 대한 체계적이고 효율적인 접근을 가능하게 한다. DERA는 의료 진단 또는 고객 서비스에서와 같이 복잡한 의사 결정 및 문제 해결을 필요로 하는 시나리오에서 특히 유리하다. DERA 에이전트의 협력적이고 상호 작용적인 특성으로 인해 단일 에이전트 시스템이 어려움을 겪을 수 있는 깊이 및 뉘앙스의 수준으로 복잡한 쿼리를 처리할 수 있다. 또한, 이 접근법은 인간의 의사 결정 프로세스와 잘 일치하여 AI 추론을 보다 신뢰할 수 있고 신뢰할 수 있게 만든다.

## V Popular Dataets for LLMs

대규모 언어 모델은 유망한 성과를 나타내지만, 발생하는 주요 질문은 그들이 얼마나 효과적으로 기능하고 특정 작업이나 응용 프로그램에서 성능을 평가할 수 있는지이다.

LLM에 대한 평가는 응용 프로그램의 진화하는 풍경으로 인해 특정 문제를 제기한다. LLM 개발의 원래 목적은 번역, 요약, 질문 응답 등과 같은 NLP 작업의 성능을 향상시키는 것이었다[178]. 그러나 오늘날 이러한 모델들이 코드 생성 및 금융을 포함한 다양한 영역에서 효용을 찾고 있다는 것은 분명하다. 또한 LLM의 평가는 공정성과 편향, 사실 확인 및 추론과 같은 몇 가지 중요한 고려 사항을 포함한다. 이 섹션에서는 LLM을 평가하기 위해 일반적으로 사용되는 벤치마크를 설명한다. 이러한 벤치마크는 LLM 능력을 훈련하거나 평가하는 것을 기반으로 분류된다.

### _기본 작업에 대 한 데이터 세트_ 언어 모델링/이해/generation_

이 섹션에서는 LLM의 기본 능력을 평가하는 데 적합한 벤치마크 및 데이터 세트에 대한 개요를 제공한다.

* **자연 질문**[179]은 Google 검색 엔진에 질문으로 제출된 실제 익명화된 집계 쿼리로 구성된 QA 데이터 세트입니다. 주석자는 상단 \(5\) 검색 결과에서 위키피디아 페이지와 함께 질문을 제시하며, 페이지에 존재하는 경우 긴 답변(일반적으로 단락)과 짧은 답변(하나 이상의 엔티티)에 주석을 달거나 긴/짧은 답변이 존재하지 않는 경우 null을 표시한다.
* **MMLU**[180]은 제로 샷 및 소수의 샷 시나리오에서 얻은 지식을 평가하기 위한 것입니다. 이는 MMLU가 모델의 일반적인 지식과 문제 해결 능력을 모두 평가한다는 것을 의미한다. STEM, 인문, 사회과학, 기타 분야 57개 과목을 다루고 있다. 벤치마크는 기본에서 고급 전문가에 이르기까지 복잡성이 다양합니다. 이 데이터 세트의 주요 기여는 다중 작업 언어 이해, 질문 응답 및 산술 추론에 대한 것임을 언급할 가치가 있다.
* **MBPP**[181]은 "대부분 기본 Python 문제"를 나타내며 코드 생성을 위해 설계된 모델의 성능을 평가하기 위한 벤치마크를 제공합니다. 벤치마크는 기본 프로그래밍 개념과 표준 라이브러리 사용 등을 포함한 광범위한 주제를 포함하는 \(974\) 짧은 파이썬 프로그램을 포함한다. 각 챌린지는 작업 설명, 코드 솔루션 및 3개의 자동화된 테스트 케이스로 구성된다.
* **HumanEval**[182]는 코드 생성 작업을 위한 데이터 세트입니다. 이 데이터 세트는 \(164\) 수작업 프로그래밍 문제로 구성됩니다. 각 도전에는 함수 서명, 문서 문자열, 코드 본문 및 여러 단위 테스트가 수반됩니다. 이 데이터 세트를 개발하는 주요 직관은 코드 생성 모델을 위한 훈련 데이터 세트에서 콘텐츠를 제외하는 것을 보장하는 것이다.
* **APPS**[183]은 Python 프로그래밍 언어에 중점을 둔 코드 생성 작업을 위해 설계되었습니다. APPS 데이터 세트에는 \(232,444\) Python 프로그램 모음이 포함되어 있습니다. 데이터 세트의 각 프로그램에는 Python 코드의 평균 \(18\) 줄이 있습니다. 또한 APPS는 각각 텍스트 기반 문제 설명이 있는 \(10,000\) 고유한 프로그래밍 연습 리포지토리에 액세스할 수 있습니다. 마지막으로 강조할 점은 테스트 케이스를 포함한다는 것입니다.
* **WikiSQL**[184]는 코드 생성 작업을 위해 만들어졌으며 위키피디아 테이블의 87,726개의 신중하게 레이블이 지정된 SQL 쿼리 쌍과 해당 자연어 질문이 있습니다. SQL 쿼리는 테스트 집합 (\(17,284\) 예), 개발 (\(9,145\) 예), 훈련 (\(61,297\) 예)의 세 가지 하위 집합으로 구성 됩니다.
* **TriviaQA**[185]는 QA 작업을 위해 설계되었습니다. 이 데이터 세트는 \(650,000\) 이상의 질문-답변-증거 트리플을 포함한다. 이 데이터 세트에는 \(95,000\)개의 질문-답변 쌍이 있으며, 각 쌍은 상식 애호가들이 저작하고 평균 6개의 독립적으로 조달된 증거 문서에서 지원된다. 이러한 문서는 위키피디아 또는 광범위한 웹 검색 결과에서 자동으로 획득됩니다. 데이터셋은 위키피디아와 웹 도메인의 정답을 포함한 두 개의 세그먼트로 분류되며, 검증된 세트는 위키피디아와 온라인 모두의 관련 문서와 함께 정확하게 답변된 질문을 구현한다.

도. 40: 대화 정보 탐색을 위한 LLM 기반 에이전트. [36]의 예의.

도. 39: HuggingGPT: 도구를 사용하고 계획하는 에이전트 기반 접근 방식 [[171]]* 읽기 이해 작업을 위한 **RACE**[186] 슈트입니다. 이 데이터셋은 중·고등학생(12\)에서 18\(18\)까지의 중국 학생들이 작성한 영어 테스트를 기반으로 하며, 대략 2만 8천\(28,000\)개의 텍스트와 1차 영어 강사를 중심으로 한 인간 전문가가 엄격하게 작성한 10만\(10만\)개의 질문이 포함되어 있다. 이 데이터 세트에는 학생들의 이해력과 추론 능력을 평가하기 위해 의도적으로 선택된 광범위한 과목이 포함되어 있다. 이 데이터 세트는 RACE-M, RACE-H 및 RACE의 세 가지 하위 그룹에서 사용할 수 있다. RACE-M은 중학교 시험을 의미하고, RACE-H는 고등학교 시험을 의미한다. 마지막으로 RACE는 RACE-M과 RACE-H의 합성이다.
* **SQuAD**[187]은 "Stanford Question Answering Dataset"을 나타내며 위키피디아 기사를 기반으로 하는 크라우드 소싱 읽기 이해 데이터 세트입니다. 약 \(100,000\)의 질문-답변 쌍이 \(500\) 이상의 기사에 연결되어 있다. 이러한 질문들에 대한 답변들은 전형적으로 대응하는 판독 구절들로부터 취해진 텍스트 단편들 또는 스팬들이다. 어떤 경우에는 질문에 답할 수 없습니다. 데이터 세트는 \(80\%\) 훈련 세트, \(10\%\) 개발 세트 및 \(10\%\) 숨겨진 테스트 세트의 세 세 세 세 세트로 나뉜다.

도. 41: 데이터셋 애플리케이션.

* **BoolQ**[188]은 목표가 읽기 이해 작업인 예/아니오 질문 응답 데이터 세트입니다. BoolQ는 \(15,942\) 예를 포함한다. 각각의 예는 질문, 관련 단락 및 해결책을 포함하는 삼중항이다. 이 데이터 세트의 주요 직관은 읽기 이해이지만 추론, 자연어 추론 및 질문 응답 작업에 사용할 수 있다.
* **MultiRC**[189]는 읽기 이해 작업에 맞는 또 다른 데이터 세트입니다. 멀티RC는 짧은 단락뿐만 아니라 단락의 정보를 사용하여 답할 수 있는 다중 문장 질문을 포함한다. 이 데이터 세트의 단락은 뉴스, 소설, 역사 텍스트, 위키피디아 기사, 사회와 법에 대한 토론, 초등학교 과학 교과서 및 9/11 보고서를 포함한 다양한 출처에서 비롯된다. 각 문항은 응답 선택지가 많고, 그중 하나 이상이 정답이다. 질문에 답하려면 여러 문장에 걸친 추론이 필요합니다. 다중 RC 데이터 세트는 800개 이상의 단락에서 수집된 약 \(6,000\)개의 다중 문장 질문을 포함한다. 평균적으로 각 문항은 총 5개 중 약 2개의 타당한 답안 대안을 제시하고 있다.

### _Datasets for Emergent: ICL, reasoning (CoT), instruction following_

이 섹션에서는 LLM의 비상 능력을 평가하기 위해 사용된 벤치마크 및 데이터 세트에 중점을 둔다.

* **GSM8K**[190]은 다단계 수학적 추론을 위한 모델의 능력을 평가하도록 설계되었습니다. GSM8K에는 인간이 작성한 언어적으로 다양한 초등학교 수학 단어 문제가 8.5K 포함되어 있다. 데이터 세트는 \(7.5K\) 문제가 있는 훈련 세트와 \(1\)K 문제가 있는 테스트 세트로 나뉜다. 이러한 문제를 해결하기 위해서는 \(2\)에서 \(8\) 단계가 필요하다. 솔루션은 주로 기본적인 산술 연산을 이용한 일련의 기초 연산이다.
* **MATH**[191]을 사용하면 모델이 수학 문제를 얼마나 잘 해결할 수 있는지 평가할 수 있습니다. MATH 데이터셋은 고등학교 수학 경진대회 마지막 \(12\), \(500\)의 문제를 포함한다. 데이터 세트의 각 문제에는 단계별 솔루션과 최종 답변이 상자에 포함되어 있습니다. 그 문제들은 광범위한 주제를 다루며 다양한 수준의 복잡성을 가지고 있다. 총 7개의 과목이 있습니다. 또한, 각 문제의 난이도는 \({}^{\prime}1^{\prime}\)에서 \({}^{\prime}5^{\prime}\)까지의 척도로 AoPS 기준에 따라 평가된다. ({}^{\prime}1^{\prime}\)이 가장 쉬운 문제이고, ({}^{\prime}5^{\prime}\)이 가장 어려운 문제이다. 형식화 측면에서 LATEX와 Asymptote 벡터 그래픽스 언어를 사용하여 모든 문제와 해결책을 제시한다.
* **HellaSwag**[192]는 LLM에서 상식 추론을 평가하도록 설계되었습니다. 이 벤치마크에는 (70,000\) 객관식 문항이 포함된다. 각 질문은 ActivityNet 또는 WikiHow의 두 도메인 중 하나에서 파생되며 다음 상황에서 발생할 수 있는 문제에 대한 네 가지 답변 선택을 제공합니다. 상기 정답은 상기 정답을 설명하는 실제 진술을 제공하고,

도. 42: 상이한 라이센스에 의해 라이센스된 데이터 세트.

앞으로 다가올 사건이지만, 기계들을 혼란스럽게 하기 위해 세 가지 오답이 만들어진다.
* **AI2 추론 도전 (ARC)**[193]은 상식 추론에 사용 됩니다. 이 벤치마크는 \(7,787\) 과학 시험 문제를 포함한다. 이러한 문항은 영어로 되어 있으며, 대부분 객관식 형식으로 설정되어 있다. 질문은 난이도가 2,590\인 챌린지 세트와 5,197문항인 이지 세트로 나누어졌다. 각 컬렉션은 또한 기차, 개발 및 테스트 하위 집합으로 사전 분할되었습니다.
* **PIQA**[194]는 물리적 상식에 대한 지식에 대한 언어 표현을 평가하기 위한 것입니다. 이 데이터 세트에서는 일반적이지 않은 솔루션을 선호하는 일상적인 상황에 초점을 맞춥니다. 중심 과제는 객관식 질문 응답으로, 질문 \((q)\)과 두 개의 잠재적 해 \((s1,s2)\)이 제공된다. 그런 다음 모델인지 인간인지 여부에 따라 최상의 솔루션이 선택됩니다. 각 질문에 대해 솔루션 중 하나만 정답입니다.
* **SIQA**[195]는 사회적 상황에 대한 상식 추론에 대한 모델의 능력을 평가하기 위한 프레임워크를 제공합니다. SIQA 데이터 세트에는 일상적인 상황에서 감정 및 사회 지능을 평가하기 위해 설계된 \(38,000\) 객관식 질문이 있습니다. 이 데이터 세트는 다양한 소셜 시나리오를 다룹니다. SIQA에서 잠재적 답변은 인간 선택 응답과 적대적 프로세스를 통해 필터링된 기계 생성 응답의 혼합물이다.
* **OpenBookQA(OBQA)**[196]은 질문에 응답하려면 책에 포함되지 않은 추가 일반 및 상식 지식과 풍부한 텍스트 이해가 필요한 새로운 종류의 질문 응답 데이터 세트입니다. 이 데이터 세트에는 약 6,000개의 객관식 질문이 포함되어 있습니다. 각 질문은 하나의 핵심 사실과 \(6000\) 이상의 사실의 추가 수집으로 연결된다. 질문들은 다단계 크라우드소싱과 전문가 필터링 절차를 사용하여 개발되었다. OpenBookQA 질문은 제한된 배경을 가진 멀티홉 추론이 필요하기 때문에 어렵다.
* **TruthfulQA**[197]은 질문에 대한 답변을 생성하는 데 있어 언어 모델의 진실성을 평가하기 위해 특별히 설계되었습니다. 이 데이터 세트에는 보건, 법률, 금융 및 정치를 포함한 \(38\) 다양한 범주에서 저자가 작성한 817개의 질문이 포함된다. 이러한 질문은 오답으로 이어지는 일반적인 오해를 포함할 수 있기 때문에 인간 응답자에게 도전하기 위해 의도적으로 설계되었다.
* **OPT-IML 벤치**[103]은 교육 메타 학습에 대한 포괄적인 벤치마크입니다. 기존 8개 벤치마크에서 2000개의 NLP 작업을 다룬다. OPT-IML 벤치는 17.9 M 예제를 가진 훈련 세트, 145K 샘플을 가진 개발 세트, 321K 샘플을 가진 테스트 세트로 구성된다.

### _Datasets for Augmented: using external knowledge/tools_

이 섹션에서는 LLM의 향상된 능력을 위해 설계된 데이터 세트에 중점을 둔다.

* **HotpotQA**[198]는 다중 홉 추론이 필요한 다양하고 설명할 수 있는 질문 응답 데이터 세트를 포함하도록 설계되었습니다. 이 데이터 세트는 영어 위키피디아에서 파생되었습니다. 이는 대략 \(113,000\)개의 문항으로 구성되어 있다. 데이터 세트의 각 질문은 2개의 위키피디아 기사에서 골드 단락이라고 하는 2개의 단락과 함께 제공된다. 또한, 군중 노동자들이 질문에 답하기 위해 중요하게 선택한 문단의 문장 목록이 있습니다.
* **ToolQA**[199]는 질문에 응답하기 위해 외부 도구를 사용하는 LLM의 기능을 평가하는 질문 응답 벤치마크입니다.
* **GPT4도구** 는 시각적 콘텐츠 및 도구 설명을 조건으로 하는 지침을 사용 하 여 고급 교사 (예: Chat-GPT)에게 지시 하 여 생성 된 교육 데이터 집합 역할을 합니다. 이러한 과정은 도구 사용과 관련된 명령어의 생성으로 귀결된다. 이 데이터 세트에는 세 가지 버전이 있습니다. 첫 번째 버전은 GPT4Tools 모델을 미세 조정하기 위해 사용되는 71,000개의 명령 후속 데이터 포인트로 구성된다. 다음 버전은 검증에 사용되는 수동으로 청소된 명령어 데이터로 구성되며, 첫 번째 버전의 도구와 관련된 명령어를 포함한다. 마지막 버전은 테스트에 사용되는 정리된 명령어 데이터이며, 첫 번째 버전에 존재하지 않는 일부 도구와 관련된 명령어를 포함한다.

## VI Prominent LLMs' Performance on Benchmark

이 섹션에서는 먼저 다양한 시나리오에서 LLM의 성능을 평가하는 데 사용되는 몇 가지 인기 있는 메트릭에 대한 개요를 제공한다. 그런 다음 인기 있는 데이터 세트 및 벤치마크 중 일부에 대한 두드러진 대형 언어 모델의 성능을 살펴본다.

### _Popular Metrics for Evaluation LLMs_

생성 언어 모델의 성능을 평가하는 것은 그들이 사용할 기본 작업에 달려 있다. 감성 분석과 같은 주어진 것 중에서 선택을 선택하는 것이 대부분인 작업은 분류와 같이 단순하다고 볼 수 있으며 분류 메트릭을 사용하여 성능을 평가할 수 있다. 이 경우 정확도, 정밀도, 재현율, F1 등의 메트릭이 적용 가능하다. 또한, 다중 선택 질의 응답과 같은 특정 태스크에 대해 모델에 의해 생성된 답변은 항상 True 또는 False 중 어느 하나임을 주목하는 것이 중요하다. 답변이 옵션 집합에 없으면 거짓이라고도 볼 수 있습니다.

그러나 순수하게 개방형 텍스트 생성인 일부 작업은 범주화와 동일한 방식으로 평가할 수 없다. 평가의 특정 목적에 따라 다른 측정 기준이 필요합니다. 코드 생성은 개방형 생성 평가에서 매우 다른 경우이다. 생성된 코드는 테스트 슈트를 통과해야 하지만 다른 한편으로는 모델이 다른 솔루션을 코드로 생성할 수 있는지, 그 중 올바른 솔루션을 선택할 확률은 얼마인지 이해하는 것도 중요하다. Pass@k는 이 경우에 매우 좋은 메트릭이다. 문제가 주어지면 코드와 같은 다른 솔루션이 생성되는 방식으로 작동한다. 다양한 기능 테스트를 사용하여 정확성에 대해 테스트됩니다. 그 후, 솔루션에서 생성된 각각의 c 수는 정확한 방정식 4가 최종 값을 제공한다.

\[\text{pass@}k:=\underset{\text{Problems}}{\mathbb{E}}\left[1-\frac{\binom{n-c}{k }}{\binom{n}{k}}\right] \tag{4}\]

정확한 일치(EM)는 (미리 정의된) 답변으로부터의 정확한 일치와 대부분 관련된 또 다른 메트릭이다. 하나 이상의 원하는 참조 텍스트 토큰 중 하나를 토큰별로 정확하게 일치하면 예측이 올바른 것으로 계산됩니다. 어떤 경우에는 정확도와 같을 수 있으며 방정식 5는 수학적 정의를 보여준다. 여기서, M은 총 정답수이고, N은 총 문항수이다[202].

\[EM=\frac{M}{N} \tag{5}\]

한편, 인간 동등성 점수(HEQ)는 F1 점수에 대한 대안이다[203]. HEQ-Q는 개별 질문의 정밀도를 나타내며, 모델의 F1 점수가 평균 인간 F1 점수를 초과하는 경우 정답이 올바른 것으로 간주된다. 마찬가지로, HEQ-D는 각 대화의 정밀도를 나타내며, 대화 내의 모든 질문이 HEQ의 기준을 충족할 때 정확하다고 간주된다[182].

기계 번역과 같은 다른 생성 작업의 평가는 루즈 및 BLEU와 같은 메트릭에 기초한다. 이 점수는 지상 진리(번역)와 같은 참조 텍스트와 생성 모델에 의해 생성된 가설이 있을 때 잘 작동한다. 이 점수들은 대부분 정답과 지상 진리의 유사성을 계산 방식으로 탐지하는 것을 목표로 하는 경우에 사용된다. 계산 방식으로 N-Grams 이상의 것이 사용되지 않는다는 것을 의미했다. 그러나 BERT-Score와 같은 메트릭도 이러한 경우에 적합하지만 심각합니다.

\begin{table}
\begin{tabular}{|l|l|l|l|l|} \hline
**Benchmark Name** & **Evaluation Metric** & **Leaderboard** & **Source** & **paperswithcode** \\ \hline HumanEval & Pass@k & Link & Link & Link \\ \hline MBPP & Pass@k, Accuracy & - & Link & Link \\ \hline APPS & Pass@k, Accuracy & - & Link & Link \\ \hline WikiSQL & Accuracy & - & Link & Link \\ \hline CoNLA & BLEU & Link & Link \\ \hline CodeParrot & Pass@k & - & Link & - \\ \hline HellaSWang & Accuracy & Link & Link & Link \\ \hline AI2 & Reasoning & Accuracy & Link & Link \\ Challenge (ARC) & Accuracy & Link & Link & Link \\ \hline BoolQ & Accuracy & - & Link & Link \\ \hline MultiRC & F1-score, Accuracy & - & Link & Link \\ \hline CNN/Daily Mail [200] & Accuracy & - & Link & - \\ \hline SQuAD & F1-score, EM & Link & Link & Link \\ \hline RAC & Accuracy & - & Link & Link \\ \hline CNN/Daily Mail [201] & ROUGE & - & Link & Link \\ \hline Drop & F1-score, EM & Link & Link & Link \\ \hline QuAC: & F1-score, HEQ-Q, HEQ-D & Link & Link & Link \\ \hline TriviaQA & EM, F1-score, Accuracy & Link & Link & Link \\ \hline Natural Questions & EM, F1-score, Accuracy & Link & Link & Link \\ \hline StrategyQA & Accuracy, Recall@10, SARI & Link & Link & Link \\ \hline CoQA & F1-score & Link & Link & Link \\ \hline XSum & ROUGE & - & Link & Link \\ \hline SAMSun & ROUGE & - & Link & Link \\ \hline WikiSum & ROUGE & - & Link & - \\ \hline DialogSum & ROUGE & - & Link & Link \\ \hline TruthIUQA & MCI, MC2, \% true, \% info, BLEU/RT & Link & Link & Link \\ \hline MMLU & Accuracy & Link & Link & Link \\ \hline GSM8K & Accuracy & Link & Link & Link \\ \hline PIQA & Accuracy & Link & Link & Link \\ \hline SIQA & Accuracy & Link & Link & Link \\ \hline OpenBookQA (OBQA) & Accuracy & Link & Link & Link \\ \hline HotopQA & EM, F1-score, Joint EM, Joint F1-score, & Link & Link & Link \\ \hline MATH & Accuracy & - & Link & Link \\ \hline CommonseQA & Accuracy & Link & Link & Link \\ \hline Natural Instructions & ROUGE-L, Human & Link & Link & Link \\ \hline BIG-bench & Accuracy, Average & - & Link & Link \\ \hline ToolTalk & Success rate, Precision, Recall, Incorrect & - & Link & Link \\ \hline MetaTool & Accuracy, Precision, Recall, F1-score & - & Link & Link \\ \hline GPT4Tools & Successful Rate of Thought, Successful Rate of Action, Successful Rate of Arguments, Success Rate & - & Link & Link \\ \hline API-Bank & Correctness, ROUGE, Error(API Hallution, Has Exception, Invalid Input Parameters, False API Call Format, API Call, Miss Input Parameters) & - & Link & Link \\ \hline Alpaca-CoT & - & - & Link & Link \\ \hline \end{tabular}
\end{table} TABLE II: LLM Datasets Overview.

다른 모델이 판단을 위해 사용되기 때문에 불가능합니다. 여전히, 오늘날에도, 순수하게 생성된 콘텐츠를 평가하는 것은 매우 어렵고 완전히 적합한 메트릭을 찾지 못하거나, 메트릭은 N-Gram, SkipGram 등과 같은 단순한 특징을 찾거나, 알려지지 않은 정확성과 정밀성을 갖는 모델이다[204].

생성 평가 메트릭은 또한 답변을 평가하기 위해 다른 LLM을 사용하는 LLM에 대한 또 다른 유형의 평가 메트릭이다. 다만 과제 자체에 따라 이와 같이 평가가 가능할 수도 있고 그렇지 않을 수도 있다. 생성적 평가 오류를 발생하기 쉬운 또 다른 종속성은 프롬프트 자체에 의존하는 것이다. RAGAS는 생성 평가의 사용을 포함하는 좋은 사례 중 하나이다.

대형 언어 모델의 세계에서 가장 어려운 문제를 해결하기 위해 다양한 벤치마크와 리더보드가 제안되었는데, 어떤 것이 더 나은가? 그러나 간단한 답은 이 문제를 해결할 수 없다. 답은 큰 언어 모델의 다양한 측면에 달려 있다. 섹션 V는 서로 다른 작업의 범주형 제시와 각 범주에서 가장 중요한 데이터 세트를 보여준다. 우리는 동일한 범주화를 따르고 각 범주를 기준으로 비교를 제공할 것이다. 각 범주에 대 한 비교를 제공 하 여 여러 작업에 대 한 보고 된 성능 메트릭을 평균 하 여 집계 된 성능에 대 한 광범위한 개요를 제공 합니다.

다른 LLM을 평가하는 것은 다른 관점에서도 볼 수 있다. 예를 들어, 파라미터 수가 급격히 적은 LLM은 파라미터 수가 많은 LLM과 완전히 비교할 수 없다. 이러한 관점에서 LLM을 **작은** (10억 매개 변수 이하), **중간** (10~100억), **큰** (100~1000억) 및 **매우 큰** (1,000억 이상)의 네 가지 범주로 분류 합니다. 우리가 사용하는 LLM에 대한 또 다른 분류는 주요 사용 사례이다. 각 LLM을 **Foundation** 모델(명령어 미세 조정 및 채팅 미세 조정이 없는 사전 훈련 언어 모델), **명령어 미세 조정만 있는 사전 훈련 언어 모델) 및 **채팅** 모델(명령어 및 채팅 미세 조정이 있는 사전 훈련 언어 모델) 중 하나로 간주합니다. 설명된 모든 범주화 외에도 원래 모델과 조정된 모델을 구별하기 위해 또 다른 범주가 필요하다. **오리지널** 모델은 기초 모델 또는 미세 조정 모델로 출시된 모델입니다. **튜닝된** 모델은 원래 모델을 파악하고 다른 데이터 세트 또는 다른 학습 접근 방식으로 튜닝한 모델입니다. 또한 원래 모델은 일반적으로 특정 데이터 세트 또는 심지어 다른 접근법에서 미세 조정된 기초 모델이라는 점에 유의하는 것이 좋다. 라이센스에 관계없이 모델 가중치의 가용성은 분류의 또 다른 범주이다. (요청을 통해서도) 공개적으로 사용할 수 있는 가중치가 있는 모델은 **공개** 모델로 표시되고 다른 모델은 **비공개** 모델로 표시됩니다. 표 III은 나머지 기사에서 사용된 이러한 정의와 약어를 모두 보여준다. 도 43은 이들을 시각적으로 도시한다.

제공된 분류에 따라 표 IV와 같이 각 주목할만한 LLM을 분류하고 레이블링할 수 있다. 이 표에서 알 수 있듯이 매우 큰 것으로 분류된 모델도 사용할 수 없다.

### _LLMs' Performance on Different Tasks_

상식 추론은 각 모델이 얻을 수 있는 중요한 능력 중 하나이다. 이 능력은 모델이 추론 기술과 결합하여 사전 지식을 사용할 수 있는 능력을 나타낸다. 예를 들어, HellaSwag의 경우, 주어진 텍스트가 스토리의 일부 부분을 포함하는 반면 주어진 선택들이 연속으로서 선택하기가 까다롭고 사전이 없기 때문에 텍스트의 연속을 찾는 것은 어렵다.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|} \hline Model & Size & \#Params (B) & Type & Availability & Origin \\ \hline Davinci-002 & Very Large & 175 & Instruction & Unavailable & Tuned \\ \hline Davinci-003 & Very Large & 175 & Instruction & Unavailable & Tuned \\ \hline GPT 3.5-turbo & Large & 20 & Chat & Unavailable & Tuned \\ \hline Falcon 7B & Medium & 7 & Foundation & Public & Original \\ \hline Alpaca & Large & 13 & Chat & Public & Tuned \\ \hline Pythia 7B & Medium & 7 & Foundation & Public & Original \\ \hline Pythia 12B & Large & 12 & Foundation & Public & Original \\ \hline LLAMA 7B & Medium & 7 & Chat & Public & Original \\ \hline LLAMA 2 7B & Medium & 7 & Chat & Public & Tuned \\ \hline LLAMA 2 7B & Medium & 7 & Foundation & Public & Original \\ \hline Vicuna 13B & Large & 13 & Foundation & Public & Tuned \\ \hline Vicuna 7B & Medium & 7 & Foundation & Public & Tuned \\ \hline Claude & Large & 93 & Chat & Unavailable & Original \\ \hline Claude 2 & Very Large & 137 & Chat & Unavailable & Original \\ \hline \end{tabular}
\end{table} TABLE IV: Different LLM categorization.

\begin{table}
\begin{tabular}{|l|l|l|} \hline Classification & Category & Description \\ \hline \multirow{3}{*}{Size} & Small & Number of parameters \(\leq\) 1B \\ \cline{2-3}  & Medium & 1B \(<\) Number of parameters \(\leq\) 10B \\ \cline{2-3}  & Large & 100B \(<\) Number of parameters \(\leq\) 100B \\ \cline{2-3}  & Very Large & 100B \(<\) Number of parameters \\ \hline \multirow{3}{*}{Type} & Foundation model & Pretrained language model \\ \cline{2-3}  & Instruction model & Pretrained and instruction fine-tuned language model \\ \cline{2-3}  & Chat model & Pretrained, instruction fine-tuned, and chat fine-tuned language model \\ \hline \multirow{2}{*}{Origin} & Original model & An original model released with either Foundation, Instruction, or Chat model \\ \cline{2-3}  & Tuned model & Fine-tuned version of an original model \\ \hline \multirow{2}{*}{Availability} & Publicly available & Model and weights are available due to request to without request \\ \cline{2-3}  & Publicly unavailable & Model and weights are not publicly available \\ \hline \end{tabular}
\end{table} TABLE III: LLM categories and respective definitions.

세상에 대한 지식은 불가능하다. 이러한 특정 유형의 추론은 텍스트가 공개된 장면이나 사실로 이전 지식을 활용하는 것과 관련이 있기 때문에 높은 관심을 받을 만하다. 표 V에서 알 수 있듯이 Unavailable 모델뿐만 아니라 Public 모델도 다양한 테스트에서 좋은 결과를 얻을 수 있다.

표 V에 제시된 결과로부터 GPT-4가 HellaSwag에 대해 최상의 결과를 달성하는 반면 Davinci-003은 OBQA에 대한 최상의 모델임이 분명하다. OBQA에 대한 결과가 모든 모델에 대해 보고되지 않았으며 아마도 다빈치-003이 OBQA에서 가장 높은 결과를 달성하는 최상의 모델이 아닐 수 있다는 점에 주목하는 것도 좋다.

모든 모델이 모든 데이터 세트에 대해 성능을 보고하는 것은 아니며, 그 때문에 다른 테이블에 성능이 보고되는 모델의 수가 다르다.

세계 지식은 대부분 일반적인 지식 질문에 관한 것으로, 예를 들어 "특정 잘 알려진 책의 저자가 누구인가"와 같은 위크 아티팩트 데이터세트 질문에서 찾을 수 있고 참고 문헌도 제공된다. 결과를 표 VII에 나타낸다.

\begin{table}
\begin{tabular}{|l|l|l|} \hline Model & OBQA & HellaSwag \\ \hline Davinci-003 & **51** & 83.4 \\ \hline Falcon 7B & 44.4 & 76.3 \\ \hline Alpaca & 43.4 & 73.9 \\ \hline Pythia 7B & 37.2 & 64 \\ \hline Pythia 12B & 43.2 & 68.1 \\ \hline LLAMA 7B & 42.4 & 73 \\ \hline Dolly 6B & 41.2 & 67.6 \\ \hline Dolly 12B & 40.4 & 71 \\ \hline Alpaca 7B & 43.4 & 73.9 \\ \hline Alpaca Lora 7B & 42.6 & 74 \\ \hline GPT-6 7B & 38.2 & 66.2 \\ \hline LLAMA 7B & 42.4 & 73 \\ \hline LLAMA 13B & 42.2 & 76.2 \\ \hline Pythia 7B & 37.2 & 64 \\ \hline Pythia 12B & 38 & 67.3 \\ \hline StableLM Tuned & 33.4 & 53.6 \\ \hline Koula 13B & 42.8 & 72.6 \\ \hline Moisea pipel-7B & 42.6 & 76.3 \\ \hline \hline LLAMA 20B & - & 87.33 \\ \hline LLAMA 65B & - & 86.09 \\ \hline Falcon 40B & - & 85.3 \\ \hline Falcon 180B & - & 88.36 \\ \hline MPT Insitract 30B & - & 84.31 \\ \hline MPT Insitract 7B & - & 77.91 \\ \hline Yi 6B & - & 76.42 \\ \hline Yi 34B & - & 85.69 \\ \hline GPT-4 & - & **95.3** \\ \hline Gemini Ultra & - & 87.8 \\ \hline \end{tabular}
\end{table} TABLE V: Commonsense reasoning comparison.

\begin{table}
\begin{tabular}{|l|l|l|} \hline Model & Objects & Penguins \\ \hline GPT-NeoX & 26 & 33.56 \\ \hline OPT 66B8 & 31.2 & 28.08 \\ \hline Bloomberg GPT & 34.8 & 37.67 \\ \hline BLOOM 176B & 36.8 & 40.41 \\ \hline PJM 540B & 38 & 44.5 \\ \hline Gopher-280B & 49.2 & 40.6 \\ \hline Chinchilla-70B & 59.7 & 48.7 \\ \hline PuLM 2 & 61.2 & 65.8 \\ \hline \end{tabular}
\end{table} TABLE VI: Symbolic reasoning comparison.

도. 43: LLM 분류.

일부 특정 유스케이스 모델의 경우, 코딩 및 코드 생성 능력이 매우 요구된다. 표 VIII는 코딩 능력에 대한 다양한 모델의 결과를 보여준다.

산술 추론은 달성하기 위한 또 다른 도전적인 추론 능력이다. 예를 들어 GSM8K는 그들의 답변과 관련하여 초등학교 수학 문제를 포함한다. 표 IX는 다양한 모델 비교에 대한 통찰력을 제공한다.

어떤 경우에는 큰 언어 모델은 단순히 다음 토큰 예측 기계이기 때문에 환각적인 대답이다. 환각은 큰 언어 모델이 얼마나 신뢰할 수 있고 신뢰할 수 있는지를 측정하는 데 중요한 요소 중 하나이다. 다른 한편으로 환각을 측정하는 것도 각 사실을 다른 스타일로 쓸 수 있고 글쓰기의 작은 변화조차도 감지하기 어렵게 만들기 때문에 쉽지 않다. 특정 LLM이 텍스트에서 허위 정보의 환각을 감지할 수 있는 능력이 더 있다면 더 신뢰할 수 있다고 가정하는 것이 공정하다. HaluEval은 이 분야에서 환각을 측정하는 것을 목표로 하는 데이터 세트 중 하나이다[205]. 평가는 또한 실제 답변과 관련하여 응답을 판단하는 다른 모델에 의해 수행될 수 있다[206]. 표 X는 이러한 데이터 세트를 기반으로 한 다양한 모델의 평가를 보여준다.

## VII Challengees and Future Direction

앞 절에서 살펴보았듯이, 대형 언어 모델은 지난 1-2년 동안 인상적인 성과를 거두었다.

\begin{table}
\begin{tabular}{|l|l|l|} \hline Model & GSM8k & MATH \\ \hline Gemini Ultra & 94.4 & 53.2 \\ \hline GPT-4 & 87.1 & 42.5 \\ \hline Gemini Pro & 86.5 & 32.6 \\ \hline ToRA 70B & 84.3 & 49.7 \\ \hline MatchCoder-L-70B & 83.9 & - \\ \hline MetaMM 70B & 82.3 & 26 \\ \hline MuggelMATH 70B & 82.3 & - \\ \hline MatchCoder-L-34B & 81.7 & 45.2 \\ \hline ToRA-Code 34B & 80.7 & 50.8 \\ \hline MetaMath-Mistral-7B & 77.7 & - \\ \hline Arithm2-Mistral-7B & 76.4 & - \\ \hline ToRA-Code 13B & 75.8 & 48.1 \\ \hline Arithmo-Mistral-7B & 74.7 & - \\ \hline MatchCoder-L-13B & 74.1 & 35.9 \\ \hline MuggelMATH 13B & 74. & - \\ \hline CodeT5+ & 73.8 & - \\ \hline Kavai YuMish 13B & 73.3 & - \\ \hline ToRA-Code 7B & 72.6 & 44.6 \\ \hline MathlCoder-L-13B & 72.6 & 29.9 \\ \hline Metahali 13B & 71 & 22.5 \\ \hline LMAM 65B & 69.7 & 10.6 \\ \hline MuggelMATH 7B & 68.4 & - \\ \hline MatchCoder-CL-7B & 67.8 & 23.3 \\ \hline MetaMM 7B & 66.4 & 19.4 \\ \hline RFT 70B & 64.8 & - \\ \hline MatchCoder-T-7B & 64.2 & - \\ \hline Orca -21B & 59.14 & - \\ \hline U-PalM & 58.5 & - \\ \hline PalM-540B & 58.1 & 8.8 \\ \hline LLAMA 2 70B & 56.8 & - \\ \hline RFT 13B & 53.3 & - \\ \hline LiLMA 33B & 53.1 & 7.1 \\ \hline Mistral 7B & 52.2 & 13.1 \\ \hline RFT 7B & 51.2 & - \\ \hline LLAMA 65B & 50.9 & 20.5 \\ \hline Orac 2-7B & 47.23 & - \\ \hline Text-davni-002 & 40.7 & 19.1 \\ \hline LAMA 3B & 33.6 & 3.9 \\ \hline GPT-27B & 19.5 & - \\ \hline GLPM 7B & 18.1 & 2.9 \\ \hline PalM 540B & 17.9 & 8.8 \\ \hline LLAMA 13B & 17.8 & 3.9 \\ \hline LLAMA 7B & 11.2 & 2.9 \\ \hline GLPM-Neo-125M & 7.5 & - \\ \hline Pahl 8B & 4.1 & 1.5 \\ \hline GPT-2 & - & 5.4 \\ \hline GPT-3 175Bb & - & 5.2 \\ \hline PALM 62B & - & 4.4 \\ \hline GPT-3-13B & - & 3.3 \\ \hline LLAMA 7B & 11 & 2.9 \\ \hline LLAMA 7B & 11 & 2.9 \\ \hline PLM 8B & - & 1.5 \\ \hline \end{tabular}
\end{table} TABLE IX: Arithmetic reasoning comparison.

\begin{table}
\begin{tabular}{|l|l|l|l|l|} \hline Model & TriviaQA & NaturalQ & WebQ & ARC \\ \hline BLOMOM & - & - & - & 32.9 \\ \hline BLOMOM 176B & - & - & - & 50.85 \\ \hline Bloomberg GPT & - & - & - & 48.63 \\ \hline Chinchilla & - & 35.5 & - & - \\ \hline Code + REPLUG & 76.8 & 44.7 & - & - \\ \hline GAL 120B & - & - & - & 67.9 \\ \hline GLAM 62R64E & 75.8 & 32.5 & 15.5 & 50.3 \\ \hline Gopher & - & 28.2 & - & - \\ \hline GPT-3 175B & 71.2 & 29.9 & 41.5 & 85.2 \\ \hline GPT-4 & - & - & - & 96.4 \\ \hline GPT-NoK & - & - & - & 45.9 \\ \hline LLAMA 13B & - & - & - & 52.7 \\ \hline LLAMA 2 70B & 85 & 33 & - & - \\ \hline LLAMA 33B & - & 24.9 & - & 57.8 \\ \hline LMAM 65B & 72.6 & 39.9 & - & - \\ \hline LLAMA 7B & - & - & 47.6 \\ \hline Mistral 7B & 69.9 & 28.8 & - & 55.5 \\ \hline ToR-Code 7B & - & 13.7 & - & - \\ \hline OptF & - & - & - & 31.1 \\ \hline OFI 66B & - & - & - & 44.54 \\ \hline OPT-175B & - & - & - & 43.94 \\ \hline OPT-175B & - & - & - & 25.6 \\ \hline PalM 2-1 & 86.1 & 37.5 & 28.2 & 95.1 \\ \hline PALM 2-M & 81.7 & 32 & 26.9 & 64.9 \\ \hline PalM 2-5B & 75.2 & 25.3 & 21.8 & 59.6 \\ \hline PalM 34-540B & 81.4 & 39.6 & 43.5 & 87.1 \\ \hline PriL-5web 1.3B & - & - & - & 44.9 \\ \hline SparseGPT & - & - & - & 38.99 \\ \hline SparseGPT & - & - & - & 39.85 \\ \hline SparseGPT & - & - & - & 41.3 \\ \hline \end{tabular}
\end{table} TABLE VIII: Coding capability comparison.

\begin{table}
\begin{tabular}{|l|l|l|} \hline Model & HumanEval \\ \hline Gemini Ultra & 74.4 \\ \hline Gemini Pro & 67.7 \\ \hline GPT-4 & 67 \\ \hline Wizard-Coder 15B & 57.3 \\ \hline phi-1 1.3B & 50.6 \\ \hline Code Llama & 48.8 \\ \hline OPT-3.5 & 48.1 \\ \hline OtotCoder & 46.2 \\ \hline phi-1-small & 45 \\ \hline PALM 2-5 & 37.6 \\ \hline InstructCodeT5+ 16B & 35 \\ \hline Mistral 7B & 30.5 \\ \hline LLAMA 2 & 29.9 \\ \hline phi-1-base & 29 \\ \hline Code-12B & 28.81 \\ \hline PALM 540B & 26.2 \\ \hline LLCAMA 7B & 24.2 \\ \hline L-LAMA 65B & 23.7 \\ \hline LLAMA 33B & 21.7 \\ \hline PALM 62B & 15.9 \\ \hline LLAMA 13B & 15.8 \\ \hline LMMA 137B & 14 \\ \hline MIM-350M & 13.7 \\ \hline LLAMA 7B & 10.5 \\ \hline PALM 8B & 3.6 \\ \hline \end{tabular}
\end{table} TABLE VII: World knowledge comparison.

동시에 이것은 여전히 혁신의 속도가 느려지기보다는 증가하고 있는 새롭고 극도로 활발한 연구 영역이다. 그러나 다른 진화하는 지역과 마찬가지로 앞으로 많은 과제가 있습니다. 여기에서는 지금까지 알려져 있는 몇 가지 도전과 주요 활동 영역에 대해 간략하게 언급한다. LLM 과제는 Kaddour et al. [207]의 작업에서 자세히 논의된다는 점에 주목할 필요가 있다.

### _더 작고 더 효율적인 언어 모델_

이것은 _큰_ 언어 모델에 대한 조사이며, GPT-4와 같은 더 큰 모델이 벤치마크에서 더 나은 정확도와 성능을 얻는 것으로 분명히 보상된 "더 큰 것이 더 낫다"를 향한 초기 추진이 있었다. 그러나 이러한 큰 모델은 여러 차원(예: 높은 대기 시간)에서 비용이 많이 들고 비효율적이다. 이 모든 것에 대응하여, 특히 더 큰 모델의 완전한 일반성을 요구하지 않을 수 있는 특정 작업에 사용될 때 LLM의 비용 효율적인 대안으로 소형 언어 모델(SLM)을 고안하려는 현재 연구 경향이 있다. 이 방향에서 눈에 띄는 작업은 마이크로소프트사의 Phi-1[208], Phi-1.5[209], Phi-2를 포함한다.

더 일반적으로 우리는 이 분야에서 더 작고 효율적인 모델을 훈련하는 방법에 대한 많은 연구 노력을 기대해야 한다. 파라미터-효율적인 미세 조정(PEFT), 교사/학생, 및 다른 형태의 증류와 같은 기술 - 섹션 III-I 참조 - 는 더 큰 모델들 중 더 작은 모델을 구축하기 위해 계속 사용될 것이다.

### 새 Post-attention Architectural Paradigms_

트랜스포머 블록은 현재 LLM 프레임워크의 대부분에서 중요하고 지속적인 부분이었고, 이 아키텍처가 얼마나 오래 유행할 것인지, 그리고 딥러닝(및 NLP) 분야에서 다음 큰 아키텍처 돌파구는 무엇인지 중요한 질문이다. 2012년 AlexNet 이후 LSTM, GRU, seq2seq 등 많은 아키텍처가 유행을 오가는 것을 볼 수 있었지만, 트랜스포머는 그 시작부터 지배적인 접근 방식이었다. 앞서 설명한 바와 같이, 주목은 변압기를 구동하는 주요 메커니즘이다. 보다 최근에는 사후 주의로 분류되는 대안적 접근법에 대한 유망한 연구가 있었다.

이러한 사후 주의 모델 클래스의 중요한 클래스는 소위 상태 공간 모델(SSM)입니다. 상태 공간 모델의 개념은 기계 학습에서 오랜 역사를 가지고 있지만, 언어 모델의 맥락에서 SSM은 일반적으로 더 새로운 구조 상태 공간 모델 아키텍처 또는 S4를 참조하여 짧게 사용된다는 점에 유의해야 한다(구 등 [29] 참조). 이 범주의 최근 모델에는 맘바[30], 하이에나[210], 줄무늬 하이에나[211]가 있다.

이러한 모든 모델은 리더보드의 성능과 효율성 측면에서 매우 경쟁력이 있지만 더 전통적인 주의 기반 아키텍처에서 중요한 문제를 해결합니다. _더 큰 컨텍스트 창에 대한 지원 부족_

많은 프롬프트에 대한 좋은 답변을 얻으려면 컨텍스트가 필요합니다. 예를 들어, "나에게 몇몇 좋은 영화들을 추천해줘"에 대한 응답은 "나"에 대한 많은 컨텍스트뿐만 아니라 어떤 영화들이 이용가능하고 어떤 영화들 1이 시청하지 않았는지에 대한 많은 컨텍스트를 요구한다. 문맥 길이는 RAG에 특히 중요하며, 여기서 텍스트의 많은 부분이 검색되어 생성을 위해 프롬프트에 주입될 수 있다(섹션 IV-C 참조).

문맥 길이가 길수록 우리는 문맥에 더 많은 토큰을 압착할 수 있다. 모델에 액세스할 수 있는 정보가 많을수록 반응이 향상됩니다. 그러나 다른 한편으로, 매우 긴 맥락으로, 모델이 모든 것을 기억하고 모든 정보를 효율적으로 처리하는 것은 어려울 것이다. 어텐션 기반 모델은 더 긴 컨텍스트에 대해 매우 비효율적이며, 그래서 우리는 더 긴 컨텍스트를 처리할 수 있도록 하고 일반적으로 더 효율적인 아키텍처를 생성하는 다양한 메커니즘에서 더 많은 연구를 기대해야 한다.

즉, 새로운 아키텍처가 제안만 하는 것은 아닐 수 있다.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|} \hline Model & HREM & HaluEval QA & HaluEval Dialogue & HaluEval Sum. & HaluEval General \\ \hline GPT 4 & 97 & - & - & - & - \\ \hline GPT 4 Turbo & 97 & - & - & - & - \\ \hline GPT 3.5 Turbo & 96.5 & 62.59 & 72.4 & 58.53 & 79.44 \\ \hline Davinci002 & - & 60.05 & 60.81 & 47.77 & 80.42 \\ \hline Davinci003 & - & 49.65 & 68.37 & 48.07 & 80.4 \\ \hline GPT-3 & - & 49.21 & 50.02 & 51.23 & 72.72 \\ \hline Google Gemini Pro & 95.2 & - & - & - & - \\ \hline Lima 2.70B & 94.9 & - & - & - & - \\ \hline Lima 2.7B & 94.4 & 49.6 & 43.99 & 49.55 & 20.46 \\ \hline Lima 2 13B & 94.1 & - & - & - & - \\ \hline Cohere-Chat & 92.5 & - & - & - & - \\ \hline Cohere & 91.5 & - & - & - & - \\ \hline Claude 2 & 91.5 & 69.78 & 64.73 & 57.75 & 75 \\ \hline Claude 1 & & 67.6 & 64.83 & 53.76 & 73.88 \\ \hline Microsoft Phi 2 & 91.5 & - & - & - & - \\ \hline Google Palm 2 (beta) & 91.4 & - & - & - & - \\ \hline Mixtral S8/7B & 90.7 & - & - & - & - \\ \hline Amazon Tran Express & 90.6 & - & - & - & - \\ \hline Mistral 7B & 90.6 & - & - & - & - \\ \hline Google Palm 2 Chat (beta) & 90 & - & - & - & - \\ \hline Google Palm 2 & 87.9 & - & - & - & - \\ \hline Google Palm 2 Chat & 72.8 & - & - & - & - \\ \hline ChatGLM & - & 47.93 & 44.41 & 48.57 & 30.92 \\ \hline Falcon & - & 39.66 & 29.08 & 42.71 & 18.98 \\ \hline Vicuna & - & 60.34 & 46.35 & 45.62 & 19.48 \\ \hline Alpaca & - & 6.68 & 17.55 & 20.63 & 9.54 \\ \hline \end{tabular}
\end{table} TABLE X: Hallucination evaluation alternatives for the attention mechanism but rather rethink the whole Transformer architecture. As an early example of this, Monarch Mixer [212] proposes a new architecture that uses the same sub-quadratic primitive that achieves high hardware efficiency on GPUs - Monarch matrices - along both sequence length and model dimension.

스펙트럼의 다른 쪽 끝에는 최근 증기를 얻고 더 좋고 강력한 LLM을 만드는 데 있어 그 가치를 입증하는 몇 가지 주의 호환 아키텍처 메커니즘이 있음을 언급할 가치가 있다. 아마도 그러한 메커니즘의 가장 좋은 예는 혼합물 전문가(MoE)일 것이다. MoE는 딥 러닝 시대[213] 이전에도 수년 동안 기계 학습 분야에서 존재해 왔지만, 그 이후로, 특히 트랜스포머 모델과 LLM의 맥락에서 인기를 얻고 있다.

LLM에서 MoE는 게이팅/가중치 함수가 할당된 가중치가 낮은 곳이면 전문가의 일부가 꺼질 때 추론 중에 부분적으로 인스턴스화되는 것보다 매우 큰 모델을 훈련할 수 있게 한다. 예로서, GLaM 모델은 1.2조 개의 파라미터를 갖지만, 추론 동안 64명의 전문가 중 2명만이 사용된다[84].

MoE는 오늘날 소위 프론티어 LLM(즉, 가장 진보되고 유능한 모델)의 중요한 구성요소이다. GPT-4 자체는 MoE 아키텍처를 기반으로 한다는 소문이 있으며, Mixtral [117]과 같은 가장 성능이 좋은 LLM 중 일부는 기본적으로 기존 LLM의 MoE 버전이다.

마지막으로, MoE가 주의력에 기초하는지 여부에 관계없이 임의의 아키텍처의 컴포넌트로서 사용될 수 있다는 점에 유의하는 것이 중요하다. 실제로 MoE는 맘바 시테피오로2024모엠바와 같은 SSM 기반 LLM에도 적용되었다. 우리는 근본적인 아키텍처에 관계없이 미래에 MoE 주도의 개선을 계속 보아야 한다.

### _Multi-modal Models_

향후 LLM은 멀티모달로 텍스트, 이미지, 비디오, 오디오 등 다양한 데이터 유형을 통일적으로 처리할 것으로 예상된다. 이는 질문 응답, 콘텐츠 생성, 창작 예술, 의료, 로봇 공학 및 그 밖의 분야에서 보다 다양한 응용 분야에 대한 가능성을 열어준다. LLAVA [214], LLAVA-Plus [215], GPT-4 [33], Qwen-vl [116], Next-GPT [216] 등 이미 몇 가지 두드러진 다중 모드 LLM이 있지만 추세는 계속될 것으로 예상된다. 이러한 모델의 평가는 또한 새로운 연구 주제, 특히 대화형 생성 비전 모델이다[217]. 멀티모달 LLM은 다양한 작업에서 거대한 잠재력을 해제할 수 있으며 이 방향으로 이미 하강 진전이 있어 모든 세부 사항을 논의하기 위한 전용 논문이 필요하다.

### _개선된 LLM 사용 및 확장 기술_

섹션 IV에서 설명한 바와 같이 _환각_과 같은 LLM의 많은 단점과 한계는 고급 신속한 엔지니어링, 도구 사용 또는 기타 증강 기술을 통해 해결할 수 있다. 우리는 이 분야에 대한 지속적인 연구뿐만 아니라 가속화된 연구를 기대해야 한다. 소프트웨어 엔지니어링의 특정 사례에서 일부 작업([218])이 전체 소프트웨어 엔지니어링 워크플로우에서 이 문제를 자동으로 제거하려고 시도했음을 언급할 가치가 있다.

LLM 기반 시스템은 이미 최근까지 다른 접근법을 사용하던 기계 학습 시스템을 대체하기 시작했다. 이것의 명확한 예로서, LLM은 이제 사람들의 선호 및 관심사를 더 잘 이해하고, 고객 서비스, 콘텐츠 추천 또는 다른 애플리케이션에서든 더 개인화된 상호작용을 제공하기 위해 배포되고 있다. 이것은 사용자 선호도에 대한 더 나은 이해와 그들의 과거 상호작용을 분석하고 컨텍스트로 사용하는 것을 포함한다. 우리는 계속해서 개인화 및 권장 사항뿐만 아니라 다른 기계 학습 기술을 사용하는 많은 다른 응용 분야에 대한 LLM의 응용 및 사용에 대한 연구를 볼 것이다.

마지막으로, 더 많은 관심을 끌 것으로 예상되는 또 다른 중요한 연구 분야는 _LLM 기반 에이전트 및 다중 에이전트 시스템_[172, 173, 174]이다. 외부 도구에 대한 접근과 의사 결정 기능을 갖춘 LLM 시스템의 개발은 흥미롭고 도전적이다. 우리는 일각에서 인공지능(AGI)으로 이어질 수 있다고 주장하는 이 중요한 분야에 대한 지속적인 연구와 진전을 볼 것이다.

### _보안 및 윤리적/책임적 AI_

적대적 공격 및 기타 취약성에 대한 LLM의 견고성과 보안을 보장하는 것은 연구의 중요한 영역이다[219]. LLM이 실제 응용 프로그램에 점점 더 많이 배치됨에 따라, LLM은 사람들을 조작하거나 잘못된 정보를 퍼뜨리는 데 사용되는 것을 방지하기 위해 잠재적인 위협으로부터 보호되어야 한다.

LLM의 윤리적 우려와 편향을 해결하는 것은 또 다른 활발한 연구 영역이다. LLM이 공정하고 편향되지 않으며 민감한 정보를 책임감 있게 처리할 수 있도록 하기 위한 노력이 이루어지고 있다. LLM은 매일 많은 사람들이 점점 더 많이 사용하고 있기 때문에, 그들이 편견 없이 책임감 있게 행동하는지 확인하는 것이 중요하다.

## VIII Conclusion

이 논문은 지난 몇 년 동안 개발된 LLMs에 대한 조사를 제시한다. 먼저 초기 사전 훈련 언어 모델(예: BERT)에 대한 개요를 제공한 다음 세 가지 인기 LLM 패밀리(GPT, LLaMA, PaLM) 및 기타 대표적인 LLM을 검토한다. 그런 다음 LLM을 구축, 증강 및 사용하는 방법과 기술을 조사한다. 우리는 인기 있는 LLM 데이터 세트와 벤치마크를 검토하고 공개 벤치마크에 대한 주요 모델 세트의 성능을 비교한다. 마지막으로, 우리는 열린 도전과 향후 연구 방향을 제시한다.

## References

* [1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," _arXiv preprint arXiv:2001.08361_, 2020.
* [2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark _et al._, "Training compute-optimal large language models," _arXiv preprint arXiv:2203.15556_, 2022.
* [3] C. E. Shannon, "Prediction and entropy of printed english," _Bell system technical journal_, vol. 30, no. 1, pp. 50-64, 1951.
* [4] F. Jelinek, _Statistical methods for speech recognition_. MIT press, 1998.
* [5] C. Manning and H. Schutze, _Foundations of statistical natural language processing_. MIT press, 1999.

* [6] C. D. Manning, _An introduction to information retrieval_. Cambridge university press, 2009.
* [7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong _et al._, "A survey of large language models," _arXiv preprint arXiv:2303.18223_, 2023.
* [8] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He _et al._, "A comprehensive survey on pretrained foundation models: A history from bert to chatpst," _arXiv preprint arXiv:2302.09419_, 2023.
* [9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," _ACM Computing Surveys_, vol. 55, no. 9, pp. 1-35, 2023.
* [10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in-context learning," _arXiv preprint arXiv:2301.00234_, 2022.
* [11] J. Huang and K. C.-C. Chang, "Towards reasoning in large language models: A survey," _arXiv preprint arXiv:2212.10403_, 2022.
* [12] S. F. Chen and J. Goodman, "An empirical study of smoothing techniques for language modeling," _Computer Speech & Language_, vol. 13, no. 4, pp. 359-394, 1999.
* [13] Y. Bengio, R. Ducharme, and P. Vincent, "A neural probabilistic language model," _Advances in neural information processing systems_, vol. 13, 2000.
* [14] H. Schwenk, D. Dechelotte, and J.-L. Gauvain, "Continuous space language models for statistical machine translation," in _Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions_, 2006, pp. 723-730.
* [15] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur, "Recurrent neural network based language model." in _Interspeech_, vol. 2, no. 3. Makhunai, 2010, pp. 1045-1048.
* [16] A. Graves, "Generating sequences with recurrent neural networks," _arXiv preprint arXiv:1308.0850_, 2013.
* [17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, "Learning deep structured semantic models for web search using clickthrough data," in _Proceedings of the 22nd ACM international conference on Information & Knowledge Management_, 2013, pp. 2333-2338.
* [18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, _Neural Approaches to Conversational Information Retrieval_. Springer Nature, 2023, vol. 44.
* [19] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," _Advances in neural information processing systems_, vol. 27, 2014.
* [20] K. Cho, B. Van Merrienboer, D. Bahdanau, and Y. Bengio, "On the properties of neural machine translation: Encoder-decoder approaches," _arXiv preprint arXiv:1409.1259_, 2014.
* [21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. C. Plat _et al._, "From captions to visual concepts and back," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015, pp. 1473-1482.
* [22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, "Show and tell: A neural image caption generator," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2015, pp. 3156-3164.
* [23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, "Deep contextualized word representations. corr abs/1802.05365 (2018)," _arXiv preprint arXiv:1802.05365_, 2018.
* [24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," _arXiv preprint arXiv:1810.04805_, 2018.
* [25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," _arXiv preprint arXiv:1907.11692_, 2019.
* [26] P. He, X. Liu, J. Gao, and W. Chen, "Deberta: Decoding-enhanced bert with disentangled attention," _arXiv preprint arXiv:2006.03654_, 2020.
* [27] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang _et al._, "Pre-trained models: Past, present and future," _AI Open_, vol. 2, pp. 225-250, 2021.
* [28] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," _Science China Technological Sciences_, vol. 63, no. 10, pp. 1872-1897, 2020.
* [29] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," 2022.
* [30] A. Gu and T. Dao, "Mambu: Linear-time sequence modeling with selective state spaces," _arXiv preprint arXiv:2312.00752_, 2023.
* [31] A. Chowdhory, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann _et al._, "Palm: Scaling language modeling with pathways," _arXiv preprint arXiv:2204.02311_, 2022.
* [32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacorix, B. Roziere, N. Goyal, E. Hambro, F. Azhar _et al._, "Llama: Open and efficient foundation language models," _arXiv preprint arXiv:2302.13971_, 2023.
* [33] OpenAI, "GPT-4 Technical Report," [https://arxiv.org/pdf/2303.08774v3.pdf](https://arxiv.org/pdf/2303.08774v3.pdf), 2023.
* [34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," in _Advances in Neural Information Processing Systems_, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824-24 837. [Online]. Available: [https://proceedings.neurips.cc/paper_files/paper/2022/file/945069613524ec4f15af0f7b7b3lac84e4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/945069613524ec4f15af0f7b7b3lac84e4-Paper-Conference.pdf)
* [35] G. Mialon, R. Dessl, M. Lomeli, C. Naimipnis, R. Pasunuru, R. Raileanu, B. Roziere, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz _et al._, "Augmented language models: a survey," _arXiv preprint arXiv:2302.07842_, 2023.
* [36] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Lidem, Z. Yu, W. Chen, and J. Gao, "Check your facts and try again: Improving large language models with external knowledge and automated feedback," _arXiv preprint arXiv:2302.12813_, 2023.
* [37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergistic reasoning and acting in language models," _arXiv preprint arXiv:2120.03629_, 2022.
* [38] D. E. Rumelhart, G. E. Hinton, R. J. Williams _et al._, "Learning internal representations by error propagation," 1985.
* [39] J. L. Elman, "Finding structure in time," _Cognitive science_, vol. 14, no. 2, pp. 179-211, 1990.
* [40] M. V. Mahoney, "Fast text compression with neural networks." in _FLAIRS conference_, 2000, pp. 230-234.
* [41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernocky, "Strategies for training large scale neural network language models," in _2011 IEEE Workshop on Automatic Speech Recognition & Understanding_. IEEE, 2011, pp. 196-201.
* [42] tmiikolov, "mlm. [Online]. Available: [https://www.fit.vutbr.cz/~imikolov/mml/](https://www.fit.vutbr.cz/~imikolov/mml/)
* [43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao, "Deep learning-based text classification: a comprehensive review," _ACM computing surveys (CSUR)_, vol. 54, no. 3, pp. 1-40, 2021.
* [44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," _Advances in neural information processing systems_, vol. 30, 2017.
* [45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, "Albert: A lite for self-supervised learning of language representations," _arXiv preprint arXiv:1909.11942_, 2019.
* [46] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, "Electra: Pre-training text encoders as discriminators rather than generators," _arXiv preprint arXiv:2003.10555_, 2020.
* [47] G. Lample and A. Conneau, "Cross-lingual language model pretraining," _arXiv preprint arXiv:1901.07291_, 2019.
* [48] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," _Advances in neural information processing systems_, vol. 32, 2019.
* [49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, "Unified language model pre-training for natural language understanding and generation," _Advances in neural information processing systems_, vol. 32, 2019.
* [50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever _et al._, "Improving language understanding by generative pre-training," 2018.
* [51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, vol. 1, no. 8, p. 9, 2019.
* [52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," _The Journal of Machine Learning Research_, vol. 21, no. 1, pp. 5485-5551, 2020.
* [53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, "mt5: A massively multilingual pre-trained text-to-text transformer," _arXiv preprint arXiv:2010.11934_, 2020.
* [54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, "Mass: Masked sequence to sequence pre-training for language generation," _arXiv preprint arXiv:1905.02450_, 2019.
* [55] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," _arXiv preprint arXiv:1910.13461_, 2019.
* [56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020.
* [57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman _et al._, "Evaluating large language models trained on code," _arXiv preprint arXiv:2107.03374_, 2021.
* [58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders _et al._, "Webgpt: Browser-assisted question-answering with human feedback," _arXiv preprint arXiv:2112.09332_, 2021.
* [59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray _et al._, "Training language models to follow instructions with human feedback," _Advances in Neural Information Processing Systems_, vol. 35, pp. 27 730-27 744, 2022.
* [60] OpenAI. (2022) Introducing chatqpt. [Online]. Available: [https://openai.com/blog/chatqpt](https://openai.com/blog/chatqpt)
* [61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale _et al._, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.
* [62] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, "Alpaca: A strong, replicable instruction-following model," _Stanford Center for Research on Foundation Models. https://crfcn. stanford. edu/2023/03/13/alpaca. html_, vol. 3, no. 6, p. 7, 2023.
* [63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Qlora: Efficient finetuning of quantized llms," _arXiv preprint arXiv:2305.14314_, 2023.
* [64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song, "Koala: A dialogue model for academic research," _Blog post, April_, vol. 1, 2023.
* [65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. I. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier _et al._, "Mistral 7b," _arXiv preprint arXiv:2310.06825_, 2023.
* [66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin _et al._, "Code llama: Open foundation models for code," _arXiv preprint arXiv:2308.12950_, 2023.
* [67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, "Gorilla: Large language model connected with massive apis," 2023.
* [68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and S. Naidu, "Giraffe: Adventures in expanding context lengths in lms," _arXiv preprint arXiv:2308.10882_, 2023.
* [69] B. Huang, "Vigogne: French instruction-following and chat models," [https://github.com/bofenghuang/vigogne](https://github.com/bofenghuang/vigogne), 2023.
* [70] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy _et al._, "How far can camels go? exploring the state of instruction tuning on open resources," _arXiv preprint arXiv:2306.04751_, 2023.
* [71] S. Tworkowski, K. Staniszewski, M. Paacek, Y. Wu, H. Michalewski, and P. Milos, "Focused transformer: Contrastive training for context scaling," _arXiv preprint arXiv:2307.03170_, 2023.
* [72] D. Mahan, R. Carlow, L. Castricato, N. Cooper, and C. Lafotte, "Stable bequa models." [Online]. Available: [[https://huggingface.c/ots/blibii/ai/StableBeluga2](https://huggingface.c/ots/blibii/ai/StableBeluga2)][https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)][https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2))
* [73] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng, J. Rao, A. Chowdhery _et al._, "Transcending scaling laws with 0.1% extra compute," _arXiv preprint arXiv:2210.11399_, 2022.
* [74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma _et al._, "Scaling instruction-finetuned language models," _arXiv preprint arXiv:2210.11416_, 2022.
* [75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen _et al._, "Palm 2 technical report," _arXiv preprint arXiv:2305.10403_, 2023.
* [76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl _et al._, "Large language models encode clinical knowledge," _arXiv preprint arXiv:2212.13138_, 2022.
* [77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal _et al._, "Towards expert-level medical question answering with large language models," _arXiv preprint arXiv:2305.09617_, 2023.
* [78] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," _arXiv preprint arXiv:2109.01652_, 2021.
* [79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young _et al._, "Scaling language models: Methods, analysis & insights from training gopher," _arXiv preprint arXiv:2112.11446_, 2021.
* [80] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja _et al._, "Multi-task prompted training enables zero-shot task generalization," _arXiv preprint arXiv:2110.08207_, 2021.
* [81] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu _et al._, "Emie 3:0: Large-scale knowledge enhanced pre-training for language understanding and generation," _arXiv preprint arXiv:2107.02137_, 2021.
* [82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark _et al._, "Improving language models by retrieving from trillions of tokens," in _International conference on machine learning_. PMLR, 2022, pp. 2206-2240.
* [83] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, "Jurassic-1: Technical details and evaluation," _White Paper. AI21 Labs_, vol. 1, p. 9, 2021.
* [84] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat _et al._, "Glam: Efficient scaling of language models with mixture-of-experts," in _International Conference on Machine Learning_. PMLR, 2022, pp. 5547-5569.
* [85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du _et al._, "Lambda: Language models for dialog applications," _arXiv preprint arXiv:2201.08239_, 2022.
* [86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin _et al._, "Opt: Open pre-trained transformer language models," _arXiv preprint arXiv:2205.01068_, 2022.
* [87] R. Taylor, M. Kardas, G. Cucurull,* [89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky _et al._, "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model," _arXiv preprint arXiv:2208.01448_, 2022.
* [90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker _et al._, "Improving alignment of dialogue agents via targeted human judgements," _arXiv preprint arXiv:2209.14375_, 2022.
* [91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo _et al._, "Solving quantitative reasoning problems with language models," _Advances in Neural Information Processing Systems_, vol. 35, pp. 3843-3857, 2022.
* [92] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, H. S. Zheng, N. Houlsby, and D. Metzler, "Unifying language learning paradigms," _arXiv preprint arXiv:2205.05131_, 2022.
* [93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle _et al._, "Bloom: A 176b-parameter open-access multilingual language model," _arXiv preprint arXiv:2211.05100_, 2022.
* [94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia _et al._, "Glim-130b: An open bilingual pre-trained model," _arXiv preprint arXiv:2210.02414_, 2022.
* [95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff _et al._, "Pythia: A suite for analyzing large language models across training and scaling," in _International Conference on Machine Learning_. PMLR, 2023, pp. 2397-2430.
* [96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah, "Orca: Progressive learning from complex explanation traces of gpt-4," _arXiv preprint arXiv:2306.02707_, 2023.
* [97] R. Li, L. B. Allal, Y. Zi, N. Muenthigth, D. Kocektov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim _et al._, "Stracorder: may the source be with you!" _arXiv preprint arXiv:2305.06161_, 2023.
* [98] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu _et al._, "Language is not all you need: Aligning perception with language models," _arXiv preprint arXiv:2302.14045_, 2023.
* [99] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth _et al._, "Gemini: a family of highly capable multimodal models," _arXiv preprint arXiv:2312.11805_, 2023.
* [100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar _et al._, "Inner monologue: Embodied reasoning through planning with language models," _arXiv preprint arXiv:2207.05608_, 2022.
* [101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti _et al._, "Using deepspeed and megatron to train megatron-turing meg 530b, a large-scale generative language model," _arXiv preprint arXiv:2201.11990_, 2022.
* [102] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long-document transformer," _arXiv preprint arXiv:2004.05150_, 2020.
* [103] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura _et al._, "Opt-iml: Scaling language model instruction meta learning through the lens of generalization," _arXiv preprint arXiv:2212.12017_, 2022.
* [104] Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei, "Language models are general-purpose interfaces," _arXiv preprint arXiv:2206.06366_, 2022.
* [105] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, "Principle-driven self-alignment of language models from scratch with minimal human supervision," _arXiv preprint arXiv:2305.03047_, 2023.
* [106] W. E. team, "Palmyra-base Parameter Autoregressive Language Model," [https://dev.writer.com](https://dev.writer.com), 2023.
* [107] ----, "Camel-5b instructept," [https://dev.writer.com](https://dev.writer.com), 2023.
* [108] Yandex. Yalm. [Online]. Available: [https://github.com/yandex/Yal.M-100B](https://github.com/yandex/Yal.M-100B)
* [109] M. Team _et al._, "Introducing mpt-7b: a new standard for open-source, commercially usable llms," 2023.
* [110] A. Mitra, L. D. Corto, S. Mahajan, A. Codas, C. Simoes, S. Agarwal, X. Chen, A. Razdiabiedina, E. Jones, K. Aggarwal, H. Palangi, G. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, "Orca 2: Teaching small language models how to reason," 2023.
* [111] L. Gao, A. Madan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "Pal: Program-aided language models," in _International Conference on Machine Learning_. PMLR, 2023, pp. 10 764-10 799.
* [112] Anthropic. calude. [Online]. Available: [https://www.anthropic.com/news/introducing-claude](https://www.anthropic.com/news/introducing-claude)
* [113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, "Codegen22: Lessons for training llms on programming and natural languages," _arXiv preprint arXiv:2305.02309_, 2023.
* [114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier, N. Habib _et al._, "Zephyr: Direct distillation of lm alignment," _arXiv preprint arXiv:2310.16944_, 2023.
* [115] X. team. Grok. [Online]. Available: [https://grok.xai/](https://grok.xai/)
* [116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, I. Lin, C. Zhou, and J. Zhou, "Qwen-vl: A frontier large vision-language model with versatile abilities," _arXiv preprint arXiv:2308.12966_, 2023.
* [117] mixtral. mixtral. [Online]. Available: [https://mistral.ai/news/](https://mistral.ai/news/) mixtral-of-experts/
* [118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei, A. Nourbakhsh, and X. Liu, "Docllm: A layout-aware generative language model for multimodal document understanding," 2023.
* 코드 인텔리전스의 상승," 2024.
* [120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, "Knowledge fusion of large language models," 2024.
* [121] P. Zhang, G. Zeng, T. Wang, and W. Lu, "Tinyllama: An open-source small language model," 2024.
* [122] C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan, "Llama pro: Progressive llama with block expansion," 2024.
* [123] X. Amatraina, A. Sankar, J. Bing, P. K. Bodiguta, T. J. Hazen, and M. Kazzi, "Transformer models: an introduction and catalog," 2023.
* [124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, "The refined-web dataset for falcon llm: outperforming curated corpora with web data, and web data only," _arXiv preprint arXiv:2306.01116_, 2016.
* [125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume _et al._, "Scaling laws and interpretability of learning from repeated data," _arXiv preprint arXiv:2205.10487_, 2022.
* [126] P. Shaw, J. Uszkoreit, and A. Vaswani, "Self-attention with relative position representations," _arXiv preprint arXiv:1803.02155_, 2018.
* [127] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, "Roformer: Enhanced transformer with rotary position embedding," _arXiv preprint arXiv:2104.09864_, 2021.
* [128] O. Press, N. A. Smith, and M. Lewis, "Train short, test long: Attention with linear biases enables input length extrapolation," _arXiv preprint arXiv:2108.12409_, 2021.
* [129] G. Ke, D. He, and T.-Y. Liu, "Rethinking positional encoding in language pre-training," _arXiv preprint arXiv:2006.15595_, 2020.
* [130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," _arXiv preprint arXiv:1701.06538_, 2017.
* [131] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," _The Journal of Machine Learning Research_, vol. 23, no. 1, pp. 5232-5270, 2022.
* [132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J * [134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, "Cross-task generalization via natural language crowdsourcing instructions," _arXiv preprint arXiv:2104.08773_, 2021.
* [135] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, "Self-instruct: Aligning language model with self generated instructions," _arXiv preprint arXiv:2212.10560_, 2022.
* [136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online]. Available: [https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
* [137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, "Deep reinforcement learning from human preferences," _Advances in neural information processing systems_, vol. 30, 2017.
* [138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbone, and A. Rastogi, "Rlafi: Scaling reinforcement learning from human feedback with ai feedback," _arXiv preprint arXiv:2309.00267_, 2023.
* [139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, "Direct preference optimization: Your language model is secretly a reward model," _arXiv preprint arXiv:2305.18290_, 2023.
* [140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, "Zero: Memory optimizations toward training trillion parameter models," in _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_. IEEE, 2020, pp. 1-16.
* [141] B. Peng, E. Alcalide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV _et al._, "Rwkv: Reinventing rnns for the transformer era," _arXiv preprint arXiv:2305.13048_, 2023.
* [142] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," _arXiv preprint arXiv:2106.09685_, 2021.
* [143] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," _arXiv preprint arXiv:1503.02531_, 2015.
* [144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, "Knowledge distillation: A survey," _International Journal of Computer Vision_, vol. 129, pp. 1789-1819, 2021.
* [145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, "Survey of hallucination in natural language generation," _ACM Comput. Surv._, vol. 55, no. 12, mar 2023. [Online]. Available: [https://doi.org/10.1145/3571730](https://doi.org/10.1145/3571730)
* [146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and M. Steedman, "Sources of hallucination by large language models on inference tasks," 2023.
* [147] C.-Y. Lin, "ROUGE: A package for automatic evaluation of summaries," in _Text Summarization Branches Out_. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74-81. [Online]. Available: [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)
* [148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, P. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, Jul. 2002, pp. 311-318. [Online]. Available: [https://aclanthology.org/P02-1040](https://aclanthology.org/P02-1040)
* [149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and W. Cohen, "Handling divergent reference texts when evaluating table-to-text generation," in _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, A. Korhonen, D. Traum, and L. Matrieuz, Eds. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 4884-4895. [Online]. Available: [https://aclanthology.org/P19-1483](https://aclanthology.org/P19-1483)
* [150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, "Towards faithful neural table-to-text generation with content-matching constraints," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, Eds. Online: Association for Computational Linguistics, Jul. 2020, pp. 1072-1086. [Online]. Available: [https://aclanthology.org/2020.acl-main.101](https://aclanthology.org/2020.acl-main.101)
* [151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, "Generating person consistent dialogues by exploiting natural language inference," _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 34, no. 05, pp. 8878-8885, Apr. 2020.
* [152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, and O. Abend, "\(q^{2}\): Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, M.-F. Moons, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 7856-7870. [Online]. Available: [https://aclanthology.org/2021.emnlp-main.619](https://aclanthology.org/2021.emnlp-main.619)
* [153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, "Evaluating attribution in dialogue systems: The BEGIN benchmark," _Transactions of the Association for Computational Linguistics_, vol. 10, pp. 1066-1068, 2022. [Online]. Available: [https://aclanthology.org/2022.tacl-1.62](https://aclanthology.org/2022.tacl-1.62)
* [154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim, Y. Liu, and D. Z. Hakkani-Tur, "Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation," _ArXiv_, vol. abs/2110.05456, 2021.
* [155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi, "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation," 2023.
* [156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young, "Machine learning: The high interest credit card of technical debt," in _SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)_, 2014.
* [157] Z. Zhang, A. Zhang, M. Li, and A. Smola, "Automatic chain of thought prompting in large language models," 2022.
* [158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," 2023.
* [159] P. Manakul, A. Liuise, and M. J. F. Gales, "Selfcheckkgtr: Zero-resource black-box hallucination detection for generative large language models," 2023.
* [160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao, "Reflexion: Language agents with verbal reinforcement learning," 2023.
* [161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang, K. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi, A. Solar-Lezama, and I. Drori, "Exploring the mit mathematics and ecces curriculum using large language models," 2023.
* [162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai, "Prompt chainer: Chaining large language model prompts through visual programming," 2022.
* [163] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, "Large language models are human-level prompt engineers," 2023.
* [164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela, "Retrieval-augmented generation for knowledge-intensive NLP tasks," _CoRR_, vol. abs/2005.11401, 2020. [Online]. Available: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)
* [165] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang, "Retrieval-augmented generation for large language models: A survey," _arXiv preprint arXiv:2312.10997_, 2023.
* [166] A. W. Services. (Year of publication, e.g., 2023) Question answering using retrieval augmented generation with foundation models in amazon asgemaker jumpstart. Accessed: Date of access, e.g., December 5, 2023. [Online]. Available: [https://shorturl.at/d/5v47](https://shorturl.at/d/5v47)
* [167] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, "Unifying large language models and knowledge graphs: A roadmap," _arXiv preprint arXiv:2306.08302_, 2023.
* [168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig, "Active retrieval augmented generation," 2023.
* [169] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," 2023.
* [170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, and M. T. Ribeiro, "Art: Automatic multi-step reasoning and tool-use for large language models," 2023.
* [171] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, "Huggingnet: Solving ai tasks with chatgpt and its friends in huggingface," _arXiv preprint arXiv:2303.17580_, 2023.

* [172] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou _et al._, "The rise and potential of large language model based agents: A survey," _arXiv preprint arXiv:2309.07864_, 2023.
* [173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin _et al._, "A survey on large language model based autonomous agents," _arXiv preprint arXiv:2308.11432_, 2023.
* [174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar, R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, and J. Gao, "Agent ai: Surveying the horizons of multimodal interaction," _arXiv preprint arXiv:2401.03568_, 2024.
* [175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, "Rewoo: Decoupling reasoning from observations for efficient augmented language models," 2023.
* [176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergizing reasoning and acting in language models," 2023.
* [177] V. Nair, E. Schumacher, G. Tso, and A. Kannan, "Dera: Enhancing large language model completions with dialog-enabled resolving agents," 2023.
* [178] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, "A survey on evaluation of large language models," 2023.
* [179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov, "Natural questions: A benchmark for question answering research," _Transactions of the Association for Computational Linguistics_, vol. 7, pp. 452-466, 2019. [Online]. Available: [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026)
* [180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, "Measuring massive multitask language understanding," 2021.
* [181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le _et al._, "Program synthesis with large language models," _arXiv preprint arXiv:2108.07732_, 2021.
* [182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and L. Zettlemoyer, "QuAC: Question answering in context," in _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsuji, Eds. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2174-2184. [Online]. Available: [https://aclanthology.org/D18-1241](https://aclanthology.org/D18-1241)
* [183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Paranik, H. He, D. Song, and J. Steinhardt, "Measuring coding challenge competence with apps," _NeurIPS_, 2021.
* [184] V. Zhong, C. Xiong, and R. Socher, "Seq2sql: Generating structured queries from natural language using reinforcement learning," _arXiv preprint arXiv:1709.00103_, 2017.
* [185] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension," in _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601-1611. [Online]. Available: [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147)
* [186] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, "RACE: Large-scale ReAding comprehension dataset from examinations," in _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 785-794. [Online]. Available: [https://aclanthology.org/D17-1082](https://aclanthology.org/D17-1082)
* [187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, "SQuAD: 100.000+ questions for machine comprehension of text," in _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383-2392. [Online]. Available: [https://aclanthology.org/D16-1264](https://aclanthology.org/D16-1264)
* [188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, "Bool: Exploring the surprising difficulty of natural yes/no questions," _CoRR_, vol. abs/1905.10044, 2019. [Online]. Available: [http://arxiv.org/abs/1905.10044](http://arxiv.org/abs/1905.10044)
* [189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth, "Looking beyond the surface:a challenge set for reading comprehension over multiple sentences," in _Proceedings of North American American Conference of the Association for Computational Linguistics (NAACL)_, 2018.
* [190] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, "Training verifiers to solve math word problems," _CoRR_, vol. abs/2110.14168, 2021. [Online]. Available: [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168)
* [191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, "Measuring mathematical problem solving with the MATH dataset," _CoRR_, vol. abs/2103.03874, 2021. [Online]. Available: [https://arxiv.org/abs/2103.03874](https://arxiv.org/abs/2103.03874)
* [192] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "Hellaswag: Can a machine really finish your sentence?" 2019.
* [193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, "Think you have solved question answering? try arc, the AI2 reasoning challenge," _CoRR_, vol. abs/1803.05457, 2018. [Online]. Available: [http://arxiv.org/abs/1803.05457](http://arxiv.org/abs/1803.05457)
* [194] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, "PIQA: reasoning about physical commonsense in natural language," _CoRR_, vol. abs/1911.11641, 2019. [Online]. Available: [http://arxiv.org/abs/1911.11641](http://arxiv.org/abs/1911.11641)
* [195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, "Socialiqa: Commonsense reasoning about social interactions," _CoRR_, vol. abs/1904.09728, 2019. [Online]. Available: [http://arxiv.org/abs/1904.09728](http://arxiv.org/abs/1904.09728)
* [196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, "Can a suit of armor conduct electricity? A new dataset for open book question answering," _CoRR_, vol. abs/1809.02789, 2018. [Online]. Available: [http://arxiv.org/abs/1809.02789](http://arxiv.org/abs/1809.02789)
* [197] S. Lin, J. Hilton, and O. Evans, "Truthfulqa: Measuring how models mimic human falsehoods," _arXiv preprint arXiv:2109.07958_, 2021.
* [198] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning, "Hotpotqa: A dataset for diverse, explainable multi-hop question answering," _CoRR_, vol. abs/1809.09600, 2018. [Online]. Available: [http://arxiv.org/abs/1809.09600](http://arxiv.org/abs/1809.09600)
* [199] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, "Toolqa: A dataset for llm question answering with external tools," _arXiv preprint arXiv:2306.13304_, 2023.
* [200] D. Chen, J. Bolton, and C. D. Manning, "A thorough examination of the cnn/daily mail reading comprehension task," in _Association for Computational Linguistics (ACL)_, 2016.
* [201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang _et al._, "Abstractive text summarization using sequence-to-sequence rnns and beyond," _arXiv preprint arXiv:1602.06023_, 2016.
* [202] Y. Bai and D. Z. Wang, "More than reading comprehension: A survey on datasets and metrics of textual question answering," _arXiv preprint arXiv:2109.12264_, 2021.
* [203] H.-Y. Huang, E. Choi, and W.-t. Yih, "Flowqa: Grasping flow in history for conversational machine comprehension," _arXiv preprint arXiv:1810.06683_, 2018.
* [204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, "A survey on evaluation metrics for machine translation," _Mathematics_, vol. 11, no. 4, p. 1006, 2023.
* [205] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, "Halueval: A large-scale hallucination evaluation benchmark for large language models," in _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, 2023, pp. 6449-6464.
* [206] Simon Mark Hughes, "Hughes hallucination evaluation model (hlem) leaderboard," 2024, [https://huggingface.co/spaces/vectar/Hallucination-evaluation-leaderboard](https://huggingface.co/spaces/vectar/Hallucination-evaluation-leaderboard), Last accessed on 2024-01-21.
* [207] J. Kadodur, J. Harris, M. Mozes, H. Bradley, R. Railleau, and R. McHardy, "Challenges and applications of large language models," _arXiv preprint arXiv:2307.10169_, 2023.
* [208] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaherpi, P. Kauffmann, G. de Rosa, O. Saarikivi* [209] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee, "Textbooks are all you need ii: phi-1.5 technical report," _arXiv preprint arXiv:2309.05463_, 2023.
* [210] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re, "Hyena hierarchy: Towards larger convolutional language models," 2023.
* [211] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and A. Thomas, "StripedHyeena: Moving Beyond Transformers with Hybrid Signal Processing Models," 12 2023. [Online]. Available: [https://github.com/togethercomputer/stripedhgena](https://github.com/togethercomputer/stripedhgena)
* [212] D. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Fayboglu, A. W. Thomas, B. Spector, M. Poli, A. Rudra, and C. Re, "Monarch mixer: A simple sub-quadratic gemm-based architecture," 2023.
* [213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, "Finite mixture models," _Annual review of statistics and its application_, vol. 6, pp. 355-378, 2019.
* [214] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," _arXiv preprint arXiv:2304.08485_, 2023.
* [215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, "Llava-plus: Learning to use tools for creating multimodal agents," _arXiv preprint arXiv:2311.05437_, 2023.
* [216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, "Next-gpt: Any-to-any multimodal llm," _arXiv preprint arXiv:2309.05519_, 2023.
* [217] N. N. Khasmaki, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and D. Zihike, "Convexing: Evaluation of conversational generative vision models," 2023.
* [218] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman, I. Harper, A. Marginean, S. Sengupta, and E. Wang, "Automated unit test improvement using large language models at meta," _arXiv preprint arXiv:2402.09171_, 2024.
* [219] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li _et al._, "Trustllm: Trustworthiness in large language models," _arXiv preprint arXiv:2401.05561_, 2024.
* [220] Microsoft. Deepspeed. [Online]. Available: [https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
* [221] HuggingFace. Transformers. [Online]. Available: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)
* [222] Nvidia. Megatron. [Online]. Available: [https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
* [223] BMTrain. Bmtrain. [Online]. Available: [https://github.com/OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain)
* [224] EleutherAI. gpt-neox. [Online]. Available: [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox)
* [225] microsoft. Lora. [Online]. Available: [https://github.com/microsoft/LoRA](https://github.com/microsoft/LoRA)
* [226] ColossalAI. Colossalai. [Online]. Available: [https://github.com/bioaiteatch/ColossalAI](https://github.com/bioaiteatch/ColossalAI)
* [227] FastChat. FastChat. [Online]. Available: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)
* [228] skypilot. skypilot. [Online]. Available: [https://github.com/skypilot-org/skypilot](https://github.com/skypilot-org/skypilot)
* [229] vllm. vllm. [Online]. Available: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)
* [230] huggingface. text-generation-inference. [Online]. Available: [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference)
* [231] langchain. langchain. [Online]. Available: [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)
* [232] bentoml. Openllm. [Online]. Available: [https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)
* [233] embedchain. embbedchain. [Online]. Available: [https://github.com/embedchain](https://github.com/embedchain)
* [234] microsoft. autogen. [Online]. Available: [https://github.com/microsoft/autogen](https://github.com/microsoft/autogen)
* [235] babyagi. babyagi. [Online]. Available: [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi)
* [236] guidance. guidance. [Online]. Available: [https://github.com/guidance-ai/guidance](https://github.com/guidance-ai/guidance)
* [237] promptiools. prompttools. [Online]. Available: [https://github.com/hegelai/promptools](https://github.com/hegelai/promptools)
* [238] promptfoo. promptfoo. [Online]. Available: [https://github.com/promptfoo/promptfoo](https://github.com/promptfoo/promptfoo)
* [239] facebook. faiss. [Online]. Available: [https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)
* [240] mixus. mixus. [Online]. Available: [https://github.com/milvus-io/mixus](https://github.com/milvus-io/mixus)
* [241] qdrant. qdrant. [Online]. Available: [https://github.com/qdrant/qdrant](https://github.com/qdrant/qdrant)
* [242] weaviate. weaviate. [Online]. Available: [https://github.com/weaviate/weaviate](https://github.com/weaviate/weaviate)
* [243] Ilama index. Ilama-index. [Online]. Available: [https://github.com/run-lama/Ilama_index](https://github.com/run-lama/Ilama_index)

## 부록 LLM 개발 및 배포를 위한 오픈 소스 도구

LLM 교육, 평가 및 배치를 위해 개발된 다양한 프레임워크와 라이브러리가 있으며, 모든 프레임워크를 포괄하는 것은 본 논문의 범위를 벗어난다. 그러나 우리는 다른 범주로 그룹화된 가장 인기 있는 몇 가지에 대한 간략한 소개를 제공하고자 한다.

### _LLM Training/Inference Frameworks_

LLM 훈련에 유용한 인기 프레임워크 중 일부는 다음을 포함한다(그들 중 일부는 LLM 훈련을 넘어 사용될 수 있다는 점에 유의함):

**DeepSpeed**[220]은 분산 학습 및 추론을 쉽고 효율적이며 효과적으로 만드는 딥러닝 최적화 라이브러리입니다. 딥스피드는 MT-530B 및 BLOOM과 같은 세계에서 가장 강력한 언어 모델을 가능하게 합니다. 훈련과 추론 모두에 전례 없는 규모와 속도를 제공하는 사용하기 쉬운 딥러닝 최적화 소프트웨어 제품군입니다. 딥스피드를 이용하면

**트랜스포머**[221]는 텍스트, 비전 및 오디오와 같은 다양한 모달리티에서 작업을 수행할 수 있도록 수천 개의 사전 훈련된 모델을 제공하는 HuggingFace의 라이브러리입니다. 사전 훈련된 모델을 사용하면 컴퓨팅 비용, 탄소 발자국을 줄이고 모델을 처음부터 훈련하는 데 필요한 시간과 자원을 절약할 수 있다.

**메가트론-LM**[222]는 NVIDIA의 응용 심층 학습 연구 팀에서 개발한 크고 강력한 변압기입니다. 이 모델은 GPT, BERT, T5와 같은 변압기 기반 모델의 효율적인 모델-병렬(텐서, 시퀀스, 파이프라인) 및 다중 노드 사전 훈련을 혼합 정밀도를 사용하여 포함한다.

**BMTrain**[223]은 수백억 개의 매개 변수를 사용 하 여 대형 모델을 훈련 하는 데 사용할 수 있는 효율적인 대형 모델 훈련 도구 키트입니다. 독립형 훈련처럼 코드를 단순하게 유지하면서 분산 방식으로 모델을 훈련할 수 있습니다.

**GPT-NeoX**[224]는 인기 있는 메가트론-딥스피드 라이브러리와 동일한 기능 및 기술을 많이 활용하지만 사용성과 신규 최적화가 크게 향상되었습니다.

**LoRA**[225] 라이브러리는 대규모 언어 모델의 낮은 순위 적응을 지원합니다. 이는 원래 가중치를 동결하면서 순위 분해 행렬 쌍을 학습하여 훈련 가능한 매개 변수의 수를 줄인다. 이는 특정 작업에 적응된 대규모 언어 모델에 대한 스토리지 요구 사항을 크게 감소시키고 추론 대기 시간을 도입하지 않고 모두 배포 동안 효율적인 작업 전환을 가능하게 한다. LoRA는 어댑터, 프리픽스 조정 및 미세 조정을 포함한 여러 다른 적응 방법보다 성능이 우수하다.

**ColossalAI** 라이브러리 [226]은 병렬 구성 요소의 컬렉션을 제공합니다. 개발자가 노트북에 자신의 모델을 쓰는 것처럼 분산 딥러닝 모델을 쓸 수 있도록 지원하는 것을 목표로 한다. 그들은 분산된 훈련과 추론을 몇 줄로 시작할 수 있는 사용자 친화적인 도구를 제공한다. 병렬화 전략으로는 데이터병렬, 파이프라인병렬, 시퀀스병렬, Zero Redundancy Optimizer(ZeRO)[140], Auto-Parallelism을 지원한다.

### _Deployment Tools_

우리는 여기에서 가장 인기 있는 LLM 배포 도구 중 일부에 대한 개요를 제공한다.

**FastChat**[227]은 대규모 언어 모델 기반 챗봇을 교육, 서비스 및 평가하기 위한 개방형 플랫폼입니다. FastChat의 핵심 기능은 다음과 같습니다. 최신 모델 (예: Vicuna, MT-Bench)에 대 한 교육 및 평가 코드 및 웹 UI 및 OpenAI 호환 RESTful API를 사용 하는 분산 다중 모델 서빙 시스템입니다.

**Skypilot**[228]은 모든 클라우드에서 LLMs, AI 및 배치 작업을 실행 하 여 최대 비용 절감, GPU 가용성 및 관리 되는 실행을 제공 하는 프레임워크입니다.

**vLLM**[229]는 LLM 추론 및 서비스를 위해 빠르고 사용하기 쉬운 라이브러리입니다. vLLM은 Aquila, Baichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT BigCode, LLaMA, LLaMA 2, Mistral, Mistral, MPT, OPT, Qwen, Yi 등 다양한 Hugging Face 모델을 원활하게 지원합니다.

**텍스트 생성 추론**[230]은 LMM(대용량 언어 모델)을 배포하고 서비스하기 위한 도구 키트입니다. TGI는 라마, 팔콘, 스타코더, BLOOM, GPT-NeoX 등을 포함한 가장 인기 있는 오픈 소스 LLM에 대해 고성능 텍스트 생성을 가능하게 한다.

**LangChain**[231]은 언어 모델로 구동되는 애플리케이션을 개발하기 위한 프레임워크입니다. 를 포함하는 애플리케이션들을 가능하게 한다.

* 상황 인식: 언어 모델을 컨텍스트 원본에 연결 합니다 (프롬프트 명령, 몇 가지 샷 예제, 응답 접지를 위한 콘텐츠 등).
* 이유: 언어 모델에 의존 하 여 추론 (제공 된 컨텍스트에 따라 응답 하는 방법, 수행할 작업 등에 대해) 합니다.

**OpenLLM**[232]는 실제 응용 프로그램에서 대규모 언어 모델 (LLM)의 배포 및 작동을 용이하게 하도록 설계 된 오픈 소스 플랫폼입니다. OpenLLM을 사용하면 모든 오픈 소스 LLM에서 추론을 실행하고 클라우드 또는 온-프레미스에 배포하고 강력한 AI 애플리케이션을 빌드할 수 있습니다.

**Embedchain**[233]은 AI 앱을 쉽게 만들고 배포할 수 있는 오픈 소스 RAG 프레임워크입니다. 임베디드 체인은 RAG 애플리케이션 생성을 간소화하여 다양한 유형의 비정형 데이터를 관리하기 위한 끊김 없는 프로세스를 제공한다. 이는 효율적으로 데이터를 관리 가능한 청크로 분할하고 관련 임베딩을 생성하여 벡터 데이터베이스에 저장하여 최적화된 검색을 수행한다.

**Autogen**[234]는 작업을 해결하기 위해 서로 대화할 수 있는 여러 에이전트를 사용하여 LLM 애플리케이션을 개발할 수 있도록 하는 프레임워크입니다. 오토젠 에이전트는 사용자 지정이 가능하고 볼록하며 사람이 원활하게 참여할 수 있습니다. LLM, 인간 입력 및 도구의 조합을 사용하는 다양한 모드로 작동할 수 있습니다.

**BabyAGI**[235]는 지정된 목표에 따라 작업을 생성하고 실행하도록 설계된 자율적 인공지능 에이전트입니다. 오픈AI, 핀콘, 랑체인, 크로마의 최첨단 기술을 활용하여 작업을 자동화하고 특정 목표를 달성합니다. 이 블로그 게시물에서 BabyAGI의 고유한 기능에 대해 살펴보고 작업 자동화를 간소화할 수 있는 방법을 탐색할 것입니다.

### _Prompting Libraries_

**지침**[236]은 기존 프롬프트 및 연결에 비해 우수한 제어 및 효율성을 제공하는 프로그래밍 패러다임입니다. 이를 통해 사용자는 생성(예: regex 및 CFG)을 제한할 뿐만 아니라 제어(조건부, 루프) 및 생성을 원활하게 인터리빙할 수 있습니다.

**PromptTools**[237]는 LLM, 벡터 데이터베이스 및 프롬프트를 실험, 테스트 및 평가하기 위한 오픈 소스 자체 호스팅 가능한 도구 세트를 제공합니다. 핵심 아이디어는 개발자가 코드, 노트북 및 지역 놀이터와 같은 친숙한 인터페이스를 사용하여 평가할 수 있도록 하는 것입니다.

**PromptBench**[29]는 LMM(대용량 언어 모델 평가용 Pytorch 기반 Python 패키지입니다. 연구자가 LLM에 대한 평가를 수행할 수 있도록 사용자 친화적인 API를 제공한다.

**Promptfoo**[238]은 LLM 출력 품질을 테스트하고 평가하는 도구입니다. 미리 정의된 테스트 케이스로 프롬프트, 모델 및 RAG를 체계적으로 테스트합니다.

### _VectorDB_

**Faiss**[239]는 Facebook AI 리서치가 개발한 라이브러리로 밀도 벡터의 효율적인 유사성 검색 및 클러스터링을 제공합니다. 대규모 고차원 데이터와 함께 사용하도록 설계되었으며 다양한 사용 사례에 대한 여러 인덱스 유형과 알고리즘을 지원합니다.

**Milvus**[240]은 유사성 검색 및 AI 응용 프로그램에 전원을 공급 하기 위해 빌드된 오픈 소스 벡터 데이터베이스입니다. Milvus는 비정형 데이터 검색에 더 쉽게 액세스할 수 있도록 하며 배포 환경에 관계없이 일관된 사용자 환경을 제공합니다.

**Qdrant**[241]은 벡터 유사성 검색 엔진 및 벡터 데이터베이스입니다. 포인트를 저장, 검색 및 관리할 수 있는 편리한 API로 프로덕션 준비 서비스를 제공합니다. 추가 페이로드가 있는 벡터는 확장 필터링 지원에 맞게 조정됩니다. 환경.

**Weaviate**[242]는 고차원 데이터에서 유사성 검색을 가능하게 하는 오픈 소스 GraphQL 기반 벡터 검색 엔진입니다. 오픈 소스이지만 상용 버전은 추가 기능, 지원 및 관리 서비스를 제공합니다.

다른 인기 있는 옵션 중 일부에는 **LlamaIndex**[243] 및 **Pinecone** 이 포함됩니다.

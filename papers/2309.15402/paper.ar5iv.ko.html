<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.15402] A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future</title><meta property="og:description" content="Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing.
However, there still remain…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.15402">

<!--Generated on Wed Feb 28 04:09:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zheng Chu<sup id="15.15.6" class="ltx_sup"><span id="15.15.6.1" class="ltx_text ltx_font_italic">1</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>,
Jingchang Chen<sup id="16.16.7" class="ltx_sup"><span id="16.16.7.1" class="ltx_text ltx_font_italic">1</span></sup><span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>,
Qianglong Chen<sup id="17.17.8" class="ltx_sup"><span id="17.17.8.1" class="ltx_text ltx_font_italic">2</span></sup>,
Weijiang Yu<sup id="18.18.9" class="ltx_sup"><span id="18.18.9.1" class="ltx_text ltx_font_italic">2</span></sup>,
Tao He<sup id="19.19.10" class="ltx_sup"><span id="19.19.10.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><span id="id11.11.5" class="ltx_text ltx_font_bold">
Haotian Wang<sup id="id11.11.5.1" class="ltx_sup"><span id="id11.11.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Weihua Peng<sup id="id11.11.5.2" class="ltx_sup"><span id="id11.11.5.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>, Ming Liu<sup id="id11.11.5.3" class="ltx_sup"><span id="id11.11.5.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1†</span></sup>, Bing Qin<sup id="id11.11.5.4" class="ltx_sup"><span id="id11.11.5.4.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Ting Liu<sup id="id11.11.5.5" class="ltx_sup"><span id="id11.11.5.5.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>
</span> 
<br class="ltx_break"><sup id="20.20.11" class="ltx_sup"><span id="20.20.11.1" class="ltx_text ltx_font_italic">1</span></sup>Harbin Institute of Technology, Harbin, China 
<br class="ltx_break"><sup id="21.21.12" class="ltx_sup"><span id="21.21.12.1" class="ltx_text ltx_font_italic">2</span></sup>Huawei Inc., Shenzhen, China 
<br class="ltx_break"><span id="22.22.13" class="ltx_text ltx_font_typewriter">{zchu, jcchen, the, mliu , qinb, tliu}@ir.hit.edu.cn </span> 
<br class="ltx_break"><span id="23.23.14" class="ltx_text ltx_font_typewriter">{chenqianglong.ai, wanght1998, weijiangyu8, pengwh.hit}@gmail.com</span>
</span><span class="ltx_author_notes">&nbsp;&nbsp;&nbsp; Equal Contribution.<span id="24.24.1" class="ltx_text ltx_font_typewriter">&nbsp;&nbsp;&nbsp;</span><span id="25.25.2" class="ltx_text ltx_font_typewriter"> Corresponding Author.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="26.1">인간의 지능에 기초하는 인지 과정인 연쇄 사고 추론은 인공 지능과 자연 언어 처리의 영역에서 상당한 주목을 받았다. 그러나, 이 경기장에 대한 종합적인 조사는 여전히 부족하다. 이를 위해 첫발을 내딛고 이 연구 분야에 대한 철저한 조사를 신중하고 폭넓게 제시한다. 우리는 X-of-Thought을 넓은 의미에서 Chain-of-Thought을 지칭하기 위해 사용한다. 세부적으로, 우리는 XoT 구성, XoT 구조 변형 및 향상된 XoT를 포함한 방법의 분류학에 따라 현재 연구를 체계적으로 정리한다. 또한 계획, 도구 사용 및 증류를 다루는 프런티어 애플리케이션이 있는 XoT에 대해 설명합니다. 또한, 우리는 도전을 다루고 충실성, 멀티모달 및 이론을 포함한 몇 가지 향후 방향에 대해 논의한다. 우리는 이 조사가 연쇄 사상 추론<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Resources are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/zchuz/CoT-Reasoning-Survey" target="_blank" title="">https://github.com/zchuz/CoT-Reasoning-Survey</a></span></span></span>의 영역 내에서 혁신을 추구하는 연구자들에게 귀중한 자원이 되기를 바란다.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1" style="color:#000000;">Pre-trained language models (PLMs) can automatically learn general representation from unlabeled text and achieve excellent performance on fine-tuning on downstream tasks. <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>; Raffel et al., <a class="ltx_ref" href="#bib.bib121" title="">2020</a>; Radford and Narasimhan, <a class="ltx_ref" href="#bib.bib119" title="">2018</a>)</cite>. 최근 언어 모델을 확장하면 성능이 크게 향상되고 창발 능력<cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib160" title="">2022a</a>; Schaeffer et al., <a class="ltx_ref" href="#bib.bib127" title="">2023</a>)</cite>와 같은 많은 놀라움을 가져온다. 따라서 자연어 처리의 패러다임이 미세 조정을 통한 사전 학습에서 인맥 학습을 통한 사전 학습으로 전환되고 있다. 그러나 현재 대규모 언어 모델(LLM)은 수학적 추론 <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="#bib.bib17" title="">2021</a>; Patel et al., <a class="ltx_ref" href="#bib.bib114" title="">2021</a>)</cite>, 상식 추론 <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="#bib.bib141" title="">2021</a>; Mihaylov et al., <a class="ltx_ref" href="#bib.bib102" title="">2018</a>)</cite> 등과 같이 복잡한 추론 작업에 대해 여전히 상당한 개선의 여지가 있다. </span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">LLMs을 활용하여 복잡한 추론 작업을 다루기 위해 <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite>는 단계별 추론 프로세스로 컨텍스트 내 학습을 확장하며, 먼저 CoT(Chain-of-thought) 프롬프트 개념을 도입한다. <cite class="ltx_cite ltx_citemacro_citet">Kojima et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>는 매직 프레이즈 <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Let’s think step by step</span> in prompts enables LLMs to perform zero-shot chain-of-thought reasoning without any human annotation. 이러한 연구들은 복잡한 추론에 대한 모델의 능력을 향상시키고 추론과 계획 능력을 향상시키는 데 있어 연쇄적 사고의 중요성을 강조했다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">그 후, NLP 커뮤니티에서 비가 내린 후 자동 XoT 구성 <cite class="ltx_cite ltx_citemacro_citep">(Kojima et al., <a class="ltx_ref" href="#bib.bib64" title="">2022</a>; Zhang et al., <a class="ltx_ref" href="#bib.bib196" title="">2023f</a>; Xu et al., <a class="ltx_ref" href="#bib.bib167" title="">2023</a>)</cite>, XoT 구조 변형 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib12" title="">2022a</a>; Ning et al., <a class="ltx_ref" href="#bib.bib107" title="">2023</a>; Lei et al., <a class="ltx_ref" href="#bib.bib71" title="">2023a</a>; Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>)</cite> 등과 같이 XoT에 대한 상당한 작품이 버섯처럼 등장한다. 원시 CoT와 구별하기 위해 우리는 XoT를 사용하여 넓은 의미에서 CoT를 지칭하는데, 이는 단계별 추론 방법의 사용을 총칭하는 용어이다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">그러나, 이러한 방법 및 데이터 세트는 아직 체계적인 검토 및 분석을 거치지 않았다. 이러한 공백을 메우기 위해 XoT 패밀리에 대한 포괄적이고 상세한 분석을 수행하는 이 작업을 제안한다. 연쇄적 사고에 대한 몇 가지 조사가 있었지만 프롬프트가 있는 LLM 추론 <cite class="ltx_cite ltx_citemacro_citep">(Qiao et al., <a class="ltx_ref" href="#bib.bib118" title="">2023</a>)</cite> 및 연쇄적 사고에 대한 프롬프트 전략 <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="#bib.bib185" title="">2023c</a>)</cite>와 같은 특정 측면에 국한된다. 대조적으로, 우리의 조사는 그들이 이미 다룬 주제에 대해 더 철저하고 포괄적인 토론을 제공할 뿐만 아니라 XoT 구성, XoT 구조적 변형 및 프론티어 애플리케이션 등과 같은 추가 주제 및 토론을 포함한다. 구체적으로, 본 논문에서는 먼저 관련 배경과 예비(§<a class="ltx_ref" href="#S2" title="2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">2</span></a>)를 소개한다. 또한, XoT 일련의 작업을 여러 관점에서 신중하게 분류하고 XoT 구성 방법(§<a class="ltx_ref" href="#S4.SS1" title="4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.1</span></a>), XoT 구조 변형(§<a class="ltx_ref" href="#S4.SS2" title="4.2 XoT Structural Variants ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.2</span></a>) 및 XoT 향상 방법(§<a class="ltx_ref" href="#S4.SS3" title="4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3</span></a>)을 포함하여 심층 분석(§<a class="ltx_ref" href="#S4" title="4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4</span></a>)을 완료한다. 그리고 프런티어 필드(§<a class="ltx_ref" href="#S5" title="5 Frontier Application ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">5</span></a>)에서 XoT의 실제 응용을 제공한다. XoT의 후속 작업에 영감을 주기 위해 이 영역에서 향후 연구를 위한 잠재적인 방법에 대한 통찰력을 제공한다. 마지막으로 기존 방법(§<a class="ltx_ref" href="#S7" title="7 Discussion ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">7</span></a>)을 비교, 논의한다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Preliminary</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background </h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">최근 컴퓨팅 파워의 지속적인 확대로 대규모 언어 모델들이 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>); OpenAI (<a class="ltx_ref" href="#bib.bib109" title="">2023</a>); Touvron et al. (<a class="ltx_ref" href="#bib.bib143" title="">2023a</a>); Scao et al. (<a class="ltx_ref" href="#bib.bib126" title="">2022</a>); Touvron et al. (<a class="ltx_ref" href="#bib.bib144" title="">2023b</a>); Zhao et al. (<a class="ltx_ref" href="#bib.bib200" title="">2023b</a>)</cite>가 생겨났고, 모델 크기가 계속 커지면서 상황 내 학습, 연쇄 사상 추론<cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>); Wei et al. (<a class="ltx_ref" href="#bib.bib161" title="">2022b</a>, <a class="ltx_ref" href="#bib.bib160" title="">a</a>); Schaeffer et al. (<a class="ltx_ref" href="#bib.bib127" title="">2023</a>)</cite> 등 많은 새로운 능력들이 등장했다.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Brown et al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>)</cite>는 대규모 언어 모델들이 ICL(in-context learning) 능력이 우수함을 발견한다. ICL은 입력-출력 데모를 프롬프트 텍스트에 통합한다. ICL을 사용하면 추가 미세 조정 없이 기성 LLM을 사용할 수 있으며 유사한 성능을 얻을 수 있다. 그럼에도 불구하고 이러한 종단 간 접근은 복잡한 추론 과제에 직면했을 때 성능이 떨어지는 경향이 있다.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite>는 시연에 단계별 추론 과정을 추가하여 LLM의 추론 능력을 향상시킬 수 있음을 발견하는데, 이를 연쇄적 사고 프롬프트라고 한다. CoT 프롬프트를 통해 모델은 질문의 복잡성과 추론 과정 모두에 대한 보다 정확한 이해를 얻을 수 있다. 또한, 모델은 일련의 추론 단계를 생성하며, 이는 모델의 인지 과정에 대한 투명한 관점을 제공하여 해석 가능성을 더욱 향상시킨다.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Preliminary </h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.6">이 절에서는 LLM을 사용한 예비 연쇄 사고 추론을 소개하고 <cite class="ltx_cite ltx_citemacro_citep">(Qiao et al., <a class="ltx_ref" href="#bib.bib118" title="">2023</a>)</cite>의 공식 정의를 참조한다. 질문 <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathcal{Q}</annotation></semantics></math>, 프롬프트 <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\mathcal{T}</annotation></semantics></math> 및 확률 언어 모델 <math alttext="P_{LM}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">P</mi><mrow id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml"><mi id="S2.SS2.p1.3.m3.1.1.3.2" xref="S2.SS2.p1.3.m3.1.1.3.2.cmml">L</mi><mo id="S2.SS2.p1.3.m3.1.1.3.1" lspace="0em" rspace="0em" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.3.m3.1.1.3.3" xref="S2.SS2.p1.3.m3.1.1.3.3.cmml">M</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝑃</ci><apply id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3"><times id="S2.SS2.p1.3.m3.1.1.3.1.cmml" xref="S2.SS2.p1.3.m3.1.1.3.1"></times><ci id="S2.SS2.p1.3.m3.1.1.3.2.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2">𝐿</ci><ci id="S2.SS2.p1.3.m3.1.1.3.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3.3">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">P_{LM}</annotation></semantics></math>가 있다고 가정하자. 모델은 질문과 프롬프트를 입력으로 하여 근거 <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\mathcal{R}</annotation></semantics></math>와 답변 <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">\mathcal{A}</annotation></semantics></math>를 제공한다. 우리는 먼저 시연이 추론 사슬을 포함하지 않는 상황 내 시나리오를 고려한다. 우리는 Equ(<a class="ltx_ref" href="#S2.E1" title="In 2.2 Preliminary ‣ 2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">1</span></a>,<a class="ltx_ref" href="#S2.E2" title="In 2.2 Preliminary ‣ 2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">2</span></a>)와 같이 Answer <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.1"><semantics id="S2.SS2.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><ci id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">\mathcal{A}</annotation></semantics></math>의 가능성을 극대화할 필요가 있다.</p>
<table id="S8.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m2.7" class="ltx_Math" alttext="\displaystyle p(\mathcal{A}~{}|~{}\mathcal{T,Q})=\prod_{i=1}^{|\mathcal{A}|}p_{LM}(a_{i}~{}|~{}\mathcal{T,Q},a_{<i})" display="inline"><semantics id="S2.E1.m2.7a"><mrow id="S2.E1.m2.7.7" xref="S2.E1.m2.7.7.cmml"><mrow id="S2.E1.m2.6.6.1" xref="S2.E1.m2.6.6.1.cmml"><mi id="S2.E1.m2.6.6.1.3" xref="S2.E1.m2.6.6.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.6.6.1.2" xref="S2.E1.m2.6.6.1.2.cmml">​</mo><mrow id="S2.E1.m2.6.6.1.1.1" xref="S2.E1.m2.6.6.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.6.6.1.1.1.2" xref="S2.E1.m2.6.6.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m2.6.6.1.1.1.1" xref="S2.E1.m2.6.6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.6.6.1.1.1.1.2" xref="S2.E1.m2.6.6.1.1.1.1.2.cmml">𝒜</mi><mo fence="false" lspace="0.608em" rspace="0.608em" id="S2.E1.m2.6.6.1.1.1.1.1" xref="S2.E1.m2.6.6.1.1.1.1.1.cmml">|</mo><mrow id="S2.E1.m2.6.6.1.1.1.1.3.2" xref="S2.E1.m2.6.6.1.1.1.1.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.2.2" xref="S2.E1.m2.2.2.cmml">𝒯</mi><mo id="S2.E1.m2.6.6.1.1.1.1.3.2.1" xref="S2.E1.m2.6.6.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.3.3" xref="S2.E1.m2.3.3.cmml">𝒬</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m2.6.6.1.1.1.3" xref="S2.E1.m2.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m2.7.7.3" xref="S2.E1.m2.7.7.3.cmml">=</mo><mrow id="S2.E1.m2.7.7.2" xref="S2.E1.m2.7.7.2.cmml"><mstyle displaystyle="true" id="S2.E1.m2.7.7.2.2" xref="S2.E1.m2.7.7.2.2.cmml"><munderover id="S2.E1.m2.7.7.2.2a" xref="S2.E1.m2.7.7.2.2.cmml"><mo movablelimits="false" id="S2.E1.m2.7.7.2.2.2.2" xref="S2.E1.m2.7.7.2.2.2.2.cmml">∏</mo><mrow id="S2.E1.m2.7.7.2.2.2.3" xref="S2.E1.m2.7.7.2.2.2.3.cmml"><mi id="S2.E1.m2.7.7.2.2.2.3.2" xref="S2.E1.m2.7.7.2.2.2.3.2.cmml">i</mi><mo id="S2.E1.m2.7.7.2.2.2.3.1" xref="S2.E1.m2.7.7.2.2.2.3.1.cmml">=</mo><mn id="S2.E1.m2.7.7.2.2.2.3.3" xref="S2.E1.m2.7.7.2.2.2.3.3.cmml">1</mn></mrow><mrow id="S2.E1.m2.1.1.1.3" xref="S2.E1.m2.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m2.1.1.1.3.1" xref="S2.E1.m2.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.1.1.1.1" xref="S2.E1.m2.1.1.1.1.cmml">𝒜</mi><mo stretchy="false" id="S2.E1.m2.1.1.1.3.2" xref="S2.E1.m2.1.1.1.2.1.cmml">|</mo></mrow></munderover></mstyle><mrow id="S2.E1.m2.7.7.2.1" xref="S2.E1.m2.7.7.2.1.cmml"><msub id="S2.E1.m2.7.7.2.1.3" xref="S2.E1.m2.7.7.2.1.3.cmml"><mi id="S2.E1.m2.7.7.2.1.3.2" xref="S2.E1.m2.7.7.2.1.3.2.cmml">p</mi><mrow id="S2.E1.m2.7.7.2.1.3.3" xref="S2.E1.m2.7.7.2.1.3.3.cmml"><mi id="S2.E1.m2.7.7.2.1.3.3.2" xref="S2.E1.m2.7.7.2.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.7.7.2.1.3.3.1" xref="S2.E1.m2.7.7.2.1.3.3.1.cmml">​</mo><mi id="S2.E1.m2.7.7.2.1.3.3.3" xref="S2.E1.m2.7.7.2.1.3.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m2.7.7.2.1.2" xref="S2.E1.m2.7.7.2.1.2.cmml">​</mo><mrow id="S2.E1.m2.7.7.2.1.1.1" xref="S2.E1.m2.7.7.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.7.7.2.1.1.1.2" xref="S2.E1.m2.7.7.2.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m2.7.7.2.1.1.1.1" xref="S2.E1.m2.7.7.2.1.1.1.1.cmml"><msub id="S2.E1.m2.7.7.2.1.1.1.1.3" xref="S2.E1.m2.7.7.2.1.1.1.1.3.cmml"><mi id="S2.E1.m2.7.7.2.1.1.1.1.3.2" xref="S2.E1.m2.7.7.2.1.1.1.1.3.2.cmml">a</mi><mi id="S2.E1.m2.7.7.2.1.1.1.1.3.3" xref="S2.E1.m2.7.7.2.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" rspace="0.608em" id="S2.E1.m2.7.7.2.1.1.1.1.2" xref="S2.E1.m2.7.7.2.1.1.1.1.2.cmml">|</mo><mrow id="S2.E1.m2.7.7.2.1.1.1.1.1.1" xref="S2.E1.m2.7.7.2.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.4.4" xref="S2.E1.m2.4.4.cmml">𝒯</mi><mo id="S2.E1.m2.7.7.2.1.1.1.1.1.1.2" xref="S2.E1.m2.7.7.2.1.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.5.5" xref="S2.E1.m2.5.5.cmml">𝒬</mi><mo id="S2.E1.m2.7.7.2.1.1.1.1.1.1.3" xref="S2.E1.m2.7.7.2.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.2" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.2" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.1" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.3" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.E1.m2.7.7.2.1.1.1.3" xref="S2.E1.m2.7.7.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m2.7b"><apply id="S2.E1.m2.7.7.cmml" xref="S2.E1.m2.7.7"><eq id="S2.E1.m2.7.7.3.cmml" xref="S2.E1.m2.7.7.3"></eq><apply id="S2.E1.m2.6.6.1.cmml" xref="S2.E1.m2.6.6.1"><times id="S2.E1.m2.6.6.1.2.cmml" xref="S2.E1.m2.6.6.1.2"></times><ci id="S2.E1.m2.6.6.1.3.cmml" xref="S2.E1.m2.6.6.1.3">𝑝</ci><apply id="S2.E1.m2.6.6.1.1.1.1.cmml" xref="S2.E1.m2.6.6.1.1.1"><csymbol cd="latexml" id="S2.E1.m2.6.6.1.1.1.1.1.cmml" xref="S2.E1.m2.6.6.1.1.1.1.1">conditional</csymbol><ci id="S2.E1.m2.6.6.1.1.1.1.2.cmml" xref="S2.E1.m2.6.6.1.1.1.1.2">𝒜</ci><list id="S2.E1.m2.6.6.1.1.1.1.3.1.cmml" xref="S2.E1.m2.6.6.1.1.1.1.3.2"><ci id="S2.E1.m2.2.2.cmml" xref="S2.E1.m2.2.2">𝒯</ci><ci id="S2.E1.m2.3.3.cmml" xref="S2.E1.m2.3.3">𝒬</ci></list></apply></apply><apply id="S2.E1.m2.7.7.2.cmml" xref="S2.E1.m2.7.7.2"><apply id="S2.E1.m2.7.7.2.2.cmml" xref="S2.E1.m2.7.7.2.2"><csymbol cd="ambiguous" id="S2.E1.m2.7.7.2.2.1.cmml" xref="S2.E1.m2.7.7.2.2">superscript</csymbol><apply id="S2.E1.m2.7.7.2.2.2.cmml" xref="S2.E1.m2.7.7.2.2"><csymbol cd="ambiguous" id="S2.E1.m2.7.7.2.2.2.1.cmml" xref="S2.E1.m2.7.7.2.2">subscript</csymbol><csymbol cd="latexml" id="S2.E1.m2.7.7.2.2.2.2.cmml" xref="S2.E1.m2.7.7.2.2.2.2">product</csymbol><apply id="S2.E1.m2.7.7.2.2.2.3.cmml" xref="S2.E1.m2.7.7.2.2.2.3"><eq id="S2.E1.m2.7.7.2.2.2.3.1.cmml" xref="S2.E1.m2.7.7.2.2.2.3.1"></eq><ci id="S2.E1.m2.7.7.2.2.2.3.2.cmml" xref="S2.E1.m2.7.7.2.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E1.m2.7.7.2.2.2.3.3.cmml" xref="S2.E1.m2.7.7.2.2.2.3.3">1</cn></apply></apply><apply id="S2.E1.m2.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.3"><abs id="S2.E1.m2.1.1.1.2.1.cmml" xref="S2.E1.m2.1.1.1.3.1"></abs><ci id="S2.E1.m2.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1">𝒜</ci></apply></apply><apply id="S2.E1.m2.7.7.2.1.cmml" xref="S2.E1.m2.7.7.2.1"><times id="S2.E1.m2.7.7.2.1.2.cmml" xref="S2.E1.m2.7.7.2.1.2"></times><apply id="S2.E1.m2.7.7.2.1.3.cmml" xref="S2.E1.m2.7.7.2.1.3"><csymbol cd="ambiguous" id="S2.E1.m2.7.7.2.1.3.1.cmml" xref="S2.E1.m2.7.7.2.1.3">subscript</csymbol><ci id="S2.E1.m2.7.7.2.1.3.2.cmml" xref="S2.E1.m2.7.7.2.1.3.2">𝑝</ci><apply id="S2.E1.m2.7.7.2.1.3.3.cmml" xref="S2.E1.m2.7.7.2.1.3.3"><times id="S2.E1.m2.7.7.2.1.3.3.1.cmml" xref="S2.E1.m2.7.7.2.1.3.3.1"></times><ci id="S2.E1.m2.7.7.2.1.3.3.2.cmml" xref="S2.E1.m2.7.7.2.1.3.3.2">𝐿</ci><ci id="S2.E1.m2.7.7.2.1.3.3.3.cmml" xref="S2.E1.m2.7.7.2.1.3.3.3">𝑀</ci></apply></apply><apply id="S2.E1.m2.7.7.2.1.1.1.1.cmml" xref="S2.E1.m2.7.7.2.1.1.1"><csymbol cd="latexml" id="S2.E1.m2.7.7.2.1.1.1.1.2.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.2">conditional</csymbol><apply id="S2.E1.m2.7.7.2.1.1.1.1.3.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m2.7.7.2.1.1.1.1.3.1.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m2.7.7.2.1.1.1.1.3.2.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.3.2">𝑎</ci><ci id="S2.E1.m2.7.7.2.1.1.1.1.3.3.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E1.m2.7.7.2.1.1.1.1.1.2.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1"><ci id="S2.E1.m2.4.4.cmml" xref="S2.E1.m2.4.4">𝒯</ci><ci id="S2.E1.m2.5.5.cmml" xref="S2.E1.m2.5.5">𝒬</ci><apply id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3"><lt id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m2.7.7.2.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.7c">\displaystyle p(\mathcal{A}~{}|~{}\mathcal{T,Q})=\prod_{i=1}^{|\mathcal{A}|}p_{LM}(a_{i}~{}|~{}\mathcal{T,Q},a_{&lt;i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m1.4" class="ltx_Math" alttext="\displaystyle\mathcal{T}_{ICL}=\{I,(x_{1},y_{1}),\cdots,(x_{n},y_{n})\}" display="inline"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><msub id="S2.E2.m1.4.4.4" xref="S2.E2.m1.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.4.2" xref="S2.E2.m1.4.4.4.2.cmml">𝒯</mi><mrow id="S2.E2.m1.4.4.4.3" xref="S2.E2.m1.4.4.4.3.cmml"><mi id="S2.E2.m1.4.4.4.3.2" xref="S2.E2.m1.4.4.4.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.4.3.1" xref="S2.E2.m1.4.4.4.3.1.cmml">​</mo><mi id="S2.E2.m1.4.4.4.3.3" xref="S2.E2.m1.4.4.4.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.4.3.1a" xref="S2.E2.m1.4.4.4.3.1.cmml">​</mo><mi id="S2.E2.m1.4.4.4.3.4" xref="S2.E2.m1.4.4.4.3.4.cmml">L</mi></mrow></msub><mo id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml">=</mo><mrow id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.2.2.3" xref="S2.E2.m1.4.4.2.3.cmml">{</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">I</mi><mo id="S2.E2.m1.4.4.2.2.4" xref="S2.E2.m1.4.4.2.3.cmml">,</mo><mrow id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.3.cmml">(</mo><msub id="S2.E2.m1.3.3.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E2.m1.3.3.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E2.m1.3.3.1.1.1.2.4" xref="S2.E2.m1.3.3.1.1.1.3.cmml">,</mo><msub id="S2.E2.m1.3.3.1.1.1.2.2" xref="S2.E2.m1.3.3.1.1.1.2.2.cmml"><mi id="S2.E2.m1.3.3.1.1.1.2.2.2" xref="S2.E2.m1.3.3.1.1.1.2.2.2.cmml">y</mi><mn id="S2.E2.m1.3.3.1.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.2.5" xref="S2.E2.m1.3.3.1.1.1.3.cmml">)</mo></mrow><mo id="S2.E2.m1.4.4.2.2.5" xref="S2.E2.m1.4.4.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">⋯</mi><mo id="S2.E2.m1.4.4.2.2.6" xref="S2.E2.m1.4.4.2.3.cmml">,</mo><mrow id="S2.E2.m1.4.4.2.2.2.2" xref="S2.E2.m1.4.4.2.2.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.2.2.2.2.3" xref="S2.E2.m1.4.4.2.2.2.3.cmml">(</mo><msub id="S2.E2.m1.4.4.2.2.2.1.1" xref="S2.E2.m1.4.4.2.2.2.1.1.cmml"><mi id="S2.E2.m1.4.4.2.2.2.1.1.2" xref="S2.E2.m1.4.4.2.2.2.1.1.2.cmml">x</mi><mi id="S2.E2.m1.4.4.2.2.2.1.1.3" xref="S2.E2.m1.4.4.2.2.2.1.1.3.cmml">n</mi></msub><mo id="S2.E2.m1.4.4.2.2.2.2.4" xref="S2.E2.m1.4.4.2.2.2.3.cmml">,</mo><msub id="S2.E2.m1.4.4.2.2.2.2.2" xref="S2.E2.m1.4.4.2.2.2.2.2.cmml"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2" xref="S2.E2.m1.4.4.2.2.2.2.2.2.cmml">y</mi><mi id="S2.E2.m1.4.4.2.2.2.2.2.3" xref="S2.E2.m1.4.4.2.2.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.E2.m1.4.4.2.2.2.2.5" xref="S2.E2.m1.4.4.2.2.2.3.cmml">)</mo></mrow><mo stretchy="false" id="S2.E2.m1.4.4.2.2.7" xref="S2.E2.m1.4.4.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"></eq><apply id="S2.E2.m1.4.4.4.cmml" xref="S2.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.4.1.cmml" xref="S2.E2.m1.4.4.4">subscript</csymbol><ci id="S2.E2.m1.4.4.4.2.cmml" xref="S2.E2.m1.4.4.4.2">𝒯</ci><apply id="S2.E2.m1.4.4.4.3.cmml" xref="S2.E2.m1.4.4.4.3"><times id="S2.E2.m1.4.4.4.3.1.cmml" xref="S2.E2.m1.4.4.4.3.1"></times><ci id="S2.E2.m1.4.4.4.3.2.cmml" xref="S2.E2.m1.4.4.4.3.2">𝐼</ci><ci id="S2.E2.m1.4.4.4.3.3.cmml" xref="S2.E2.m1.4.4.4.3.3">𝐶</ci><ci id="S2.E2.m1.4.4.4.3.4.cmml" xref="S2.E2.m1.4.4.4.3.4">𝐿</ci></apply></apply><set id="S2.E2.m1.4.4.2.3.cmml" xref="S2.E2.m1.4.4.2.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝐼</ci><interval closure="open" id="S2.E2.m1.3.3.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><apply id="S2.E2.m1.3.3.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.3">1</cn></apply><apply id="S2.E2.m1.3.3.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.2">𝑦</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3">1</cn></apply></interval><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">⋯</ci><interval closure="open" id="S2.E2.m1.4.4.2.2.2.3.cmml" xref="S2.E2.m1.4.4.2.2.2.2"><apply id="S2.E2.m1.4.4.2.2.2.1.1.cmml" xref="S2.E2.m1.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.2.2.1.1.1.cmml" xref="S2.E2.m1.4.4.2.2.2.1.1">subscript</csymbol><ci id="S2.E2.m1.4.4.2.2.2.1.1.2.cmml" xref="S2.E2.m1.4.4.2.2.2.1.1.2">𝑥</ci><ci id="S2.E2.m1.4.4.2.2.2.1.1.3.cmml" xref="S2.E2.m1.4.4.2.2.2.1.1.3">𝑛</ci></apply><apply id="S2.E2.m1.4.4.2.2.2.2.2.cmml" xref="S2.E2.m1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.2.2.2.2.1.cmml" xref="S2.E2.m1.4.4.2.2.2.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.2.2.2.2.2.2.cmml" xref="S2.E2.m1.4.4.2.2.2.2.2.2">𝑦</ci><ci id="S2.E2.m1.4.4.2.2.2.2.2.3.cmml" xref="S2.E2.m1.4.4.2.2.2.2.2.3">𝑛</ci></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\displaystyle\mathcal{T}_{ICL}=\{I,(x_{1},y_{1}),\cdots,(x_{n},y_{n})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p2.2">시연이 추론 과정을 포함하는 연쇄 사상 추론 시나리오에서 Equ(<a class="ltx_ref" href="#S2.E3" title="In 2.2 Preliminary ‣ 2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">3</span></a>,<a class="ltx_ref" href="#S2.E4" title="In 2.2 Preliminary ‣ 2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4</span></a>,<a class="ltx_ref" href="#S2.E5" title="In 2.2 Preliminary ‣ 2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">5</span></a>,<a class="ltx_ref" href="#S2.E6" title="In 2.2 Preliminary ‣ 2 Background and Preliminary ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">6</span></a>)와 같이 Answer <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\mathcal{A}</annotation></semantics></math>와 rationale <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1"><semantics id="S2.SS2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\mathcal{R}</annotation></semantics></math>의 우도를 최대화할 필요가 있다.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<table id="S8.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E3.m2.10" class="ltx_Math" alttext="\displaystyle p(\mathcal{A}~{}|~{}\mathcal{T,Q})=p(\mathcal{A}~{}|~{}\mathcal{T,Q,R})p(\mathcal{R}~{}|~{}\mathcal{T,Q})" display="inline"><semantics id="S2.E3.m2.10a"><mrow id="S2.E3.m2.10.10" xref="S2.E3.m2.10.10.cmml"><mrow id="S2.E3.m2.8.8.1" xref="S2.E3.m2.8.8.1.cmml"><mi id="S2.E3.m2.8.8.1.3" xref="S2.E3.m2.8.8.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m2.8.8.1.2" xref="S2.E3.m2.8.8.1.2.cmml">​</mo><mrow id="S2.E3.m2.8.8.1.1.1" xref="S2.E3.m2.8.8.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m2.8.8.1.1.1.2" xref="S2.E3.m2.8.8.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m2.8.8.1.1.1.1" xref="S2.E3.m2.8.8.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.8.8.1.1.1.1.2" xref="S2.E3.m2.8.8.1.1.1.1.2.cmml">𝒜</mi><mo fence="false" lspace="0.608em" rspace="0.608em" id="S2.E3.m2.8.8.1.1.1.1.1" xref="S2.E3.m2.8.8.1.1.1.1.1.cmml">|</mo><mrow id="S2.E3.m2.8.8.1.1.1.1.3.2" xref="S2.E3.m2.8.8.1.1.1.1.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.1.1" xref="S2.E3.m2.1.1.cmml">𝒯</mi><mo id="S2.E3.m2.8.8.1.1.1.1.3.2.1" xref="S2.E3.m2.8.8.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.2.2" xref="S2.E3.m2.2.2.cmml">𝒬</mi></mrow></mrow><mo stretchy="false" id="S2.E3.m2.8.8.1.1.1.3" xref="S2.E3.m2.8.8.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m2.10.10.4" xref="S2.E3.m2.10.10.4.cmml">=</mo><mrow id="S2.E3.m2.10.10.3" xref="S2.E3.m2.10.10.3.cmml"><mi id="S2.E3.m2.10.10.3.4" xref="S2.E3.m2.10.10.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m2.10.10.3.3" xref="S2.E3.m2.10.10.3.3.cmml">​</mo><mrow id="S2.E3.m2.9.9.2.1.1" xref="S2.E3.m2.9.9.2.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m2.9.9.2.1.1.2" xref="S2.E3.m2.9.9.2.1.1.1.cmml">(</mo><mrow id="S2.E3.m2.9.9.2.1.1.1" xref="S2.E3.m2.9.9.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.9.9.2.1.1.1.2" xref="S2.E3.m2.9.9.2.1.1.1.2.cmml">𝒜</mi><mo fence="false" lspace="0.608em" rspace="0.608em" id="S2.E3.m2.9.9.2.1.1.1.1" xref="S2.E3.m2.9.9.2.1.1.1.1.cmml">|</mo><mrow id="S2.E3.m2.9.9.2.1.1.1.3.2" xref="S2.E3.m2.9.9.2.1.1.1.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.3.3" xref="S2.E3.m2.3.3.cmml">𝒯</mi><mo id="S2.E3.m2.9.9.2.1.1.1.3.2.1" xref="S2.E3.m2.9.9.2.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.4.4" xref="S2.E3.m2.4.4.cmml">𝒬</mi><mo id="S2.E3.m2.9.9.2.1.1.1.3.2.2" xref="S2.E3.m2.9.9.2.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.5.5" xref="S2.E3.m2.5.5.cmml">ℛ</mi></mrow></mrow><mo stretchy="false" id="S2.E3.m2.9.9.2.1.1.3" xref="S2.E3.m2.9.9.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m2.10.10.3.3a" xref="S2.E3.m2.10.10.3.3.cmml">​</mo><mi id="S2.E3.m2.10.10.3.5" xref="S2.E3.m2.10.10.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m2.10.10.3.3b" xref="S2.E3.m2.10.10.3.3.cmml">​</mo><mrow id="S2.E3.m2.10.10.3.2.1" xref="S2.E3.m2.10.10.3.2.1.1.cmml"><mo stretchy="false" id="S2.E3.m2.10.10.3.2.1.2" xref="S2.E3.m2.10.10.3.2.1.1.cmml">(</mo><mrow id="S2.E3.m2.10.10.3.2.1.1" xref="S2.E3.m2.10.10.3.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.10.10.3.2.1.1.2" xref="S2.E3.m2.10.10.3.2.1.1.2.cmml">ℛ</mi><mo fence="false" lspace="0.608em" rspace="0.608em" id="S2.E3.m2.10.10.3.2.1.1.1" xref="S2.E3.m2.10.10.3.2.1.1.1.cmml">|</mo><mrow id="S2.E3.m2.10.10.3.2.1.1.3.2" xref="S2.E3.m2.10.10.3.2.1.1.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.6.6" xref="S2.E3.m2.6.6.cmml">𝒯</mi><mo id="S2.E3.m2.10.10.3.2.1.1.3.2.1" xref="S2.E3.m2.10.10.3.2.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E3.m2.7.7" xref="S2.E3.m2.7.7.cmml">𝒬</mi></mrow></mrow><mo stretchy="false" id="S2.E3.m2.10.10.3.2.1.3" xref="S2.E3.m2.10.10.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m2.10b"><apply id="S2.E3.m2.10.10.cmml" xref="S2.E3.m2.10.10"><eq id="S2.E3.m2.10.10.4.cmml" xref="S2.E3.m2.10.10.4"></eq><apply id="S2.E3.m2.8.8.1.cmml" xref="S2.E3.m2.8.8.1"><times id="S2.E3.m2.8.8.1.2.cmml" xref="S2.E3.m2.8.8.1.2"></times><ci id="S2.E3.m2.8.8.1.3.cmml" xref="S2.E3.m2.8.8.1.3">𝑝</ci><apply id="S2.E3.m2.8.8.1.1.1.1.cmml" xref="S2.E3.m2.8.8.1.1.1"><csymbol cd="latexml" id="S2.E3.m2.8.8.1.1.1.1.1.cmml" xref="S2.E3.m2.8.8.1.1.1.1.1">conditional</csymbol><ci id="S2.E3.m2.8.8.1.1.1.1.2.cmml" xref="S2.E3.m2.8.8.1.1.1.1.2">𝒜</ci><list id="S2.E3.m2.8.8.1.1.1.1.3.1.cmml" xref="S2.E3.m2.8.8.1.1.1.1.3.2"><ci id="S2.E3.m2.1.1.cmml" xref="S2.E3.m2.1.1">𝒯</ci><ci id="S2.E3.m2.2.2.cmml" xref="S2.E3.m2.2.2">𝒬</ci></list></apply></apply><apply id="S2.E3.m2.10.10.3.cmml" xref="S2.E3.m2.10.10.3"><times id="S2.E3.m2.10.10.3.3.cmml" xref="S2.E3.m2.10.10.3.3"></times><ci id="S2.E3.m2.10.10.3.4.cmml" xref="S2.E3.m2.10.10.3.4">𝑝</ci><apply id="S2.E3.m2.9.9.2.1.1.1.cmml" xref="S2.E3.m2.9.9.2.1.1"><csymbol cd="latexml" id="S2.E3.m2.9.9.2.1.1.1.1.cmml" xref="S2.E3.m2.9.9.2.1.1.1.1">conditional</csymbol><ci id="S2.E3.m2.9.9.2.1.1.1.2.cmml" xref="S2.E3.m2.9.9.2.1.1.1.2">𝒜</ci><list id="S2.E3.m2.9.9.2.1.1.1.3.1.cmml" xref="S2.E3.m2.9.9.2.1.1.1.3.2"><ci id="S2.E3.m2.3.3.cmml" xref="S2.E3.m2.3.3">𝒯</ci><ci id="S2.E3.m2.4.4.cmml" xref="S2.E3.m2.4.4">𝒬</ci><ci id="S2.E3.m2.5.5.cmml" xref="S2.E3.m2.5.5">ℛ</ci></list></apply><ci id="S2.E3.m2.10.10.3.5.cmml" xref="S2.E3.m2.10.10.3.5">𝑝</ci><apply id="S2.E3.m2.10.10.3.2.1.1.cmml" xref="S2.E3.m2.10.10.3.2.1"><csymbol cd="latexml" id="S2.E3.m2.10.10.3.2.1.1.1.cmml" xref="S2.E3.m2.10.10.3.2.1.1.1">conditional</csymbol><ci id="S2.E3.m2.10.10.3.2.1.1.2.cmml" xref="S2.E3.m2.10.10.3.2.1.1.2">ℛ</ci><list id="S2.E3.m2.10.10.3.2.1.1.3.1.cmml" xref="S2.E3.m2.10.10.3.2.1.1.3.2"><ci id="S2.E3.m2.6.6.cmml" xref="S2.E3.m2.6.6">𝒯</ci><ci id="S2.E3.m2.7.7.cmml" xref="S2.E3.m2.7.7">𝒬</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m2.10c">\displaystyle p(\mathcal{A}~{}|~{}\mathcal{T,Q})=p(\mathcal{A}~{}|~{}\mathcal{T,Q,R})p(\mathcal{R}~{}|~{}\mathcal{T,Q})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E4.m1.7" class="ltx_Math" alttext="\displaystyle p(\mathcal{R}~{}|~{}\mathcal{T,Q})=\prod_{i=1}^{|\mathcal{R}|}p_{LM}(r_{i}~{}|~{}\mathcal{T,Q},r_{<i})" display="inline"><semantics id="S2.E4.m1.7a"><mrow id="S2.E4.m1.7.7" xref="S2.E4.m1.7.7.cmml"><mrow id="S2.E4.m1.6.6.1" xref="S2.E4.m1.6.6.1.cmml"><mi id="S2.E4.m1.6.6.1.3" xref="S2.E4.m1.6.6.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.6.6.1.2" xref="S2.E4.m1.6.6.1.2.cmml">​</mo><mrow id="S2.E4.m1.6.6.1.1.1" xref="S2.E4.m1.6.6.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.6.6.1.1.1.2" xref="S2.E4.m1.6.6.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.6.6.1.1.1.1" xref="S2.E4.m1.6.6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.6.6.1.1.1.1.2" xref="S2.E4.m1.6.6.1.1.1.1.2.cmml">ℛ</mi><mo fence="false" lspace="0.608em" rspace="0.608em" id="S2.E4.m1.6.6.1.1.1.1.1" xref="S2.E4.m1.6.6.1.1.1.1.1.cmml">|</mo><mrow id="S2.E4.m1.6.6.1.1.1.1.3.2" xref="S2.E4.m1.6.6.1.1.1.1.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">𝒯</mi><mo id="S2.E4.m1.6.6.1.1.1.1.3.2.1" xref="S2.E4.m1.6.6.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml">𝒬</mi></mrow></mrow><mo stretchy="false" id="S2.E4.m1.6.6.1.1.1.3" xref="S2.E4.m1.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.7.7.3" xref="S2.E4.m1.7.7.3.cmml">=</mo><mrow id="S2.E4.m1.7.7.2" xref="S2.E4.m1.7.7.2.cmml"><mstyle displaystyle="true" id="S2.E4.m1.7.7.2.2" xref="S2.E4.m1.7.7.2.2.cmml"><munderover id="S2.E4.m1.7.7.2.2a" xref="S2.E4.m1.7.7.2.2.cmml"><mo movablelimits="false" id="S2.E4.m1.7.7.2.2.2.2" xref="S2.E4.m1.7.7.2.2.2.2.cmml">∏</mo><mrow id="S2.E4.m1.7.7.2.2.2.3" xref="S2.E4.m1.7.7.2.2.2.3.cmml"><mi id="S2.E4.m1.7.7.2.2.2.3.2" xref="S2.E4.m1.7.7.2.2.2.3.2.cmml">i</mi><mo id="S2.E4.m1.7.7.2.2.2.3.1" xref="S2.E4.m1.7.7.2.2.2.3.1.cmml">=</mo><mn id="S2.E4.m1.7.7.2.2.2.3.3" xref="S2.E4.m1.7.7.2.2.2.3.3.cmml">1</mn></mrow><mrow id="S2.E4.m1.1.1.1.3" xref="S2.E4.m1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.3.1" xref="S2.E4.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml">ℛ</mi><mo stretchy="false" id="S2.E4.m1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover></mstyle><mrow id="S2.E4.m1.7.7.2.1" xref="S2.E4.m1.7.7.2.1.cmml"><msub id="S2.E4.m1.7.7.2.1.3" xref="S2.E4.m1.7.7.2.1.3.cmml"><mi id="S2.E4.m1.7.7.2.1.3.2" xref="S2.E4.m1.7.7.2.1.3.2.cmml">p</mi><mrow id="S2.E4.m1.7.7.2.1.3.3" xref="S2.E4.m1.7.7.2.1.3.3.cmml"><mi id="S2.E4.m1.7.7.2.1.3.3.2" xref="S2.E4.m1.7.7.2.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.7.7.2.1.3.3.1" xref="S2.E4.m1.7.7.2.1.3.3.1.cmml">​</mo><mi id="S2.E4.m1.7.7.2.1.3.3.3" xref="S2.E4.m1.7.7.2.1.3.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.7.7.2.1.2" xref="S2.E4.m1.7.7.2.1.2.cmml">​</mo><mrow id="S2.E4.m1.7.7.2.1.1.1" xref="S2.E4.m1.7.7.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.7.7.2.1.1.1.2" xref="S2.E4.m1.7.7.2.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.7.7.2.1.1.1.1" xref="S2.E4.m1.7.7.2.1.1.1.1.cmml"><msub id="S2.E4.m1.7.7.2.1.1.1.1.3" xref="S2.E4.m1.7.7.2.1.1.1.1.3.cmml"><mi id="S2.E4.m1.7.7.2.1.1.1.1.3.2" xref="S2.E4.m1.7.7.2.1.1.1.1.3.2.cmml">r</mi><mi id="S2.E4.m1.7.7.2.1.1.1.1.3.3" xref="S2.E4.m1.7.7.2.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" rspace="0.608em" id="S2.E4.m1.7.7.2.1.1.1.1.2" xref="S2.E4.m1.7.7.2.1.1.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.7.7.2.1.1.1.1.1.1" xref="S2.E4.m1.7.7.2.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml">𝒯</mi><mo id="S2.E4.m1.7.7.2.1.1.1.1.1.1.2" xref="S2.E4.m1.7.7.2.1.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.5.5" xref="S2.E4.m1.5.5.cmml">𝒬</mi><mo id="S2.E4.m1.7.7.2.1.1.1.1.1.1.3" xref="S2.E4.m1.7.7.2.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.2" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.2.cmml">r</mi><mrow id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.1" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.E4.m1.7.7.2.1.1.1.3" xref="S2.E4.m1.7.7.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.7b"><apply id="S2.E4.m1.7.7.cmml" xref="S2.E4.m1.7.7"><eq id="S2.E4.m1.7.7.3.cmml" xref="S2.E4.m1.7.7.3"></eq><apply id="S2.E4.m1.6.6.1.cmml" xref="S2.E4.m1.6.6.1"><times id="S2.E4.m1.6.6.1.2.cmml" xref="S2.E4.m1.6.6.1.2"></times><ci id="S2.E4.m1.6.6.1.3.cmml" xref="S2.E4.m1.6.6.1.3">𝑝</ci><apply id="S2.E4.m1.6.6.1.1.1.1.cmml" xref="S2.E4.m1.6.6.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.6.6.1.1.1.1.1.cmml" xref="S2.E4.m1.6.6.1.1.1.1.1">conditional</csymbol><ci id="S2.E4.m1.6.6.1.1.1.1.2.cmml" xref="S2.E4.m1.6.6.1.1.1.1.2">ℛ</ci><list id="S2.E4.m1.6.6.1.1.1.1.3.1.cmml" xref="S2.E4.m1.6.6.1.1.1.1.3.2"><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝒯</ci><ci id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3">𝒬</ci></list></apply></apply><apply id="S2.E4.m1.7.7.2.cmml" xref="S2.E4.m1.7.7.2"><apply id="S2.E4.m1.7.7.2.2.cmml" xref="S2.E4.m1.7.7.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.2.2.1.cmml" xref="S2.E4.m1.7.7.2.2">superscript</csymbol><apply id="S2.E4.m1.7.7.2.2.2.cmml" xref="S2.E4.m1.7.7.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.2.2.2.1.cmml" xref="S2.E4.m1.7.7.2.2">subscript</csymbol><csymbol cd="latexml" id="S2.E4.m1.7.7.2.2.2.2.cmml" xref="S2.E4.m1.7.7.2.2.2.2">product</csymbol><apply id="S2.E4.m1.7.7.2.2.2.3.cmml" xref="S2.E4.m1.7.7.2.2.2.3"><eq id="S2.E4.m1.7.7.2.2.2.3.1.cmml" xref="S2.E4.m1.7.7.2.2.2.3.1"></eq><ci id="S2.E4.m1.7.7.2.2.2.3.2.cmml" xref="S2.E4.m1.7.7.2.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E4.m1.7.7.2.2.2.3.3.cmml" xref="S2.E4.m1.7.7.2.2.2.3.3">1</cn></apply></apply><apply id="S2.E4.m1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.3"><abs id="S2.E4.m1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.3.1"></abs><ci id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1">ℛ</ci></apply></apply><apply id="S2.E4.m1.7.7.2.1.cmml" xref="S2.E4.m1.7.7.2.1"><times id="S2.E4.m1.7.7.2.1.2.cmml" xref="S2.E4.m1.7.7.2.1.2"></times><apply id="S2.E4.m1.7.7.2.1.3.cmml" xref="S2.E4.m1.7.7.2.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.2.1.3.1.cmml" xref="S2.E4.m1.7.7.2.1.3">subscript</csymbol><ci id="S2.E4.m1.7.7.2.1.3.2.cmml" xref="S2.E4.m1.7.7.2.1.3.2">𝑝</ci><apply id="S2.E4.m1.7.7.2.1.3.3.cmml" xref="S2.E4.m1.7.7.2.1.3.3"><times id="S2.E4.m1.7.7.2.1.3.3.1.cmml" xref="S2.E4.m1.7.7.2.1.3.3.1"></times><ci id="S2.E4.m1.7.7.2.1.3.3.2.cmml" xref="S2.E4.m1.7.7.2.1.3.3.2">𝐿</ci><ci id="S2.E4.m1.7.7.2.1.3.3.3.cmml" xref="S2.E4.m1.7.7.2.1.3.3.3">𝑀</ci></apply></apply><apply id="S2.E4.m1.7.7.2.1.1.1.1.cmml" xref="S2.E4.m1.7.7.2.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.7.7.2.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.2">conditional</csymbol><apply id="S2.E4.m1.7.7.2.1.1.1.1.3.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.2.1.1.1.1.3.1.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.7.7.2.1.1.1.1.3.2.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.3.2">𝑟</ci><ci id="S2.E4.m1.7.7.2.1.1.1.1.3.3.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E4.m1.7.7.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1"><ci id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4">𝒯</ci><ci id="S2.E4.m1.5.5.cmml" xref="S2.E4.m1.5.5">𝒬</ci><apply id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.2">𝑟</ci><apply id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3"><lt id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.7.7.2.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.7c">\displaystyle p(\mathcal{R}~{}|~{}\mathcal{T,Q})=\prod_{i=1}^{|\mathcal{R}|}p_{LM}(r_{i}~{}|~{}\mathcal{T,Q},r_{&lt;i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E5.m1.9" class="ltx_Math" alttext="\displaystyle p(\mathcal{A}|\mathcal{T,Q,R})=\prod_{j=1}^{|\mathcal{A}|}p_{LM}(a_{i}|\mathcal{T,Q,R},a_{<j})" display="inline"><semantics id="S2.E5.m1.9a"><mrow id="S2.E5.m1.9.9" xref="S2.E5.m1.9.9.cmml"><mrow id="S2.E5.m1.8.8.1" xref="S2.E5.m1.8.8.1.cmml"><mi id="S2.E5.m1.8.8.1.3" xref="S2.E5.m1.8.8.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.8.8.1.2" xref="S2.E5.m1.8.8.1.2.cmml">​</mo><mrow id="S2.E5.m1.8.8.1.1.1" xref="S2.E5.m1.8.8.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.8.8.1.1.1.2" xref="S2.E5.m1.8.8.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.8.8.1.1.1.1" xref="S2.E5.m1.8.8.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.8.8.1.1.1.1.2" xref="S2.E5.m1.8.8.1.1.1.1.2.cmml">𝒜</mi><mo fence="false" id="S2.E5.m1.8.8.1.1.1.1.1" xref="S2.E5.m1.8.8.1.1.1.1.1.cmml">|</mo><mrow id="S2.E5.m1.8.8.1.1.1.1.3.2" xref="S2.E5.m1.8.8.1.1.1.1.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2" xref="S2.E5.m1.2.2.cmml">𝒯</mi><mo id="S2.E5.m1.8.8.1.1.1.1.3.2.1" xref="S2.E5.m1.8.8.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.3.3" xref="S2.E5.m1.3.3.cmml">𝒬</mi><mo id="S2.E5.m1.8.8.1.1.1.1.3.2.2" xref="S2.E5.m1.8.8.1.1.1.1.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.4.4" xref="S2.E5.m1.4.4.cmml">ℛ</mi></mrow></mrow><mo stretchy="false" id="S2.E5.m1.8.8.1.1.1.3" xref="S2.E5.m1.8.8.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.9.9.3" xref="S2.E5.m1.9.9.3.cmml">=</mo><mrow id="S2.E5.m1.9.9.2" xref="S2.E5.m1.9.9.2.cmml"><mstyle displaystyle="true" id="S2.E5.m1.9.9.2.2" xref="S2.E5.m1.9.9.2.2.cmml"><munderover id="S2.E5.m1.9.9.2.2a" xref="S2.E5.m1.9.9.2.2.cmml"><mo movablelimits="false" id="S2.E5.m1.9.9.2.2.2.2" xref="S2.E5.m1.9.9.2.2.2.2.cmml">∏</mo><mrow id="S2.E5.m1.9.9.2.2.2.3" xref="S2.E5.m1.9.9.2.2.2.3.cmml"><mi id="S2.E5.m1.9.9.2.2.2.3.2" xref="S2.E5.m1.9.9.2.2.2.3.2.cmml">j</mi><mo id="S2.E5.m1.9.9.2.2.2.3.1" xref="S2.E5.m1.9.9.2.2.2.3.1.cmml">=</mo><mn id="S2.E5.m1.9.9.2.2.2.3.3" xref="S2.E5.m1.9.9.2.2.2.3.3.cmml">1</mn></mrow><mrow id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.3.1" xref="S2.E5.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml">𝒜</mi><mo stretchy="false" id="S2.E5.m1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover></mstyle><mrow id="S2.E5.m1.9.9.2.1" xref="S2.E5.m1.9.9.2.1.cmml"><msub id="S2.E5.m1.9.9.2.1.3" xref="S2.E5.m1.9.9.2.1.3.cmml"><mi id="S2.E5.m1.9.9.2.1.3.2" xref="S2.E5.m1.9.9.2.1.3.2.cmml">p</mi><mrow id="S2.E5.m1.9.9.2.1.3.3" xref="S2.E5.m1.9.9.2.1.3.3.cmml"><mi id="S2.E5.m1.9.9.2.1.3.3.2" xref="S2.E5.m1.9.9.2.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.9.9.2.1.3.3.1" xref="S2.E5.m1.9.9.2.1.3.3.1.cmml">​</mo><mi id="S2.E5.m1.9.9.2.1.3.3.3" xref="S2.E5.m1.9.9.2.1.3.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.9.9.2.1.2" xref="S2.E5.m1.9.9.2.1.2.cmml">​</mo><mrow id="S2.E5.m1.9.9.2.1.1.1" xref="S2.E5.m1.9.9.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.9.9.2.1.1.1.2" xref="S2.E5.m1.9.9.2.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.9.9.2.1.1.1.1" xref="S2.E5.m1.9.9.2.1.1.1.1.cmml"><msub id="S2.E5.m1.9.9.2.1.1.1.1.3" xref="S2.E5.m1.9.9.2.1.1.1.1.3.cmml"><mi id="S2.E5.m1.9.9.2.1.1.1.1.3.2" xref="S2.E5.m1.9.9.2.1.1.1.1.3.2.cmml">a</mi><mi id="S2.E5.m1.9.9.2.1.1.1.1.3.3" xref="S2.E5.m1.9.9.2.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E5.m1.9.9.2.1.1.1.1.2" xref="S2.E5.m1.9.9.2.1.1.1.1.2.cmml">|</mo><mrow id="S2.E5.m1.9.9.2.1.1.1.1.1.1" xref="S2.E5.m1.9.9.2.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.5.5" xref="S2.E5.m1.5.5.cmml">𝒯</mi><mo id="S2.E5.m1.9.9.2.1.1.1.1.1.1.2" xref="S2.E5.m1.9.9.2.1.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.6.6" xref="S2.E5.m1.6.6.cmml">𝒬</mi><mo id="S2.E5.m1.9.9.2.1.1.1.1.1.1.3" xref="S2.E5.m1.9.9.2.1.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.7.7" xref="S2.E5.m1.7.7.cmml">ℛ</mi><mo id="S2.E5.m1.9.9.2.1.1.1.1.1.1.4" xref="S2.E5.m1.9.9.2.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.2" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.2" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.1" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.3" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.E5.m1.9.9.2.1.1.1.3" xref="S2.E5.m1.9.9.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.9b"><apply id="S2.E5.m1.9.9.cmml" xref="S2.E5.m1.9.9"><eq id="S2.E5.m1.9.9.3.cmml" xref="S2.E5.m1.9.9.3"></eq><apply id="S2.E5.m1.8.8.1.cmml" xref="S2.E5.m1.8.8.1"><times id="S2.E5.m1.8.8.1.2.cmml" xref="S2.E5.m1.8.8.1.2"></times><ci id="S2.E5.m1.8.8.1.3.cmml" xref="S2.E5.m1.8.8.1.3">𝑝</ci><apply id="S2.E5.m1.8.8.1.1.1.1.cmml" xref="S2.E5.m1.8.8.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.8.8.1.1.1.1.1.cmml" xref="S2.E5.m1.8.8.1.1.1.1.1">conditional</csymbol><ci id="S2.E5.m1.8.8.1.1.1.1.2.cmml" xref="S2.E5.m1.8.8.1.1.1.1.2">𝒜</ci><list id="S2.E5.m1.8.8.1.1.1.1.3.1.cmml" xref="S2.E5.m1.8.8.1.1.1.1.3.2"><ci id="S2.E5.m1.2.2.cmml" xref="S2.E5.m1.2.2">𝒯</ci><ci id="S2.E5.m1.3.3.cmml" xref="S2.E5.m1.3.3">𝒬</ci><ci id="S2.E5.m1.4.4.cmml" xref="S2.E5.m1.4.4">ℛ</ci></list></apply></apply><apply id="S2.E5.m1.9.9.2.cmml" xref="S2.E5.m1.9.9.2"><apply id="S2.E5.m1.9.9.2.2.cmml" xref="S2.E5.m1.9.9.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.9.9.2.2.1.cmml" xref="S2.E5.m1.9.9.2.2">superscript</csymbol><apply id="S2.E5.m1.9.9.2.2.2.cmml" xref="S2.E5.m1.9.9.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.9.9.2.2.2.1.cmml" xref="S2.E5.m1.9.9.2.2">subscript</csymbol><csymbol cd="latexml" id="S2.E5.m1.9.9.2.2.2.2.cmml" xref="S2.E5.m1.9.9.2.2.2.2">product</csymbol><apply id="S2.E5.m1.9.9.2.2.2.3.cmml" xref="S2.E5.m1.9.9.2.2.2.3"><eq id="S2.E5.m1.9.9.2.2.2.3.1.cmml" xref="S2.E5.m1.9.9.2.2.2.3.1"></eq><ci id="S2.E5.m1.9.9.2.2.2.3.2.cmml" xref="S2.E5.m1.9.9.2.2.2.3.2">𝑗</ci><cn type="integer" id="S2.E5.m1.9.9.2.2.2.3.3.cmml" xref="S2.E5.m1.9.9.2.2.2.3.3">1</cn></apply></apply><apply id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.3"><abs id="S2.E5.m1.1.1.1.2.1.cmml" xref="S2.E5.m1.1.1.1.3.1"></abs><ci id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1">𝒜</ci></apply></apply><apply id="S2.E5.m1.9.9.2.1.cmml" xref="S2.E5.m1.9.9.2.1"><times id="S2.E5.m1.9.9.2.1.2.cmml" xref="S2.E5.m1.9.9.2.1.2"></times><apply id="S2.E5.m1.9.9.2.1.3.cmml" xref="S2.E5.m1.9.9.2.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.9.9.2.1.3.1.cmml" xref="S2.E5.m1.9.9.2.1.3">subscript</csymbol><ci id="S2.E5.m1.9.9.2.1.3.2.cmml" xref="S2.E5.m1.9.9.2.1.3.2">𝑝</ci><apply id="S2.E5.m1.9.9.2.1.3.3.cmml" xref="S2.E5.m1.9.9.2.1.3.3"><times id="S2.E5.m1.9.9.2.1.3.3.1.cmml" xref="S2.E5.m1.9.9.2.1.3.3.1"></times><ci id="S2.E5.m1.9.9.2.1.3.3.2.cmml" xref="S2.E5.m1.9.9.2.1.3.3.2">𝐿</ci><ci id="S2.E5.m1.9.9.2.1.3.3.3.cmml" xref="S2.E5.m1.9.9.2.1.3.3.3">𝑀</ci></apply></apply><apply id="S2.E5.m1.9.9.2.1.1.1.1.cmml" xref="S2.E5.m1.9.9.2.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.9.9.2.1.1.1.1.2.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.2">conditional</csymbol><apply id="S2.E5.m1.9.9.2.1.1.1.1.3.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.9.9.2.1.1.1.1.3.1.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.9.9.2.1.1.1.1.3.2.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.3.2">𝑎</ci><ci id="S2.E5.m1.9.9.2.1.1.1.1.3.3.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E5.m1.9.9.2.1.1.1.1.1.2.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1"><ci id="S2.E5.m1.5.5.cmml" xref="S2.E5.m1.5.5">𝒯</ci><ci id="S2.E5.m1.6.6.cmml" xref="S2.E5.m1.6.6">𝒬</ci><ci id="S2.E5.m1.7.7.cmml" xref="S2.E5.m1.7.7">ℛ</ci><apply id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3"><lt id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.9.9.2.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.9c">\displaystyle p(\mathcal{A}|\mathcal{T,Q,R})=\prod_{j=1}^{|\mathcal{A}|}p_{LM}(a_{i}|\mathcal{T,Q,R},a_{&lt;j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S2.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E6.m2.4" class="ltx_Math" alttext="\displaystyle\mathcal{T}_{\mathrm{CoT}}=\{I,(x_{1},e_{1},y_{1}),\cdots,(x_{n},e_{n},y_{n})\}" display="inline"><semantics id="S2.E6.m2.4a"><mrow id="S2.E6.m2.4.4" xref="S2.E6.m2.4.4.cmml"><msub id="S2.E6.m2.4.4.4" xref="S2.E6.m2.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E6.m2.4.4.4.2" xref="S2.E6.m2.4.4.4.2.cmml">𝒯</mi><mi id="S2.E6.m2.4.4.4.3" xref="S2.E6.m2.4.4.4.3.cmml">CoT</mi></msub><mo id="S2.E6.m2.4.4.3" xref="S2.E6.m2.4.4.3.cmml">=</mo><mrow id="S2.E6.m2.4.4.2.2" xref="S2.E6.m2.4.4.2.3.cmml"><mo stretchy="false" id="S2.E6.m2.4.4.2.2.3" xref="S2.E6.m2.4.4.2.3.cmml">{</mo><mi id="S2.E6.m2.1.1" xref="S2.E6.m2.1.1.cmml">I</mi><mo id="S2.E6.m2.4.4.2.2.4" xref="S2.E6.m2.4.4.2.3.cmml">,</mo><mrow id="S2.E6.m2.3.3.1.1.1.3" xref="S2.E6.m2.3.3.1.1.1.4.cmml"><mo stretchy="false" id="S2.E6.m2.3.3.1.1.1.3.4" xref="S2.E6.m2.3.3.1.1.1.4.cmml">(</mo><msub id="S2.E6.m2.3.3.1.1.1.1.1" xref="S2.E6.m2.3.3.1.1.1.1.1.cmml"><mi id="S2.E6.m2.3.3.1.1.1.1.1.2" xref="S2.E6.m2.3.3.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E6.m2.3.3.1.1.1.1.1.3" xref="S2.E6.m2.3.3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E6.m2.3.3.1.1.1.3.5" xref="S2.E6.m2.3.3.1.1.1.4.cmml">,</mo><msub id="S2.E6.m2.3.3.1.1.1.2.2" xref="S2.E6.m2.3.3.1.1.1.2.2.cmml"><mi id="S2.E6.m2.3.3.1.1.1.2.2.2" xref="S2.E6.m2.3.3.1.1.1.2.2.2.cmml">e</mi><mn id="S2.E6.m2.3.3.1.1.1.2.2.3" xref="S2.E6.m2.3.3.1.1.1.2.2.3.cmml">1</mn></msub><mo id="S2.E6.m2.3.3.1.1.1.3.6" xref="S2.E6.m2.3.3.1.1.1.4.cmml">,</mo><msub id="S2.E6.m2.3.3.1.1.1.3.3" xref="S2.E6.m2.3.3.1.1.1.3.3.cmml"><mi id="S2.E6.m2.3.3.1.1.1.3.3.2" xref="S2.E6.m2.3.3.1.1.1.3.3.2.cmml">y</mi><mn id="S2.E6.m2.3.3.1.1.1.3.3.3" xref="S2.E6.m2.3.3.1.1.1.3.3.3.cmml">1</mn></msub><mo stretchy="false" id="S2.E6.m2.3.3.1.1.1.3.7" xref="S2.E6.m2.3.3.1.1.1.4.cmml">)</mo></mrow><mo id="S2.E6.m2.4.4.2.2.5" xref="S2.E6.m2.4.4.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.E6.m2.2.2" xref="S2.E6.m2.2.2.cmml">⋯</mi><mo id="S2.E6.m2.4.4.2.2.6" xref="S2.E6.m2.4.4.2.3.cmml">,</mo><mrow id="S2.E6.m2.4.4.2.2.2.3" xref="S2.E6.m2.4.4.2.2.2.4.cmml"><mo stretchy="false" id="S2.E6.m2.4.4.2.2.2.3.4" xref="S2.E6.m2.4.4.2.2.2.4.cmml">(</mo><msub id="S2.E6.m2.4.4.2.2.2.1.1" xref="S2.E6.m2.4.4.2.2.2.1.1.cmml"><mi id="S2.E6.m2.4.4.2.2.2.1.1.2" xref="S2.E6.m2.4.4.2.2.2.1.1.2.cmml">x</mi><mi id="S2.E6.m2.4.4.2.2.2.1.1.3" xref="S2.E6.m2.4.4.2.2.2.1.1.3.cmml">n</mi></msub><mo id="S2.E6.m2.4.4.2.2.2.3.5" xref="S2.E6.m2.4.4.2.2.2.4.cmml">,</mo><msub id="S2.E6.m2.4.4.2.2.2.2.2" xref="S2.E6.m2.4.4.2.2.2.2.2.cmml"><mi id="S2.E6.m2.4.4.2.2.2.2.2.2" xref="S2.E6.m2.4.4.2.2.2.2.2.2.cmml">e</mi><mi id="S2.E6.m2.4.4.2.2.2.2.2.3" xref="S2.E6.m2.4.4.2.2.2.2.2.3.cmml">n</mi></msub><mo id="S2.E6.m2.4.4.2.2.2.3.6" xref="S2.E6.m2.4.4.2.2.2.4.cmml">,</mo><msub id="S2.E6.m2.4.4.2.2.2.3.3" xref="S2.E6.m2.4.4.2.2.2.3.3.cmml"><mi id="S2.E6.m2.4.4.2.2.2.3.3.2" xref="S2.E6.m2.4.4.2.2.2.3.3.2.cmml">y</mi><mi id="S2.E6.m2.4.4.2.2.2.3.3.3" xref="S2.E6.m2.4.4.2.2.2.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S2.E6.m2.4.4.2.2.2.3.7" xref="S2.E6.m2.4.4.2.2.2.4.cmml">)</mo></mrow><mo stretchy="false" id="S2.E6.m2.4.4.2.2.7" xref="S2.E6.m2.4.4.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m2.4b"><apply id="S2.E6.m2.4.4.cmml" xref="S2.E6.m2.4.4"><eq id="S2.E6.m2.4.4.3.cmml" xref="S2.E6.m2.4.4.3"></eq><apply id="S2.E6.m2.4.4.4.cmml" xref="S2.E6.m2.4.4.4"><csymbol cd="ambiguous" id="S2.E6.m2.4.4.4.1.cmml" xref="S2.E6.m2.4.4.4">subscript</csymbol><ci id="S2.E6.m2.4.4.4.2.cmml" xref="S2.E6.m2.4.4.4.2">𝒯</ci><ci id="S2.E6.m2.4.4.4.3.cmml" xref="S2.E6.m2.4.4.4.3">CoT</ci></apply><set id="S2.E6.m2.4.4.2.3.cmml" xref="S2.E6.m2.4.4.2.2"><ci id="S2.E6.m2.1.1.cmml" xref="S2.E6.m2.1.1">𝐼</ci><vector id="S2.E6.m2.3.3.1.1.1.4.cmml" xref="S2.E6.m2.3.3.1.1.1.3"><apply id="S2.E6.m2.3.3.1.1.1.1.1.cmml" xref="S2.E6.m2.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m2.3.3.1.1.1.1.1.1.cmml" xref="S2.E6.m2.3.3.1.1.1.1.1">subscript</csymbol><ci id="S2.E6.m2.3.3.1.1.1.1.1.2.cmml" xref="S2.E6.m2.3.3.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S2.E6.m2.3.3.1.1.1.1.1.3.cmml" xref="S2.E6.m2.3.3.1.1.1.1.1.3">1</cn></apply><apply id="S2.E6.m2.3.3.1.1.1.2.2.cmml" xref="S2.E6.m2.3.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E6.m2.3.3.1.1.1.2.2.1.cmml" xref="S2.E6.m2.3.3.1.1.1.2.2">subscript</csymbol><ci id="S2.E6.m2.3.3.1.1.1.2.2.2.cmml" xref="S2.E6.m2.3.3.1.1.1.2.2.2">𝑒</ci><cn type="integer" id="S2.E6.m2.3.3.1.1.1.2.2.3.cmml" xref="S2.E6.m2.3.3.1.1.1.2.2.3">1</cn></apply><apply id="S2.E6.m2.3.3.1.1.1.3.3.cmml" xref="S2.E6.m2.3.3.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E6.m2.3.3.1.1.1.3.3.1.cmml" xref="S2.E6.m2.3.3.1.1.1.3.3">subscript</csymbol><ci id="S2.E6.m2.3.3.1.1.1.3.3.2.cmml" xref="S2.E6.m2.3.3.1.1.1.3.3.2">𝑦</ci><cn type="integer" id="S2.E6.m2.3.3.1.1.1.3.3.3.cmml" xref="S2.E6.m2.3.3.1.1.1.3.3.3">1</cn></apply></vector><ci id="S2.E6.m2.2.2.cmml" xref="S2.E6.m2.2.2">⋯</ci><vector id="S2.E6.m2.4.4.2.2.2.4.cmml" xref="S2.E6.m2.4.4.2.2.2.3"><apply id="S2.E6.m2.4.4.2.2.2.1.1.cmml" xref="S2.E6.m2.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E6.m2.4.4.2.2.2.1.1.1.cmml" xref="S2.E6.m2.4.4.2.2.2.1.1">subscript</csymbol><ci id="S2.E6.m2.4.4.2.2.2.1.1.2.cmml" xref="S2.E6.m2.4.4.2.2.2.1.1.2">𝑥</ci><ci id="S2.E6.m2.4.4.2.2.2.1.1.3.cmml" xref="S2.E6.m2.4.4.2.2.2.1.1.3">𝑛</ci></apply><apply id="S2.E6.m2.4.4.2.2.2.2.2.cmml" xref="S2.E6.m2.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E6.m2.4.4.2.2.2.2.2.1.cmml" xref="S2.E6.m2.4.4.2.2.2.2.2">subscript</csymbol><ci id="S2.E6.m2.4.4.2.2.2.2.2.2.cmml" xref="S2.E6.m2.4.4.2.2.2.2.2.2">𝑒</ci><ci id="S2.E6.m2.4.4.2.2.2.2.2.3.cmml" xref="S2.E6.m2.4.4.2.2.2.2.2.3">𝑛</ci></apply><apply id="S2.E6.m2.4.4.2.2.2.3.3.cmml" xref="S2.E6.m2.4.4.2.2.2.3.3"><csymbol cd="ambiguous" id="S2.E6.m2.4.4.2.2.2.3.3.1.cmml" xref="S2.E6.m2.4.4.2.2.2.3.3">subscript</csymbol><ci id="S2.E6.m2.4.4.2.2.2.3.3.2.cmml" xref="S2.E6.m2.4.4.2.2.2.3.3.2">𝑦</ci><ci id="S2.E6.m2.4.4.2.2.2.3.3.3.cmml" xref="S2.E6.m2.4.4.2.2.2.3.3.3">𝑛</ci></apply></vector></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m2.4c">\displaystyle\mathcal{T}_{\mathrm{CoT}}=\{I,(x_{1},e_{1},y_{1}),\cdots,(x_{n},e_{n},y_{n})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Benchmarks</h2>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:389.2pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-295.1pt,264.6pt) scale(0.42353662949067,0.42353662949067) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Task</th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Dataset</th>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Size</td>
<td id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Input</td>
<td id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Output</td>
<td id="S3.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Rationale</td>
<td id="S3.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Description</td>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AddSub <cite class="ltx_cite ltx_citemacro_cite">Hosseini et&nbsp;al. (<a href="#bib.bib44" title="" class="ltx_ref">2014</a>)</cite>
</th>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">395</td>
<td id="S3.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Question</td>
<td id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Number</td>
<td id="S3.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Equation</td>
<td id="S3.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Simple arithmetic</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.1.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">SingleEq <cite class="ltx_cite ltx_citemacro_cite">Koncel-Kedziorski et&nbsp;al. (<a href="#bib.bib65" title="" class="ltx_ref">2015</a>)</cite>
</th>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_center">508</td>
<td id="S3.T1.1.1.3.3.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.3.3.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.3.3.6" class="ltx_td ltx_align_center">Equation</td>
<td id="S3.T1.1.1.3.3.7" class="ltx_td ltx_align_center">Simple arithmetic</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.1.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">MultiArith <cite class="ltx_cite ltx_citemacro_cite">Roy and Roth (<a href="#bib.bib123" title="" class="ltx_ref">2015</a>)</cite>
</th>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.1.4.4.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.4.4.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.4.4.6" class="ltx_td ltx_align_center">Equation</td>
<td id="S3.T1.1.1.4.4.7" class="ltx_td ltx_align_center">Simple arithmetic</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.1.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">MAWPS <cite class="ltx_cite ltx_citemacro_cite">Koncel-Kedziorski et&nbsp;al. (<a href="#bib.bib66" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_align_center">3320</td>
<td id="S3.T1.1.1.5.5.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.5.5.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.5.5.6" class="ltx_td ltx_align_center">Equation</td>
<td id="S3.T1.1.1.5.5.7" class="ltx_td ltx_align_center">Simple arithmetic</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.1.6.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">AQUA-RAT <cite class="ltx_cite ltx_citemacro_cite">Ling et&nbsp;al. (<a href="#bib.bib84" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_align_center">100,000</td>
<td id="S3.T1.1.1.6.6.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.6.6.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.6.6.6" class="ltx_td ltx_align_center">Natural Language</td>
<td id="S3.T1.1.1.6.6.7" class="ltx_td ltx_align_center">Math reasoning with NL rationale</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<th id="S3.T1.1.1.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ASDiv <cite class="ltx_cite ltx_citemacro_cite">Miao et&nbsp;al. (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.7.7.3" class="ltx_td ltx_align_center">2305</td>
<td id="S3.T1.1.1.7.7.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.7.7.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.7.7.6" class="ltx_td ltx_align_center">Equation</td>
<td id="S3.T1.1.1.7.7.7" class="ltx_td ltx_align_center">Multi-step math reasoning</td>
</tr>
<tr id="S3.T1.1.1.8.8" class="ltx_tr">
<th id="S3.T1.1.1.8.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">SVAMP <cite class="ltx_cite ltx_citemacro_cite">Patel et&nbsp;al. (<a href="#bib.bib114" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.8.8.3" class="ltx_td ltx_align_center">1,000</td>
<td id="S3.T1.1.1.8.8.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.8.8.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.8.8.6" class="ltx_td ltx_align_center">Equation</td>
<td id="S3.T1.1.1.8.8.7" class="ltx_td ltx_align_center">Multi-step math reasoning</td>
</tr>
<tr id="S3.T1.1.1.9.9" class="ltx_tr">
<th id="S3.T1.1.1.9.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">GSM8K <cite class="ltx_cite ltx_citemacro_cite">Cobbe et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.9.9.3" class="ltx_td ltx_align_center">8,792</td>
<td id="S3.T1.1.1.9.9.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.9.9.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.9.9.6" class="ltx_td ltx_align_center">Natural Language</td>
<td id="S3.T1.1.1.9.9.7" class="ltx_td ltx_align_center">Multi-step math reasoning</td>
</tr>
<tr id="S3.T1.1.1.10.10" class="ltx_tr">
<th id="S3.T1.1.1.10.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">GSM-Hard <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S3.T1.1.1.10.10.3" class="ltx_td ltx_align_center">936</td>
<td id="S3.T1.1.1.10.10.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.10.10.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.10.10.6" class="ltx_td ltx_align_center">Natural Language</td>
<td id="S3.T1.1.1.10.10.7" class="ltx_td ltx_align_center">GSM8K with larger number</td>
</tr>
<tr id="S3.T1.1.1.11.11" class="ltx_tr">
<th id="S3.T1.1.1.11.11.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">MathQA <cite class="ltx_cite ltx_citemacro_cite">Amini et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.11.11.3" class="ltx_td ltx_align_center">37,297</td>
<td id="S3.T1.1.1.11.11.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.11.11.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.11.11.6" class="ltx_td ltx_align_center">Operation</td>
<td id="S3.T1.1.1.11.11.7" class="ltx_td ltx_align_center">Annotated based on AQUA</td>
</tr>
<tr id="S3.T1.1.1.12.12" class="ltx_tr">
<th id="S3.T1.1.1.12.12.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">DROP <cite class="ltx_cite ltx_citemacro_cite">Dua et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.12.12.3" class="ltx_td ltx_align_center">96,567</td>
<td id="S3.T1.1.1.12.12.4" class="ltx_td ltx_align_center">Question+Passage</td>
<td id="S3.T1.1.1.12.12.5" class="ltx_td ltx_align_center">Number+Span</td>
<td id="S3.T1.1.1.12.12.6" class="ltx_td ltx_align_center">Equation</td>
<td id="S3.T1.1.1.12.12.7" class="ltx_td ltx_align_center">Reading comprehension form</td>
</tr>
<tr id="S3.T1.1.1.13.13" class="ltx_tr">
<th id="S3.T1.1.1.13.13.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">TheoremQA <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S3.T1.1.1.13.13.3" class="ltx_td ltx_align_center">800</td>
<td id="S3.T1.1.1.13.13.4" class="ltx_td ltx_align_center">Question+Theorem</td>
<td id="S3.T1.1.1.13.13.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.13.13.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.13.13.7" class="ltx_td ltx_align_center">Answer based on theorems</td>
</tr>
<tr id="S3.T1.1.1.14.14" class="ltx_tr">
<th id="S3.T1.1.1.14.14.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">TAT-QA <cite class="ltx_cite ltx_citemacro_cite">Zhu et&nbsp;al. (<a href="#bib.bib207" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.14.14.3" class="ltx_td ltx_align_center">16,552</td>
<td id="S3.T1.1.1.14.14.4" class="ltx_td ltx_align_center">Question+Table+Text</td>
<td id="S3.T1.1.1.14.14.5" class="ltx_td ltx_align_center">Number+Span</td>
<td id="S3.T1.1.1.14.14.6" class="ltx_td ltx_align_center">Operation</td>
<td id="S3.T1.1.1.14.14.7" class="ltx_td ltx_align_center">Answer based on tables</td>
</tr>
<tr id="S3.T1.1.1.15.15" class="ltx_tr">
<th id="S3.T1.1.1.15.15.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">FinQA <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.15.15.3" class="ltx_td ltx_align_center">8,281</td>
<td id="S3.T1.1.1.15.15.4" class="ltx_td ltx_align_center">Question+Table+Text</td>
<td id="S3.T1.1.1.15.15.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.15.15.6" class="ltx_td ltx_align_center">Operation</td>
<td id="S3.T1.1.1.15.15.7" class="ltx_td ltx_align_center">Answer based on tables</td>
</tr>
<tr id="S3.T1.1.1.16.16" class="ltx_tr">
<th id="S3.T1.1.1.16.16.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.16.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ConvFinQA <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2022b</a>)</cite>
</th>
<td id="S3.T1.1.1.16.16.3" class="ltx_td ltx_align_center">3892</td>
<td id="S3.T1.1.1.16.16.4" class="ltx_td ltx_align_center">Question+Table+Dialog</td>
<td id="S3.T1.1.1.16.16.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.16.16.6" class="ltx_td ltx_align_center">Operation</td>
<td id="S3.T1.1.1.16.16.7" class="ltx_td ltx_align_center">Multi-turn dialogs</td>
</tr>
<tr id="S3.T1.1.1.17.17" class="ltx_tr">
<th id="S3.T1.1.1.17.17.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.17.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">MATH <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.17.17.3" class="ltx_td ltx_align_center">12500</td>
<td id="S3.T1.1.1.17.17.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.17.17.5" class="ltx_td ltx_align_center">Number</td>
<td id="S3.T1.1.1.17.17.6" class="ltx_td ltx_align_center">Natural Language</td>
<td id="S3.T1.1.1.17.17.7" class="ltx_td ltx_align_center">Challenging competition math problems</td>
</tr>
<tr id="S3.T1.1.1.18.18" class="ltx_tr">
<th id="S3.T1.1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.18.18.1.1" class="ltx_text">Mathematical</span></th>
<th id="S3.T1.1.1.18.18.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">NumGLUE <cite class="ltx_cite ltx_citemacro_cite">Mishra et&nbsp;al. (<a href="#bib.bib104" title="" class="ltx_ref">2022b</a>)</cite>
</th>
<td id="S3.T1.1.1.18.18.3" class="ltx_td ltx_align_center">101,835</td>
<td id="S3.T1.1.1.18.18.4" class="ltx_td ltx_align_center">Question+Text</td>
<td id="S3.T1.1.1.18.18.5" class="ltx_td ltx_align_center">Number+Span</td>
<td id="S3.T1.1.1.18.18.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.18.18.7" class="ltx_td ltx_align_center">Multi-task benchmark</td>
</tr>
<tr id="S3.T1.1.1.19.19" class="ltx_tr">
<th id="S3.T1.1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.19.19.1.1" class="ltx_text">Reasoning</span></th>
<th id="S3.T1.1.1.19.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">LILA <cite class="ltx_cite ltx_citemacro_cite">Mishra et&nbsp;al. (<a href="#bib.bib103" title="" class="ltx_ref">2022a</a>)</cite>
</th>
<td id="S3.T1.1.1.19.19.3" class="ltx_td ltx_align_center">133,815</td>
<td id="S3.T1.1.1.19.19.4" class="ltx_td ltx_align_center">Question+Text</td>
<td id="S3.T1.1.1.19.19.5" class="ltx_td ltx_align_center">Free-form</td>
<td id="S3.T1.1.1.19.19.6" class="ltx_td ltx_align_center">Program</td>
<td id="S3.T1.1.1.19.19.7" class="ltx_td ltx_align_center">Multi-task benchmark</td>
</tr>
<tr id="S3.T1.1.1.20.20" class="ltx_tr">
<th id="S3.T1.1.1.20.20.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.1.1.20.20.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ARC <cite class="ltx_cite ltx_citemacro_cite">Bhakthavatsalam et&nbsp;al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.20.20.3" class="ltx_td ltx_align_center ltx_border_t">7787</td>
<td id="S3.T1.1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_t">Question</td>
<td id="S3.T1.1.1.20.20.5" class="ltx_td ltx_align_center ltx_border_t">Option</td>
<td id="S3.T1.1.1.20.20.6" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S3.T1.1.1.20.20.7" class="ltx_td ltx_align_center ltx_border_t">From science exam</td>
</tr>
<tr id="S3.T1.1.1.21.21" class="ltx_tr">
<th id="S3.T1.1.1.21.21.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.21.21.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">OpenBookQA <cite class="ltx_cite ltx_citemacro_cite">Mihaylov et&nbsp;al. (<a href="#bib.bib102" title="" class="ltx_ref">2018</a>)</cite>
</th>
<td id="S3.T1.1.1.21.21.3" class="ltx_td ltx_align_center">5,957</td>
<td id="S3.T1.1.1.21.21.4" class="ltx_td ltx_align_center">Question+Context</td>
<td id="S3.T1.1.1.21.21.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.21.21.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.21.21.7" class="ltx_td ltx_align_center">Open-book knowledges</td>
</tr>
<tr id="S3.T1.1.1.22.22" class="ltx_tr">
<th id="S3.T1.1.1.22.22.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.22.22.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">PIQA <cite class="ltx_cite ltx_citemacro_cite">Bisk et&nbsp;al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.22.22.3" class="ltx_td ltx_align_center">21000</td>
<td id="S3.T1.1.1.22.22.4" class="ltx_td ltx_align_center">Goal+Solution</td>
<td id="S3.T1.1.1.22.22.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.22.22.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.22.22.7" class="ltx_td ltx_align_center">Physical commonsense knowledge</td>
</tr>
<tr id="S3.T1.1.1.23.23" class="ltx_tr">
<th id="S3.T1.1.1.23.23.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.23.23.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">CommonsenseQA <cite class="ltx_cite ltx_citemacro_cite">Talmor et&nbsp;al. (<a href="#bib.bib140" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.23.23.3" class="ltx_td ltx_align_center">12247</td>
<td id="S3.T1.1.1.23.23.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.23.23.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.23.23.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.23.23.7" class="ltx_td ltx_align_center">Derived from ConceptNet</td>
</tr>
<tr id="S3.T1.1.1.24.24" class="ltx_tr">
<th id="S3.T1.1.1.24.24.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.24.24.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">CommonsenseQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">Talmor et&nbsp;al. (<a href="#bib.bib141" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.24.24.3" class="ltx_td ltx_align_center">14343</td>
<td id="S3.T1.1.1.24.24.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.24.24.5" class="ltx_td ltx_align_center">Yes/No</td>
<td id="S3.T1.1.1.24.24.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.24.24.7" class="ltx_td ltx_align_center">Gaming annotation with high quality</td>
</tr>
<tr id="S3.T1.1.1.25.25" class="ltx_tr">
<th id="S3.T1.1.1.25.25.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.25.25.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Event2Mind <cite class="ltx_cite ltx_citemacro_cite">Rashkin et&nbsp;al. (<a href="#bib.bib122" title="" class="ltx_ref">2018</a>)</cite>
</th>
<td id="S3.T1.1.1.25.25.3" class="ltx_td ltx_align_center">25000</td>
<td id="S3.T1.1.1.25.25.4" class="ltx_td ltx_align_center">Event</td>
<td id="S3.T1.1.1.25.25.5" class="ltx_td ltx_align_center">Intent+Reaction</td>
<td id="S3.T1.1.1.25.25.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.25.25.7" class="ltx_td ltx_align_center">Intension commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.26.26" class="ltx_tr">
<th id="S3.T1.1.1.26.26.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.26.26.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">McTaco <cite class="ltx_cite ltx_citemacro_cite">Zhou et&nbsp;al. (<a href="#bib.bib204" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.26.26.3" class="ltx_td ltx_align_center">13225</td>
<td id="S3.T1.1.1.26.26.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.26.26.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.26.26.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.26.26.7" class="ltx_td ltx_align_center">Event temporal commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.27.27" class="ltx_tr">
<th id="S3.T1.1.1.27.27.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.27.27.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">CosmosQA <cite class="ltx_cite ltx_citemacro_cite">Huang et&nbsp;al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.27.27.3" class="ltx_td ltx_align_center">35588</td>
<td id="S3.T1.1.1.27.27.4" class="ltx_td ltx_align_center">Question+Paragraph</td>
<td id="S3.T1.1.1.27.27.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.27.27.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.27.27.7" class="ltx_td ltx_align_center">Narrative commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.28.28" class="ltx_tr">
<th id="S3.T1.1.1.28.28.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.28.28.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ComValidation <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib148" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.28.28.3" class="ltx_td ltx_align_center">11997</td>
<td id="S3.T1.1.1.28.28.4" class="ltx_td ltx_align_center">Statement</td>
<td id="S3.T1.1.1.28.28.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.28.28.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.28.28.7" class="ltx_td ltx_align_center">Commonsense verification</td>
</tr>
<tr id="S3.T1.1.1.29.29" class="ltx_tr">
<th id="S3.T1.1.1.29.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.29.29.1.1" class="ltx_text">Commonsense</span></th>
<th id="S3.T1.1.1.29.29.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ComExplanation <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib148" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.29.29.3" class="ltx_td ltx_align_center">11997</td>
<td id="S3.T1.1.1.29.29.4" class="ltx_td ltx_align_center">Statement</td>
<td id="S3.T1.1.1.29.29.5" class="ltx_td ltx_align_center">Option/Free-form</td>
<td id="S3.T1.1.1.29.29.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.29.29.7" class="ltx_td ltx_align_center">Commonsense explanation</td>
</tr>
<tr id="S3.T1.1.1.30.30" class="ltx_tr">
<th id="S3.T1.1.1.30.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.30.30.1.1" class="ltx_text">Reasoning</span></th>
<th id="S3.T1.1.1.30.30.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">StrategyQA <cite class="ltx_cite ltx_citemacro_cite">Geva et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.30.30.3" class="ltx_td ltx_align_center">2,780</td>
<td id="S3.T1.1.1.30.30.4" class="ltx_td ltx_align_center">Question</td>
<td id="S3.T1.1.1.30.30.5" class="ltx_td ltx_align_center">Yes/No</td>
<td id="S3.T1.1.1.30.30.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.30.30.7" class="ltx_td ltx_align_center">Multi-hop commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.31.31" class="ltx_tr">
<th id="S3.T1.1.1.31.31.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.1.1.31.31.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Last Letter Concat. <cite class="ltx_cite ltx_citemacro_cite">Wei et&nbsp;al. (<a href="#bib.bib161" title="" class="ltx_ref">2022b</a>)</cite>
</th>
<td id="S3.T1.1.1.31.31.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.1.1.31.31.4" class="ltx_td ltx_align_center ltx_border_t">Words</td>
<td id="S3.T1.1.1.31.31.5" class="ltx_td ltx_align_center ltx_border_t">Letters</td>
<td id="S3.T1.1.1.31.31.6" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S3.T1.1.1.31.31.7" class="ltx_td ltx_align_center ltx_border_t">Rule-based</td>
</tr>
<tr id="S3.T1.1.1.32.32" class="ltx_tr">
<th id="S3.T1.1.1.32.32.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.32.32.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Coin Flip <cite class="ltx_cite ltx_citemacro_cite">Wei et&nbsp;al. (<a href="#bib.bib161" title="" class="ltx_ref">2022b</a>)</cite>
</th>
<td id="S3.T1.1.1.32.32.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.32.32.4" class="ltx_td ltx_align_center">Statement</td>
<td id="S3.T1.1.1.32.32.5" class="ltx_td ltx_align_center">Yes/No</td>
<td id="S3.T1.1.1.32.32.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.32.32.7" class="ltx_td ltx_align_center">Rule-based</td>
</tr>
<tr id="S3.T1.1.1.33.33" class="ltx_tr">
<th id="S3.T1.1.1.33.33.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.33.33.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Reverse List <cite class="ltx_cite ltx_citemacro_cite">Wei et&nbsp;al. (<a href="#bib.bib161" title="" class="ltx_ref">2022b</a>)</cite>
</th>
<td id="S3.T1.1.1.33.33.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.33.33.4" class="ltx_td ltx_align_center">List</td>
<td id="S3.T1.1.1.33.33.5" class="ltx_td ltx_align_center">Reversed List</td>
<td id="S3.T1.1.1.33.33.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.33.33.7" class="ltx_td ltx_align_center">Rule-based</td>
</tr>
<tr id="S3.T1.1.1.34.34" class="ltx_tr">
<th id="S3.T1.1.1.34.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.34.34.1.1" class="ltx_text">Symbolic</span></th>
<th id="S3.T1.1.1.34.34.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">BigBench <cite class="ltx_cite ltx_citemacro_cite">Srivastava et&nbsp;al. (<a href="#bib.bib136" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S3.T1.1.1.34.34.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.34.34.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.34.34.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.34.34.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.34.34.7" class="ltx_td ltx_align_center">Contains multiple symbolic reasoning datasets</td>
</tr>
<tr id="S3.T1.1.1.35.35" class="ltx_tr">
<th id="S3.T1.1.1.35.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.35.35.1.1" class="ltx_text">Reasoning</span></th>
<th id="S3.T1.1.1.35.35.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">BigBench-Hard
<cite class="ltx_cite ltx_citemacro_cite">Suzgun et&nbsp;al. (<a href="#bib.bib138" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S3.T1.1.1.35.35.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.35.35.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.35.35.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.35.35.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.35.35.7" class="ltx_td ltx_align_center">Contains multiple symbolic reasoning datasets</td>
</tr>
<tr id="S3.T1.1.1.36.36" class="ltx_tr">
<th id="S3.T1.1.1.36.36.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.1.1.36.36.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ReClor <cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a href="#bib.bib182" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.36.36.3" class="ltx_td ltx_align_center ltx_border_t">6,138</td>
<td id="S3.T1.1.1.36.36.4" class="ltx_td ltx_align_center ltx_border_t">Question+Context</td>
<td id="S3.T1.1.1.36.36.5" class="ltx_td ltx_align_center ltx_border_t">Option</td>
<td id="S3.T1.1.1.36.36.6" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S3.T1.1.1.36.36.7" class="ltx_td ltx_align_center ltx_border_t">Questions from GMAT and LSAT</td>
</tr>
<tr id="S3.T1.1.1.37.37" class="ltx_tr">
<th id="S3.T1.1.1.37.37.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.37.37.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">LogiQA <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib88" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.37.37.3" class="ltx_td ltx_align_center">8,678</td>
<td id="S3.T1.1.1.37.37.4" class="ltx_td ltx_align_center">Question+Paragraph</td>
<td id="S3.T1.1.1.37.37.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.37.37.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.37.37.7" class="ltx_td ltx_align_center">Questions from China Civil Service Exam</td>
</tr>
<tr id="S3.T1.1.1.38.38" class="ltx_tr">
<th id="S3.T1.1.1.38.38.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.38.38.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ProofWriter <cite class="ltx_cite ltx_citemacro_cite">Tafjord et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.38.38.3" class="ltx_td ltx_align_center">20192</td>
<td id="S3.T1.1.1.38.38.4" class="ltx_td ltx_align_center">Question+Rule</td>
<td id="S3.T1.1.1.38.38.5" class="ltx_td ltx_align_center">Answer+Proof</td>
<td id="S3.T1.1.1.38.38.6" class="ltx_td ltx_align_center">Entailment Tree</td>
<td id="S3.T1.1.1.38.38.7" class="ltx_td ltx_align_center">Reasoning process generation</td>
</tr>
<tr id="S3.T1.1.1.39.39" class="ltx_tr">
<th id="S3.T1.1.1.39.39.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.39.39.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">FOLIO <cite class="ltx_cite ltx_citemacro_cite">Han et&nbsp;al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S3.T1.1.1.39.39.3" class="ltx_td ltx_align_center">1435</td>
<td id="S3.T1.1.1.39.39.4" class="ltx_td ltx_align_center">Conclusion+Premise</td>
<td id="S3.T1.1.1.39.39.5" class="ltx_td ltx_align_center">Yes/No</td>
<td id="S3.T1.1.1.39.39.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.39.39.7" class="ltx_td ltx_align_center">First-order logic</td>
</tr>
<tr id="S3.T1.1.1.40.40" class="ltx_tr">
<th id="S3.T1.1.1.40.40.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.40.40.1.1" class="ltx_text">Logical</span></th>
<th id="S3.T1.1.1.40.40.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">DEER <cite class="ltx_cite ltx_citemacro_cite">Yang et&nbsp;al. (<a href="#bib.bib170" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S3.T1.1.1.40.40.3" class="ltx_td ltx_align_center">1,200</td>
<td id="S3.T1.1.1.40.40.4" class="ltx_td ltx_align_center">Fact</td>
<td id="S3.T1.1.1.40.40.5" class="ltx_td ltx_align_center">Rule</td>
<td id="S3.T1.1.1.40.40.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.40.40.7" class="ltx_td ltx_align_center">Inductive reasoning</td>
</tr>
<tr id="S3.T1.1.1.41.41" class="ltx_tr">
<th id="S3.T1.1.1.41.41.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.41.41.1.1" class="ltx_text">Reasoning</span></th>
<th id="S3.T1.1.1.41.41.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">PrOntoQA <cite class="ltx_cite ltx_citemacro_cite">Saparov and He (<a href="#bib.bib125" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S3.T1.1.1.41.41.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.41.41.4" class="ltx_td ltx_align_center">Question+Context</td>
<td id="S3.T1.1.1.41.41.5" class="ltx_td ltx_align_center">Yes/No+Proccess</td>
<td id="S3.T1.1.1.41.41.6" class="ltx_td ltx_align_center">First-Order Logic</td>
<td id="S3.T1.1.1.41.41.7" class="ltx_td ltx_align_center">Deductive reasoning</td>
</tr>
<tr id="S3.T1.1.1.42.42" class="ltx_tr">
<th id="S3.T1.1.1.42.42.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.1.1.42.42.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VCR <cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a href="#bib.bib187" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S3.T1.1.1.42.42.3" class="ltx_td ltx_align_center ltx_border_t">264,720</td>
<td id="S3.T1.1.1.42.42.4" class="ltx_td ltx_align_center ltx_border_t">Question+Image</td>
<td id="S3.T1.1.1.42.42.5" class="ltx_td ltx_align_center ltx_border_t">Option</td>
<td id="S3.T1.1.1.42.42.6" class="ltx_td ltx_align_center ltx_border_t">Natural Language</td>
<td id="S3.T1.1.1.42.42.7" class="ltx_td ltx_align_center ltx_border_t">Visual commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.43.43" class="ltx_tr">
<th id="S3.T1.1.1.43.43.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.43.43.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">VisualCOMET <cite class="ltx_cite ltx_citemacro_cite">Park et&nbsp;al. (<a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.43.43.3" class="ltx_td ltx_align_center">1,465,704</td>
<td id="S3.T1.1.1.43.43.4" class="ltx_td ltx_align_center">Image+Event</td>
<td id="S3.T1.1.1.43.43.5" class="ltx_td ltx_align_center">Action+Intent</td>
<td id="S3.T1.1.1.43.43.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.43.43.7" class="ltx_td ltx_align_center">Visual commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.44.44" class="ltx_tr">
<th id="S3.T1.1.1.44.44.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.44.44.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">PMR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Dong et&nbsp;al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S3.T1.1.1.44.44.3" class="ltx_td ltx_align_center">15,360</td>
<td id="S3.T1.1.1.44.44.4" class="ltx_td ltx_align_center">Image+Background</td>
<td id="S3.T1.1.1.44.44.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.44.44.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.44.44.7" class="ltx_td ltx_align_center">Premise-based multi-modal reasoning</td>
</tr>
<tr id="S3.T1.1.1.45.45" class="ltx_tr">
<th id="S3.T1.1.1.45.45.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.45.45.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ScienceQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lu et&nbsp;al. (<a href="#bib.bib91" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S3.T1.1.1.45.45.3" class="ltx_td ltx_align_center">21,208</td>
<td id="S3.T1.1.1.45.45.4" class="ltx_td ltx_align_center">Q+Image+Context</td>
<td id="S3.T1.1.1.45.45.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.45.45.6" class="ltx_td ltx_align_center">Natural Language</td>
<td id="S3.T1.1.1.45.45.7" class="ltx_td ltx_align_center">Multi-modal reasoning with NL rationales</td>
</tr>
<tr id="S3.T1.1.1.46.46" class="ltx_tr">
<th id="S3.T1.1.1.46.46.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.46.46.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">VLEP <cite class="ltx_cite ltx_citemacro_cite">Lei et&nbsp;al. (<a href="#bib.bib73" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.46.46.3" class="ltx_td ltx_align_center">28,726</td>
<td id="S3.T1.1.1.46.46.4" class="ltx_td ltx_align_center">Premise+Video</td>
<td id="S3.T1.1.1.46.46.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.46.46.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.46.46.7" class="ltx_td ltx_align_center">Video event prediction</td>
</tr>
<tr id="S3.T1.1.1.47.47" class="ltx_tr">
<th id="S3.T1.1.1.47.47.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.47.47.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">CLEVRER <cite class="ltx_cite ltx_citemacro_cite">Yi et&nbsp;al. (<a href="#bib.bib178" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S3.T1.1.1.47.47.3" class="ltx_td ltx_align_center">305,280</td>
<td id="S3.T1.1.1.47.47.4" class="ltx_td ltx_align_center">Question+Video</td>
<td id="S3.T1.1.1.47.47.5" class="ltx_td ltx_align_center">Option/Free-form</td>
<td id="S3.T1.1.1.47.47.6" class="ltx_td ltx_align_center">Program</td>
<td id="S3.T1.1.1.47.47.7" class="ltx_td ltx_align_center">Video temporal and causal reasoning</td>
</tr>
<tr id="S3.T1.1.1.48.48" class="ltx_tr">
<th id="S3.T1.1.1.48.48.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.48.48.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">STAR <cite class="ltx_cite ltx_citemacro_cite">Wu et&nbsp;al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.48.48.3" class="ltx_td ltx_align_center">600,000</td>
<td id="S3.T1.1.1.48.48.4" class="ltx_td ltx_align_center">Question+Video</td>
<td id="S3.T1.1.1.48.48.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.48.48.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.48.48.7" class="ltx_td ltx_align_center">Video situated reasoning</td>
</tr>
<tr id="S3.T1.1.1.49.49" class="ltx_tr">
<th id="S3.T1.1.1.49.49.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.49.49.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">NEXT-QA <cite class="ltx_cite ltx_citemacro_cite">Xiao et&nbsp;al. (<a href="#bib.bib166" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S3.T1.1.1.49.49.3" class="ltx_td ltx_align_center">47,692</td>
<td id="S3.T1.1.1.49.49.4" class="ltx_td ltx_align_center">Question+Video</td>
<td id="S3.T1.1.1.49.49.5" class="ltx_td ltx_align_center">Option</td>
<td id="S3.T1.1.1.49.49.6" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T1.1.1.49.49.7" class="ltx_td ltx_align_center">Video temporal,causal,commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.50.50" class="ltx_tr">
<th id="S3.T1.1.1.50.50.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.50.50.1.1" class="ltx_text">Multimodal</span></th>
<th id="S3.T1.1.1.50.50.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Causal-VidQA <cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2022a</a>)</cite>
</th>
<td id="S3.T1.1.1.50.50.3" class="ltx_td ltx_align_center">107,600</td>
<td id="S3.T1.1.1.50.50.4" class="ltx_td ltx_align_center">Question+Video</td>
<td id="S3.T1.1.1.50.50.5" class="ltx_td ltx_align_center">Free-form</td>
<td id="S3.T1.1.1.50.50.6" class="ltx_td ltx_align_center">Natural Language</td>
<td id="S3.T1.1.1.50.50.7" class="ltx_td ltx_align_center">Video causal and commonsense reasoning</td>
</tr>
<tr id="S3.T1.1.1.51.51" class="ltx_tr">
<th id="S3.T1.1.1.51.51.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S3.T1.1.1.51.51.1.1" class="ltx_text">Reasoning</span></th>
<th id="S3.T1.1.1.51.51.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">News-KVQA <cite class="ltx_cite ltx_citemacro_cite">Gupta and Gupta (<a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S3.T1.1.1.51.51.3" class="ltx_td ltx_align_center ltx_border_bb">1,041,352</td>
<td id="S3.T1.1.1.51.51.4" class="ltx_td ltx_align_center ltx_border_bb">Q+V+KG</td>
<td id="S3.T1.1.1.51.51.5" class="ltx_td ltx_align_center ltx_border_bb">Option</td>
<td id="S3.T1.1.1.51.51.6" class="ltx_td ltx_align_center ltx_border_bb">✗</td>
<td id="S3.T1.1.1.51.51.7" class="ltx_td ltx_align_center ltx_border_bb">Video reasoning with external knowledge</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>추론에 대한 벤치마크 및 태스크 개요.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Mathematical Reasoning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">수학적 추론은 모델의 추론력을 측정하는 데 자주 사용된다. 초기 벤치마크는 간단한 산술 연산<cite class="ltx_cite ltx_citemacro_cite">Hosseini et al. (<a class="ltx_ref" href="#bib.bib44" title="">2014</a>); Koncel-Kedziorski et al. (<a class="ltx_ref" href="#bib.bib65" title="">2015</a>); Roy and Roth (<a class="ltx_ref" href="#bib.bib123" title="">2015</a>); Koncel-Kedziorski et al. (<a class="ltx_ref" href="#bib.bib66" title="">2016</a>)</cite>를 포함한다. <cite class="ltx_cite ltx_citemacro_citet">Ling et al. (<a class="ltx_ref" href="#bib.bib84" title="">2017</a>)</cite>는 자연어 형태로 추론 과정을 레이블링하고, <cite class="ltx_cite ltx_citemacro_citet">Amini et al. (<a class="ltx_ref" href="#bib.bib3" title="">2019</a>)</cite>는 프로그램 형태로 추론 과정을 레이블링하여 AQUA를 구축한다. 이후 벤치마크 <cite class="ltx_cite ltx_citemacro_citep">(Miao et al., <a class="ltx_ref" href="#bib.bib101" title="">2020</a>; Patel et al., <a class="ltx_ref" href="#bib.bib114" title="">2021</a>; Cobbe et al., <a class="ltx_ref" href="#bib.bib17" title="">2021</a>; Gao et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>)</cite>는 더 복잡하고 다양한 질문을 포함하고 있다. <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib207" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib14" title="">2021</a>, <a class="ltx_ref" href="#bib.bib15" title="">2022b</a>)</cite>는 테이블 내용에 기반한 추론을 필요로 한다. 일반적인 벤치마크인 <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="#bib.bib42" title="">2021</a>); Mishra et al. (<a class="ltx_ref" href="#bib.bib103" title="">2022a</a>, <a class="ltx_ref" href="#bib.bib104" title="">b</a>)</cite>와 독해 형태의 벤치마크인 <cite class="ltx_cite ltx_citemacro_cite">Dua et al. (<a class="ltx_ref" href="#bib.bib27" title="">2019</a>); Chen et al. (<a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>도 있다. 최근 <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="#bib.bib183" title="">2021a</a>)</cite>는 계층적 추론과 지식을 활용하여 수학적 추론 능력을 갖춘 사전 학습 모델을 부여하였다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Commonsense Reasoning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">상식 추론은 일상 세계에서 일반적으로 알려져 있고 일반적으로 인식되는 지식을 바탕으로 추론, 판단, 이해를 하는 과정이다. 상식 지식을 습득하고 이해하는 방법은 상식 추론에 직면한 모델의 주요 장애물이다. 많은 벤치마크와 태스크가 상식 이해 <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="#bib.bib140" title="">2019</a>, <a class="ltx_ref" href="#bib.bib141" title="">2021</a>; Bhakthavatsalam et al., <a class="ltx_ref" href="#bib.bib6" title="">2021</a>; Mihaylov et al., <a class="ltx_ref" href="#bib.bib102" title="">2018</a>; Geva et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>; Huang et al., <a class="ltx_ref" href="#bib.bib50" title="">2019</a>; Bisk et al., <a class="ltx_ref" href="#bib.bib8" title="">2020</a>)</cite>, 이벤트 시간적 상식 추론 <cite class="ltx_cite ltx_citemacro_citep">(Rashkin et al., <a class="ltx_ref" href="#bib.bib122" title="">2018</a>; Zhou et al., <a class="ltx_ref" href="#bib.bib204" title="">2019</a>)</cite> 및 상식 검증 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib148" title="">2019</a>)</cite>에 초점을 맞추어 제안된다.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Symbolic Reasoning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">여기서 상징적 추론은 특히 인간에게 단순하지만 LLM에 도전적인 일부 간단한 연산의 시뮬레이션을 나타낸다. 마지막 문자 연결, 동전 뒤집기 및 역방향 목록 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite>는 가장 일반적으로 사용되는 기호 추론 작업이다. 또한 협업 벤치마크인 BigBench<cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="#bib.bib136" title="">2022</a>)</cite>와 BigBench-Hard<cite class="ltx_cite ltx_citemacro_citep">(Suzgun et al., <a class="ltx_ref" href="#bib.bib138" title="">2023</a>)</cite>에는 상태 추적 및 객체 카운팅과 같은 여러 심볼릭 추론 데이터 세트도 포함되어 있다.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Logical Reasoning</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.1">논리 추론은 연역추론, 귀납추론, 귀납추론<cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="#bib.bib180" title="">2023a</a>)</cite>로 나뉜다. 연역적 추론은 일반적인 전제<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib88" title="">2020</a>; Yu et al., <a class="ltx_ref" href="#bib.bib182" title="">2020</a>; Tafjord et al., <a class="ltx_ref" href="#bib.bib139" title="">2021</a>; Han et al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>에서 결론을 도출한다. 귀납적 추론은 특별한 경우 <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="#bib.bib170" title="">2022</a>)</cite>로부터 일반적인 결론을 도출한다. 귀납적 추론은 관찰된 현상 <cite class="ltx_cite ltx_citemacro_cite">Saparov and He (<a class="ltx_ref" href="#bib.bib125" title="">2023</a>)</cite>에 대한 합리적인 설명을 제공한다.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Multi-modal Reasoning</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS5.p1.1">현실 세계에서 추론은 또한 텍스트 이외의 양식에서 정보를 포함하며 시각적 양식들이 가장 널리 퍼져 있다. 이를 위해 시각적인 멀티모달 추론을 위한 많은 벤치마크들이 제안되었는데, 그 중 ScienceQA<cite class="ltx_cite ltx_citemacro_citep">(Zellers et al., <a class="ltx_ref" href="#bib.bib187" title="">2019</a>; Park et al., <a class="ltx_ref" href="#bib.bib113" title="">2020</a>; Dong et al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Lu et al., <a class="ltx_ref" href="#bib.bib91" title="">2022</a>)</cite>는 추론 과정에 주석을 달며 가장 많이 사용되는 시각적인 멀티모달 추론 벤치마크이다. 비디오 멀티모달 추론 <cite class="ltx_cite ltx_citemacro_citep">(Lei et al., <a class="ltx_ref" href="#bib.bib73" title="">2020</a>; Yi et al., <a class="ltx_ref" href="#bib.bib178" title="">2020</a>; Wu et al., <a class="ltx_ref" href="#bib.bib163" title="">2021</a>; Xiao et al., <a class="ltx_ref" href="#bib.bib166" title="">2021</a>; Li et al., <a class="ltx_ref" href="#bib.bib75" title="">2022a</a>; Gupta and Gupta, <a class="ltx_ref" href="#bib.bib36" title="">2022</a>)</cite>는 시각적 멀티모달 추론에 비해 추가적인 시간 정보를 도입하기 때문에 더 어렵다.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Metrics</h3>

<section id="S3.SS6.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Accuracy</h5>

<div id="S3.SS6.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS6.SSS0.Px1.p1.1">정확도는 분류 작업에 대한 모델의 능력을 평가하는 데 사용되며 다중 선택 <cite class="ltx_cite ltx_citemacro_citep">(Ling et al., <a class="ltx_ref" href="#bib.bib84" title="">2017</a>; Mihaylov et al., <a class="ltx_ref" href="#bib.bib102" title="">2018</a>; Liu et al., <a class="ltx_ref" href="#bib.bib88" title="">2020</a>; Lu et al., <a class="ltx_ref" href="#bib.bib91" title="">2022</a>)</cite> 및 yes/no <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="#bib.bib141" title="">2021</a>; Geva et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>; Han et al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite> 작업에 일반적으로 사용된다.</p>
</div>
<div id="S3.SS6.SSS0.Px1.p2" class="ltx_para">
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\mathrm{Accuracy}=\frac{\mathrm{N_{correct}}}{\mathrm{N_{total}}}" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mi id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml">Accuracy</mi><mo id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml">=</mo><mfrac id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><msub id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="S3.E7.m1.1.1.3.2.2" xref="S3.E7.m1.1.1.3.2.2.cmml">N</mi><mi id="S3.E7.m1.1.1.3.2.3" xref="S3.E7.m1.1.1.3.2.3.cmml">correct</mi></msub><msub id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml"><mi mathvariant="normal" id="S3.E7.m1.1.1.3.3.2" xref="S3.E7.m1.1.1.3.3.2.cmml">N</mi><mi id="S3.E7.m1.1.1.3.3.3" xref="S3.E7.m1.1.1.3.3.3.cmml">total</mi></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"></eq><ci id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2">Accuracy</ci><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><divide id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3"></divide><apply id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.3.2.2">N</ci><ci id="S3.E7.m1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.3.2.3">correct</ci></apply><apply id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.3.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.3.3.2">N</ci><ci id="S3.E7.m1.1.1.3.3.3.cmml" xref="S3.E7.m1.1.1.3.3.3">total</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\mathrm{Accuracy}=\frac{\mathrm{N_{correct}}}{\mathrm{N_{total}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS6.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">EM and F1</h5>

<div id="S3.SS6.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS6.SSS0.Px2.p1.1">EM 및 F1은 자유 형태 <cite class="ltx_cite ltx_citemacro_citep">(Mishra et al., <a class="ltx_ref" href="#bib.bib103" title="">2022a</a>; Wang et al., <a class="ltx_ref" href="#bib.bib148" title="">2019</a>; Yi et al., <a class="ltx_ref" href="#bib.bib178" title="">2020</a>)</cite> 및 스팬 추출 <cite class="ltx_cite ltx_citemacro_citep">(Dua et al., <a class="ltx_ref" href="#bib.bib27" title="">2019</a>; Zhu et al., <a class="ltx_ref" href="#bib.bib207" title="">2021</a>; Mishra et al., <a class="ltx_ref" href="#bib.bib104" title="">2022b</a>)</cite> 작업을 평가하는 데 사용되는 메트릭이다. 둘 다 토큰 수준에서 계산됩니다.</p>
<table id="S8.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E8.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{F1}=\frac{2\cdot\mathrm{P}\cdot\mathrm{R}}{\mathrm{P}+\mathrm{R}}" display="inline"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><mi id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">F1</mi><mo id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml"><mfrac id="S3.E8.m1.1.1.3a" xref="S3.E8.m1.1.1.3.cmml"><mrow id="S3.E8.m1.1.1.3.2" xref="S3.E8.m1.1.1.3.2.cmml"><mn id="S3.E8.m1.1.1.3.2.2" xref="S3.E8.m1.1.1.3.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E8.m1.1.1.3.2.1" xref="S3.E8.m1.1.1.3.2.1.cmml">⋅</mo><mi mathvariant="normal" id="S3.E8.m1.1.1.3.2.3" xref="S3.E8.m1.1.1.3.2.3.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E8.m1.1.1.3.2.1a" xref="S3.E8.m1.1.1.3.2.1.cmml">⋅</mo><mi mathvariant="normal" id="S3.E8.m1.1.1.3.2.4" xref="S3.E8.m1.1.1.3.2.4.cmml">R</mi></mrow><mrow id="S3.E8.m1.1.1.3.3" xref="S3.E8.m1.1.1.3.3.cmml"><mi mathvariant="normal" id="S3.E8.m1.1.1.3.3.2" xref="S3.E8.m1.1.1.3.3.2.cmml">P</mi><mo id="S3.E8.m1.1.1.3.3.1" xref="S3.E8.m1.1.1.3.3.1.cmml">+</mo><mi mathvariant="normal" id="S3.E8.m1.1.1.3.3.3" xref="S3.E8.m1.1.1.3.3.3.cmml">R</mi></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"></eq><ci id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2">F1</ci><apply id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3"><divide id="S3.E8.m1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.3"></divide><apply id="S3.E8.m1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.3.2"><ci id="S3.E8.m1.1.1.3.2.1.cmml" xref="S3.E8.m1.1.1.3.2.1">⋅</ci><cn type="integer" id="S3.E8.m1.1.1.3.2.2.cmml" xref="S3.E8.m1.1.1.3.2.2">2</cn><ci id="S3.E8.m1.1.1.3.2.3.cmml" xref="S3.E8.m1.1.1.3.2.3">P</ci><ci id="S3.E8.m1.1.1.3.2.4.cmml" xref="S3.E8.m1.1.1.3.2.4">R</ci></apply><apply id="S3.E8.m1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.3.3"><plus id="S3.E8.m1.1.1.3.3.1.cmml" xref="S3.E8.m1.1.1.3.3.1"></plus><ci id="S3.E8.m1.1.1.3.3.2.cmml" xref="S3.E8.m1.1.1.3.3.2">P</ci><ci id="S3.E8.m1.1.1.3.3.3.cmml" xref="S3.E8.m1.1.1.3.3.3">R</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\displaystyle\mathrm{F1}=\frac{2\cdot\mathrm{P}\cdot\mathrm{R}}{\mathrm{P}+\mathrm{R}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E9.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{EM}=\frac{\sum\mathbb{I}[A=A^{\prime}]}{\mathrm{N_{total}}}" display="inline"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.2" xref="S3.E9.m1.1.2.cmml"><mi id="S3.E9.m1.1.2.2" xref="S3.E9.m1.1.2.2.cmml">EM</mi><mo id="S3.E9.m1.1.2.1" xref="S3.E9.m1.1.2.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><mfrac id="S3.E9.m1.1.1a" xref="S3.E9.m1.1.1.cmml"><mrow id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.cmml"><mo id="S3.E9.m1.1.1.1.2" xref="S3.E9.m1.1.1.1.2.cmml">∑</mo><mrow id="S3.E9.m1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.cmml"><mi id="S3.E9.m1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.3.cmml">𝕀</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.1.2.cmml">A</mi><mo id="S3.E9.m1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.1.cmml">=</mo><msup id="S3.E9.m1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.1.3.2" xref="S3.E9.m1.1.1.1.1.1.1.1.3.2.cmml">A</mi><mo id="S3.E9.m1.1.1.1.1.1.1.1.3.3" xref="S3.E9.m1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S3.E9.m1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><msub id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E9.m1.1.1.3.2" xref="S3.E9.m1.1.1.3.2.cmml">N</mi><mi id="S3.E9.m1.1.1.3.3" xref="S3.E9.m1.1.1.3.3.cmml">total</mi></msub></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.2.cmml" xref="S3.E9.m1.1.2"><eq id="S3.E9.m1.1.2.1.cmml" xref="S3.E9.m1.1.2.1"></eq><ci id="S3.E9.m1.1.2.2.cmml" xref="S3.E9.m1.1.2.2">EM</ci><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1"><divide id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1"></divide><apply id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"><sum id="S3.E9.m1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.2"></sum><apply id="S3.E9.m1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1"><times id="S3.E9.m1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.2"></times><ci id="S3.E9.m1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.3">𝕀</ci><apply id="S3.E9.m1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E9.m1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1"><eq id="S3.E9.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.1"></eq><ci id="S3.E9.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2">𝐴</ci><apply id="S3.E9.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.3.2">𝐴</ci><ci id="S3.E9.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.3.3">′</ci></apply></apply></apply></apply></apply><apply id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.3">subscript</csymbol><ci id="S3.E9.m1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.3.2">N</ci><ci id="S3.E9.m1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.3.3">total</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\displaystyle\mathrm{EM}=\frac{\sum\mathbb{I}[A=A^{\prime}]}{\mathrm{N_{total}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS6.SSS0.Px2.p1.2">여기서 P와 R은 정밀도와 재현도를 나타내며, EM은 정확히 같은 예측과 답의 비율을 계산한다.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">이 절에서는 X-of-thought의 구성(§<a class="ltx_ref" href="#S4.SS1" title="4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.1</span></a>), X-of-thought의 구조적 변형(§<a class="ltx_ref" href="#S4.SS2" title="4.2 XoT Structural Variants ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.2</span></a>), X-of-thought의 향상된 방법(§<a class="ltx_ref" href="#S4.SS3" title="4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3</span></a>)의 세 가지 분류를 통해 X-of-thought 추론을 탐구한다.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<div id="S4.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:514.6pt;vertical-align:-510.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-121.1pt,1.2pt) scale(0.6415635648267,0.6415635648267) ;"><span id="S4.F1.1.1" class="ltx_ERROR undefined">{forest}</span>
<p class="ltx_p" id="S4.F1.1.2">[XoT  <br class="ltx_break"/>Construction   (§<a class="ltx_ref" href="#S4.SS1" title="4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.1</span></a>) [XoT  <br class="ltx_break"/>Construction   (§<a class="ltx_ref" href="#S4.SS1" title="4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.1</span></a>) [XoT  <br class="ltx_break"/>Variants  (§<a idx=6></cite>, ToT  <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>)</cite>, SoT  <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib12" title="">2022a</a>)</cite>] [XoT  <br class="ltx_break"/>Variants  (§<a class="ltx_ref" href="#S4.SS2" title="4.2 XoT Structural Variants ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.2</span></a>) [XoT  <br class="ltx_break"/>Variants  (§<a idx=6></cite>, ToT  <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>)</cite>, SoT  <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib12" title="">2022a</a>)</cite>] [XoT  <br class="ltx_break"/>Variants  (§<a class="ltx_ref" href="#S4.SS2" title="4.2 XoT Structural Variants ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.2</span></a>) [X <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite>, Coin Flip <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite>, BigBench-Hard <cite class="ltx_cite ltx_citemacro_cite">Srivastava et al. (<a class="ltx_ref" href="#bib.bib136" title="">2022</a>)</cite>, ProofWriter <cite class="ltx_cite ltx_citemacro_cite">Tafjord et al. (<a class="ltx_ref" href="#bib.bib139" title="">2021</a>)</cite>,  <br class="ltx_break"/>FOLIO <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>, PrOntoQA <cite class="ltx_cite ltx_citemacro_cite">Saparov and He (<a class="ltx_ref" href="#bib.bib125" title="">2023</a>)</cite>, leaf, text width=44.6em ] ] [Image: ScienceQA <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a class="ltx_ref" href="#bib.bib91" title="">2022</a>)</cite>, VCR <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a class="ltx_ref" href="#bib.bib187" title="">2019</a>)</cite>, VisualCOMET <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a class="ltx_ref" href="#bib.bib113" title="">2020</a>)</cite> <br class="ltx_break"/>Video: CLEVRER <cite class="ltx_cite ltx_citemacro_cite">Yi et al. (<a class="ltx_ref" href="#bib.bib178" title="">2020</a>)</cite>, STAR <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="#bib.bib163" title="">2021</a>)</cite>, NExT-QA <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="#bib.bib166" title="">2021</a>)</cite>, leaf, text width=44.6em ] ] ]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span>XoT Methods, Frontier Application, Future Direction, Benchmarks.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Construction Approach</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">철저한 분석 후, 우리는 X-of-thought의 구성을 다음과 같이 설명하는 1) Manual XoT, 2) Automatic XoT, 3) Semi-automatic XoT의 세 가지 범주로 나눈다.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Manual XoT</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">대형 언어 모델은 프롬프트를 통해 적은 샷의 인컨텍스트 학습을 수행하지만, 여전히 추론 작업에 한계가 있다. 대형 언어 모델의 잠재적인 추론 능력을 탐구하기 위해 한 가지 표준 접근 방식은 시연에서 다양한 형태의 생각을 제공하는 것이다.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite> 먼저 자연 언어 형식의 근거를 데모에 수동으로 제공함으로써 연쇄적 사고 프롬프트(Few-shot CoT)를 제안한다. 추론 과정에서 확실성을 더 보장하고 추론 경로와 답변 간의 불일치를 줄이기 위해 PAL<cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>)</cite>, PoT<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib12" title="">2022a</a>)</cite> 및 NLEP<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib194" title="">2023e</a>)</cite> 주석이 달린 근거로서 프로그래밍 언어를 활용하여 문제 해결을 실행 가능한 파이썬 프로그램으로 변환한다. 한편, 자연어와 프로그래밍 언어의 장점을 모두 취하고 추론 출력의 신뢰도를 높이기 위해 MathPrompter <cite class="ltx_cite ltx_citemacro_citep">(Imani et al., <a class="ltx_ref" href="#bib.bib53" title="">2023</a>)</cite>는 Zero-shot chain-of-thought prompting을 사용하여 다수의 대수식 또는 파이썬 함수를 생성함으로써 서로 검증하고 결과의 신뢰성을 향상시킬 수 있다. 또한, 더 많은 추론 단계를 가진 체인과 같은 시연에서 샘플의 추론 복잡도는 성능 향상을 가져오기 때문에 <cite class="ltx_cite ltx_citemacro_citet">Fu et al. (<a class="ltx_ref" href="#bib.bib30" title="">2023a</a>)</cite>는 복잡도 기반 프롬프트를 제안하며, 여기서 고복잡도 논리 중 투표를 수행하여 최종 답변을 얻는다.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">수동으로 구성된 X-of-thought 방법은 다양한 유형의 단계별 중간 추론 과정을 데모에 추가하여 인-컨텍스트 학습을 확장한다. 그들은 LLM들이 추론 경로들을 모방하고 생성할 수 있게 한다. 수동 XoT 방법은 복잡한 작업, 즉 수학적 추론, 상식 추론, 상징 추론 등에 대한 인간의 이해와 성능에 대한 신뢰성뿐만 아니라 더 큰 해석 가능성을 제공하지만, 근거의 수동 주석은 상당한 비용을 수반하고 시연 선택 및 작업 일반화의 어려움과 같은 단점을 겪는다. 구체적으로, 다른 작업들은 다른 방식의 데모를 필요로 한다. 따라서 다른 작업들은 §<a class="ltx_ref" href="#S4.SS1.SSS2" title="4.1.2 Automatic XoT ‣ 4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>에서 논의한 것처럼 추론 경로를 자동으로 구성하려고 시도한다.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Automatic XoT</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib161" title="">2022b</a>)</cite>는 몇 번의 샷 설정에서 태스크별 예시로 LLM의 복잡한 추론 능력을 이끌어내어 확장성과 일반화를 제한한다. 수공예 소샷 예시의 비용을 줄이기 위해 <cite class="ltx_cite ltx_citemacro_citet">Kojima et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>는 매직 문구 <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">Let’s step by step</span> after question을 도입하여 제로샷 CoT를 제안하며, 이는 LLMs가 제로샷 방식으로 추론 체인을 생성할 수 있게 한다. 그러나 제로샷 CoT는 많은 실수를 수반하며 품질이 좋지 않은 추론 경로를 겪는다. 시연의 다양성은 추론 체인 생성에 중요한 역할을 하기 때문에 Auto-CoT<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib196" title="">2023f</a>)</cite>는 클러스터링과 대표 예제 선택을 통해 자동으로 시연을 생성하므로 다양성을 향상시키고 Few-shot CoT의 성능과 일관되게 일치하거나 초과한다. COSP <cite class="ltx_cite ltx_citemacro_citep">(Wan et al., <a class="ltx_ref" href="#bib.bib145" title="">2023</a>)</cite> introduces the outcome entropy of the question to help demonstration selection. <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="#bib.bib167" title="">2023</a>)</cite>는 깁스 샘플링을 반복적으로 사용하여 효과적인 CoT 프롬프트를 찾기 위한 Reprompting을 제안한다. 한편, 추론 체인의 일부 오류는 누락된 단계 오류에서 비롯되며, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib153" title="">2023f</a>)</cite>는 제로샷 CoT를 Plan-and-Solve(PS)로 확장한다. 전체 태스크를 더 작은 서브 태스크로 분할하는 계획을 고안하고 더 상세한 지침을 사용하여 계획에 따라 서브 태스크를 수행한다. LogiCoT<cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="#bib.bib201" title="">2023c</a>)</cite>는 심볼릭 로직을 사용하여 제로샷 추론 과정을 검증하여 추론 오류를 줄인다. 또한, PoT<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib12" title="">2022a</a>)</cite>는 Codex와 같은 언어 모델을 탐색하여 <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.2">Python 프로그램을 단계별로 작성하자…</span>을 추가하여 제로샷 설정에서 수학 문제를 해결하기 위한 실행 가능한 Python 프로그램을 생성하여 중간 추론 단계의 오류를 완화한다. 일부 작업에서는 추론 문제를 해결하기 위해 에이전트를 도입합니다. 예를 들어, 에이전트 명령어 <cite class="ltx_cite ltx_citemacro_citep">(Crispino et al., <a class="ltx_ref" href="#bib.bib18" title="">2023a</a>)</cite>는 에이전트를 활용하여 태스크 관련 정보 지침을 생성하며, 이는 LLM이 제로 샷 추론을 수행하도록 안내한다.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">수동 XoT와 달리 제로 샷 프롬프트 엔지니어링 또는 샘플링을 사용하는 자동 XoT는 확장 가능하며 인간의 개입 없이 도메인 간에 일반화될 수 있다. 그러나 인간의 정렬 부족으로 인해 자동으로 생성된 연쇄적 사고는 열악한 품질, 환각 및 사실 불일치와 같은 문제에 직면한다. 따라서 §<a class="ltx_ref" href="#S4.SS1.SSS3" title="4.1.3 Semi-automatic XoT ‣ 4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>에 소개된 XoT를 반자동 방식으로 구성할 필요가 있다.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Semi-automatic XoT</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">반자동 XoT 방법은 수동 및 자동 구축 방법 모두의 장점을 통합한다. <cite class="ltx_cite ltx_citemacro_citet">Shao et al. (<a class="ltx_ref" href="#bib.bib130" title="">2023</a>)</cite>는 AutoCoT에서 인간 정렬의 부족을 완화하면서, 몇 개의 인간 주석이 있는 예제를 사용하여 모델을 프롬프트하여 교대로 전진-후진 프로세스를 통해 더 많은 예제를 생성하고 효과적인 데모를 선택하여 더 나은 추론을 이끌어내는 Synthetic Prompting을 제안한다. 이전 작업은 수동 주석의 문제를 해결했지만 데모 선택도 성능에 상당한 영향을 미칠 수 있다. Automate-CoT<cite class="ltx_cite ltx_citemacro_citep">(Shum et al., <a class="ltx_ref" href="#bib.bib135" title="">2023</a>)</cite>는 블랙박스 언어 모델에서 각 예의 유의성을 추정하기 위해 분산 감소 정책 기울기 전략을 사용한 강화 학습을 사용하여 더 나은 데모 선택을 유도한다. 마찬가지로 <cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a class="ltx_ref" href="#bib.bib92" title="">2023b</a>)</cite>는 정책 기울기를 활용하여 표식 추론에서 데모를 선택하는 방법을 학습하는 PromptPG를 제안한다. <cite class="ltx_cite ltx_citemacro_citet">Ye and Durrett (<a class="ltx_ref" href="#bib.bib176" title="">2023</a>)</cite>는 처음에 두 개의 프록시 메트릭을 사용하여 각 예를 평가한 다음 예를 검색하여 실버 라벨 개발 세트에서 최상의 성능을 내는 데모를 찾습니다. 한편 <cite class="ltx_cite ltx_citemacro_citet">Pitis et al. (<a class="ltx_ref" href="#bib.bib117" title="">2023</a>)</cite>는 성능을 향상시키기 위한 프롬프트 앙상블 방식인 Boosted Prompting을 제안하며, 이는 현재 시연이 다루기 어려운 문제에 직면할 때 반복적으로 예를 확장한다. <cite class="ltx_cite ltx_citemacro_citet">Zou et al. (<a class="ltx_ref" href="#bib.bib208" title="">2023</a>)</cite>는 질문 카테고리에 따라 데모를 자동으로 선택하는 Meta-CoT를 도입하여 태스크별 프롬프트 설계의 필요성을 없앴다.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x1.png" id="S4.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
The evolution of reasoning, from direct I/O to chain structure, then to tree and graph structure.</figcaption>
</figure>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">반자동 XoT 방법은 추론 능력과 안정성을 향상시키기 위해 인간 정렬 신호와 시연 선택 전략을 도입하면서 수동 라벨링의 작업 부하를 줄인다. 또한 비용 효율적인 도메인 일반화를 가능하게 합니다. 그러나 실증 선정 문제는 완전히 해결되지 않았고 더 많은 노력과 연구가 필요하다.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>XoT Structural Variants</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">가장 원시적인 연쇄적 사고는 자연어로 중간추론 단계를 기술하는 연쇄 구조이다. 이 섹션에서는 체인 구조 변형, 트리 구조 변형 및 그래프 구조 변형을 포함하여 원래 체인 구조를 수정하는 구조 변형을 소개한다.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Chain Structure</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">PAL<cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>)</cite>와 PoT<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib12" title="">2022a</a>)</cite>는 프로그래밍 언어를 도입하여 추론 과정을 기술함으로써 추론 문제를 실행 가능한 프로그램의 구현으로 변환하여 최종 답변을 얻는다. 프로그램 실행은 결정적이고, 산술 연산을 정확하게 수행하기 때문에, 이 접근법은 수학적 추론에서 우수한 성능을 보인다. 게다가, 기호 시퀀스는 또 다른 유형의 사고 표현이다. Chain-of-Symbol <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="#bib.bib46" title="">2023a</a>)</cite>는 계획 중에 응축된 심볼릭 체인 표현으로 복잡한 환경을 나타내며, 이는 시뮬레이션 환경의 복잡성을 감소시킨다. 사슬 구조 변형은 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4.1.3 Semi-automatic XoT ‣ 4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">2</span></a>(c,d) 생각의 알고리즘 <cite class="ltx_cite ltx_citemacro_citep">(Sel et al., <a class="ltx_ref" href="#bib.bib129" title="">2023</a>)</cite>에 알고리즘 능력을 모델에 주입하여 알고리즘을 기반으로 예를 추가함으로써 모델의 추론을 보다 논리적으로 만든다. 트리 검색의 거대한 검색 공간이 없는 <cite class="ltx_cite ltx_citemacro_citep">(Long, <a class="ltx_ref" href="#bib.bib89" title="">2023</a>; Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>)</cite>는 계산 자원을 절약하고 우수한 성능을 달성한다.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tree Structure</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">원래의 사슬 구조는 본질적으로 탐사의 범위를 제한한다. 트리 구조와 트리 탐색 알고리즘의 통합을 통해 모델은 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4.1.3 Semi-automatic XoT ‣ 4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">2</span></a>(e)와 같이 추론 과정 <cite class="ltx_cite ltx_citemacro_citep">(Long, <a class="ltx_ref" href="#bib.bib89" title="">2023</a>; Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>)</cite>에서 효율적으로 탐색하고 역추적할 수 있는 능력을 얻게 된다. 중간 생각에 대한 자체 평가와 결합하여 모델은 글로벌 최적 솔루션을 달성할 수 있습니다. ToT의 추론 과정은 불확실성을 수반하며, 이는 잠재적으로 계단식 오류로 이어질 수 있다. TouT<cite class="ltx_cite ltx_citemacro_citep">(Mo and Xin, <a class="ltx_ref" href="#bib.bib105" title="">2023</a>)</cite>는 불확실성을 고려하여 몬테카를로 드롭아웃을 추론에 도입한다. <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="#bib.bib181" title="">2023b</a>)</cite>는 LLM의 복잡한 추론 능력을 향상시키기 위해 그들의 솔루션을 활용하여 유사한 문제를 탐구한다. 이러한 유사한 문제들은 나무와 같은 구조를 보이며, 궁극적으로 주요 문제를 해결하기 위해 수렴한다. 그러나 현재의 사고 트리는 작업 선택에 상당한 한계가 있으며 각 작업에 대한 구체적인 신속한 설계가 요구되어 광범위한 적용을 방해한다. SoT<cite class="ltx_cite ltx_citemacro_citep">(Ning et al., <a class="ltx_ref" href="#bib.bib107" title="">2023</a>)</cite>는 트리 구조의 또 다른 변형으로, 문제를 병렬로 처리하고 동시에 해결할 수 있는 하위 문제로 분해하여 추론의 속도를 높인다. 그러나 그 효용성은 병렬 분해 가능한 문제로 제한되며 복잡한 추론 작업에는 적합하지 않다.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Graph Structure</h5>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">트리에 비해 그래프는 루프와 고리를 도입하여 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4.1.3 Semi-automatic XoT ‣ 4.1 Construction Approach ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">2</span></a>(f)와 같이 더 복잡한 위상 관계를 가져오고 더 복잡한 추론을 모델링할 수 있다. GoT<cite class="ltx_cite ltx_citemacro_citep">(Besta et al., <a class="ltx_ref" href="#bib.bib5" title="">2023</a>; Lei et al., <a class="ltx_ref" href="#bib.bib71" title="">2023a</a>)</cite>는 중간 사고를 그래프 내의 노드로 간주하고 탐색과 역추적 연산을 결합하며, 사고 트리와 비교하여 집계 및 정제 연산을 추가로 도입한다. 추가 연산, 집계 및 개선은 복잡한 작업에서 더 나은 추론을 이끌어낸다. 그럼에도 불구하고, 그것은 사고의 나무와 동일한 딜레마, 즉 과제 제한과 열악한 일반화 가능성에 직면해 있다. 게다가 추론 비용이 증가했습니다. 사고 그래프를 명시적으로 구성하는 GoT와 달리 ResPrompt<cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="#bib.bib55" title="">2023a</a>)</cite>는 프롬프트 텍스트에서 사고 간의 잔차 연결을 도입하여 서로 다른 단계의 추론이 상호 작용할 수 있도록 한다.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p2.1">모델이 선형 사슬에서 계층적 트리 및 복잡한 그래프로 전환됨에 따라 생각의 상호 작용은 점진적으로 더 복잡해지며, 이에 따라 복잡한 문제를 해결할 수 있는 능력이 점차 향상된다. 그러나 토폴로지의 복잡도가 증가함에 따라 관련 방법은 태스크 선택에 더 많은 제약을 가하여 일반화 가능성을 크게 감소시키고 응용을 어렵게 만든다. 복잡한 토폴로지 구조 기반 방법을 일반 영역으로 확장하는 것은 향후 연구의 주요 과제이다.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>XoT Enhancement Methods</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">본 절에서는 XoT 향상 방법을 제시한다. 전체적으로 검증 및 정제 추가(§<a class="ltx_ref" href="#S4.SS3.SSS1" title="4.3.1 Verify and Refine ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>), 질문 분해(§<a class="ltx_ref" href="#S4.SS3.SSS2" title="4.3.2 Question Decomposition ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>), 외부 지식 활용(§<a class="ltx_ref" href="#S4.SS3.SSS3" title="4.3.3 External Knowledge ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3.3</span></a>), 투표 및 순위 부여(§<a class="ltx_ref" href="#S4.SS3.SSS4" title="4.3.4 Vote and Rank ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3.4</span></a>), 효율성 향상(§<a class="ltx_ref" href="#S4.SS3.SSS5" title="4.3.5 Efficiency ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4.3.5</span></a>)의 5가지 범주에 대한 개요를 제공할 것이다.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x2.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Verification and refinement reduce cascading errors in reasoning.</figcaption>
</figure>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Verify and Refine</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">연쇄 사고 추론은 종종 환각이 되어 잘못된 추론 단계를 생성하는 경향이 있다. 중간 추론 단계의 오류는 차례로 일련의 오류를 유발할 수 있다. 피드백을 얻기 위해 검증을 통합하고 후속적으로 이 피드백을 기반으로 추론 과정을 정제하는 것은 인간의 성찰 과정과 유사한 이 현상을 완화하는 매우 효과적인 전략이 될 수 있다. <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">3</span></a>는 검증 및 개선의 개요를 묘사한다.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">VerifyCoT<cite class="ltx_cite ltx_citemacro_citep">(Ling et al., <a class="ltx_ref" href="#bib.bib85" title="">2023</a>)</cite>는 모델이 정확한 추론 단계를 생성할 수 있도록 하는 연역적 추론 형식인 Natural Program을 고안하며, 각 후속 단계는 이전 단계를 엄격하게 기반으로 한다. <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p2.1.1">DiVeRSe</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib82" title="">2022c</a>)</cite>는 투표 메커니즘을 활용하여 오답을 제거한 후, 각 추론 단계를 독립적으로 세밀하게 검증한다. SCREWS<cite class="ltx_cite ltx_citemacro_citep">(Shridhar et al., <a class="ltx_ref" href="#bib.bib134" title="">2023</a>)</cite>는 수정 후 결과가 반드시 원점보다 우수하지 않을 수 있다고 생각하므로 원점과 수정 사이에서 더 나은 결과를 선택하기 위해 선택 모듈을 도입한다. 지식 집약적인 작업을 용이하게 하기 위해 Verify-and-Edit <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="#bib.bib198" title="">2023a</a>)</cite>는 불확실한 예제를 재추론하기 위해 외부 지식을 통합하여 추론의 사실적 실수를 줄입니다. 일부 연구 노력은 모델의 내부 지식을 밝히려고 시도한다. 일부 연구 노력은 모델의 내부 지식을 밝히려고 시도한다. 사실적 오류를 해결하기 위해 일부 연구에서는 LLM에 대한 내재적 지식을 밝히려고 시도한다. 그들은 <cite class="ltx_cite ltx_citemacro_citep">(Dhuliawala et al., <a class="ltx_ref" href="#bib.bib22" title="">2023</a>; Zheng et al., <a class="ltx_ref" href="#bib.bib202" title="">2023</a>)</cite> 질문에 답하기 전에 모델로부터 지식을 습득한다. <cite class="ltx_cite ltx_citemacro_citet">Ji et al. (<a class="ltx_ref" href="#bib.bib54" title="">2023</a>)</cite>는 내재적 지식의 정확성을 더욱 검증하고, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="#bib.bib87" title="">2023b</a>)</cite>는 강화학습을 통해 내재적 지식 습득의 정확성을 높인다.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">비일관성은 추론에서 또 다른 주요 과제이며, <cite class="ltx_cite ltx_citemacro_citet">Dua et al. (<a class="ltx_ref" href="#bib.bib26" title="">2022</a>)</cite>는 모델이 일관된 답변을 제공할 때까지 이전 추론 결과를 프롬프트로 반복적으로 사용한다. <cite class="ltx_cite ltx_citemacro_citet">Paul et al. (<a class="ltx_ref" href="#bib.bib115" title="">2023</a>)</cite>는 비평가 모델을 훈련시켜 추론 과정에 대한 구조화된 피드백을 제공한다. Self-Refine <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a class="ltx_ref" href="#bib.bib95" title="">2023</a>)</cite>는 반복적인 self-feedback과 refinement을 수행하여 추론의 오류를 완화한다. Self-Refine과 비교했을 때, Reflexion<cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a class="ltx_ref" href="#bib.bib133" title="">2023</a>)</cite>는 성찰을 위한 강화학습을 도입하여 의사결정 능력을 추가적으로 가져온다. 한편, 일부 연구에서는 검증을 위해 backward reasoning <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="#bib.bib180" title="">2023a</a>)</cite>를 소개하고 있다. RCoT<cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="#bib.bib168" title="">2023</a>)</cite>는 추론 체인에 따라 질문을 재구성하며, 원래 질문과의 불일치는 추론 과정에서 오류를 노출한다. FOBAR <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="#bib.bib56" title="">2023b</a>)</cite>와 Self Verification <cite class="ltx_cite ltx_citemacro_citep">(Weng et al., <a class="ltx_ref" href="#bib.bib162" title="">2022</a>)</cite>는 답변에서 질문의 조건을 추론하여 검증을 수행한다. FOBAR은 질문의 변수를 추론하고, Self Verification은 질문의 조건을 추론한다. 그러나 <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="#bib.bib49" title="">2023a</a>)</cite>는 LLMs가 외부 피드백 없이 스스로 수정하기 위해 고군분투하는 것을 발견하며, 성능 저하로 이어질 수도 있다.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS1.p4.1">LLM 추론은 중간 추론 단계의 피드백 신호가 추론을 개선하는 데 중요한 역할을 하는 감독되지 않은 과정이다. 피드백 신호로부터의 안내는 추론에서 환각 현상을 효과적으로 감소시킬 수 있다. 적절한 피드백을 얻고 그 피드백을 기반으로 정확한 보정을 할 수 있는 상당한 연구 공간이 여전히 있다.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Question Decomposition</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">X-of-thought 추론의 본질은 단계적 문제 해결에 있다. 그러나, 독창적인 연쇄적 사고 추론 접근법은 단계적 추론 과정을 명시적으로 제거하지 않고 여전히 1단계 생성을 사용한다. 본 절에서는 질문을 단계별로 명시적으로 해결하는 질문 분해 접근법에 대해 논의한다. 개요는 그림<a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.3.2 Question Decomposition ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">4</span></a>에 나와 있다.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x3.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 4:</span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Question decomposition solves complex questions progressively by solving simple sub-questions.</figcaption>
</figure>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib146" title="">2022a</a>)</cite>는 반복적으로 모델로부터 지식을 획득하여 멀티홉 QA에서 진전을 이루고 있다. <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a class="ltx_ref" href="#bib.bib205" title="">2023b</a>)</cite>는 Least-to-Most Prompting을 제안하는데, 이는 처음에 질문을 하향식으로 하위 질문으로 분해한 후, 한 번에 한 하위 질문을 해결하고 그 솔루션을 활용하여 후속 하위 질문을 용이하게 한다. Successive Prompting <cite class="ltx_cite ltx_citemacro_citep">(Dua et al., <a class="ltx_ref" href="#bib.bib26" title="">2022</a>)</cite>는 Least-to-Most Prompting과 유사한 접근 방식을 취하며, 차이점은 2단계 분해가 아닌 인터리빙된 하위 질문과 답변을 갖는 분해를 취한다는 것이다. 위의 방법은 다양한 하위 문제에 대한 맞춤형 솔루션을 공식화하지 않는다. Decomposed Prompting <cite class="ltx_cite ltx_citemacro_citep">(Khot et al., <a class="ltx_ref" href="#bib.bib63" title="">2023</a>)</cite>는 모듈형 공유 라이브러리를 설계하며, 이는 각각 하위 문제 클래스에 전용되며, 하위 문제 클래스에 따라 더 효과적인 솔루션을 조정할 수 있다. 일반적인 작업과 별도로, 일부 작업은 표식 추론에 대한 질문 분해에 초점을 맞춘다. BINDER<cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite>는 추론을 신경기호 방식으로 프로그램에 매핑하고 파이썬이나 SQL과 같은 프로그램 실행기를 통해 최종 답변을 얻는다. <cite class="ltx_cite ltx_citemacro_citet">Ye et al. (<a class="ltx_ref" href="#bib.bib177" title="">2023</a>)</cite>는 큰 테이블을 작은 테이블로, 복잡한 질문을 간단한 테이블로 나누는 DATER를 소개한다. 전자는 무관한 정보를 감소시키는 반면, 후자는 추론의 복잡성을 감소시킨다.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1">복잡한 질문에 대한 직접적인 답변을 제공하는 것은 어려울 수 있다. 질문을 간단한 하위 문항으로 분해하여 단계적으로 해결함으로써 난이도를 줄인다. 더욱이, 각각의 하위 질문은 특정 추론 단계로 역추적될 수 있어 추론 프로세스를 보다 투명하고 설명할 수 있게 한다. 현재 작업은 대부분 하향식 분해 전략을 사용하는 반면, 역방향 추론에 기반한 상향식 분해 전략은 향후 작업에서 탐구해야 한다.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>External Knowledge</h4>

<figure id="S4.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x4.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 5:</span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Introducing external knowledge reduces factual errors in reasoning.</figcaption>
</figure>
<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">모델 내에서 매개 변수화된 지식은 제한적이고 구식입니다. 따라서 지식 집약적인 작업에 직면할 때 사실상의 실수가 종종 발생한다. 외부 지식의 도입은 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.3.3 External Knowledge ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">5</span></a>와 같이 이러한 현상을 완화시킬 수 있다.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a class="ltx_ref" href="#bib.bib90" title="">2023a</a>)</cite>는 기계 번역을 향상시키기 위해 프롬프트에 다국어 사전을 도입한다. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="#bib.bib81" title="">2023d</a>)</cite>는 지식 유도 추론을 수행하기 위해 쿼리 생성기를 통해 지식 베이스로부터 구조화된 지식을 얻는 CoK-Li(chain-of-knowledge)를 제안한다. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib149" title="">2023b</a>)</cite> (CoK-Wang)도 KB로부터 구조화된 지식을 검색한다. 또한, 사실성과 충실성 측면에서 추론 사슬을 추정하고 모델이 신뢰할 수 없는 추론에 대해 다시 생각하게 하여 CoK-Li의 지식 검색 오류를 완화한다. KD-CoT<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib150" title="">2023c</a>)</cite>는 멀티턴 QA 방식을 통해 사실 추론 문제를 해결한다. 그들은 추론 과정을 보정하기 위해 QA의 각 라운드에서 관련 외부 지식을 검색하기 위한 피드백 강화 리트리버를 설계한다. 다른 연구들은 모델 자신의 기억을 외적 지식으로 사용하고 있다. 예를 들어 Memory-of-Thought <cite class="ltx_cite ltx_citemacro_citep">(Li and Qiu, <a class="ltx_ref" href="#bib.bib80" title="">2023</a>)</cite>는 먼저 높은 자신감의 생각을 외부 기억으로 저장하기 위해 사전 사고를 수행하고 추론하는 동안 LLM이 추론을 돕기 위해 관련 기억을 회상하도록 한다.</p>
</div>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS3.p3.1">모델에서 매개변수화된 지식은 사전 훈련이 끝날 때 고정되어 있어 지식 용량 및 지식 업데이트 측면에서 단점이 있다. 외부의 지식을 도입하는 것은 이를 어느 정도 완화시킬 수 있지만 여전히 불완전한 해결책으로 남아 있다. 이 문제를 근본적으로 해결하기 위해 지속적인 학습 <cite class="ltx_cite ltx_citemacro_citep">(Lange et al., <a class="ltx_ref" href="#bib.bib68" title="">2022</a>; Wang et al., <a class="ltx_ref" href="#bib.bib154" title="">2023g</a>)</cite>는 향후 연구 노력을 위한 유망한 길로 서 있다.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Vote and Rank</h4>

<figure id="S4.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x5.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 6:</span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Voting and ranking reduce inconsistency by selecting final answers from multiple samplings.</figcaption>
</figure>
<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">생성 과정에서 내재된 확률성으로 인해 LLM 추론은 무작위성과 불확실성의 요소를 나타낸다. 이 문제는 그림 <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.3.4 Vote and Rank ‣ 4.3 XoT Enhancement Methods ‣ 4 Methods ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">6</span></a>와 같이 다중 샘플링 전략을 통해 효과적으로 완화될 수 있다.</p>
</div>
<div id="S4.SS3.SSS4.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS4.p2.1"><cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="#bib.bib17" title="">2021</a>)</cite>와 같은 몇 가지 방법은 순위를 통해 높은 신뢰도의 추론 체인을 선택하도록 검증자를 훈련시키는 순위를 채택한다. 한편, 다른 방법들은 투표 메커니즘을 통해 추론 체인들을 선택한다. Self-consistency <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib158" title="">2023j</a>)</cite>는 최종 답변을 기반으로 샘플링된 추론 체인 중 다수결 투표를 통해 가장 일관된 답변을 선택한다. 또한 <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="#bib.bib30" title="">2023a</a>)</cite>는 복잡한 추론 사슬에 의해 생성된 답을 선택하는 쪽으로 기울어지는 복잡도 기반 투표 전략을 활용하는 Complex CoT를 제안한다. 그러나 답변 기반 투표 메커니즘은 추론 사슬의 정확성을 고려하지 않는다. <cite class="ltx_cite ltx_citemacro_citet">Miao et al. (<a class="ltx_ref" href="#bib.bib100" title="">2023</a>)</cite>는 투표 시 추론 단계를 고려하여 일관된 답변과 신뢰할 수 있는 추론 과정을 동시에 얻을 수 있다. 또한, 체인 간의 중간 단계 사이의 관계를 고려하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Yoran et al. (<a class="ltx_ref" href="#bib.bib179" title="">2023</a>)</cite>는 추론 체인 간의 정보를 혼합하고 가장 관련성이 높은 사실을 선택하여 다중 추론 체인에 대한 메타추론을 수행한다. GRACE<cite class="ltx_cite ltx_citemacro_citep">(Khalifa et al., <a class="ltx_ref" href="#bib.bib62" title="">2023</a>)</cite>는 대조적 학습을 통해 판별기를 학습시키고, 이 판별기를 이용하여 각 중간 추론 단계의 순위를 매긴다. 기존의 방법들은 확률 분포를 기반으로 표본을 추출하는 반면, Diversity-of-Thought <cite class="ltx_cite ltx_citemacro_citep">(Naik et al., <a class="ltx_ref" href="#bib.bib106" title="">2023</a>)</cite>는 서로 다른 명령어로 프롬프트함으로써 다중 추론 경로를 얻는다.</p>
</div>
<div id="S4.SS3.SSS4.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS4.p3.1">앙상블 학습에서 영감을 끌어내어 다중 샘플링으로 투표하고 순위를 매기는 관행은 불확실성을 줄이는 역할을 한다. 또한 단일 표본 접근법에 비해 상당한 성능 개선을 보여주었다. 투표와 함께 다중 샘플링은 현재 X-of-thought 연구에서 일반적인 기술이 되었다. 추론 사슬을 투표에 통합하는 것은 미래를 위한 중요한 연구 영역으로 남아 있다.</p>
</div>
</section>
<section id="S4.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Efficiency</h4>

<div id="S4.SS3.SSS5.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">LLM 추론 및 수동으로 주석이 달린 추론 체인은 비싼 오버헤드를 부과한다. <cite class="ltx_cite ltx_citemacro_citet">Aggarwal et al. (<a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite>는 샘플 수를 동적으로 조정하여 자기 일관성을 향상시키며, 이는 한계 성능 저하와 함께 추론 비용을 크게 줄일 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Ning et al. (<a class="ltx_ref" href="#bib.bib107" title="">2023</a>)</cite>는 문제를 병렬적으로 분해하여 동시에 처리함으로써 추론 시간 오버헤드를 줄였다. 그러나 그것은 복잡한 질문들을 다룰 수 없다. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib190" title="">2023b</a>)</cite>는 일부 중간 계층을 선택적으로 건너뛰어 추론을 가속화한 다음 다른 순방향 패스에서 드래프트를 검증한다. <cite class="ltx_cite ltx_citemacro_citet">Diao et al. (<a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>는 능동적 학습에서 아이디어를 빌려 불확실성이 높은 예제에 주석을 달아서 사람 주석 비용을 줄인다.</p>
</div>
<div id="S4.SS3.SSS5.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS5.p2.1">대규모 언어 모델은 엄청난 능력을 보여주었지만 상당한 오버헤드를 수반하기도 한다. 성과와 오버헤드 사이의 균형을 맞추려면 향후 연구 노력에 상당한 주의가 필요할 수 있다.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Frontier Application</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Tool Use</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.1">LLM이 보여주는 광범위한 지식에도 불구하고 몇 가지 문제가 수반된다. 여기에는 최신 뉴스에 액세스할 수 없는 능력, 영역 외 지식과 관련된 쿼리에 응답할 때 환각에 대한 성향, 수학적 계산이나 상징적 추론과 같은 정교한 추론 능력의 부재가 포함된다. LLMs에게 외부 도구를 사용할 수 있는 능력을 부여함으로써 모델의 추론 능력을 증강하고 외부 지식을 동화하여 정보 검색 및 환경 상호 작용에 참여할 수 있게 된다.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p2.1">MRKL<cite class="ltx_cite ltx_citemacro_citep">(Karpas et al., <a class="ltx_ref" href="#bib.bib60" title="">2022</a>)</cite>는 확장 가능한 모듈(전문가라고 함)과 라우터로 구성된 새로운 프레임워크를 소개한다. 이러한 전문가들은 신경망이나 기호의 형태를 취할 수 있다. 그러나 본 연구는 다른 모듈 콘텐츠를 구현하지 않고 수학적 계산을 위한 LLM을 개념화하고 훈련하는 데 중점을 둔다. TALM<cite class="ltx_cite ltx_citemacro_citep">(Parisi et al., <a class="ltx_ref" href="#bib.bib111" title="">2022a</a>)</cite>와 Toolformer<cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="#bib.bib128" title="">2023</a>)</cite>는 텍스트 중심 방법론과 보조 도구를 통합하여 언어 모델의 기능을 향상시킨다. 그들은 제한된 툴팁 세트로 시작하여 성능 향상을 시작하기 위해 자체 감독 메커니즘을 사용한다. 유사한 맥락에서 HuggingGPT<cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a class="ltx_ref" href="#bib.bib131" title="">2023</a>)</cite>는 시각 및 음성 모델을 활용하여 다양한 모달리티의 정보를 처리함으로써 LLM에 다중 모달 이해 및 생성 능력을 부여한다. 또 다른 질문은 적절한 도구를 선택하는 방법이다. LATM<cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="#bib.bib10" title="">2023</a>)</cite>는 LLMs의 도구 제작 능력이 다양한 작업에 걸쳐 일반화된 API를 만들 수 있도록 하고 GEAR<cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="#bib.bib93" title="">2023c</a>)</cite>는 도구 접지와 실행을 위임하기 위해 더 작은 모델을 사용하여 도구 사용의 효율성을 고려한다.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p3.1">그러나 사용자 요청을 API 형식으로 변환하는 것은 간단하지 않은 경우가 많습니다. 위에서 언급한 기존 접근법은 도구의 여러 호출을 촉진하고 쿼리 오류를 수정하는 데 한계가 있다. 이 문제를 해결하기 위해 ReAct<cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib173" title="">2023c</a>)</cite>는 추론과 행동의 장점을 통합하여 서로를 향상하고 보완하여 문제 해결 능력을 상호 보완한다. ART <cite class="ltx_cite ltx_citemacro_citep">(Paranjape et al., <a class="ltx_ref" href="#bib.bib110" title="">2023</a>)</cite>는 작업 라이브러리를 사용하여 관련 도구 사용 및 추론 체인을 선택합니다. MM-REACT <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib169" title="">2023</a>)</cite>는 비전 전문가를 더 활용하여 멀티모달 추론과 동작을 가능하게 한다.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p4.1">앞서 언급한 연구는 다양한 영역에서 LLM의 기능을 향상시키기 위한 도구(또는 API)를 설계하는 데 중점을 둔다. XoT와 도구를 결합하면 LLM이 직면한 문제를 효과적으로 해결할 수 있습니다. X-of-thought 추론은 모델들이 예외들을 관리하면서 액션 플랜들을 효과적으로 이끌어내고, 추적하고, 업데이트할 수 있게 한다. 동시에 액션 연산은 모델의 지식 베이스 및 환경과 같은 외부 소스와의 상호 작용을 촉진하여 추가 정보를 수집할 수 있게 한다. 도구의 숙련도를 평가하기 위해 API-Bank <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib78" title="">2023c</a>)</cite>와 MetaTool <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="#bib.bib52" title="">2023c</a>)</cite>는 포괄적인 벤치마크를 도입하여 도구 증강 LLM의 성능과 효율성을 평가할 수 있는 강력한 기반을 제공한다.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Planning</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p1.1">LLM은 복잡한 문제에 대해 직접 정확한 응답을 제공하는 데 어려움이 있어 순차적인 단계와 하위 작업으로 분해할 필요가 있다. CoT는 계획에 대한 간단한 접근법을 제공하지만 매우 복잡한 문제를 해결하는 데 부족하고 역추적을 통해 오류를 평가하고 수정하는 능력이 부족하다.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p2.1">수많은 연구가 계획 능력을 더욱 향상시키기 위해 사고 사슬의 틀을 다양한 형식으로 확장했다. Tree-of-Thought <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>)</cite>는 LLMs가 트리에서 여러 추론 경로를 고려하고 다음 행동 과정을 결정하기 위해 자체 평가할 수 있도록 한다. 글로벌 결정이 필요한 경우 ToT는 심층 우선 탐색 또는 너비 우선 탐색과 같은 기술을 통해 전진 또는 후진 탐색을 허용한다. 또한 계획(RAP)을 통한 추론(<cite class="ltx_cite ltx_citemacro_citep">(Hao et al., <a class="ltx_ref" href="#bib.bib39" title="">2023</a>)</cite>)은 문제를 트리로 나누고 몬토카를로 트리 탐색 알고리즘을 사용하여 LLM을 월드 모델과 추론 에이전트로 사용한다. 또 다른 방법인 Graph of Thought (GoT) <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib174" title="">2023d</a>)</cite>는 개인의 생각을 표현하기 위해 그래프 노드를 사용하고 조직화를 위해 외부 그래프 신경망을 사용한다. LLM+P<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib86" title="">2023a</a>)</cite>와 LLM+DP<cite class="ltx_cite ltx_citemacro_citep">(Dagan et al., <a class="ltx_ref" href="#bib.bib20" title="">2023</a>)</cite>는 LLM에 의한 PDDL( Planning Domain Definition Language)  <cite class="ltx_cite ltx_citemacro_citep">(Gerevini, <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>의 생성을 용이하게 한다. PDDL은 LLM 처리를 위해 결과를 자연어로 변환하기 전에 복잡한 문제를 분해하고 계획을 위한 특수 모델을 활용하는 데 도움이 된다. 그러나 이러한 방법은 트리/그래프/PDDL 노드를 사용하여 생각을 표현하는데, 이는 표현 형태에 제한이 있고 특정 계획 문제만 처리할 수 있다는 점에 유의해야 한다.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p3.1">또 다른 기법은 모델의 오류를 수정하고 역사적 경험을 요약하는 능력을 향상시키는 것이다. Self-Refine <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a class="ltx_ref" href="#bib.bib95" title="">2023</a>)</cite>는 모델에 의해 생성된 출력이 평가되고 동일한 모델을 사용하여 피드백이 제공되는 고유한 접근법을 사용한다. Reflexion <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a class="ltx_ref" href="#bib.bib133" title="">2023</a>)</cite>를 사용하면 모델이 이전 작업에서 발생한 오류를 반성하고 수정할 수 있으며 텍스트 형식의 강화 학습과 유사하며 메모리를 장단기 구성 요소로 분할하는 것을 포함한다. 그러나 계획 외 오류가 발생할 경우 반사는 계획을 업데이트할 수 없습니다. AdaPlanner <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="#bib.bib137" title="">2023</a>)</cite>는 환경의 피드백을 기반으로 작업 계획을 반복적으로 정제하는 적응형 폐루프 계획 정제를 도입한다. ISR-LLM <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="#bib.bib206" title="">2023c</a>)</cite>는 Self-Refine과 PDDL을 결합하여 긴 수평 순차 작업에서 더 나은 성공률을 달성한다. 한편, LATS<cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="#bib.bib203" title="">2023a</a>)</cite>는 LM 기반의 몬테카를로 트리 탐색을 활용하여 보다 유연한 계획 절차를 수행한다.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p4.1">계획은 추론 능력을 풍부하게 하기 위해 도구 <cite class="ltx_cite ltx_citemacro_citep">(Ruan et al., <a class="ltx_ref" href="#bib.bib124" title="">2023</a>)</cite> 또는 에이전트 <cite class="ltx_cite ltx_citemacro_citep">(Crispino et al., <a class="ltx_ref" href="#bib.bib19" title="">2023b</a>)</cite>와 유연하게 결합될 수 있다. ToRA<cite class="ltx_cite ltx_citemacro_citep">(Gou et al., <a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite>는 외부 도구를 사용하여 수학적 전문 에이전트를 설계하며, AutoUI<cite class="ltx_cite ltx_citemacro_citep">(Zhang and Zhang, <a class="ltx_ref" href="#bib.bib195" title="">2023</a>)</cite>는 시각적 입력을 텍스트로 변환하는 대신 멀티모달 환경과 직접 상호작용하여 추론 효율을 높이고 오류 전파를 줄인다.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p5.1">계획 증강 접근법은 검색 기반, 그래프 기반 및 정의 언어 기반 방법을 도입하여 기존의 순차 계획을 발전시켰다. 반면에 일부 방법은 액션, 계획, 반영 또는 도구를 통합하여 LLM의 장기 계획 및 오류 복원력 기능을 향상시키는 것을 목표로 한다.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>CoT Distillation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p1.1">LLM은 복잡한 문제를 해결하기 위해 추론 단계를 증류함으로써 자체 개선될 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="#bib.bib48" title="">2022</a>)</cite>는 자체 일관성을 가진 LLM을 사용하여 레이블이 지정되지 않은 데이터에서 추론 체인을 생성한다. 이러한 체인은 이후에 모델을 미세 조정하는 데 활용되어 일반화된 추론 능력을 향상시킨다. <cite class="ltx_cite ltx_citemacro_citet">Zelikman et al. (<a class="ltx_ref" href="#bib.bib186" title="">2022</a>)</cite>는 셀프 루프 부트스트랩 전략을 사용하여 LM의 추론 능력을 향상시키기 위한 소수 샷 학습 접근법인 STaR을 제안한다. SECToR<cite class="ltx_cite ltx_citemacro_citep">(Zhang and Parkes, <a class="ltx_ref" href="#bib.bib189" title="">2023</a>)</cite>는 연쇄적 사고를 사용하여 산술적 답변을 얻은 다음 모델을 미세 조정하여 CoT 없이 직접 답변을 생성한다.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p2.1">CoT는 소형 모델에서 제한된 발전과 함께 LLM에서 주로 관찰되는 새로운 능력이다. 그러나 소형 모델의 CoT 능력을 향상시키는 것은 증류와 같은 기술을 통해 생각할 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Magister et al. (<a class="ltx_ref" href="#bib.bib98" title="">2023</a>)</cite>는 더 큰 교사 모델에 의해 생성된 추론 체인으로 T5를 미세 조정하고 답변 해결을 위해 외부 계산기를 사용하는 것이 다양한 데이터 세트에 걸쳐 작업 성능을 실질적으로 향상시킬 수 있음을 보여준다. <cite class="ltx_cite ltx_citemacro_citet">Ho et al. (<a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>는 다양성을 풍부하게 하기 위해 여러 추론 경로를 생성하고 필터링한다.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p3.1">자체 일관성 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib158" title="">2023j</a>)</cite>를 활용하여 주석이 없는(또는 주석이 거의 없는) 데이터를 사용하여 인적 비용을 줄이기 위한 수많은 노력이 수행될 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Hsieh et al. (<a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>는 훨씬 적은 수의 라벨링된/라벨링되지 않은 데이터로부터 답변을 생성하기 위해 프롬프트를 채용하고, 이어서 주어진 답변에 대한 추론을 제공하기 위해 언어 모델을 프롬프트하는 근거의 생성을 채용한다. SCoTD<cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib77" title="">2023b</a>)</cite>는 교사의 인스턴스당 여러 추론 체인을 샘플링하는 것이 학생들의 능력을 향상시키는 데 가장 중요하다는 것을 발견했다. SCOTT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib155" title="">2023h</a>)</cite>는 교사 모델에 대한 근거 생성 동안 대비 디코딩 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib79" title="">2022b</a>; O’Brien and Lewis, <a class="ltx_ref" href="#bib.bib108" title="">2023</a>)</cite>를 활용한다. 또한, 바로가기 문제를 해결하기 위해 학생 모델을 훈련하는 동안 반사실적 추론 목표를 사용한다. DialCoT<cite class="ltx_cite ltx_citemacro_citep">(Han et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>는 추론 단계를 다중 라운드 대화 상자로 분해하고 PPO 알고리즘을 사용하여 올바른 경로를 선택한다. <cite class="ltx_cite ltx_citemacro_citet">Jie et al. (<a class="ltx_ref" href="#bib.bib57" title="">2023</a>); Wang et al. (<a class="ltx_ref" href="#bib.bib157" title="">2023i</a>)</cite>는 수학 문제를 위한 특별한 토큰을 추가한다. 이러한 높은 수준의 정보는 추론 단계의 일관성을 향상시킨다.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p4.1">위의 연구들은 우수한 추론 능력을 가진 LLM을 통해 추론 체인이 생성되는 공유 패러다임을 채택한다. 그런 다음 이러한 추론 사슬을 더 작은 모델로 증류한다. 증류 공정의 유효성은 예를 들어, 다수의 샘플링 경로, 일관성 또는 대조적 디코딩의 활용을 통해 더 큰 모델로부터 샘플링 전략을 증대시킴으로써 개선되며, 이는 생성된 추론 체인에서 향상된 다양성 및 정확성으로 이어져 궁극적으로 증류 공정을 더 작은 모델에 유리하게 한다. 언어 모델이 다차원 기능과 관련된 복잡한 절충안과 복잡한 균형을 가지고 있다는 점은 주목할 만하다. <cite class="ltx_cite ltx_citemacro_citet">Fu et al. (<a class="ltx_ref" href="#bib.bib31" title="">2023b</a>)</cite>는 증류를 통해 과제 특이적 사고 연쇄 능력을 높이는 것도 일반화된 문제를 해결하는 모델의 성능에 부정적인 영향을 미칠 수 있음을 강조한다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">연쇄적 사고 추론은 수많은 과제에서 놀라운 성과를 보여주었지만 일부 과제는 여전히 추가 탐구가 필요하다. 이 절에서는 향후 연구를 위한 세 가지 유망한 방법, 즉 멀티모달 X-of-thought 추론(§<a class="ltx_ref" href="#S6.SS1" title="6.1 Multi-modal CoT ‣ 6 Future Directions ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">6.1</span></a>), 충실한 X-of-thought 추론(§<a class="ltx_ref" href="#S6.SS2" title="6.2 Faithfulness ‣ 6 Future Directions ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">6.2</span></a>), X-of-thought 추론 이론(§<a class="ltx_ref" href="#S6.SS3" title="6.3 CoT Theory ‣ 6 Future Directions ‣ A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"><span class="ltx_text ltx_ref_tag">6.3</span></a>)에 대한 간략한 개요를 제공한다.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Multi-modal CoT</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.p1.1">텍스트 유니모달에서 비전 텍스트 멀티모달로의 전환은 더 풍부한 정보를 도입하면서 더 많은 도전을 가져온다. 일부 연구에서는 고품질 사고 사슬을 생성하기 위해 다중 모드 모델을 미세 조정함으로써 다중 모드 시나리오에서 사고 X-of-사고 추론을 탐구하려고 시도했다.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS1.p2.1">Multimodal-CoT<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib197" title="">2023g</a>)</cite>는 먼저 Multi-modal 모델을 미세 조정하여 연쇄 사상(chain-of-thoughts)을 생성한 다음 최종 답변을 얻기 위한 근거에 대한 이유를 설명한다. 그러나 추론 과정의 선형성의 한계를 가지고 있으며, 서로 다른 양식의 상호 작용에는 어려움이 있다. Multimodal-CoT가 직면하는 문제를 완화하기 위해 <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib174" title="">2023d</a>)</cite>는 사고 과정을 그래프로 모델링하는 GoT(Graph-of-Thought)를 제안한다. 추론 사슬을 사고 그래프로 파싱하여 비순차적인 정보 상호 작용을 포착하여 사고 과정을 보다 사실적으로 표현할 수 있게 한다. 이 측정은 그래픽 구조를 통해 선형 구조의 한계를 깨고 성능을 더욱 향상시킨다. 또한, <cite class="ltx_cite ltx_citemacro_citet">Yao et al. (<a class="ltx_ref" href="#bib.bib171" title="">2023a</a>)</cite>는 HoT(Hypergraph-of-Think)를 제안하여 사고 그래프를 하이퍼그래프로 대체함으로써 고차 다중 홉 추론과 다중 모달 비교 판단 능력이 우수한 모델을 가능하게 한다. 한편, 일부 작업은 지식 증류에 기반한 접근법을 취한다. T-SciQ<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib151" title="">2023d</a>)</cite>는 미세 조정 신호로서 LLMs로부터 고품질 CoT 근거를 생성하고 다양한 질문에 대한 효과적인 샘플을 생성하기 위한 새로운 데이터 혼합 전략을 도입한다.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS1.p3.1">앞서 언급한 연구들은 작은 모형과 미세 조정 시나리오에서 다중 모달 추론을 탐구하는데, 이는 다중 모달 사고 연쇄 추론의 영역에서 초기 시도라고 간주한다. 우리는 맥락 내 학습과 결합된 비디오 멀티모달 추론이 향후 연구의 초점이 되어야 한다고 생각한다. 한편으로 비디오는 이미지에 비해 타고난 연쇄 관계를 가진 추가적인 시간 정보를 도입한다. 연쇄적 사고 추론을 통해 서로 다른 프레임 내의 정보가 자연스럽게 연결되어 시간 관계를 명시적으로 모델링할 수 있으며, 이는 비디오 멀티모달 추론에 적합하다. 반면에 소형 모델은 용량이 제한적이며 연쇄적 사고 능력을 얻기 위해 미세 조정이 필요하다. 더 나쁜 것은 다중 모드 추론 사슬을 얻기 어렵고, 이는 도전을 더욱 악화시킨다. 이에 비해 현대 비전 언어 기반 모델(VLMs) <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a class="ltx_ref" href="#bib.bib2" title="">2022</a>; Li et al., <a class="ltx_ref" href="#bib.bib76" title="">2023a</a>; Wang et al., <a class="ltx_ref" href="#bib.bib156" title="">2022b</a>; Huang et al., <a class="ltx_ref" href="#bib.bib51" title="">2023b</a>; Peng et al., <a class="ltx_ref" href="#bib.bib116" title="">2023</a>; Yu et al., <a class="ltx_ref" href="#bib.bib184" title="">2021b</a>)</cite>는 비전 언어 이해력이 강하고 이미 인터리브된 텍스트와 이미지로 인컨텍스트 학습이 가능하다. 그들은 맥락 내 학습으로 사고 연쇄 추론을 위한 견고한 기반을 제공한다. 비디오 추론을 위해 연쇄적 사고를 활용하는 것은 몇 가지 연구만 있는 미개척 영역으로 남아 있다. CoMT<cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="#bib.bib47" title="">2023b</a>)</cite>는 비디오 추론에서 빠른 사고와 느린 사고를 결합하고 계획을 위한 트리 탐색 전략을 도입하는데, 먼저 비디오 멀티모달 추론에서 CoT를 적용한다.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS1.p4.1">일부 작업은 연쇄 사고 추론을 활용하고 다중 모드 추론 작업을 해결하기 시작했지만 이전 작업은 고품질 미세 조정 데이터를 구성하는 방법에만 초점을 맞추고 있으며 여전히 몇 가지 과제가 남아 있다.</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.i1.p1.1">시각 및 언어 기능을 통합하여 보다 나은 멀티모달 이해를 이끌어내는 방법.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.i2.p1.1">미세 조정 없이 연쇄 사고 추론에 VLMs을 사용하는 방법.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.i3.p1.1">영상 멀티모달 추론을 동영상 멀티모달 추론에 적용하는 방법.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Faithfulness</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p1.1">광범위한 연구는 사고 연쇄 추론이 사실적 실수 및 맥락적 불일치와 같은 환각 현상을 유발할 수 있음을 나타낸다. 언어 모델은 근본적으로 통계적 모델에 속하며, 데이터 노이즈, 지식 망각 등의 요인으로 인해 환각 현상이 불가피하다.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p2.1">일부 작업은 사실상의 실수를 완화하는 데 중점을 둡니다. <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a class="ltx_ref" href="#bib.bib40" title="">2023a</a>)</cite>는 추론 체인을 평가하기 위해 외부 지식을 도입하고 사실적 실수를 포함하지만 수정하지 않은 체인을 걸러내기 위해 표를 도입합니다. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib149" title="">2023b</a>)</cite>는 유사한 방식을 채택하며, 저점수의 추론을 수정하기 위해 반영 메커니즘을 추가로 도입한다는 차이점이 있습니다. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a class="ltx_ref" href="#bib.bib198" title="">2023a</a>)</cite>는 일관성에 의한 저신뢰 추론을 걸러내고 관련 외부 지식을 바탕으로 모델을 재추론하도록 안내한다. 앞서 언급한 방법은 지식 집약적인 작업에서 잘 작동하지만 맥락적 불일치의 문제를 해결하는 데 부족한다. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib192" title="">2023d</a>)</cite>는 추론 과정에서 환각 눈덩이 현상을 탐구한다. 다른 사람들은 불일치 문제를 해결하는 것을 목표로 한다. <cite class="ltx_cite ltx_citemacro_citet">Radhakrishnan et al. (<a class="ltx_ref" href="#bib.bib120" title="">2023</a>)</cite>는 단순한 질문을 다룰 때 모델이 더 충실하다는 것을 관찰한다. 따라서 질문 분해를 통해 충실성을 향상시킵니다. Faithful CoT<cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a class="ltx_ref" href="#bib.bib94" title="">2023</a>)</cite>는 처음에 기호 추론 체인을 생성하고 나중에 결정론적으로 기호 함수를 실행하여 추론 불일치를 완화한다. <cite class="ltx_cite ltx_citemacro_citet">Lanham et al. (<a class="ltx_ref" href="#bib.bib69" title="">2023</a>)</cite>는 충성에 영향을 미치는 요인을 탐색하여 경험적 관점을 제공한다. 그것은 충실성이 다른 작업에 따라 다르며 모델 크기가 증가함에 따라 감소한다는 것을 발견한다. CoNLI<cite class="ltx_cite ltx_citemacro_citep">(Lei et al., <a class="ltx_ref" href="#bib.bib72" title="">2023b</a>)</cite>는 환각을 줄이기 위한 사후 편집 전략을 제안한다. SynTra<cite class="ltx_cite ltx_citemacro_citep">(Jones et al., <a class="ltx_ref" href="#bib.bib59" title="">2023</a>)</cite>는 환각을 쉽게 이끌어낼 수 있도록 설계된 합성 데이터 세트에서 prefix-tuning을 수행한 다음 이 기능을 실제 작업으로 전달한다.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p3.1">큰 언어 모델에서 환각 문제를 해결하기 위한 수많은 노력에도 불구하고 이러한 작업은 문제를 어느 정도 완화했을 뿐이다. 대형 언어 모델의 충실성을 완전히 높이려면 아직 갈 길이 멀다. 향후 방향을 정리하면 다음과 같다.</p>
<ul id="S6.I2" class="ltx_itemize">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.i1.p1.1">추론 과정에서 환각 현상을 인식하는 능력을 향상시킨다.</p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.i2.p1.1">외부 지식 검색 및 활용의 정확성을 향상시켜 사실상의 실수를 줄입니다.</p>
</div>
</li>
<li id="S6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.i3.p1.1">맥락적 불일치와 논리적 오류를 인식하고 수정하는 능력을 향상시키는 것은 더 어렵다.</p>
</div>
</li>
<li id="S6.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.i4.p1.1">특정 사전 훈련과 같은 대안적 접근으로부터 환각 현상을 근본적으로 제거하는 방법.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>CoT Theory</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p1.1">연쇄적 사고 추론의 인상적인 능력에도 불구하고, 지시에 따라 연쇄적 사고를 생성하는 능력은 여전히 포괄적인 설명이 부족하다.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p2.1">일부는 경험적 관점에서 다루며 실질적인 가이드 역할을 할 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Madaan and Yazdanbakhsh (<a class="ltx_ref" href="#bib.bib96" title="">2022</a>)</cite>는 프롬프트를 기호, 패턴, 텍스트의 세 가지 구성요소로 분해하여 반사실적 프롬프트를 통해 CoT의 영향을 탐구한다. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib147" title="">2023a</a>)</cite>는 데모 선택의 영향을 분석한다. 그들은 추론 사슬의 정확성은 무시할 수 있는 영향을 미치는 반면 질문과 올바른 추론 순서와의 관련성은 중요하다는 것을 발견했다. <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="#bib.bib142" title="">2023</a>)</cite>는 의미론의 역할을 탐구한다. 그들은 연쇄적 사고 추론이 사전 훈련 동안 도입된 의미적 지식에 크게 의존하고 상징적 추론에서 제대로 수행되지 않는다는 것을 발견했다.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p3.1">다른 사람들은 근본적인 원리와 내부 메커니즘을 탐구하면서 이론적으로 분석한다. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="#bib.bib83" title="">2023e</a>)</cite> deconstructs chain-of-thought reasoning as a multi-step combinatorial function. 그들은 연쇄적 사고가 복잡한 문제를 해결하기 위한 맥락 내 학습의 복잡성을 감소시킨다는 것을 보여준다. <cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a class="ltx_ref" href="#bib.bib29" title="">2023</a>)</cite>는 이론적으로 고정된 크기의 Transformer가 연쇄 사상(chain-of-thought)을 갖는 계산 작업 및 동적 계획에 충분하다는 것을 증명한다. <cite class="ltx_cite ltx_citemacro_citet">Merrill and Sabharwal (<a class="ltx_ref" href="#bib.bib99" title="">2023</a>)</cite>는 중간 추론 단계의 수가 증가함에 따라 개선 정도가 증가하는 사고 연쇄가 추론 능력을 향상시킬 수 있음을 관찰한다. <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="#bib.bib164" title="">2023</a>)</cite>는 기울기 기반 특징 속성 방법을 활용하여 연쇄 사상이 출력에 미치는 영향을 탐구합니다. 결과는 사고 사슬이 질문의 섭동 및 변화에 대한 견고성을 나타낸다는 것을 나타낸다. 또한, 연쇄 사상 능력이 사전 훈련 단계 <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a class="ltx_ref" href="#bib.bib97" title="">2022</a>; Zhang et al., <a class="ltx_ref" href="#bib.bib191" title="">2023c</a>)</cite> 동안 코드 데이터에서 비롯되었다는 주장도 있지만, 현재 이 의견을 입증하기 위한 체계적인 작업은 없다.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p4.1">현재 연쇄 사상 이론에 대한 연구는 아직 초기 탐구 단계에 있다. 향후 연구 방향을 정리하면 다음과 같다.</p>
<ul id="S6.I3" class="ltx_itemize">
<li id="S6.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I3.i1.p1" class="ltx_para">
<p class="ltx_p" id="S6.I3.i1.p1.1">CoT 추론에서 목표된 개선을 달성하기 위한 연쇄 사고 능력의 출처를 탐색합니다.</p>
</div>
</li>
<li id="S6.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I3.i2.p1" class="ltx_para">
<p class="ltx_p" id="S6.I3.i2.p1.1">맥락 내 학습보다 사고 연쇄의 장점을 이론적으로 분석하고 역량의 경계를 탐구한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Comparison of XoT Construction</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS1.p1.1">기존 방법에 대한 X-of-thought 구성 방법에는 (1) <span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.1">Manual</span> 레이블링 추론 체인의 세 가지 주요 방법이 있습니다. (2) <span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.2">Automatic</span> generating reasoning chains by models. (3) <span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.3">Semi-automatic</span> generation with automatic expansion on a small number manually labeled reasoning chains.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS1.p2.1">수동 구성 방법 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib161" title="">2022b</a>; Gao et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>)</cite>는 상황 내 학습, 즉 데모 선택, 명령어 형식화 등과 유사한 문제에 직면해 있음을 관찰한다. 이는 그 적용에 많은 어려움을 야기하고 다른 업무들에 걸친 전이 능력을 방해한다. 자동 구성 방법<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib196" title="">2023f</a>; Chen et al., <a class="ltx_ref" href="#bib.bib12" title="">2022a</a>; Xu et al., <a class="ltx_ref" href="#bib.bib167" title="">2023</a>)</cite>는 고품질 주석의 지침이 부족하여 성능 부족이 발생합니다. 수동 주석이 가져오는 신호를 이용하여 반자동 방법<cite class="ltx_cite ltx_citemacro_citep">(Shum et al., <a class="ltx_ref" href="#bib.bib135" title="">2023</a>; Shao et al., <a class="ltx_ref" href="#bib.bib130" title="">2023</a>)</cite>는 자체 부트스트래핑 및 유사한 기술을 통해 고품질 추론 체인을 생성할 수 있어 이전 접근법이 직면한 문제를 효과적으로 해결할 수 있다. 뛰어난 성능을 달성하면서도 다양한 작업에 쉽게 이동할 수 있습니다.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Comparison between Verification/Refinement and Planning</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.p1.1">계획 방법과 검증/정제 기반 방법 사이에는 수많은 병렬이 존재하며, 둘 다 동작을 조정하고 정제하기 위해 중간 프로세스의 피드백에 의존하기 때문이다. 차이점은 계획 방법이 의사 결정을 포함하는 반면 검증/정제 기반 방법은 상위 수준의 인지 과정을 탐구하지 않고 중간 오류만 해결한다는 사실에 있다.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS2.p2.1">LLM 추론 과정은 종종 환각적이어서 사실적이고 논리적인 실수를 일으킨다. <cite class="ltx_cite ltx_citemacro_citep">(Ling et al., <a class="ltx_ref" href="#bib.bib85" title="">2023</a>; Zhao et al., <a class="ltx_ref" href="#bib.bib198" title="">2023a</a>; Madaan et al., <a class="ltx_ref" href="#bib.bib95" title="">2023</a>; Shinn et al., <a class="ltx_ref" href="#bib.bib133" title="">2023</a>)</cite> 추론 과정의 정확성을 검증하고 환각을 유발할 수 있는 추론 단계를 정제한다. 검증 및 정교화를 통해 추론 과정에서의 계단식 오류와 환각 현상을 현저히 감소시킨다.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S7.SS2.p3.1">계획 방법<cite class="ltx_cite ltx_citemacro_citep">(Long, <a class="ltx_ref" href="#bib.bib89" title="">2023</a>; Yao et al., <a class="ltx_ref" href="#bib.bib172" title="">2023b</a>, <a class="ltx_ref" href="#bib.bib173" title="">c</a>; Liu et al., <a class="ltx_ref" href="#bib.bib86" title="">2023a</a>; Shinn et al., <a class="ltx_ref" href="#bib.bib133" title="">2023</a>)</cite>는 추론에서 의사 결정 과정을 도입한다. 그들은 피드백을 얻기 위해 중간 추론 단계를 평가하고 피드백을 기반으로 글로벌 수준에서 우수한 솔루션을 달성하기 위해 탐색 및 역추적에 참여한다. 그들의 전문화는 복잡한 문제를 처리하는 데 있으며, 특히 복잡한 다중 홉 추론 및 계획 작업에 직면할 때 놀라운 성능을 달성할 수 있도록 한다.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Compensate for Innate Weaknesses</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p1.1">LLM은 외부 정보에 접근할 수 없는 점, 산술 오류, 일관성 없는 추론 등 추론에 있어 내재적 한계가 많다. 이러한 문제는 특정 책임을 전담 모듈이나 모델에 맡김으로써 교묘하게 회피할 수 있다.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p2.1"><cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib81" title="">2023d</a>; Wang et al., <a class="ltx_ref" href="#bib.bib149" title="">2023b</a>; Lu et al., <a class="ltx_ref" href="#bib.bib90" title="">2023a</a>; Schick et al., <a class="ltx_ref" href="#bib.bib128" title="">2023</a>; Karpas et al., <a class="ltx_ref" href="#bib.bib60" title="">2022</a>; Yoran et al., <a class="ltx_ref" href="#bib.bib179" title="">2023</a>)</cite>는 모델의 외부 정보 접근 제한에 대응하여 지식 베이스, 검색 엔진, 오픈 도메인 질의 응답 시스템과 같은 외부 지식 자원을 활용한다. 일부 작업은 연산 오류를 해결하기 위해 계산기를 도입한다. <cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="#bib.bib128" title="">2023</a>; Karpas et al., <a class="ltx_ref" href="#bib.bib60" title="">2022</a>; Parisi et al., <a class="ltx_ref" href="#bib.bib112" title="">2022b</a>)</cite> 코드 실행은 결정적이며, 특정 작업은 코드 실행기 <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>; Chen et al., <a class="ltx_ref" href="#bib.bib12" title="">2022a</a>; Bi et al., <a class="ltx_ref" href="#bib.bib7" title="">2023</a>; Imani et al., <a class="ltx_ref" href="#bib.bib53" title="">2023</a>)</cite>를 도입하여 추론 과정의 일관성을 향상시킨다. 우리는 LLM을 중앙 계획 및 추론을 위한 에이전트로 채택하여 특정 하위 작업을 전용 하위 모델에 위임하는 것이 향후 복잡한 시나리오에서 대규모 모델을 적용할 수 있는 잠재적인 방법이라고 믿는다.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Other Work</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS4.p1.1">이 장에서는 연쇄 사고 추론에 대한 초기 시도를 나타내거나 특정 영역을 위해 설계된 다른 작업을 나열할 것이다.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS4.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Katz et al. (<a class="ltx_ref" href="#bib.bib61" title="">2022</a>); Zhang et al. (<a class="ltx_ref" href="#bib.bib193" title="">2022</a>)</cite>는 벤치마크 및 리소스를 제공합니다. <cite class="ltx_cite ltx_citemacro_citep">(Lampinen et al., <a class="ltx_ref" href="#bib.bib67" title="">2022</a>; Ye and Durrett, <a class="ltx_ref" href="#bib.bib175" title="">2022</a>; Arora et al., <a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Shi et al. (<a class="ltx_ref" href="#bib.bib132" title="">2023</a>)</cite>가 다국어 CoT 추론을 탐구하는 연쇄적 사고 프롬프트의 효과를 실증적으로 입증한 연구도 있다. 다른 작업은 기계 번역 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="#bib.bib41" title="">2023b</a>)</cite>, 감정 분석 <cite class="ltx_cite ltx_citemacro_citep">(Fei et al., <a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite>, 문장 임베딩 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib188" title="">2023a</a>)</cite>, 요약 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib159" title="">2023k</a>)</cite>, 산술 <cite class="ltx_cite ltx_citemacro_citep">(Lee and Kim, <a class="ltx_ref" href="#bib.bib70" title="">2023</a>)</cite>, 표식 추론 <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a class="ltx_ref" href="#bib.bib11" title="">2023</a>; Jin and Lu, <a class="ltx_ref" href="#bib.bib58" title="">2023</a>)</cite> 등과 같이 특정 도메인에 초점을 맞춘다. 또한 수학적 추론 <cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et al., <a class="ltx_ref" href="#bib.bib74" title="">2022</a>; Zhao et al., <a class="ltx_ref" href="#bib.bib199" title="">2022</a>)</cite>와 같은 특정 능력을 향상시키기 위해 특정 사전 훈련을 활용하는 연구도 있다.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p" id="S8.p1.1">본 논문에서는 X-of-thought 추론에 대한 기존 연구에 대한 광범위한 조사를 수행하여 해당 분야에 대한 포괄적인 검토를 제공한다. 일반화된 연쇄 사상(X-of-thought) 개념을 소개하고, X-of-thought 추론의 발전을 다각도로 살펴본다. 또한, 첨단 도메인에서 X-of-thought의 응용을 조사한다. 또한 본 연구가 직면한 당면 과제를 조명하고 향후 전망을 제시한다. 우리가 아는 한, 이 조사는 연쇄 사고 추론의 첫 번째 체계적인 탐색을 나타낸다. 우리의 목표는 이 조사가 이 분야의 추가 연구를 촉진할 것이라는 희망과 함께 철저한 개요와 함께 연쇄 사고 추론에 관심이 있는 연구자에게 제공하는 것이다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aggarwal et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.11860" title="" class="ltx_ref ltx_href">Let’s sample step by step: Adaptive-consistency for efficient reasoning with llms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.11860.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob&nbsp;L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Flamingo: a visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1245" title="" class="ltx_ref ltx_href">Mathqa: Towards interpretable math word problem solving with operation-based formalisms</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 2357–2367. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Simran Arora, Avanika Narayan, Mayee&nbsp;F. Chen, Laurel&nbsp;J. Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Ré. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=bhUPJnS2g0X" title="" class="ltx_ref ltx_href">Ask me anything: A simple strategy for prompting language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2308.09687" title="" class="ltx_ref ltx_href">Graph of thoughts: Solving elaborate problems with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.09687.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhakthavatsalam et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana&nbsp;Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2102.03315" title="" class="ltx_ref ltx_href">Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2102.03315.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.15452" title="" class="ltx_ref ltx_href">When do program-of-thoughts work for reasoning?</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/6239" title="" class="ltx_ref ltx_href">PIQA: reasoning about physical commonsense in natural language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, pages 7432–7439. AAAI Press.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.17126" title="" class="ltx_ref ltx_href">Large language models as tool makers</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2023)</span>
<span class="ltx_bibblock">
Wenhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.findings-eacl.83" title="" class="ltx_ref ltx_href">Large language models are few(1)-shot table reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>, pages 1090–1100. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William&nbsp;W. Cohen. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2211.12588" title="" class="ltx_ref ltx_href">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.12588.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.12524" title="" class="ltx_ref ltx_href">Theoremqa: A theorem-driven question answering dataset</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.12524.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan&nbsp;R. Routledge, and William&nbsp;Yang Wang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.300" title="" class="ltx_ref ltx_href">Finqa: A dataset of numerical reasoning over financial data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 3697–3711. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William&nbsp;Yang Wang. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.421" title="" class="ltx_ref ltx_href">Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 6279–6292. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah&nbsp;A. Smith, and Tao Yu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=lH1PV42cbF" title="" class="ltx_ref ltx_href">Binding language models in symbolic languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_href">Training verifiers to solve math word problems</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.14168.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crispino et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, and Chenguang Wang. 2023a.

</span>
<span class="ltx_bibblock">Agent instructs large language models to be general zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03710</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crispino et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, and Chenguang Wang. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.03710" title="" class="ltx_ref ltx_href">Agent instructs large language models to be general zero-shot reasoners</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dagan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Gautier Dagan, Frank Keller, and Alex Lascarides. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:260887774" title="" class="ltx_ref ltx_href">Dynamic planning with a llm</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2308.06391.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1423" title="" class="ltx_ref ltx_href">BERT: pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 4171–4186. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuliawala et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.

</span>
<span class="ltx_bibblock">Chain-of-verification reduces hallucination in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.11495</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.12246" title="" class="ltx_ref ltx_href">Active prompting with chain-of-thought for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.12246.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce&nbsp;Zheng, Zhiyong Wu, Baobao Chang, Xu&nbsp;Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2301.00234" title="" class="ltx_ref ltx_href">A survey for in-context learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.00234.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng, Shoujie Tong, Haoran Meng, Lin Xu, Zhongyu Wei, Weidong Zhan, Baobao Chang, Sujian Li, Tianyu Liu, and Zhifang Sui. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.66" title="" class="ltx_ref ltx_href">Premise-based multimodal reasoning: Conditional inference on joint textual and visual clues</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 932–946. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.81" title="" class="ltx_ref ltx_href">Successive prompting for decomposing complex questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 1251–1265. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1246" title="" class="ltx_ref ltx_href">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 2368–2378, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, and Tat-Seng Chua. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-short.101" title="" class="ltx_ref ltx_href">Reasoning implicit sentiment with chain-of-thought prompting</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 1171–1182. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di&nbsp;He, and Liwei Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.15408" title="" class="ltx_ref ltx_href">Towards revealing the mystery behind chain of thought: a theoretical perspective</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.15408.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=yf1icZHC-l9" title="" class="ltx_ref ltx_href">Complexity-based prompting for multi-step reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Yao Fu, Hao-Chun Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:256390607" title="" class="ltx_ref ltx_href">Specializing smaller language models towards multi-step reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/gao23f.html" title="" class="ltx_ref ltx_href">PAL: Program-aided language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 10764–10799. PMLR.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerevini (2020)</span>
<span class="ltx_bibblock">
Alfonso&nbsp;Emilio Gerevini. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.artint.2019.103221" title="" class="ltx_ref ltx_href">An introduction to the planning domain definition language (PDDL): book review</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Artif. Intell.</em>, 280:103221.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00370" title="" class="ltx_ref ltx_href">Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, 9:346–361.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.17452" title="" class="ltx_ref ltx_href">Tora: A tool-integrated reasoning agent for mathematical problem solving</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Gupta (2022)</span>
<span class="ltx_bibblock">
Pranay Gupta and Manish Gupta. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-031-05981-0_1" title="" class="ltx_ref ltx_href">Newskvqa: Knowledge-aware news video question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Knowledge Discovery and Data Mining - 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16-19, 2022, Proceedings, Part III</em>, volume 13282 of <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 3–15. Springer.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, and Baoyuan Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.05074" title="" class="ltx_ref ltx_href">Dialcot meets ppo: Decomposing and exploring reasoning paths in smaller language models</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq&nbsp;R. Joty, Alexander&nbsp;R. Fabbri, Wojciech Kryscinski, Xi&nbsp;Victoria Lin, Caiming Xiong, and Dragomir Radev. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2209.00840" title="" class="ltx_ref ltx_href">FOLIO: natural language reasoning with first-order logic</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2209.00840.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shibo Hao, Yilan Gu, Haodi Ma, Joshua&nbsp;Jiahua Hong, Zhen Wang, Daisy&nbsp;Zhe Wang, and Zhiting Hu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258865812" title="" class="ltx_ref ltx_href">Reasoning with language model is planning with world model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.14992.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hangfeng He, Hongming Zhang, and Dan Roth. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2301.00303" title="" class="ltx_ref ltx_href">Rethinking with retrieval: Faithful large language model inference</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.00303.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.04118" title="" class="ltx_ref ltx_href">Exploring human-like translation strategy with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.04118.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html" title="" class="ltx_ref ltx_href">Measuring mathematical problem solving with the MATH dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.830" title="" class="ltx_ref ltx_href">Large language models are reasoning teachers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 14852–14882. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
Mohammad&nbsp;Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/d14-1058" title="" class="ltx_ref ltx_href">Learning to solve arithmetic word problems with verb categorization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, pages 523–533. ACL.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander&nbsp;J. Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258461606" title="" class="ltx_ref ltx_href">Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.02301.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, and Yue Zhang. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.10276" title="" class="ltx_ref ltx_href">Chain-of-symbol prompting elicits planning in large langauge models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.10276.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Pengbo Hu, Ji&nbsp;Qi, Xingyu Li, Hong Li, Xinqi Wang, Bing Quan, Ruiyu Wang, and Yi&nbsp;Zhou. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2308.09658" title="" class="ltx_ref ltx_href">Tree-of-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.09658.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang&nbsp;Shane Gu, Le&nbsp;Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2210.11610" title="" class="ltx_ref ltx_href">Large language models can self-improve</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2210.11610.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu&nbsp;Steven Zheng, Adams&nbsp;Wei Yu, Xinying Song, and Denny Zhou. 2023a.

</span>
<span class="ltx_bibblock">Large language models cannot self-correct reasoning yet.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01798</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Lifu Huang, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1243" title="" class="ltx_ref ltx_href">Cosmos QA: machine reading comprehension with contextual commonsense reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, pages 2391–2401. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Shaohan Huang, Li&nbsp;Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais&nbsp;Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.14045" title="" class="ltx_ref ltx_href">Language is not all you need: Aligning perception with language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.14045.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil&nbsp;Zhenqiang Gong, and Lichao Sun. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.03128" title="" class="ltx_ref ltx_href">Metatool benchmark: Deciding whether to use tools and which to use</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imani et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shima Imani, Liang Du, and Harsh Shrivastava. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-industry.4" title="" class="ltx_ref ltx_href">Mathprompter: Mathematical reasoning using large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track, ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 37–42. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Towards mitigating hallucination in large language models via self-reflection.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06271</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Song Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz, Yinglong Xia, Bugra Akyildiz, Yizhou Sun, Jinchao Li, Qifan Wang, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Resprompt: Residual connection prompting advances multi-step reasoning in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.04743</em>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu&nbsp;Zhang, Zhenguo Li, and James&nbsp;T. Kwok. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2308.07758" title="" class="ltx_ref ltx_href">Forward-backward reasoning in large language models for verification</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.07758.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jie et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhanming Jie, Trung&nbsp;Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.11054" title="" class="ltx_ref ltx_href">Design of chain-of-thought in math problem solving</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin and Lu (2023)</span>
<span class="ltx_bibblock">
Ziqi Jin and Wei Lu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.651" title="" class="ltx_ref ltx_href">Tab-cot: Zero-shot tabular chain of thought</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 10259–10277. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. 2023.

</span>
<span class="ltx_bibblock">Teaching language models to hallucinate less with synthetic tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06827</em>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpas et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ehud&nbsp;D. Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, and Moshe Tenenholtz. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:248496374" title="" class="ltx_ref ltx_href">Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.00445.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katz et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Uri Katz, Mor Geva, and Jonathan Berant. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-emnlp.188" title="" class="ltx_ref ltx_href">Inferring implicit relations in complex questions with language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 2548–2566. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khalifa et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu&nbsp;Wang. 2023.

</span>
<span class="ltx_bibblock">Discriminator-guided multi-step reasoning with language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14934</em>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khot et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=_nGgzQjzaRy" title="" class="ltx_ref ltx_href">Decomposed prompting: A modular approach for solving complex tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang&nbsp;Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Large language models are zero-shot reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koncel-Kedziorski et&nbsp;al. (2015)</span>
<span class="ltx_bibblock">
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena&nbsp;Dumas Ang. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00160" title="" class="ltx_ref ltx_href">Parsing algebraic word problems into equations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 3:585–597.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koncel-Kedziorski et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n16-1136" title="" class="ltx_ref ltx_href">MAWPS: A math word problem repository</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016</em>, pages 1152–1157. The Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lampinen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Andrew&nbsp;K. Lampinen, Ishita Dasgupta, Stephanie C.&nbsp;Y. Chan, Kory&nbsp;W. Mathewson, Mh&nbsp;Tessler, Antonia Creswell, James&nbsp;L. McClelland, Jane Wang, and Felix Hill. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-emnlp.38" title="" class="ltx_ref ltx_href">Can language models learn from explanations in context?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 537–563. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lange et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Matthias&nbsp;De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu&nbsp;Jia, Ales Leonardis, Gregory&nbsp;G. Slabaugh, and Tinne Tuytelaars. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPAMI.2021.3057446" title="" class="ltx_ref ltx_href">A continual learning survey: Defying forgetting in classification tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>, 44(7):3366–3385.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lanham et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel&nbsp;R. Bowman, and Ethan Perez. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.13702" title="" class="ltx_ref ltx_href">Measuring faithfulness in chain-of-thought reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.13702.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Kim (2023)</span>
<span class="ltx_bibblock">
Soochan Lee and Gunhee Kim. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.40" title="" class="ltx_ref ltx_href">Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 623–658. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Bin Lei, Pei-Hung Lin, Chunhua Liao, and Caiwen Ding. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2308.08614" title="" class="ltx_ref ltx_href">Boosting logical reasoning in large language models through a new framework: The graph of thought</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.08614.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Chain of natural language inference for reducing large language model ungrounded hallucinations.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03951</em>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jie Lei, Licheng Yu, Tamara&nbsp;L. Berg, and Mohit Bansal. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.706" title="" class="ltx_ref ltx_href">What is more likely to happen next? video-and-language future event prediction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 8769–8784. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay&nbsp;V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Solving quantitative reasoning problems with language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Jiangtong Li, Li&nbsp;Niu, and Liqing Zhang. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR52688.2022.02059" title="" class="ltx_ref ltx_href">From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</em>, pages 21241–21250. IEEE.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C.&nbsp;H. Hoi. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/li23q.html" title="" class="ltx_ref ltx_href">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em id="bib.bib76.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 19730–19742. PMLR.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Liunian&nbsp;Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.150" title="" class="ltx_ref ltx_href">Symbolic chain-of-thought distillation: Small models can also "think" step-by-step</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 2665–2679. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258179056" title="" class="ltx_ref ltx_href">Api-bank: A benchmark for tool-augmented llms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2304.08244.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Xiang&nbsp;Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:253157949" title="" class="ltx_ref ltx_href">Contrastive decoding: Open-ended text generation as optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Qiu (2023)</span>
<span class="ltx_bibblock">
Xiaonan Li and Xipeng Qiu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.05181" title="" class="ltx_ref ltx_href">Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.05181.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
Xingxuan Li, Ruochen Zhao, Yew&nbsp;Ken Chia, Bosheng Ding, Lidong Bing, Shafiq&nbsp;R. Joty, and Soujanya Poria. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.13269" title="" class="ltx_ref ltx_href">Chain of knowledge: A framework for grounding large language models with structured knowledge bases</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13269.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022c)</span>
<span class="ltx_bibblock">
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, B.&nbsp;Chen, Jian-Guang Lou, and Weizhu Chen. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:259370847" title="" class="ltx_ref ltx_href">Making language models better reasoners with step-aware verifier</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023e)</span>
<span class="ltx_bibblock">
Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris&nbsp;S. Papailiopoulos, and Samet Oymak. 2023e.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.18869" title="" class="ltx_ref ltx_href">Dissecting chain-of-thought: A study on compositional in-context learning of mlps</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.18869.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1015" title="" class="ltx_ref ltx_href">Program induction by rationale generation: Learning to solve and explain algebraic word problems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers</em>, pages 158–167. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.03872" title="" class="ltx_ref ltx_href">Deductive verification of chain-of-thought reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.03872.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Bo&nbsp;Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.11477" title="" class="ltx_ref ltx_href">Llm+p: Empowering large language models with optimal planning proficiency</a>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi, and Asli Celikyilmaz. 2023b.

</span>
<span class="ltx_bibblock">Crystal: Introspective reasoners reinforced with self-feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.04921</em>.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/ijcai.2020/501" title="" class="ltx_ref ltx_href">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</em>, pages 3622–3628. ijcai.org.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long (2023)</span>
<span class="ltx_bibblock">
Jieyi Long. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.08291" title="" class="ltx_ref ltx_href">Large language model guided tree-of-thought</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.08291.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.06575" title="" class="ltx_ref ltx_href">Chain-of-dictionary prompting elicits translation in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.06575.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/11332b6b6cf4485b84afadb1352d3a9a-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Learn to explain: Multimodal reasoning via thought chains for science question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying&nbsp;Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=DHyHRBwJUTN" title="" class="ltx_ref ltx_href">Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Yining Lu, Haoping Yu, and Daniel Khashabi. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.08775" title="" class="ltx_ref ltx_href">Gear: Augmenting language models with generalizable and efficient tool resolution</a>.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Qing Lyu, Shreya Havaldar, Adam Stein, Li&nbsp;Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2301.13379" title="" class="ltx_ref ltx_href">Faithful chain-of-thought reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.13379.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa&nbsp;Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2303.17651" title="" class="ltx_ref ltx_href">Self-refine: Iterative refinement with self-feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.17651.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan and Yazdanbakhsh (2022)</span>
<span class="ltx_bibblock">
Aman Madaan and Amir Yazdanbakhsh. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2209.07686" title="" class="ltx_ref ltx_href">Text and patterns: For effective chain of thought, it takes two to tango</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2209.07686.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.90" title="" class="ltx_ref ltx_href">Language models of code are few-shot commonsense learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 1384–1403. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magister et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lucie&nbsp;Charlotte Magister, Jonathan Mallinson, Jakub Adámek, Eric Malmi, and Aliaksei Severyn. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-short.151" title="" class="ltx_ref ltx_href">Teaching small language models to reason</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 1773–1781. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merrill and Sabharwal (2023)</span>
<span class="ltx_bibblock">
William Merrill and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.07923" title="" class="ltx_ref ltx_href">The expresssive power of transformers with chain of thought</a>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ning Miao, Yee&nbsp;Whye Teh, and Tom Rainforth. 2023.

</span>
<span class="ltx_bibblock">Selfcheck: Using llms to zero-shot check their own step-by-step reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.00436</em>.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.92" title="" class="ltx_ref ltx_href">A diverse corpus for evaluating and developing English math word problem solvers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 975–984, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1260" title="" class="ltx_ref ltx_href">Can a suit of armor conduct electricity? a new dataset for open book question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2381–2391, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.392" title="" class="ltx_ref ltx_href">LILA: A unified benchmark for mathematical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 5807–5832. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep&nbsp;Singh Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.246" title="" class="ltx_ref ltx_href">Numglue: A suite of fundamental yet challenging mathematical reasoning tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 3505–3523. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo and Xin (2023)</span>
<span class="ltx_bibblock">
Shentong Mo and Miao Xin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2309.07694" title="" class="ltx_ref ltx_href">Tree of uncertain thoughts reasoning for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.07694.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naik et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi. 2023.

</span>
<span class="ltx_bibblock">Diversity of thought improves reasoning abilities of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07088</em>.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu&nbsp;Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.15337" title="" class="ltx_ref ltx_href">Skeleton-of-thought: Large language models can do parallel decoding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.15337.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Brien and Lewis (2023)</span>
<span class="ltx_bibblock">
Sean O’Brien and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:261884427" title="" class="ltx_ref ltx_href">Contrastive decoding improves reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2309.09117.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2303.08774" title="" class="ltx_ref ltx_href">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paranjape et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco&nbsp;Tulio Ribeiro. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.09014" title="" class="ltx_ref ltx_href">Art: Automatic multi-step reasoning and tool-use for large language models</a>.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parisi et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:249017698" title="" class="ltx_ref ltx_href">Talm: Tool augmented language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.12255.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parisi et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2205.12255" title="" class="ltx_ref ltx_href">TALM: tool augmented language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2205.12255.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jae&nbsp;Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-58558-7_30" title="" class="ltx_ref ltx_href">Visualcomet: Reasoning about the dynamic context of a still image</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V</em>, volume 12350 of <em id="bib.bib113.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 508–524. Springer.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.168" title="" class="ltx_ref ltx_href">Are NLP models really able to solve simple math word problems?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, pages 2080–2094. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2304.01904" title="" class="ltx_ref ltx_href">REFINER: reasoning feedback on intermediate representations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.01904.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhiliang Peng, Wenhui Wang, Li&nbsp;Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.14824" title="" class="ltx_ref ltx_href">Kosmos-2: Grounding multimodal large language models to the world</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.14824.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pitis et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Silviu Pitis, Michael&nbsp;R. Zhang, Andrew Wang, and Jimmy Ba. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2304.05970" title="" class="ltx_ref ltx_href">Boosted prompt ensembles for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.05970.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.acl-long.294" title="" class="ltx_ref ltx_href">Reasoning with language model prompting: A survey</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 5368–5393. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford and Narasimhan (2018)</span>
<span class="ltx_bibblock">
Alec Radford and Karthik Narasimhan. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:49313245" title="" class="ltx_ref ltx_href">Improving language understanding by generative pre-training</a>.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radhakrishnan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer&nbsp;El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel&nbsp;R. Bowman, and Ethan Perez. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.11768" title="" class="ltx_ref ltx_href">Question decomposition improves the faithfulness of model-generated reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.11768.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 21(1).

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashkin et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Hannah Rashkin, Maarten Sap, Emily Allaway, Noah&nbsp;A. Smith, and Yejin Choi. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1043" title="" class="ltx_ref ltx_href">Event2mind: Commonsense inference on events, intents, and reactions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers</em>, pages 463–473. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy and Roth (2015)</span>
<span class="ltx_bibblock">
Subhro Roy and Dan Roth. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D15-1202" title="" class="ltx_ref ltx_href">Solving general arithmetic word problems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, pages 1743–1752, Lisbon, Portugal. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.03427" title="" class="ltx_ref ltx_href">Tptu: Task planning and tool usage of large language model-based ai agents</a>.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saparov and He (2023)</span>
<span class="ltx_bibblock">
Abulhair Saparov and He&nbsp;He. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=qFVVBzXxR2V" title="" class="ltx_ref ltx_href">Language models are greedy reasoners: A systematic formal analysis of chain-of-thought</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush, Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa Adelani, and et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2211.05100" title="" class="ltx_ref ltx_href">BLOOM: A 176b-parameter open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.05100.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2304.15004" title="" class="ltx_ref ltx_href">Are emergent abilities of large language models a mirage?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.15004.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.04761" title="" class="ltx_ref ltx_href">Toolformer: Language models can teach themselves to use tools</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.04761.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sel et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu&nbsp;Wang, Ruoxi Jia, and Ming Jin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2308.10379" title="" class="ltx_ref ltx_href">Algorithm of thoughts: Enhancing exploration of ideas in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.10379.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.00618" title="" class="ltx_ref ltx_href">Synthetic prompting: Generating chain-of-thought demonstrations for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.00618.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, Xu&nbsp;Tan, Dong&nbsp;Sheng Li, Weiming Lu, and Yue&nbsp;Ting Zhuang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:257833781" title="" class="ltx_ref ltx_href">Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.17580.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=fR3wGCk-IXp" title="" class="ltx_ref ltx_href">Language models are multilingual chain-of-thought reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258833055" title="" class="ltx_ref ltx_href">Reflexion: Language agents with verbal reinforcement learning</a>.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shridhar et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Kumar Shridhar, Harsh Jhamtani, Hao Fang, Benjamin Van&nbsp;Durme, Jason Eisner, and Patrick Xia. 2023.

</span>
<span class="ltx_bibblock">Screws: A modular framework for reasoning with revisions.

</span>
<span class="ltx_bibblock"><em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.13075</em>.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shum et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Kashun Shum, Shizhe Diao, and Tong Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.12822" title="" class="ltx_ref ltx_href">Automatic prompt augmentation and selection with chain-of-thought from labeled data</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.12822.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal&nbsp;Md Shoeb, Abubakar Abid, Adam Fisch, Adam&nbsp;R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander&nbsp;W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman&nbsp;S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew&nbsp;M. Dai, Andrew La, Andrew&nbsp;K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2206.04615" title="" class="ltx_ref ltx_href">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2206.04615.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo&nbsp;Dai, and Chao Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258947337" title="" class="ltx_ref ltx_href">Adaplanner: Adaptive planning from feedback with language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.16653.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V. Le, Ed&nbsp;Chi, Denny Zhou, and Jason Wei. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.824" title="" class="ltx_ref ltx_href">Challenging big-bench tasks and whether chain-of-thought can solve them</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13003–13051. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tafjord et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-acl.317" title="" class="ltx_ref ltx_href">Proofwriter: Generating implications, proofs, and abductive statements over natural language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, volume ACL/IJCNLP 2021 of <em id="bib.bib139.2.2" class="ltx_emph ltx_font_italic">Findings of ACL</em>, pages 3621–3634. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1421" title="" class="ltx_ref ltx_href">Commonsenseqa: A question answering challenge targeting commonsense knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 4149–4158. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Alon Talmor, Ori Yoran, Ronan&nbsp;Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3ef815416f775098fe977004015c6193-Abstract-round1.html" title="" class="ltx_ref ltx_href">Commonsenseqa 2.0: Exposing the limits of AI through gamification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.14825" title="" class="ltx_ref ltx_href">Large language models are in-context semantic reasoners rather than symbolic reasoners</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14825.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.13971" title="" class="ltx_ref ltx_href">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.13971.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.09288.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan&nbsp;Ö. Arik, and Tomas Pfister. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.216" title="" class="ltx_ref ltx_href">Better zero-shot reasoning with self-adaptive prompting</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 3493–3514. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Boshi Wang, Xiang Deng, and Huan Sun. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.174" title="" class="ltx_ref ltx_href">Iteratively prompt pre-trained language models for chain of thought</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 2714–2730. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.153" title="" class="ltx_ref ltx_href">Towards understanding chain-of-thought prompting: An empirical study of what matters</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 2717–2739. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1393" title="" class="ltx_ref ltx_href">Does it make sense? and why? a pilot study for sense making and explanation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4020–4026, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and Ming Gao. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.06427" title="" class="ltx_ref ltx_href">Boosting language models reasoning with chain-of-knowledge prompting</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.06427.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, and Zhang Xiong. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.13259" title="" class="ltx_ref ltx_href">Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-intensive question answering</a>.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
Lei Wang, Yi&nbsp;Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, and Heng&nbsp;Tao Shen. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.03453" title="" class="ltx_ref ltx_href">T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.03453.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023e)</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu&nbsp;Chen, Yankai Lin, Wayne&nbsp;Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023e.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2308.11432" title="" class="ltx_ref ltx_href">A survey on large language model based autonomous agents</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.11432.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023f)</span>
<span class="ltx_bibblock">
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy&nbsp;Ka-Wei Lee, and Ee-Peng Lim. 2023f.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.acl-long.147" title="" class="ltx_ref ltx_href">Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 2609–2634. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023g)</span>
<span class="ltx_bibblock">
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023g.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.00487" title="" class="ltx_ref ltx_href">A comprehensive survey of continual learning: Theory, method and application</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.00487.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023h)</span>
<span class="ltx_bibblock">
Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023h.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258461058" title="" class="ltx_ref ltx_href">Scott: Self-consistent chain-of-thought distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Wenhui Wang, Hangbo Bao, Li&nbsp;Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais&nbsp;Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2208.10442" title="" class="ltx_ref ltx_href">Image as a foreign language: Beit pretraining for all vision and vision-language tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2208.10442.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023i)</span>
<span class="ltx_bibblock">
Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, and Alessandro Sordoni. 2023i.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.05707" title="" class="ltx_ref ltx_href">Guiding language model reasoning with planning tokens</a>.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023j)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc&nbsp;V. Le, Ed&nbsp;H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023j.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=1PL1NIMMrw" title="" class="ltx_ref ltx_href">Self-consistency improves chain of thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023k)</span>
<span class="ltx_bibblock">
Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023k.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.482" title="" class="ltx_ref ltx_href">Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 8640–8665. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Jason Wei, Yi&nbsp;Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed&nbsp;H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=yzkSU5zdwD" title="" class="ltx_ref ltx_href">Emergent abilities of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed&nbsp;H. Chi, Quoc&nbsp;V. Le, and Denny Zhou. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. 2022.

</span>
<span class="ltx_bibblock">Large language models are reasoners with self-verification.

</span>
<span class="ltx_bibblock"><em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.09561</em>.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Bo&nbsp;Wu, Shoubin Yu, Zhenfang Chen, Josh Tenenbaum, and Chuang Gan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/5ef059938ba799aaa845e1c2e8a762bd-Abstract-round2.html" title="" class="ltx_ref ltx_href">STAR: A benchmark for situated reasoning in real-world videos</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Skyler Wu, Eric&nbsp;Meng Shen, Charumathi Badrinath, Jiaqi Ma, and Himabindu Lakkaraju. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.13339" title="" class="ltx_ref ltx_href">Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.13339.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi&nbsp;Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2309.07864" title="" class="ltx_ref ltx_href">The rise and potential of large language model based agents: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.07864.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR46437.2021.00965" title="" class="ltx_ref ltx_href">Next-qa: Next phase of question-answering to explaining temporal actions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021</em>, pages 9777–9786. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Weijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.09993" title="" class="ltx_ref ltx_href">Reprompting: Automated chain-of-thought prompt inference through gibbs sampling</a>.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.11499" title="" class="ltx_ref ltx_href">RCOT: detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.11499.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce&nbsp;Liu, Michael Zeng, and Lijuan Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2303.11381" title="" class="ltx_ref ltx_href">MM-REACT: prompting chatgpt for multimodal reasoning and action</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.11381.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zonglin Yang, Li&nbsp;Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2212.10923" title="" class="ltx_ref ltx_href">Language models as inductive reasoners</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.10923.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li&nbsp;Jin, Shuchao Li, Xiaoyu Li, and Xian Sun. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.06207" title="" class="ltx_ref ltx_href">Thinking like an expert:multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals</a>.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas&nbsp;L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.10601" title="" class="ltx_ref ltx_href">Tree of thoughts: Deliberate problem solving with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.10601.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik&nbsp;R. Narasimhan, and Yuan Cao. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=WE_vluYUL-X" title="" class="ltx_ref ltx_href">React: Synergizing reasoning and acting in language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
Yao Yao, Zuchao Li, and Hai Zhao. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.16582" title="" class="ltx_ref ltx_href">Beyond chain-of-thought, effective graph-of-thought reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.16582.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Durrett (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Ye and Greg Durrett. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2205.03401" title="" class="ltx_ref ltx_href">The unreliability of explanations in few-shot in-context learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2205.03401.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Durrett (2023)</span>
<span class="ltx_bibblock">
Xi&nbsp;Ye and Greg Durrett. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.04813" title="" class="ltx_ref ltx_href">Explanation selection using unlabeled data for in-context learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.04813.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3539618.3591708" title="" class="ltx_ref ltx_href">Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023</em>, pages 174–184. ACM.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua&nbsp;B. Tenenbaum. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=HkxYzANYDB" title="" class="ltx_ref ltx_href">CLEVRER: collision events for video representation and reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2304.13007" title="" class="ltx_ref ltx_href">Answering questions by meta-reasoning over multiple chains of thought</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.13007.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Fei Yu, Hongbo Zhang, and Benyou Wang. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2303.14725" title="" class="ltx_ref ltx_href">Nature language reasoning, A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.14725.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Junchi Yu, Ran He, and Rex Ying. 2023b.

</span>
<span class="ltx_bibblock">Thought propagation: An analogical approach to complex reasoning with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03965</em>.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=HJgJtT4tvB" title="" class="ltx_ref ltx_href">Reclor: A reading comprehension dataset requiring logical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2021a)</span>
<span class="ltx_bibblock">
Weijiang Yu, Yingpeng Wen, Fudan Zheng, and Nong Xiao. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.272" title="" class="ltx_ref ltx_href">Improving math word problems with pre-trained knowledge and hierarchical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3384–3394, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2021b)</span>
<span class="ltx_bibblock">
Weijiang Yu, Haoteng Zheng, Mengfei Li, Lei Ji, Lijun Wu, Nong Xiao, and Nan Duan. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/dea184826614d3f4c608731389ed0c74-Paper.pdf" title="" class="ltx_ref ltx_href">Learning from inside: Self-driven siamese sampling and reasoning for video question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34:26462–26474.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.04959" title="" class="ltx_ref ltx_href">Towards better chain-of-thought prompting strategies: A survey</a>.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah&nbsp;D. Goodman. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Star: Bootstrapping reasoning with reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00688" title="" class="ltx_ref ltx_href">From recognition to cognition: Visual commonsense reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages 6720–6731. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Bowen Zhang, Kehua Chang, and Chunping Li. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2309.11143" title="" class="ltx_ref ltx_href">Cot-bert: Enhancing unsupervised sentence representation through chain-of-thought</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.11143.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Parkes (2023)</span>
<span class="ltx_bibblock">
Hugh Zhang and David&nbsp;C. Parkes. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.08589" title="" class="ltx_ref ltx_href">Chain-of-thought reasoning is a policy improvement operator</a>.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke&nbsp;Chen, Gang Chen, and Sharad Mehrotra. 2023b.

</span>
<span class="ltx_bibblock">Draft &amp; verify: Lossless large language model acceleration via self-speculative decoding.

</span>
<span class="ltx_bibblock"><em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.08168</em>.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Li&nbsp;Zhang, Liam Dugan, Hainiu Xu, and Chris Callison-Burch. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2304.13250" title="" class="ltx_ref ltx_href">Exploring the curious case of code prompts</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.13250.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah&nbsp;A. Smith. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.13534" title="" class="ltx_ref ltx_href">How language model hallucinations can snowball</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.13534.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sarah&nbsp;J. Zhang, Reece Shuttleworth, Derek Austin, Yann Hicke, Leonard Tang, Sathwik Karnik, Darnell Granberry, and Iddo Drori. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2206.05442" title="" class="ltx_ref ltx_href">A dataset and benchmark for automatically answering and generating machine learning final exams</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2206.05442.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023e)</span>
<span class="ltx_bibblock">
Tianhua Zhang, Jiaxin Ge, Hongyin Luo, Yung-Sung Chuang, Mingye Gao, Yuan Gong, Xixin Wu, Yoon Kim, Helen Meng, and James Glass. 2023e.

</span>
<span class="ltx_bibblock">Natural language embedded programs for hybrid language symbolic reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10814</em>.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Zhang (2023)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang and Aston Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.11436" title="" class="ltx_ref ltx_href">You only look at screens: Multimodal chain-of-action agents</a>.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023f)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Aston Zhang, Mu&nbsp;Li, and Alex Smola. 2023f.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=5NTt8GFjUHkr" title="" class="ltx_ref ltx_href">Automatic chain of thought prompting in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023g)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Aston Zhang, Mu&nbsp;Li, Hai Zhao, George Karypis, and Alex Smola. 2023g.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2302.00923" title="" class="ltx_ref ltx_href">Multimodal chain-of-thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.00923.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.320" title="" class="ltx_ref ltx_href">Verify-and-edit: A knowledge-enhanced chain-of-thought framework</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 5823–5840. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wayne&nbsp;Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang Zhou, Jing Sha, Zhigang Chen, Shijin Wang, Cong Liu, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3534678.3539131" title="" class="ltx_ref ltx_href">Jiuzhang: A chinese pre-trained language model for mathematical problem understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">KDD ’22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022</em>, pages 4571–4581. ACM.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Wayne&nbsp;Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2303.18223" title="" class="ltx_ref ltx_href">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.18223.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae&nbsp;Hee Lee, Kun Chu, and Stefan Wermter. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2309.13339" title="" class="ltx_ref ltx_href">Enhancing zero-shot chain-of-thought reasoning in large language models through logic</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.13339.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Huaixiu&nbsp;Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed&nbsp;H Chi, Quoc&nbsp;V Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Take a step back: Evoking reasoning via abstraction in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06117</em>.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.04406" title="" class="ltx_ref ltx_href">Language agent tree search unifies reasoning acting and planning in language models</a>.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1332" title="" class="ltx_ref ltx_href">"going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, pages 3361–3367. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Denny Zhou, Nathanael Schärli, Le&nbsp;Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc&nbsp;V. Le, and Ed&nbsp;H. Chi. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=WZH7099tgfM" title="" class="ltx_ref ltx_href">Least-to-most prompting enables complex reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, and Lei Ma. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.13724" title="" class="ltx_ref ltx_href">Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning</a>.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.254" title="" class="ltx_ref ltx_href">TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 3277–3287. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Anni Zou, Zhuosheng Zhang, Hai Zhao, and Xiangru Tang. 2023.

</span>
<span class="ltx_bibblock">Meta-cot: Generalizable chain-of-thought prompting in mixed-task scenarios with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06692</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2309.15401" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2309.15402" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2309.15402">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.15402" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2309.15404" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 04:09:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
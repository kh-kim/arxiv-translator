# 대용량 언어 모델 조사

웨인 신자오, 건주({}^{\star}\), 준이 리({}^{\star}\), 톈이 탕, 샤오레이 왕, 유펑허우, 잉건민, 백천장, 준지장, 지칸동, 이판두, 천양, 유좌천, 지펑천, 진하오장, 루이양렌, 이판리, 신유탕, 지강류, 페이유류, 지안윤니, 지롱원

* _버전: v13 (11월 23일, 2023일 주요 업데이트). _ GitHub 링크: [https://github.com/RICAIBox/LMSurvey_Chinese](https://github.com/RICAIBox/LMSurvey_Chinese) 버전 링크: [https://github.com/RICAIBox/LMSurvey/blob/unit/assets/LLM_Survey_Chinese.pdf_](https://github.com/RICAIBox/LMSurvey/blob/unit/assets/LLM_Survey_Chinese.pdf_)* _K. Zhou와 J. Li는 이 작업에 동등하게 기여했습니다. _ 저자들은 주로 중국 베이징에 있는 중국 렌민 대학의 가올링 인공지능 학파와 정보 학파, 지안윤 니는 캐나다 몬트리올 대학의 DIRO와 함께 있다. 연락처 이메일: bitmantly@gmail.com_이 조사 논문의 저자는 그림/표의 모든 공동 권리를 예약하며, 출판 목적을 위한 이러한 자료의 사용은 조사 저자에 의해 공식적으로 승인되어야 합니다._

###### Abstract

1950년대에 튜링 테스트가 제안된 이후로 인간은 기계에 의한 언어 지능의 숙달을 탐구해 왔다. 언어는 본질적으로 문법적 규칙에 의해 지배되는 인간 표현의 복잡하고 복잡한 시스템이다. 언어를 이해하고 이해하기 위한 능력 있는 인공지능(AI) 알고리즘을 개발하는 것은 상당한 도전을 제기한다. 주요 접근법으로 _언어 모델링_은 지난 20년 동안 언어 이해와 생성을 위해 널리 연구되어 통계 언어 모델에서 신경 언어 모델로 진화했다. 최근, 대규모 말뭉치에 대한 트랜스포머 모델의 사전 훈련에 의해 사전 훈련된 언어 모델(PLM)이 제안되어 다양한 자연어 처리(NLP) 작업을 해결하는 데 강력한 능력을 보여주고 있다. 연구진은 모델 스케일링이 개선된 모델 용량으로 이어질 수 있음을 발견했기 때문에 매개변수 스케일을 훨씬 더 큰 크기로 증가시켜 스케일링 효과를 추가로 조사한다. 흥미롭게도, 파라미터 스케일이 일정 수준을 초과하는 경우, 이들 확대된 언어 모델들은 상당한 성능 향상을 달성할 뿐만 아니라, 소규모 언어 모델들(_예를 들어,_BERT)에는 존재하지 않는 일부 특수 능력들(_예를 들어,_컨텍스트 내 학습)을 나타낸다. 다양한 매개변수 규모에서 언어 모델을 구별하기 위해 연구 커뮤니티는 상당한 크기의 PLM(예: 수백억 또는 수천억 개의 매개변수를 포함하는)에 대해 _대규모 언어 모델(LLM)_이라는 용어를 만들었다. 최근 LIM에 대한 연구는 학계와 산업계 모두에서 크게 발전해 왔으며, 주목할 만한 진전은 LIM을 기반으로 개발된 강력한 AI 챗봇인 ChatGPT(ChatGPT)의 출시로 사회로부터 많은 관심을 받고 있다. LIM의 기술적 진화는 AI 커뮤니티 전체에 중요한 영향을 미치고 있으며, 이는 AI 알고리즘을 개발하고 사용하는 방식에 혁명을 일으킬 것이다. 이러한 급속한 기술 진보를 고려하여 본 조사에서는 배경, 주요 발견 및 주류 기술을 도입하여 LIM의 최근 발전을 검토한다. 특히 LIM의 4가지 주요 측면, 즉 사전 훈련, 적응 조정, 활용 및 용량 평가에 중점을 둔다. 또한, LIM 개발을 위한 가용 자원을 요약하고 향후 방향을 위한 나머지 문제에 대해 논의한다. 이 조사는 LIM에 대한 문헌에 대한 최신 검토를 제공하며, 이는 연구자와 엔지니어 모두에게 유용한 자원이 될 수 있다.

 대용량 언어 모델; 비상 능력; 적응 조정; 활용; 정렬; 용량 평가

## 1 Introduction

__"내 언어의 한계는 내 세계의 한계를 의미합니다._

_--Ludwig Wittgenstein_

언어는 인간의 두드러진 표현 능력과 의사소통 능력으로 유아기에 발달하여 평생에 걸쳐 진화한다[3, 4]. 그러나 기계는 강력한 인공지능(AI) 알고리즘을 탑재하지 않으면 인간의 언어 형태로 이해하고 소통하는 능력을 자연스럽게 파악할 수 없다. 이 목표를 달성하고 기계가 인간처럼 읽고 쓰고 소통할 수 있게 하는 것은 오랜 연구 과제였다[5].

기술적으로 _언어 모델링(LM)_은 기계의 언어 지능을 향상시키는 주요 접근법 중 하나이다. 일반적으로 LM은 미래의 (또는 누락된) 토큰의 확률을 예측하기 위해 단어 시퀀스의 생성 가능성을 모델링하는 것을 목표로 한다. LM에 대한 연구는 문헌에서 광범위하게 주목을 받았으며, 이는 크게 네 가지 개발 단계로 나눌 수 있다.

\(\bullet\)_통계 언어 모델(SLM)_. SLMs[6, 7, 8, 9]는 1990년대에 상승했던 _통계적 학습_ 방법을 기반으로 개발되었다. 기본 아이디어는 마르코프 가정을 기반으로 단어 예측 모델을 구축하고, _예를 들어,_ 가장 최근의 컨텍스트를 기반으로 다음 단어를 예측하는 것이다. 문맥 길이가 \(n\)인 SLM은 \(n\)-gram 언어 모델, _예:_ bigram 및 trigram 언어 모델이라고도 합니다. SLM은 정보 검색(IR)[10, 11] 및 자연어 처리(NLP)[12, 13, 14]에서 태스크 성능을 향상시키기 위해 널리 적용되어 왔다. 그러나 이들은 차원이라는 저주에 시달리는 경우가 많다. 즉, 기하급수적인 수의 전이 확률을 추정해야 하기 때문에 고차 언어 모델을 정확하게 추정하는 것은 어렵다. 따라서 데이터 희소성 문제를 완화하기 위해 백오프 추정[15] 및 Good-Turing 추정[16]과 같은 특별히 설계된 스무딩 전략이 도입되었다.

\(\bullet\)_신경 언어 모델(NLM)_. NLM[17, 18, 1]은 신경망, _예를 들어,_MLP(multi-layer perceptron) 및 RNN(recurrent neural networks)에 의한 워드 시퀀스의 확률을 특성화한다. 뛰어난 공헌으로 [1]의 작업은 단어의 _분산 표현_ 개념을 도입하고 집계된 컨텍스트 특징(즉, 분산 단어 벡터)에 조건화된 단어 예측 기능을 구축했다. 텍스트 데이터에 대한 효과적인 특징을 학습한다는 개념을 확장하여 다양한 NLP 작업에 대한 통일된 종단 간 솔루션을 구축하기 위해 일반적인 신경망 접근법이 개발되었다[2]. 또한, 워드2vec[19, 20]은 분산 워드 표현을 학습하기 위한 단순화된 얕은 신경망을 구축하기 위해 제안되었으며, 이는 다양한 NLP 태스크들에 걸쳐 매우 효과적인 것으로 입증되었다. 이러한 연구는 표상 학습을 위한 언어 모델의 사용을 시작했으며(단어 시퀀스 모델링 외에도), NLP 분야에 중요한 영향을 미쳤다.

\(\bullet\)_PLM(Pre-trained language models)_. 초기 시도로서, ELMo[21]은 먼저 양방향 LSTM(biLSTM) 네트워크(고정 단어 표현을 학습하는 대신)를 사전 트레이닝한 다음 특정 다운스트림 태스크에 따라 biLSTM 네트워크를 미세 조정함으로써 상황 인식 단어 표현을 캡처하도록 제안되었다. 또한, 자체 주목 메커니즘을 갖는 병렬화 가능한 트랜스포머 구조 [22]를 기반으로 대규모 언라벨 코퍼라에서 특별히 설계된 사전 훈련 태스크를 갖는 양방향 언어 모델을 사전 훈련함으로써 BERT [23]을 제안하였다. 이러한 사전 훈련된 문맥 인식 단어 표현은 범용 의미 자질로서 매우 효과적이며, 이는 NLP 태스크의 성능 바를 크게 상승시켰다. 이 연구는 _"사전 훈련"_ 및 _미세 조정"_ 학습 패러다임을 설정하는 많은 후속 작업에 영감을 주었다. 이러한 패러다임에 따라 PLM에 대한 많은 연구가 개발되어 서로 다른 아키텍처[24, 25](_e.g._, GPT-2[26] 및 BART[24]) 또는 개선된 사전 훈련 전략[27, 28, 29]을 도입했다. 이 패러다임에서는 다양한 다운스트림 작업에 적응하기 위해 PLM을 미세 조정해야 하는 경우가 많다.

\(\bullet\)_LLM(Large Language Model)_. 연구자들은 PLM(_e.g._, 스케일링 모델 크기 또는 데이터 크기)을 스케일링하는 것이 다운스트림 태스크에서 종종 개선된 모델 용량으로 이어진다는 것을 발견한다.

도. 1: 키프레이즈 _“언어 모델”_(2018년 6월 이후)와 _“대형 언어 모델”_(2019년 10월 이후)가 각각 포함된 arXiv 논문의 누적 수 추이. 통계량은 제목 또는 초록의 키프레이즈를 월별로 조회하여 정확한 일치를 사용하여 계산됩니다. 우리는 두 키프레이즈에 대해 서로 다른 x축 범위를 설정했는데, 이는 "언어 모델"이 이전 시간에 탐색되었기 때문이다. 우리는 LLM의 연구 진행 과정에서 중요한 랜드마크에 해당하는 지점에 레이블을 붙인다. ChatGPT가 발표된 후 급격한 증가가 일어나는데, 제목이나 초록에 _“큰 언어 모델”_이 포함된 arXiv 논문의 평균 게재 건수는 하루 0.40건에서 8.58건으로 늘어난다(그림 1(b)).

도. 2: 과제해결역량의 관점에서 4세대 언어모델(LM)의 진화과정. 각 단계에 대한 시점은 매우 정확하지 않을 수 있으며, 각 단계에서 가장 대표적인 연구의 발표일에 따라 주로 시간을 설정한다. 신경 언어 모델의 경우, NPLM[1](_“신경 확률 언어 모델”_)과 NLPS[2](_“자연 언어 처리(거의 처음부터)”_)의 두 가지 접근법을 명명하기 위해 두 대표적인 연구의 논문 제목을 축약한다. 공간 제한으로 인해 우리는 이 그림에서 모든 대표적인 연구를 나열하지 않는다.

(_i.e._, scaling law[30])를 따른다. 많은 연구에서 훨씬 더 큰 PLM(예: 175B-파라미터 GPT-3 및 540B-파라미터 PaLM)을 훈련하여 성능 한계를 탐구했다. 스케일링은 주로 모델 크기(유사한 아키텍처 및 사전 훈련 작업 포함)에서 수행되지만, 이러한 큰 크기의 PLM은 더 작은 PLM(예:_330M-파라미터 BERT 및 1.5B-파라미터 GPT-2)과 다른 동작을 나타내고 일련의 복잡한 작업을 해결하는 데 놀라운 능력(_응급 능력_[31])을 나타낸다. 예를 들어 GPT-3는 _in-context learning_을 통해 적은 샷 작업을 해결할 수 있는 반면 GPT-2는 잘 할 수 없습니다. 따라서, 연구 커뮤니티는 증가하는 연구 관심을 끄는 이러한 대형 PLM[32, 33, 34, 35]에 대한 용어 _"대형 언어 모델(LLM)"1_을 동화한다(도 1 참조). LLM의 놀라운 적용은 GPT 시리즈의 LLM을 대화용으로 적응시키는 _ChatGPT2_이며, 이는 인간과 놀라운 대화 능력을 보여준다. 그림 1에서 ChatGPT의 출시 이후 LLM과 관련된 arXiv 논문의 급격한 증가를 관찰할 수 있다.

각주 1: LLM이 반드시 작은 PLM보다 더 능력이 있는 것은 아니며 일부 LLM에서는 출현 능력이 발생하지 않을 수 있다.

각주 2: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)

앞서 논의한 바와 같이 언어 모델은 LLM을 특별히 위한 새로운 기술적 개념이 아니라 수십 년에 걸쳐 인공지능의 발전과 함께 진화해 왔다. 초기 언어 모델은 주로 텍스트 데이터를 모델링하고 생성하는 것을 목표로 하는 반면, 최신 언어 모델(_예:_GPT-4)은 복잡한 과제 해결에 중점을 둔다. 언어 모델링에서 과제 해결까지 과학적 사고의 중요한 도약이며, 이는 연구사에서 언어 모델 개발을 이해하는 핵심이다. 과제 해결의 관점에서 4세대 언어 모델은 서로 다른 수준의 모델 역량을 보여주었다. 그림 2에서는 언어 모델의 진화 과정을 과제 해결 능력 측면에서 설명한다. 처음에, 통계적 언어 모델들은 주로 예측되거나 추정된 확률들이 태스크-특정 접근법들의 성능을 향상시킬 수 있는 일부 특정 태스크들(예컨대, 검색 또는 스피치 태스크들)에서 보조하였다. 이어서, 신경망 언어 모델들은 인간 특징 엔지니어링을 위한 노력들을 감소시키는 것을 목표로 하여, 태스크-불가지론 표현들(_예를 들어,_특징들)을 학습하는데 초점을 맞추었다. 나아가 사전 학습된 언어 모델들은 다운스트림 태스크에 따라 최적화될 수 있는 상황 인지 표현을 학습하였다. 최신 언어 모델의 생성을 위해 범용 태스크 해결 수단으로 간주될 수 있는 모델 용량에 대한 스케일링 효과를 탐색하여 LLMs을 향상시킨다. 요약하자면, 진화 과정에서 언어 모델이 해결할 수 있는 작업 범위가 크게 확장되었고, 언어 모델이 달성한 작업 성능이 크게 향상되었다.

기존 문헌에서 PLM은 널리 논의되고 조사되었지만 [36, 37, 38, 39] LLM은 체계적인 방식으로 거의 검토되지 않았다. 설문 조사에 동기를 부여하기 위해 먼저 LLM과 PLM 간의 세 가지 주요 차이점을 강조한다. 첫째, LLM은 이전의 더 작은 PLM에서 관찰되지 않을 수 있는 몇 가지 놀라운 출현 능력을 보여준다. 이러한 능력은 복잡한 작업에 대한 언어 모델의 성능에 핵심이며, AI 알고리즘을 예기치 않게 강력하고 효과적으로 만든다. 둘째, LLM은 인간이 AI 알고리즘을 개발하고 사용하는 방식에 혁명을 일으킬 것이다. 소형 PLM과 달리 LLM에 액세스하는 주요 접근법은 프롬프트 인터페이스(예를 들어, GPT-4 API)를 통한 것이다. 인간은 LLM이 어떻게 작동하는지 이해하고 LLM이 따를 수 있는 방식으로 작업을 포맷해야 한다. 셋째, LLM의 개발은 더 이상 연구와 공학의 명확한 구별을 이끌어내지 못한다. LLM의 학습은 대규모 데이터 처리 및 분산 병렬 학습에서 광범위한 실제 경험을 필요로 한다. 유능한 LLM을 개발하기 위해 연구자들은 엔지니어와 함께 작업하거나 엔지니어가 되는 복잡한 엔지니어링 문제를 해결해야 한다.

오늘날 LLM은 AI 커뮤니티에 상당한 영향을 미치고 있으며 ChatGPT와 GPT-4의 출현은 인공지능(AGI)의 가능성을 다시 생각하게 한다. OpenAI는 AGI에 접근하기 위한 단기 및 장기 계획을 논의하는 _"AGI를 위한 계획 및 그 이상"_이라는 제목의 기술 기사를 발표했으며[40], 보다 최근의 논문은 GPT-4가 AGI 시스템의 초기 버전으로 간주될 수 있다고 주장했다[41]. 인공지능의 연구 분야는 LLM이 빠르게 발전함에 따라 혁명을 일으키고 있다. NLP 분야에서 LLMs은 (어느 정도) 범용 언어 과제 해결자의 역할을 할 수 있으며, 연구 패러다임은 LLMs의 활용으로 전환되고 있다. IR 분야에서 기존 검색 엔진은 AI 챗봇(_i.e.,_ChatGPT)을 통한 새로운 정보 탐색 방식으로 도전을 받고 있으며 _New Bing3_는 LLM을 기반으로 검색 결과를 향상시키는 초기 시도를 제시한다. CV 분야에서 연구자들은 멀티모달 대화[42, 43, 44, 45]를 더 잘 제공할 수 있는 ChatGPT 유사 비전 언어 모델을 개발하려고 시도했으며 GPT-4 [46]은 시각 정보를 통합하여 멀티모달 입력을 지원했다. 이 새로운 기술의 물결은 잠재적으로 LLM을 기반으로 하는 실제 응용 프로그램의 번영한 생태계로 이어질 것이다. 예를 들어, Microsoft 365는 사무 업무를 자동화하기 위해 LLM(_i.e.,_ Copilot)에서 권한을 부여받고 있으며, OpenAI는 특수 기능을 구현하기 위해 ChatGPT에서 플러그인 사용을 지원합니다.

각주 3: [https://www.bing.com/new](https://www.bing.com/new)

진행과 영향에도 불구하고 LLM의 기본 원칙은 여전히 잘 탐구되지 않는다. 첫째, 왜 작은 PLM 대신 LLM에서 창발 능력이 발생하는지 불가사의하다. 보다 일반적인 문제로서 LLM의 우수한 능력에 기여하는 핵심 요인에 대한 깊고 상세한 조사가 부족하다. LLM이 언제 어떻게 그러한 능력을 얻는지 연구하는 것이 중요하다[47]. 이 문제에 대해 몇 가지 의미 있는 논의가 있지만 [47, 31] LLM의 _"비밀"_을 밝히기 위해서는 보다 원칙적인 조사가 필요하다. 둘째, 연구 커뮤니티가 능력 있는 LLM을 훈련하는 것은 어렵다. 연산 자원의 엄청난 수요로 인해 LLM을 훈련하기 위한 다양한 전략의 효과를 조사하기 위한 반복적인 연구를 수행하는 것은 매우 비용이 많이 든다. 실제로, LLM은 주로 산업별로 훈련되며, 여기서 많은 중요한 훈련 세부사항(예를 들어, 데이터 수집 및 청소)은 대중에게 공개되지 않는다. 셋째, LLM을 인간의 가치나 선호도와 맞추는 것은 어렵다. 용량에도 불구하고 LLM은 독성, 가상 또는 유해한 내용물을 생성할 가능성이 있다. LLM 사용의 잠재적 위험을 제거하기 위한 효과적이고 효율적인 제어 접근법이 필요하다[46].

기회와 도전 모두에 직면한 LLM의 연구 및 개발에 대한 더 많은 관심이 필요하다. LLM에 대한 기본적인 이해를 제공하기 위해, 이 조사는 LLM이 최근 발전한 4가지 주요 측면, 즉 _사전 훈련_ (유능한 LLM을 사전 훈련하는 방법), _적응_ (더 나은 사용을 위해 사전 훈련된 LLM을 효과적으로 적응하는 방법), _활용_ (다양한 다운스트림 작업을 해결하기 위해 LLM을 사용하는 방법) 및 _능력 평가_ (LLM과 기존 경험적 발견의 능력을 평가하는 방법)에 대한 문헌 검토를 수행한다. 우리는 문헌을 철저히 조사하고 LLM의 주요 발견, 기술 및 방법을 요약한다. 이 설문 조사를 위해 링크 [https://github.com/RUCAlBox/LLMSurvey](https://github.com/RUCAlBox/LLMSurvey)에서 LLMs에 대한 지원 리소스를 수집하여 GitHub 프로젝트 웹 사이트를 만듭니다. 우리는 또한 PLM 또는 LLM에 대한 몇 가지 관련 검토 기사를 알고 있다[32, 36, 38, 39, 43, 48, 49, 50, 51, 52, 53, 54]. 이러한 논문은 PLM 또는 LLM의 일부 특정(또는 일반적인) 측면에 대해 논의한다. 이와 비교하여 LLM을 개발하고 사용하는 기술과 방법에 초점을 맞추고 LLM의 중요한 측면에 대한 비교적 포괄적인 참조를 제공한다.

이 조사의 나머지 부분은 다음과 같이 구성된다. 2절에서는 LLM에 대한 배경과 GPT 시리즈 모델의 진화를 소개하고, 3절에서는 LLM 개발을 위한 가용 자원을 요약한다. 4절, 5절, 6절 및 7절에서는 사전 훈련, 적응, 활용 및 용량 평가의 네 가지 측면에서 최근 진행 상황을 각각 검토하고 요약한다. 그런 다음 섹션 8에서는 신속한 설계를 위한 실용적인 가이드에 대해 논의하고 섹션 9에서는 여러 대표적인 도메인에서 LLM의 적용을 검토한다. 마지막으로 주요 연구 결과를 요약하여 섹션 10의 조사를 마무리하고 향후 작업을 위한 나머지 문제에 대해 논의한다.

## 2 Overview

이 섹션에서는 LLM의 배경에 대한 개요를 제시한 다음 GPT 시리즈 모델의 기술적 진화를 요약한다.

### _Background for LLMs_

일반적으로 _대규모 언어 모델_ (LLM)은 GPT-3 [55], PaLM [56], Galactica [35] 및 LLMA [57]과 같은 대규모 텍스트 데이터 [32]에 대해 훈련된 수천억 개(또는 그 이상)의 매개 변수 4를 포함하는 트랜스포머 언어 모델을 나타냅니다. LLM은 (텍스트 생성을 통해) 자연어를 이해하고 복잡한 작업을 해결하는 강력한 능력을 나타낸다. LLM이 어떻게 작동하는지 빠르게 이해하기 위해 이 부분은 스케일링 법칙, 새로운 능력 및 핵심 기술을 포함한 LLM에 대한 기본 배경을 소개한다.

각주 4: 기존 문헌에서는 모델 용량이 데이터 크기 및 총 계산과도 관련이 있기 때문에 LLMs에 대한 최소 매개변수 척도에 대한 공식적인 합의가 없다. 이 조사에서 우리는 LLMs에 대한 약간 느슨한 정의를 취하며 주로 모델 크기가 10B보다 큰 언어 모델을 논의하는 데 중점을 둔다.

**LLM에 대한 크기 조정 법칙의 공식**. 현재 LLM은 주로 트랜스포머 아키텍처[22]를 기반으로 하며, 여기서 멀티 헤드 어텐션 레이어는 매우 깊은 신경망에 적층된다. 기존의 LLM들은 작은 언어 모델들로서 유사한 트랜스포머 아키텍처들 및 사전 트레이닝 목적들(_e.g._, 언어 모델링)을 채택한다. 그러나 LLM은 모델 크기, 데이터 크기 및 총 계산(확대 순서)을 크게 확장합니다. 광범위한 연구는 스케일링이 LLMs의 모델 용량을 크게 향상시킬 수 있다는 것을 보여주었다[56, 55, 26]. 따라서, 스케일링 효과를 특성화하기 위한 정량적 접근법을 확립하는 것이 유용하다. 다음으로, 트랜스포머 언어 모델에 대한 두 가지 대표적인 스케일링 법칙을 소개한다[30, 34].

\(\bullet\)_KM scaling law5_. 2020년에 Kaplan et al. [30](OpenAI team)은 신경망 모델에 대해 모델 크기(\(N\)), 데이터 세트 크기(\(D\)), 훈련 계산량(\(C\))의 세 가지 주요 요인으로 모델 성능의 멱함수 관계를 모델링하는 것을 처음으로 제안했다. 계산 예산 \(c\)이 주어지면, 그들은 스케일링 법칙6에 대한 세 가지 기본 공식을 경험적으로 제시했다:

각주 5: 원래 논문에는 이 법칙에 따라 훈련된 모델이 없었기 때문에, 우리는 이 스케일링 법칙의 이름을 짓기 위해 두 명의 공동 첫 번째 저자의 성을 취했다.

각주 6: 여기서, \(N_{c}\)\(D_{c}\) 및 \(C_{c}\)는 각각 비 임베딩 파라미터 수, 트레이닝 토큰 수 및 FP-일 수로 측정된다. 원본 논문 [30]에 따르면, \(C_{c}\)와 \(C\)는 계산의 최적 사용에 해당하는 \(C_{c}^{min}\)와 \(C_{min}\로 표시되어야 한다. 우리는 논의의 용이성을 위해 단순화된 표기법을 사용한다.

\[L(N) = \left(\frac{N_{c}}{N}\right)^{\alpha_{N}},\ \ \ \ alpha_{N}\sim 0.076,N_{c}\sim 8.8 \times 10^{13} \tag{1}\] \[L(D) = \left(\frac{D_{c}}{D}\right)^{\alpha_{D}},\ \ \ \ alpha_{D}\sim 0.095,D_{c}\sim 5.4 \times 10^{13}\] \[L(C) = \left(\frac{C_{c}}{C}\right)^{\alpha_{C}},\ \ \ \ alpha_{C}\sim 0.050,C_{c}\sim 3.1 \times 10^{8}\]

여기서 \(L(\cdot)\)은 nats에서 교차 엔트로피 손실을 나타내며 OpenAI의 후속 연구 [58]은 언어 모델링 손실이 _환원성 손실_ (참 데이터 분포의 엔트로피) 및 _환원성 손실_ (참 및 모델 분포 간의 KL 발산 추정치)의 두 부분으로 분해될 수 있음을 보여주었다. 세 가지 법칙은 모델 성능을 다양한 데이터 크기(22M~23B 토큰), 모델 크기(768M~1.5B 비포매 파라미터) 및 훈련 컴퓨팅으로 피팅하여 도출되었으며, 일부 가정(_예:_)에서 한 요인의 분석이 다른 두 요인에 의해 병목되지 않아야 한다. 그들은 모델 성능이 세 가지 요인에 강한 의존 관계를 가지고 있음을 보여주었다.

\(\bullet\)_Chinchilla scaling law_. 또 다른 대표적인 연구로서, Hoffmann et al. [34](the Google DeepMind team)는 LLMs에 대한 컴퓨팅-최적 트레이닝을 지시하기 위해 스케일링 법칙에 대한 대안적인 형태를 제안하였다. 그들은 더 큰 범위의 모델 크기(70M에서 16B) 및 데이터 크기(5B에서 500B 토큰)를 변경하여 엄격한 실험을 수행했으며, 아래 [34]와 같이 다른 계수를 갖으면서도 유사한 스케일링 법칙을 적합시켰다:

\[L(N,D)=E+\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}}, \tag{2}\]

여기서 \(E=1.69,A=406.4,B=410.7\), \(\alpha=0.34\) 및 \(\beta=0.28\). 제약 조건 \(C\approx 6ND\) 하에서 손실 \(L(N,D)\)을 최적화함으로써 모델 크기 및 데이터 크기에 대한 컴퓨팅 예산의 최적 할당이 다음과 같이 도출될 수 있음을 보여주었다.

\[N_{opt}(C)=G\bigg{(}\frac{C}{6}\bigg{)}^{a},\ \ \ D_{opt}(C)=G^{-1}\bigg{(}\frac{C}{6} \bigg{)}^{b}, \tag{3}\]

여기서, \(a=\frac{\alpha}{\alpha+\beta}\), \(b=\frac{\beta}{\alpha+\beta}\) 및 \(G\)는 \(A\), \(B\), \(\alpha\) 및 \(\beta\)에 의해 계산될 수 있는 스케일링 계수이다. [34]에서 분석한 바와 같이 계산 예산의 증가를 고려할 때 KM 스케일링 법칙은 데이터 크기보다 모델 크기에서 더 큰 예산 할당을 선호하는 반면, Chinchilla 스케일링 법칙은 식 (3)에서 \(a\) 및 \(b\)에 대해 유사한 값을 갖는 동일한 크기, 즉 _i.e._로 두 크기를 증가시켜야 한다고 주장한다.

**크기 조정 법률에 대한 논의**. 수식을 도입한 후, 우리는 스케일링 법칙에 대한 이해를 높이기 위해 다음 두 가지 측면에서 스케일링 법칙에 대해 계속 논의한다.

\(\bullet\)_Predictable scaling_. 실제로, 스케일링 법칙은 LLM들의 트레이닝을 지시하기 위해 사용될 수 있고, _예측가능한 스케일링_[46]이라고 불리는 더 작은 모델들의 성능을 기반으로 더 큰 모델들의 성능을 신뢰성 있게 추정하는 것이 실현가능하다는 것이 입증되었다. LLM 훈련을 위한 예측 가능한 스케일링의 이점은 주로 두 가지이다. 첫째, 대형 모델의 경우 다양한 훈련 트릭이나 변형을 엄격하게 조사하는 것이 불가능하며, 소형 모델에서 얻은 경험이 대형 모델에도 적용될 수 있다면 매우 도움이 될 것이다. 예를 들어, 작은 프록시 모델들은 큰 모델들에 대한 데이터 혼합물의 최적 스케줄을 찾기 위해 트레이닝될 수 있다[59]. 둘째, 대규모 모델의 훈련은 훈련 손실 급증과 같은 문제로 인해 많은 시간이 소요되며, 스케일링 법칙을 사용하여 LLM의 훈련 상태를 모니터링하여 조기에 비정상적인 성능을 식별할 수 있다. 스케일링 법칙은 성능 증가(또는 손실 감소)의 부드러운 추세를 특징짓지만 _감소 returns7_이 모델 스케일링으로 발생할 수 있음을 나타냅니다. OpenAI 팀의 경험적 연구 [58]은 수익 체감의 지점(_즉, 환원 불가능한 손실로 접근)에 접근하더라도 표현 품질 또는 의미 콘텐츠가 여전히 효과적으로 개선될 수 있음을 보여주었다[58]. 이 결과는 대규모 모델을 훈련하는 것이 다운스트림 작업의 성능을 향상시키는 데 유망하다는 것을 시사한다. 스케일링 효과를 더 탐구하기 위해 잠재적인 문제는 LLM을 훈련하는 데 사용할 수 있는 데이터의 양이 실제로 제한된다는 것이다. 지속적으로 증가하는 모델 척도로, 공개 텍스트 데이터는 LLM[60]에 대해 곧 "지쳐버릴" 것이다. 따라서 데이터 반복 또는 증대가 데이터 부족을 완화하는데 유용할 수 있는 데이터 제한 레짐 [61]에 스케일링 법칙이 어떻게 적용되는지 연구하는 것은 의미가 있을 것이다.

각주 7: [https://en.wikipedia.org/wiki/Diminishing_returns](https://en.wikipedia.org/wiki/Diminishing_returns)

\(\bullet\)_작업 수준 예측 가능성_입니다. 스케일링 법칙에 대한 기존 연구는 대부분 언어 모델링 손실(_e.g._, nats[30]) 측면에서 수행되는 반면, 실제로는 실제 작업에 대한 LLM의 성능에 더 관심이 있다. 따라서, 기본적인 문제는 언어 모델링 손실의 감소가 작업 수행의 개선으로 어떻게 변환되는지이다[58]. 직관적으로, 언어 모델링 손실이 전체 모델 용량의 일반적인 척도로 간주될 수 있기 때문에, 언어 모델링 손실이 더 작은 모델은 다운스트림 태스크에서 더 나은 성능을 산출하는 경향이 있다. GPT-4 [46]은 일부 능력들(_e.g._, 코딩 능력)이 스케일링 법칙을 통해 정확하게 예측될 수 있다고 보고했다. 그럼에도 불구하고 독자들은 언어 모델링 손실의 직접적인 감소가 항상 다운스트림 태스크에 대한 모델 성능의 개선을 나타내는 것은 아니라는 것을 알아야 한다. 특히, 언어 모델링 손실이 감소함에 따라 태스크 성능이 놀랍게도 악화되는 일부 태스크에 대해 _역 스케일링_ 현상이 발생한다[62]. 전반적으로, 태스크-레벨 스케일링 법칙은 또한 태스크-관련 정보(태스크 메트릭, 태스크 난이도 등)에 의존할 수 있기 때문에, 태스크-레벨 스케일링 법칙을 탐색하고 특성화하는 것이 더 어렵다. 또한, 일부 용량들(_e.g._, 인-컨텍스트 학습[55])은 스케일링 법칙에 따라 예측할 수 없으며, 이는 모델 크기가 특정 레벨(아래에서 논의되는 바와 같이)을 초과할 때만 관찰될 수 있다.

**LLM의 비상 능력**. 문헌 [31]에서 LLM의 _에머젯 능력_ 은 "작은 모델에는 존재하지 않지만 큰 모델에서는 발생하는 능력"으로 공식적으로 정의되며, 이는 LLM을 이전 PLM과 구별하는 가장 두드러진 기능 중 하나이다. 그것은 또한 창발 능력이 발생할 때 주목할 만한 특성을 도입한다[31]: 척도가 일정 수준에 도달할 때 성과는 무작위 이상으로 크게 상승한다. 비유하자면, 그러한 창발적 패턴은 물리학의 _위상 전이_ 현상과 밀접한 관련이 있다[31, 63]. 창발적 능력은 원칙적으로 일부 복잡한 과제[31, 64]와 관련하여 정의될 수 있는 반면, 우리는 다양한 과제를 해결하기 위해 적용될 수 있는 일반적인 능력에 더 관심이 있다. 여기서는 LLM에 대한 세 가지 전형적인 창발 능력과 그러한 능력 8을 가진 대표적인 모델을 간략하게 소개한다.

각주 8: LLMs의 출현 능력에 대한 임계 크기(_i.e._, 능력을 보유하기 위한 최소 크기)는 모델이나 작업에 따라 다를 수 있기 때문에 정확하게 검사하기 어렵다. 또한 기존 연구에서는 특정 LLM에 대해 매우 제한된 모델 크기에 대해 창발 능력을 테스트하는 경우가 많다. 예를 들어, PaLM은 종종 8B, 62B 및 540B의 세 가지 크기로 테스트된다. 테스트되지 않은 크기의 모델 성능에 대해서는 불분명하다.

\(\bullet\)_In-context learning_. 인-컨텍스트 학습(ICL) 능력은 GPT-3에 의해 공식적으로 도입된다[55]: 언어 모델이 자연 언어 명령 및/또는 여러 태스크 데모를 제공받았다고 가정하면, 추가 트레이닝 또는 그래디언트 업데이트9를 요구하지 않고, 입력 텍스트의 워드 시퀀스를 완료함으로써 테스트 인스턴스들에 대한 예상 출력을 생성할 수 있다. GPT-시리즈 모델들 중, 175B GPT-3 모델은 일반적으로 강력한 ICL 능력을 나타냈지만, GPT-1 및 GPT-2 모델은 그렇지 않았다. 이러한 능력은 또한 특정 다운스트림 작업에 달려 있다. 예를 들어, ICL 능력은 13B GPT-3에 대한 산술 과제(_e.g._, 3자리 덧셈 및 뺄셈)에서 나타날 수 있지만, 175B GPT-3은 페르시아 QA 과제에서도 잘 작동할 수 없다[31].

각주 9: 최근 연구[65]에서도 맥락 내 학습이 주의 메커니즘을 통해 암묵적으로 메타 최적화를 수행한다는 것을 보여준다.

\(\bullet\)_Instruction following_. 자연어 설명(_명령 튜닝_이라고 함)을 통해 포맷된 다중 작업 데이터세트의 혼합물로 미세 조정함으로써, LLMs는 명령[28, 66, 67]의 형태로 또한 기술되는 보이지 않는 작업에 대해 잘 수행하는 것으로 보여진다. 명령어 튜닝을 통해, LLM들은 명시적인 예들을 사용하지 않고 새로운 태스크들에 대한 태스크 명령들을 따르도록 인에이블되고, 따라서 향상된 일반화 능력을 갖는다. [67]의 실험에 따르면, 명령어 조정 LaMDA-PT [68]은 모델 크기가 68B에 도달했을 때 보이지 않는 태스크에서 조정되지 않은 태스크를 크게 능가하기 시작했지만 8B 이하의 모델 크기에서는 그렇지 않았다. 최근 연구[69]는 PaLM이 4개의 평가 벤치마크(_i.e._, MMLU, BBH, TyDiQA 및 MGSM)에서 다양한 작업에 대해 잘 수행하려면 62B의 모델 크기가 적어도 필요하다는 것을 발견했지만, 훨씬 작은 크기는 일부 특정 작업(_e.g._, MMLU)에 충분할 수 있다.

\(\bullet\)_단계별 추론_. 작은 언어 모델의 경우 일반적으로 여러 추론 단계, 예를 들어 수학적 단어 문제를 포함하는 복잡한 작업을 해결하는 것이 어렵다. 대조적으로, CoT(Chain-of-thought) 프롬프팅 전략[33]으로, LLMs는 최종 답변을 도출하기 위한 중간 추론 단계들을 수반하는 프롬프팅 메커니즘을 이용함으로써 그러한 태스크들을 해결할 수 있다. 이 능력은 코드에 대한 훈련에 의해 잠재적으로 획득될 것으로 추측된다[33, 47]. 경험적 연구[33]에 따르면 CoT 프롬프트는 모델 크기가 60B보다 큰 PaLM 및 LaMDA 변형에 적용될 때 성능 향상(산술 추론 벤치마크에서)을 가져올 수 있는 반면 표준 프롬프트에 대한 이점은 모델 크기가 100B를 초과할 때 더 분명해진다. 또한 CoT 프롬프트에 의한 성능 향상은 PaLM [33]을 위한 _예:_ GSM8K \(>\) MAWPS \(>\) SWAMP와 같은 작업에 따라 달라지는 것으로 보인다.

**출현 능력이 크기 조정 법과 관련된 방법**. 기존 문헌[30, 31, 34]에서 스케일링 법칙과 창발 능력은 작은 모델보다 큰 모델의 이점을 이해하는 두 가지 관점을 제공한다. 일반적으로 스케일링 법칙(종종 _언어 모델링 손실_로 측정됨)은 수익 체감의 잠재적 효과와 예측 가능한 성능 관계를 설명하는 반면, 창발 능력(종종 _작업 성능_으로 측정됨)은 예측할 수 없지만 그러한 능력이 실제로 나타나면 매우 수익성이 높습니다. 두 관점은 서로 다른 성능 추세(지속적인 개선 _v.s_ 급격한 성능 도약)를 반영하기 때문에 잘못된 결과 또는 관찰로 이어질 수 있습니다. 창발적 능력의 합리성에 대한 광범위한 논쟁도 있다. 대중적인 추측은 창발 능력이 특수 작업에 대한 평가 설정(예: 불연속 평가 메트릭)에 부분적으로 기인할 수 있다는 것이다[70, 71]: 그에 따라 평가 메트릭이 변경되면 창발 능력 곡선의 선명도가 사라질 것이다. 그러나 대부분의 태스크에서 LLM의 성능은 사용자가 불연속적인 방식으로 자연스럽게 인지한다. 예를 들어, 최종 사용자는 테스트 케이스를 성공적으로 통과할 수 있는 LLM에 의해 생성된 신뢰할 수 있는 코드를 선호하지만, 실패한 두 코드 사이의 오류가 적은 더 나은 코드를 선택하는 데 관심이 적다. 보다 최근에, 연구[72]는 태스크 메트릭의 해상도를 확대하여 태스크 성능을 보다 예측 가능하게 할 수 있는 새로운 평가 설정을 제안한다. 이러한 노력에도 불구하고 LLM의 작동 메커니즘에 대한 보다 근본적인 연구(예: grokking10)는 여전히 특정 능력의 출현을 이해해야 한다. 스케일링 법칙과 창발 능력 사이의 미묘한 관계는 인간11의 능력 습득과 유추하여 설명할 수 있다. 말하기 능력을 예로 들 수 있다. 아동에게 언어 발달(특히 유아)은 "출현 능력"이 발생하는 다단계 과정으로 간주될 수도 있다. 특히, 언어 능력은 시간 간격 내에서 비교적 안정적일 것이지만, 질적인 변화는 다른 능력 수준(예를 들어, 단순 단어 말하기에서 단순 문장 말하기)으로 진화할 때만 발생한다. 이러한 학습 과정은 본질적으로 _매끄럽지 않고 _안정적이지 않다(즉, 언어 능력은 시간이 지남에 따라 일정한 속도로 발달하지 않음). 비록 어린이는 실제로 매일 성장하지만 말이다. 어린 부모들이 그들의 아기들이 보여주는 말하기 능력의 예상치 못한 진보에 종종 놀랄 것이라는 것은 흥미롭다.

각주 10: Grokking은 원본 논문 [73]에서 인용한 "데이터의 패턴, 무작위 확률 수준에서 완전 일반화로 일반화 성능을 향상"을 의미한다.

각주 11: 이 설명은 이해의 용이성을 위한 것일 뿐, 두 점을 연결시킬 직접적인 증거는 없다.

**LLM에 대 한 키 기술**. LLM이 현재 상태, 즉 _일반_ 및 _능력_ 학습자로 진화하는 것은 오랜 시간이 걸렸다. 개발 과정에서 LLM의 용량을 크게 향상시키는 중요한 기법들이 많이 제안되고 있다. 여기서는 (잠재적으로) LLM의 성공으로 이어지는 몇 가지 중요한 기술을 간략하게 나열하면 다음과 같다.

\(\bullet\)_Scaling_. 이전 부분들에서 논의된 바와 같이, 트랜스포머 언어 모델들에서 명백한 스케일링 효과가 존재한다: 더 큰 모델/데이터 크기들 및 더 많은 트레이닝 컴퓨트는 전형적으로 개선된 모델 용량으로 이어진다[30, 34]. 두 가지 대표적인 모델로 GPT-3와 PaLM은 모델 크기를 각각 175B와 540B로 증가시켜 스케일링 한계를 탐구했다. 컴퓨팅 예산은 일반적으로 제한적이기 때문에, 스케일링 법칙들은 컴퓨팅 리소스들의 보다 컴퓨팅-효율적인 할당을 수행하기 위해 추가로 채용될 수 있다. 예를 들어, 친칠라는 동일한 컴퓨팅 예산으로 데이터 스케일을 증가시킴으로써 (더 많은 트레이닝 토큰을 갖는) 상대 모델 고퍼(더 큰 모델 크기를 갖는)를 능가한다[34]. 또한 사전 훈련 데이터의 품질이 모델 용량에 중요한 역할을 하기 때문에 데이터 크기 조정은 세심한 세척 과정을 거쳐야 한다.

\(\bullet\)_Training_. 대형 모델 크기 때문에, 유능한 LLM을 성공적으로 훈련시키는 것은 매우 어렵다. 다양한 병렬 전략들이 종종 공동으로 활용되는 LLM들의 네트워크 파라미터들을 학습하기 위해 분산 트레이닝 알고리즘들이 필요하다. 분산 훈련을 지원하기 위해 DeepSpeed[74] 및 Megatron-LM[75, 76, 77]과 같은 병렬 알고리즘의 구현 및 배치를 용이하게 하기 위한 여러 최적화 프레임워크가 출시되었다. 또한, 최적화 트릭은 훈련 안정성 및 모델 성능, _예를 들어, 훈련 손실 스파이크를 극복하기 위한 _재시작_에도 중요하다[56] 및 혼합 정밀도 훈련[78]. 보다 최근에, GPT-4[46]은 훨씬 더 작은 모델들로 큰 모델들의 성능을 신뢰성 있게 예측하는 특별한 인프라 및 최적화 방법들을 개발하는 것을 제안한다.

\(\bullet\)_Ability eliciting_. 대규모 코퍼라에 대해 사전 훈련된 후 LLM은 범용 작업 해결사로서 잠재적인 능력을 부여받는다. 이러한 능력은 LLM이 일부 특정 작업을 수행할 때 명시적으로 표시되지 않을 수 있다. 기술적 접근으로는 그러한 능력을 이끌어내기 위해 적절한 과제 지시나 구체적인 맥락 내 학습 전략을 설계하는 것이 유용하다. 예를 들어, 연쇄적 사고 프롬프트는 중간 추론 단계를 포함함으로써 복잡한 추론 작업을 해결하는 데 유용한 것으로 나타났다. 또한 자연어로 표현된 태스크 설명으로 LLMs에 대한 명령어 튜닝을 수행하여 보이지 않는 태스크에 대한 LLMs의 일반화 가능성을 향상시킬 수 있다. 이러한 유도 기술은 주로 LLM의 출현 능력에 해당하며, 이는 작은 언어 모델에 동일한 효과를 나타내지 않을 수 있다.

\(\bullet\)_Alignment tuning_. LLM은 사전 훈련된 말뭉치(고품질 및 저품질 데이터 모두 포함)의 데이터 특성을 캡처하도록 훈련되기 때문에 인간에게 독성, 편향 또는 심지어 유해한 콘텐츠를 생성할 가능성이 있다. LLM을 인간 가치, 예를 들어 도움이 되는, 정직하고 무해한 것과 정렬해야 한다. 이를 위해, InstructGPT [66]은 LLM들이 예상된 명령들을 따를 수 있게 하는 효과적인 튜닝 접근법을 설계하며, 이는 인간 피드백을 갖는 _강화 학습_ 기법을 활용한다. 그것은 정교하게 설계된 라벨링 전략과 함께 훈련 루프에 인간을 통합한다. ChatGPT는 실제로 InstructGPT와 유사한 기술로 개발되며, 이는 모욕적인 질문에 답하기 위해 거부하는 고품질, 무해한 응답을 생성하는 강력한 정렬 능력을 보여준다.

\(\bullet\)_Tools manipulation_. 본질적으로, LLM들은 방대한 평문 말뭉치에 걸쳐 텍스트 생성기들로서 트레이닝되고, 따라서 텍스트의 형태(_e.g._, 수치 계산)로 가장 잘 표현되지 않는 태스크들에 대해 덜 잘 수행된다. 또한 해당 용량은 최신 정보를 캡처할 수 없는 사전 훈련 데이터 _예:_ 로 제한 됩니다. 이러한 문제를 해결하기 위해 최근에 제안된 기술은 LLM[80, 81]의 결함을 보완하기 위해 외부 도구를 사용하는 것이다. 예를 들어, LLMs는 정확한 계산을 위해 계산기를 활용하고[80] 미지의 정보를 검색하기 위해 검색 엔진을 채용할 수 있다[81]. 보다 최근에 ChatGPT는 LLM의 _"눈 및 as"_와 유추되는 외부 플러그인(기존 또는 새로 만든 앱)12를 사용하는 메커니즘을 활성화했다. 이러한 메커니즘은 LLM에 대한 용량 범위를 광범위하게 확장할 수 있다.

각주 12: [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)

또한, 많은 다른 요인들(_e.g._, 하드웨어의 업그레이드)도 LLM의 성공에 기여한다. 현재, 우리는 논의를 LLM 개발을 위한 주요 기술적 접근법과 주요 발견으로 제한한다.

### _GPT 시리즈 모델의 기술 진화_

ChatGPT는 인간과 소통하는 능력이 뛰어나 출시 이후 AI 커뮤니티의 흥분에 불을 붙였다. ChatGPT는 특히 최적화된 대화 용량을 가진 강력한 GPT 모델을 기반으로 개발됩니다. ChatGPT 및 GPT 모델에 대한 계속 증가하는 관심을 고려하여 GPT 시리즈 모델의 기술적 진화에 대한 특별한 논의를 추가하여 지난 몇 년 동안 어떻게 발전했는지 간략하게 요약한다. 한편, 우리는 그림 4에서 GPT 시리즈 모델의 기술적 진화를 묘사한 개략도를 그렸다. GPT 모델의 기본 원리는 언어 모델링에 의해 세계 지식을 디코더 전용 트랜스포머 모델로 압축하여 세계 지식의 의미를 복구(또는 암기)하고 범용 태스크 해결자 역할을 할 수 있도록 하는 것이다. 성공의 두 가지 핵심 사항은 (I) 다음 단어를 정확하게 예측할 수 있는 (II) 언어 모델의 크기를 스케일링할 수 있는 (I) 훈련 디코더 전용 트랜스포머 언어 모델이다. LLM에 대한 OpenAI의 연구는 전반적으로 다음과 같은 13단계로 대별할 수 있다.

각주 13: 이 부분에 대한 논의는 다소 주관적일 수 있음에 유의한다. 전체적인 관점과 요약은 OpenAI가 발표한 논문, 블로그 기사, 인터뷰 보고서 및 API를 읽고 설문 작성자의 이해를 바탕으로 이루어진다.

**초기 탐색**. Ilya Sutskever14(OpenAI의 공동 설립자이자 수석 과학자)와의 한 인터뷰에 따르면, 언어 모델로 지능 시스템에 접근하는 아이디어는 OpenAI 초기에 이미 탐구된 반면, 순환 신경망(RNN)으로 시도되었다[121]. 트랜스포머의 등장으로 OpenAI는 두 개의 초기 GPT 모델, 즉 GPT-1[122] 및 GPT-2[26]을 개발했으며, 이는 후속적으로 더 강력한 모델 _i.e._, GPT-3 및 GPT-4의 기반으로 간주될 수 있다.

각주 14: [https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai](https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai)

\(\bullet\)_GPT-1_. 2017년, 트랜스포머 모델[22]은 구글에 의해 소개되었고, OpenAI 팀은 그들의 언어 모델링 작업을 이 새로운 신경망 아키텍처에 빠르게 적응시켰다. 그들은 2018년에 첫 번째 GPT 모델인 _i.e._, GPT-1[122]을 출시하고 약어 용어인 _GPT_를 모델 이름으로 만들어 _생성 사전 훈련_에 해당한다. GPT-1은 생성형 디코더 전용 트랜스포머 구조를 기반으로 개발되었으며, 비감독 사전 훈련과 감독 미세 조정의 하이브리드 접근법을 채택했다. GPT-1은 GPT 시리즈 모델에 대한 핵심 아키텍처를 설정하고 다음 단어를 예측하는 자연어 텍스트 _즉_를 모델링하는 기본 원칙을 설정했다.

\(\bullet\)_GPT-2_. GPT-1의 유사한 아키텍처에 따라 GPT-2 [26]은 매개변수 규모를 1.5B로 증가시켰으며, 이는 대규모 웹 페이지 데이터 세트 WebText로 훈련되었다. GPT-2의 논문에서 주장한 바와 같이 라벨링된 데이터를 사용하여 명시적인 미세 조정 없이 비감독 언어 모델링을 통해 작업을 수행하려고 했다. 이 접근법의 동기를 부여하기 위해, 그들은 다중 작업 해결을 위한 확률적 형태를 도입했다. _i.e._, \(p(output|input,task)\)([123]에서 유사한 접근법이 채택되었다. 이러한 조건부 확률을 모델링하기 위해, 언어 텍스트는 입력, 출력 및 태스크 정보를 포맷하는 통일된 방법으로 자연스럽게 채용될 수 있다. 이와 같이, 과제를 해결하는 과정을 풀이 텍스트를 생성하기 위한 단어 예측 문제로 캐스팅할 수 있다. 또한, 그들은 "(작업-특정) 감독된 목적은 비감독(언어 모델링) 목적과 동일하지만 시퀀스의 서브세트에 대해서만 평가되기 때문에, 비감독된 목적의 전역 최소치는 (다양한 작업에 대한) 감독된 목적의 전역 최소치이기도 하다)" [26]15. 이 주장에 대한 기본적인 이해는 각각의 (NLP) 작업이 세계 텍스트의 서브세트에 기초한 단어 예측 문제로서 고려될 수 있다는 것이다. 따라서 비감독 언어 모델링은 세계 텍스트를 복구하는 데 충분한 능력을 갖도록 훈련된다면 다양한 작업을 해결할 수 있다. GPT-2의 이 초창기 논의는 젠슨 황의 일리야 수츠키버 인터뷰에서 메아리쳤다. "신경망이 학습하는 것은 텍스트를 생성하는 과정의 일부 표현이다. 이 텍스트는 사실 세계의 투영이다. 다음 단어를 예측하는 데 더 정확할수록, 충실도가 높을수록, 이 과정에서 더 많은 해상도를 얻는다." 16.

각주 15: 이 문장을 더 잘 이해하기 위해, 우리는 괄호 안에 몇 가지 설명 단어를 넣습니다.

**용량 도약**. GPT-2는 "감독되지 않은 멀티태스크 학습자"로 의도되지만, 전반적으로 감독된 미세 조정 최첨단 방법에 비해 성능이 떨어진다. 상대적으로 작은 모델 크기를 갖기 때문에, 다운스트림 태스크들, 특히 대화 태스크들[124, 125]에서 광범위하게 미세 조정되었다. GPT-2, GPT-3를 기반으로

(거의 동일한) 생성 사전 훈련 아키텍처의 스케일링을 통해 주요 용량 도약을 시연합니다.

\(\bullet\)_GPT-3_. GPT-3 [55]는 2020년에 출시되었으며 모델 매개변수를 175B의 훨씬 더 큰 크기로 조정했다. GPT-3의 논문에는 LLMs을 몇 번의 샷 또는 제로 샷 방식으로 활용하는 인컨텍스트 학습(ICL)17이라는 개념이 공식적으로 소개되어 있다. 전반적으로 GPT-3는 PLM에서 LLM으로 진화하는 여정에서 주목할 만한 랜드마크로 볼 수 있다. 신경망을 상당한 크기로 확장하면 모델 용량이 크게 증가할 수 있음을 경험적으로 입증했다.

각주 17: GPT-2는 그 당시에는 ICL이라고 불리지 않았지만, 감독되지 않은 과제 학습을 위해 본질적으로 ICL을 사용했다.

**용량 향상**. 강력한 용량으로 인해 GPT-3는 훨씬 더 능력 있는 개발을 위한 기본 모델이었다.

도. 4: GPT-시리즈 모델의 기술적 진화에 대한 간략한 예시. 우리는 주로 OpenAI의 논문, 블로그 기사 및 공식 API를 기반으로 이 수치를 플로팅한다. 여기서, _실선_은 두 모델 사이의 진화 경로 상에 명시적 증거(예를 들어, 기본 모델을 기반으로 새로운 모델이 개발된다는 공식 진술)가 있음을 나타내는 반면, _점선_은 상대적으로 약한 진화 관계를 나타낸다.

도. 3: 최근 몇 년간 (10B보다 큰 크기를 갖는) 기존의 대형 언어 모델들의 타임라인. 타임라인은 주로 모델에 대한 기술 논문의 출시 날짜(_예:_ arXiv로의 제출 날짜)에 따라 설정되었다. 해당 논문이 없는 경우 모델의 날짜를 공개 또는 발표의 가장 빠른 시간으로 설정했습니다. LLM에 공개적으로 사용 가능한 모델 체크포인트를 노란색으로 표시합니다. 그림의 공간 한계로 인해 공개적으로 보고된 평가 결과가 있는 LLM만 포함한다.

LMs for OpenAI. 전반적으로 OpenAI는 GPT-3 모델을 더욱 개선하기 위한 두 가지 주요 접근법, 즉 코드 데이터에 대한 훈련 및 인간 선호도와의 정렬을 탐구했으며 다음과 같이 자세히 설명되어 있다.

\(\bullet\)_코드 데이터에 대한 훈련._ 원래 GPT-3 모델의 주요 한계는 복잡한 과제, 예를 들어 코드를 완성하고 수학 문제를 해결하는 추론 능력이 부족하다는 것이다. 이러한 능력을 향상시키기 위해 코덱스[105]는 2021년 7월 OpenAI에 의해 도입되었으며, 이는 GitHub 코드의 대규모 코퍼스에서 미세 조정된 GPT 모델이었다. 그것은 코덱스가 매우 어려운 프로그래밍 문제를 해결할 수 있고, 또한 수학 문제를 해결하는 데 있어 상당한 성능 향상을 가져올 수 있다는 것을 보여주었다[126]. 또한, 훈련 텍스트 및 코드 임베딩에 대한 대조적 접근[127]이 2022년 1월에 보고되었으며, 이는 일련의 관련 작업(_i.e._, 선형 프로브 분류, 텍스트 검색 및 코드 검색)을 개선하는 것으로 나타났다. 실제로 GPT-3.5 모델은 코드 기반 GPT 모델(_i.e._, code-davinci-002)을 기반으로 개발되었으며, 이는 코드 데이터에 대한 훈련이 GPT 모델의 모델 용량, 특히 추론 능력을 향상시키는 데 매우 유용한 실습임을 나타낸다. 또한, 코드 데이터에 대한 교육이 LLM[47]의 연쇄적 사고 촉진 능력을 크게 증가시킬 수 있다는 추측도 있지만, 더 철저한 검증으로 추가 조사가 필요하다.

\(\bullet\)_인간 정렬_. 인간 정렬에 대한 관련 연구는 OpenAI에 대한 2017년(또는 이전)으로 거슬러 올라갈 수 있다: "인간 선호도로부터 학습" 18이라는 제목의 블로그 기사는 인간이 주석한 _선호도 비교_로부터 학습하기 위해 강화 학습(RL)을 적용한 작업을 설명하는 OpenAI 블로그에 게시되었다[79](도 12의 InstructGPT의 정렬 알고리즘에서 _보상 훈련_ 단계와 유사하다). 이 RL 논문[79]이 발표된 직후, 2017년 7월에 Proximal Policy Optimization(PPO)[128]의 논문이 발표되었는데, 이는 현재 인간의 선호도로부터 학습하는 기초 RL 알고리즘이다[66]. 이후 2020년 1월, GPT-2는 앞서 언급한 RL 알고리즘[79, 128]을 사용하여 미세 조정되었으며, 이는 NLP 작업에서 GPT-2의 용량을 개선하기 위해 인간의 선호도를 활용했다. 같은 해에, 다른 작업[129]은 유사한 방식으로 인간의 선호도를 최적화하기 위한 요약 모델을 훈련시켰다. 이러한 선행 연구를 바탕으로 2022년 1월 인간 정렬을 위한 GPT-3 모델을 개선하기 위해 InstructGPT [66]이 제안되었으며, 이는 공식적으로 인간 피드백으로부터 3단계 _강화 학습(RLHF)_ 알고리즘을 확립했다. "_명령 조정_"의 작업은 OpenAI의 논문 및 문서에서 거의 사용되지 않은 것으로 보이며, 이는 인간의 시연(_즉, RLHF 알고리즘 [66])의 첫 번째 단계인 _감독 미세 조정_으로 대체된다. 명령 후속 용량을 개선하는 것 외에도, RLHF 알고리즘은 LLMs에 대한 위해 또는 독성 콘텐츠를 생성하는 문제를 완화하는 데 특히 유용하며, 이는 실제로 LLMs의 안전한 배치에 핵심이다. OpenAI는 기술 기사 [130]에서 정렬 연구에 대한 그들의 접근 방식을 설명하며, 이는 "인간 피드백을 사용하고, 인간 평가를 돕고, 정렬 연구를 수행하도록 AI 시스템을 훈련"하는 세 가지 유망한 방향을 요약했다.

각주 18: [https://openai.com/research/learning-from-human-preferences](https://openai.com/research/learning-from-human-preferences)

이러한 향상 기술은 더 강한 용량을 가진 개선된 GPT-3 모델로 이어지며, 이를 OpenAI에 의한 GPT-3.5 모델이라고 한다(섹션 3.1의 OpenAI API에 대한 논의를 참조).

**언어 모델의 마일스톤**. 모든 탐사 노력을 바탕으로 OpenAI에 의해 달성된 두 가지 주요 이정표, 즉 ChatGPT[131]와 GPT-4[46]는 기존 AI 시스템의 용량 기준을 크게 높였다.

\(\bullet\)_ChatGPT_. 2022년 11월 OpenAI는 GPT 모델(GPT-3.5, GPT-4)을 기반으로 대화 모델 ChatGPT를 출시했다. 공식 블로그 기사 [131]에서 소개된 바와 같이, ChatGPT는 InstructGPT(원래 게시물에서 "InstructGPT에 대한 형제 모델"로 불림)와 유사한 방식으로 훈련되는 한편, 특히 대화에 최적화되었다. 그들은 데이터 수집 설정에서 ChatGPT와 InstructGPT의 훈련 간의 차이를 보고했다: 인간 생성 대화(사용자 및 AI의 역할 모두 수행)는 ChatGPT 훈련을 위한 대화 형식으로 InstructGPT 데이터 세트와 결합된다. ChatGPT는 방대한 지식의 저장, 수학적 문제에 대한 추론 능력, 멀티턴 대화에서 정확한 맥락 추적, 안전한 사용을 위해 인간의 가치와 잘 일치한다는 점에서 인간과 소통하는 데 탁월한 능력을 보였다. 이후, 플러그인 메커니즘은 ChatGPT에서 지원되었으며, 이는 기존 도구 또는 앱으로 ChatGPT의 용량을 더욱 확장한다. 지금까지 그것은 AI 역사상 가장 강력한 챗봇인 것 같다. ChatGPT의 출시는 향후 AI 연구에 큰 영향을 미치며, 이는 인간과 유사한 AI 시스템 탐구를 조명한다.

\(\bullet\)_GPT-4_. 또 다른 괄목할 만한 진전으로 2023년 3월 GPT-4[46]이 출시되어 텍스트 입력을 멀티모달 신호로 확장하였다. 전반적으로 GPT-4는 GPT-3.5보다 복잡한 과제 해결 능력이 강해 많은 평가 과제에서 큰 성능 향상을 보였다. 최근 연구[41]는 다양한 난제 범위에 걸쳐 인간 생성 문제에 대한 정성적 테스트를 수행하여 GPT-4의 용량을 조사했으며 GPT-4가 ChatGPT와 같은 이전 GPT 모델보다 더 우수한 성능을 달성할 수 있음을 보여주었다. 또한, GPT-4는 6개월 반복 정렬(RLHF 훈련에서 추가 안전 보상 신호와 함께)로 인해 악의적이거나 자극적인 쿼리에 더 안전하게 응답한다. 기술 보고서에서 OpenAI는 GPT-4를 안전하게 개발하는 방법을 강조하고 환각, 개인 정보 보호 및 과잉 의존과 같은 LLM의 가능한 문제를 완화하기 위해 여러 개입 전략을 적용했다. 예를 들어, 그들은 위해 또는 독성 콘텐츠 생성을 줄이기 위해 _red teaming_[132]라는 메커니즘을 도입했다. 또 다른 중요한 측면으로서, GPT-4는 개선된 최적화 방법을 갖는 잘 확립된 딥 러닝 인프라 상에서 개발되었다. 그들은 모델 훈련 동안 적은 비율의 계산으로 최종 성능을 정확하게 예측할 수 있는 _예측 가능한 스케일링_이라는 새로운 메커니즘을 도입했다.

\(\bullet\)_GPT-4V, GPT-4 turbo, beyond_. OpenAI는 GPT-4를 위해 수행된 작업을 기반으로 2023년 9월에 GPT-4V를 추가로 출시했으며, 이는 GPT-4의 비전 능력을 안전하게 배치하는 데 중점을 두었다. GPT-4V의 시스템 카드[133]에서는 시각적으로 증강된 입력과 관련된 위험의 평가 및 완화에 대해 광범위하게 논의했다. 특히 GPT-4V는 다양한 응용 시나리오에서 강력한 비전 능력을 보여 강력한 멀티모달 학습 시스템으로서의 큰 잠재력을 보여주었다. 보다 최근에 오픈AI는 2023년 11월 데브데이에서 일련의 기술적 개선과 함께 _GPT-4 터보_라는 이름의 업그레이드된 세대의 GPT-4 모델을 출시했다. GPT-4 Turbo는 개선된 모델 용량(GPT-4보다 더 가능), 확장된 지식 소스(최대 2023년 4월), 긴 컨텍스트 윈도우(최대 128k 토큰), 최적화된 모델 성능(저렴한 가격), 및 기타 유용한 기능 업데이트(기능 호출, 재현 가능한 출력 등)에 의해 특징지어지며, 동시에 에이전트 유사 어시스턴트의 신속한 개발을 용이하게 하기 위해 어시스턴트 API를 개시하였다. 이 API를 사용하면 개발자는 특정 명령, 추가 지식 및 도구 사용을 활용하여 애플리케이션 내에서 목표 지향 비서를 쉽게 만들 수 있습니다. 또한, GPT-4 Turbo with vision, DALL-E 3, Text-to-speech (TTS) 및 Listen to voice 샘플에 의해 지원되는 이 새로운 릴리스에서 멀티모달 용량(참조, 듣기 및 말하기)도 향상되었습니다. 이러한 개선은 용량 범위를 크게 확장하고 GPT 모델의 작업 성능을 향상시켰다. 더 중요한 것은 개선된 모델, API 및 기능에서 기술 업그레이드로 애플리케이션 생태계가 크게 강화된다는 것입니다.

엄청난 진전에도 불구하고, 이러한 우수한 LLM에는 여전히 한계가 있으며, 예를 들어 일부 특정 컨텍스트 내에서 사실적 오류 또는 잠재적으로 위험한 반응을 갖는 환각을 생성한다[46]. LLM의 더 많은 제한 사항 또는 문제는 섹션 7에서 논의될 것이다. 더 유능하고 안전한 LLM을 개발하기 위해 오랜 연구 과제를 제기한다. 공학적인 관점에서 OpenAI는 모델 사용의 잠재적 위험을 효과적으로 줄이는 것을 목표로 하는 5단계 개발 및 배포 수명 주기에 따라 모델 및 제품을 개발하는 반복적인 배포 전략[134]을 채택했다. 아래에서는 어떻게 개발되었는지 구체적으로 이해하기 위해 기술 세부 사항에 대해 자세히 알아보겠습니다.

## 3 Resources of LLMs

어려운 기술적 문제와 막대한 계산 자원 수요를 고려할 때 LLM을 개발하거나 재현하는 것은 결코 쉬운 일이 아니다. 실행 가능한 방법은 기존 LLM에서 경험을 배우고 증분 개발 또는 실험 연구를 위해 공개적으로 사용 가능한 리소스를 재사용하는 것이다. 이 섹션에서는 모델 체크포인트(또는 API), 말뭉치 및 라이브러리를 포함하여 LLM 개발을 위해 공개적으로 사용할 수 있는 리소스를 간략하게 요약한다.

### _Public_ Available Model Checkpoints or APIs_

모델 사전 훈련의 막대한 비용을 감안할 때 잘 훈련된 모델 체크포인트는 연구 커뮤니티를 위한 LLM의 연구 및 개발에 매우 중요하다. 매개변수 척도는 LLM을 사용하기 위해 고려해야 할 핵심 요소이기 때문에, 우리는 이러한 공개 모델을 두 가지 척도 수준(즉, 수백억 개의 매개 변수_와 수천억 개의 매개 변수_)으로 분류하며, 이는 사용자가 자원 예산에 따라 적합한 자원을 식별하는 데 유용하다. 또한 추론을 위해 모델을 로컬로 실행하지 않고 공용 API를 직접 사용하여 작업을 수행할 수 있습니다. 다음으로 공개적으로 사용 가능한 모델 체크포인트와 API를 소개합니다.

**수십억 개의 매개 변수가 있는 모델**. 이 범주의 대부분의 모델은 LLaMA[57] 및 LLaMA2[99](가장 큰 버전에서 70B 매개변수 포함), NLLB[91](가장 큰 버전에서 54.5B 매개변수 포함) 및 팔콘[135](가장 큰 버전에서 40B 매개변수 포함)를 제외하고 10B에서 20B 범위의 매개변수 규모를 가지고 있다. 이 범위 내의 다른 모델로는 mT5[83], PanGu-\(\alpha\)[84], T0[28], GPT-NeoX-20B[87], CodeGen[86], UL2[89], Flan-T5[69], mT0[94] 등이 있다. 그 중 Flan-T5(11B version)는 명령어 튜닝을 세 가지 측면에서 탐구하기 때문에 명령어 튜닝을 위한 선행 모델 역할을 할 수 있다[69]: 태스크 수 증가, 모델 크기 스케일링, 연쇄적 사고 프롬프트 데이터로 미세 조정. 또한 코드 생성을 위해 설계된 자기회귀 언어 모델인 CodeGen(11B 버전)은 코드 생성 능력을 탐구하는 데 좋은 후보로 간주될 수 있다. 또한 115개의 전문가 생성 문제로 구성된 멀티턴 프로그램 합성을 위한 새로운 벤치마크 MTPB [86]을 소개한다. 이러한 문제를 해결하기 위해서는 LLM이 충분한 프로그래밍 지식(예: 수학, 배열 연산 및 알고리즘)을 습득해야 한다. 보다 최근에는 모델 아키텍처, 학습 알고리즘 및 데이터 분포의 선택이 모델에 미치는 영향을 탐구하기 위해 CodeGen2 [97]이 출시되었다. 코딩 능력에 특화된 또 다른 LLM으로서 스타코더[98]도 우수한 성과를 거두었다. 다국어 작업의 경우 mT0(13B 버전)이 좋은 후보 모델일 수 있으며, 이는 다국어 프롬프트를 사용하여 다국어 작업에 대해 미세 조정되었다. 또한 PanGu-\(\alpha\)[84]는 딥 러닝 프레임워크 MindSpore[136]를 기반으로 개발된 제로 샷 또는 소수 샷 설정에서 중국어 다운스트림 태스크에서 좋은 성능을 보인다. PanGu-\(\alpha\)[84]는 여러 버전의 모델(최대 200B 매개 변수)을 보유하는 반면, 가장 큰 공개 버전에는 13B 매개 변수가 있습니다. 대중적인 LLM으로서, 다른 모델보다 약 5배 많은 파라미터를 포함하는 LLaMA(65B 버전) [57]은 다음 명령과 관련된 태스크에서 우수한 성능을 나타냈다. LLaMA에 비해 LLaMA2 [99]는 인간 피드백(RLHF)으로부터 강화 학습에 대한 더 많은 탐색을 수행하고 _LLaMA-chat_라는 채팅 지향 버전을 개발했으며, 이는 일반적으로 다양한 유용성과 안전성 벤치마크에서 기존 오픈 소스 모델을 능가한다. 개방성과 효과성으로 인해 LLaMA는 연구 커뮤니티의 상당한 관심을 끌었으며 새로운 모델 또는 도구를 구현하기 위해 다양한 모델 버전을 미세 조정하거나 지속적으로 사전 훈련하는 데 많은 노력[137, 138, 139, 140]을 기울였다. 보다 최근에는 또 다른 오픈 소스 LLM인 팔콘[135]도 오픈 벤치마크에서 매우 우수한 성능을 달성했다. 사전 훈련 데이터(공개 공유 데이터 세트 _RefinedWeb [141]_ 포함)를 준비하기 위한 보다 신중한 데이터 클리닝 프로세스가 특징입니다. 일반적으로 이 규모의 사전 훈련 모델은 수백 또는 수천 개의 GPU 또는 TPU를 필요로 한다. 예를 들어 GPT-NeoX-20B는 각각 8개의 NVIDIA A100-SXM4-40GB GPU가 장착된 12개의 슈퍼마이크로 서버를 사용하는 반면 LLaMA는 원래 간행물에 보고된 대로 2,048개의 A100-80G GPU를 사용한다. 필요한 계산 자원을 정확하게 추정하기 위해 _FLOPS_ (_즉, 초당 FLoating point number Operations) [30]과 같은 관련된 계산 횟수를 측정하는 메트릭을 사용하는 것이 제안된다.

**수백억 개의 매개 변수가 있는 모델**. 이 범주에 속하는 모델의 경우 소수의 모델만 공개적으로 출시되었다. 예를 들어, OPT[90], OPT-IML[95], BLOOM[78], 및 BLOOMZ[94]는 GPT-3(175B 버전)과 거의 동일한 수의 파라미터를 갖는 반면, GLM[93] 및 Galactica[35]는 각각 130B 및 120B 파라미터를 갖는다. 이 중 OPT(175B version)는 명령어 조정 버전인 OPT-IML과 함께 오픈 셰어링에 특별한 동기를 부여했으며, 이는 연구자들이 대규모로 재현 가능한 연구를 수행할 수 있도록 하는 것을 목표로 한다. 언어 간 일반화 연구를 위해 다국어 언어 모델링 작업의 능력으로 인해 BLOOM(176B 버전)과 BLOOMZ(176B 버전)를 기본 모델로 사용할 수 있다. 이중 언어 LLM으로서, GLM은 또한 인기 있는 작은 크기의 중국 채팅 모델 ChatGLM2-6B(ChatGLM-6B에 대한 업데이트된 버전)를 제공했는데, 이는 효율 및 용량(_e.g._, 양자화, 32K-길이 컨텍스트, 빠른 추론 속도)에서 많은 개선을 특징으로 한다. 이 규모의 모델은 일반적으로 수천 개의 GPU 또는 TPU를 훈련시켜야 합니다. 예를 들어 OPT(175B 버전)는 992개의 A100-80GB GPU를 사용했으며 GLM(130B 버전)은 96개의 NVIDIA DGX-A100(8x40G) GPU 노드의 클러스터를 사용했다.

**LLaMA 모델 패밀리**. LLaMA 모델 모음[57]은 2023년 2월 메타 AI에 의해 도입되었으며, 4가지 크기(7B, 13B, 30B 및 65B)로 구성되었다. 공개된 이후 LLaMA는 연구와 산업 커뮤니티 모두에서 광범위한 관심을 끌었다. LLaMA 모델은 지금까지 가장 인기 있는 개방형 언어 모델이 된 다양한 개방형 벤치마크에서 매우 우수한 성능을 달성했다. 많은 연구자들이 LLaMA 모델을 명령어 튜닝 또는 지속적인 사전 훈련으로 확장했다. 특히, 명령어 튜닝 LLaMA는 상대적으로 낮은 계산 비용으로 인해 맞춤형 또는 전문화된 모델을 개발하는 주요 접근법이 되었다. LLaMA 모델을 비영어권에서 효과적으로 적용하기 위해서는 원어휘(영어 코퍼스 위주로 학습)를 확장하거나 목표 언어의 명령어나 데이터로 미세 조정해야 하는 경우가 많다. 이러한 확장 모델 중 LLaMA(7B)를 기반으로 미세 조정된 최초의 개방형 명령어 추종 모델은 Stanford Alpaca[142]이다. 텍스트-다빈치-003을 사용하여 자체 명령 [143]을 통해 생성된 52K 명령어 후속 데모에 의해 훈련된다. _Alpaca-52K_라는 명령어 데이터와 훈련 코드는 Alpaca-LoRA [144](LoRA를 사용한 Stanford Alpaca의 복제본 [145], Koala [146], BELLE [147)와 같은 후속 작업에서 광범위하게 채택되었다. 또한, Vicuna[138]은 ShareGPT[148]로부터 수집된 사용자 공유 대화들에 따라 트레이닝된, 또 다른 인기 있는 LLaMA 변형이다. LLaMA 모델 패밀리의 뛰어난 성능과 가용성으로 인해 많은 멀티모달 모델이 기본 언어 모델로 통합되어 강력한 언어 이해 및 생성 능력을 달성한다. 다른 변형과 비교하여 비쿠나는 멀티모달에 더 선호된다.

도. 5: LLaMA에 대해 수행된 연구 작업의 진화 그래프. 엄청난 숫자로 인해, 우리는 이 그림에 모든 LLaMA 변형을 포함할 수 없으며, 심지어 훨씬 우수한 작업도 포함할 수 없다. 증분 업데이트를 지원 하기 위해이 그림의 원본 파일을 공유 하 고 GitHub 페이지에 끌어오기 요청을 제출 하 여 독자가 원하는 모델을 포함 하는 것을 환영합니다.

언어 모델은 LLaVA[149], MiniGPT-4[150], InstructBLIP[151], PandaGPT[152] 등 다양한 인기 모델이 등장하게 되었다. LLMA의 출시는 LLM의 연구 진전을 크게 발전시켰다. LLaMA에 대해 수행된 연구 작업을 요약하기 위해 그림 5에 간략한 진화 그래프를 제시한다.

**LLM의 공용 API**. 모델 복사본을 직접 사용하는 대신 API를 사용하면 모델을 로컬로 실행할 필요 없이 일반 사용자가 LLM을 사용할 수 있는 더 편리한 방법을 제공합니다. LLM을 사용하기 위한 대표적인 인터페이스로서, GPT 시리즈 모델들에 대한 API들[46, 55, 66, 105]은 학계와 산업계 모두에서 널리 사용되고 있다. OpenAI는 GPT-3 시리즈 모델들에 대해 ada, babbage, curie, davinci(GPT-3 시리즈에서 가장 강력한 버전), text-ada-001, text-babbbage-001, text-curie-001의 7가지 주요 인터페이스들을 제공하고 있으며, 이들 중 처음 4가지 인터페이스들은 OpenAI의 호스트 서버에서 더욱 미세 조정될 수 있다. 특히, 바비지, 큐리 및 다빈치는 각각 GPT-3(1B), GPT-3(6.7B) 및 GPT-3(175B) 모델에 대응한다[55]. 또한 code-cushman-001(Codex의 강력하고 다국어 버전(12B [105]) 및 code-davinci-002라고 하는 Codex [105]와 관련된 두 개의 API도 있다. 또한 GPT-3.5 시리즈는 하나의 기본 모델 코드-davinci-002 및 세 개의 향상된 버전, 즉 text-davinci-002, text-davinci-003 및 gpt-3.5-turbo를 포함한다. 보다 강력한 대안으로 올해 OpenAI는 GPT-4 시리즈를 위한 모델 인터페이스인 gpt-4, gpt-4-32k, gpt-4-1106-preview(_i.e._, GPT-4 Turbo)와 gpt-4-vision-preview(_i.e._, GPT-4 Turbo with vision, multimodal 모델)를 출시했다. OpenAI가 이러한 모델 인터페이스(gpt-3.5-turbo, gpt-4, gpt-4-32k)를 유지 및 업그레이드해왔기 때문에 API 이름이 실제로 최신 버전을 가리킬 것이라는 점에 주목할 필요가 있다. 현재 ChatGPT는 GPT-3.5 또는 GPT-4 모델로 구동될 수 있다. 전반적으로 특정 애플리케이션 시나리오 및 응답 요구 사항에 따라 적합한 모델 인터페이스를 선택합니다. 자세한 사용법은 프로젝트 웹사이트 20에서 확인할 수 있습니다.

각주 19: [https://platform.openai.com/docs/api-reference/introduction](https://platform.openai.com/docs/api-reference/introduction)

각주 20: [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview)

### _Commonly Used Corpora for Pre-training_

이전 PLM과 달리 훨씬 더 많은 수의 매개변수로 구성된 LLM은 광범위한 콘텐츠를 다루는 더 많은 양의 훈련 데이터를 필요로 한다. 이러한 필요를 위해 연구를 위해 공개된 접근 가능한 훈련 데이터 세트가 점점 더 많아지고 있다. 이 섹션에서는 LLM을 훈련하기 위해 널리 사용되는 몇 가지 말뭉치를 간략하게 요약할 것이다. 콘텐츠 유형에 따라 이 말뭉치를 책, 커먼크롤, 레딧 링크, 위키피디아, 코드 및 기타의 6개 그룹으로 분류한다.

**책.** BookCorpus [153]은 광범위한 주제 및 장르(예: 소설 및 전기)를 다루는 11,000권 이상의 책으로 구성된 이전 소규모 모델(예: GPT [122] 및 GPT-2 [26])에서 일반적으로 사용되는 데이터 세트입니다. 또 다른 대규모의 책 말뭉치는 프로젝트 구텐베르크[154]로, 소설, 수필, 시, 연극, 역사, 과학, 철학, 그리고 공공 영역의 다른 유형의 작품들을 포함한 7만 권 이상의 문학 서적들로 구성되어 있다. 그것은 현재 MT-NLG[113]와 LLaMA[57]의 훈련에 사용되는 가장 큰 오픈 소스 도서 컬렉션 중 하나이다. GPT-3 [55]에서 사용되는 Books1 [55]와 Books2 [55]의 경우 BookCorpus보다 훨씬 크지만 지금까지 공개되지 않았다.

**CommonCrawl.** CommonCrawl [163]은 기존 LLM의 학습 데이터로 널리 사용되는 페타바이트 규모의 데이터 볼륨을 포함하는 가장 큰 오픈 소스 웹 크롤링 데이터베이스 중 하나입니다. 전체 데이터세트가 매우 크기 때문에 기존 연구들은 주로 특정 기간 내에 웹 페이지의 하위 집합을 추출한다. 그러나 웹 데이터에 잡음이 많고 품질이 낮은 정보가 널리 존재하기 때문에 사용 전에 데이터 전처리를 수행해야 한다. CommonCrawl을 기반으로 기존 작업에서 일반적으로 사용되는 필터링된 데이터 세트는 C4 [82], CC-Stories [155], CC-News [27], RealNews [156]의 네 가지입니다. 거대 클린 크롤드 코퍼스(C4)는 en(806G), en.noclean(6T), realnewslike(36G), web-textlike(17G) 및 다국어(38T)의 5가지 변형 21을 포함한다. _en_ 버전은 사전 훈련 T5 [82], LaMDA [68], Go-pher [64] 및 UL2 [89]에 사용되었습니다. mC4라고도 불리는 다국어 C4는 mT5[83]에서 사용되었다. CC-Stories(31G)는 CommonCrawl 데이터의 부분집합으로 구성되어 있으며, 이 부분집합에서 내용이 이야기와 같은 방식으로 만들어진다. CC-Stories의 원본 원본을 사용할 수 없기 때문에 표 II에 복제 버전 _CC-Stories-R_[164]를 포함합니다. 또한 CommonCrawl에서 추출된 두 개의 뉴스 코퍼라, 즉 _i.e._, REALNEWS(120G) 및 CC-News(76G)도 사전 학습 데이터로 사용된다.

각주 21: [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4)

**리딧 링크** 리딧은 사용자가 링크 및 텍스트 게시물을 제출할 수 있도록 하는 소셜 미디어 플랫폼으로, "상표" 또는 "하표"를 통해 다른 사람이 투표할 수 있습니다. 높은 지지를 받은 게시물은 종종 유용한 것으로 간주되며 고품질 데이터 세트를 만드는 데 활용할 수 있다. WebText[26]은 Reddit에서 많이 인용된 링크로 구성된 잘 알려진 말뭉치이지만 공개적으로 사용할 수 없다. 대리인으로서 OpenWebText[157]이라는 쉽게 접근할 수 있는 오픈 소스 대안이 있다. Reddit에서 추출한 또 다른 코퍼스는 생성일 이후 Reddit의 과거 데이터로 구성된 실시간 업데이트 데이터 세트인 PushShift.io [158]이다. Pushshift는 월별 데이터 덤프뿐만 아니라 사용자가 검색, 요약 및 수행할 수 있도록 지원하는 유용한 유틸리티 도구도 제공합니다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Corpora** & **Size** & **Source** & **Latest Update Time** \\ \hline BookCorpus [153] & 5GB & Books & Dec-2015 \\ Gutenberg [154] & - & Books & Dec-2021 \\ Cat [82] & 800GB & CommonCrawl & Apr-2019 \\ CC-Stories-R [155] & 31GB & CommonCrawl & Sep-2019 \\ CC-NEWS [27] & 78GB & CommonCrawl & Feb-2019 \\ REALNEWS [156] & 120GB & CommonCrawl & Apr-2019 \\ OpenWebText [157] & 38GB & Reddit links & Mar-2023 \\ Pushshift.io [158] & 2TB & Reddit links & Mar-2023 \\ Wikipedia [159] & 21GB & Wikipedia & Mar-2023 \\ BigQuery [160] & - & Codes & Mar-2023 \\ the Pile [161] & 800GB & Other & Dec-2020 \\ ROOTS [162] & 1.6TB & Other & Jun-2022 \\ \hline \hline \end{tabular}
\end{table} TABLE II: Statistics of commonly-used data sources.

전체 데이터 세트에 대한 예비 조사입니다. 이를 통해 사용자가 레딧 데이터를 쉽게 수집하고 처리할 수 있습니다.

**위키피디아.** 위키피디아 [159]는 다양한 주제에 대한 고품질 기사를 대량으로 포함하는 온라인 백과사전입니다. 이 기사의 대부분은 광범위한 언어와 분야를 포괄하는 (지원 참조와 함께) 설명서 스타일의 글로 구성되어 있다. 전형적으로, 위키피디아의 영어 전용 필터링된 버전들은 대부분의 LLM들(_e.g._, GPT-3[55], LaMDA[68], 및 LLaMA[57])에서 널리 사용된다. 위키피디아는 여러 언어로 사용할 수 있으므로 다국어 설정에서 사용할 수 있습니다.

**코드.** 코드 데이터를 수집 하기 위해 기존 작업은 주로 인터넷에서 오픈 소스 라이선스 코드를 크롤링 합니다. 두 가지 주요 출처는 오픈 소스 라이선스(_e.g._, GitHub) 및 코드 관련 질문 답변 플랫폼(_e.g._, StackOverflow)에 따른 공개 코드 리포지토리입니다. 구글은 다양한 프로그래밍 언어로 상당수의 오픈소스 라이선스 코드 스니핏이 포함된 빅쿼리 데이터셋[160]을 공개해 대표적인 코드 데이터셋 역할을 하고 있다. CodeGen은 CodeGen(CodeGen-Multi)의 다국어 버전을 학습하기 위해 BigQuery 데이터셋의 하위 집합인 BIGQUERY [86]을 활용했다.

**기타.** 파일 [161]은 책, 웹 사이트, 코드, 과학 논문 및 소셜 미디어 플랫폼을 포함한 여러 소스의 800GB 이상의 데이터로 구성된 대규모, 다양하고 오픈 소스 텍스트 데이터 세트입니다. 22개의 다양한 고품질 하위 집합으로 구성됩니다. 파일 데이터 세트는 GPT-J(6B)[165], CodeGen(16B)[86], Megatron-Turing NLG(530B)[113]와 같이 서로 다른 매개변수 척도를 가진 모델에서 널리 사용된다. ROOTS[162]는 다양한 더 작은 데이터 세트(총 1.61 TB의 텍스트)로 구성되며, 59개의 다른 언어(자연 언어 및 프로그래밍 언어를 포함)를 커버하며, 이는 BLOOM[78]을 트레이닝하기 위해 사용되어 왔다.

실제로, 그것은 일반적으로 단일 코퍼스 대신에 LLM들을 사전 트레이닝하기 위해 상이한 데이터 소스들의 혼합을 필요로 한다(도 6 참조). 따라서 기존 연구들은 일반적으로 여러 기성 데이터 세트(_e.g._, C4, OpenWebText, The Pile)를 혼합한 후 추가 처리를 수행하여 사전 학습 코퍼스를 획득한다. 또한, 특정 응용에 적응적인 LLM을 학습하기 위해서는 사전 학습 데이터에서 해당 정보를 풍부하게 하기 위해 관련 소스(예:_e.g._, Wikipedia 및 BigQuery)에서 데이터를 추출하는 것도 중요하다. 기존 LLMs에서 사용되는 데이터 소스를 빠르게 참조하기 위해 세 가지 대표적인 LLMs의 사전 훈련 코퍼스를 제시한다:

CommonCrawl [163], WebText2 [55], Books1 [55], Books2 [55] 및 Wikipedia [159]를 포함하여 300B 토큰의 혼합 데이터 세트에 대해\(\bullet\)_GPT-3_(175B) [55]를 학습했습니다.

\(\bullet\)_PaLM_ (540B) [56]은 소셜 미디어 대화, 필터링된 웹 페이지, 책, 깃허브, 다국어 위키피디아 및 뉴스로부터 소스되는 780B 토큰의 사전 트레이닝 데이터세트를 사용한다.

\(\bullet\)_LLaMA_[57]은 CommonCrawl, C4 [82], Github, Wikipedia, books, ArXiv, StackExchange 등 다양한 소스로부터 학습 데이터를 추출한다. LLaMA(6B) 및 LLaMA(13B)에 대한 트레이닝 데이터 크기는 1.0T 토큰인 반면, LLaMA(32B) 및 LLaMA(65B)에 대해 1.4T 토큰이 사용된다.

### _Commonly Used Datasets for Fine-tuning_

사전 훈련 후 모델 용량을 향상시키기 위해 추가 미세 조정 LLM이 필요하며, 이는 종종 명령어 튜닝(지도 미세 조정) 및 정렬 튜닝의 두 가지 주요 단계를 포함한다. 이 절에서는 주로 두 가지 종류의 조정 접근법에 대한 관련 사용 가능한 데이터 세트에 대해 논의하는 데 중점을 두며, 더 많은 알고리즘 세부 사항은 섹션 5에서 찾을 수 있다.

#### 3.3.1 명령어 튜닝 데이터 세트

사전 훈련 후, 명령어 튜닝(_a.k.a._, 감독 미세 조정)은 LLMs(_e.g._, 명령어 후속)의 특정 능력을 향상시키거나 해제하기 위한 중요한 방법이다. 이 부분에서, 우리는 명령어 튜닝을 위해 널리 사용되는 몇 가지 데이터 세트를 소개하고, 포맷된 명령어 인스턴스의 구성 방법에 따라 NLP 태스크 데이터 세트, 일일 채팅 데이터 세트 및 합성 데이터 세트의 세 가지 주요 유형으로 분류한다. 우리는 표 III에 그들의 세부 사항을 보여준다.

**NLP 작업 데이터 세트.** 이러한 종류의 데이터 세트는 해당 자연어 작업 설명과 함께 수집된 NLP 작업 데이터 세트(예:_e.g._, 텍스트 분류 및 요약)를 기반으로 포맷됩니다. 이 범주에서 P3 [182] 및 FLAN [67, 183]은 명령어 튜닝을 위해 널리 사용되는 두 개의 데이터 세트이다.

\(\bullet\)_P3_[182]는 170개의 영어 NLP 데이터셋과 2,052개의 영어 프롬프트 템플릿으로 구성되며, 각 데이터 예제의 입력 및 출력은 훈련 인스턴스를 구성하기 위한 특정 프롬프트 템플릿으로 포맷되었다.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Dataset** & **Release Time** & **\#Examples** \\ \hline \multicolumn{3}{l}{Summarize from Feedback [129]} & Sep-2020 & 193K \\ SHP [177] & Oct-2021 & 385K \\ WebGPT Comparisons [81] & Dec-2021 & 19K \\ Stack Exchange Preferences [178] & Dec-2021 & 10M \\ HH-RLHF [170] & Apr-2022 & 169K \\ Sandbox Alignment Data [179] & May-2023 & 169K \\ CValues [180] & Jul-2023 & 145K \\ PKU-SafeRLHF [181] & Oct-2023 & 330K \\ \hline \hline \end{tabular}
\end{table} TABLE IV: A list of available collections for alignment.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Categories** & **Collections** & **Time** & **\#Examples** \\ \hline \multirow{8}{*}{Task} & Nat. Inst. [166] & Apr-2021 & 193K \\  & FLAN [67] & Sep-2021 & 4.4M \\  & P3 [167] & Oct-2021 & 12.1M \\  & Super Nat. Inst. [88] & Apr-2022 & 5M \\  & MVPCrupus [168] & Jun-2022 & 41M \\  & xP3 [94] & Nov-2022 & 81M \\  & OIG[169] & Mar-2023 & 43M \\ \hline \multirow{8}{*}{Chat} & HH-RLHF [170] & Apr-2022 & 160K \\  & HCGs [171] & Jan-2023 & 87K \\  & ShazeGT[148] & Mar-2023 & 90K \\  & Dolly [172] & Apr-2023 & 15K \\  & OpenAssistant [173] & Apr-2023 & 161K \\ \hline \multirow{8}{*}{Synthetic} & Self-Instruct [143] & Dec-2022 & 82K \\  & Alpha [137] & Mar-2023 & 52K \\ \cline{1-1}  & Guancao [174] & Mar-2023 & 535K \\ \cline{1-1}  & Baire [175] & Apr-2023 & 158K \\ \cline{1-1}  & BELLE [176] & Apr-2023 & 1.5M \\ \hline \hline \end{tabular}
\end{table} TABLE III: A detailed list of available collections for instruction tuning.

\(\bullet\)_FLAN_[67]은 원래 버전에서 널리 사용되는 62개의 NLP 벤치마크로 구성된다. 최근에는 Muffin[67], NIV2[88], T0-SF[28], CoT[184, 185, 186] 등 추가적인 명령어 데이터셋을 혼합하여 FLAN을 확장하는 FLAN-v2[183]도 제안되고 있다. 머핀은 원래 FLAN에서 62개의 태스크와 대화 및 코드 합성 태스크를 포함하여 추가로 26개의 태스크를 포함한다. T0-SF는 머핀과의 중첩을 보장하면서 T0[28]로부터 추출된다. NIV2는 자연 명령 v2 데이터 세트 [88]을 참조하고 CoT [184, 185, 186]은 9개의 추론 태스크와 해당 사고 연쇄 프롬프트 및 출력의 조합이다.

**매일 채팅 데이터 세트** 이러한 종류의 데이터 세트는 쿼리가 인간에 의해 제기되고 응답이 주로 인간 레이블러 또는 LLM(예:_ ChatGPT, GPT-4)에 의해 생성되는 실제 사용자 대화를 기반으로 구성됩니다. 대화 유형에는 개방형 생성, 질문 응답, 브레인스토밍 및 채팅이 포함됩니다. 이 범주에서 ShareGPT [148], OpenAssistant [173] 및 Dolly [172]는 LLM 미세 조정을 위해 일반적으로 사용되는 세 가지 데이터 세트입니다.

\(\bullet\)_ShareGPT_[148]은 사용자가 ShareGPT API를 통해 ChatGPT 또는 GPT-4와 대화를 업로드할 수 있는 데이터 수집 플랫폼에서 수집된다. 현재 이 데이터 세트는 인간의 실제 지시 또는 문의와 ChatGPT의 응답을 포함하여 약 90,000개의 대화로 구성된다.

\(\bullet\)_OpenAssistant_[173]은 인간과 AI 어시스턴트 사이의 66,497개의 실제 대화 트리를 포함하는 다국어 코퍼스이다. 각 대화 트리는 여러 개의 노드로 구성되며, 각 노드는 대화에서 역할에 의해 생성된 정보를 나타낸다. 35개 언어에 걸쳐 있으며 461,292개의 수동으로 주석이 달린 응답 품질 등급이 포함되어 있습니다.

\(\bullet\)_Dolly_[172]는 Databricks로부터 15,000개의 인간 생성 데이터 인스턴스(prompt-response pairs)를 포함하는 영어 데이터세트이다. 이 데이터 세트는 브레인스토밍, 분류, 폐쇄형 도서 품질 보증, 생성, 정보 추출, 개방형 도서 품질 보증 및 요약을 포함하여 InstructGPT [66]에 요약된 7개 영역을 다룬다.

**합성 데이터 세트** 이러한 종류의 데이터 세트는 일반적으로 미리 정의 된 지침 규칙 또는 메서드를 기반으로 LLM에 지시 하 여 구성 됩니다. 이 범주에서 Self-Instruct-52K [143], Alpaca [142] 및 Baire [175]는 LLM에 일반적으로 사용되는 세 가지 합성 데이터 세트이다.

\(\bullet\)_Self-Instruct-52K_[143]은 52,000개의 명령어를 가진 82,000개의 인스턴스로 구성된 self-instruct [143] 방법을 통해 생성된 명령어 데이터셋이다. 구체적으로, 저자는 175개의 시드 인스턴스를 구성한 다음 LLM[55]에 참조로 무작위로 선택된 8개의 명령을 기반으로 추가 명령을 합성하도록 반복적으로 촉구한다. 이어서, LLM은 합성 명령들에 기초하여 인스턴스 입력들 및 그들의 대응하는 출력들을 생성하도록 추가로 지시되고, 최종적으로 Self-Instruct-52K 데이터세트를 획득한다.

\(\bullet\)_Alpaca_[142]는 또한 자체 명령 [143] 방법을 기반으로 하는 합성 데이터 세트이다. 셀프 명령어-52K의 175개 시드 데이터 세트에 대한 텍스트-다빈치-003 모델을 활용하여 52,000개의 새로운 명령어와 해당 입력 및 출력을 얻는다. 더욱이, 예시의 60%는 최종 데이터세트에서 입력 부분이 없는 순수한 명령어들이다.

\(\bullet\)_Baize_[175]는 111.5K 인스턴스로 구성된 ChatGPT를 사용하여 구성된 영어 멀티턴 대화 코퍼스이다. Baire를 만들기 위해 "셀프 채팅" [175]라는 방법이 목적이며, 여기서 ChatGPT는 사용자와 AI 어시스턴트의 역할을 번갈아 맡아 대화 형식으로 정보를 생성한다.

#### 3.3.2 Alignment Datasets

명령어 튜닝과는 별도로 LLM을 인간의 가치 및 선호도(예: 유용성, 정직성 및 무해성)와 정렬하기 위한 고품질 데이터 세트를 구성하는 것이 중요하다. 이 섹션에서는 HH-RLHF[170], SHP[177], PKU-SafeRLHF[181], Stack Exchange Preferences[178] 및 Sandbox Alignment Data[179]를 포함하여 정렬 조정을 위해 널리 사용되는 몇 가지 데이터 세트를 소개한다. 우리는 표 IV에 그들의 세부 사항을 보여준다.

\(\bullet\)**HH-RLHF**[170]은 약 169K개의 인스턴스로 구성되며, LLM의 유용성과 무해성에 각각 초점을 맞춘 두 부분으로 나눌 수 있다. 각 인스턴스는 도움, 조언 또는 작업 완료에 대한 크라우드 워커와 채팅 모델 간의 개방형 대화입니다. 채팅 모델은 각 사용자 질의에 대해 두 개의 응답을 제공하며, 더 유용하거나 유해한 응답이 주석으로 선택될 것이다.

\(\bullet\)**SHP**[177]은 응답의 유용성에 중점을 둡니다. 그것은 요리에서 법률 조언에 이르는 주제에 걸쳐 18개의 다양한 주제 영역에 걸쳐 질문/지침에 대한 응답보다 385K의 집단적 인간 선호도로 구성된다. 각 인스턴스는 질문이나 지시가 포함된 레딧 게시물과 한 쌍의 최상위 댓글이며, 그 중 하나는 레딧 사용자에 의해 더 바람직한 것으로 간주되고 다른 하나는 덜 도움이 되는 것으로 간주된다. HH-RLHF [170]과 달리 SHP의 데이터는 자연 발생 및 인간 작성 응답으로 구성된다.

\(\bullet\)**PKU-SafeRLHF**[181]은 유용성과 무해성에 초점을 맞춘 330K개 이상의 전문가 비교 데이터 인스턴스를 포함한다. 데이터세트의 각 인스턴스는 질문과 두 개의 응답을 포함하며, 각 응답에 대한 안전 라벨과 유용성과 무해성에 따라 두 응답 사이의 두 개의 선호도 주석이 함께 표시된다. 응답의 무해성은 14개의 모든 피해 범주에 걸쳐 위험 중립으로 분류를 나타내는 반면, 응답의 유용성은 질문 해결의 효율성을 기반으로 평가된다.

\(\총알\)**스택 교환 환경설정**[178]은 답변의 유용성에 중점을 둡니다. 스택 오버플로우의 약 10M 질문과 답변으로 구성되어 있습니다. 각 인스턴스는 질문과 두 개 이상의 대응 답변으로 구성된다. 각 답변에는 투표에 따라 계산된 점수와 선택 여부를 나타내는 레이블로 주석이 달린다.

\(\bullet\)**샌드박스 정렬 데이터**[179]는 사람이 아닌 LLM의 피드백을 포함하는 정렬 데이터 세트입니다. SAND-BOX라는 가상 상호작용 환경에서 나온 것으로, 이 모델은 다른 모델과의 사회적 상호작용을 시뮬레이션하고 다른 모델의 피드백에 따라 응답을 수정한다. 데이터 세트에는 169K 인스턴스가 포함되어 있으며 각 인스턴스는 사회적 쿼리, 여러 응답 및 다른 모델의 해당 등급으로 구성된다.

### _Library Resource_

이 부분에서는 LLM 개발을 위한 일련의 이용 가능한 라이브러리에 대해 간략하게 소개한다.

\(\bullet\)**Transformers**[187]은 Transformer 아키텍처를 사용하여 모델을 빌드하기 위한 오픈 소스 Python 라이브러리이며, 이는 Hugging Face에서 개발 및 유지 관리합니다. 간편하고 사용자 친화적인 API가 있어 다양한 사전 훈련 모델을 쉽게 사용하고 커스터마이징할 수 있습니다. 모델과 알고리즘을 정기적으로 업데이트하고 개선하는 사용자와 개발자의 크고 활발한 커뮤니티가 있는 강력한 라이브러리입니다.

\(\bullet\)**DeepSpeed**[74]는 MT-NLG [113] 및 BLOOM [78]과 같은 여러 LLM을 훈련하는 데 사용된 Microsoft에서 개발한 딥 러닝 최적화 라이브러리(PyTorch와 호환됨)입니다. 메모리 최적화(ZeRO 기법, Gradient Checkpointing), 파이프라인 병렬화 등 분산 학습을 위한 다양한 최적화 기법의 지원을 제공한다.

\(\bullet\)**Megatron-LM**[75, 76, 77]은 NVIDIA에서 대규모 언어 모델을 훈련하기 위해 개발한 딥 러닝 라이브러리이다. 또한 모델 및 데이터 병렬성, 혼합 정밀도 훈련, 플래시 어텐션 등 분산 훈련을 위한 풍부한 최적화 기법을 제공한다. 이러한 최적화 기술은 훈련 효율성과 속도를 크게 향상시켜 GPU 전반에 걸쳐 효율적인 분산 훈련을 가능하게 할 수 있다.

\(\bullet\)**JAX**[188]은 Google에서 개발한 고성능 기계 학습 알고리즘을 위한 Python 라이브러리이며, 사용자가 하드웨어 가속(_예:_, GPU 또는 TPU)으로 배열에 대한 계산을 쉽게 수행할 수 있도록 합니다. 다양한 장치에서 효율적인 연산을 가능하게 하고 자동 미분, 정시 컴파일 등 여러 가지 기능을 지원한다.

\(\bullet\)**Colossal-AI**[189]는 HPC-AI Tech에서 대규모 AI 모델을 훈련하기 위해 개발한 딥러닝 라이브러리이다. PyTorch를 기반으로 구현되며 풍부한 병렬 훈련 전략 모음을 지원합니다. 또한, 패트릭스타[190]가 제안한 방법으로 이기종 메모리 관리를 최적화할 수도 있다. 최근 ColossalChat[140]이라는 ChatGPT 유사 모델이 LLaMA[57] 기반의 Colossal-AI를 이용하여 개발된 두 가지 버전(7B와 13B)으로 공개 출시되었다.

\(\bullet\)**BMTrain**[191]은 OpenBMB에서 대규모 매개 변수를 분산 방식으로 학습 모델에 대해 개발한 효율적인 라이브러리이며, 이는 코드의 단순성, 낮은 리소스 및 고가용성을 강조한다. BMtrain은 이미 여러 일반적인 LLM(_e.g._, Flan-T5 [69] 및 GLM [93])을 모델센터에 통합했으며, 개발자는 이러한 모델을 직접 사용할 수 있다.

\(\bullet\)**FastMoE**[192]는 MoE(_i.e._, mixture-of-experts) 모델에 대한 전문 교육 라이브러리입니다. PyTorch를 기반으로 개발되었으며, 디자인에서 효율성과 사용자 친화성을 모두 우선시합니다. FastMoE는 Transformer 모델을 MoE 모델로 전달하는 과정을 단순화하고 훈련 시 데이터 병렬성과 모델 병렬성을 모두 지원한다.

\(\bullet\)**VLM**[193]은 LLM 추론 및 서빙을 위한 빠르고 메모리 효율적이며 사용하기 쉬운 라이브러리이다. 빠른 추론을 가능하게 하기 위해 높은 서빙 처리량, PagedAttention [193]을 이용한 효과적인 주의 집중 메모리 관리, 연속 배칭, 최적화된 CUDA 커널로 특별히 최적화된다. 또한, vLLM은 다양한 디코딩 알고리즘, 텐서 병렬성 및 스트리밍 출력을 지원한다. 다른 시스템과의 통합을 쉽게 하기 위해 vLLM은 HuggingFace 모델의 사용에 친화적이며 OpenAI 호환 API 서버도 제공한다.

\(\bullet\)**DeepSpeed-MII**[194]도 DeepSpeed [74]에서 개발한 메모리 효율적인 Python 라이브러리입니다. 높은 처리량, 낮은 대기 시간 및 비용 효율성을 우선시하여 LLMs 추론을 민주화하는 것을 목표로 한다. DeepSpeed-MII는 차단된 KV 캐싱, 연속 배치, 동적 분할퓨즈 및 고성능 CUDA 커널의 4가지 필수 기술을 활용하여 가속화된 텍스트 생성 추론을 달성한다. 현재 LLaMA[57], Mistral[195] 및 OPT[90]과 같은 세 가지 인기 있는 모델 아키텍처에 걸쳐 13,000개 이상의 모델을 지원합니다.

\(\bullet\)**DeepSpeed-Chat**[196]은 모델 훈련 동안 완전한 RLHF 프로세스를 통합할 수 있는 빠르고 비용 효율적이며 사용하기 쉬운 시스템 프레임워크입니다. 본 논문에서는 ChatGPT 유사 모델에 대한 학습 및 추론 과정을 단순화하고, 간단한 스크립트를 사용하여 다중 학습 또는 추론 단계를 구현하며, InstructGPT [66]의 학습 모드를 복제하고, 3단계(_i.e._, SFT, 보상 모델 미세 조정 및 RLHF)에 대한 완전한 파이프라인을 제공한다. (3) Deepspeed의 학습 엔진과 추론 엔진을 통합한 RLHF 훈련용 하이브리드 엔진(Deepspeed HE)으로 통합하여 학습 및 추론 모드 간의 원활한 전환을 가능하게 하며, DeepSpeed 추론에서 다양한 최적화를 활용한다.

위의 라이브러리 리소스 외에도 기존 딥 러닝 프레임워크(_e.g._, PyTorch[197], TensorFlow[198], MXNet[199], PaddlePaddle[200], MindSpore[136] 및 OneFlow[201])도 병렬 알고리즘에 대한 지원을 제공했으며, 이는 대규모 모델의 훈련에 일반적으로 사용된다.

## 4 Pre-training

사전 훈련은 LLM의 능력의 기초를 확립한다. 대규모 코퍼라에 대한 사전 교육을 통해 LLMs은 필수적인 언어 이해와 생성 기술을 습득할 수 있다[55, 56]. 이 과정에서 사전 훈련 말뭉치의 규모와 품질은 LLM이 강력한 능력을 갖추는데 매우 중요하다. 또한 LLM을 효과적으로 사전 학습하기 위해서는 모델 아키텍처, 가속 방법 및 최적화 기술이 잘 설계되어야 한다. 다음에서는 먼저 섹션 4.1에서 데이터 수집 및 처리에 대해 논의한 다음 섹션 4.2에서 일반적으로 사용되는 모델 아키텍처를 소개하고 마지막으로 섹션 4.3에서 LLM을 안정적이고 효율적으로 최적화하는 훈련 기술을 제시한다.

### _Data Collection and Preparation_

LLM은 소규모 언어 모델과 비교하여 모델 사전 훈련을 위한 고품질 데이터에 대한 수요가 더 강하며, 모델 용량은 사전 훈련 말뭉치와 사전 처리된 방법에 크게 의존한다. 이 부분에서는 데이터 소스, 전처리 방법 및 사전 훈련 데이터가 LLM의 성능에 어떻게 영향을 미치는지에 대한 중요한 분석을 포함하여 사전 훈련 데이터의 모델 및 처리에 대해 논의한다.

#### 4.1.1 데이터 원본

유능한 LLM을 개발하기 위해서는 다양한 데이터 소스로부터 대량의 자연어 코퍼스를 수집하는 것이 핵심이다. 기존의 LLM은 주로 다양한 공개 텍스트 데이터 세트의 혼합물을 사전 훈련 코퍼스로 활용한다. 그림 6은 다수의 대표적인 LLMs에 대한 사전 훈련 데이터의 출처 분포를 보여준다.

사전 훈련 말뭉치의 출처는 크게 일반 데이터와 전문 데이터의 두 가지 유형으로 분류할 수 있다. 웹 페이지, 책, 대화 텍스트와 같은 일반적인 데이터는 크고 다양하며 접근 가능한 특성으로 인해 대부분의 LLMs[55, 56, 90]에서 활용되며, 이는 LLMs의 언어 모델링 및 일반화 능력을 향상시킬 수 있다. LLM이 보여주는 인상적인 일반화 능력에 비추어 볼 때, 사전 훈련 코퍼스를 다국어 데이터, 과학 데이터 및 코드와 같은 보다 전문화된 데이터 세트로 확장하여 LLM에게 특정 과제 해결 능력을 부여하는 연구도 있다[35, 56, 86]. 다음은 이 두 가지 유형의 사전 훈련 데이터 소스와 LLM에 미치는 영향에 대해 설명한다. 일반적으로 사용되는 말뭉치에 대한 자세한 소개는 3.2절을 참고할 수 있다.

**일반 텍스트 데이터.** 그림 6에서 볼 수 있듯이 대다수의 LLM은 웹 페이지, 책 및 대화 텍스트와 같은 범용 사전 훈련 데이터를 채택하여 다양한 주제에 대한 풍부한 텍스트 소스를 제공합니다. 다음으로, 우리는 세 가지 중요한 일반 데이터를 간략하게 요약한다.

\(\bullet\)_Webpages._ 인터넷의 확산으로 인해 다양한 유형의 데이터가 생성되었으며, 이는 LLM이 다양한 언어 지식을 얻고 일반화 능력을 향상시킬 수 있게 한다[82, 26]. 이러한 데이터 자원의 편리한 사용을 위해 CommonCrawl[163]과 같이 이전 작업에서 웹으로부터 대량의 데이터를 크롤링한다. 그러나 크롤링된 웹 데이터는 스팸 메일과 같이 위키피디아와 같은 고품질 텍스트와 저품질 텍스트를 모두 포함하는 경향이 있으므로 데이터 품질을 향상시키기 위해서는 웹 페이지를 필터링하고 처리하는 것이 중요하다.

\(\bullet\)_대화 텍스트._ 대화 데이터는 LLMs[90]의 대화 능력을 향상시키고 잠재적으로 다양한 질의 응답 태스크에서 그들의 성능을 향상시킬 수 있다[56]. 연구자들은 공개 대화 코퍼스(_예를 들어,_ PushShift.io Reddit 코퍼스) [202, 158]의 서브세트를 활용하거나 온라인 소셜 미디어로부터 대화 데이터를 수집할 수 있다. 온라인 대화 데이터는 종종 다수의 참가자 간의 토론을 수반하기 때문에, 효과적인 처리 방법은 대화를 트리 구조로 변환하는 것이며, 여기서 발화는 대응하는 발화와 연결된다. 이러한 방식으로, 다자간 대화 트리는 다수의 서브-대화들로 분할될 수 있으며, 이는 사전 트레이닝 코퍼스에서 수집될 수 있다. 또한 잠재적 위험은 대화 데이터를 LLM에 과도하게 통합하면 부작용이 발생할 수 있다[90]: 선언적 지시와 직접적인 질문은 대화의 시작으로 잘못 인식되어 지시의 효율성이 감소한다.

\(\bullet\)_Books._ 다른 코퍼스에 비해 책은 형식적 긴 텍스트의 중요한 출처를 제공하며, 이는 LLM이 언어 지식을 배우고 장기 종속성을 모델링하며 내러티브 및 일관된 텍스트를 생성하는 데 잠재적으로 유익하다. 오픈 소스 도서 데이터를 얻기 위해 기존 연구에서는 일반적으로 파일 데이터 세트 [161]에서 사용할 수 있는 Books3 및 Bookcorpus2 데이터 세트를 채택한다.

**전문화된 텍스트 데이터.** 전문화된 데이터 세트는 다운스트림 작업에서 LLM의 특정 기능을 개선하는 데 유용합니다. 다음으로 세 가지 종류의 전문 데이터를 소개합니다.

\(\bullet\)_다국어 텍스트._ 대상 언어의 텍스트 외에도 다국어 말뭉치를 통합하면 언어 이해와 생성의 다국어 능력을 향상시킬 수 있다. 예를 들어, BLOOM[78] 및 PaLM[56]은 사전 훈련 말뭉치 내에서 각각 46개 및 122개 언어를 포함하는 선별된 다국어 데이터를 갖는다. FLM[102]은 중국어와 영어 말뭉치를 거의 같은 비율로 섞는다. 이러한 모델은 번역, 다국어 요약 및 다국어 질문 응답과 같은 다국어 작업에서 인상적인 성능을 보여주며, 목표 언어(들)의 코퍼스에서 미세 조정되는 최첨단 모델과 비교되거나 우수한 성능을 달성한다.

도. 6: 기존 LLMs에 대한 사전 트레이닝 데이터에서 다양한 데이터 소스의 비율.

[MISSING_PAGE_FAIL:18]

프라이버시 위험을 어느 정도 줄입니다.

**토큰화** 토큰화는 데이터 전처리를 위한 중요한 단계이기도 합니다. 그것은 원시 텍스트를 개별 토큰의 시퀀스로 세그먼트화하는 것을 목표로 하며, 이는 후속적으로 LLM의 입력으로 사용된다. 전통적인 NLP 연구(예를 들어 조건부 랜덤 필드를 사용한 시퀀스 라벨링[220])에서 단어 기반 토큰화는 인간의 언어 인식과 더 잘 일치하는 우세한 접근법이다. 그러나, 단어 기반 토큰화는 일부 언어들(예를 들어,_중국어 단어 세분화)에서 동일한 입력에 대해 상이한 세분화 결과들을 산출할 수 있고, 많은 저빈도 단어들을 포함하는 거대한 단어 어휘를 생성할 수 있으며, 또한 _"어휘 외"_ 이슈에 시달릴 수 있다. 따라서, 몇몇 신경망 모델들은 단어 표현을 도출하기 위해 최소 단위로서 _character_ 를 채용한다(예를 들어, ELMo[21] 내의 CNN 단어 인코더). 최근, _subword tokenizers_ 는 Byte-Pair Encoding tokenization, WordPiece tokenization 및 Unigram tokenization을 포함하는 Transformer 기반 언어 모델에서 널리 사용되고 있다. 허깅페이스는 실행 사례와 함께 토나이저22에서 우수한 온라인 NLP 과정을 유지했으며, 우리는 이 과정을 초보자에게 참조한다. 다음으로 대표적인 세 가지 토큰화 방법에 대해 간략히 설명한다.

각주 22: [https://huggingface.co/learn/nlp-course/chapter6](https://huggingface.co/learn/nlp-course/chapter6)

\(\bullet\)_Byte-Pair Encoding (BPE) 토큰화._ BPE는 원래 1994년 [221]에 일반적인 데이터 압축 알고리즘으로 제안된 후 토큰화를 위해 NLP에 적응되었다[222]. 기본 기호 집합(예: 알파벳 및 경계 문자)으로 시작하여 코퍼스에서 연속되는 두 토큰의 빈발 쌍을 새로운 토큰( _merge_라고 함)으로 반복적으로 결합합니다. 각각의 합병에 대해, 선택 기준은 두 개의 연속 토큰들의 동시 발생 빈도에 기초한다: 최상위 빈발 쌍이 선택될 것이다. 병합 프로세스는 미리 정의된 크기에 도달할 때까지 계속됩니다. 또한 바이트 레벨 BPE는 바이트_를 병합을 위한 기본 심볼로 간주하여 다국어 말뭉치(예를 들어, 비ASCII 문자가 포함된 텍스트)에 대한 토큰화 품질을 향상시키는 데 사용되었다. 이러한 토큰화 접근법을 갖는 대표적인 언어 모델에는 GPT-2, BART 및 LLaMA가 포함된다.

\(\bullet\)_WordPiece tokenization._ 워드피스는 구글 내부 서브워드 토큰화 알고리즘이었다. 그것은 원래 구글에 의해 음성 검색 시스템 개발에 제안되었다[223]. 그런 다음 2016년 신경 기계 번역 시스템에 사용되었고[224], 2018년 BERT용 단어 토큰화기로 채택되었다[23]. WordPiece는 연속 토큰을 반복적으로 병합하는 반면 병합에 대해 약간 다른 선택 기준을 취함으로써 BPE와 매우 유사한 아이디어를 갖는다. 병합을 수행하기 위해 먼저 언어 모델을 훈련하고 가능한 모든 쌍을 점수화하는 데 사용한다. 그런 다음 각 병합에서 학습 데이터의 가능성이 가장 많이 증가하는 쌍을 선택합니다. 구글이 워드피스 알고리즘의 공식 구현을 공개하지 않았기 때문에 휴징페이스는 온라인 NLP 과정에서 더 직관적인 선택 척도를 제공한다: 한 쌍은 훈련 코퍼스를 기반으로 한 쌍에서 두 토큰의 발생 카운트의 곱으로 동시 발생 카운트를 나누어 점수를 매긴다.

\(\bullet\)_Unigram tokenization._ BPE 및 WordPiece와 달리 Unigram 토큰화[225]는 코퍼스에 대해 가능한 부분 문자열 또는 서브토큰의 충분히 큰 집합으로 시작하고, 예상된 어휘 크기에 도달할 때까지 현재 어휘 내의 토큰을 반복적으로 제거한다. 선택 기준으로 현재 어휘에서 일부 토큰이 제거되었다고 가정하여 학습 말뭉치의 산출 가능성 증가를 계산한다. 이 단계는 훈련된 유니그램 언어 모델에 기초하여 수행된다. Unigram 언어 모델을 추정하기 위해, 기대-최대화(EM: expectation-maximization) 알고리즘을 채택한다: 각 반복에서, 우리는 먼저 구 언어 모델을 기반으로 단어의 현재 최적 토큰화를 찾고, 언어 모델을 업데이트하기 위해 Unigram의 확률을 재추정한다. 이 과정에서 언어 모델이 주어진 단어의 최적 분해 방법을 효율적으로 찾기 위해 동적 프로그래밍 알고리즘(_즉,_비터비 알고리즘)이 사용된다. 이러한 토큰화 접근법을 채택하는 대표적인 모델로는 T5와 mBART가 있다.

기존의 토큰화기를 활용하는 것이 편법적이지만(_예:_OPT[90] 및 GPT-3[55]는 GPT-2[26]의 토큰화기를 활용함), 사전 훈련 말뭉치를 위해 특별히 설계된 토큰화기를 사용하는 것은 특히 다양한 도메인, 언어 및 포맷으로 구성된 말뭉치에 매우 유익할 수 있다[78]. 따라서 최근 LLM은 Byte-level BPE 및 Unigram 토큰화를 포함하는 SentencePiece 라이브러리 [226]를 사용하여 사전 훈련 코퍼스에 대해 특별히 맞춤형 토큰라이저를 훈련하는 경우가 많다. 참고로, NFKC[227]와 같은 BPE에서의 정규화 기법들은 토큰화 성능을 저하시킬 수 있다[34, 64, 78]. 기존 LLM(_즉,_지속적인 사전 훈련 또는 명령어 튜닝)을 확장할 때 맞춤형 토큰라이저의 잠재적인 부작용도 인식해야 한다. 예를 들어, LLaMA는 주로 영어 텍스트로 구성된 사전 훈련 코퍼스를 기반으로 BPE 토큰화기를 훈련하고, 도출된 어휘는 비영어 데이터를 처리하는 데 덜 능숙할 수 있으며, 예를 들어, 추론 지연 시간을 더 길게 하여 중국어 텍스트를 생성할 수 있다.

도. 도 7: 대형 언어 모델들을 사전 트레이닝하기 위한 전형적인 데이터 전처리 파이프라인의 예시.

**데이터 품질의 영향에 대한 논의** 사전 훈련의 경우 사전 훈련 데이터의 품질은 LLM의 모델 용량에 매우 중요합니다. 기존의 연구는 노이즈, 독성 및 중복 데이터와 같은 저품질 코퍼스에 대한 사전 훈련이 모델의 성능에 큰 영향을 미친다는 것을 보여주었다[64, 214, 216, 219]. T5 [82], GLaM [112] 및 Gopher [64]와 같은 최근 연구에서는 데이터 품질이 LLM의 용량에 미치는 영향을 조사했다. 필터링된 코퍼스와 필터링되지 않은 코퍼스에 대해 훈련된 모델의 성능을 비교함으로써, 그들은 청소된 데이터에 대해 LLM을 사전 훈련시키는 것이 모델 성능을 향상시킬 수 있다는 유사한 결론에 도달했다. 보다 구체적으로, 데이터의 중복은 "_double descent_"(초기에 성능이 악화되고 후속적으로 향상되는 현상을 지칭함)을 초래할 수 있거나[214, 228], 심지어 트레이닝 프로세스[214]를 압도할 수 있다. 또한, 중복 데이터는 문맥에서 복사하는 LLMs의 능력을 저하시키고, 이는 문맥 내 학습을 사용하는 LLMs의 일반화 능력에 더 영향을 미칠 수 있음을 보여주었다[214]. 따라서 [56, 64, 78, 212]에서 제안된 바와 같이, 훈련 프로세스의 안정성을 향상시키고 모델 성능에 영향을 미치지 않도록 사전 훈련 말뭉치를 조심스럽게 청소하기 위해 품질 필터링, 독성 필터링 및 중복 제거와 같은 전처리 방법을 활용하는 것이 필수적이다(섹션 4.1.2에서 예시된 바와 같이).

#### 4.1.3 데이터 스케줄링

데이터 전처리 후, 가능한 LLM을 사전 훈련하기 위해 이러한 다중 소스 데이터를 스케줄링하기 위한 적절한 전략을 설계하는 것이 필수적이다. 일반적으로 데이터 스케줄링을 위해서는 각 데이터 소스의 비율(_data mixture_)과 각 데이터 소스가 훈련(_data curriculum_)을 위해 스케줄링되는 순서의 두 가지 핵심적 측면을 세심하게 고려해야 한다. 다음으로, 두 가지 측면에 대해 자세히 논의한다. 데이터 스케줄링의 예시는 그림 8에 제시되어 있다.

**데이터 혼합** 각 종류의 데이터 원본은 LLM에 대한 특정 용량 개발(섹션 4.1의 논의를 참조)과 밀접한 관련이 있으므로 이러한 데이터를 혼합하기 위해 적절한 분포를 설정하는 것이 중요합니다. 데이터 혼합물은 일반적으로 전역 레벨(_즉, 전체 사전 트레이닝 데이터의 분포)로 설정되며, 또한 상이한 트레이닝 스테이지들에서 다양한 비율들로 국부적으로 설정될 수 있다. 사전 트레이닝 동안, 상이한 소스들로부터의 데이터 샘플들이 혼합물 비율들에 따라 선택될 것이다: 더 많은 데이터가 더 큰 가중치를 갖는 데이터 소스로부터 샘플링될 것이다. 전형적으로, LLaMA[57]과 같은 기존의 LLM들은 사전 트레이닝 데이터로서 특정 데이터 혼합물들을 생성하기 위해 각 소스의 전체 데이터에 업샘플링 또는 다운샘플링을 채용할 수 있다. 도 6이 예시하는 바와 같이, 기존의 LLM들은 사전 트레이닝 데이터를 구성하기 위해 상이한 데이터 혼합물들을 사용한다. 대표적인 모델로 LLaMA[57]의 사전 학습 데이터는 주로 웹 페이지(80% 이상)로 구성되어 있으며, GitHub 및 StackExchange의 코드 헤비 데이터 6.5%, 도서 4.5%, arXiv의 과학 데이터 2.5%로 범용 LLMs 학습에 중요한 참고가 되고 있다. 또한, 특수 데이터 혼합물은 상이한 목적을 용이하게 하기 위해 사용될 수 있다. 예를 들어, Falcon[141]은 순수 웹페이지 상에서 트레이닝되며, CodeGen[86]은 코드 데이터의 양을 크게 증가시킨다. 실제로, 데이터 혼합물은 종종 경험적으로 결정되며, 우리는 다음과 같이 효과적인 데이터 혼합물을 찾기 위한 몇 가지 일반적인 전략을 요약한다:

\(\bullet\)_데이터 소스의 다양성을 증가시킵니다._ 최근 연구에 따르면 특정 도메인에 대한 과도한 데이터에 대한 훈련은 다른 도메인에서 LLM의 일반화 능력을 저하시킬 것이다[35, 64]. 대조적으로, 데이터 소스 이질성(예를 들어, 다양한 데이터 소스를 포함하는_)을 증가시키는 것은 LLMs[229, 230, 212]의 다운스트림 성능을 개선하는데 중요하다. 서로 다른 데이터 소스의 영향을 더 조사하기 위해 일부 연구에서는 각 데이터 소스를 하나씩 제거하고 특별히 선별된 데이터 세트로 LLM을 사전 훈련하여 절제 실험을 수행했다[212]. 이질성이 높은 데이터 소스(예: 웹 페이지)를 삭제하면 이질성이 낮은 소스(예: 학술 말뭉치)를 삭제하는 것보다 LLM의 능력에 더 심각한 영향을 미치는 것으로 나타났다.

\(\bullet\)_최적화 데이터 혼합물._ 데이터 혼합물을 수동으로 설정하는 것 외에도, 모델 사전 트레이닝을 개선하기 위해 데이터 혼합물을 최적화하기 위한 여러 연구가 제안되었다[59, 231]. 타겟 다운스트림 태스크들이 주어지면, 피처 공간에서 더 높은 근접도를 갖는 사전 트레이닝 데이터들[231] 또는 다운스트림 태스크 성능에 긍정적인 영향을 제공하는 데이터들[232]을 선택할 수 있다. 또한, 목표 태스크의 의존도를 줄이기 위해 DoReMi[59]는 주어진 초기 도메인 가중치를 사용하여 작은 참조 모델을 먼저 학습시킨 다음, 두 모델 사이의 가능성에서 가장 큰 불일치가 관찰되는 도메인에 가중치를 부여하여 또 다른 작은 프록시 모델을 학습시킨다. 마지막으로, 프락시 모델의 학습된 도메인 가중치를 적용하여 훨씬 더 큰 LLM을 학습한다. 보다 간단한 방법으로, 여러 개의 작은 언어 모델을 서로 다른 데이터 혼합물로 훈련시키고, 가장 바람직한 성능으로 이어지는 데이터 혼합물을 선택할 수 있다. 그러나 이 접근법에서 이루어진 가정은 유사한 방식으로 훈련될 때 작은 모델이 모델 능력 또는 행동에서 큰 모델과 유사할 것이며, 이는 항상 실제로 유지되는 것은 아닐 수 있다.

\(\bullet\)_대상 능력을 전문화합니다._ LLM의 모델 용량은 데이터 선택 및 혼합에 크게 의존하며, 특정 모델 능력을 향상시키기 위해 특정 데이터 소스의 비율을 증가시킬 수 있다[64, 212]. 예를 들어, 수학적 추론과 코딩 능력은 각각 더 많은 수학적 텍스트와 코드 데이터로 훈련함으로써 특별히 향상될 수 있다. 또한 LAMBADA 데이터셋에 대한 실험 결과 [233]은 장서 데이터의 비율을 증가시키면 텍스트로부터 장기 종속성을 포착하는 데 있어 모델 용량을 향상시킬 수 있으며, C4 데이터셋의 비율을 증가시키면 [82]는 C4 검증 데이터셋에 대한 성능 향상으로 이어진다[64]. 일반적으로, 보다 암묵적인 관계를 식별하는 것이 중요하다.

도. 도 8: LLM들을 사전 트레이닝하기 위한 데이터 스케줄링의 예시.

데이터 원본 및 모델 기능. LLM에서 수학 및 코딩과 같은 특정 기술을 향상시키거나 전문화된 LLM을 개발하기 위해 실용적인 방법은 다단계 훈련 접근법을 사용하는 것이며, _예를 들어_ 일반 및 기술 특정 데이터는 두 개의 연속 단계에서 예약될 수 있다. 여러 단계에 걸쳐 다양한 데이터 출처 또는 비율에 대해 LLM을 훈련하는 이러한 접근법은 "데이터 커리큘럼"으로도 알려져 있으며, 이는 아래에서 소개될 것이다.

**데이터 커리큘럼.** 데이터 혼합물을 준비한 후 사전 교육을 위해 특정 데이터가 LLM에 제공되는 순서를 예약하는 것이 중요합니다. 어떤 경우에, 어떤 스킬을 학습하기 위해, 스킬-세트 시퀀스(_예를 들어,_기본 스킬 \(\rightarrow\) 타겟 스킬)에서의 학습이 타겟 스킬에만 초점을 맞춘 코퍼스로부터의 직접 학습보다 우수한 것으로 나타났다[234, 235]. 커리큘럼 학습[236]의 아이디어에 따라 _데이터 커리큘럼_이 제안되었고 모델 사전 훈련[234, 235, 237, 238]에서 널리 사용되었다. 이는 LLM에 대한 사전 훈련 데이터의 다양한 부분을 쉬운/일반적인 예에서 시작하여 점진적으로 더 도전적인/전문적인 사례를 도입하는 _예를 들어_ 특정 순서로 구성하는 것을 목표로 한다. 보다 일반적으로, 사전 트레이닝 동안 상이한 소스들에 대한 데이터 비율들의 적응적 조정을 광범위하게 지칭할 수 있다. 데이터 커리큘럼에 관한 기존의 작업은 주로 전문화된 코딩 LLM(_e.g.,_CodeLLMa[235]) 또는 긴 컨텍스트 LLM(_e.g.,_LongLLaMA[238])과 같은 지속적인 사전 훈련에 초점을 맞춘다. 그러나, 범용 LLMs(예를 들어, LLaMA)를 위한 데이터 커리큘럼에 대한 보다 상세한 연구는 아직 부족한 실정이다. 데이터 커리큘럼을 결정하기 위해서는 특별히 구성된 평가 벤치마크를 기반으로 LLM의 핵심 능력 개발을 모니터링한 다음 사전 훈련 중에 데이터 혼합을 적응적으로 조정하는 실용적인 접근법이 있다. 다음으로 데이터 커리큘럼23의 개념이 지속적인 사전 훈련에서 어떻게 적용되는지를 소개하기 위해 세 가지 공통 능력을 예로 든다.

각주 23: 데이터 교육과정에서 데이터 순서를 나타내기 위해 “\(\rightarrow\)”라는 기호를 활용한다. 예를 들어, "2T 웹 페이지 토큰 \(\rightarrow\) 500B 코드 토큰"은 LLM이 먼저 2T 웹 페이지 토큰으로 트레이닝되고 이어서 500B 코드 데이터 토큰으로 트레이닝된다는 것을 의미한다.

\(\bullet\)_Coding_. LLM의 코딩 능력을 향상시키기 위해, CodeLLAMA [235]는 LLaMA 2 [99] (2T 일반 토큰 \(\rightarrow\) 500B 코드-헤비 토큰)을 기반으로 개발되었으며, 코드 생성 능력을 향상시키고 자연어 이해 능력을 유지하는 것을 목표로 한다. 또한 CodeLLaMA는 특정 프로그래밍 언어에 더 특화된 버전, 즉 CodeLLaMA-Python(2T 일반 토큰 \(\rightarrow\) 500B 코드-헤비 토큰 \(\rightarrow\) 100B Python-헤비 토큰을 제공합니다.

\(\bullet\)_Mathematics_. Liemma[239]는 범용 LLM들의 수학적 능력들을 향상시키기 위해 제안된다. CodeLLaMA를 기반으로 개발되었습니다. CodeLLaMA[235]는 코딩 능력에 주로 초점을 맞추고 있지만, 수학 벤치마크[239]에서 기본 모델 LLaMA 2보다 성능이 우수함을 실험을 통해 보여주었다. CodeLLaMA를 기반으로 Lemma는 과학 논문, 수학 텍스트를 포함하는 웹 데이터 및 코드(2T 일반 토큰 \(\rightarrow\) 500B 코드-헤비 토큰 \(\rightarrow\) 50\(\sim\)200B 수학-헤비 토큰)의 혼합에 대해 지속적으로 학습된다. 렘마의 사전 훈련 데이터도 정규화의 한 형태로서 5%의 일반 도메인 데이터를 포함한다는 점에 유의한다.

\(\bullet\)_Long context_. 긴 컨텍스트 모델링은 LLMs에 중요한 능력이며, 많은 연구들이 LLMs의 컨텍스트 윈도우를 지속적으로 트레이닝을 통해 확장하는 것을 탐구했다[235, 238]. RoPE 기반 LLMs[57, 99, 240]의 위치 임베딩(_i.e.,_위치 보간)에 대한 수정으로, CodeLLaMA는 LLaMA 2의 컨텍스트 윈도우를 추가로 확장한다(4K 컨텍스트 윈도우를 갖는 2.5T 토큰 \(\rightarrow\) 20B 토큰은 16K 컨텍스트 윈도우를 갖는다). LongLLaMA [238]은 또한 외부 메모리 및 고유한 트레이닝 목적(2K 컨텍스트 윈도우를 갖는 1T 토큰 \(\rightarrow\) 8K 컨텍스트 윈도우를 갖는 10B 토큰)의 도움으로 더 긴 컨텍스트 윈도우를 달성한다.

#### 4.1.4 데이터 준비 요약

이 부분에서는 LLMs에 대한 사전 훈련 자료를 마련하기 위한 일반적인 절차와 핵심 사항을 정리하는데, 다음 세 가지 측면에서 자세히 설명된다.

\(\bullet\)_Data collection_. 사전 훈련 데이터에 다양한 데이터 소스를 포함할 것을 제안한다. 팰컨[141]은 강력한 LLM을 훈련하기 위해 웹 페이지만 사용할 수 있음을 보여주지만, 보다 일반적인 접근 방식은 코드, 책, 과학 논문, _등등과 같은 다양한 고품질 텍스트도 통합하는 것이다._ LLM이 특정 기술로 전문화되면 그에 따라 해당 데이터 소스의 비율이 증가해야 한다. 예를 들어, 고퍼[64]와 친칠라[34]는 책의 약 40%의 데이터로 학습된다. PaLM[44] 및 LaMDA[68]은 대략 50%의 대화 데이터를 사용한다.

\(\bullet\)_데이터 청소_. 데이터 수집 후에는 가능한 한 품질을 향상시키기 위해 원시 말뭉치를 청소하는 것이 중요합니다. 먼저, 중복제거는 기존 작업에서 일반적으로 사용된다[99, 141, 229]. 둘째, 프라이버시 염려가 있는 저품질 텍스트, 독성 콘텐츠 및 데이터는 상이한 세분성(예를 들어, 문서, 통과 또는 문장)에서 제거되어야 한다. 실제로, 휴리스틱 및 분류기-기반 방법들 모두는 품질 및 독성 필터링(_예를 들어,_CCNet[241], fastText[242], 및 Data-Juicer[243])을 위해 채용될 수 있다. 셋째, 세정된 데이터로, 데이터를 사전 트레이닝하기 위한 포맷을 더 통일하거나 특정할 수 있고, SentencePiece와 같은 라이브러리로 필터링되고 중복 제거된 코퍼스에 토큰라이저를 트레이닝함으로써 토큰화를 수행할 수 있다[226].

\(\bullet\)_Data scheduling_. 전처리된 데이터를 사용하여, 다음 단계는 LLM을 사전 트레이닝하기 위한 데이터 혼합 및 데이터의 특정 순서를 결정하는 것이다. 두 설정 모두를 결정하기 위해, 실용적인 방법은 먼저 다수의 후보 계획들을 갖는 몇몇 작은 언어 모델들을 트레이닝한 다음, 그들 중에서 좋은 계획을 선택하는 것이다[59]. 전반적으로 적합한 데이터 커리큘럼을 찾기가 더 어렵다. 실제로 특정 평가 벤치마크에 대한 중간 모델 체크포인트의 성능을 모니터링하고 사전 훈련 동안 데이터 혼합 및 분포를 동적으로 조정할 수 있다. 이 과정에서 데이터 소스와 모델 능력 사이의 잠재적 관계를 탐색하여 데이터 커리큘럼 설계를 지도하는 것도 유용하다.

### _Architecture_

이 섹션에서는 LLM의 아키텍처 설계, 즉 주류 아키텍처, 사전 훈련 목표 및 세부 구성을 검토한다. 표 V는 공개 세부 정보가 있는 여러 대표 LLM의 모델 카드를 보여준다.

#### 4.2.1 전형적인 아키텍처

뛰어난 병렬성과 용량으로 인해 트랜스포머 아키텍처[22]는 다양한 LLM을 개발하는 사실상의 백본이 되어 언어 모델을 수백억 또는 수천억 개의 매개변수로 확장할 수 있게 되었다. 일반적으로 기존 LLM의 주류 아키텍처는 그림 9와 같이 크게 인코더-디코더, 인과 디코더, 프리픽스 디코더의 세 가지 유형으로 분류할 수 있다.

**인코더-디코더 아키텍처.** 바닐라 트랜스포머 모델은 인코더와 디코더로 각각 두 개의 트랜스포머 블록 스택으로 구성된 인코더-디코더 아키텍처 [22]를 기반으로 합니다. 인코더는 그의 잠재 표현들을 생성하기 위해 입력 시퀀스를 인코딩하기 위해 적층된 멀티헤드 셀프-어텐션 계층들을 채택하는 반면, 디코더는 이들 표현들에 대해 교차-어텐션을 수행하고 타겟 시퀀스를 자동-순차적으로 생성한다. 인코더-디코더 PLM들(_e.g._, T5[82] 및 BART[24])은 다양한 NLP 태스크들에 대해 유효성을 보였다. 지금까지 인코더-디코더 아키텍처, _e.g._, Flan-T5[69]를 기반으로 구축된 LLM은 소수이다. 우리는 섹션 4.2.6에서 아키텍처 선택에 대한 자세한 논의를 남긴다.

**인과적 디코더 아키텍처** 인과적 디코더 아키텍처는 각 입력 토큰이 과거 토큰 및 자체에만 참석할 수 있도록 단방향 주의 마스크를 통합합니다. 입력 토큰과 출력 토큰은 디코더를 통해 동일한 방식으로 처리된다. 이 아키텍처의 대표적인 언어 모델로서, GPT-시리즈 모델[26, 55, 122]은 인과-디코더 아키텍처를 기반으로 개발된다. 특히 GPT-3 [55]는 LLM의 놀라운 상황 내 학습 능력을 보여주면서 이 아키텍처의 효과를 성공적으로 입증했다. 흥미롭게도 GPT-1 [122]와 GPT-2 [26]은 GPT-3과 같은 우수한 능력을 나타내지 않으며, 스케일링이 이 모델 아키텍처의 모델 용량을 증가시키는 데 중요한 역할을 하는 것으로 보인다. 지금까지 OPT[90], BLOOM[78], Gopher[64]와 같은 기존의 다양한 LLM에 의해 LLM의 아키텍처로 인과 디코더가 널리 채택되었다. 다음에 논의되는 인과 디코더 및 프리픽스 디코더 모두가 속한다는 점에 유의한다.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Model** & **Category** & **Size** & **Normalization** & **PE** & **Activation** & **Bias** & **\#L** & **\#H** & \(d_{model}\) & **MCL** \\ \hline GPT3 [55] & Causal decoder & 175B & Pre LayerNorm & Learned & GeLU & \(\checkmark\) & 96 & 96 & 12288 & 2048 \\ PanGU- \(\alpha\)[84] & Causal decoder & 207B & Pre LayerNorm & Learned & GeLU & \(\checkmark\) & 64 & 128 & 16384 & 1024 \\ OPT [90] & Causal decoder & 175B & Pre LayerNorm & Learned & ReLU & \(\checkmark\) & 96 & 96 & 12288 & 2048 \\ PalM [56] & Causal decoder & 540B & Pre LayerNorm & RoPE & SwiGLU & \(\times\) & 118 & 48 & 18432 & 2048 \\ BLOOM [78] & Causal decoder & 176B & Pre LayerNorm & ALiBi & GeLU & \(\checkmark\) & 70 & 112 & 14336 & 2048 \\ MT-NLG [113] & Causal decoder & 530B & - & - & - & - & 105 & 128 & 20480 & 2048 \\ Gopher [64] & Causal decoder & 280B & Pre RMSNorm & Relative & - & - & 80 & 128 & 16384 & 2048 \\ Chinchilla [34] & Causal decoder & 70B & Pre RMSNorm & Relative & - & - & 80 & 64 & 8192 & - \\ Galactic [35] & Causal decoder & 120B & Pre LayerNorm & Learned & GeLU & \(\times\) & 96 & 80 & 10240 & 2048 \\ LaMDA [68] & Causal decoder & 137B & - & Relative & GeLU & - & 64 & 128 & 8192 & - \\ Jurassic-1 [107] & Causal decoder & 178B & Pre LayerNorm & Learned & GeLU & \(\checkmark\) & 76 & 96 & 13824 & 2048 \\ LLAMA [57] & Causal decoder & 65B & Pre RMSNorm & RoPE & SwiGLU & \(\times\) & 80 & 64 & 8192 & 2048 \\ LLMaA 2 [99] & Causal decoder & 70B & Pre RMSNorm & RePE & SwiGLU & \(\times\) & 80 & 64 & 8192 & 4096 \\ Falcon [141] & Causal decoder & 40B & Pre LayerNorm & RoPE & GeLU & \(\times\) & 60 & 64 & 8192 & 2048 \\ CLM-130B [93] & Prefix decoder & 130B & Post DeepNorm & RoPE & GeCLU & \(\checkmark\) & 70 & 96 & 12288 & 2048 \\ T5 [82] & Encoder-decoder & 11B & Pre RMSNorm & Relative & ReLU & \(\times\) & 24 & 128 & 1024 & 512 \\ \hline \hline \end{tabular}
\end{table} TABLE V: Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, \(d_{model}\) denotes the size of hidden states, and MCL denotes the maximum context length during training.

도. 9: 세 가지 주류 아키텍처에서의 주의 패턴의 비교. 여기서, 청색, 녹색, 황색 및 회색의 둥근 직사각형들은 각각 접두사 토큰들 사이의 주의, 접두사와 타겟 토큰들 사이의 주의, 타겟 토큰들 사이의 주의, 마스킹된 주의들을 나타낸다.

를 포함하는 것을 특징으로 하는 복호화기 전용 아키텍쳐. "디코더 전용 아키텍처"를 언급할 때, 명시되지 않는 한, 주로 기존 문헌에서 인과적 디코더 아키텍처를 언급한다.

**접두사 디코더 아키텍처** 접두사 디코더 아키텍처(_a.k.a._, 비인과 디코더[244])는 인과 디코더의 마스킹 메커니즘을 수정하여 생성된 토큰에만 접두사 토큰[245] 및 단방향 어텐션을 통해 양방향 어텐션을 수행할 수 있습니다. 이러한 방식으로, 인코더-디코더 아키텍처와 같이, 프리픽스 디코더들은 프리픽스 시퀀스를 양방향으로 인코딩하고 출력 토큰들을 하나씩 자동회귀적으로 예측할 수 있으며, 여기서 동일한 파라미터들이 인코딩 및 디코딩 동안 공유된다. 처음부터 사전 훈련하는 대신 인과적 디코더를 지속적으로 훈련한 다음 수렴을 가속화하기 위해 접두사 디코더로 변환하는 것이 실용적인 제안이다[29], _예:_ U-PaLM[118]은 PaLM[56]에서 파생된다. 프리픽스 디코더를 기반으로 하는 기존의 대표적인 LLM으로는 GLM-130B[93]와 U-PaLM[118]이 있다.

**Mixture-of-Experts.** 위의 세 가지 유형의 아키텍처에 대해 각 입력에 대한 신경망 가중치의 하위 집합이 희박하게 활성화되는 MoE(Mixture-of-experts) 스케일링을 통해 추가로 확장할 수 있습니다. 주요 장점은 MoE가 일정한 계산 비용을 유지하면서 모델 매개변수를 확장하는 유연한 방법이라는 것이다[25]. 전문가 수 또는 전체 파라미터 크기를 증가시킴으로써 실질적인 성능 향상을 관찰할 수 있는 것으로 나타났다[246]. 장점에도 불구하고 대규모 MoE 모델을 훈련하는 것은 라우팅 작업의 복잡하고 하드 스위칭 특성으로 인해 불안정성 문제를 겪을 수 있다. MoE 기반 언어 모델의 훈련 안정성을 높이기 위해 라우팅 모듈에서 고정밀 텐서를 선택적으로 사용하거나 더 작은 범위로 모델을 초기화하는 등의 기술이 소개되었다[25]. 보다 최근에는 GPT-4가 MoE 아키텍처를 기반으로 개발되었지만 공식 검증이 없다는 추측이 널리 퍼지고 있다.

**최신 아키텍처** 기존 트랜스포머 아키텍처는 일반적으로 2차 계산 복잡성을 겪습니다. 이 때문에 긴 투입으로 훈련하고 추론을 할 때 효율성이 중요한 이슈가 되었다. 효율성을 향상시키기 위해, 일부 연구들은 파라미터화된 상태 공간 모델들(예를 들어,_S4[247], GSS[248], 및 H3[249])과 같은 긴 컨볼루션들, 및 재귀적 업데이트 메커니즘들(예를 들어,_RWKV[251] 및 RetNet[252])을 통합하는 트랜스포머-유사 아키텍처들을 포함하는 언어 모델링을 위한 새로운 아키텍처들을 고안하는 것을 목표로 한다. 이러한 새로운 아키텍처의 주요 장점은 두 가지입니다. 첫째, 이들 모델은 RNN과 같이 재귀적으로 출력을 생성할 수 있으며, 이는 디코딩 동안 단일 이전 상태를 참조하기만 하면 된다는 것을 의미한다. 기존의 트랜스포머에서와 같이 이전의 모든 상태를 다시 방문할 필요가 없기 때문에 디코딩 프로세스를 더 효율적으로 만든다. 둘째, 이 모델들은 트랜스포머와 같이 전체 문장을 병렬로 인코딩할 수 있는 능력을 가지고 있다. 이것은 토큰 단위로 문장을 인코딩해야 하는 기존의 RNN과 대조된다. 따라서, 그들은 병렬 스캔[253, 254], FFT[250, 251], 및 Chunkwise Recurrent[252]와 같은 기술들을 갖는 GPU들의 병렬성으로부터 이익을 얻을 수 있다. 이러한 기술들은 이러한 새로운 아키텍처들을 갖는 모델들이 매우 병렬적이고 효율적인 방식으로 훈련될 수 있게 한다.

#### 4.2.2 상세 구성

트랜스포머[22]의 출시 이후, 훈련 안정성, 성능 및 계산 효율을 향상시키기 위한 다양한 개선들이 제안되었다. 이 부분에서는 정규화, 위치 임베딩, 활성화 함수, 주의 및 편향 등 트랜스포머의 4가지 주요 부분에 대한 해당 구성에 대해 논의할 것이다. 이 조사를 보다 독립적으로 만들기 위해 표 VI에 이러한 구성에 대한 자세한 공식을 제시한다.

**정규화 방법** 훈련 불안정성은 사전 훈련 LLM에 대해 어려운 문제입니다. 이 문제를 완화하기 위해 정규화는 신경망의 학습을 안정화하기 위해 널리 채택되는 전략이다. 바닐라 트랜스포머[22]에는 LayerNorm[256]이 채용된다. 최근에는 LayerNorm, _e.g._, RMSNorm 및 DeepNorm의 대안으로 몇 가지 고급 정규화 기술이 제안되었다.

\(\bullet\)_LayerNorm._ 초기 연구에서 BatchNorm[265]는 일반적으로 사용되는 정규화 방법이다. 그러나 가변 길이의 시퀀스 데이터와 소규모 배치 데이터는 다루기가 어렵다. 따라서, 계층별 정규화를 수행하기 위해 LayerNorm[256]이 도입된다. 특히, 층당 모든 활성화에 대한 평균 및 분산은 최근에 계산되고 활성화를 재스케일링한다.

\(\bullet\)_RMSNorm._ LN(LayerNorm)의 학습 속도를 향상시키기 위해, RMSNorm[257]은 평균과 분산 대신에, 합산된 활성화들의 RMS(root mean square)만으로 활성화들을 재스케일링함으로써 제안된다. 관련 연구는 트랜스포머에 대한 훈련 속도 및 성능에서 그 우수성을 입증하였다[266]. RMSNorm을 채택한 대표적인 모델로는 고퍼[64]와 친칠라[34]가 있다.

\(\bullet\)_DeepNorm._ DeepNorm은 Microsoft [258]에 의해 딥 트랜스포머의 트레이닝을 안정화시키기 위해 제안되었다. DeepNorm을 잔류 연결로 사용하여 트랜스포머는 최대 1,000 레이어[258]까지 확장될 수 있으며, 이는 안정성과 우수한 성능의 이점을 보여주었다. GLM-130B[93]에 의해 채택되었다.

**정규화 위치** 정규화 방법 외에도 정규화 위치는 LLM에서 중요한 역할을 합니다. 정규화 위치에는 일반적으로 다음 LN, 이전 LN 및 샌드위치 LN의 세 가지 선택이 있다.

\(\bullet\)_Post-LN._ Post-LN은 잔차 블록 사이에 배치되는 바닐라 트랜스포머[22]에서 사용된다. 그러나, 기존의 연구는 포스트-LN을 갖는 트랜스포머의 트레이닝이 출력층 근처의 큰 구배 때문에 불안정해지는 경향이 있다는 것을 발견하였다[267]. 따라서, 포스트-LN은 다른 전략들과 결합된 것 외에는 현존하는 LLM들에서 거의 채용되지 않는다(예를 들어, GLM-130B[93]에서 포스트-LN과 프리-LN을 결합하는 것).

\(\bullet\)_Pre-LN._ Post-LN과는 다르게, pre-LN[268]이 각각의 서브-계층 전에 적용되고, 추가 LN이 최종 예측 전에 배치된다. Post-LN과 비교하여, Pre-LN을 갖는 트랜스포머는 훈련에서 더 안정적이다. 그러나, 이는 포스트-LN을 갖는 변형들보다 더 나쁜 성능을 수행한다[269]. 성능 감소에도 불구하고 대부분의 LLM은 훈련 안정성 때문에 여전히 사전 LN을 채택한다. 그러나 한 가지 예외는 100B 이상의 파라미터를 훈련시킬 때 GLM에서 pre-LN이 불안정하게 발견되었다는 것이다[93].

\(\bullet\)_Sandwich-LN_. 사전 LN을 기반으로 Sandwich-LN [255]는 트랜스포머 계층 출력의 값 폭발 문제를 피하기 위해 잔여 연결 전에 추가 LN을 추가합니다. 그러나, 샌드위치-LN은 때때로 LLM들의 트레이닝을 안정화시키지 못하고 트레이닝의 붕괴로 이어질 수 있다는 것이 발견되었다[93].

**활성화 함수** 좋은 성능을 얻으려면 피드포워드 네트워크에서도 활성화 함수를 올바르게 설정해야 합니다. 기존의 LLMs에서는 GeLU activations[270]이 널리 사용되고 있다. 특히, 최신 LLM(_e.g._, PaLM 및 LaMDA)에서는 GLU 활성화의 변이체[262, 271]도 활용되었으며, 특히 SwiGLU 및 GeGLU 변이체는 종종 실제에서 더 나은 성능을 달성한다[266]. 그러나, GeLU와 비교하여, 이들은 피드-포워드 네트워크들에서 여분의 파라미터들(약 50%)을 필요로 한다[272].

**위치 임베딩** Transformer의 자체 주의 모듈은 순열 등분산이기 때문에 위치 임베딩(PE)을 사용하여 모델링 시퀀스에 대한 절대 또는 상대 위치 정보를 주입합니다.

\(\bullet\)_절대 위치 임베딩._ 바닐라 트랜스포머[22]에서는 절대 위치 임베딩을 채용하고 있다. 인코더 및 디코더의 바닥에서, 절대 위치 임베딩들은 입력 임베딩들에 추가된다. 바닐라 트랜스포머 [22]에서 제안된 절대 위치 임베딩의 두 가지 변형, 즉 _i.e._, 정현파 및 학습된 위치 임베딩이 있으며, 여기서 후자는 기존의 사전 학습된 언어 모델에서 일반적으로 사용된다.

\(\bullet\)_상대 위치 임베딩._ 절대 위치 임베딩과 달리, 상대 위치 임베딩은 키와 쿼리 사이의 오프셋에 따라 생성된다[273]. 상대 PE의 인기 있는 변형은 트랜스포머-XL[274, 275]에 도입되었다. 키들과 쿼리들 사이의 어텐션 스코어들의 계산은 상대 위치들에 대응하는 학습가능한 임베딩들을 도입하도록 수정되었다. T5 [82]는 상대 위치 임베딩을 더 단순화하고, 이는 후속적으로 고퍼 [64]에 의해 채택되었다. 구체적으로, 주의 점수에 학습 가능한 스칼라들을 추가하는데, 여기서 스칼라들은 질의의 위치들과 키 사이의 거리들에 기초하여 계산된다. 절대 PE와 비교하여 상대적 위치 임베딩을 갖는 트랜스포머는 훈련, _즉_, 외삽[264]을 위한 시퀀스보다 긴 시퀀스로 일반화할 수 있다.

\(\bullet\)_회전 위치 임베딩._ 회전 위치 임베딩(RoPE, Rotary Position Embedding) [263]은 각 키 또는 쿼리의 절대 위치를 기반으로 특정 회전 행렬을 설정한다. 키들과 쿼리들 사이의 스코어들은 상대적인 위치 정보로 계산될 수 있다(표 VI). RoPE는 쿼리 및 키 벡터의 각 연속 요소 쌍을 _차원_으로 결합하므로 원본 \(d\)-길이 임베딩에 대한 \(d/2\) 차원이 있습니다. 각 차원 \(i\in\{1,\ldots,d/2\}\)에 대해, 관련된 요소 쌍은 회전 각도 \(t\cdot\theta_{i}\)에 기초하여 회전할 것이며, 여기서 \(t\)는 위치 인덱스를 나타내고 \(\theta_{i}\)는 차원의 기저이다. 정현파 위치 임베딩 [22] 다음에 RoPE는 _basis_\(\theta_{i}\)를 _base_\(b\)(기본적으로 \(10000\로 설정됨)의 멱승으로 정의합니다.

\[\Theta=\{\theta_{i}=b^{-2(i-1)/d}|i\in\{1,2,\ldots,d/2\}\}. \tag{4}\]

또한, 최근의 연구 [276]은 각 차원에 대해 한 사이클(\(2\pi\))을 회전시키는데 필요한 거리를 파장으로 정의한다:

\[\lambda_{i}=2\pi b^{2(i-1)/d}=2\pi/\theta_{i}. \tag{5}\]

우수한 성능과 장기 감쇠 특성으로 인해 RoPE는 최신 LLM, _e.g._, PaLM[56] 및 LLMA[57]에 널리 채택된다. RoPE에 기초하여, xPos[277]은 트랜스포머의 변환 불변 및 길이 외삽을 더욱 개선한다. 회전각 벡터의 각 차원에서 xPos는 기저가 더 클 때 더 작은 특별한 지수 감쇠를 추가한다. 거리가 멀어질수록 훈련 시 불안정한 현상을 완화할 수 있다.

\(\bullet\)_ALBi._ ALBi[264]는 Transformer의 외삽을 개선하기 위해 제안되었다. 상대 위치 임베딩과 유사하게, 그것은 그에 기초하여 벌점으로 주의 스코어들을 편향시킨다.

\begin{table}
\begin{tabular}{c|c|l} \hline \hline
**Configuration** & **Method** & **Equation** \\ \hline \multirow{3}{*}{Normalization position} & Post Norm [22] & \(\mathrm{Norm}(\mathbf{x}+\mathrm{Sublayer}(\mathbf{x}))\) \\  & Pre Norm [26] & \(\mathbf{x}+\mathrm{Sublayer}(\mathrm{Norm}(\mathbf{x}))\) \\  & Sandwich Norm [255] & \(\mathbf{x}+\mathrm{Norm}(\mathrm{Sublayer}(\mathrm{Norm}(\mathbf{x})))\) \\ \hline \multirow{3}{*}{Normalization method} & LayerNorm [256] & \(\frac{\mathbf{x}-\mu}{\sigma}\cdot\gamma+\beta\), \(\mu=\frac{1}{d}\sum_{i=1}^{d}x_{i}\), \(\sigma=\sqrt{\frac{1}{2}\sum_{i=1}^{d}(x_{i}-\mu))^{2}}\) \\  & RMSNorm [257] & \(\frac{\mathbf{x}}{\mathrm{N}(\mathbf{x})}\cdot\gamma\), \(\mathrm{RMS}(\mathbf{x})=\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_{i}^{2}}\) \\  & DeepNorm [258] & LayerNorm \((\alpha\cdot\mathbf{x}+\mathrm{Sublayer}(\mathbf{x}))\) \\ \hline \multirow{3}{*}{Activation function} & ReLU [259] & \(\mathrm{ReLU}(\mathbf{x})=\max(\mathbf{x},\mathbf{0})\) \\  & GeLU [260] & \(\mathrm{GeLU}(\mathbf{x})=0.5\mathbf{x}\otimes[1+\mathrm{erf}(\mathbf{x}/ \sqrt{2})]\), \(\mathrm{erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt\) \\  & Swish [261] & \(\mathrm{Swish}(\mathbf{x})=\mathbf{x}\otimes\mathrm{sigmoid}(\mathbf{x})\) \\  & SwiGLU [262] & \(\mathrm{SwiGLU}(\mathbf{x}_{1},\mathbf{x}_{2})=\mathrm{Swish}(\mathbf{x}_{1} )\otimes\mathbf{x}_{2}\) \\  & GeGLU [262] & \(\mathrm{GeGLU}(\mathbf{x}_{1},\mathbf{x}_{2})=\mathrm{GeLU}(\mathbf{x}_{1}) \otimes\mathbf{x}_{2}\) \\ \hline \multirow{3}{*}{Position embedding} & Absolute [22] & \(\mathbf{x}_{i}=\mathbf{x}_{i}+\mathbf{x}_{i}\) \\  & Relative [82] & \(A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}+r_{i-j}\) \\  & RoPE [263] & \(A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{R}_{\Theta,i-j}\mathbf{x}_{j}^{T} \mathbf{W}_{k}^{T}=(\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{R}_{\Theta,i})( \mathbf{W}_{k}\mathbf{x}_{j}R_{\Theta,j})^{T}\) \\  & ALBi [264] & \(A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}-m(i-j)\) \\ \hline \hline \end{tabular}
\end{table} TABLE VI: Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, \(d\) denotes the size of hidden states, \(\mathbf{p}_{i}\) denotes position embedding at position \(i\), \(A_{ij}\) denotes the attention score between a query and a key, \(r_{i-j}\) denotes a learnable scalar based on the offset between the query and the key, and \(\mathbf{R}_{\Theta,t}\) denotes a rotary matrix with rotation degree \(t\cdot\Theta\).

키와 쿼리 간의 거리입니다. T5[82]와 같은 상대적 위치 임베딩 방법과 달리 ALBi의 벌점 점수는 훈련 가능한 매개변수 없이 미리 정의된다. [264]의 경험적 결과들은 ALBi가 정현파 PE[22], RoPE[263], 및 T5 바이어스[82]와 같은 몇몇 대중적인 위치 임베딩 방법들보다 트레이닝을 위한 것보다 긴 시퀀스들에 대해 더 나은 외삽 성능을 갖는다는 것을 보여주었다. 또한, ALBi도 BLOOM[78]에서 트레이닝 안정성을 향상시킬 수 있는 것으로 나타났다.

**주의** Attention mechanism은 Transformer의 중요한 구성요소이다. 그것은 시퀀스에 걸친 토큰들이 서로 상호작용하고 입력 및 출력 시퀀스의 표현들을 계산할 수 있게 한다.

\(\bullet\)_Full attention_. 바닐라 트랜스포머[22]에서 어텐션 메커니즘은 시퀀스의 모든 토큰 쌍 간의 관계를 고려하여 쌍별 방식으로 수행된다. 숨겨진 상태를 쿼리, 키 및 값으로 매핑하는 스케일링된 dot-product attention를 채택한다. 또한 Transformer는 단일 어텐션 대신 다중 헤드 어텐션을 사용하여 쿼리, 키 및 값을 서로 다른 헤드에 투영합니다. 각 헤드의 출력의 연결은 최종 출력으로 간주된다.

\(\bullet\)_Sparse attention_. 완전한 관심의 중요한 과제는 긴 수열을 다룰 때 부담이 되는 2차 계산 복잡도이다. 따라서, 어텐션 메커니즘의 계산 복잡도를 감소시키기 위해 다양한 효율적인 트랜스포머 변형들이 제안된다[278, 279]. 예를 들어, 국부적으로 밴딩된 희소 주의력(_i.e._, Factorized Attention[280])은 GPT-3[55]에서 채택되었다. 전체 시퀀스 대신 각 쿼리는 위치에 따라 토큰의 하위 집합에만 참석할 수 있습니다.

\(\bullet\)_Multi-query/grouped-query attention_. 다중 쿼리 주의는 키 및 값에서 서로 다른 헤드가 동일한 선형 변환 행렬을 공유하는 주의 변형(attention variant)을 지칭한다[281]. 모델 품질에서 약간의 희생만으로 더 높은 추론 속도를 달성합니다. 다중 쿼리 주의를 갖는 대표적인 모델로는 PaLM[56] 및 StarCoder[98]가 있다. 멀티-쿼리 어텐션과 멀티-헤드 어텐션 사이의 트레이드오프를 만들기 위해, 그룹화된-쿼리 어텐션(GQA)[282]이 탐색되었다. GQA에서 헤드는 다른 그룹으로 할당되며 동일한 그룹에 속하는 헤드는 동일한 변환 행렬을 공유한다. 특히, GQA는 최근에 발표된 LLaMA 2 모델[99]에서 채택되고 경험적으로 테스트되었다.

\(\bullet\)_FlashAttention_. 플래시 어텐션[283]은 컴퓨팅 효율성을 향상시키기 위해 모델 품질을 트레이드오프하는 대부분의 기존 근사 어텐션 방법과 달리 IO 인식 관점에서 GPU에 대한 어텐션 모듈의 속도와 메모리 소비를 최적화하는 것을 제안한다. 현대 GPU에는 다양한 수준의 메모리가 있으며, 예를 들어 빠른 IO를 가진 SRAM과 상대적으로 느린 IO를 가진 HBM이 있다. 플래시 어텐션은 빠른 메모리 SRAM을 더 잘 사용하기 위해 입력을 블록으로 구성하고 필요한 재계산을 도입한다. CUDA에서 융합 커널로 구현된 FlashAttention는 PyTorch[197], DeepSpeed[74], Megatron-LM[75]에 통합되었다. 업데이트된 버전 FlashAttention-2[284]는 GPU 스레드 블록 및 워프의 작업 분할을 더욱 최적화하여 원래 FlashAttention에 비해 약 2\(\times\)의 속도 향상을 가져온다.

\(\bullet\)_PagedAttention_. LLM이 서버에 배포될 때 GPU 메모리가 캐시된 주의 키 및 값 텐서( _KV 캐시_라고 함)에 의해 크게 점유되는 것이 관찰되었습니다. 주요 이유는 입력 길이가 종종 다양하여 단편화 및 과잉 예약 문제로 이어지기 때문이다. 운영 체제에서 고전적인 페이징 기술에 영감을 받아, PagedAttention는 배치된 LLMs의 메모리 효율 및 처리량을 개선하기 위해 제안되었다[285]. 세부적으로, PagedAttention는 각 시퀀스를 서브시퀀스로 분할하고, 이들 서브시퀀스의 대응하는 KV 캐시는 비연속 물리 블록으로 할당된다. 페이징 기법은 GPU 활용도를 높이고 병렬 샘플링에서 효율적인 메모리 공유를 가능하게 한다.

이 모든 논의를 종합하기 위해 세부 구성을 위한 기존 문헌의 제안을 요약한다. 더 강력한 일반화 및 훈련 안정성을 위해 계층 정규화를 위한 사전 RMSNorm을 선택하고 활성화 함수로 SwiGLU 또는 GeGLU를 선택하는 것이 제안된다. 또한, LN은 레이어 임베딩 직후에 사용되지 않을 수 있으며, 이는 성능 저하를 초래할 가능성이 있다. 위치 임베딩의 경우 RoPE 또는 ALBi가 긴 시퀀스에서 더 잘 수행되기 때문에 더 나은 선택이다.

#### 4.2.3 사전 훈련 작업

사전 훈련은 대규모 코퍼스에서 대규모 모델 매개변수로 일반 지식을 인코딩하는 중요한 역할을 한다. LLM 트레이닝을 위해, 일반적으로 사용되는 사전 트레이닝 태스크들, 즉 언어 모델링 및 노이즈 제거 오토인코딩이 있다.

**언어 모델링** 언어 모델링 작업(LM)은 디코더 전용 LLM, _e.g._, GPT3 [55] 및 PaLM [56]을 사전 훈련하는 데 가장 일반적으로 사용되는 목표입니다. 토큰들 \(\mathbf{x}=\{x_{1},\dots,x_{n}\}\)의 시퀀스가 주어지면, LM 태스크는 시퀀스 내의 선행 토큰들 \(x_{<i}\)에 기초하여 타겟 토큰들 \(x_{i}\)을 자동으로 예측하는 것을 목표로 한다. 일반적인 훈련 목표는 다음과 같은 가능성을 최대화하는 것이다:

\[\mathcal{L}_{LM}(\mathbf{x})=\sum_{i=1}^{n}\log P(x_{i}|\mathbf{x}_{<i}). \tag{6}\]

대부분의 언어 태스크들은 입력에 기초하여 예측 문제로서 캐스팅될 수 있기 때문에, 이러한 디코더-전용 LLM들은 통합 LM 방식으로 이러한 태스크들을 달성하는 방법을 암묵적으로 학습하는데 잠재적으로 유리할 수 있다. 일부 연구는 또한 디코더 전용 LLM이 미세 조정 없이 다음 토큰 [26, 55]을 자동으로 예측함으로써 특정 태스크로 자연스럽게 전달될 수 있다는 것을 밝혀냈다. LM의 중요한 변형은 접두사 디코더 아키텍처를 사용하여 사전 훈련 모델을 위해 설계된 _접두사 언어 모델링_ 작업이다. 임의로 선택된 접두사 내의 토큰은 접두사 언어 모델링의 손실을 계산하는 데 사용되지 않는다. 사전 트레이닝 동안 보여지는 동일한 양의 토큰들로, 프리픽스 언어 모델링은 언어 모델링보다 약간 더 나쁜 성능을 수행하는데, 이는 시퀀스 내의 더 적은 토큰들이 모델 사전 트레이닝에 관여하기 때문이다[29].

**디노이징 오토인코딩** 기존 LM 외에도 디노이징 오토인코딩 태스크(DAE)는 언어 모델을 사전 훈련하는 데 널리 사용되었습니다[82, 24]. DAE 작업에 대한 입력 \(\mathbf{x}_{\setminus\hat{\mathbf{x}}}\)은 무작위로 교체된 스팬으로 손상된 텍스트이다. 그런 다음, 대체된 토큰 \(\tilde{\mathbf{x}}\)을 복구하도록 언어 모델을 학습한다. 형식적으로, DAE의 트레이닝 목적은 다음과 같이 표시된다:

\[\mathcal{L}_{DAE}(\mathbf{x})=\log P(\tilde{\mathbf{x}}|\mathbf{x}_{\setminus \tilde{\mathbf{x}}}). \tag{7}\

그러나 DAE 작업은 LM 작업보다 실행이 더 복잡해 보인다. 그 결과, 대형 언어 모델을 사전 훈련하는 데 널리 사용되지 않았다. DAE를 사전 훈련 목표로 하는 기존의 LLM에는 T5[82]와 GLM-130B[93]가 있다. 이러한 모델은 주로 자기회귀 방식으로 교체된 경간을 복구하도록 훈련된다.

**Mixture-of-Denoisers.** UL2 손실이라고도 하는 MoD(Mixture-of-Denoisers) [89]는 사전 훈련 언어 모델에 대한 통일된 목표로 도입되었습니다. MoD는 LM과 DAE 목표를 모두 다른 유형의 노이즈 제거 작업, 즉 S-denoiser(LM), R-denoiser(DAE, 짧은 스팬 및 낮은 부패), X-denoiser(DAE, 긴 스팬 또는 높은 부패)로 간주한다. 세 가지 노이즈 제거 작업 중 S-denoiser는 기존의 LM 목적(식 (6))과 유사한 반면 R-denoiser와 X-denoiser는 DAE 목적(식 (7))과 유사하지만 스팬의 길이와 손상된 텍스트의 비율에서 서로 다르다. 상이한 특수 토큰들(_i.e._, {[R],[S],[X]})로 시작된 입력 문장들에 대해, 모델은 대응하는 디노이저들을 사용하여 최적화될 것이다. MoD는 최신 PaLM 2 모델[120]에 적용되었다.

#### 4.2.4 Long Context Modeling

실제 응용에서, PDF 처리 및 스토리 작성과 같은 LLM의 긴 컨텍스트 모델링 용량에 대한 요구가 증가하고 있다[286]. 많은 폐쇄 소스 LLM은 긴 텍스트 처리를 위한 전문적인 지원을 제공한다. 예를 들어 OpenAI는 128K 컨텍스트 창을 가진 GPT-4 Turbo를, Anthropic은 200K 컨텍스트 창을 가진 Claude 2.1을 방출한다. 긴 컨텍스트 모델링 능력을 향상시키기 위해 일반적으로 위치 임베딩의 크기 조정과 컨텍스트 윈도우의 적응이라는 두 가지 실행 가능한 방향이 있다. 다음으로, 두 부분에 대해 상세히 소개한다.

**위치 임베딩 크기 조정.** 트랜스포머 기반 LLM은 최대 학습 길이 내에서 효과적인 위치 임베딩을 학습할 수 있습니다. 따라서, LLM들을 최대 트레이닝 길이를 넘어 언어 태스크들에 적응시킬 때, 더 큰 포지션 인덱스들로 스케일링하는 것이 필요하다. 일부 특정 위치 임베딩은 T5 바이어스[82], ALiBi[264], xPos[277] 및 심지어 NoPE[287]을 포함하여 형식적으로 _외삽 능력_이라고 하는 훈련 길이를 초과하는 텍스트에 일반화하는 어느 정도의 능력을 갖는 것으로 나타났다. 그러나, 주류의 위치 임베딩 방법의 하나로서 RoPE는 경험적 연구에서 제한된 외삽 능력을 나타낸다[240]. 아래에서는 RoPE를 더 긴 텍스트로 확장할 수 있는 몇 가지 방법에 대해 논의한다.

\(\bullet\)_Direct 모델 미세 조정._ LLM을 긴 컨텍스트 창에 적용하기 위해 간단한 접근법은 원하는 길이로 긴 텍스트에서 모델을 직접 미세 조정하는 것이다. 컨텍스트 확장은 다단계 접근법(_e.g._, \(2\text{K}\to 8\text{K}\to 32\text{K}\to)에서 증가된 길이로 스케줄링될 수 있다. 효과적인 확장을 수행하기 위해서는 훈련을 위해 특별히 준비된 긴 텍스트가 필요하다. 특히, 최근 일부 연구에서는 긴 컨텍스트 모델에서 학습 텍스트의 길이보다 품질이 더 중요하다는 것을 보여주었다[288]. 그러나 최근 연구에서는 긴 텍스트에 LLM을 적용할 때 미세 조정 접근법이 본질적으로 느린 경향이 있음을 강조했다[240].

\(\bullet\)_위치 보간._ 이 방법은 사전 트레이닝 동안 분포 외 회전 각도들을 피하기 위해, 원래 컨텍스트 윈도우 내의 위치 인덱스들을 다운스케일링한다[240, 289]. 보다 구체적으로, 이 접근법은 모든 위치 인덱스에 계수 \(L/L^{\prime}\)(\(L<L^{\prime}\)), 여기서 \(L\) 및 \(L^{\prime}\)는 각각 원래 및 목표 컨텍스트 윈도우 길이를 나타낸다. 실험 결과 [240]은 이 방법이 직접 모델 미세조정의 상기 접근법과 비교하여, 컨텍스트 윈도우를 효과적이고 효율적으로 확장할 수 있음을 보여주었다. 그러나 이 기법은 더 짧은 텍스트를 처리할 때 모델의 성능에 부정적인 영향을 미칠 수 있다는 점에 주목할 필요가 있다[240, 290].

\(\bullet\)_위치 절단._ 분포 외 회전 각도들에 의해 제기되는 도전들을 완화하기 위해, 또 다른 실용적인 접근법은 최대 트레이닝 길이의 요건을 만족시키기 위해 더 긴 상대 위치들을 절단하는 것이다. 구체적으로, ReRoPE 및 LeakyReRoPE[291]은 미리 정의된 윈도우 길이를 도입하는데, 이는 최대 트레이닝 길이보다 작다. 이 미리 정의된 윈도우 내의 위치 인덱스는 유지되는 반면, 윈도우를 벗어난 인덱스들은 미리 정의된 윈도우 길이로 잘려지거나 최대 트레이닝 길이와 정렬되도록 보간된다. 이 전략은 지역 위치 관계를 예약하고 외삽 능력을 향상시킬 수 있다. 그러나, 이 방법은 추가적인 계산 예산을 수용하여 주의 행렬을 두 번 계산해야 한다.

\(\bullet\)_Base 수정._ LLM은 일반적으로 Llama 2 [99]에서 미리 설정된 최대 훈련 길이 _예._, 4096으로 훈련된다. 그러나, RoPE의 특정 차원에서의 파장은 더 긴 텍스트[276]에 대한 트레이닝 길이를 초과할 수 있어서, 언어 모델들은 이러한 차원들에 대한 충분한 트레이닝(_즉, 완전한 회전 사이클)을 거치지 않았다. 따라서 LLM을 더 긴 텍스트에 적용할 때 특정 차원에 대한 회전 각도는 훈련 단계에서 볼 수 없다[292]. 고정 회전각 \(t\cdot\theta_{i}\), 더 작은 기저 \(\theta_{i}\)는 더 큰 거리 \(t\), _i.e._를 허용하여 더 긴 텍스트를 모델링할 수 있다[235, 276, 288]. 식 4의 공식 \(\theta_{i}=b^{-2(i-1)/d}\)에 따르면, 기저의 감소는 기저의 값을 증가시킴으로써 달성될 수 있다. 또한, 베이스를 감소시키는 것은 또한 트레이닝 길이 미만의 모든 차원들의 파장들을 재스케일링하는 것을 도울 수 있는 반면, LLMs들을 긴 컨텍스트 윈도우들에 적응시키기 위해 종종 지속적인 사전 트레이닝이 필요하다[292]. 최근 연구[292]는 이 두 가지 염기 수정 방법을 경험적으로 비교했으며, 염기 감소를 통해 더 나은 외삽을 입증한다는 것을 보여주었다.

도. 10: 문맥의 다음 토큰에 대한 내림차순으로 어휘에 대한 확률 분포 "_I am sleepy. I start a pot of"_. 논의의 용이성을 위해, 이 예는 서브워드 유닛 대신에 워드 유닛으로 주어진다.

트레이닝 길이를 초과하는 용량을 갖는 반면, 베이스를 증가시키는 것은 트레이닝 길이 내에서 더 나은 성능을 수행한다.

\(\bullet\)_기본 절단._ 베이스 수정과 유사하게, 기저의 절단은 또한 트레이닝 길이를 초과하는 파장을 갖는 단수 치수들을 다루는 것에 집중한다[293]. 식 5의 정의 \(\lambda_{i}=2\pi/\theta_{i}\)에 따르면, 파장이 큰 차원 \(\lambda_{i}\)은 그에 따라 기저 \(\theta_{i}\)가 작다. 이러한 관찰에 기초하여, 이 접근법은 먼저 기저 범위 \([a,c]\)를 정의한다. 기저 범위가 주어지면 기저 값은 다음과 같은 방법에 따라 수정된다: (1) \(\theta_{i}\geq c\) 값이 유지될 때, (2) \(\theta_{i}\leq a\) 값이 0으로 설정될 때, 그리고 (3) \(a<\theta_{i}<c\) 값이 고정된 작은 값으로 잘려질 때. 기저 절단을 통해 더 큰 위치 지수에서 분포 외 회전 각도를 피할 수 있다. 그러나, 이러한 접근법은 긴 컨텍스트 태스크들에서 매우 잘 수행되지 않는다[293].

**컨텍스트 창 적응** Transformer 기반 LLM은 컨텍스트 창이 제한적이기 때문에 컨텍스트 창을 초과하는 긴 시퀀스의 전체 정보를 직접 통합하거나 활용할 수 없습니다. 제한을 완화하기 위해 아래에서 논의되는 바와 같이 LLM을 긴 컨텍스트에 적응시키는 몇 가지 방법이 제안되었다.

\(\bullet\)_Parallel context window._ 융합-인-디코더[294]에 의해 영감을 받은 병렬 컨텍스트 윈도우 방법들[295, 296]은 입력 텍스트를 처리하기 위해 분할-정복 전략을 채택한다. 특히, 입력 텍스트를 다수의 세그먼트로 분할하고, 각각 독립적으로 공유 위치 임베딩으로 인코딩한다. 생성 단계에서, 어텐션 마스크들은 후속 토큰들이 각각의 세그먼트에서의 이전 토큰들에 액세스할 수 있도록 수정된다. 그럼에도 불구하고, 이 방법은 특정 태스크들에 대한 모델 용량을 제약하면서 상이한 세그먼트들의 순서를 구별할 수 없다.

\(\bullet\)_\(\Lambda\) 모양의 컨텍스트 창._ 일부 선행 연구에서는 LLM이 모든 이전 토큰 중 시작 및 가장 가까운 토큰에 더 큰 주의 가중치를 할당하는 경향이 있음을 보여주었다. 이 관찰을 기반으로 LM-Infinite [300] 및 StreamingLLM [298]은 "\(\Lambda\" 모양의 주의 마스크를 사용할 것을 제안하며, 이는 초기 토큰 및 각 쿼리가 참석할 수 있는 가장 가까운 토큰을 선택적으로 보존한 다음 이 범위를 초과하는 모든 토큰을 폐기한다. 실험들은 이 방법이 고정된 메모리로 여분의 긴 텍스트 생성을 용이하게 할 수 있음을 입증한다[298]. 그러나 폐기된 토큰의 정보를 효과적으로 활용할 수 없기 때문에 프롬프트에서 장거리 종속성을 모델링하는 데 어려움을 겪을 수 있다[298].

\(\bullet\)_External memory._ 비교적 작은 토큰의 하위 집합은 트랜스포머 [301]에서 대부분의 주의 패턴을 효과적으로 캡처할 수 있습니다. 즉, 최상위 \(k\) 주의 키는 원래 전체 주의에 잘 근접할 수 있습니다. 따라서, 다수의 연구들은 과거 키들을 외부 메모리에 저장하고, \(k\)-NN 검색 방법을 사용하여 생성에 가장 관련된 \(k\) 토큰들을 검색하는 것을 제안한다[302, 301, 238]. 디코더 모델의 경우, 일반적으로 하나의 특정 계층을 사용하여 이러한 상위-\(k\) 외부 토큰에 액세스하는 반면, 나머지 계층에서는 여전히 정상 컨텍스트 창을 채택한다[302, 238].

바닐라 트랜스포머를 기반으로 한 연구 외에도, 긴 텍스트를 모델링하기 위한 높은 계산 비용을 완화하기 위해 효율적인 관심 및 기타 효율적인 아키텍처를 가진 트랜스포머 변형이 급증하고 있다. 이러한 연구들은 섹션 4.2.1 및 섹션 4.2.2에서 광범위하게 논의되었다. 또한, 컨텍스트 압축 및 프롬프트 기술들(예를 들어,_반복 추론[303])은 또한 모델 적응의 필요 없이 긴 텍스트 태스크들[304, 305, 306, 303]을 처리하기 위한 실행가능한 전략인 것으로 입증되었다.

#### 4.2.5 디코딩 전략

LLM들이 사전 트레이닝된 후, LLM들로부터 적절한 출력을 생성하기 위해 특정 디코딩 전략을 채용하는 것이 필수적이다.

**배경.** 일반적인 디코더 전용 아키텍처로 논의를 시작하고 자동 회귀 디코딩 메커니즘을 소개합니다. 이러한 LLM들은 언어 모델링 작업(수학식 6)에 기초하여 미리 트레이닝되기 때문에, 기본 디코딩 방법은 이전에 생성된 토큰들을 기초로 각 단계에서 가장 가능성이 높은 토큰을 예측하는 _greedy search_로서, 정형적으로 모델링된다:

\[x_{i}=\operatorname*{arg\,max}_{x}\!P(x|\mathbf{x}_{<i}), \tag{8}\]

여기서, \(x_{i}\)는 컨텍스트 \(\mathbf{x}_{<i}\)를 조건으로 하는 생성 단계 \(i\)에서 가장 높은 확률을 갖는 토큰이다. 예를 들어 그림 10에서 문장의 다음 토큰을 예측할 때 _"졸립니다. _의 냄비를 시작할 때 탐욕스러운 검색은 현재 단계에서 가장 높은 확률을 가진 토큰 "커피"를 선택합니다. 그리디 검색은 텍스트 생성 작업(예를 들어,_기계 번역 및 텍스트 요약)에서 만족스러운 결과를 달성할 수 있으며, 여기서 출력은 입력에 크게 의존한다[307]. 그러나, 개방형 생성 태스크들(예컨대,_스토리 생성 및 대화)의 관점에서, 탐욕스러운 검색은 때때로 어색하고 반복적인 문장들을 생성하는 경향이 있다[308].

다른 대안적인 디코딩 전략으로서, 생성 동안 랜덤성 및 다양성을 향상시키기 위해 확률 분포에 기초하여 다음 토큰을 랜덤하게 선택하는 샘플링-기반 방법들이 제안된다:

\[x_{i}\sim P(x|\mathbf{x}_{<i}). \tag{9}\]

그림 10의 예에 대해 샘플링 기반 방법은 더 높은 확률로 단어 "커피"를 샘플링하는 동시에 나머지 단어 "물", "차", "쌀", _등"을 선택할 가능성을 유지한다._

디코더 전용 아키텍처에 제한되지 않고, 이 두 디코딩 방법은 일반적으로 유사한 방식으로 인코더-디코더 모델 및 프리픽스 디코더 모델에 적용될 수 있다.

**탐욕 검색에 대한 개선** 각 단계에서 가장 높은 확률을 가진 토큰을 선택하면 전체 확률은 높지만 로컬 추정치는 낮은 문장을 간과할 수 있습니다. 다음으로 이 문제를 완화하기 위한 몇 가지 개선 전략을 소개한다.

\(\bullet\)_Beam search._ 빔 탐색[309]은 디코딩 과정 동안 각 단계에서 \(n\)(빔 크기)의 확률이 가장 높은 문장을 유지하고, 최종적으로 생성된 응답을 최상위 확률로 선택한다. 일반적으로, 빔 크기는 3 내지 6의 범위 내에서 구성된다. 그러나, 더 큰 빔 크기를 선택하는 것은 성능 저하를 초래할 수 있다[310].

\(\bullet\)_Length penalty._ 빔 탐색은 더 짧은 문장을 선호하기 때문에, 이 문제를 극복하기 위해 일반적으로 사용되는 기법인 길이 벌점 부과(_a.k.a._, 길이 정규화)는 문장 길이에 따라 문장 확률을 정규화하는 기법이다[311].

또한, 일부 연구자[312]는 반복 생성의 문제를 완화하기 위해 이전에 생성된 토큰 또는 \(n\)-그램의 생성에 불이익을 줄 것을 제안한다. 또한, 다양한 빔 탐색[313]은 동일한 입력에 기초하여 다양한 출력들의 세트를 생성하기 위해 레버리지될 수 있다.

**랜덤 샘플링에 대한 개선** 샘플링 기반 메서드는 전체 어휘에 걸쳐 토큰을 샘플링하며, 이는 컨텍스트에 따라 잘못되거나 관련 없는 토큰(그림 10의 _예:_, "행복" 및 "보")을 선택할 수 있습니다. 생성 품질을 개선하기 위해 확률이 매우 낮은 단어의 선택을 완화하거나 방지하기 위한 몇 가지 전략이 제안되었다.

\(\bullet\)_Temperature sampling._ 샘플링의 랜덤성을 변조하기 위해, 실용적인 방법은 어휘에 걸쳐 \(j\)-번째 토큰의 확률을 계산하기 위한 소프트맥스 함수의 온도 계수를 조정하는 것이다:

\[P(x_{j}|\mathbf{x}_{<i})=\frac{\exp{(l_{j}/t)}}{\sum_{j^{\prime}}\exp{(l_{j^{ \prime}}/t)}}, \tag{10}\]

여기서 \(l_{j^{\prime}}\)는 각 단어의 로짓이고 \(t\)는 온도 계수이다. 온도 \(t\)를 줄이면 확률이 높은 단어를 선택할 확률이 높아지는 반면 확률이 낮은 단어를 선택할 확률은 낮아진다. \(t\)가 1로 설정되면 기본 랜덤 샘플링이 되며, \(t\)가 0에 가까워지면 탐욕 검색과 동일합니다. 또한, \(t\)가 무한대로 가면, 균일 샘플링으로 퇴화된다.

\(\bullet\)_Top-\(k\) 샘플링._ 온도 샘플링과 달리 상위 \(k\) 샘플링은 낮은 확률로 토큰을 직접 잘라내고 상위 \(k\) 가장 높은 확률을 가진 토큰에서 샘플만 잘라낸다[314]. 예를 들어 그림 10에서 상위\(5\) 샘플링은 "커피", "물", "차", "쌀" 및 "차이"라는 단어를 재스케일된 확률에서 샘플링할 것이다.

\(\bullet\)_Top-\(p\) 샘플링._ Top-\(k\) 샘플링은 전체 가능성 분포를 고려하지 않기 때문에 \(k\)의 상수 값은 서로 다른 컨텍스트에 적합하지 않을 수 있다. 따라서, 상위 \(p\) 샘플링(_a.k.a._, 핵 샘플링)은 상기(또는 동일) \(p\)[308] 이상의 누적 확률을 갖는 가장 작은 집합으로부터 샘플링함으로써 제안된다. 실제로 가장 작은 집합은 생성 확률의 내림차순으로 정렬된 어휘에서 누적 값이 \(p\)를 초과할 때까지 토큰을 점진적으로 추가하여 구성할 수 있다.

최근 연구자들은 LLM에 대한 다른 샘플링 전략도 탐구했다. 예를 들어, \(\eta\)_-sampling_[315]는 확률 분포에 기초한 동적 임계치를 도입함으로써 top-\(p\)sampling을 더욱 개선한다. 또한, 대조적 검색[316] 및 전형적인 샘플링[317]은 디코딩 동안 생성 코히어런스를 개선하는데 활용될 수 있다. 큰 모델은 작은 모델에 비해 중요한 토큰에 더 높은 확률을 할당하는 경향이 있는 것으로 밝혀졌기 때문에 _대비 디코딩_[318]은 로그 우도 차이를 측정하기 위해 더 큰 LM(_e.g._, OPT-13B)과 더 작은 LM(_e.g._, OPT-125M)을 활용한다. 이어서 확률분포의 델타 값을 기준으로 토큰을 샘플링하여 중요 토큰의 영향을 증폭시킨다. 이 대조적인 아이디어에 기초하여, DoLa[319]는 상위 계층들이 중요한 토큰들에 더 많은 가중치를 할당하는 경향이 있기 때문에, 단일 LLM의 상이한 계층들에 걸쳐 로짓들을 대조하는 것으로 이 접근법을 추가로 확장한다.

\(\bullet\)_Towards Wall_

새 토큰을 생성할 때 가장 많은 시간이 걸리는 단계는 데이터 전송 및 가중치 계산을 중심으로 진행됩니다. 주요 문제는 종종 _메모리 벽_ 문제라고 하는 데이터 전송에 의해 압도되는 상당한 시간입니다.

이 문제를 해결하기 위해 연구자들은 I/O의 바이트 수를 사용하여 GPU 메모리에서 GPU 캐시로의 데이터 전송을 공식적으로 정량화하고 FLOP의 수를 측정하여 가중치 계산을 평가한다[320]. 구체적으로, \(b\), \(s\), \(n\), \(d\) 및 \(h\)는 배치 크기, 시퀀스 길이, 주의 헤드 수, 각 헤드의 숨겨진 크기 및 전체 숨겨진 크기(\(h=n\cdot d\))를 나타낸다. 인과 디코더에서 계층별 멀티헤드 셀프 어텐션 계산 시, 각 디코딩 단계에서의 입출력 바이트와 FLOP는 각각 \(8bsn+4bsnd+4bnd\)와 \(8bsnd\)로 표현될 수 있다[320].

_산술 강도_는 I/O 바이트들에 대한 FLOP들의 비율로서 추가로 정의된다:

\[\text{intensity}=\frac{\text{FLOPs}}{\text{I/O bytes}}=\frac{2}{1+\frac{2}{d}+ \frac{1}{s}} \tag{11}\]

열 길이가 1024(\(s=1024\))인 LLaMA 13B(\(d=128\))를 예로 들어보자. 계산된 산술 강도는 \(1.97\). 그러나 A100 80G GPU는 \(312\) TFLOP를 수행하고 1초에 2 TB의 데이터를 전송할 수 있으며, 이상적인 연산 강도는 \(156\)이다. 이는 주의력 계산의 병목 현상이 데이터 전송 과정(_i.e._, 과도한 I/O 로딩)에 있음을 나타낸다.

**디코딩 효율성 문제** 이 부분에서는 LLM의 디코딩 효율성 문제를 간략하게 분석 합니다. 전반적으로, LLM의 디코딩 프로세스는 오버헤드 분석을 위해 (1) 입력 시퀀스의 히든 상태들을 계산하는 _프리필_ 스테이지와 (2) 토큰을 생성하고 오토-레그레시브 방식으로 히든 상태들을 업데이트하는 _증가 디코딩_ 스테이지의 두 단계로 나누어질 수 있다[321]. 위의 _메모리 벽_ 상자에 표시된 것처럼 증분 디코딩 단계의 산술 강도는 \(1.97\)에 불과하여 예상 값 156(A100 80GB GPU의 표준 구성에 따라 계산됨)과 거리가 멀다. 반면, LLaMA-13B의 경우 프리필 단계의 산술 강도가 \(113.78\)을 달성하였다. 결과적으로, 기존 연구는 주로 증분 디코딩 알고리즘의 효율성을 향상시키는 방법을 조사하는데, 이는 크게 두 가지 접근법으로 분류될 수 있다:

\(\bullet\)_데이터 전송 감소_는 주로 GPU 메모리 액세스를 최적화하여 산술 강도를 높이는 데 중점을 둡니다. 섹션 4.2.2에서 소개된 바와 같이, KV 캐시는 이전 토큰들의 중복 계산을 피할 수 있고, PagedAttention는 메모리 단편화를 감소시키기 위해 KV 캐시들을 연속적인 블록들에 할당한다. 또한 Flash-Decoding [322]는 키와 값을 병렬로 로드하여 주의력 계산을 빠르게 하며, 특히 긴 텍스트 생성에 효과적이다. 다른 대안적 접근법으로서, 다중-쿼리 및 그룹화된-쿼리 주의는 KV 파라미터들을 공유함으로써 GPU 메모리 대역폭 오버헤드를 감소시킬 수 있다(더 적은 가중치들을 로딩함).

\(\bullet\)_디코딩 전략 최적화_는 서로 다른 방식으로 자동 회귀 생성 방식의 순차적 특성을 개선하는 것을 목표로 한다. 대표적인 연구로서 _추측 디코딩_[323, 324]는 먼저 작지만 효율적인 모델(_e.g._, \(n\)-gram 모델 또는 작은 PLM)을 활용하여 짧은 세그먼트를 생성한 다음 LLM을 활용하여 이러한 드래프트를 확인하고 수정한다. 이는 발전 품질을 저하시키지 않으면서 \(2\times\)에서 \(3\times\)의 속도 향상을 가져올 수 있다. 연구자들은 또한 이 접근법의 효율성을 개선하기 위해 여러 소형 모델들을 결합하는 학습 기반 방법[325] 및 소형 LM을 먼저 가속하기 위해 더 작은 LM을 사용하는 단계적 가속과 같은 몇 가지 변형들을 제안한다[326]. 또한, 모든 계층을 경유하지 않고, 하위 트랜스포머 계층에서 토큰을 생성할 수 있도록 하는 토큰 수준의 조기 종료 기법들이 제안되었다[327]. 그것은 더 큰 속도 향상을 달성할 수 있지만, 발전 품질을 희생하는 대가를 치르게 된다.

**실제 설정** 실제로 기존 라이브러리(_e.g._, Transformers [187]) 및 LLM의 공용 API(_e.g._, OpenAI)는 텍스트 생성의 다양한 시나리오를 제공하기 위해 다양한 디코딩 전략을 지원했습니다. 다음으로, 우리는 몇몇 대표적인 LLM의 디코딩 설정을 제시한다:

\(\bullet\) T5 [82]는 그리디 검색을 기본 설정으로 사용하고 번역 및 요약 작업에 대해 길이 벌점이 0.6인 빔 검색(빔 크기 4)을 적용한다.

\(\bullet\)_GPT-3_[55]는 모든 생성 작업에 대해 빔 크기가 4이고 길이 페널티가 0.6인 빔 탐색을 사용한다.

\(\bullet\)_Alpaca_[142]는 개방형 생성을 위해 top-\(k\)(\(k=50\)), top-\(p\)(\(p=0.9\)), 온도가 0.7인 샘플링 기반 전략을 사용한다.

\(\bullet\)_LLaMA_[57]은 특정 작업에 맞춘 다양한 디코딩 전략을 적용한다. 예를 들어, 코드 생성을 위해 0.1(pass@1) 및 0.8(pass@100)의 온도 설정으로 샘플링 전략을 사용하는 동안 질문 응답 작업에 대한 탐욕 검색을 사용합니다.

\(\bullet\)_OpenAI API_는 그리디 검색(0으로 설정됨), 빔 검색(최상의 설정됨), 온도 샘플링(설정됨), 핵 샘플링(설정 top_p로 설정됨)을 포함한 몇 가지 기본 디코딩 전략을 지원한다. 또한 생성의 반복 정도를 제어하기 위해 파라미터 presence_penalty와 frequency_penalty를 도입한다. OpenAI의 문서에 따르면, 그들의 API들은 입력과 하이퍼-파라미터들이 동일하더라도 상이한 출력들을 생성할 것이다. 온도를 0으로 설정하면 변동 가능성은 약간 있지만 더 결정적 출력을 산출할 수 있다.

#### 4.2.6 요약 및 토론

아키텍처 및 사전 훈련 작업의 선택은 LLM에 대해 상이한 귀납적 편향을 야기할 수 있고, 이는 상이한 모델 용량으로 이어질 수 있다. 이 부분에서는 LLM에 대한 아키텍처 선택에 대한 한 가지 열린 문제에 대해 논의한다.

**아키텍처 선택** 사전 훈련된 언어 모델의 이전 문헌에서는 다양한 아키텍처의 효과에 대한 많은 논의가 있습니다. [89, 29]. 그러나 대부분의 LLM은 인과 디코더 구조를 기반으로 개발되었으며, 다른 대안들에 비해 그 장점에 대한 이론적 분석이 부족하다. 다음으로, 이 문제에 대한 기존의 논의를 간략히 정리한다.

\(\bullet\) LM 목표를 가지고 사전 훈련함으로써, 인과 디코더 아키텍처는 우수한 제로-샷 및 적은-샷 일반화 용량을 달성할 수 있는 것으로 보인다. 기존의 연구는 멀티-태스크 미세-튜닝이 없으면, 인과 디코더는 다른 아키텍처들보다 더 나은 제로-샷 성능을 갖는다는 것을 보여주었다[29]. GPT-3[55]의 성공은 큰 인과 디코더 모델이 좋은 소수의 샷 학습자가 될 수 있다는 것을 보여준다. 또한, 섹션 5에서 논의된 명령 튜닝 및 정렬 튜닝은 대형 인과 디코더 모델의 능력을 더욱 향상시키는 것으로 입증되었다[66, 67, 69].

\(\bullet\) 스케일링 법칙은 인과적 디코더에서 널리 관찰되어 왔다. 모델 크기, 데이터세트 크기, 및 총 계산을 스케일링함으로써, 인과적 디코더들의 성능이 실질적으로 개선될 수 있다[55, 30]. 따라서, 스케일링을 통해 인과 디코더의 모델 용량을 증가시키는 것이 중요한 전략이 되었다. 그러나, 인코더-디코더 모델에 대한 보다 상세한 연구는 여전히 부족하며, 인코더-디코더 모델의 성능을 대규모로 조사하기 위한 더 많은 노력이 필요하다.

특히 인코더-디코더 아키텍처의 경우, 아키텍처 및 사전 트레이닝 태스크의 선택이 LLM의 용량에 어떻게 영향을 미치는지 분석하기 위해서는 아키텍처 및 사전 트레이닝 목표에 대한 논의에 대한 더 많은 연구 노력이 필요하다. 디코더 전용 아키텍처의 효과에도 불구하고, 아키텍처 설계에 대한 보다 다양한 탐색이 제안된다. 주요 아키텍처 외에도 LLM의 세부 구성도 주목할 가치가 있으며, 이는 섹션 4.2.2에서 논의되었다.

### _Model Training_

이 부분에서는 LLM을 훈련하기 위한 중요한 설정, 기술 또는 트릭을 검토한다.

#### 4.3.1 Optimization Setting

LLM의 매개변수 최적화를 위해 배치 학습, 학습률, 최적화기 및 훈련 안정성에 일반적으로 사용되는 설정을 제시한다.

**일괄 처리 훈련** 언어 모델 사전 훈련의 경우 기존 작업은 일반적으로 일괄 처리 크기를 많은 수(예:_2,048 예 또는 4M 토큰)로 설정하여 훈련 안정성 및 처리량을 향상시킵니다. GPT-3 및 PaLM과 같은 LLM에 대해, 그들은 훈련 동안 배치 크기를 동적으로 증가시켜 궁극적으로 백만 척도에 도달하는 새로운 전략을 도입했다. 구체적으로 GPT-3의 배치 크기는 32K에서 3.2M 토큰으로 점차 증가하고 있다. 실험 결과는 배치 크기의 동적 스케줄이 LLMs의 트레이닝 프로세스를 효과적으로 안정화시킬 수 있음을 입증하였다[56].

**학습률.** 기존 LLM은 일반적으로 사전 훈련 중 준비 및 붕괴 전략과 유사한 학습률 일정을 채택합니다. 구체적으로, 초기 0.1%에서 0.5%까지의 훈련 단계에서, GPT-3의 경우 대략 \(5\times 10^{-5}\)에서 \(1\times 10^{-4}\)까지의 최대값까지 학습률을 점진적으로 증가시키기 위해 선형 워밍업 스케줄을 사용한다. 그런 다음, 훈련 손실이 수렴할 때까지 학습률을 최대값의 약 10%로 점진적으로 줄이는 코사인 감쇠 전략을 후속 단계에서 채택한다.

**최적화기** Adam 최적화기 [328] 및 AdamW 최적화기 [329]는 1차 그래디언트 기반 최적화를 위한 하위 모멘트의 적응형 추정치를 기반으로 하는 LLM(예:_ GPT-3) 학습에 널리 사용됩니다. 일반적으로 하이퍼 매개 변수는 \(\beta_{1}=0.9\), \(\beta_{2}=0.95\) 및 \(\epsilon=10^{-8}\)로 설정됩니다. 한편, Adafactor optimizer[330]은 훈련 중 GPU 메모리를 보존하기 위해 특별히 설계된 Adam optimizer의 변형인 LLMs(_e.g.,_PaLM 및 T5) 훈련에도 활용되었다. Adafactor optimizer의 hyper-parameters는 \(\beta_{1}=0.9\)와 \(\beta_{2}=1.0-k^{-0.8}\)로 설정되며, 여기서 \(k\)는 훈련 단계의 수를 나타낸다.

**훈련 안정화.** LLM을 사전 훈련 하는 동안 모델 붕괴를 유발할 수 있는 훈련 불안정성 문제를 자주 겪습니다. 이 문제를 해결하기 위해 가중치 감소 및 기울기 클리핑이 널리 활용되었으며, 기존 연구[55, 78, 90, 93, 113]는 일반적으로 기울기 클리핑의 임계값을 1.0으로, 가중치 감소율을 0.1로 설정했다. 그러나 LLM의 스케일링과 함께 훈련 손실 스파이크도 발생할 가능성이 더 높아 불안정한 훈련으로 이어진다. 이 문제를 완화하기 위해 PaLM [56] 및 OPT [90]은 스파이크가 발생하기 전에 이전 체크포인트에서 훈련 프로세스를 다시 시작하고 문제를 일으켰을 수 있는 데이터를 건너뛰는 간단한 전략을 사용합니다. 또한, GLM[93]은 임베딩 층의 비정상적인 기울기가 보통 스파이크로 이어지는 것을 발견하고, 이를 완화하기 위해 임베딩 층 기울기를 축소할 것을 제안한다.

#### 4.3.2 Scalable Training Techniques

모델 및 데이터 크기가 증가함에 따라 제한된 계산 자원 하에서 LLM을 효율적으로 훈련시키는 것이 어려워졌다. 특히, 훈련 처리량을 늘리고 GPU 메모리에 더 큰 모델을 로드하는 두 가지 주요 기술적 문제가 해결되어야 한다. 이 부분에서는 위의 두 가지 과제, 즉 3D 병렬[75, 331, 332], ZeRO[333], 혼합 정밀 훈련[334]을 해결하기 위해 기존 작업에서 널리 사용되는 몇 가지 접근법을 검토하고 훈련에 활용하는 방법에 대한 일반적인 제안을 제공한다.

**3D 병렬성.** 3D 병렬성은 실제로 일반적으로 사용되는 세 가지 병렬 학습 기술, 즉 데이터 병렬성, 파이프라인 병렬성 [331, 332] 및 텐서 병렬성 [75]24의 조합입니다. 다음으로 세 가지 병렬 학습 기술을 소개합니다.

각주 24: 모델 병렬성은 일부 작업에서 텐서 병렬성과 파이프라인 병렬성을 포함하는 보다 광범위한 용어이다[75].

\(\bullet\)_Data parallelism._ 데이터 병렬화는 훈련 처리량을 향상시키기 위한 가장 근본적인 접근법 중 하나이다. 여러 GPU에 걸쳐 모델 매개 변수 및 최적화 상태를 복제한 다음 전체 훈련 코퍼스를 이러한 GPU에 분산합니다. 이러한 방식으로, 각각의 GPU는 그것에 대해 할당된 데이터를 프로세싱하기만 하면 되고, 그래디언트들을 획득하기 위해 순방향 및 역방향 전파를 수행한다. 다른 GPU에서 계산된 기울기는 모든 GPU에서 모델을 업데이트하기 위한 전체 배치의 기울기를 얻기 위해 추가로 집계됩니다. 이러한 방식으로, 기울기들의 계산들이 상이한 GPU들에 대해 독립적으로 수행됨에 따라, 데이터 병렬화 메커니즘은 고도로 확장가능하여, 트레이닝 처리량을 개선하기 위해 GPU들의 수를 증가시키는 방법을 가능하게 한다. 또한, 이 기법은 구현이 간단하며, 기존의 대부분의 인기 있는 딥러닝 라이브러리들은 이미 TensorFlow 및 PyTorch와 같은 데이터 병렬성을 구현했다.

\(\bullet\)_Pipeline parallelism._ 파이프라인 병렬화는 LLM의 다른 계층을 여러 GPU에 분산하는 것을 목표로 한다. 특히 Transformer 모델의 경우 파이프라인 병렬화는 GPU 간의 연산된 은닉 상태나 그라디언트 전송 비용을 줄이기 위해 동일한 GPU에 연속적인 레이어를 로딩한다. 그러나 파이프라인 병렬 처리의 순진한 구현은 각 GPU가 계산을 완료하기 위해 이전 GPU를 기다려야 하기 때문에 GPU 사용률이 낮아질 수 있으며, 이는 _기포 오버헤드_[331]의 불필요한 비용을 초래할 수 있다. 파이프라인 병렬 처리에서 이러한 버블을 줄이기 위해 GPipe [331]과 PipeDream [332]는 파이프라인 효율성을 향상시키기 위해 데이터의 여러 배치를 패딩하고 비동기식 기울기 업데이트를 수행하는 기술을 제안한다.

\(\bullet\)_Tensor parallelism._ 텐서 병렬성은 다중 GPU 로딩을 위해 LLM을 분해하는 것을 목표로 하는 일반적으로 사용되는 기술이기도 하다. 파이프라인 병렬성과 달리 텐서 병렬성은 LLM의 텐서(매개변수 행렬)를 분해하는 데 중점을 둔다. LLM에서 행렬 곱셈 연산 \(Y=XA\)의 경우, 매개변수 행렬 \(A\)은 열로 두 개의 하위 행렬인 \(A_{1}\)과 \(A_{2}\)로 분할될 수 있으며, 이는 \(Y=[XA_{1},XA_{2}]\)로 표현될 수 있다. 행렬 \(A_{1}\)과 \(A_{2}\)을 서로 다른 GPU에 배치함으로써, 행렬 곱셈 연산은 두 GPU에서 병렬로 호출되고, 최종 결과는 두 GPU의 출력을 GPU 간 통신을 통해 결합함으로써 얻어질 수 있다. 현재 텐서 병렬성은 여러 오픈 소스 라이브러리 _e.g._, Megatron-LM [75]에서 지원되며 고차원 텐서로 확장될 수 있습니다. 또한 Colossal-AI는 고차원 텐서[335, 336, 337]에 대해 텐서 병렬성을 구현하였고, 특히 시퀀스 데이터에 대해 시퀀스 병렬성[338]을 제안하여 Transformer 모델의 어텐션 연산을 더욱 분해할 수 있다.

**ZeRO.** DeepSpeed [74] 라이브러리에서 제안한 ZeRO [333] 기술은 데이터 병렬 처리의 메모리 중복성 문제에 중점을 둡니다. 앞서 언급한 바와 같이, 데이터 병렬성은 각 GPU가 모델 파라미터, 모델 구배 및 최적화기 파라미터를 포함하여 LL의 동일한 사본을 저장해야 한다. 반면, 위의 모든 데이터가 각각의 GPU에 보유될 필요는 없으며, 이는 메모리 중복 문제를 야기할 것이다. 이를 해결하기 위해 ZeRO 기법은 각 GPU에 일부 데이터만 유지하는 것을 목표로 하며, 나머지 데이터는 필요할 때 다른 GPU에서 검색할 수 있다. 특히 ZeRO는 데이터의 세 부분, 즉 최적화기 상태 분할, 그래디언트 분할 및 매개변수 분할이 저장되는 방법에 따라 세 가지 솔루션을 제공합니다. 실험 결과는 처음 두 해는 통신 오버헤드를 증가시키지 않으며, 세 번째 해는 약 50%의 통신 오버헤드를 증가시키지만 GPU의 수에 비례하는 메모리를 절약함을 보여준다. PyTorch는 FSDP[339]라고 불리는 ZeRO와 유사한 기술을 구현했다.

**혼합 정밀 훈련.** 이전 PLM(_e.g._, BERT [23])에서 FP32라고도 하는 32비트 부동 소수점 숫자가 사전 훈련에 주로 사용되었습니다. 최근, 극히 큰 언어 모델들을 사전 트레이닝하기 위해, 메모리 사용량 및 통신 오버헤드를 감소시키는 16-비트 부동 소수점 수들(FP16)을 활용하기 위한 몇몇 연구들[334]이 시작되었다. 추가적으로, 인기 있는 NVIDIA GPU들(_e.g._, A100)이 FP32보다 두 배의 FP16 계산 유닛들을 가짐에 따라, FP16의 계산 효율은 더 향상될 수 있다. 그러나 기존 연구에서는 FP16이 최종 모델 성능에 영향을 미치는 계산 정확도[64, 78]의 손실을 초래할 수 있음을 발견했다. 이를 완화하기 위해 BF16(Brain Floating Point)_라는 대안이 학습에 사용되었는데, 이는 FP16보다 더 많은 지수 비트와 더 적은 유의 비트를 할당한다. 사전 트레이닝을 위해 BF16은 일반적으로 표현 정확도에 대해 FP16보다 더 나은 성능을 수행한다[78].

**전체 교육 제안** 실제로 위의 교육 기술, 특히 3D 병렬 처리는 종종 훈련 처리량 및 대규모 모델 로딩을 개선하기 위해 공동으로 사용됩니다. 예를 들어, 연구자들은 8-방향 데이터 병렬성, 4-방향 텐서 병렬성 및 12-방향 파이프라인 병렬성을 통합하여 384개의 A100 GPU에서 BLOOM[78]의 훈련을 가능하게 했다. 현재 DeepSpeed [74], Colossal-AI [189], Alpa [340]과 같은 오픈 소스 라이브러리는 세 가지 병렬 훈련 방법을 잘 지원할 수 있다. 메모리 중복성을 줄이기 위해 ZeRO, FSDP 및 활성화 재계산 기술 [77, 341]은 이미 DeepSpeed, PyTorch 및 Megatron-LM에 통합된 LLM을 훈련하는 데에도 사용할 수 있다. 또한, BF16과 같은 혼합 정밀도 훈련 기법 또한 훈련 효율을 향상시키고 GPU 메모리 사용량을 감소시키기 위해 레버리지될 수 있는 반면, 하드웨어(_e.g._, A100 GPU)에 대한 필요한 지원이 필요하다. 대규모 모델을 훈련하는 것은 시간 집약적인 과정이기 때문에 모델 성능을 예측하고 초기 단계에서 비정상적인 문제를 감지하는 것이 유용할 것이다. 이를 위해 GPT-4 [46]은 최근 딥러닝 스택에 구축된 _예측 가능한 스케일링_이라는 새로운 메커니즘을 도입하여 훨씬 작은 모델로 큰 모델의 성능 예측을 가능하게 하며, 이는 LLM 개발에 매우 유용할 수 있다. 실제로, 주류 딥 러닝 프레임워크의 지원 훈련 기술을 더 활용할 수 있다. 예를 들어, PyTorch는 부분 오프로딩을 허용하는 데이터 병렬 트레이닝 알고리즘 FSDP[339](_i.e._, fully sharded data parallel)를 지원한다.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Model** & \begin{tabular}{c} **Batch Size** \\ **(\#tokens)** \\ \end{tabular} & \begin{tabular}{c} **Learning** \\ **Rate** \\ \end{tabular} & **Warmup** & **Decay Method** & **Optimizer** & \begin{tabular}{c} **Precision** \\ **Type** \\ \end{tabular} &
\begin{tabular}{c} **Weight** \\ **Decay** \\ \end{tabular} & **Grad** & **Dropout** \\ \hline GPT3 (175B) & 32K\(\rightarrow\)3.2M & \(6\times 10^{-5}\) & yes & cosine decay to 10\% & Adam & FP16 & 0.1 & 1.0 & - \\ PanGu-\(\alpha\) (200B) & - & \(2\times 10^{-5}\) & - & - & Adam & - & 0.1 & - & - \\ OPT (175B) & 2M & \(1.2\times 10^{-4}\) & yes & manual decay & AdamW & FP16 & 0.1 & - & 0.1 \\ PALM (540B) & 1M\(\rightarrow\)4M & \(1\times 10^{-2}\) & no & inverse square root & Adafactor & BF16 & \(lr^{2}\) & 1.0 & 0.1 \\ BLOOM (176B) & 4M & \(6\times 10^{-5}\) & yes & cosine decay to 10\% & Adam & BF16 & 0.1 & 1.0 & 0.0 \\ MT-NLG (530B) & 64 K\(\rightarrow\)3.57M & \(5\times 10^{-5}\) & yes & cosine decay to 10\% & Adam & BF16 & 0.1 & 1.0 & - \\ Gopher (280B) & 3M\(\rightarrow\)6M & \(4\times 10^{-5}\) & yes & cosine decay to 10\% & Adam & BF16 & - & 1.0 & - \\ Chinchilla (70B) & 1.5M\(\rightarrow\)3M & \(1\times 10^{-4}\) & yes & cosine decay to 10\% & AdamW & BF16 & - & - & - \\ Galactic (120B) & 2M & \(7\times 10^{-6}\) & yes & linear decay to 10\% & AdamW & - & 0.1 & 1.0 & 0.1 \\ LaMDA (137B) & 256K & - & - & - & - & - & BF16 & - & - \\ Jurassic-1 (178B) & 32 K\(\rightarrow\)3.2M & \(6\times 10^{-5}\) & yes & - & - & - & - & - & - \\ LLMaM (65B) & 4M & \(1.5\times 10^{-4}\) & yes & cosine decay to 10\% & AdamW & - & 0.1 & 1.0 & - \\ LLMaM (2 70B) & 4M & \(1.5\times 10^{-4}\) & yes & cosine decay to 10\% & AdamW & - & 0.1 & 1.0 & - \\ Falcon (40B) & 2M & \(1.85\times 10^{-4}\) & yes & cosine decay to 10\% & AdamW & BF16 & 0.1 & - & - \\ GLM (130B) & 0.4M\(\rightarrow\)8.25M & \(8\times 10^{-5}\) & yes & cosine decay to 10\% & AdamW & FP16 & 0.1 & 1.0 & 0.1 \\ TS (11B) & 64K & \(1\times 10^{-2}\) & no & inverse square root & Adafactor & - & - & - & 0.1 \\ ERNE 3.0 Titan (260B) & - & \(1\times 10^{-4}\) & - & - & Adam & FP16 & 0.1 & 1.0 & - \\ PanGu-\(\Sigma\) (1.085T) & 0.5M & \(2\times 10^{-5}\) & yes & - & Adam & FP16 & - & - & - \\ \hline \hline \end{tabular}
\end{table} TABLE VII: Detailed optimization settings of several existing LLMs.

원하는 경우 CPU에 대한 훈련 계산입니다.

## 5 LLMs의 적응

사전 훈련 후 LLM은 다양한 과제 해결을 위한 일반적인 능력을 습득할 수 있다. 그러나 점점 더 많은 연구에서 LLM의 능력이 특정 목표에 따라 더 적응할 수 있음을 보여주었다. 이 절에서는 사전 훈련된 LLM을 적응시키기 위한 두 가지 주요 접근법, 즉 명령어 튜닝과 정렬 튜닝을 소개한다. 전자의 접근법은 주로 LLM의 능력을 향상(또는 해제)하는 것을 목표로 하는 반면, 후자의 접근법은 LLM의 행동을 인간의 가치 또는 선호도와 정렬하는 것을 목표로 한다. 또한, 자원 제한 설정에서 모델 적응을 위한 효율적인 튜닝 및 양자화에 대해서도 논의할 것이다. 이하에서는 네 가지 부분에 대해 구체적으로 소개하고자 한다.

### _Instruction Tuning_

본질적으로, 명령어 튜닝은 자연 언어 형태의 포맷된 인스턴스들의 컬렉션 상에서 사전-트레이닝 LLM들을 미세-튜닝하는 접근법[67]이며, 이는 감독된 미세-튜닝[66] 및 멀티-태스크 프롬프트 트레이닝[28]과 매우 관련된다. 명령어 튜닝을 수행하기 위해서는 먼저 명령어 형식의 인스턴스를 수집하거나 구성해야 한다. 그런 다음, 이러한 포맷된 인스턴스를 사용하여 지도 학습 방식으로 LLM을 미세 조정한다(예를 들어, 시퀀스 대 시퀀스 손실을 갖는 트레이닝). 명령어 튜닝 후, LLMs는 다중 언어 설정에서도 보이지 않는 태스크로 일반화하는 우수한 능력을 입증할 수 있다[67, 69, 28].

최근의 조사[342]는 수업 튜닝을 위한 연구의 체계적인 개요를 제시한다. 이에 비해 주로 수업 튜닝이 LLMs에 미치는 영향에 초점을 맞추고 사례 수집 및 튜닝을 위한 세부 지침이나 전략을 제공한다. 또한, 기존의 LLMs, _e.g.,_ InstructGPT[66] 및 GPT-4[46]에서 널리 적용되고 있는 사용자의 실제 요구를 만족시키기 위한 명령어 튜닝의 사용에 대해서도 논의한다.

#### 5.1.1 형식 인스턴스 구성

일반적으로 명령 형식 인스턴스는 작업 설명(명령어_라고 함), 선택적 입력, 해당 출력 및 적은 수의 시연(선택 사항)으로 구성됩니다. 중요한 공공 자원으로서, 기존 연구들은 섹션 3.3.1에서 소개된 바와 같이 자연어로 포맷된 라벨링된 데이터(표 III의 가용 자원 목록 참조)를 다수 공개하였다. 다음으로, 포맷된 인스턴스를 구성하기 위한 세 가지 주요 방법(그림 11의 예시 참조)을 소개하고 인스턴스 구축을 위한 몇 가지 주요 인자에 대해 논의한다.

**NLP 작업 데이터 세트 형식화** 명령 튜닝을 제안하기 전에 여러 초기 연구 [343, 168, 344]는 다양한 범위의 전통적인 NLP 작업(예: 텍스트 요약, 텍스트 분류 및 번역)에서 인스턴스를 수집하여 감독된 다중 작업 훈련 데이터 세트를 만들었습니다. 명령어 튜닝 인스턴스의 주요 소스로서, 이러한 다중 태스크 트레이닝 데이터 세트를 자연어 태스크 설명으로 포맷하는 것이 편리하다. 구체적으로, 최근 작업[28, 66, 67, 88]은 사람이 작성한 작업 설명으로 라벨링된 데이터 세트를 보강하며, 이는 LLM에게 작업 목표를 설명함으로써 작업을 이해하도록 지시한다. 예를 들어, 도 11의 (a)에서는 질문-답변 태스크에서 각 예에 대해 _"이 질문에 답해 주세요"_라는 태스크 설명이 추가된다. 명령어 튜닝 후에, LLMs는 그들의 태스크 설명을 따라 다른 보이지 않는 태스크들에 잘 일반화할 수 있다[28, 67, 69]. 특히, 명령어가 LLMs[67]의 태스크 일반화 능력에 중요한 요소임을 보였다 : 태스크 설명이 제거된 라벨링된 데이터 세트에서 모델을 미세 조정함으로써 모델 성능의 급격한 저하를 초래한다. 명령어 튜닝을 위한 라벨링된 인스턴스를 더 잘 생성하기 위해, 크라우드 소싱 플랫폼인 PromptSource[167]는 상이한 데이터세트에 대한 태스크 설명을 효과적으로 생성, 공유 및 검증하기 위해 제안되었다. 트레이닝 인스턴스들을 풍부하게 하기 위해, 몇몇 연구들 [28, 345, 168]은 또한 명령어 튜닝을 위해 특별히 설계된 태스크 설명들로 기존 인스턴스들의 입력-출력 쌍들을 반전시키려고 시도한다. 예를 들어, 질문-답변 쌍이 주어지면, 우리는 답변-조건화된 질문을 예측함으로써 새로운 인스턴스를 생성할 수 있다(_예를 들어,_ "_답변:_에 기초하여 질문을 생성하세요).

**일별 채팅 데이터 형식화** 많은 수의 교육 인스턴스가 지침으로 형식화되었지만 주로 지침 다양성이 부족하거나 실제 인간의 요구와 일치하지 않는 공용 NLP 데이터 세트에서 나옵니다. [66] 이 문제를 극복하기 위해 InstructGPT [66]은 실제 사용자가 OpenAI API에 제출한 쿼리를 작업 설명으로 사용하는 것을 제안한다. 또한 작업 다양성을 풍부하게 하기 위해 인간 라벨러도 개방형 생성, 개방형 질문 응답, 브레인스토밍 및 채팅을 포함한 실제 작업에 대한 지침을 작성하도록 요청받는다. 그런 다음 다른 레이블러 그룹이 이러한 지침에 직접 응답하도록 출력으로 허용합니다. 마지막으로, 이들은 하나의 명령어(_즉,_수집된 사용자 질의)와 예상 출력(_즉,_인간-작성된 답변)을 트레이닝 인스턴스로서 페어링한다. InstructGPT는 또한 정렬 튜닝(섹션 5.2에서 논의됨)을 위해 자연 언어로 포맷된 이러한 실세계 태스크들을 채용한다는 것에 유의한다. 또한 GPT-4 [46]은 잠재적으로 고위험 지침을 설계하고 안전 문제에 대한 감독된 미세 조정을 통해 이러한 지침을 거부하도록 모델을 안내했다. 고품질 공개 채팅 데이터의 부재를 고려하여 여러 연구에서도 사용자의 채팅 요청을 입력 데이터로 수집한 후 ChatGPT 또는 GPT-4를 활용하여 응답을 출력 데이터로 생성하였다. 그러한 데이터세트의 주목할만한 예는 ShareGPT[148]로부터의 대화 데이터이다. 또한, 돌리[172] 및 오픈 어시스턴트[173]는 높은 수준의 품질을 달성하기 위해 인간 주석자에 의해 신중하게 라벨링된 그들의 대화 데이터를 추가로 공개했다.

**합성 데이터 형식화** 인간 주석 또는 수동 컬렉션의 부담을 줄이기 위해 기존 인스턴스를 LLM에 공급 하 여 다양한 작업 설명 및 인스턴스를 합성 하 여 인스턴스를 구성 하는 여러 반자동 접근법 [143]이 제안 되었습니다. 도 11(c)에 예시된 바와 같이, Self-Instruct 방법은 초기 태스크 풀로서 175개의 인스턴스들만을 필요로 한다. 그런 다음 풀에서 몇 개의 인스턴스를 시범으로 무작위로 선택하고 LLM을 프롬프트하여 새 명령어와 해당 입력 출력 쌍을 생성합니다. 품질 및 다양성 필터링 후 새로 생성된 인스턴스가 작업 풀에 추가됩니다. 따라서, 합성 방법은 LLMs에 대한 대규모 명령어 데이터를 생성하는데 효과적이고 경제적인 방법이다. 그러나 Self-Instruct 방법에 의해 생성된 인스턴스는 단순하거나 다양성이 부족할 수 있다. 합성 int ructions의 품질을 향상시키기 위해, WizardLM [346]은 인스턴스의 복잡성과 다양성을 풍부하게 하기 위해 진화하는 심층적이고 광범위한 것을 제안함으로써 Evol-Instruct를 도입한다. 또한, Self-Align[347]은 합성된 인스턴스들을 필터링하기 위해 다수의 인간-정렬 원리들을 확립한다. 그런 다음 이러한 인스턴스를 사용하여 LLM을 훈련하여 더 정렬된 인스턴스를 생성합니다. 인스턴스 출력의 품질을 향상시키기 위해, 연구자들은 직접 인간-작성된 텍스트들을 출력으로 채택하고 ICL 예들을 사용하여 대응하는 명령어들을 합성한다[348].

**인스턴스 구성에 대한 주요 요인** 명령 인스턴스의 품질은 모델의 성능에 중요한 영향을 미칩니다. 여기서는 건설과 같은 몇 가지 필수 요소에 대해 논의합니다.

\(\bullet\)_명령을 스케일링하는 단계._ 작업의 수를 스케일링하는 것은 LLMs[28, 67, 88]의 일반화 능력을 크게 향상시킬 수 있다는 것이 널리 보여졌다. 과제 수가 증가함에 따라 모델 성능은 초기에 지속적인 성장 패턴을 보이는 반면, 이득은 일정 수준에 도달하면 무시할 수 있게 된다[69, 88]. 그럴듯한 추측은 특정 수의 대표 과제가 상대적으로 충분한 지식을 제공할 수 있고 더 많은 과제를 추가하는 것은 추가적인 이득을 가져오지 않을 수 있다는 것이다[69]. 또한, 길이, 구조, 창의성과 같은 여러 측면에서 과제 서술의 다양성을 향상시키는 것이 유익하다[28]. 작업당 인스턴스의 수에 대해서는, 소수의 인스턴스가 일반적으로 특정 작업을 수행하기 위해 모델의 일반화 성능을 포화시킬 수 있다는 것이 발견되었다[67, 69]. 특히, 최근의 여러 연구[349, 350]는 소량의 고품질 명령어 데이터(예: 1 또는 수천 개의 인스턴스)로 미세 조정 효과를 탐구하여 평가 작업에 매우 유망한 결과를 보여준다. 대조적으로, 또 다른 연구 라인은 명령어 데이터의 스케일링 효과를 계속 탐색하고 있다[351, 352]. 예를 들어 Orca [351]은 단계별 설명을 통해 합성된 인스턴스를 500만 개로 확장하고, 명령 데이터로 튜닝된 방법에 비해 광범위한 작업에 걸쳐 우수한 성능을 달성한다.

\(\bullet\)_Formatting design._ 중요한 요소로서 자연어 포맷의 설계는 LLMs[88]의 일반화 성능에도 큰 영향을 미친다. 일반적으로 기존 데이터 세트의 입력 출력 쌍에 작업 설명 및 선택적 데모를 추가할 수 있으며, 여기서 작업 설명은 LLM이 작업을 이해하는 데 가장 중요한 부분입니다[88]. 또한, 적절한 수의 예시들을 시연으로 사용함으로써 실질적인 개선으로 이어질 수 있으며, 이는 또한 수업 공학에 대한 모델 민감도를 완화시킨다[67, 69]. 그러나, 다른 컴포넌트들(예를 들어, 회피할 것들, 이유들, 및 제안들)을 명령들에 통합하는 것은 LLM들의 성능에 무시할 수 있거나 심지어 악영향을 미칠 수 있다[88, 166]. 최근 LLM의 단계별 추론 능력을 도출하기 위해, 일부 작업[69]은 산술 추론과 같은 일부 추론 데이터 세트에 대한 CoT(Chain-of-thought) 예를 포함하는 것을 제안한다. CoT 및 비-CoT 예들 둘 모두를 갖는 미세-튜닝 LLM들은 다중-홉 추론 능력(예를 들어,_상식 질문 응답 및 산술 추론)을 필요로 하는 것뿐만 아니라 그러한 추론 방식이 필요하지 않은 것(예를 들어,_감성 분석 및 추출 질문 응답)을 포함하는 다양한 추론 태스크들에 걸쳐 양호한 성능을 유도할 수 있음을 보여주었다[69, 95].

요약하면, 잘 수행되는 InstructGPT [66] 및 LLAMA-2-Chat [99]가 플랜 시리즈 LLM [67, 69]보다 더 적지만 더 다양한 명령(또는 인스턴스)을 활용하기 때문에 명령의 다양성과 품질이 인스턴스 수[349]보다 더 중요한 것 같다. 그러나, 많은 양의 학습 데이터는 고품질의 데이터가 없는 것을 보상할 수 있다[351]. 또한 데이터 세트별 작업을 사용하는 것보다 레이블러를 불러 사람이 필요한 작업을 구성하는 것이 더 유용합니다. 그러나 여전히 인간이 필요로 하는 인스턴스에 주석을 달기 위한 일반적인 지침이 부족하여 작업 구성이 어떻게든 휴리스틱하게 만든다. 인간의 노력을 줄이기 위해, 우리는 기존의 포맷된 데이터 세트(표 III)를 재사용하거나 기존의 LLMs[143]를 사용하여 명령어를 자동으로 구성할 수 있다. 우리는 보여주기 위한 예비 실험을 수행한다.

도. 11: 인스턴스 포맷팅의 예시 및 명령-포맷된 인스턴스들을 구성하기 위한 세 가지 상이한 방법들.

5.1.4절에서 다양한 시공 방법의 효과.

#### 5.1.2 명령어 조정 전략

사전 트레이닝과 달리, 트레이닝을 위해 적당한 수의 인스턴스들만이 사용되기 때문에, 명령어 튜닝은 종종 더 효율적이다. 명령어 튜닝은 지도 트레이닝 프로세스로서 고려될 수 있기 때문에, 그것의 최적화는 훈련 목적(_즉,_시퀀스-투-시퀀스 손실) 및 최적화 구성(_예컨대,_더 작은 배치 크기 및 학습 속도)과 같은 여러 측면들[69]에서 사전 트레이닝과는 상이하며, 이는 실무에서 특별한 주의를 필요로 한다. 이러한 최적화 구성들 외에도, 명령어 튜닝을 위해 고려해야 할 네 가지 중요한 측면들이 또한 존재한다:

**데이터 배포의 균형 조정** 명령 조정에는 서로 다른 작업이 혼합되어 있으므로 미세 조정 중에 서로 다른 작업의 비율을 조정하는 것이 중요합니다. 널리 사용되는 방법은 모든 데이터 세트를 결합하고 혼합된 데이터 세트에서 각 인스턴스를 균등하게 샘플링하는 _예제-비례 혼합_ 전략[82]이다. 더욱이, 고품질 컬렉션들(예를 들어,_FLAN[67] 및 P3[167])의 샘플링 비율을 증가시키는 것은 일반적으로 최근의 발견들[95, 69]에 따른 성능 개선으로 이어질 수 있다. 또한, 명령 튜닝 동안 데이터세트가 포함할 수 있는 최대 수의 예를 제어하기 위해 _최대 cap_을 설정하는 것이 일반적이며, 이는 더 큰 데이터세트가 전체 분포를 압도하는 것을 방지하기 위해 설정된다[82, 95]. 실제로, 최대 캡은 전형적으로 상이한 데이터세트들에 따라 수 천 또는 수만 개로 설정된다[67, 69]. 최근, 기존의 명령어 데이터셋(표 Ⅲ)은 주로 특정 측면에서 LLMs의 능력 향상에 초점을 맞추고 있으며, 단일 데이터셋만으로는 모델 용량의 포괄적인 향상으로 이어질 수 없다는 것이 실증적으로 밝혀졌다[353]. 따라서, NLP 태스크 데이터(_예를 들어,_FLAN v2[292])와 채팅 데이터(_예를 들어,_ShareGPT[148]) 및 합성 데이터(_예를 들어,_GPT4-Alpaca[354])를 포함하여 상이한 용량에서 균형 잡힌 개선을 달성하기 위해 기존의 명령어 데이터세트의 혼합물을 사용하는 것이 종종 제안된다.

**명령 조정 및 사전 훈련 결합** 튜닝 프로세스를 보다 효과적이고 안정적으로 만들기 위해 OPT-IML [95]는 명령 조정 중에 사전 훈련 데이터를 통합하며, 이는 모델 튜닝을 위한 정규화로 간주할 수 있습니다. 또한, 별도의 2단계 프로세스(_pre-training_ then _instruction tuning_)를 사용하는 대신에, 일부 연구는 멀티-태스크 학습을 사용하여 사전-트레이닝 데이터(_즉,_평문) 및 명령어 튜닝 데이터(_즉,_포맷된 데이터세트)의 혼합물로 모델을 처음부터 트레이닝하려고 시도한다[82]. 구체적으로, GLM-130B [93] 및 Galactica [35]는 사전 트레이닝 LLM에 사전 트레이닝 코퍼라의 작은 비율로서 명령어 포맷 데이터 세트를 통합하며, 이는 잠재적으로 사전 트레이닝 및 명령어 튜닝의 이점을 동시에 달성한다.

**다단계 명령 튜닝** 명령 튜닝의 경우 두 가지 종류의 중요한 명령 데이터, 즉 작업 형식 명령과 일일 채팅 명령이 있습니다. 일반적으로, 전자는 후자에 비해 상당히 큰 부피를 갖는다. 훈련과 두 종류의 훈련 데이터의 균형을 맞추는 것이 중요하다. 서로 다른 명령어 데이터를 신중하게 혼합하는 것 외에도, 우리는 다단계 명령어 튜닝 전략(352)을 채택할 수 있는데, 여기서 LLMs는 먼저 대규모 태스크-포맷된 명령어들로 미세 조정되고 이어서 일일 채팅들 상에서 미세 조정된다. 용량 망각 문제를 방지하기 위해 두 번째 단계에서 작업 형식 지침을 추가하는 것도 유용합니다. 실제로 이러한 다단계 튜닝 전략은 명령어 튜닝을 위한 다른 설정에도 적용될 수 있다. 예를 들어, 난이도 및 복잡도에 대해 점진적으로 증가된 레벨을 갖는 상이한 미세 조정 단계를 스케줄링할 수 있고, 복잡한 지시를 따르도록 LLM의 용량을 점진적으로 개선할 수 있다.

**기타 실제 트릭** 실제로 LLM의 미세 조정 성능을 개선하는 데 도움이 되는 몇 가지 유용한 전략과 트릭도 있습니다. 우리는 다음과 같이 몇 가지 대표적인 것을 나열한다.

\(\bullet\)_멀티턴 채팅 데이터에 대한 효율적인 훈련_ 멀티턴 채팅 예(사용자와 챗봇 간의 대화)가 주어지면, 간단한 미세 조정 방법은 트레이닝을 위해 이를 다수의 컨텍스트-응답 쌍들로 분할하는 것이다: LLM은 모든 분할들에 대한 대응하는 컨텍스트에 기초하여 응답을 생성하기 위해 미세 조정된다(_즉, 사용자로부터의 각각의 발화에서_). 이러한 미세 조정 방식으로, 대화로부터 분리된 예들에서 중복되는 발화가 존재하는 것은 명백하다. 훈련 비용을 절약하기 위해, Vicuna[138]는 대화 전체를 LLM에 공급하는 효율적인 방법을 채택했지만, 훈련을 위해 챗봇의 응답에 대한 손실만을 계산하는 손실 마스크에 의존한다. 이는 중첩된 발화로부터 도출되는 계산 비용을 상당히 감소시킬 수 있다.

\(\bullet\)_LLM에 대한 자기 식별 설정_ 실제 응용 프로그램에 LLM을 배포 하려면 LLM이 id를 설정 하 고 이름, 개발자 및 소속과 같은 id 정보를 인식 해야 합니다. 실제적인 방법은 LLM을 미세 조정하기 위한 ID 관련 지침을 만드는 것이다. 또한 입력을 자기 식별 프롬프트, _예:_ _"로 접두사를 붙이는 것이 가능하며, 여기서 ChatbotName과 Developer는 각각 챗봇의 이름과 개발자를 지칭하는, 인간과 AI 어시스턴트인 _ChatbotName의 대화는 다음과 같다.

위의 실용적인 전략 및 트릭 외에도 기존 작업은 다른 트릭을 사용했으며, _예를 들어_ 여러 예를 단일 시퀀스로 연결하여 최대 길이에 접근했다[355].

#### 5.1.3 Instruction Tuning 효과

이 부분에서는 수업 튜닝이 LLMs에 미치는 영향을 크게 세 가지 측면에서 논의한다.

**성능 향상.** 적당한 수의 인스턴스에서 튜닝 되었지만 지침 튜닝은 LLMs의 기능을 향상 하거나 잠금 해제 하는 중요한 방법이 되었습니다 [69]. 최근 연구에서는 여러 척도(77M에서 540B 범위)의 언어 모델을 실험하여 서로 다른 척도의 모델이 모두 명령어 튜닝의 이점을 얻을 수 있음을 보여주었으며[69, 345], 매개변수 척도가 증가함에 따라 성능이 향상되었다[94]. 또한, 명령어 튜닝을 갖는 더 작은 모델들은 심지어 미세-튜닝 없이 더 큰 모델들보다 더 잘 수행할 수 있다[28, 69]. 모델 규모 외에도, 명령어 튜닝은 다양한 모델 아키텍처, 사전 훈련 목표 및 모델 적응 방법에서 일관된 개선을 보여준다[69]. 실제로, 명령어 튜닝은 기존 언어 모델[69](작은 크기의 PLM 포함)의 능력을 향상시키는 일반적인 접근법을 제공한다. 또한, LLMs에 의해 요구되는 명령어 데이터의 양이 사전 트레이닝 데이터보다 상당히 작기 때문에 사전 트레이닝보다 훨씬 비용이 적게 든다.

**작업 일반화.** 명령 조정은 모델이 작업 완료를 위한 자연어 지침을 이해하도록 권장합니다. 그것은 LLM에게 보이지 않는 작업에서도 시연 없이 특정 작업을 수행하기 위해 인간의 지시를 따르는 능력(종종 새로운 능력으로 간주됨)을 부여한다[31]. 많은 연구에서 보이는 과제와 보이지 않는 과제 모두에서 우수한 성능을 달성하기 위한 명령어 튜닝의 효과를 확인하였다[95, 345]. 또한, 명령어 튜닝은 LLMs(_e.g._, 특정 태스크를 달성하지 않고 입력을 반복 생성하거나 보완하는)의 몇 가지 약점을 완화하는 데 유용한 것으로 나타났다[66, 69], LLMs에 대한 실세계 태스크를 해결할 수 있는 우수한 용량으로 이어진다. 또한, 명령어 튜닝을 통해 훈련된 LLM은 언어 전반에 걸쳐 관련 작업에 일반화할 수 있다. 예를 들어, BLOOMZ-P3[94]는 영어 전용 태스크 모음 P3[167]을 사용하여 BLOOM[78]에 기초하여 미세 조정된다. 흥미롭게도, BLOOMZ-P3는 BLOOM에 비해 다국어 문장 완성 작업에서 50% 이상의 개선을 달성할 수 있으며, 이는 명령어 튜닝이 LLMs가 영어 전용 데이터 세트로부터 일반적인 작업 기술을 습득하고 그러한 기술을 다른 언어로 전달하는 데 도움이 될 수 있음을 보여준다[94]. 또한, 영어 전용 명령어를 사용하면 다국어 과제[94]에서 만족스러운 결과를 얻을 수 있으며, 이는 특정 언어에 대한 수업 공학의 노력을 줄이는 데 도움이 되는 것으로 밝혀졌다.

**도메인 전문화.** 기존 LLM은 기존 NLP 작업(_예:_, 생성 및 추론) 및 일일 질문에서 우수한 기능을 보여주었습니다. 그러나 여전히 의학, 법률 및 재정과 같은 특정 작업을 수행하기 위한 도메인 지식이 부족할 수 있다(다양한 응용 프로그램에서 LLMs에 대한 자세한 논의는 섹션 8 참조). 명령어 조정은 기존의 일반적인 LLM을 도메인별 전문가가 되도록 적응시키는 효과적인 접근법이다. 예를 들어, 연구자들은 의료 데이터 세트를 사용하여 플랜-PaLM[69]을 미세 조정하여 전문 임상의와 유사한 성능 수준을 달성하는 의료 지식 보조인 Med-PaLM[356]을 만들 것을 제안한다. 또한, 최근의 연구[357]은 자연 언어 명령어를 갖는 전자 상거래 추천 시스템을 지원하기 위해 FLAN-T5를 미세 조정함으로써, 다양한 추천 작업에서 강한 성능을 보여준다. 또한 BenTsao[358]와 같이 LLaMA[57]를 기반으로 지시 조정된 여러 개의 오픈 소스 의료 모델이 있다. 또한, 연구자들은 법칙[359], 재정[360], 산술 계산[361]에 대한 수업 튜닝을 탐구한다.

#### 5.1.4 Instruction Tuning에 대한 경험적 분석

서로 다른 명령어 세트를 갖는 미세 조정 LLM은 다운스트림 태스크에서 다양한 성능을 갖는 모델 변형으로 이어지는 경향이 있다. 이 절에서는 LLMs(_i.e._, LLaMA(7B) 및 LLaMA(13B)25)의 미세 조정에서 다양한 유형의 지시가 미치는 영향을 조사하고 몇 가지 지시 개선 전략의 유용성을 조사할 것이다.

각주 25: 계산 리소스의 한계로 인해, 우리는 현재 더 큰 LLaMA 변형에 대한 대규모 실험을 수행할 수 없으며, 이는 향후 버전에서 예정될 것이다.

**명령 데이터 세트.** 섹션 5.1.1의 논의에 따라 다음과 같이 세 가지 일반적인 종류의 지침을 주로 고려한다.

* _작업별 지침_ 첫 번째 유형의 명령어에는 1,836개의 태스크와 15M 이상의 명령어를 포함하는 가장 일반적으로 사용되는 다중 태스크 명령어 데이터 세트 _FLAN-T5_[69]를 채택한다.
* _일별 채팅 지침_ 이러한 유형의 지침은 실생활 시나리오와 더 밀접하게 관련된 일상 생활에 대해 사용자가 제기하는 대화이다. 우리는 63K개의 실제 사용자 지침으로 구성된 ShareGPT 인스트루시톤 세트를 채택한다. 비쿠나의 핵심 지침으로 사용되었습니다.
* _합성 지침_ 기존 명령어를 재사용할 뿐만 아니라 LLM을 이용하여 대량의 명령어를 자동으로 합성할 수도 있다. 우리는 약 82K 인스턴스 입력 및 출력과 쌍을 이루는 52K 명령어로 구성된 인기 있는 합성 명령어 데이터 세트 Self-Instruct-52K [143]을 채택한다. 이러한 생성된 명령어들은 인간이 작성한 시드 태스크(_e.g._, 문법 검사, 브레인스토밍)와 유사한 데이터 분포를 갖는다.

원래 FLAN-T5 데이터셋은 매우 크기 때문에(_i.e._, over 15M), 다른 명령어 데이터셋과 공정한 비교를 수행하기 위해 무작위로 80,000개의 명령어를 샘플링한다.

\begin{table}
\begin{tabular}{c|c c c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{**Models**} & \multirow{2}{*}{\begin{tabular}{c} **AS00** \\ \#GPU \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Full** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Training** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **AS00** \\ \#GPU \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **LoRa** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Training** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **AS00** \\ \#Tone \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **As00** \\ \#GPU \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Inference (16-bit)** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **3090** \\ \#Tone \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Inference (16-bit)** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **3090** \\ \#Tone \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} **Inference (8-bit)** \\ \end{tabular} } \\  & \#GPU & & & & & & & & & & & \\ \hline LLaMA (7B) & 2 & 8 & 3.0h & 1 & 80 & 3.5h & 1 & 36.6 & 1 & 24.3 & 1 & 7.5 \\ LLaMA (13B) & 4 & 8 & 3.1h & 1 & 48 & 5.1h & 1 & 26.8 & 2 & 9.9 & 1 & 4.5 \\ LLaMA (30B) & 8 & 4 & 6.1h & 1 & 24 & 14.3h & 1 & 17.7 & 4 & 3.8 & 2 & 2.6 \\ LLaMA (65B) & 16 & 2 & 11.2h & 1 & 4 & 60.6h & 2 & 8.8 & 8 & 2.0 & 4 & 1.5 \\ \hline \hline \end{tabular}
\end{table} TABLE VIII: Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning and LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based on two Linux servers having 8 A800-80G SXM4 GPUs with 6 NVSwitch and 8 3090-24G GPUs, respectively. The major difference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and inference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same. For full tuning experiments, we use data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally, the LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. All the experiments are conducted with Alpaca-52K dataset by training LLaMA models three epochs. The max sequence length for both training settings is set to 512. The inference experiments are performed with the batch size set to 1.

(_i.e._, ShareGPT 및 Self-Instruct-52K)를 유사한 스케일로 포함하는, 방법. 실험에서는 각 개별 명령 집합에 대해 테스트하여 자신의 효과를 탐색하고 모델 성능에 대한 조합 효과를 조사한다.

**개선 전략** 인간 사용자의 실제 지침이 LLM을 미세 조정 하는 데 더 적합 하지만 대규모로 수집 하는 것은 어렵습니다. 인간 생성 명령의 대안으로 기존의 대부분의 연구는 주로 LLMs에 의해 생성된 합성 명령을 채택한다. 그러나, 잘못된 주제 다양성과 고르지 못한 수업 난이도(너무 단순하거나 너무 어렵거나)와 같은 합성 수업에 대한 몇 가지 잠재적인 문제가 있다. 따라서 합성 지침의 품질을 개선할 필요가 있다. 다음으로 기존 작업에서 널리 사용되고 있는 4대 개선 전략을 정리하면 다음과 같다.

\(\bullet\)_Enhancing the instruction complexity._ 기존 작업 [346]에서 논의된 바와 같이, 명령어들의 복잡성을 향상시키는 것은 더 많은 태스크 요구들을 포함하거나 더 많은 추론 단계들을 요구하는, _예를 들어_ 복잡한 명령어들을 따르는 LLM들의 모델 용량을 향상시킬 수 있다. 이 전략의 유효성을 검사하기 위해 복잡성 수준을 점진적으로 증가시키고, 제약 조건을 추가하고, 추론 단계를 늘리고, 입력을 복잡하게 하여 위저드LM [346]을 따릅니다. 우리는 공개적으로 공개된 WizardLM-70K 명령어[346]를 복잡성 강화 명령어 데이터세트로 활용하며, 이는 Self-Instruct-52K 데이터세트[346]에 기반한 위의 강화 접근법을 통해 생성되었다.

\(\bullet\)_ 토픽 다양성의 증가_ 복잡성 외에도, 명령어 데이터세트의 토픽 다양성을 향상시키는 것은 실제 세계의 다양한 태스크들에 대한 LLM의 상이한 능력들을 이끌어내는 것을 도울 수 있다[347]. 그러나, 다양한 명령어를 생성하기 위한 자체 명령어 프로세스를 직접 제어하는 것은 어렵다. 우리는 YuLan-Chat[352]에 이어 ChatGPT를 사용하여 Self-Instruct-52K 데이터 세트의 지침을 특정 프롬프트를 통해 293개의 주제로 적응시키기 위해 다시 작성한다. 마지막으로, 다양성 증가 데이터세트로 70K 명령어를 획득한다.

\(\bullet\)_명령 번호를 스케일링하는 단계._ 상기 양상들 외에도, 명령어들의 개수는 모델 성능에 영향을 미칠 수 있는 중요한 요소이기도 하다. 특히, 더 많은 명령어를 사용하면 LLMs에 대한 작업 지식을 확장하고 명령어 수행 능력을 향상시킬 수 있다[69]. 이 전략을 조사하기 위해 MOSS 프로젝트[362]에서 릴리스된 합성 명령어 세트에서 새로운 명령어를 샘플링하는데, 이는 동일한 자체 명령어 방법[143]을 사용하여 합성되기 때문이다. 이를 Self-Instruct-52K 데이터셋과 혼합하여 220K 명령어를 포함하는 더 큰 데이터셋을 구성한다.

\(\bullet\)_수업 난이도 균형._ 합성 지침은 너무 쉽거나 딱딱한 것을 포함하는 경향이 있기 때문에 LLM에 대한 훈련 불안정성 또는 심지어 과적합으로 이어질 가능성이 있다. 잠재적인 효과를 탐색하기 위해 LLM의 복잡도 점수를 활용하여 지침의 난이도를 추정하고 너무 쉽거나 어려운 지침을 제거한다. 공정한 비교를 위해 동일한 크기의 명령어를 생성하기 위해 LLMA (7B) 모델을 채택하여 대용량 명령어 데이터셋으로부터 220K개의 명령어에 대한 복잡도를 계산하고, 70K개의 중간 복잡도 점수를 난이도 균형 데이터셋으로 유지한다.

**실험 설정** 명령 데이터의 영향에 대 한 실험을 수행 하기 위해 명령 조정에 널리 사용 된 인기 있는 LLM 백본인 LLAMA를 조정 하기 위해 이러한 새 명령 데이터 집합을 활용 합니다. 우리는 실험을 위해 YuLan-Chat [352]의 코드를 사용하고, 8개의 A800-80G GPU의 서버에서 LLAMA 7B와 13B를 훈련시킨다. 전부 다

\begin{table}
\begin{tabular}{l l c c|c c c} \hline \hline \multirow{2}{*}{**Models**} & **Dataset** & **Instruction** & **Lexical** & \multicolumn{2}{c}{**Chat**} & \multicolumn{2}{c}{**QA**} \\ \cline{3-7}  & **Mixtures** & **Numbers** & **Diversity** & **AlpacaFarm** & **MMLU** & **BBH3k** \\ \hline LLAMA (7B) & \(\ddagger\) FLAN-T5 & 80,000 & 48.48 & 23.77 & 38.58 & 32.79 \\  & \(\ddagger\) ShareGPT & 63,184 & 77.31 & 81.30 & 38.11 & 27.71 \\  & \(\ddagger\) Self-Instruct-52K & 82,439 & 25.92 & /\(\star\) & 37.52 & 29.81 \\  & \(\ddagger+\ddagger\) & 145,623 & 48.22 & 71.36 & **41.26** & 28.36 \\  & \(\ddagger+\ddagger\) & 225,623 & 48.28 & 70.00 & **43.69** & 29.69 \\ \hline \multirow{7}{*}{\(\ddagger\) Self-Instruct-52K} & 82,439 & 25.92 & /\(\star\) & 37.52 & 29.81 \\  & \(\warrow\) complexity & 70,000 & 70.43 & 76.96 & 39.73 & 33.25 \\  & \(\warrow\) diversity & 70,000 & 75.59 & 81.55 & 38.01 & 30.03 \\  & \(\warrow\) difficulty & 70,000 & 73.48 & 79.15 & 32.55 & 31.25 \\  & \(\warrow\) scaling & 220,000 & 57.78 & 51.13 & 33.81 & 26.63 \\ \hline LLAMA (13B) & \(\ddagger\) FLAN-T5 & 80,000 & 48.48 & 22.12 & 34.12 & 34.05 \\  & \(\ddagger\) ShareGPT & 63,184 & 77.31 & 72.13 & **47.49** & 33.82 \\  & \(\ddagger\) Self-Instruct-52K & 82,439 & 25.92 & /\(\star\) & 36.73 & 25.43 \\  & \(\ddagger+\ddagger\) & 145,623 & 48.22 & 72.85 & 41.16 & 29.49 \\  & \(\ddagger+\ddagger+\ddagger\) & 225,623 & 48.28 & 69.49 & **43.50** & 31.16 \\ \hline \multirow{7}{*}{\(\ddagger\) Self-Instruct-52K} & 82,439 & 25.92 & /\(\star\) & 36.73 & 25.43 \\  & \(\warrow\) complexity & 70,000 & 70.43 & 77.94 & 46.89 & 35.75 \\ \cline{1-1}  & \(\warrow\) diversity & 70,000 & 75.59 & 78.92 & 44.97 & 36.40 \\ \cline{1-1}  & \(\warrow\) difficulty & 70,000 & 73.48 & 80.45 & 43.15 & 34.59 \\ \cline{1-1}  & \(\warrow\) scaling & 220,000 & 57.78 & 58.12 & 38.07 & 27.28 \\ \hline \hline \end{tabular}
\end{table} TABLE IX: Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLAMA (7B) and LLAMA (13B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K dataset, _i.e._, enhancing the complexity (_w/ complexity_), increasing the diversity (_w/ diversity_), balancing the difficulty (_w/ difficulty_), and scaling the instruction number (_w/ scaling_). \(\star\)Since we select the LLAMA (7B)/(13B) model fine-tuned on Self-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.

초매개 변수 설정은 스탠포드 알파카와 동일하게 유지됩니다. 미세 조정된 모델의 명령어 추종 능력을 더 잘 평가하기 위해 두 가지 설정, 즉 _채팅 설정_ 및 _QA 설정_을 고려한다. 채팅 설정은 주로 일일 채팅의 사용자 명령 및 쿼리를 활용하는 반면, QA 설정은 주로 기존 NLP 데이터 세트의 질문 응답 예를 사용한다. 채팅 설정에 대한 평가는 AlpacaFarm 평가 세트에 기초하여 수행된다[363]. 전체 쌍별 비교를 사용하는 대신 Self-Instruct-52K에서 미세 조정된 LLaMA 7B 및 13B 모델을 참조 기준선으로 선택한 다음 각각 다른 지침을 사용하여 미세 조정된 다른 LLaMA 7B 및 13B 모델과 비교한다. 우리의 초점은 지침을 생성하기 위한 다양한 전략의 유용성을 조사하는 것이기 때문에 Self-Instruct-52K에서 미세 조정된 모델은 좋은 참조가 될 수 있다. AlpacaFarm[363]에 이어 각 비교에 대해 ChatGPT를 사용하여 매번 두 개의 비교 모델의 응답이 사용자 쿼리에 가장 적합한지 자동으로 주석하고 승률(%)을 평가 메트릭으로 보고한다. QA 설정을 위해 MMLU [364]와 BBH [365]의 두 벤치마크를 선택하고 휴리스틱 규칙을 사용하여 이러한 LLM의 답변을 구문 분석하여 기본 설정에 따라 정확도를 평가한다.

명령 조정 및 평가를 위해 다음과 같은 프롬프트를 채택합니다. _"다음은 인간과 AI 어시스턴트의 대화입니다. AI 어시스턴트는 사용자의 질문에 대해 유용하고 상세하며 예의 바른 답변을 제공합니다.\(\(|\)Humann\(|\):\(\{input\}\backslash n|AI|\)]_. 결과를 재현하기 위해 [https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments](https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments) 링크에서 코드와 데이터를 릴리스합니다.

**결과 및 분석** 7B 및 13B LLaMA를 기반으로 하는 서로 다른 명령 데이터 세트를 사용한 결과는 표 IX에 나와 있습니다. 다음으로, 우리의 연구 결과를 자세히 요약하고 분석한다.

\(\bullet\)_Task-formatted 명령어는 QA 설정에 더 적합하지만 채팅 설정에는 유용하지 않을 수 있습니다._ FLAN-T5를 이용한 명령어 튜닝의 성능을 ShareGPT와 Self-Instruct-52K의 명령어 튜닝 성능과 비교하여, 채팅 환경에서 FLAN-T5가 ShareGPT보다 성능이 떨어지는 반면, QA 벤치마크에서는 FLAN-T5가 가장 우수한 성능을 보임을 알 수 있다. 그 이유는 FLAN-T5가 기존의 NLP 작업, _예:_, 번역 및 읽기 이해의 명령어와 예제의 혼합물로 구성되어 있기 때문이다. 결과적으로 FLAN-T5로 미세 조정된 LLaMA는 QA 작업에서는 더 나은 성능을 보이지만 사용자 쿼리에서는 좋지 않다. 대조적으로, ShareGPT는 실제 인간-ChatGPT 대화로 구성되며, 이는 일상 생활에서 사용자 지시를 따르도록 LLaMA를 더 잘 이끌어낼 수 있지만 QA 작업을 수행하기에 적합하지 않을 수 있다.

다양한 지침의 혼합은 LLM의 포괄적인 능력을 향상시키는데 도움이 된다._ 미세 조정을 위해 세 가지 종류의 명령어를 혼합한 후, 도출된 LLaMA 변형(FLAN-T5, ShareGPT 및 Self-Instruct-52K 포함)이 두 작업 설정 모두에서 잘 수행됨을 알 수 있다. MLU에서 LLaMA (7B)의 성능은 개별 명령어를 사용하는 명령어보다 큰 마진, 즉 43.69 vs. 38.58 (FLAN-T5). 이는 명령어 데이터 집합의 여러 소스를 혼합하는 것이 명령어 수를 확장하고 다양성을 증가시키는 명령어 조정 LLM의 성능을 향상시키는 데 도움이 된다는 것을 보여준다.

\(\bullet\)_명령어의 복잡성과 다양성을 향상시키면 모델 성능이 향상된다._ Self-Instruct-52K 데이터 세트의 복잡성과 다양성을 각각 증가시킴으로써 LLaMA의 채팅 및 QA 성능을 LLaMA(7B)용 MMLU에서 37.52에서 39.73으로 일관되게 향상시킬 수 있다. 두 전략 모두 LLM의 수업 추종 능력을 향상시키는 데 유용함을 보여준다. 또한, 복잡도를 개선하면 QA 작업에서 더 큰 성능 향상을 얻을 수 있음을 알 수 있다. 그 이유는 QA 과제는 대부분 LLM을 평가하기 어려운 문항으로 구성되어 있으며, 미세 조정 단계에서 복잡한 지시를 배운 LLM이 더 잘 해결할 수 있기 때문이다.

\(\bullet\)_단순히 명령어들의 수를 증가시키는 것은 그다지 유용하지 않을 수 있고, 난이도들의 균형을 맞추는 것이 항상 도움이 되는 것은 아니다_ 표 IX에 표시된 결과와 같이 난이도의 균형을 맞추고 미세 조정 지침의 수를 늘리는 것은 우리의 실험에 큰 도움이 되지 않는다. 특히 명령어 수를 스케일링하는 경우, LLaMA(7B)에 대한 BBH에서 29.81에서 26.63으로 감소하는 _예를 들어_ 성능까지 손상시킨다. 이는 품질 관리 없이 단순히 합성 명령어의 수를 스케일링하는 것이 성능 향상에 효과적이지 않을 수 있음을 보여준다. 또한, 중간 난이도의 명령어에 대한 미세 조정은 채팅 설정에서도 잘 수행되지만 QA 설정에서는 약간 성능이 저하된다. 가능한 이유는 복잡성 점수가 큰 복잡하고 어려운 지침을 필터링하여 복잡한 질문에 응답하는 모델 성능을 손상시키기 때문이다.

\(\bullet\)_ 모델 규모가 클수록 성능이 향상됩니다._ 동일한 명령어 데이터로 미세 조정된 LLaMA(7B) 모델과 LLaMA(13B) 모델의 성능을 비교하여 LLaMA(13B) 모델이 대부분 더 나은 성능을 달성한다는 것을 알 수 있다. 이는 모델 크기를 스케일링하는 것이 명령어 추종 능력을 향상시키는데 도움이 된다는 것을 나타낸다. 또한, MMLU에서 QA 성능이 38.11에서 47.49로 많이 향상되었음을 알 수 있다. 더 큰 모델은 일반적으로 더 복잡한 질문에 정확하게 대답할 수 있는 지식 활용 및 추론 능력[33, 55]이 더 우수하기 때문일 수 있다.

\(\bullet\)**Instruct-tuning Suggestions**

LLM에 대한 명령어 튜닝을 수행하기 위해 표 VIII의 필요한 GPU 수와 튜닝 시간에 대한 기본 통계에 따라 계산 자원을 준비할 수 있다. 개발 환경을 설정한 후, 초보자는 명령어 튜닝을 위해 Alpaca 저장소 [137]의 코드를 따르는 것이 좋다. 그런 다음 기본 모델을 선택하고 이 섹션에서 논의하는 대로 명령어 데이터 세트를 구성해야 한다. 훈련을 위한 계산 리소스가 제약될 때, 사용자는 파라미터-효율적 튜닝을 위해 LoRA를 활용할 수 있다(섹션 5.3 참조). 추론에 대해, 사용자는 더 적거나 더 작은 GPU에 LLM을 배포하기 위해 양자화 방법을 더 사용할 수 있다(섹션 5.4 참조).

### _Alignment Tuning_

이 부분은 먼저 정렬의 배경과 그 정의 및 기준을 제시하고, LLM을 정렬하기 위한 인간 피드백 데이터 수집에 초점을 맞추고, 마지막으로 정렬 조정을 위한 인간 피드백(RLHF)으로부터의 강화 학습의 핵심 기술에 대해 논의한다.

#### 5.2.1 배경 및 정렬 기준

**배경.** LLM은 광범위한 NLP 작업에서 놀라운 기능을 보여 줍니다. [55, 56, 67, 90]. 그러나 이러한 모델은 때때로 의도하지 않은 행동, 예를 들어 허위 정보 조작, 부정확한 목표 추구, 유해하고 오해의 소지가 있으며 편향된 표현을 생성할 수 있다[66, 366]. LLM의 경우 언어 모델링 목표는 인간의 가치나 선호도에 대한 고려가 부족하면서 단어 예측에 의해 모델 매개변수를 사전 훈련한다. 이러한 예상치 못한 행동을 방지하기 위해, LLM이 인간의 기대에 따라 행동하도록 하는 인간 정렬이 제안되었다[66, 367]. 그러나, 원래의 사전 훈련 및 적응 튜닝(_e.g._, 명령어 튜닝)과는 달리, 그러한 정렬은 매우 상이한 기준들(_e.g._, 유용성, 정직성 및 무해성)을 고려해야 한다. 정렬은 LLM들의 일반적인 능력들을 어느 정도 손상시킬 수 있는 것으로 나타났으며, 이는 관련 문헌들에서 _정렬 세금_이라고 불린다[368].

**정렬 기준.** 최근 LLM의 동작을 규제하기 위한 다양한 기준을 개발하는 데 관심이 증가하고 있습니다. 여기서는 기존 문헌[66, 368]에서 널리 채택된 논의의 예로서 세 가지 대표적인 정렬 기준(_i.e._, helpful, honest, and harmless)을 취한다. 또한, 행동, 의도, 인센티브, 및 내부 양상들을 포함하는 상이한 관점들 [366]로부터의 LLMs에 대한 다른 정렬 기준들이 존재하며, 이들은 상기 세 가지 기준들과 본질적으로 유사(또는 적어도 유사한 정렬 기법들을 가짐)하다. 또한 정직성을 정확성으로 대체하면서 구체적인 필요에 따라 세 가지 기준을 수정할 수 있다[116]. 다음으로 대표적인 세 가지 정렬 기준에 대해 간략히 설명한다:

\(\bullet\)_Helpfulness._ 도움이 되려면 LLM은 사용자가 작업을 해결하거나 질문에 가능한 간결하고 효율적인 방식으로 답변하는 데 도움이 되는 명확한 시도를 보여야 한다. 더 높은 수준에서 추가 설명이 필요할 때 LLM은 관련 조사를 통해 추가 관련 정보를 이끌어내는 능력을 입증하고 적절한 수준의 민감도, 통찰력 및 신중함을 보여야 한다[368]. 유용한 행동의 정렬을 실현하는 것은 사용자의 의도를 정확하게 정의하고 측정하기 어렵기 때문에 LLM에게 어렵다[366].

\(\bullet\)_정직함_ 기본적인 수준에서 정직하게 정렬된 LLM은 정보를 조작하는 대신 사용자에게 정확한 내용을 제시해야 한다. 또한 LLM은 정보의 기만이나 잘못된 표현을 피하기 위해 산출물에 적절한 정도의 불확실성을 전달하는 것이 중요하다. 이는 모델이 자신의 능력 및 지식 수준(_e.g._, "알 수 없음")에 대해 알 것을 요구한다. [368]의 논의에 따르면 정직은 유용성과 무해성에 비해 더 객관적인 기준이므로 정직 정렬은 잠재적으로 인간의 노력에 덜 의존하여 개발될 수 있다.

\(\bullet\)_Harmlessness._ 해롭지 않기 위해서는 모델이 생산하는 언어가 모욕적이거나 차별적이어서는 안 된다는 것을 요구한다. 최선의 능력으로, 모델은 악의적인 목적으로 요청을 요청하는 것을 목표로 하는 은밀한 시도를 탐지할 수 있어야 한다. 이상적으로, 모델이 위험한 행동(_예를 들어, 범죄를 저지르도록 유도되었을 때, LLM은 정중하게 거절해야 한다. 그럼에도 불구하고, _어떤 행동_ 은 유해한 것으로 간주되며 개인 또는 사회 간에 _어느 정도까지_ 다양합니다. [368]은 LLM을 사용하는 사람, 제기된 질문 유형 및 LLM이 사용되는 컨텍스트(예: 시간)에 크게 의존합니다.

우리가 볼 수 있듯이, 이러한 기준은 상당히 주관적이며, 인간의 인지를 기반으로 개발된다. 따라서 LLM에 대한 최적화 목표로 직접 공식화하는 것은 어렵다. 기존 작업에서는 LLM을 정렬할 때 이러한 기준을 충족하는 방법이 많이 있다. 유망한 기술은 수동 또는 자동화된 수단을 사용하여 적대적인 방식으로 LLM을 프로브하여 유해한 출력을 생성한 다음 이러한 출력을 방지하기 위해 LLM을 업데이트하는 _red teaming_[369]이다.

#### 5.2.2 Collecting Human Feedback

사전 훈련 단계에서 LLM은 대규모 코퍼스에서 언어 모델링 목표를 사용하여 훈련된다. 그러나 이는 인간에 의한 LLM 산출물에 대한 주관적이고 질적인 평가(이 조사에서 _인간 피드백_이라고 함)를 고려할 수 없다. 고품질의 인간 피드백은 LLM을 인간의 선호도 및 가치와 정렬하는 데 매우 중요하다. 이 부분에서는 피드백 데이터 수집을 위해 인간 레이블러 팀을 선택하는 방법에 대해 논의한다.

**인간 레이블러 선택** 기존 작업에서 인간 피드백 데이터를 생성하는 주요 방법은 인간 주석입니다[66, 367, 116]. 이것은 적절한 인간 라벨러를 선택하는 중요한 역할을 강조한다. 고품질 피드백을 제공하기 위해 인간 라벨러는 자격 있는 수준의 교육과 우수한 영어 능력을 갖추어야 합니다. 예를 들어, Sparrow[116]은 인간 라벨러들이 적어도 학부 수준의 교육 자격을 획득한 영국 기반 원어민 화자일 것을 요구한다. 그럼에도 불구하고 여러 연구[367]에서 연구자와 인간 라벨러의 의도 사이에 여전히 불일치가 있음을 발견했으며, 이는 낮은 품질의 인간 피드백으로 이어질 수 있고 LLM이 예상치 못한 출력을 생성하게 할 수 있다. 이 문제를 해결하기 위해 InstructGPT [66]은 인간 라벨러와 연구원 간의 일치를 평가하여 라벨러를 필터링하는 스크리닝 프로세스를 추가로 수행한다. 구체적으로, 연구자들은 먼저 소량의 데이터에 라벨을 붙인 다음 자신과 인간 라벨러 사이의 일치를 측정한다. 일치도가 가장 높은 레이블러를 선택하여 후속 주석 작업을 진행합니다. 일부 다른 작업[370]에서, "슈퍼 평가자"는 인간 피드백의 높은 품질을 보장하기 위해 사용된다. 연구자들은 인간 라벨러의 성능을 평가하고 성능이 좋은 인간 라벨러 그룹(예: 높은 일치도)을 슈퍼 평가자로 선택한다. 초평가자들은 후속 연구에서 연구자들과 협력하는 것이 우선시될 것이다. 인간 라벨러가 LLM의 출력에 주석을 달 때, 상세한 지침을 지정하고 인간 라벨러에 대한 즉각적인 안내를 제공하는 것이 도움이 되며, 이는 라벨러의 주석을 추가로 규제할 수 있다.

**Human Feedback Collection.** 기존 작업에서는 주로 인간 레이블러에서 피드백 및 선호도 데이터를 수집하는 세 가지 접근 방식이 있습니다.

* _랭킹 기반 접근법_ 초기 작업[367]에서 인간 라벨러는 종종 더 세밀한 정렬 기준을 고려하지 않고 거친 입자 방식(_i.e._, 최상의 선택만)으로 모델 생성 출력을 평가한다. 그럼에도 불구하고, 다른 라벨러는 최상의 후보 출력 선택에 대해 다양한 의견을 가질 수 있으며, 이 방법은 선택되지 않은 샘플을 무시하여 부정확하거나 불완전한 인간 피드백으로 이어질 수 있다. 이 문제를 해결하기 위해 후속 연구[116]에서는 Elo 등급제를 도입하여 후보 산출물을 비교하여 선호도 순위를 도출한다. 출력의 순위는 모델이 다른 출력보다 특정 출력을 선호하도록 안내하는 훈련 신호 역할을 하며, 따라서 더 신뢰할 수 있고 안전한 출력을 유도한다.
* _질문 기반 접근 방식_ 또한, 인간 라벨러는 LLM에 대한 추가 제약뿐만 아니라 정렬 기준을 커버하면서 연구자들에 의해 설계된 특정 질문에 응답함으로써 보다 상세한 피드백을 제공할 수 있다[81]. 특히, WebGPT[81]에서는 모델이 검색된 문서로부터 관련 정보를 필터링하고 활용하는 것을 돕기 위해, 인간 라벨러는 검색된 문서가 주어진 입력에 응답하는데 유용한지 여부에 대한 여러 옵션으로 질문에 대답해야 한다.
* _규칙 기반 접근법_ 또한 많은 연구에서 보다 상세한 인간 피드백을 제공하기 위해 규칙 기반 방법을 개발한다. 전형적인 경우로서, Sparrow[116]은 라벨러들이 가장 잘 고려하는 응답을 선택할 뿐만 아니라 모델 생성 응답이 유용하고 정확하며 무해하다는 정렬 기준을 충족하는지 여부를 테스트하기 위해 일련의 규칙을 사용한다. 이러한 방식으로, 두 종류의 인간 피드백 데이터가 획득될 수 있다 : (1) 응답 선호도 피드백은 모델 생성 출력의 품질을 쌍으로 비교함으로써 획득되고, (2) 규칙 위반 피드백은 인간 라벨러로부터 평가를 수집함으로써 획득된다(_i.e._, 생성된 출력이 규칙을 어느 정도 위반했는지 나타내는 점수). 더욱이, GPT-4 [46]은 규칙-기반 보상 모델들로서 제로-샷 분류기들의 세트(GPT-4 자체에 기초함)를 활용하는데, 이는 모델-생성 출력들이 인간-작성된 규칙들의 세트를 위반하는지 여부를 자동으로 결정할 수 있다.

이하에서는 ChatGPT와 같은 최근 강력한 LLMs에서 널리 사용되고 있는 인간 피드백(RLHF)으로부터의 강화 학습(reinforcement learning) 기술에 초점을 맞춘다. 아래에서 논의되는 바와 같이, 섹션 5.2.1에서 소개된 정렬 기준은 사용자의 질의에 대한 LLM의 응답에 대한 인간 피드백으로부터 학습함으로써 충족될 수 있다.

#### 5.2.3 Reinforcement Learning from Human Feedback

LLM을 인간 값과 정렬하기 위해, 수집된 인간 피드백 데이터로 LLM을 미세 조정하기 위해 인간 피드백(RLHF)으로부터의 강화 학습[79, 367]이 제안되었는데, 이는 정렬 기준(_e.g._, 유용성, 정직성 및 무해성)을 개선하는데 유용하다. RLHF는 보상 모델을 학습하여 LLM을 인간의 피드백에 적응시키기 위해 강화 학습(RL) 알고리즘(_e.g._, PPO(Proximal Policy Optimization) [128])을 사용한다. 이러한 접근법은 InstructGPT[66]에 예시된 바와 같이, 잘 정렬된 LLM들을 개발하기 위한 트레이닝 루프에 인간을 통합한다.

**RLHF 시스템.** RLHF 시스템은 주로 정렬될 사전 훈련된 LM, 인간 피드백으로부터의 보상 모델 학습 및 LM을 훈련시키는 RL 알고리즘의 세 가지 주요 구성 요소로 구성된다. 구체적으로, _사전 트레이닝된 LM_은 일반적으로 기존의 사전 트레이닝된 LM 파라미터로 초기화되는 생성 모델이다. 예를 들어 OpenAI는 첫 번째 인기 RLHF 모델인 InstructGPT[66]에 175B GPT-3를 사용하고, DeepMind는 GopherCite 모델인 Go-pher[64]를 사용한다[370]. 또한, _보상 모델(RM)_은 일반적으로 스칼라 값의 형태로 LM에 의해 생성된 텍스트에 대한 인간의 선호도를 반영하는 (학습된) 안내 신호를 제공한다. 보상 모델은 미세 조정된 LM 또는 인간 선호도 데이터를 사용하여 LM 트레이닝된 새로운 두 가지 형태를 취할 수 있다. 기존의 작업은 통상적으로 정렬된 LM[66, 370]과 상이한 파라미터 스케일을 갖는 보상 모델들을 채용한다. 예를 들어 OpenAI는 6B GPT-3를, DeepMind는 7B Gopher를 각각 보상 모델로 사용한다. 마지막으로 보상 모델의 신호를 사용하여 사전 훈련된 LM을 최적화하기 위해 대규모 모델 튜닝을 위해 특정 _RL 알고리즘_이 설계된다. 구체적으로, PPO(Proximal Policy Optimization) [128]은 기존 작업에서 정렬을 위해 널리 사용되는 RL 알고리즘이다[66, 116, 370].

**RLHF에 대 한 주요 단계** 그림 12는 아래에 소개 된 RLHF [66]의 전체 3 단계 프로세스를 보여 줍니다.

\(\bullet\)_Supervised fine-tuning._ LM이 초기에 원하는 동작을 수행하도록 하기 위해서는 일반적으로 LM을 미세 조정하기 위해 입력 프롬프트(명령어) 및 원하는 출력을 포함하는 감독된 데이터 세트를 수집해야 한다. 이러한 프롬프트 및 출력은 작업의 다양성을 보장하면서 일부 특정 작업에 대해 인간 라벨러에 의해 작성될 수 있다. 예를 들어, InstructGPT [66]은 인간 라벨러에게 프롬프트(_예: _, _"내 경력에 대한 열정을 되찾는 방법에 대한 다섯 가지 아이디어를 나열"_)를 작성하고 개방형 QA, 브레인스토밍, 채팅 및 다시 쓰기와 같은 여러 생성 작업에 대해 원하는 출력을 작성하도록 요청한다. 첫 번째 단계는 특정 설정 또는 시나리오에서 선택 사항입니다.

\(\bullet\)_Reward model training._ 두 번째 단계는 인간의 피드백 데이터를 이용하여 RM을 학습시키는 것이다. 특히 LM을 사용하여 샘플링된 프롬프트(감독 데이터 세트 또는 인간 생성 프롬프트 중 하나)를 입력으로 사용하여 특정 수의 출력 텍스트를 생성한다. 그런 다음 초대

도. 12: RLHF 알고리즘의 워크플로우.

이러한 쌍에 대한 기본 설정에 주석을 달기 위해 인간 레이블러입니다. 어노테이션 과정은 여러 형태로 진행될 수 있으며, 공통적인 접근 방식은 생성된 후보 텍스트들의 순위를 매겨 주석하는 것으로 주석자 간의 불일치를 줄일 수 있다. 그런 다음, RM은 인간이 선호하는 출력을 예측하도록 훈련된다. InstructGPT에서 라벨러는 모델 생성 출력을 최상의 출력에서 최악의 출력으로 순위를 매기고 RM(_i.e._, 6B GPT-3)은 순위를 예측하도록 훈련된다. 최근 작업[371]에서 응답 쌍에 대한 선호도 주석은 인간 대신 AI 에이전트(일반적으로 정렬된 LLM)에 의해 수행되었으며, 이를 _"AI 피드백으로부터의 강화 학습(RLAIF)"_이라고 한다. 전형적인 RLHF 알고리즘으로 훈련된 LLM은 도움이 덜 되는 무해한 응답을 생성하는 경향이 있으며, 이를 _회피 문제_[371]이라고 한다. 무해성과 유용성을 모두 보장하기 위해, RLAIF는 명령어들 [371, 372]에서 미리 설정된 정렬 원리들에 기초하여 AI 피드백을 생성하며, 이는 또한 인간 주석의 노력을 감소시킬 수 있다.

\(\bullet\)_RL fine-tuning_. 이 단계에서, LM을 정렬(_i.e._, 미세-조정)하는 것은 RL 문제로서 공식화된다. 이 설정에서, 미리 훈련된 LM은 프롬프트를 입력으로서 취하고 출력 텍스트를 반환하는 정책으로서 동작하고, 그것의 동작 공간은 어휘이고, 상태는 현재 생성된 토큰 시퀀스이고, 보상은 RM에 의해 제공된다. 초기(튜닝 전) LM에서 크게 벗어나는 것을 방지하기 위해, 벌칙 항은 일반적으로 보상 함수에 통합된다. 예를 들어, InstructGPT는 PPO 알고리즘을 사용하여 RM에 대해 LM을 최적화한다. 각 입력 프롬프트에 대해, InstructGPT는 현재 LM으로부터 생성된 결과들과 초기 LM 사이의 KL 발산을 패널티로서 계산한다. 두 번째 및 마지막 단계는 LLM을 더 잘 정렬하기 위해 여러 턴으로 반복될 수 있다는 점에 유의한다. RL 알고리즘의 불안정성으로 인해, 최근의 작업 [373]은 더 높은 보상을 갖는 최상의 순위 샘플들을 재사용함으로써 RL 튜닝을 또 다른 감독된 미세-튜닝으로 대체한다.

**RLHE를 위한 실용적인 전략**. RLHF는 LLM과 인간의 정렬을 효과적으로 개선하는 것이 유망하지만, 연구자들이 이를 성공적으로 구현하는 것은 실질적으로 어렵다. 이 부분에서는 RLHF의 효과와 효율성을 개선하기 위한 몇 가지 유용한 전략과 요령을 논의하는 데 중점을 둔다. 구체적으로 보상 모델의 효과적인 훈련과 효율적인 RL 훈련에 중점을 둔다.

\(\bullet\)_효과적인 보상 모델 훈련_. InstructGPT가 작은 보상 모델(6B GPT 모델)을 사용했음에도 불구하고, 증가하는 작업[99]은 큰 보상 모델(_e.g._, 원래 모델 크기 이상)을 사용하는 것이 종종 더 효과적인 것으로 나타났는데, 큰 보상 모델은 일반적으로 LLM 생성 출력의 품질을 판단하는 데 더 나은 성능을 수행하기 때문이다. LLMaMa 2[99]에서는 사전 훈련된 채팅 모델 체크포인트를 사용하여 보상 모델을 초기화하고, 이러한 접근법이 동일한 사전 훈련 지식을 공유함으로써 정렬될 모델과 보상 모델 사이의 정보 불일치를 효과적으로 감소시킬 수 있다고 주장한다. 반면, 대규모 보상 모델을 훈련할 때 과적합 문제에 직면하는 것이 일반적이다. 단순하면서도 효과적인 해결책으로서, 기존의 작업 [374, 375]은 정규화기로서 인간 주석이 달린 정렬 데이터세트로부터의 입력 프롬프트의 선호 응답에 대한 LM 손실을 도입하였고, 이는 이진 분류 태스크에 대한 보상 모델의 과적합(overfitting)을 완화시킨다. 또한, 정렬(_e.g._, helpfulness and honesty)에 대한 기준이 여러 개 존재하기 때문에, 정렬 기준을 모두 만족시킬 수 있는 하나의 보상 모델을 학습시키기 어려운 경우가 많다. 따라서 서로 다른 정렬 기준에 초점을 맞춘 여러 보상 모델을 훈련하고 특별한 조합 전략(_예:_, 평균 풀링 및 가중 합)을 통해 이들로부터 생성된 보상을 기반으로 최종 보상을 계산하는 것이 유용하다. 이러한 방법을 통해 여러 기준, 예를 들어 유용성에 대한 요구 사항을 완화하면서 유해성에 대한 더 엄격한 제한을 적용할 수 있는 보다 유연한 규칙 또는 표준을 사용할 수 있습니다.

\(\bullet\)_효과적인 RL 훈련_. RL 훈련 과정이 불안정하고 하이퍼-파라미터에 민감한 경향이 있으므로, 언어 모델은 좋은 모델 용량에 도달하기 위해 RL 훈련 전에 잘 감독되고 미세 조정되어야 한다고 제안된다. 일반적으로 사용되는 방법은 선형 데이터 세트에서 RL 전에 수렴할 때까지 프롬프트의 최상의 출력(거부 샘플링_ 또는 _best-of-\(N\)_ 이라고 함)에서 LLM을 미세 조정하는 것입니다. 프롬프트가 주어지면, LLM은 먼저 샘플링 알고리즘을 통해 \(N\) 출력을 생성하고, 그 다음 모델로부터 최상의 후보는 학습을 위한 보상 모델에 의해 선택될 것이다. 수렴될 때까지 최상의 샘플에서 LLM을 미세 조정한 후, 성능을 더욱 향상시키기 위해 RL 프로세스가 수행될 것이다. LLMaA 2[99]는 5가지 버전의 RLHF 모델을 연속적으로 훈련시켰으며, 여기서 LLM은 보상 모델의 개선과 함께 점진적으로 개선되었다. 이러한 방식으로, 수집된 인간 선호도 데이터의 프롬프트 및 주석은 현재 모델 체크포인트의 문제를 더 잘 반영할 수 있고, 따라서 이러한 문제를 해결하기 위해 특별한 튜닝을 할 수 있다. 또한 LLMaA 2는 반복 최적화 동안 가능한 용량 회귀 문제를 완화하기 위해 이전 반복의 샘플을 후속 샘플에 추가한다.

\(\bullet\)_Efficient RL training_. RL 훈련이 필요한 만큼

도. 13: 네 가지 상이한 파라미터-효율적인 미세-조정 방법의 예시. MHA와 FFN은 각각 트랜스포머 계층에서 멀티헤드 어텐션 네트워크와 피드포워드 네트워크를 나타낸다.

LLM 및 보상 모델 모두의 추론 프로세스를 반복하면, 특히 더 큰 보상 모델 및 LLM에 대해 총 메모리 및 계산 비용을 크게 증가시킬 것이다. 실제 속임수로 우리는 별도의 서버에 보상 모델을 배포하고 해당 API를 호출하여 자체 서버에서 LLM과 작동할 수 있다. 또한, RLHF는 다수의 후보 출력들을 생성하기 위해 LLM을 요구하기 때문에, 샘플 디코딩 절차를 여러 번 호출하는 대신에, 빔 탐색 디코딩 알고리즘26을 활용하는 것이 더 효율적이다. 응답 생성을 위해 원 패스 디코딩만 수행하면 되는 반면, 그러한 전략은 또한 생성된 후보 응답들의 다양성을 향상시킬 수 있다.

각주 26: [https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generationtransformers.GenerationMixin.group_beam_search](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generationtransformers.GenerationMixin.group_beam_search)

**프로세스-감독된 RLHF.** RLHF의 기존 문헌 [376]에서 RL 훈련을 위한 감독 신호는 일반적으로 결과-감독 신호와 프로세스-감독 신호의 두 가지 별개의 범주로 분류될 수 있다. 결과 감독 RLHF는 LLM에 의해 생성된 전체 텍스트의 품질을 평가하기 위해 정량적 점수를 사용한다. 대조적으로, 프로세스-감독된 RLHF는 생성된 콘텐츠 내의 각각의 개별 컴포넌트(_e.g._, 문장, 단어, 또는 추론 단계)의 평가를 제공하며, 이는 트레이닝을 안내하기 위해 세밀한 감독 신호를 제공하여 LLMs가 원하지 않는 생성 콘텐츠를 정제하는 것을 도울 수 있다[376, 377]. OpenAI는 12K 프로세스 주석이 달린 수학적 문제(_i.e._, MATH 데이터 세트[378])와 이러한 문제의 LLM에 의해 생성된 75K 솔루션으로 구성된 PRM800k [377]이라는 세밀한 주석 데이터 세트를 제안했으며, 여기서 수학적 문제의 각 추론 단계는 PRM800k에서 _긍정_, _부정_ 또는 _중립_으로 표시된다. 이 세립화된 데이터셋은 기존 작업[377, 379]에서 프로세스 감독 보상 모델(PRM)을 훈련하는데 활용되었으며, RLHF 절차 중 감독 신호로 각 레이블의 예측으로 인한 확률을 고려할 수 있다. PRM의 프로세스 감독 신호를 효과적으로 활용하기 위해 기존 작업[376]은 전문가 정책에서 학습을 통해 기본 정책을 개선하는 효과적인 RL 알고리즘인 전문가 반복[380, 381]을 활용했다. 일반적으로 전문가 반복은 정책 개선과 증류[376]의 두 가지 주요 단계를 포함한다. 정책 개선 단계에서는 전문가 정책에서 체계적인 검색 절차를 처리하여 표본을 생성한다. PRM은 검색 절차에서 전문가 정책을 안내하고 샘플의 품질을 향상시키기 위해 프로세스 감독 신호를 제공한다. 이후 증류 단계에서는 1단계의 전문가 정책에 의해 생성된 샘플을 활용하여 감독 미세 조정을 통해 기본 정책을 개선한다. 전문가 반복 외에도, PRM은 LLMs[377]에 의해 생성된 최종 답변의 후보를 재순위화하거나 단계별 추론 동안 더 나은 중간 추론 단계를 선택하는 데에도 활용될 수 있다[382, 379].

#### 5.2.4 Alignment without RLHF

RLHF는 LLM의 행동을 인간의 가치 및 선호도와 정렬하는 데 큰 성공을 거두었지만 주목할 만한 한계도 가지고 있다. 첫째, RLHF는 정렬되는 모델, 보상 모델 및 참조 모델을 동시에 포함하는 다수의 LLM을 훈련시킬 필요가 있으며, 이는 알고리즘 절차에서 지루하고 실제로 메모리를 소모한다. 게다가, RLHF에서 일반적으로 사용되는 PPO 알고리즘은 다소 복잡하고 종종 하이퍼-파라미터에 민감하다. 대안으로서, 증가하는 연구는 강화 학습 없이 감독된 미세 조정을 사용하여 인간의 선호를 고수하기 위해 LLM을 직접 최적화하는 것을 탐구한다[349].

**개요.** 비RL 정렬 접근법의 기본 아이디어는 고품질 _정렬 데이터 세트_ 에서 _지도 학습_ 으로 LLM을 직접 미세 조정하는 것입니다. 기본적으로 안전하지 않은 행동을 방지하기 위한 응답 피드백 또는 황금 규칙이 특수하게 선별된 정렬 데이터 세트에 주입되거나 포함되어 LLM이 적절한 미세 조정 전략을 통해 이러한 시연 데이터에서 정렬된 행동을 직접 학습할 수 있다고 가정한다. 따라서 이 방법을 구현하기 위해 두 가지 주요 문제는 정렬 데이터 세트의 구성과 미세 조정 손실 설계이다. 첫 번째 이슈에 대해, 정렬 데이터세트는 사람이 작성한 안전 원리에 따라 정렬된 LLM에 의해 자동으로 구성될 수 있다[347] 또는 편집 동작을 사용하여 기존 예제를 정제한다[383]. 또한, 기존의 보상 모델을 재사용하여 기존의 인간 피드백 데이터에서 높은 평가를 받은 응답을 선택할 수도 있다[373]. 두 번째 이슈의 경우, 비-RL 정렬 접근법은 주로 고품질 정렬 데이터 세트에서 지도 학습 방식(원래 명령어 튜닝 손실과 동일)으로 LLM을 미세 조정하는 반면, 보조 학습 목표는 정렬 성능, _예:_, 순위 응답 또는 대조 명령-응답 쌍을 향상시키는 데 사용할 수 있다.

**선형 데이터 수집.** 선형 데이터의 구성은 LLM의 동작을 인간의 선호도와 효과적으로 정렬하는 데 중요합니다. 고품질 정렬 데이터를 수집하기 위해 일부 작업은 기존 보상 모델을 재사용하여 등급이 높은 응답을 선택하고 다른 작업은 강력한 LLM(_e.g._, ChatGPT)을 활용하거나 시뮬레이션 환경을 구축하여 합성 정렬 예를 생성합니다. 다음으로, 우리는 이 세 가지 연구 라인에 대해 논의할 것이다.

\(\bullet\)_보상 모델 기반 접근법._ RLHF의 보상 모델은 LLM의 응답에 대한 정렬 정도를 측정하기 위해 훈련되었다. 기존 보상 모델을 활용하여 고품질 반응을 후속 미세 조정을 위한 정렬 데이터로 선택하는 것은 간단합니다. 이러한 아이디어에 기초하여, RAFT[373]는 인간 선호도 데이터에 대해 트레이닝된 보상 모델들을 채택하여 LLM들의 응답들의 순위를 매기고 감독된 미세 조정에 대해 더 높은 보상을 갖는 것들을 수집한다. 또한, 보상 모델은 모델 응답들을 스코어링하고 상이한 품질 그룹들에 할당하기 위해 또한 사용될 수 있다. 쿼크[384]는 보상 점수에 기초하여 LLM의 응답을 상이한 분위수로 정렬한다. 각 분위에는 분위의 보상 수준을 나타내기 위해 특별한 보상 토큰이 부착되어 있다. 최고 리워드 토큰에 대해 조건화된 LLM은 후속적으로 고품질 응답을 생성하도록 프롬프트된다. 초기 답변 및 대응하는 인간 피드백이 주어지면, ILF[385]는 먼저 LLMs를 채택하여 정제된 답변을 생성한 다음, 보상 모델을 활용하여 추가 트레이닝을 위해 피드백과 가장 일치하는 답변을 선택한다. LLM을 정렬하기 위한 귀중한 자원으로 OpenAssistant27의 DeBERTabase/large/xlarge, Fudan28의 Moss-7B, Stanford29의 Flan-T5-xl 등 여러 보상 모델이 출시되었다.

각주 27: [https://huggingface.co/OpenAssistant](https://huggingface.co/OpenAssistant)

각주 28: [https://github.com/OpenLMLab/MOS8-RLHF](https://github.com/OpenLMLab/MOS8-RLHF)

각주 29: [https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl](https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl)

\(\bullet\)_LLM 기반 생성 접근법._ 보상 모델은 모델 반응에서 정렬된 데이터를 선택하는 데 도움이 됩니다. 그러나 훈련 보상 모델 자체는 일반적으로 비싸고 공급이 부족한 상당한 고품질 인간 라벨 데이터를 필요로 한다. 또한 기존 보상 모델은 재사용할 수 있지만 별도로 훈련된 다른 LLM에서 정렬되지 않은 행동을 정확하게 포착할 수 없을 수 있다. 따라서 일부 작업에서는 강력한 LLM을 활용하여 인간 정렬 데이터를 자동으로 생성합니다. 대표적인 작업으로서, 헌법 AI [371]은 인간 감독이 AI 행동을 지배하는 일련의 원리(즉, 자연어 명령)로부터 나온다고 제안한다. 이러한 원칙에 따라 LLMs은 자신의 해로운 반응을 비판하고 최종적으로 정렬된 반응으로 반복적으로 수정한다. 유사하게, Self-Align[347]은 먼저, 다양한 토픽들을 포괄하는 것에 초점을 맞춘 명령어들을 생성하기 위해 자기 명령어들 [143]을 채택한다. 그런 다음 모델은 또한 정렬 데이터로서 유용하고 윤리적이며 신뢰할 수 있는 응답을 생성하기 위해 예상되는 모델 행동의 규칙을 설명하는 여러 인간 작성 원칙으로 프롬프트된다. 원래 SFT 방법이 긍정적인 응답으로부터만 학습할 수 있는 한계를 완화하기 위해, 도 [386]은 개선된 감독 정렬 접근법을 개발하는데, 여기서 부정적인(낮은 품질의 원래 출력) 및 긍정적인(LLM에 의한 정제된 출력) 응답은 모두 대조적인 방식으로 레버리지되어, LLM이 실제로 어떤 세밀한 수정이 좋은 응답으로 이어지는지를 깊이 이해할 수 있게 한다.

\(\bullet\)_LLM 기반 대화형 접근법._ 기존의 대부분의 접근법들은 LLM들을 개별적으로 훈련시키는데, 여기서 LLM들은 외부 피드백 신호들을 통해 그들 자신을 개선하기 위해 실제 환경들에 존재하지 않는다. 비교로서 인간은 사회적 환경에서 타인과의 상호 작용으로부터 사회적 규범과 가치를 학습한다[387]. 이러한 학습 접근법을 모방하기 위해, 스테이블 얼라인먼트[179]는 다수의 LLM 에이전트들로 구성된 시뮬레이션된 상호작용 환경을 구축하며, 여기서 AI 에이전트들은 개선에 대한 피드백을 받으며 서로 계속 상호작용한다. 중앙 에이전트가 명령을 받으면 응답을 생성하여 주변 에이전트와 공유합니다. 이러한 비평가 에이전트는 응답 및 수정 제안에 대한 평점을 포함하는 피드백을 생성한다. 그런 다음 중앙 에이전트는 이러한 제안에 따라 원래 응답을 수정합니다. 이러한 정렬 접근법은 인간과의 실제 환경으로도 확장될 수 있다.

**감독된 선형 조정** 선형 데이터를 얻은 후 직접 정렬에 적합한 미세 조정 전략을 설계하는 것도 핵심입니다. 간단한 접근법은 정렬 데이터를 기반으로 하는 기존의 서열 대 서열 목표를 사용하여 LLM을 최적화하는 것이다. 기존의 최적화 목적 외에도 여러 연구에서 정렬 데이터로부터 학습을 향상시키는 보조 손실을 추가로 탐구한다.

\(\bullet\)_Primary training objective._ 정렬 데이터는 전형적으로 입력 명령 및 출력 응답으로 구성되기 때문에, 일차 트레이닝 손실은 여전히 시퀀스-대-시퀀스 학습을 위한 전통적인 교차 엔트로피 손실이다. 이러한 손실을 기반으로 많은 연구에서 감독 정렬 튜닝을 향상시키기 위한 여러 개선 변형을 제안한다. 예를 들어 CoH [388]은 주석이 달린 좋은 응답과 나쁜 응답에 각각 _"도움이 되는 답변:"_ 및 _"도움이 되지 않는 답변:"_을 미리 붙여 학습 데이터를 구성하고 특수 마스킹이 있는 해당 응답 토큰에 대해서만 손실을 계산합니다. 쿼크[384]는 모델 응답을 다양한 정렬 품질을 갖는 상이한 분위수로 정렬하고, 응답의 보상 레벨을 나타내기 위해 각 모델 응답에 특별한 보상 토큰을 준비한다. 또한, 최대 가능성 목표를 통해 선호도 모델링을 가능하게 하기 위해, DPO[389]는 먼저 정책 모델(_i.e._, 최적화되고 있는 언어 모델)을 사용하여 응답 보상을 재매개변수화한 다음, 원래의 보상 모델링 목표는 정책 모델에 기초하여만 재구성될 수 있다. 이러한 방식으로, DPO는 명시적 보상 모델링 단계를 제거하고, 정책 모델만을 포함하는 새로운 학습 목표를 최적화하는 것은 보상을 최적화하는 것과 동등하다. 나아가, 도[386]은 바람직한 토큰들을 장려하고, 바람직하지 않은 토큰들에 불이익을 주고, 사소한 토큰들을 무시하는 것을 목표로 하는 세밀한 대비 손실을 설계한다.

\(\bullet\)_보조 최적화 목표._ 1차 교차 엔트로피 손실 외에도 여러 연구에서 정렬 데이터에서 학습을 향상시키기 위한 보조 훈련 손실을 제안한다. 먼저, 각 명령어의 응답은 보상 모델에 의해 스코어링될 수 있기 때문에, 랭킹 손실은 이들 응답의 랭킹 순서를 보존하기 위해 모델을 트레이닝하는데 사용될 수 있다. 예를 들어 RRHF [390]은 고품질 및 저품질 인스턴스 모두에 걸쳐 있는 인간 작성 응답뿐만 아니라 모델 자체, ChatGPT 및 GPT-4에서 파생된 응답과 같은 모델 생성 응답을 포함하여 여러 소스에서 응답을 샘플링한다. 보상 모델의 점수와 일치하기 위해 모델이 더 높은 순위를 갖는 응답에 대해 더 높은 조건부 로그 확률을 갖도록 장려함으로써 순위 손실을 더욱 최적화한다. SLiCPF[391]은 잠재 공간에서의 거리를 통해 모델 출력과 인간 선호도 사이의 유사성을 평가하고, 인간 선호도 데이터를 기반으로 후보 시퀀스를 보정하기 위해 특정 보정 및 정규화 손실을 도입한다. 둘째, 응답과 명령어 사이의 관련성을 높이기 위해, 일부 연구에서는 대조적 학습을 도입하여 올바른 명령어-응답 쌍의 확률을 높이고 잘못된 명령어-응답 쌍을 밀어내린다. 구체적으로, 출력 응답에 대해, [392]에서 제안된 접근법은 타겟 명령어를 다른 관련 없는 명령어와 대조한다. 그렇게 함으로써, 그것은 모델이 명령어와 응답들 사이의 올바른 상관 관계를 학습할 수 있게 할 수 있다.

#### 5.2.5 Remarks on SFT and RLHF

섹션 5.1에서 논의된 바와 같이, 명령어 튜닝은 포맷된 데모 데이터(원하는 출력과 쌍을 이루는 명령어)로 사전 트레이닝된 언어 모델을 트레이닝하는 프로세스이다. 초기 탐색에서 명령어 데이터는 주로 NLP 작업에서 수집되었다[67]. 그러나 현재는 입력 텍스트와 출력 텍스트(예: 개방형 대화의 발화)를 쌍으로 하는 보다 다양한 감독 데이터로 확장되었다. 이러한 쌍을 이루는 텍스트를 사용한 훈련은 LLM[66]의 맥락에서 _감독 미세 조정(SFT)_이라고도 한다. 이 부분에서는 단순성과 인기 때문에 주로 토론용 약어 _SFT_를 사용하지만 명령어 튜닝은 사용하지 않는다.

SFT와 RLHF는 LLM에 대한 두 가지 주요 적응 조정 방법이기 때문에 LLM 간의 연결과 차이점을 이해하는 것이 중요하다. 다음으로, 우리는 이 이슈30에 대해 몇 가지 논의를 합니다.

각주 30: 이 부분은 주로 저자들의 의견과 경험에 따라 다소 주관적일 것이다. 이 부분을 개선하기 위해 논평이나 수정을 환영합니다.

**RL 형식과의 전반적인 비교**. 5.2.3절(RL 훈련과 관련된 부분)의 논의에 이어 텍스트 생성 문제를 RL에 기반한 의사결정 과정으로 공식화할 수 있다. 프롬프트를 입력으로 하여, LLM의 태스크는 프롬프트에 적절하게 응답하는 텍스트 완성을 생성하는 것이다. 이 작업은 단계별로 완료될 것입니다. 각각의 단계에서, 에이전트(_i.e.,_LLM)는 현재 상태(현재 생성된 토큰 시퀀스 및 다른 이용가능한 컨텍스트 정보)에 조건화된 정책(_i.e.,_LM의 생성 확률 분포)에 따라 액션(_i.e.,_토큰 생성)을 수행할 것이다. 전체 응답을 바탕으로 큰 보상 점수를 얻을 수 있는 LLM에 의해 고품질의 산출 텍스트가 생산될 것으로 기대된다. 전반적으로, RLHF와 SFT는 LLMs에 대한 위의 의사 결정 프로세스를 최적화하기 위한 두 가지 다른 훈련 접근법으로 간주될 수 있다. 특히, RLHF는 보상 모델을 먼저 학습한 후, 이를 이용하여 RL 트레이닝(예를 들어, PPO)을 통해 LLM을 개선한다. 비교로 SFT는 교사 강제 접근 방식을 채택하여 시연 출력의 가능성을 직접 최적화한다. 이러한 토큰 수준의 훈련 방법은 본질적으로 _행동 복제_(모방 학습의 특별한 알고리즘[393])를 수행합니다. 전문가의 행동(즉, 각 단계에서 대상 토큰)을 감독 레이블로 활용하고 일반적인 RL 알고리즘과 같이 보상 모델을 지정하지 않고 전문가로부터 데모를 모방하도록 직접 학습합니다. 원하는 정책을 학습하기 위해 SFT는 데모 데이터에 기초한 "로컬" 최적화 방식(_즉,_토큰-레벨 손실)을 채택하는 반면, RLHF는 인간의 선호도를 수반함으로써 "글로벌" 최적화 방식(_즉,_텍스트-레벨 손실)을 취한다. 모방 학습 및 강화 학습에 대한 보다 이론적 분석은 관련 RL 문헌[393, 394]을 참조할 수 있다.

**SFT의 장단점**. SFT는 다양한 벤치마크[67, 69, 137, 138]에서 LLM의 성능을 높이는 효과적인 접근법으로 나타났으며, 이는 태스크 일반화 능력을 크게 향상시키고 특정 기능(예를 들어, 챗봇의 아이덴티티 확립)을 유연하게 부여할 수 있다. SFT의 유용성에 대한 더 많은 논의는 섹션 5.1.3에서 찾을 수 있다. SFT는 주로 능력을 _잠금 해제_하지만 LLM에 새로운 능력을 _주입_하지 않는다는 것이 널리 인식되어 왔다. 따라서 SFT를 통해 LLM의 비내인성 능력을 자극하려고 할 때 문제가 될 수 있다. 구체적인 시나리오로, 시연 데이터가 LLM이 알 수 없는 사실에 대한 질문에 답하기 위해 LLM을 훈련하는 것과 같이 LLM이 지식이나 능력 범위를 벗어날 때 환각 행동을 잠재적으로 옹호할 것이다. RLHF[395]에 대한 John Schulman의 강연에서 흥미로운 견해는 능력이 덜한 모델을 훈련하기 위해 우수한 모델을 증류하는 것(예: GPT-4를 프롬프트하여 미세 조정 데이터로 응답을 생성하는 것)이 환각 텍스트를 생성할 가능성을 증가시켜 LLM의 사실적 정확도에 영향을 미칠 수 있다는 것이다. 또한, 행동 복제 방법으로 SFT는 시연 데이터를 구성하는 전문가의 행동(탐구 없이)을 모방하는 것을 목표로 한다. 그러나 데모 데이터의 쓰기 스타일, 품질 및 선호도에 대한 주석자 간의 차이가 종종 존재하며, 이는 SFT의 학습 성능에 영향을 미치는 경향이 있다. 따라서, 고품질 명령어 데이터(그러나 양은 아님)는 SFT 단계 동안 LLM들의 효과적인 트레이닝을 위한 주요 요소이다[99].

**RLHF의 장단점**. RLHF는 심층 RL[79]의 문헌에서 초기에 탐색된 다음 언어 모델의 용량(예: 요약[129])을 개선하기 위해 차용되었으며 이후 InstructGPT[66]를 개발하기 위한 기본 기술로 채택되었다. 최근 증가하는 증거[99, 371]는 유해한 반응을 완화하고 모델 용량을 향상시키는 RLHF의 효과를 입증했다. 특히, LLaMA 2는 RLHF가 유용성과 무해성 점수를 모두 개선할 수 있음을 입증했으며, 이는 데이터 주석을 위한 더 나은 인간-LLM 시너지 효과에 기인한다. 이러한 이유를 크게 두 가지 측면에서 설명하면 다음과 같다. 첫째, 인간 주석자는 주로 RLHF에 대한 선호 주석을 제공하기 때문에 SFT에서와 같이 주석자의 불일치를 크게 완화할 수 있다. 둘째, 선호 주석은 데모 데이터를 작성하는 것보다 훨씬 쉽고, 주석자는 자신이 만드는 것보다 더 우수한 세대의 품질을 판단할 수 있어 인간 주석자가 입증할 수 있는 것 이상의 광범위한 상태 공간을 탐색할 수 있다. 또 다른 핵심 사항은 RLHF가 본질적으로 LLM이 자체 생성된 응답(좋은 응답과 나쁜 응답을 구별함)을 대조함으로써 올바른 정책을 배우도록 장려한다는 것이다. 이는 더 이상 모델이 외부 시연 데이터를 모방하도록 강요하지 않으며, 따라서 위에서 설명한 바와 같이 SFT의 환각 문제를 완화할 수 있다. 실제로 RLHF는 GPT-4[46]에서 환각 행동을 줄이기 위한 중요한 접근법임이 입증되었다. 그러나 RLHF는 고전적인 RL 알고리즘의 단점, 예를 들어 샘플 비효율성 및 훈련 불안정성을 상속한다. LLM에 적응될 때, RLHF는 우수한 성능을 효율적으로 달성하기 위한 초기 모델 체크포인트로서 강한 SFT 모델에 추가로 의존한다. 또한, 인간 주석기들은 복잡한 반복 최적화 프로세스에 관여하며, 여기서 다수의 중요한 세부사항들(예컨대, 프롬프트 선택, 보상 모델 트레이닝 및 PPO 트레이닝의 스케쥴, 하이퍼-파라미터들의 설정)은 전체 모델 성능에 중요한 영향을 미친다.

각주 31: RLHF에서 보상 모델은 LLM이 정렬될 수 있는 지식이나 능력을 인식해야 하는 것도 중요한 것으로 보인다. 예를 들어, LLAMA 2는 보상 모델들을 초기화하기 위해 사전 트레이닝된 채팅 모델 체크포인트들을 채택한다[99].

전반적으로, SFT는 사전 훈련 직후 사전 훈련된 모델 체크포인트의 모델 용량을 증가시키는 데 특히 유용한 반면, RLHF는 SFT 모델의 모델 용량을 더욱 향상시킬 것으로 유망하다. 그러나 RLHF는 구현하기 어려웠고 잘 탐구되지 않았으며(공공 문헌에 따르면), 추가 연구를 위해 더 많은 개선(예: 효율적이고 신뢰할 수 있는 주석[371] 및 단순화된 최적화[389])이 여전히 필요하다.

### _Parameter-Efficient Model Adaptation_

위에서 우리는 특정 목표에 따라 LLM을 적응시키기 위한 명령어 튜닝 및 정렬 튜닝의 접근법에 대해 논의했다. LLM은 많은 양의 모델 파라미터로 구성되기 때문에 전체 파라미터 튜닝을 수행하는 데 많은 비용이 든다. 이 절에서는 LLM에 대한 효율적인 튜닝을 수행하는 방법에 대해 논의할 것이다. 먼저 트랜스포머 언어 모델에 대한 대표적인 파라미터 효율적인 미세 조정 방법을 검토하고, 파라미터 효율적인 미세 조정 LLM에 대한 기존 연구를 요약한다.

#### 5.3.1 파라미터-효율적인 미세 조정 방법

기존 문헌에서 파라미터 효율적인 미세 조정[145, 396, 397]은 가능한 좋은 성능을 유지하면서 훈련 가능한 파라미터의 수를 줄이는 것을 목표로 하는 중요한 주제였다. 다음은 어댑터 튜닝, 접두사 튜닝, 프롬프트 튜닝 및 LoRA를 포함한 트랜스포머 언어 모델에 대한 4가지 매개변수 효율적인 미세 조정 방법을 간략하게 검토한다. 이 네 가지 방법의 예시는 그림 13에 나와 있다.

**어댑터 조정**. 어댑터 튜닝은 트랜스포머 모델에 작은 신경망 모듈(어댑터_라고 함)을 통합한다[398]. 어댑터 모듈을 구현하기 위해, 먼저 원래의 특징 벡터를 더 작은 차원(비선형 변환에 따라)으로 압축한 다음 원래의 차원으로 복원하는 병목 구조가 [398, 399]에 제안되었다. 어댑터 모듈들은 일반적으로 트랜스포머 층의 두 개의 코어 부분들(_즉,_어텐션 층 및 피드-포워드 층) 각각 후에 직렬 삽입을 사용하여 각각의 트랜스포머 층에 통합될 것이다. 대안적으로, 병렬 어댑터들 [400]은 트랜스포머 층들에서도 사용될 수 있으며, 여기서 어텐션 층 및 그에 따라 피드-포워드 층과 병렬로 두 개의 어댑터 모듈들을 배치한다. 미세 조정 동안 어댑터 모듈은 특정 작업 목표에 따라 최적화되는 반면 원래 언어 모델의 매개변수는 이 프로세스에서 동결된다. 이러한 방식으로, 미세 조정 동안 훈련 가능한 파라미터의 수를 효과적으로 감소시킬 수 있다.

**접두사 튜닝**. 접두사 튜닝[396]은 훈련 가능한 연속 벡터의 집합인 접두사 시퀀스를 언어 모델의 각 트랜스포머 레이어에 전치한다. 이러한 접두사 벡터들은 태스크-특정적이며, 이는 가상 토큰 임베딩들로서 간주될 수 있다. 접두사 벡터들을 최적화하기 위해, 접두사를 직접 최적화하는 대신에, 더 작은 행렬을 접두사의 파라미터 행렬에 매핑하는 MLP 함수를 학습함으로써 재매개변수화 트릭[396]이 제안되었다. 이 트릭이 안정적인 훈련에 유용한 것으로 나타났다. 최적화 후에, 매핑 함수는 폐기될 것이고, 도출된 프리픽스 벡터들만이 태스크-특정 성능을 향상시키기 위해 유지된다. 접두사 매개변수만 학습되기 때문에 매개변수 효율적인 모델 최적화로 이어질 수 있다. 접두사 튜닝과 유사하게, p-튜닝 v2 [401]은 특히 자연어 이해를 위해 계층별 프롬프트 벡터를 트랜스포머 아키텍처에 통합하며, 이는 또한 공유 프롬프트를 공동으로 최적화하기 위해 다중 작업 학습을 활용한다. 자연어 이해 과제에 대한 다양한 매개변수 척도의 모델 성능을 개선하는 데 유용한 것으로 나타났다.

**프롬프트 튜닝**. 프리픽스 튜닝과는 달리, 프롬프트 튜닝[397, 402]은 주로 입력 레이어32에서 훈련가능한 프롬프트 벡터들을 통합하는 것에 초점을 맞춘다. 이산 프롬프트 방법들[404, 405]에 기초하여, 그것은 소프트 프롬프트 토큰들의 그룹(자유 형태[402] 또는 프리픽스 형태[397])을 포함함으로써 입력 텍스트를 증강하고, 그 후 특정 다운스트림 태스크들을 해결하기 위해 프롬프트 증강 입력을 취한다. 구현에서, 태스크-특정 프롬프트 임베딩들은 입력 텍스트 임베딩들과 조합되고, 이들은 후속적으로 언어 모델들로 공급된다. P-튜닝[402]은 컨텍스트, 프롬프트 및 타겟 토큰들을 조합하기 위한 자유로운 형태를 제안하였으며, 이는 자연어 이해 및 생성 모두를 위한 아키텍처들에 적용될 수 있다. 그들은 양방향 LSTM에 의한 소프트 프롬프트 토큰들의 표현들을 더 학습한다. _prompt tuning_로 명명된 또 다른 대표적인 접근법 [397]은 접두사 프롬프트를 입력에 직접 전치합니다. 훈련 중에는 태스크별 감독에 따라 신속한 임베딩만 학습한다. 이 방법은 입력 계층에서 소수의 훈련 가능한 파라미터만을 포함하기 때문에, 성능은 기초 언어 모델들의 모델 용량에 고도로 의존한다는 것이 발견되었다[397].

각주 32: 여기서, 프롬프트 튜닝은 [397]에서 사용된 바와 같은 특정 방법 대신에, [397, 402, 403]에 의해 예시된 관련 효율적인 튜닝 방법들의 카테고리를 나타낸다. 실제로, 프리픽스 기반 튜닝 방법들[396, 401]은 프롬프팅 방법들로서 또한 고려될 수 있으며, 이는 [401]에서 _딥 프롬프팅 튜닝_이라고 불린다. 이 조사에서 프롬프트 튜닝은 LLM의 맥락에서 입력 계층에 프롬프트 토큰만 포함하는 방법을 특별히 참조한다. 언어 모델에 계층별 프롬프트를 통합하기 때문에 p-튜닝 v2 [401]을 접두사 튜닝 범주에 할당한다.

**LoRA(Low-Rank Adaptation)**. LoRA[145]는 다운스트림 태스크들에 적응하기 위한 훈련가능한 파라미터들을 감소시키기 위해, 업데이트 매트릭스를 각각의 조밀한 계층에서 근사화하기 위한 저순위 제약을 부과한다. 매개변수 행렬 \(\mathbf{W}\)을 최적화하는 경우를 고려한다. 갱신 과정은 \(\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}\)와 같은 일반적인 형태로 작성될 수 있다. LoRA의 기본 개념은 원래 행렬 \(\mathbf{W}\in\mathbb{R}^{m\times n}\)을 동결하는 동시에 매개변수 갱신 \(\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}\)을 저순위 분해 행렬로 근사하는 것인데, 여기서 \(\mathbf{A}\in\mathbb{R}^{m\times k}\)와 \(\mathbf{B}\in\mathbb{R}^{n\times k}\)는 작업 적응을 위한 훈련 가능한 매개변수이고 \(k\ll\min(m,n)\)는 감소된 순위이다. LoRA의 주요 장점은 메모리 및 저장 사용량(예를 들어,_VRAM)을 크게 절약할 수 있다는 것이다. 또한, 상이한 다운스트림 태스크들에 적응하기 위한 다수의 태스크-특정 저순위 분해 매트릭스들을 유지하면서, 단일 대형 모델 복사본만을 유지할 수 있다. 또한 여러 연구에서 보다 원칙적인 접근법, 예를 들어 중요도 점수 기반 할당[406] 및 검색 없는 최적 순위 선택[407]에서 순위를 설정하는 방법에 대해서도 논의했다.

위의 방법 외에도 트랜스포머 언어 모델의 효율적인 튜닝에 대한 광범위한 연구가 있다. 그러나 효율적인 조정에 대한 보다 포괄적인 논의는 이 주제에 대한 관련 논문에서 찾을 수 있는 이 기사의 범위를 벗어난다[400, 408].

#### 5.3.2 Parameter-Efficient Fine-Tuning on LLMs

LLM이 증가함에 따라, 효율적인 튜닝은 다운스트림 태스크에서 보다 가벼운 적응 접근법을 개발하기 위해 증가하는 연구 관심을 끌었다.

특히, LoRA[145]는 오픈 소스 LLMs(_e.g.,_ LLaMA 및 BLOOM) 포파라미터-효율적인 미세-튜닝에 널리 적용되어 왔다. 이러한 연구 시도 중 LLaMA와 그 변형은 매개변수 효율적인 조정을 위해 많은 관심을 받았다. 예를 들어, Alpaca-LoRA[144]는 Alpaca[142]의 경량 튜닝된 버전으로서 LoRA를 사용하여 트레이닝되었다(52K 인간 지시의 데모를 갖는 미세 튜닝된 7B LLaMA 모델). 최근 LLaMA-Adapter [409]는 학습 가능한 프롬프트 벡터를 각 트랜스포머 계층에 삽입하고 있으며, 이 과정에서 과소적합된 프롬프트 벡터의 영향을 완화하여 학습을 향상시키는 제로-초기화 어텐션이 제안되었다. 또한 이 접근 방식을 다중 모드 설정, 예를 들어 시각적 질문 응답으로 확장합니다.

각주 33: [https://github.com/floen/alpaca-lora](https://github.com/floen/alpaca-lora)

또한, 다양한 튜닝 방법이 언어 모델에 미치는 영향을 조사하기 위한 실증적 연구[399]가 수행되었다. 3개의 오픈소스 LLMs(GPT-J(6B), BLOOM(7.1B) 및 LLaMA(7B)에서 직렬 어댑터 튜닝[398], 병렬 어댑터 튜닝[400, 410] 및 LoRA[145]를 포함한 4가지 효율적인 튜닝 방법을 비교 평가한다. 6개의 수학 추론 데이터 세트에 대한 실험 결과를 바탕으로, 이러한 효율적인 조정 방법은 어려운 과제에서는 기준 기준 GPT-3.5를 과소 수행하지만 간단한 과제에서는 유사한 성능을 달성한다는 것을 보여준다. 전반적으로 LoRA는 훨씬 적은 훈련 가능한 매개변수를 사용하여 이러한 비교 방법 중에서 비교적 잘 수행한다.

중요한 리소스로서 라이브러리 _PEFT_[411](parameter-efficient fine-tuning에 대한 스탠딩)은 GitHub34에서 릴리스되었습니다. LoRA [145]/AdaLoRA [406], prefixtuning [396, 401], P-Tuning [402], prompt-tuning [397] 등 널리 사용되는 몇 가지 효율적인 튜닝 방법을 포함했습니다. 또한, GPT-2, LLaMA 등 다수의 언어 모델을 지원하며, 대표적인 비전 트랜스포머 모델(_e.g._, VIT, Swin Transformer)도 다룹니다.

각주 34: [https://github.com/huggingface/pft](https://github.com/huggingface/pft)

5.3.1절에서 논의한 바와 같이, 기존 문헌에서 제안된 많은 수의 효율적인 튜닝 방법이 있었다. 그러나, 이러한 접근법들의 대부분은 LLMs 대신에 작은 크기의 사전 훈련된 언어 모델들에 대해 테스트된다. 지금까지, 상이한 설정 또는 태스크에서 상이한 효율적인 튜닝 방법이 대형 언어 모델에 미치는 영향에 대한 철저한 조사는 여전히 부족하다.

### _Memory-Efficient Model Adaptation_

많은 수의 모델 파라미터로 인해 LLM은 추론을 위해 상당한 메모리 공간을 차지하므로 실제 응용 프로그램에 배포하는 데 매우 비용이 많이 든다. 이 섹션에서는 인기 있는 모델 압축 접근법(_i.e._, 모델 양자화)을 통해 LLM의 메모리 풋프린트를 줄이는 방법에 대해 논의하여, 큰 크기의 LLM이 리소스 제한 설정에서 사용될 수 있으며, 이는 또한 추론 지연을 감소시킬 가능성이 있다.

#### 5.4.1 양자화에 대한 배경

이 부분에서는 신경망에 대한 양자화 기술에 대한 일반적인 소개를 제시한다.

뉴럴 네트워크 압축에서, 양자화는 종종 부동 소수점 수들로부터 정수들[412], 특히 8 비트 정수 양자화(_i.e._, _INT8 양자화_)로의 맵핑 프로세스를 지칭한다. 신경망 모델의 경우 일반적으로 양자화될 두 가지 종류의 데이터, 즉 원래 부동 소수점 숫자로 표현되는 _가중치_(모델 매개변수) 및 _활성화_(은닉 활성화)가 있다. 모델 양자화의 본질적인 개념을 설명하기 위해, 우리는 간단하면서도 대중적인 양자화 함수 \(x_{q}=R(x/S)-Z\를 도입한다: \(x_{q}\)는 부동수 \(x\)를 양자화된 값 \(x_{q}\)으로 변환한다. 이 함수에서 \(S\)와 \(Z\)는 각각 클리핑 범위를 결정하는 두 매개변수 \(\alpha\)와 \(\beta\)를 포함하는 스케일링 팩터를 나타내고, \(R(\cdot)\)는 스케일링된 플로팅 값을 근사 정수로 매핑하는 라운딩 연산을 나타낸다.

역과정으로서 _역양자화_는 양자화된 값으로부터 원래의 값을 복원한다: \(\tilde{x}=S\cdot(x_{q}+Z)\). 양자화 오차는 원래의 값 \(x\)과 복원된 값 \(\tilde{x}\)의 수치적 차이로 계산된다. 범위 매개 변수 \(\alpha\) 및 \(\beta\)는 실제 데이터 분포에 따라 종종 _정적_ (오프라인) 또는 _동적_ 방식 (런타임)으로 _보정_ 되어야 하는 양자화 성능에 큰 영향을 줍니다.

자세한 내용은 신경망에서 양자화 방법에 대한 우수한 조사[412]를 참조한다.

#### 5.4.2 LLMs 양자화 방법

일반적으로 두 가지 주요 모델 양자화 접근법, 즉 _양자화 인식 훈련_(_QAT_)(추가 전체 모델 재훈련을 필요로 함)과 _훈련 후 양자화_(_PTQ_)(모델 재훈련을 필요로 함)가 있다. 작은 크기의 언어 모델과 비교할 때 LLM에 대한 양자화 방법을 설계하거나 선택할 때 두 가지 주요 차이점을 고려해야 한다. 첫째, LLM은 매우 많은 수의 파라미터로 구성되며, 따라서 PTQ 방법은 QAT 방법보다 훨씬 낮은 계산 비용으로 인해 더 선호된다. 둘째, LLMs은 매우 다른 활성화 패턴(_i.e._, 큰 이상치 특징)을 나타내며, LLMs, 특히 숨겨진 활성화를 양자화하는 것이 더 어려워진다. 다음으로 LLM에 대한 몇 가지 대표적인 PTQ 방법35를 간략하게 검토한다.

각주 35: 우리는 주로 LLM들의 맥락에서 양자화 방법들을 논의하는 것에 초점을 맞추기 때문에, 작은 크기의 언어 모델들(_e.g._, BERT)에 대한 양자화 작업 라인은 이 조사에 포함되지 않았다.

**PTQ(교육 후 양자화)** 먼저 LLM에 대한 PTQ 방법을 소개한다.

* _혼합 정밀도 분해_. [413]에서 관찰된 바와 같이, 모델 크기가 6.7B 매개 변수 이상에 도달할 때 숨겨진 활성화( _이상값의 출현_이라고 함)에서 극단적인 큰 값이 발생한다. 흥미롭게도 이러한 특이치는 주로 트랜스포머 계층에서 특정 피쳐 치수에 분포합니다. 이 발견을 바탕으로, _LLM.int8(0)_이라고 불리는 벡터-와이즈 양자화 접근법이 [413]에서 제안되었는데, 이는 특징 차원을 이상치로 분리하고 나머지 차원을 행렬 곱셈에서 분리한다. 그런 다음 두 부분에 대한 계산을 각각 _16비트 부동 숫자_ 및 _8비트 정수_로 수행하여 이러한 이상치를 높은 정밀도로 복구한다.

* _세립 양자화_. 트랜스포머 모델의 경우 가중치 및 활성화는 일반적으로 텐서의 형태로 표현된다. 간단한 접근법은 전체 텐서(_즉,_텐서당 양자화)에 대해 거친-결정화된 양자화 파라미터들을 사용하는 것이다[414]. 그러나, 이는 일반적으로 부정확한 재구성 결과를 초래한다. 따라서 양자화 오차를 줄이기 위해 세밀한 방법을 제안한다. 제로퀀트[415]는 활성화를 압축하기 위해 동적 캘리브레이션을 갖는 토큰-와이즈 양자화 접근법을 채택한다. 가중치(양자화되기 쉬움)에 대해서는 그룹 단위 양자화를 사용한다. 실제로, 128[415, 416]의 그룹 크기가 모델 양자화를 위해 일반적으로 사용된다.
* _양자화 난이도 균형 조정_ 입니다. 가중치가 활성화보다 양자화하기 쉽다는 점을 고려하여 SmoothQuant[414]는 활성화에서 가중치로 난이도를 마이그레이션할 것을 제안한다. 특히 \(\mathbf{Y}=(\mathbf{X}\text{diag}(\mathbf{s})^{-1})\cdot(\text{diag}(\mathbf{ s})\mathbf{W})\). 수학적으로 동등한 변환을 도입함으로써, 이 공식은 스케일링 인자 \(\mathbf{s}\)를 통해 양자화 난이도를 제어한다. \(\mathbf{s}\)을 설정하기 위해 마이그레이션 강도 매개변수 \(\alpha\)를 통합하여 어려움의 균형을 맞추며, 여기서 각 엔트리 \(s_{j}=\max(\mathbf{x}_{j})^{\alpha}/\max(\mathbf{w}_{j})^{(1-\alpha)}\)는 마이그레이션 강도에 의해 결정된다.
* _계층별 양자화_. 이 방법은 계층별 재구성 손실을 최소화하는 최적의 양자화 가중치를 찾는다 : \(\arg\min_{\widetilde{\mathbf{W}}}\parallel\mathbf{W}\mathbf{X}-\widetilde{ \mathbf{W}}\mathbf{X}\parallel_{2}^{2}\). 이 목적을 효율적으로 최적화하기 위해, GPTQ[417]은 모든 행에 대한 가중치의 양자화 순서를 고정함으로써 원래의 최적 브레인 양자화(OBO)[418] 방법을 개선한다. 또한, 특별히 설계된 방법(_즉,_ lazy batch-updates 및 Cholesky reformulation)을 사용하여 GPTQ는 3 또는 4 비트 정밀도로 매우 큰 모델(_예를 들어,_175B OPT)을 양자화할 수 있다. 보다 최근에, AWQ[416]은 가중치에 대한 활성화-인식 스케일링을 통합함으로써 최적화 형태를 더욱 단순화하는데, 이는 SmoothQuant[414]의 아이디어와 유사하다 : 이상치 활성화에 대응하는 가중치는 정밀하게 양자화되는 것이 더 중요하다. 이것은 재구성 손실을 직접 최적화하는 것이 아니라, 대신에 교정 데이터에 대한 최소 손실을 달성하기 위해 간단한 하이퍼-파라미터 탐색을 수행한다.

상기 방법들에서의 이러한 전략들은 양자화 성능을 향상시키기 위해 공동으로 사용될 수 있다. 고효율 구현을 달성하기 위해, 양자화 방법들은 또한 하드웨어- 또는 시스템-레벨 지원(예를 들어,_효율적인 GPU 커널들 또는 하드웨어-친화적인 그룹 파티션)에 의존한다.

**기타 양자화 방법** 위에서 주로 PTQ 방법에 초점을 맞추고 다음으로 LLM 양자화를 위한 효율적인 미세 조정 방법 또는 QAT 방법을 탐구하는 두 가지 최근 연구를 소개한다.

* _효율적인 미세 조정 향상된 양자화_. 훈련 후 양자화의 경우, 직접 저비트 양자화(_예를 들어,_INT4 양자화)는 종종 큰 성능 저하를 초래한다. 이 문제를 극복하기 위해 QLoRA[419]는 효율적인 고정밀 모델 미세 조정을 달성하기 위해 양자화된 모델에 추가적인 작은 조정 가능한 어댑터(16비트 정밀도)를 통합한다. LoRA의 장점(섹션 5.3.1 참조)과 양자화 방법을 결합합니다. 실험 결과, 4-비트 양자화 모델은 QLoRA에 의해 전체 16-비트 미세 조정 성능을 얻을 수 있음을 보였다.
* LLMs에 대한 QAT(양자화 인식 훈련)입니다. 최근 연구[420]은 데이터 프리 증류 방법을 적용하여 가중치, 활성화 및 키 값 캐시를 압축함으로써 QAT 방법의 효과를 탐구한다. LLaMA를 기반으로 광범위한 실험을 수행하여 가중치 및 키-값 캐시 모두에서 4비트 양자화를 통해 유망한 결과를 보여주었지만, 여전히 더 많은 탐색이 필요한 4비트 활성화 양자화에서는 그렇지 않았다.

#### 5.4.3 경험적 분석 및 결과

양자화는 현재 배포에서 LLM의 메모리 풋프린트 및 대기 시간을 줄이기 위한 일반적인 기술이 되었다. 특히, 높은 정확도를 유지하면서, LLMs(_예를 들어,_ 가중치들 또는 활성화들)의 상이한 부분들을 양자화하기 위해 어떤 수준의 정밀도(_예를 들어,_INT8 또는 INT4)가 적용될 수 있는지를 이해하는 것이 중요하다. 이 부분에서는 먼저 기존 문헌에서 LLM의 양자화에 대한 주요 결과를 요약한 다음 양자화 실험을 통해 몇 가지 경험적 분석을 제시한다.

**기존 작업의 중요한 결과**. 최근, 다중 인자(예를 들어, 모델 크기 및 민감도)가 훈련 후 양자화 방법에 미치는 영향에 대해 매우 포괄적인 평가[421]가 수행되었다. 또 다른 연구[422]는 추론 성능에서 \(k\)-비트 양자화의 스케일링 법칙을 조사한다. 전체 성능 외에도 연구[423]은 특히 다양한 수준의 비트 정밀도에 걸쳐 달성할 수 있는 성능 수준뿐만 아니라 창발 능력에 대한 정량화의 잠재적 영향에 초점을 맞춘다. 또한 이전 작업(_예:_ LLM.int8(424), GPTQ[417], QLoRA[419], GLM[93])도 다양한 설정에서 양자화 방법의 성능을 광범위하게 조사했다. 다음으로, 양자화 방법의 기술적 세부 사항을 파고들지 않을 수 있는 사람들에게 유용할 이러한 연구의 몇 가지 중요한 결과를 요약한다.

* _INT8 가중치 양자화는 종종 LLM에서 매우 양호한 결과를 산출할 수 있는 반면, 더 낮은 정밀도 가중치 양자화의 성능은 특정 방법_[414, 416, 417, 421]에 의존한다. 대부분의 경우, INT8 가중치 양자화는 성능 저하 없이 메모리 풋프린트를 감소시키기 위해 효과적으로 적용될 수 있다. INT4(또는 INT3) 가중치 양자화의 경우, 기존 방법은 성능 저하를 줄이기 위해 특정 전략, 예를 들어 계층별 방법[415, 417], 활성화 인식 스케일링[416] 및 저순위 어댑터 튜닝[419]에 의존한다. 흥미롭게도 LLMs는 작은 크기의 언어 모델[421]보다 낮은 비트 가중치 양자화에 덜 민감한 것으로 보인다. 실제로, 동일한 메모리 비용으로, 더 높은 양자화 정밀도를 갖는 더 작은 언어 모델보다는 더 낮은 양자화 정밀도를 갖는 더 큰 언어 모델을 사용하는 것이 제안된다. 예를 들어, 4-비트 60GB LLM은 8-비트 30GB LLM[422]보다 더 나은 성능을 갖는 것으로 입증된다. 또한, 창발적 능력에 초점을 맞추어, 연구 [423]은 상황 내 학습, 단계별 추론 및 후속 명령이 모두 4비트 가중치 양자화에 거의 영향을 받지 않는 것으로 보인다는 것을 발견했다. 이 결과는 INT4 양자화가 전체 비트 수와 창발 능력의 성능 면에서 유리한 트레이드오프를 나타낸다는 것을 시사한다.
* _활동은 가중치_[413, 414, 421]보다 양자화하기 더 어렵습니다. 6.7B 이상의 크기를 갖는 트랜스포머 언어 모델들에 대해 큰 이상치들이 발생할 것이라는 것이 밝혀졌다[413]. 이 문제는 LLM을 양자화하는 데 가장 근본적인 어려움 중 하나였다. 이 문제를 극복하기 위해 다양한 방법, 예를 들어 혼합 정밀 분해[413], 세립 양자화[413, 425] 및 난이도 마이그레이션[414]을 적용하여 이상치 값의 영향을 완화할 수 있다. LLM의 활성화에는 주로 큰 이상치가 존재하기 때문에, 작은 언어 모델은 활성화 양자화에 더 강하다[421, 423]. 실제로 고품질 INT8 활성화 양자화는 여러 방법이 만족스러운 결과를 얻을 수 있지만 여전히 어려운 작업이다. 또한, 더 낮은 정밀도 활성화 양자화는 QAT 방법들에 대해서도 여전히 성공적으로 탐색되지 않았다[420].

\(\bullet\)_효율적인 미세 조정 강화 양자화는 양자화된 LLMs_[419, 145]의 성능을 향상시키는 좋은 옵션이다. 양자화에서 효율적인 펀튜닝 방법의 이점은 두 가지일 수 있다. 첫째, 고정밀 어댑터를 업데이트하여 피팅 용량을 증가시킴으로써 저비트 양자화[421, 423]로 인한 성능 저하를 직접 보상할 수 있다. 둘째, 작은 어댑터만 튜닝하여 LLM의 작업별 또는 목표별 미세 조정을 경량 방식으로 지원할 수 있습니다[419], _예:_ 명령어 튜닝 또는 채팅 지향 튜닝입니다. 전반적으로 효율성과 훈련 비용 사이의 좋은 트레이드오프를 만들어 양자화된 LLM의 성능을 향상시키는 유망한 접근법을 제공한다.

**양자화 실험에 대한 경험적 분석**. 독자들이 LLM에 대한 양자화의 영향을 이해하는 데 더 도움이 되도록 여기에서 양자화된 모델의 추론 성능을 조사하기 위한 실험 그룹도 수행한다. 특히 FLAN-v2 [69], Alpaca-52K [137] 및 ShareGPT [148]을 포함한 인기 있는 SFT 데이터 세트를 사용하여 미세 조정된 LLMaM 모델(_즉,_7B 및 13B)에 중점을 둔다. 평가를 위해 표 IX에서 동일한 작업을 활용하고 연구 [423]에서 4비트, 8비트 및 16비트의 세 가지 정밀도 수준에서 양자화된 언어 모델의 성능을 조사하는 양자화 설정을 따른다. 결과는 표 X에 요약되어 있다. 표 X로부터 관찰될 수 있는 바와 같이, 8-비트 및 4-비트 가중치 양자화로 얻어진 결과는 메모리 소비를 상당히 감소시키면서 16-비트 모델의 성능에 근접한다. 실제로 메모리 사용량을 줄이는 것이 배치에 중요한 고려 사항인 경우 LLM에 대한 4비트 가중치 양자화의 성능을 먼저 조사하는 것이 좋다.

#### 5.4.4 오픈 소스 라이브러리 및 양자화된 LLMs

이 부분에서는 사용 가능한 오픈 소스 양자화 라이브러리와 양자화된 LLM에 대해 간략하게 소개한다.

**양자화 라이브러리**. 다음으로 LLMs를 위한 세 가지 주요 양자화 라이브러리를 소개한다.

\(\bullet\)_Bitsandbytes36_은 LLM.int80 [413]과 8비트 최적화기 [426]의 논문에서 소개된 방법을 기반으로 개발되었다. 효율적인 추론을 위한 8-비트 및 4-비트(NF4,FP4) 행렬 곱셈에 대한 지원과 효율적인 훈련을 위한 8-비트 최적화기를 포함하여 LLM에 대한 활성화 및 가중치의 양자화에 중점을 둔다.

각주 36: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

LLMaM 모델의 양자화를 위해 특별히 개발된 모델은 \(\bullet\)_GPTQ-for-LLMaM37_이다. GPTQ 알고리즘을 기반으로 다양한 크기의 LLMaM 모델의 4비트 양자화를 가능하게 한다[417]. 또한 프로젝트 웹 사이트의 메모리 및 성능(PPL) 모두에서 비트와 비교를 제공합니다.

각주 37: [https://github.com/qwpoppop200/GPTQ-for-LLMaMa](https://github.com/qwpoppop200/GPTQ-for-LLMaMa)

\(\bullet\)_AutoGPTQ38_은 GPTQ 알고리즘 [417]을 기반으로 개발된 양자화 패키지로 LLM에 대한 INT4 양자화를 지원한다. 라이브러리에 다수의 양자화된 모델을 포함하고 있으며, HuggingFace PFFT 라이브러리와 통합하여 LoRA를 지원한다.

각주 38: [https://github.com/PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)

\(\bullet\)_llama.cpp39_는 맥북 장치에서 양자화된 LLMaM 모델을 실행하는 것을 가능하게 한다. 효율적인 C/C++ 구현을 위해 개발된 INT4, INT5, INT8 양자화를 지원한다. 또한 알파카 및 비쿠나와 같은 여러 LLMaM 기반 모델을 지원합니다.

각주 39: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

**양자화된 LLM**. 원본 모델들과 비교하여, 양자화된 언어 모델들은 더 작은 메모리 풋프린트를 취하고, 더 빠른 추론 속도를 가질 가능성이 있다[427, 93, 413]. 최근에는 BLOOM, GPT-J, ChatGLM 등 공개된 여러 언어 모델의 양자화된 모델 사본이 HuggingFace에 다수 공개되고 있다. 특히, GPTQ[417]은 생성 언어 모델을 양자화하는 데 널리 사용되어 LLMaM 및 OPT에 대한 다양한 양자화된 변형으로 이어졌다. 또한, 비쿠나 및 위저드LM과 같은 명령어 조정 모델을 양자화하는 데에도 적용되었다. 양자화된 LLM의 수가 많기 때문에 이러한 모델의 해당 링크를 직접 통합하지 않는다. 독자들은 HuggingFace를 검색하면 쉽게 찾을 수 있습니다.

## 6 Utilization

사전 훈련 또는 적응 조정 후 LLM을 사용하는 주요 접근법은 다양한 작업을 해결하기 위한 적합한 _prompting_ 전략을 설계하는 것이다. 기존 문헌에서는 수동 생성 및 자동 최적화를 통해 태스크별 프롬프트를 효과적으로 학습할 수 있다. 대표적인 프롬프트 방법은 자연어 텍스트의 형태로 태스크 설명 및/또는 데모를 공식화하는 _in-context learning_[50, 55]이다. 또한, _생각 사슬 프롬프트_[33]은 프롬프트에 일련의 중간 추론 단계를 포함시킴으로써 컨텍스트 내 학습을 향상시키는 데 사용될 수 있다. 또한, 복잡한 작업을 해결하기 위해 _planning_[439]를 제안하며, 이는 먼저 작은 하위 작업으로 분해한 다음 이러한 하위 작업을 하나씩 해결하기 위한 행동 계획을 생성한다. 표 XI에서 이러한 촉진 접근법에 대한 대표적인 작업을 요약한다. 다음으로, 우리는 네 가지 기술의 세부 사항에 대해 자세히 설명할 것이다.

### _Prompting_

이전 작업 [36]에서 논의한 바와 같이 프롬프트는 다양한 과제 해결을 위해 LLM을 활용하는 주요 접근법이다. 프롬프트의 품질은 특정 작업에서 LLM의 성능에 크게 영향을 미치기 때문에 수동 생성 또는 자동 최적화를 통해 적합한 작업 프롬프트를 생성하기 위해 제안된 일련의 연구가 이 섹션에 소개될 것이다.

#### 6.1.1 프롬프트 생성

적합한 프롬프트를 수동으로 만드는 프로세스를 _prompt engineering_[452, 453]이라고도 합니다. 잘 설계된 프롬프트는 특정 작업을 수행하는 LLM의 능력을 이끌어내는 데 매우 도움이 된다. 이 부분에서는 프롬프트의 핵심 구성 요소를 먼저 소개하고 프롬프트 설계를 위한 몇 가지 원칙에 대해 논의할 것이다. 그런 다음 여러 대표 태스크에 대한 결과를 보여주기 위해 서로 다른 프롬프트로 ChatGPT를 평가한다. 우리는 좋은 프롬프트를 설계하기 위한 제안과 지침을 제시하는 여러 기존 논문[453, 454]과 웹사이트[455, 456, 457]가 있었다는 것을 알고 있다. 비교로, 우리는 주로 신속한 창작에 유용한 핵심 요소(생성과 원리)에 대해 논의하고, 초보자에게 참고할 수 있는 인기 과제에 대한 실험 결과와 분석을 제공하는 것을 목표로 한다.

**주요 구성 요소** 일반적으로 태스크 설명, 입력 데이터, 상황 정보 및 프롬프트 스타일을 포함하여 작업을 완료하는 LLM의 기능을 이끌어내기 위한 프롬프트의 기능을 나타내는 네 가지 주요 구성 요소가 있습니다. 논의를 직관적으로 이해하기 위해 표 XIII XIII의 질문 답변, 메타 검토 생성 및 텍스트 대 SQL에 대한 세 가지 프롬프트 예도 제시한다.

\(\bullet\)_Task description._ 작업 설명은 일반적으로 LLM이 따를 것으로 예상되는 특정 명령이다. 일반적으로 자연어로 과제 목표를 명확하게 기술해야 한다. 특수 입력 또는 출력 형식을 가진 태스크의 경우 자세한 설명이 필요한 경우가 많으며, 태스크 완료 시 LLM을 더 잘 안내하기 위해 특수 설정을 강조하기 위해 키워드를 추가로 활용할 수 있다.

\(\bullet\)_입력 데이터._ 일반적인 경우, 입력 데이터(_e.g._, LLMs에 의해 응답될 인스턴스)를 자연어로 기술하는 것은 간단하다. 지식 그래프 및 표와 같은 특수 입력 데이터에 대해 LLMs에 대해 읽을 수 있도록 적절하고 편리한 방법을 적용할 필요가 있다. 구조화된 데이터의 경우, 선형화는 단순성으로 인해 원본 레코드(_e.g._, 지식 트리플)를 시퀀스[458]로 변환하는데 일반적으로 사용된다. 또한, 프로그래밍 언어(_e.g._, 실행가능 코드)는 또한 구조화된 데이터를 공식화하는 데 활용되었으며, 이는 또한 정밀한 결과를 생성하기 위해 외부 도구(_e.g._, 프로그램 실행기)를 사용하는 것을 지원할 수 있다[459, 460].

\(\bullet\)_Contextual information._ 작업 설명 및 입력 데이터 외에 상황 정보 또는 배경 정보

\begin{table}
\begin{tabular}{l|l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multirow{2}{*}{**
\begin{tabular}{c} **STT Dataset** \\ \end{tabular} } & \multicolumn{4}{c}{**16-bit**} & \multicolumn{4}{c}{**8-bit**} & \multicolumn{4}{c}{**4-bit**} \\ \cline{3-13}  & & AlpacaFarm & MMLU & BBH & Mem\({}_{(\text{SQL})}\) & AlpacaFarm & MMLU & BBH & Mem\({}_{(\text{SQL})}\) & AlpacaFarm & MMLU & BBH & Mem\({}_{(\text{SQL})}\) \\ \hline LLMaA (7B) & FLAN-\(\lambda\)2 & 6.65 & 47.34 & 35.05 & 12.58 & 6.15 & 47.02 & 35.17 & 6.65 & 7.83 & 46.23 & 34.77 & 3.94 \\  & Alpaca-52K & 35.25 & 40.87 & 33.66 & 12.58 & 33.60 & 39.38 & 34.38 & 6.65 & 29.57 & 39.24 & 32.80 & 3.94 \\  & ShareFPT & 72.05 & 41.30 & 32.90 & 12.58 & 72.86 & 39.34 & 32.71 & 6.65 & 70.31 & 40.08 & 32.11 & 3.94 \\ \hline LLMaA (138) & FLAN-\(\lambda\)2 & 81.4 & 51.67 & 41.46 & 24.40 & 7.64 & 51.02 & 41.15 & 12.53 & 7.82 & 50.48 & 40.68 & 7.34 \\  & Alpaca-52K & 33.60 & 47.63 & 36.10 & 24.40 & 31.43 & 47.04 & 35.98 & 12.53 & 30.87 & 46.20 & 36.16 & 7.34 \\  & ShareFPT & 75.59 & 47.58 & 38.00 & 24.40 & 73.79 & 47.71 & 38.31 & 12.53 & 71.99 & 45.77 & 36.97 & 7.34 \\ \hline \hline \end{tabular}
\end{table} TABLE XIII: Evaluation results for quantized LLMaA models (7B and 13B). We employ existing model checkpoints provided by [353] for quantization experiments, which have been fine-tuned on FLAN-\(\nu\)2, Alpaca-52K, and ShareGPT, respectively. Specifically, we report the performance with AlpacaFarm, MMLU, and BBH, as well as the memory usage of the loaded model (Mem.). For quantization, we employ _bitesandbytes_ to quantize the 16-bit models to 8/4 bits by specifying the commands load_in_8bit and load_in_4bit when loading the weights. It is worth noting that we select _text-davinci-003_ as the baseline model for the AlpacaFarm dataset.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Approach** & **Representative Work** & **Key Point** \\ \hline \multirow{4}{*}{\begin{tabular}{l} In-context \\ Learning (ICL) \\ Structured Prompting [296] \\ Global \& LocalE [432] \\ \end{tabular} } & \begin{tabular}{l} Demonstration selection (similar, k-NN) \\ Demonstration selection (dense retrieval; contrastive learning) \\ Demonstration selection (LLM as the demonstration generator) \\ Demonstration format (automatic generation \& selection) \\ Demonstration format (grouped context encoding; rescaled attention) \\ Demonstration order (entropy-based metric; probing set generation with LLM) \\ \end{tabular} \\ \hline \multirow{4}{*}{\begin{tabular}{l} Chain-of-though \\ Prompting (CoT) \\ \end{tabular} } & ComplexCoT [433] & Demonstration (complexity-based selection) \\ Auto-CoT [434] & Demonstration (automatic generation) \\ Selection-Inference [435] & Generation (alternate between selection selection and inference) \\ Self-consistency [436] & Generation (diverse paths; self-ensemble) \\ DIVERSE [437] & Generation (diverse paths): Verification (step-wise voting) \\ Rationale-augmented ensembles [438] & Generation (rationale sampling) \\ \hline \multirow{4}{*}{
\begin{tabular}{l} Planning \\ \end{tabular} } & Least-to-most prompting [49] & Plan generation (text-based; problem decomposition) \\ DECOMY [440] & Plan generation (text-based; problem decomposition) \\ PS [441] & Plan generation (text-based) \\ Faithful CoT [442] & Plan generation (code-based) \\ PAL [443] & Plan generation (code-based; Python) \\ HuggingGPT [444] & Plan generation (code-based; models from HuggingFace) \\ AdaPLanner [445] & Plan refinement (skill memory) \\ TIP [446] & Feedback acquisition (visual perception) \\ RAP [447] & Feedback acquisition (LIM as the world model); Plan refinement (Monte Carlo Tree Search) \\ ChatCCT [448] & Feedback acquisition (100); Plan refinement (conversation between LLM and tools) \\ ReAct [449] & Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting) \\ Reflexion [450] & Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory) \\ Tree of Thoughts [451] & Feedback acquisition (vote comparison); Plan refinement (tree-based search) \\ \end{tabular} } \\ \hline \hline \end{tabular}
\end{table} TABLE XIII: Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note that the key points only highlight the most important technical contribution.

또한 특정 작업에 필수적입니다. 예를 들어, 검색된 문서는 증거로서 오픈 도메인 질의 응답에 매우 유용하다. 검색된 문서의 품질 및 질문과의 관련성 모두는 생성된 답변들에 영향을 미친다[461]. 따라서, 적절한 프롬프트 패턴 또는 표현 형식으로 이러한 정보를 포함할 필요가 있다. 또한, 인컨텍스트 태스크 예시는 복잡한 태스크를 수행하기 위해 LLMs을 유도하는 데 도움이 되며, 이는 태스크 목표, 특수 출력 형식 및 입력과 출력 간의 매핑 관계를 더 잘 묘사할 수 있다.
* _프롬프트 스타일_ 다양한 LLM의 경우 특정 작업을 해결하는 능력을 이끌어내기 위한 적절한 프롬프트 스타일을 설계하는 것이 중요하다. 전체적으로 프롬프트를 잘 이해하고 답할 수 있는 명확한 질문이나 상세한 지시로 표현해야 한다. 경우에 따라 접두사 또는 접미사를 추가하여 LLM을 더 잘 안내하는 것도 유용합니다. 예를 들어, 접두사 _"차근차근 생각해보자"_를 사용하면 LLMs가 단계별 추론을 수행하는 데 도움이 될 수 있고, 접두사 _"당신은 이 작업에 대한 전문가(또는 이 도메인에서)"_를 사용하면 일부 특정 작업에서 LLMs의 성능을 높일 수 있다. 또한, 채팅-기반 LLM들(_e.g._, ChatGPT)에 대해, 길거나 복잡한 태스크 프롬프트를 직접 공급하는 대신에, 서브-태스크들에 대한 다수의 프롬프트들로 분해한 다음, 멀티-턴 대화를 통해 LLM들로 공급하도록 제안된다[448].

**설계 원칙** 프롬프트의 주요 구성 요소를 기반으로 다양한 작업을 해결하는 데 보다 효과적인 프롬프트를 만드는 데 도움이 될 수 있는 몇 가지 중요한 설계 원칙을 요약합니다.

\(\bullet\)_작업 목표를 명확하게 표현합니다._ 작업 설명이 모호하거나 명확하지 않아야 하며, 이는 부정확하거나 부적절한 응답으로 이어질 수 있다. 이것은 이러한 모델을 사용할 때 명확하고 명확한 지침의 필요성을 강조한다[66]. 명확하고 자세한 설명에는 작업 목표, 입력/출력 데이터(_e.g._, _"긴 문서가 주어지면 간결한 요약"_) 및 응답 제약 조건(_e.g._, _"요약 길이가 50을 초과할 수 없음)을 포함하여 작업을 설명하는 다양한 요소가 포함되어야 합니다. 잘 명료화된 태스크 디스크립션을 제공함으로써, LLMs는 타겟 태스크를 보다 효과적으로 이해하고 원하는 출력을 생성할 수 있다.

\(\bullet\)_ 쉽고 상세한 하위 작업으로 분해_ 복잡한 작업을 해결하기 위해서는 어려운 작업을 LLM이 목표를 단계적으로 달성할 수 있도록 돕기 위한 몇 가지 더 쉽고 상세한 하위 작업으로 분해하는 것이 중요하며, 이는 섹션 6.4의 계획 기술과 밀접한 관련이 있다. 예를 들어 제안 [454]에 따라 다음과 같은 작업을 수행하여 여러 번호가 매겨진 항목(_예._, _"Braid a coherent narrative by the task: 1...., 2...., 3......."_)의 형태로 하위 작업을 명시적으로 나열할 수 있다. 대상 태스크를 하위 태스크로 분해함으로써 LLMs는 더 쉬운 하위 태스크를 해결하는 데 초점을 맞추고 최종적으로 복잡한 태스크에 대해 더 정확한 결과를 얻을 수 있다.

\(\bullet\)_몇 번의 샷 시연 제공_ 섹션 6.2에서 논의된 바와 같이, LLM은 복잡한 태스크를 해결하기 위한 상황 내 학습의 이점을 얻을 수 있으며, 여기서 프롬프트는 원하는 입력-출력 쌍, _즉, 소수의 샷 데모들의 적은 수의 태스크 예들을 포함한다. 소수의 샷 데모는 LLM이 파라미터 튜닝 없이 입력과 출력 사이의 시맨틱 매핑을 학습하는 데 도움이 될 수 있다. 실제로는 목표 과제에 대한 몇 가지 고품질 데모를 생성해야 하며, 이는 최종 과제 수행에 큰 도움이 될 것이다.

\(\bullet\)_모델 친화적인 형식을 활용합니다._ LLM은 특별히 구성된 데이터 세트에 대해 미리 훈련되기 때문에 LLM이 명령을 더 잘 이해할 수 있도록 하는 몇 가지 프롬프트 형식이 있다. 예를 들어 OpenAI 설명서에서 알 수 있듯이 *** 또는 *** 를 중지 기호로 사용하여 명령과 컨텍스트를 분리할 수 있으며, 이는 LLM에서 더 잘 이해할 수 있습니다. 일반적인 지침으로 기존의 대부분의 LLMs은 영어에서 더 나은 작업을 수행하므로 기계 번역을 기반으로 어려운 작업을 해결하기 위해 영어 명령어를 사용하는 것이 유용하다.

**유용한 팁** 설계 원칙 외에도 기존 작업 또는 표 12의 경험적 경험을 기반으로 하는 유용한 프롬프트 팁 모음도 제공합니다. 이러한 팁은 일반적인 방식으로 제안되지만 해당 작업에 가장 적합한 프롬프트임을 나타내지 않습니다. 이 부분은 더 많은 지침이나 팁으로 지속적으로 업데이트될 것입니다. 우리는 독자들이 이 신속한 팁 컬렉션에 기여하는 것을 환영합니다. [https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts](https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts) 링크에서 프롬프트 팁에 기여 하는 자세한 절차를 제시 합니다.

**경험적 분석** 프롬프트가 작업 성능에 미치는 영향을 제시하기 위해 경험적 연구를 추가로 수행합니다. 실험을 수행하기 위해 언어 생성, 지식 활용, 복잡한 추론, 구조 데이터 생성, 정보 검색에 걸쳐 있는 다양한 태스크를 선택한다. 각 작업에 대해 위에서 소개한 일반적인 지침을 따르는 프롬프트를 수동으로 작성합니다. 테스트된 프롬프트는 주로 독자가 다양한 작업을 해결하기 위해 효과적인 프롬프트를 작성하는 방법을 이해하는 데 도움이 되기 때문에 이러한 작업에 최적이 아닐 수 있습니다. 또한 대부분의 작업에 대한 비교로 간략화된 프롬프트를 추가합니다. 섹션 7.4의 실험 설정에 이어, 우리는 복잡한 추론 태스크(컬러 객체 및 GSM8k)에 대한 ChatGPT의 3-샷 성능 및 다른 태스크에 대한 제로-샷 성능을 조사한다. 우리는 표 17의 실험 결과를 보고하며, 여기서 또한 기존 논문에 감독 성능을 참조로 포함한다.

\(\bullet\)_조심스럽게 설계된 프롬프트는 ChatGPT의 제로 샷 또는 소수 샷 성능을 높일 수 있습니다._ 동일한 작업에서 서로 다른 프롬프트를 사용한 결과를 비교함으로써, 우리는 신중하게 설계된 프롬프트를 사용하는 것이 단순한 프롬프트보다 더 나은 성능을 달성할 수 있음을 알 수 있다. 세심하게 설계된 프롬프트에서 보다 명확하게 표현된 태스크 설명(_e.g._, WMT 및 WikiFact)을 제공하거나 모델 친화적인 형식(_e.g._, GSM8k 및 OBQA)을 사용한다. 예를 들어, WikiFact 태스크의 경우, 더 상세한 태스크 설명을 갖는 프롬프트는 29.25에서 31.21로 성능 증가로 이어진다.

\(\bullet\)_ChatGPT에 대한 신중한 프롬프트 엔지니어링으로 인해 복잡한 작업이 더 많은 이점을 얻을 수 있습니다._ WikiFact 및 Colored Objects 태스크에서 설계된 프롬프트는 ChatGPT의 성능을 크게 향상시켰습니다. 즉, WikiFact에서 23.61에서 28.47, Colored Objects에서 53.20에서 66.75로 향상시켰습니다. 이러한 작업은 일반적으로 특정 출력 형식을 갖거나 배경 지식이 필요하기 때문에 LLM이 복잡한 작업에서 잘 수행되기 위해서는 신속한 엔지니어링이 필요함을 나타낸다. 예제 프롬프트는 보다 상세한 태스크 설명(_e.g._, 출력 형식 및 태스크 목표)을 제공하여 ChatGPT가 이를 이행하기 위한 복잡한 태스크 요구 사항을 더 잘 이해하는 데 도움이 될 수 있습니다.

[MISSING_PAGE_FAIL:50]

작업별 데이터로 특별히 최적화되었습니다.

\(\bullet\)_적절한 프롬프트 엔지니어링을 통해 LLM은 일부 비전통적인 NLP 작업을 처리할 수 있다_. 특정 프롬프트의 도움으로 ChatGPT는 비전통적인 NLP 작업, 즉 일반적인 권장 사항 및 대화 권장 사항을 수행할 수도 있습니다. 핵심적인 점은 이러한 작업들이 자연어로 잘 표현되거나 기술될 수 있다는 것이다. 그러나 LLM이 특정 도메인 지식과 작업 적응을 필요로 하는 이러한 작업에 직접 맞출 수 없기 때문에 ChatGPT의 성능은 이러한 작업에서 참조된 성능과 여전히 멀리 떨어져 있다[357, 462].

#### 6.1.2 Prompt Optimization

작업 프롬프트를 수동으로 만드는 것이 더 직관적이지만 시간이 많이 걸리고 더 중요한 것은 모델이 조작된 프롬프트에 매우 민감하다는 것입니다. 잘못된 프롬프트는 작업 성능을 낮춥니다(표 IV에 표시됨). 따라서, 많은 연구가 최적의 성능을 달성하기 위해 이산 프롬프트 및 연속 프롬프트에 대한 자동 최적화 접근법을 제안한다[396, 405]. 이 부분에서는 두 가지 관점, 즉 이산 프롬프트와 연속 프롬프트에서 이러한 연구를 자세히 설명한다.

**이산 프롬프트 최적화** 이산 프롬프트는 일반적으로 자연어 토큰의 시퀀스로 구성됩니다. 형태가 단순하고 유연함에도 불구하고, 이산 공간에서 프롬프트를 최적화하는 것은 조합적인 거대한 탐색 공간으로 인해 어려운 문제이다. 다운스트림 작업에 대한 효과적인 프롬프트를 자동으로 검색하기 위해 기존 연구에서는 광범위한 이산 프롬프트 접근법을 제안하며, 다음과 같이 자세히 설명한다.

\(\bullet\)_Gradient 기반 접근법._ 이러한 종류의 접근법은 구배 업데이트를 통해 출력 가능성을 최대화함으로써 프롬프트 검색 프로세스를 최적화하는 것을 목표로 한다[405, 464, 465, 466]. 대표적인 작업으로 Auto-Prompt[405]는 프롬프트 토큰을 어휘로부터 다른 후보 토큰으로 대체할 때 로그 우도의 변화에 의해 근사화된 그래디언트를 활용하여 프롬프트의 각 위치에 대한 최적 토큰을 탐욕적으로 탐색하는 그래디언트 유도 방법을 제안한다. 그러나, 이러한 검색 프로세스는 프롬프트의 각각의 위치에 대해 각각의 후보 토큰을 평가해야 하기 때문에 매우 고가일 수 있으며, 이는 다수의 추가적인 순방향 패스로 이어진다. 따라서, 이산 토큰을 연속 임베딩으로 변환하고 최적화 동안 연속 공간 상에서 그래디언트를 계산함으로써 개선된 그래디언트 방법[464]이 제안되었다.

\(\bullet\)_RL 기반 접근법._ 이산 프롬프트는 구배 역전파(gradient back-propagation)를 통해 학습되기 어렵기 때문에, 다수의 연구들은 이산 프롬프트 최적화를 강화 학습(reinforcement learning; RL) 문제로 공식화하고 최적화를 위한 RL 알고리즘을 레버리지하는 것을 제안한다[467, 468]. 예를 들어, RLPrompt[467]는 정책 네트워크를 트레이닝하여 다수의 보상 함수들을 갖는 원하는 프롬프트들을 생성한다. 이 접근법에서는 RL 훈련 효율성을 높이기 위해 몇 가지 효과적인 보상 안정화 전략도 제안된다. 훈련에 충분한 데이터를 필요로 하는 이전 작업과 비교하여, TEMPERA [468]은 미리 훈련된 RL 에이전트를 활용하여 수동으로 작성된 초기 프롬프트의 상이한 부분들을 순차적으로 편집함으로써 테스트 시간에 프롬프트를 직접 생성하는 것을 제안한다.

\(\bullet\)_편집 기반 접근법._ 위의 방법들에 대해, 그래디언트-기반 및 RL-기반 튜닝은 항상 더 큰 모델들에 대해 극도로 계산적으로 요구될 수 있고, API-기반 모델 호출들(_예를 들어,_ChatGPT)에 대해 실현 가능하지 않을 수 있다. 따라서, 다른 작업 라인은 작업 수행에 기초하여 기존의 프롬프트를 직접 편집하는 것을 목표로 한다. 구체적으로, GPS[469]는 유전자 알고리즘에서 아이디어를 차용하여 언어 모델(_i.e.,_T5)을 활용하여 클로즈 태스크 형태를 취하여 프롬프트를 편집하는 유전자 프롬프트 검색 방법을 제안한다. 모델 기반 편집 방법 외에도 삭제, 스왑, 패러프레이즈 및 추가를 포함한 프롬프트 편집[470]에도 인간 정의 작업이 사용될 수 있다. 이러한 작업을 기반으로 프롬프트를 반복적으로 편집하고 작은 예제 풀에서 모델 성능에 의해 안내되는 최상의 프롬프트를 탐욕스럽게 검색합니다.

\(\bullet\)_LLM 기반 접근법._ LLM의 예외적인 용량으로 인해 LLM을 프롬프트 생성기로 직접 활용하는 연구가 증가하고 있다[471, 472, 473]. 구체적으로, APE[471]은 LLM을 활용하여 초기 프롬프트를 생성한 후, 가장 높은 정확도로 최상의 프롬프트를 선택하고, 최종적으로 반복적 몬테카를로 탐색 방법을 통해 최적의 후보를 개선한다. 유사하게, APO[472]는 이전 프롬프트를 새로운 개선된 프롬프트들로 정제하는 방법에 대한 텍스트 피드백을 생성하도록 LLM에 지시한다. 그러나 프롬프트 공간에서의 검색은 이전 프롬프트의 전체 정제 추적을 완전히 고려하지 않고 비효율적일 수 있으므로 잠재적으로 차선 결과를 초래할 수 있다. 따라서 다른 연구[473]는 이전 프롬프트를 점수와 통합하여 LLM에게 더 나은 새로운 프롬프트를 점진적으로 생성하도록 지시한다. 그러나 이러한 접근 방식은 여전히 효과적인 프롬프트의 광대한 공간을 탐색하는 데 어려움을 겪고 있다. 인간과 같은 시행착오에서 영감을 얻은 신속한 최적화는 전략적 계획 문제로 추가로 공식화되며 [474] 몬테카를로 트리 검색을 사용하여 방대한 신속한 공간을 탐색합니다.

**연속 프롬프트 최적화** 이산 프롬프트와 달리 연속 프롬프트는 연속 임베딩 집합으로 구성되며, 이는 다운스트림 작업의 손실에 따라 기울기 업데이트를 통해 직접 최적화될 수 있습니다. 지속적인 신속한 최적화는 주로 PLM에서 연구되었지만 LLM 시대에는 매개변수의 엄청난 크기로 인해 제한된 관심을 끌고 있다. 콘텐츠 완성도를 위해 이 부분에 대한 논의를 포함합니다. 선행 연구에서, 대부분의 연구는 일반적으로 작업 데이터에 기초하여 연속적인 프롬프트를 트레이닝하기 위해 지도 학습에 의존한다. 또한, 데이터 부족 시나리오에서, 전이 학습 방법은 타겟 태스크에 대한 라벨링된 데이터의 부족을 완화하기 위해 사용될 수 있다. 이 두 가지 접근법은 아래에 자세히 설명되어 있다.

\(\bullet\)_충분한 데이터를 갖는 프롬프트 학습._ 이 접근법에서, 대부분의 기존 방법들은 연속 프롬프트를 훈련가능한 모델 파라미터로 간주하고, 이어서 충분한 다운스트림 태스크 데이터에 기초하여 교차 엔트로피 손실을 최소화함으로써 연속 프롬프트를 최적화하기 위해 지도 학습을 활용한다[475, 396, 397, 401]. 섹션 5.3.1에서 논의된 바와 같이, 프리픽스 튜닝[396]은 프리픽스들의 시퀀스(_즉, 훈련가능한 연속 벡터들의 세트)를 언어 모델들에서 각각의 트랜스포머 계층으로 프리펜스하는 반면, 프롬프트 튜닝[397]은 입력 계층에서 훈련가능한 프롬프트 벡터들만을 통합한다. LLM의 대규모 매개변수를 고정하고 연속 프롬프트 벡터만 튜닝함으로써, 이러한 종류의 접근법은 매우 매개변수 효율적일 수 있다(섹션 5.3). 그러나, 이러한 접근법들은 통상적으로 입력 의미들에 대한 충분한 고려가 결여된 입력들과 독립적이다. 따라서 [475]의 저자들은 연속 프롬프트가 입력 텍스트를 기반으로 유도되고 다운스트림 태스크 손실을 통해 학습되는 컨텍스트 튜닝을 제안한다.

\(\bullet\)_부족한 데이터로 전송됨_ 지도 학습 접근법은 최적의 연속 프롬프트를 학습하기 위해 충분한 훈련 데이터에서 요구하며, 이는 데이터 부족 도메인 및 태스크에서 잘 작동하지 않을 수 있다. 이러한 문제를 해결하기 위해, SPoT[476]는 프롬프트-기반 전이 학습 접근법을 제안하는데, 이는 먼저 몇몇 대표적인 소스 태스크들에 대한 단일 연속 프롬프트를 학습한 다음, 이 프롬프트를 사용하여 타겟 태스크에 대한 프롬프트를 초기화한다. 그러나 이 접근법은 대상 태스크의 모든 인스턴스를 해결하기 위해 동일한 프롬프트를 활용한다. 단일 태스크의 경우, 잘 학습된 프롬프트조차도 많은 모집단의 모든 데이터 인스턴스에 적합하지 않을 수 있다. 이 문제를 해결하기 위해, 개선된 방법[477]은 태스크-레벨 및 인스턴스-레벨 정보를 모두 고려하여 타겟 프롬프트들을 도출하기 위해 프롬프트 전송 프로세스 동안 적응적 주의 메커니즘을 설계한다. 프롬프트 전송 패러다임은 데이터 부족 대상 태스크를 해결하기 위해 소스 프롬프트에 인코딩된 데이터 부족 소스 태스크의 지식을 활용할 수 있다.

\begin{table}
\begin{tabular}{p{227.6pt}} \hline \hline Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write “I could not find an answer.” \\
**기사:** "\(\sim\)Jao Moutinho는 프리미어 리그 클럽 울버햄튼 원더러스와 포르투갈 대표팀의 중앙 미드필더로 마지막으로 뛴 포르투갈 축구 선수입니다." \\
**질문:** 다음 문장이 그럴듯한가요? 주앙 무티뉴가 3위로 아웃되었습니다. \\
**정답:** 단계별로 생각해 봅시다. 주앙 무티뉴는 축구 선수이다. 3위는 축구가 아니라 야구의 일부이다. 그래서 답은 No. \\ \(\sim\)Demonstrations\(>\) \\\
** articles:**\textlessnert articles, each delimited by triple quotes\textgreater{} \\
**질문:**\textlessnert question\textgreater{} \\
**Answer:**\textlessnert. \\ \hline \hline \end{tabular}
\begin{tabular}{p{227.6pt}} \hline \hline Prepare a meta-review by answering the following questions from the reviewer comments (provided after the questions). \\
1. 검토자의 의견을 바탕으로 이 원고의 핵심 기여는 무엇입니까? \\
2. 여러 검토자가 언급한 이 작업의 공통 강점은 무엇입니까? \\
3. 여러 리뷰어에 의해 강조된 이 작업의 공통 약점은 무엇입니까? \\
4. 이 논문을 개선하기 위해 어떤 제안을 하시겠습니까? \\
5. 개별 리뷰에서 언급한 누락된 참조는 무엇입니까? \\
**검토 텍스트는 다음과 같습니다. **검토자의 세 개의 주석 \(R_{1}\), \(R_{2}\), \(R_{3}\) \textgreater{} \\
**메타 검토:**\textlessnert 메타 검토\textgreater{} \\ \(\sim\)Demonstrations\textgreater{} \\ 실제로 선택한 이유를 설명 하 여 응답에 대 한 정당성을 자세히 제공 합니다. 좋은 출력은 일관성이 있어야 하고, 여러 리뷰어가 언급한 주요 장점/이슈를 강조해야 하며, 길이가 400 단어 미만이어야 하며, 마지막으로 응답은 영어로만 이루어져야 한다. \\
**검토 텍스트는 다음과 같습니다. **검토자의 세 개의 주석 \(R_{1}\), \(R_{2}\), \(R_{3}\) \textgreater{} \\
**메타 검토:**\textlessnert 메타 검토\textgreater{} \\ \(\sim\)Demonstrations\textgreater{} \\ 실제로 선택한 이유를 설명 하 여 응답에 대 한 정당성을 자세히 제공 합니다. 좋은 출력은 일관성이 있어야 하고, 여러 리뷰어가 언급한 주요 장점/이슈를 강조해야 하며, 길이가 400 단어 미만이어야 하며, 마지막으로 응답은 영어로만 이루어져야 한다. \\
**검토 텍스트는 다음과 같습니다. **검토자의 세 개의 주석 \(R_{1}\), \(R_{2}\), \(R_{3}\) \textgreater{} \\
**메타 검토:**\textlessnert 메타 검토\textgreater{} \\ \(\sim\) \\ 실제로 선택한 이유를 설명 하 여 응답에 대 한 정당성을 자세히 제공 합니다. 좋은 출력은 일관성이 있어야 하고, 여러 리뷰어가 언급한 주요 장점/이슈를 강조해야 하며, 길이가 400 단어 미만이어야 하며, 마지막으로 응답은 영어로만 이루어져야 한다. \\
**검토 텍스트는 다음과 같습니다. **검토자의 세 개의 주석 \(R_{1}\), \(R_{2}\), \(R_{3}\) \textgreater{} \\
**Meta-review:**\textlessnert meta-review\textgreater{} \\ \(\sim\) \\ \hline \hline CREATE TABLE Hightschooler (ID int primary key, name text, grade int ). /* \\
3 example rows: SELECT * FROM Highschooler LIMIT 3; ID name grade 1234 Janie 8 5678 Mary 8 9012 Mike 9 \\
*/ \\ 유효한 SQLite를 사용 하 여 위에서 제공 된 테이블에 대 한 다음 질문에 답 합니다. \ \
**질문:** Kyle의 id는 무엇인가요? \\ SQL: SELECT ID FROM Highschooler WHERE name=“Kyle”; \\ \(\sim\)Demonstrations\textgreater{} \\
**Question:**\textlessnert question\textgreater{} \\ SQL: \\ \hline \hline \end{tabular}
\end{table} TABLE XIII: Example instructions collected from [454, 463]. The blue text denotes the task description, the red text denotes the contextual information, the green text denotes the demonstrations, and the gold text denotes the prompt style.

### _In-Context Learning_

특수 프롬프트 형태로서, 먼저 GPT-3 [55]와 함께 인-컨텍스트 학습(in-context learning, ICL)이 제안되며, 이는 LLMs를 활용하는 전형적인 접근법이 되었다.

#### 6.2.1 ICL Formulation

[55]에 명시된 바와 같이, ICL은 태스크 설명 및/또는 몇몇 태스크 예들로 구성된 포맷된 자연 언어 프롬프트를 시연으로서 사용한다. 도 14는 ICL의 예시를 제시한다. 먼저 작업 설명부터 시작하여 작업 데이터 세트에서 몇 가지 예가 시범으로 선택된다. 그런 다음, 특정 순서로 결합되어 특수하게 설계된 템플릿과 함께 자연어 프롬프트를 형성한다. 마지막으로 테스트 인스턴스는 LLM이 출력을 생성하기 위한 입력으로 데모에 추가된다. 작업 데모에 기초하여, LLMs는 명시적인 그래디언트 업데이트 없이 새로운 작업을 인식하고 수행할 수 있다.

형식적으로, \(D_{k}=\{f(x_{1},y_{1}),\ldots,f(x_{k},y_{k})\}\)는 \(k\) 예제와 함께 일련의 데모를 나타내는데, 여기서 \(f(x_{k},y_{k})\)는 \(k\) 번째 작업 예제를 자연어 프롬프트로 변환하는 프롬프트 함수이다. 작업 설명 \(I\), 데모 \(D_{k}\) 및 새 입력 쿼리 \(x_{k+1}\)가 주어지면 LLM에서 생성된 출력 \(\hat{y}_{k+1}\)의 예측은 다음과 같이 공식화될 수 있습니다.

각주 40: ICL이 GPT-3의 논문 [55]에 소개되었을 때, 그것은 원래 태스크 설명 및 시연 사례의 조합으로 정의되었으며, 여기서 두 구성 요소 중 하나는 불필요하다. 이 정의에 따르면, 태스크 설명만을 사용하여 보이지 않는 태스크를 해결하기 위해 LLM이 요구될 때, 태스크 해결을 위해 ICL을 수행하는 것도 고려될 수 있는 반면, ICL 능력은 명령어 튜닝에 의해 향상될 수 있다.

\[\text{LLM}\big{(}I,\underbrace{f(x_{1},y_{1}),\ldots,f(x_{k},y_{k})}_{\text{ demonstrations}},f(\underbrace{x_{k+1}}_{\text{input answer}})\big{)}\rightarrow\hat{y}_{k+1}. \tag{12}\

여기서 실제 답 \(y_{k+1}\)은 LLM에 의해 예측될 공백으로 남겨진다. ICL의 성능은 시연에 크게 의존하기 때문에 프롬프트에서 적절하게 설계하는 것이 중요하다. 식 (12)의 구성 과정에 따라 시범을 구성하는 예제를 선택하고 각 예제를 함수 \(f(\cdot)\로 프롬프트에 포매하고 합리적인 순서로 시범을 배치하는 방법을 포함하여 프롬프트에서 시범을 포매하는 세 가지 주요 측면에 초점을 맞춘다.

ICL에 대한 포괄적인 검토가 조사 논문[50]에 제시되었으며, 우리는 이 주제에 대한 보다 일반적이고 상세한 논의를 위해 이를 참조하는 독자들을 제안한다. 이 조사와 비교하여 우리는 특히 ICL을 LLM에 적용하는 논의에 두 가지 주요 측면, 즉 실증 설계와 ICL의 기본 메커니즘에 초점을 맞춘다. 또한, ICL은 둘 다 자연어를 사용하여 작업 또는 인스턴스를 포맷한다는 점에서 명령어 튜닝(섹션 5.1에서 논의됨)과 밀접한 관련이 있다. 그러나 명령어 튜닝은 적응을 위해 LLM을 미세 조정해야 하는 반면 ICL은 활용을 위해 LLM만 프롬프트한다. 또한, 명령어 튜닝은 특히 제로-샷 설정(작업 설명만을 사용함)에서 타겟 작업을 수행하는 LLM의 ICL 능력을 향상시킬 수 있다[69].

#### 6.2.2 시연 설계

여러 연구에 따르면 ICL의 효과는 시연 설계에 크게 영향을 받는 것으로 나타났다[432, 478, 479]. 섹션 6.2.1의 논의에 이어, 우리는 _즉,_ 시연 선택, 형식 및 순서의 세 가지 주요 측면에서 ICL의 시연 설계를 소개할 것이다.

**시범 선택** ICL의 성능은 다른 데모 예제 [428]와 함께 분산이 큰 경향이 있으므로 LLM의 ICL 기능을 효과적으로 활용할 수 있는 예제의 하위 집합을 선택하는 것이 중요합니다. 두 가지 주요 실증 선택 접근법, 즉 휴리스틱 접근법과 LLM 기반 접근법이 있다.

\(\bullet\)_휴리스틱 접근법._ 단순성과 저렴한 비용으로 인해 기존 작업은 시연을 선택하기 위해 휴리스틱 방법을 널리 채택하고 있다. 여러 연구에서 \(k\)-NN 기반 검색기를 사용하여 쿼리와 의미적으로 관련된 예제를 선택한다[428, 480]. 그러나, 이들은 집합된 예를 전체적으로 평가하는 것이 아니라, 각각의 예에 대해 개별적으로 선택을 수행한다. 이 문제를 해결하기 위해 특정 작업에 대한 가장 대표적인 예제 집합을 선택하기 위해 다양성 기반 선택 전략이 제안된다[481, 482]. 나아가 [483]에서는 시연을 선정할 때 관련성과 다양성을 모두 고려하였다.

\(\bullet\)_LLM 기반 접근법._ 또 다른 작업 라인은 LLM을 활용하여 시연을 선정한다. 예를 들어, LLMs는 예를 추가한 후 성능 이득에 따라 각 예의 정보성을 직접 측정하는데 활용될 수 있다[484]. 또한, EPR[429]는 먼저 비감독 방법(_예를 들어,_BM25)으로 유사한 예들을 리콜한 다음, 밀집 리트리버(LLM에 의해 라벨링된 포지티브 및 네거티브 예들로 트레이닝됨)를 사용하여 이들을 순위화하는 2단계 검색 접근법을 제안한다. 대안적인 접근법으로서, 실증 선택의 과제는 RL 문제로 공식화될 수 있으며, 여기서 LLMs는 정책 모델을 트레이닝하기 위한 피드백을 제공하기 위한 보상 함수로서 기능한다[485]. LLM은 텍스트 주석에 대해 잘 수행되기 때문에[486], 일부 최근 연구에서는 인간의 개입 없이 LLM 자체를 데모 생성기로 사용한다[487].

요약하자면, [488]에서 논의된 바와 같이, ICL에서 선택된 데모 예들은 위의 두 가지 선택 접근법들에 대해, 테스트 쿼리와 관련될 뿐만 아니라 해결할 태스크에 대한 충분한 정보를 포함해야 한다.

**시범 형식** 작업 예제를 선택한 후 다음 단계는 LLMs에 대 한 자연어 프롬프트에 통합 하 고 형식 지정 하는 것입니다. 간단한 방법은 미리 정의된 템플릿을 해당 입력-출력 쌍으로 인스턴스화하는 것이다[36]. 보다 유익한 템플릿을 구성하기 위해 최근 연구에서는 태스크 설명을 추가하거나 사고 연쇄 프롬프트를 사용하여 LLM의 추론 능력을 향상시키는 것을 고려한다[69]. 예를 들어 [166]에서 저자는 인간이 작성한 작업 설명이 포함된 대규모 데이터 세트를 수집한다. 이 데이터 세트로 튜닝한 후 볼 수 있는 작업에 대한 성능이 향상될 수 있으며 LLM도 볼 수 없는 작업에 대해 어느 정도 일반화할 수 있다. 주석 비용을 줄이기 위해 [143]에서는 새로운 작업에 대한 작업 설명을 생성하기 위해 LLM을 안내하기 위해 사람이 작성한 작업 설명으로 구성된 시드 세트를 사용하여 반자동화된 접근법이 제안되었다. 다른 작업에 대해 수동으로 데모 형식을 주석하는 데 비용이 많이 들기 때문에 일부 작업에서는 고품질 형식을 자동으로 생성하는 방법도 연구한다. 두 가지 대표적인 방법으로 Auto-CoT [434]는 중간 추론 단계를 생성하기 위해 0-shot 프롬프트 _"차근차근 생각해보자"_ 로 LLMs를 활용하는 반면, 최소-tomost 프롬프트 [439]는 먼저 LLMs를 쿼리하여 문제 분해를 수행한 다음 LLMs를 활용하여 이전에 해결된 중간 답변을 기반으로 하위 문제를 순차적으로 해결한다.

**시범 순서** LLM은 때때로 최신 편향, 즉 _i.e._에 시달리는 것으로 표시되며, 시위가 끝날 무렵에 답변이 반복되기 쉽습니다[479]. 따라서, 데모(_i.e._, 작업 예)를 합리적인 순서로 배열하는 것이 중요하다. 초기 작업은 좋은 순서를 빠르게 찾기 위해 몇 가지 휴리스틱 방법을 제안한다. 예를 들어, 데모레이션들은 임베딩 공간에서의 질의에 대한 그들의 유사도에 따라 직접 조직될 수 있다[428]: 유사할수록, 끝에 가까울수록. 또한, 글로벌 및 로컬 엔트로피 메트릭은 상이한 데모 순서를 스코어링하기 위해 사용될 수 있다[432]. 더 많은 태스크 정보를 통합하기 위해, 일부 최근의 연구들은 태스크 라벨들을 압축하고 전송하는 데 필요한 코드 길이를 최소화하는 것을 제안하며, 이는 정보 이론[489]에서 영감을 받았다. 그러나 이러한 방법은 특정 데모 주문의 성능을 평가하기 위해 검증 세트로 라벨링된 추가 데이터가 필요하다. 이러한 필요성을 제거하기 위해 [432]의 저자는 LLM 자체에서 검증 데이터를 샘플링할 것을 제안한다.

#### 6.2.3 Underlying Mechanism

사전 훈련 후 LLM은 업데이트되지 않고 흥미로운 ICL 기능을 나타낼 수 있다. 다음에서는 LLM의 ICL 능력에 대한 두 가지 핵심 질문인 _i.e._, _"사전 훈련이 ICL 능력에 어떻게 영향을 미치는가"_ 및 _"추론 중에 LLM이 ICL을 수행하는가"_에 대해 논의한다.

**사전 훈련 효과가 ICL에 미치는 영향?** ICL은 GPT-3 [55]에서 처음 제안되며 모델 크기가 클수록 ICL 능력이 더 중요해지는 것으로 나타났습니다. 또한, 일부 연구에서는 소규모 PLM이 특수 설계된 훈련 작업에 대해 지속적인 사전 훈련[490] 또는 미세 조정[491]을 통해 강력한 ICL 능력을 입증할 수 있음을 보여주며, 이는 일반적으로 훈련 프로세스 동안 입력에 추가 작업 예를 포함한다. 이는 훈련과제 설계가 LLM의 ICL 역량에 중요한 영향요인임을 시사한다. 훈련 과제 외에도, 최근의 연구들은 ICL과 훈련 전 말뭉치 사이의 관계를 조사했다[488, 492]. 예를 들어, ICL은 이론적으로 장거리 일관성을 나타내는 문서에 대한 사전 훈련의 산물로 설명될 수 있다[488]. 또한, 또 다른 연구[492]는 파라미터 및 데이터를 스케일링할 때, 다음 단어 예측에 기초한 LLM이 언어 데이터에 존재하는 구성 구조(_e.g._, 단어와 구가 어떻게 결합되어 문장과 같은 더 큰 언어 단위를 형성하는지)로부터 학습함으로써 ICL의 능력을 발현할 수 있다는 것을 이론적으로 분석한다.

**LLM이 ICL을 수행하는 방법**: 추론 단계에서 명시적 학습 또는 업데이트가 포함되지 않기 때문에 연구자들은 주어진 데모를 기반으로 ICL 기능이 작동하는 방법을 분석하는 데 중점을 둡니다. [493]의 논의에 따르면 LLMs가 데모를 활용하는 방법은 크게 과제 인식과 과제 학습 두 가지가 있다.

\(\bullet\)_Task 인식._ 첫 번째 방법으로 LLMs은 시범에서 과제를 인식하고 사전 훈련에서 얻은 사전 지식을 활용하여 새로운 시험 과제를 해결한다. ICL의 학습 가능성을 평가하기 위해 PAC(Probably Approximate Correct) 프레임워크 [494]가 제안되었다. 사전 훈련 데이터에서 과제를 나타내는 잠재 변수가 있다고 가정하며, LLM은 시범에서 이 변수를 포착할 수 있어 ICL에서 과제를 인식할 수 있는 것으로 나타났다. 또한 ICL을 과제 인식으로 해석하는 것은 여러 실증연구[478, 495]에 의해 뒷받침되고 있다. 예를 들어, 데모들의 입력들 또는 라벨들을 입력 또는 라벨 공간으로부터 샘플링된 랜덤한 것들로 대체하는 것은 LLMs들의 성능을 크게 해치지 않는다는 것이 관찰되었는데, 이는 LLMs들이 그것들로부터 학습하는 대신에 데모들로부터 타겟 태스크를 주로 인식한다는 것을 나타낸다[478, 493]. 유사하게, LLMs는 프롬프트 템플릿이 관련 없거나 오해의 소지가 있는 경우에도 적절한 성능을 나타낼 수 있다[495].

도. 14: 인-컨텍스트 학습(ICL) 및 연쇄 사상(CoT) 프롬프트의 비교 예시. ICL은 자연 언어 설명, 여러 시연 및 테스트 쿼리로 LLM을 프롬프트하는 반면, CoT 프롬프트는 프롬프트에서 일련의 중간 추론 단계를 포함한다.

\(\bullet\)_Task learning._ 두 번째 방법으로 LLMs는 시범을 통해서만 사전 훈련 단계에서 보이지 않는 새로운 과제를 학습한다. 특히, 과제 학습은 주로 경사 하강의 관점에서 분석되며 암묵적 미세 조정으로 간주된다[496, 65]. 그런 다음, ICL은 다음과 같이 설명될 수 있다: 순방향 계산을 통해 LLM은 데모에 대한 메타-구배를 생성하고 주의 메커니즘을 통해 암묵적으로 기울기 하강을 수행한다. 실험들은 또한 LLM들 내의 특정 어텐션 헤드들이 태스크-불가지론적 원자 연산들(예를 들어,_복사 및 프리픽스 매칭)을 수행할 수 있다는 것을 보여주는데, 이는 ICL 능력[497]과 밀접한 관련이 있다. 나아가 알고리즘 학습 과정으로서 ICL을 추상화한 연구도 있다[498]. 예를 들어, [498]의 저자들은 LLM들이 사전 트레이닝 동안 그들의 파라미터들을 통해 암시적 모델들을 본질적으로 인코딩한다는 것을 발견한다. ICL에 제공된 예를 통해, LLM은 경사 하강과 같은 학습 알고리즘을 구현하거나 순방향 계산 동안 이들 모델을 업데이트하기 위해 폐쇄형 솔루션을 직접 계산할 수 있다. 이러한 설명 프레임워크 하에서 LLM은 ICL[498]을 사용하여 간단한 선형 함수 및 의사 결정 트리와 같은 일부 복잡한 함수도 효과적으로 학습할 수 있음을 보여주었다.

최근 연구[493]에서 논의한 바와 같이 LLM은 ICL에서 과제 인식과 과제 학습의 능력을 모두 나타내지만 두 능력은 모델 척도가 다른 것으로 보인다. 실험 [493]에 나타난 바와 같이, 태스크 인식의 능력은 획득하기가 더 쉽고, 심지어 350M 파라미터만을 갖는 작은 LM도 이러한 능력을 나타낼 수 있는 반면, 태스크 학습은 적어도 66B 파라미터를 갖는 LLM에 대해서만 나타날 수 있다. 또 다른 연구 [499]는 또한 특별히 설계된 실험을 통해 이 발견을 뒷받침한다. 그들은 ICL을 수행할 때 태스크 학습이 필요한 실험에서 플립된 레이블과 의미적으로 관련이 없는 레이블로 태스크를 설정한다. 결과는 작은 LLM이 레이블을 무시하고 주로 작업을 수행하기 위해 사전 지식에 의존하는 경향이 있는 반면 LLM은 사전 지식을 능가하고 시연에서 새로운 지식을 습득할 수 있어 더 나은 결과를 가져온다는 것을 시사한다. 또한, 태스크 학습 능력을 향상시키기 위해 메타-인-컨텍스트 학습[500]은 프롬프트에 단일 태스크가 아닌 여러 개의 관련 태스크를 포함시키는 것을 제안한다. 또한, 심볼 튜닝[501]은 의미적으로 관련이 없는 라벨(예를 들어, 감정 분석에 대해 긍정/부정 대신 foo/bar)이 있는 시연에서 LLM을 미세 조정함으로써 LLM이 사전 지식에 의존하는 대신 시연에서 작업을 학습하도록 강제한다.

### _Chain-of-Thought Prompting_

CoT(Chain-of-Thinking) 프롬프트[502, 33]는 산술 추론[503], 상식 추론[504] 및 기호 추론[33]과 같은 복잡한 추론 작업에서 LLM의 성능을 향상시키기 위한 개선된 프롬프트 전략이다. ICL과 같은 입력-출력 쌍으로 프롬프트를 단순히 구성하는 대신 CoT 프롬프트는 입력과 출력 사이의 브리지 역할을 하는 중간 추론 단계를 추가로 통합한다. 도 14는 CoT의 예시를 제시한다. 다음 부분에서는 먼저 기본 CoT 프롬프트 접근법과 개선된 전략에 대해 자세히 설명한 다음 CoT 프롬프트가 언제 및 왜 작동하는지 논의한다.

#### 6.3.1 기본 CoT 프롬프트 접근법

CoT 프롬프트는 먼저 ICL [33]의 확장으로 제안되며, 이는 각 데모 \(\langle\)_input, output_\(\rangle\)를 \(\langle\)_input, CoT, output_\(\rangle\)로 증가시킨다. A _CoT_ 는 _input_ 과 _output_ 을 연결 하기 위한 일련의 중간 추론 단계입니다. 이러한 증강된 데모를 통해 LLM은 이를 따라 CoT를 생성하고 새로운 입력에 대한 답변을 생성할 수 있다. 그러나 ICL에서 \(\langle\)_input, output_\(\rangle\) pairs와 달리 CoT는 구하기 어렵고 일반적으로 사람의 주석이 필요하다. 다행히도 LLM은 _"차근차근 생각해보자."_[505]와 같은 간단한 지침을 통해 CoT를 생성하도록 트리거될 수 있어 CoT가 쉽게 사용하도록 프롬프트할 수 있다. CoT 추론의 능력을 이끌어내고 LLM의 성능을 더욱 향상시킬 수 있는 대안적인 매직 프롬프트도 있는데, 이를테면 _"심호흡을 하고 이 문제에 대해 단계적으로 작업한다."_[473].

도 15에 예시된 바와 같이, CoT의 생성 프로세스는 기본 CoT 프롬프팅 접근법에서 체인 구조를 따르며, 여기서 LLM들은 단계적으로 CoT들을 생성한다. 통상적으로 CoT는 자연어 텍스트의 형식을 취한다. 그러나 텍스트 CoT는 추론을 위해 엄격한 논리를 필요로 하는 복잡한 작업에서는 잘 작동하지 않을 수 있다. 이를 고려할 때, 일부 작업은 구조화되고 정확한 특성으로 인해 코드 [506, 507]을 사용한다. 게다가, [508]의 저자들은 그들의 장점들을 결합하기 위해 CoT의 포맷으로서 텍스트 또는 코드를 동적으로 선택하는 것을 제안한다.

#### 6.3.2 개선된 CoT 프롬프트 전략

복잡한 추론 작업에서 성능 향상에도 불구하고, CoT 프롬프트는 여전히 잘못된 추론 및 불안정성과 같은 문제로 어려움을 겪는다. 이 부분에서는 먼저 더 나은 CoT 프롬프트와 향상된 CoT 생성 전략을 설계하는 방법을 소개하고, 이어서 CoT의 기본 사슬 구조의 확장을 소개한다. 그림 15는 대표적인 CoT 프롬프트 전략의 진화를 보여준다.

**더 나은 프롬프트 디자인** CoT 프롬프트는 프롬프트에 의존하여 LLM의 추론 기능을 이끌어내기 때문에 프롬프트의 디자인은 성능에 매우 중요합니다. 직접적인 접근 방법으로 다양한 CoT(_즉, 각 문제에 대한 다중 추론 경로)를 사용하는 것이 성능을 효과적으로 향상시킬 수 있음을 보인다[437]. 또 다른 직관적인 아이디어는 더 복잡한 추론 경로를 갖는 프롬프트가 LLM[433]의 추론 능력을 이끌어낼 가능성이 더 높다는 것이며, 이는 정답을 생성하는 데 더 높은 정확도를 초래할 수 있다. 그러나 이러한 모든 접근법은 주석이 달린 CoT 데이터 세트에 의존하여 실제 사용을 제한한다. 이러한 한계를 극복하기 위해, _"차근차근 생각해보자"_와 같은 매직 명령어들은 LLM들을 프롬프트함으로써 CoT들을 자동으로 구성하는데 사용될 수 있다[434].

**향상된 CoT 생성** LLM은 잘못된 추론 단계를 생성하고 생성 과정에서 불안정성을 나타내기 쉽기 때문에 CoT 생성을 개선하기 위한 많은 연구[509, 436]가 있다. 이 부분에서 우리는 CoT 생성을 향상시키기 위한 두 가지 전형적인 접근법, 즉 샘플링 기반 방법과 검증 기반 방법을 소개할 것이다.

\(\bullet\)_샘플링 기반 방법._ LLM은 추론 중에 불안정성을 겪는 것으로 알려져 있으며, 이는 생성된 추론 단계에서 불성실성을 초래할 수 있다. 이 문제를 해결하기 위해 일부 연구에서는 그리디 디코딩을 사용하는 대신 여러 추론 경로를 샘플링하는 것을 제안한다. 대표적인 해결책으로 자기 일관성[436]은 먼저 여러 가지 추론 경로를 생성한 후 해당 답변에 대해 앙상블을 취하여 다수결 투표를 통해 가장 일관성 있는 답변을 선택한다. 그러나, 이러한 방법은 대부분의 추론 경로들이 오도될 때 여전히 오답으로 이어질 수 있다. 이를 고려하여 [433]의 저자들은 복잡도가 높은 추론 경로(_e.g._, 더 많은 추론 단계)가 일반적으로 더 나은 성능을 갖는다는 관찰에 기초하여 대부분의 복잡한 추론 경로에 투표한다. 나아가, MCR[510]은 다음 단계를 생성할 때 다른 추론 경로들로부터의 단계들을 참조하는 것을 제안하고, 최종 답변을 생성하기 위해 다수의 추론 경로들에 걸쳐 추론을 수행한다.

\(\bullet\)_Verification 기반 메서드._ CoT에서 추론 단계의 순차적 특성은 특정 단계가 잘못될 때 생성된 CoT에 오류가 누적될 수 있다. 이러한 문제를 해결하기 위해, 최근 연구들은 학습된 검증기 또는 LLM 자체를 사용하여 생성된 추론 단계의 정확성을 검증할 것을 제안한다. 예를 들어, DIVERSE[509]는 상이한 입도에서 추론 단계들을 조사하기 위해 솔루션-레벨 및 단계-레벨 검증기들을 각각 트레이닝한다. 또 다른 접근법[511]은 LLM을 활용하여 특별히 설계된 추론 형식으로 단계별 자기 검증을 통해 추론 단계의 정확성을 검증한다. 또한 여러 연구에서 검증을 위한 백워드 추론을 제안하는데, 먼저 모델의 예측에서 필요한 질문 조건[512, 513] 또는 변수[514]를 추론한 후 원본과 비교한다.

**추론 구조 확장** 일반성에도 불구하고 기본 CoT의 연쇄 추론 구조는 추론 중 예측 및 역추적과 같은 탐색이 필요한 복잡한 작업을 해결하는 데 효율성을 제한합니다. 따라서, 많은 연구들은 보다 복잡한 사고 과정, 예를 들어, 트리 및 그래프 구조 추론을 설계함으로써 추론 구조를 확장하는 데 전념해 왔다.

\(\bullet\)_Tree 구조의 추론__ 이 접근법(Tree of Thoughts(ToT)에 의해 예시됨[451, 515])은 중간 생각이 노드인 계층적 트리 구조로 추론 프로세스를 공식화한다. 이러한 방식으로, LLM들이 다수의 추론 경로들을 병렬로 탐색할 수 있게 하고, 더 나아가 보다 포괄적인 결정들을 용이하게 하기 위해 룩어헤드 및 백트래킹의 동작을 지원한다. 또한, TouT[516]은 중간 생각의 불확실성을 고려하여 몬테카를로 드롭아웃에 기반한 사고 평가를 한다.

\(\bullet\)_그래프 구조 추론._ 트리 구조는 병렬 추론을 용이하게 하지만, 추론 과정에 제한을 가하기도 한다. 더 복잡한 위상 구조로 그래프는 추론에 더 큰 유연성을 제공하여 더 복잡한 관계와 상호 작용의 특성화를 가능하게 한다. 예를 들어, GoT(Graph of Thoughts) [517, 518]은 추론 과정을 임의의 그래프로 개념화하는데, 여기서 정점들은 중간 생각을 나타내고 간선들은 이들 생각들 사이의 상호 의존성을 나타낸다. ToT와 비교하여 새로운 생각을 생성할 때 다른 추론 경로로부터의 생각을 더 활용할 수 있다. 그러나 이러한 접근 방식은 LLM과의 많은 상호 작용을 필요로 하므로 사고 탐색 과정이 매우 비효율적이다. 잠재적으로 무의미한 사고 탐색을 줄이기 위해, XoT[519]는 미리 훈련된 정책 및 가치 네트워크로 사고 탐색을 안내할 것을 추가로 제안한다.

#### 6.3.3 CoT Promping에 대한 추가 논의

이 부분에서는 CoT 프롬프트와 관련된 두 가지 기본 질문인 _i.e._, _"가 LLMs에 대해 CoT 프롬프트가 언제 작동하는지 및 _"왜 LLMs가 CoT 추론"_을 수행할 수 있는지에 대한 논의를 제시한다.

**CoT 프롬핑이 LLMs에 대해 작동 하는 경우?** CoT 추론은 창발 능력이므로 [31] 충분히 큰 모델(일반적으로 10B 이상의 매개 변수를 포함)에만 긍정적인 영향을 미치지만 작은 모델에는 영향을 미치지 않습니다. 또한,

도. 15: CoT 프롬프팅 전략들의 진화의 예시. 기본적인 CoT 접근법에서 시작하여 샘플링 기반 및 검증 기반 방법을 포함한 향상된 CoT 생성 기술로 진행한다. 마지막으로, 트리 및 그래프와 같은 체인 구조의 변형으로 확장된다. 여기서, "생각"은 [33, 451]에 기재된 바와 같은 중간 추론 단계를 지칭한다.

CoT 프롬프트는 표준 프롬프트를 중간 추론 단계로 확장하기 때문에 주로 단계적 추론[33], _예: 산술 추론, 상식 추론 및 기호 추론이 필요한 작업에 효과적이다. 반면에 복잡한 추론에 의존하지 않는 다른 작업의 경우 CoT 프롬프트는 표준 프롬프트[438], _예: GLUE [260]의 MNLI-m/mm, SST-2 및 QQP보다 성능이 더 나빠질 수 있다. 흥미롭게도 CoT 프롬프트가 가져온 성능 이득은 표준 프롬프트가 좋지 않은 결과를 낳을 때만 중요할 수 있는 것 같다[33].

**LLM이 CoT 추론을 수행할 수 있는 이유**: 두 번째 질문으로 다음 두 가지 측면에서 CoT 프롬프트의 기본 메커니즘에 대해 논의합니다.

\(\bullet\)_CoT 추론 능력의 원천_ CoT 추론 능력의 원천과 관련하여, 그것에 대해 훈련된 모델들은 강력한 추론 능력을 나타내기 때문에 코드에 대한 훈련에 기인할 수 있다는 것이 널리 가설된다[520, 47, 521]. 직관적으로 코드 데이터는 알고리즘 로직과 프로그래밍 흐름으로 잘 구성되어 있으며, 이는 LLM의 추론 성능을 향상시키는 데 유용할 수 있다. 그러나 이 가설은 여전히 절제 실험에 대해 공개적으로 보고된 증거(코드에 대한 훈련 포함 및 미포함)가 부족하다. 또한, 비 CoT 데이터에 대한 명령어 튜닝이 보류된 CoT 추론 벤치마크에 대한 성능을 향상시키지 않는다는 것이 경험적으로 입증되었기 때문에 명령어 튜닝이 CoT 추론 능력을 얻는 핵심 이유는 아닌 것 같다[69].

\(\bullet\)_CoT 프롬프트 구성요소의 효과_ CoT 프롬프트와 표준 프롬프트의 주요 차이점은 최종 답변에 앞서 추론 경로를 통합하는 것이다. 따라서 일부 연구자들은 추론 경로에서 서로 다른 구성 요소의 영향을 조사한다. 특히 최근 연구에서는 CoT 프롬프트의 세 가지 핵심 구성 요소, 즉 _기호_(예: 산술 추론의 숫자 양), _패턴_(예: 산술 추론의 방정식) 및 _텍스트_(예: 기호 또는 패턴이 아닌 나머지 토큰)를 식별한다[522]. 후자의 두 부분(_즉,_패턴 및 텍스트)은 모델 성능에 필수적이며, 둘 중 하나를 제거하면 상당한 성능 하락으로 이어질 수 있음을 보여준다. 그러나 기호와 패턴의 정확성은 중요하지 않아 보인다. 또한, 텍스트와 패턴 사이에는 공생 관계가 존재한다: 텍스트는 LLM이 유용한 패턴을 생성하도록 돕고, 패턴은 LLM이 태스크를 이해하고 이를 해결하는 데 도움이 되는 텍스트를 생성하도록 돕는다[522].

요약하면, CoT 프롬프트는 LLM의 추론 능력을 이끌어내기 위한 일반적이고 유연한 접근법을 제공한다. 또한 멀티모달[523] 및 멀티언어 태스크[524]를 해결하기 위해 이 기술을 확장하려는 일부 예비 시도들이 있다.

### _Complex Task Solving을 위한 계획_

ICL 및 CoT로 프롬프팅하는 것은 개념적으로 간단하지만 다양한 작업을 해결하는 일반적인 접근법이다. 그러나, 이 접근법은 수학적 추론[525] 및 멀티홉 질의 응답[526]과 같은 복잡한 태스크들에 대해 고군분투한다. 향상된 접근법으로 복잡한 작업을 더 작은 하위 작업으로 나누고 작업을 수행하기 위한 작업 계획을 생성하기 위해 프롬프트 기반 계획이 제안되었다.

#### 6.4.1 Overall Framework

이 부분에서 우리는 먼저 복잡한 과제 해결을 위한 LLM의 일반적인 계획 패러다임을 공식화하며, 이는 그림 16에 나와 있다.

이 패러다임에서는 일반적으로 _태스크 플래너, 계획 실행기_ 및 _환경_41의 세 가지 구성 요소가 있습니다. 구체적으로, LLMs에서 플레이하는 태스크 플래너는 목표 태스크를 해결하기 위해 전체 계획을 생성하는 것을 목표로 합니다. 계획은 다양한 형태, _예를 들어,_자연어 형태의 액션 시퀀스[439] 또는 프로그래밍 언어로 작성된 실행 프로그램[443]으로 제시될 수 있다. LLM 기반 태스크 플래너는 계획 저장 및 검색을 위한 메모리 메커니즘으로 향상될 수 있으며, 이는 긴 수평 태스크에 도움이 된다. 그런 다음 계획 실행자는 계획에서 작업을 실행할 책임이 있습니다. 텍스트 작업에 대한 LLM과 같은 모델[441]에 의해 또는 코딩 작업에 대한 코드 해석기와 같은 툴에 의해 구현될 수 있다[450]. 또한 환경은 계획 실행기가 특정 작업, 예를 들어 LLM 자체[527] 또는 마인크래프트[528]와 같은 외부 가상 세계에 따라 다르게 설정할 수 있는 작업을 수행하는 것을 나타냅니다. 작업의 실행 결과에 대한 _피드백_ 을 자연어[450] 형태로 또는 다른 멀티모달 신호[446]로부터 태스크 플래너에 제공합니다.

각주 41: RL과의 유사성에도 불구하고, 우리의 제형은 계획 및 실행 단계를 분리하는 반면, RL에서는 일반적으로 에이전트에서 인터리빙된다. 이 패러다임은 일반적이지만 약간 느슨한 방식으로 정의되며 주로 독자들이 LLM의 계획 접근법의 기초가 되는 핵심 아이디어를 이해하는 데 도움이 되는 것을 목표로 한다.

복잡한 과제를 해결하기 위해서는 먼저 과제 계획자가 과제 목표를 명확하게 이해하고 LLMs의 추론을 바탕으로 합리적인 계획을 수립할 필요가 있다(6.4.2절 참조). 그런 다음, 계획 실행자는 환경에서 계획에 따라 행동하고, 환경은 작업 계획자에 대한 피드백을 생성할 것이다(섹션 6.4.3 참조). 태스크 플래너는 환경으로부터 획득된 피드백을 더 통합하여 자신의 초기 계획을 구체화하고, 태스크 솔루션으로서 더 나은 결과를 얻기 위해 상기 프로세스를 반복적으로 수행할 수 있다(섹션 6.4.4 참조).

도. 도 16: 복잡한 태스크들을 해결하기 위한 LLM들에 의한 프롬프트 기반 계획을 위한 포뮬레이션의 예시.

#### 6.4.2 Plan Generation

플랜 생성은 LLM을 프롬프트함으로써 액션 시퀀스를 직접 생성하는 것에 초점을 맞춘다. 생성된 계획의 형식에 따라 기존 작업은 텍스트 기반 접근법과 코드 기반 접근법의 두 그룹으로 나눌 수 있다.

**텍스트 기반 접근법** LLM이 자연 언어 형식의 계획을 생성하는 것은 간단합니다. 이 접근법에서, LLM들은 계획 실행기가 복잡한 태스크를 수행하고 해결하기 위한 일련의 액션들을 생성하도록 프롬프트된다. 예를 들어, Plan-and-Solve [441]은 "계획 수립"과 같은 명시적 지시를 추가하여 제로 샷 방식으로 계획에 대한 LLM을 직접 프롬프트하는 반면, Self-planning [529] 및 DECOMP [440]은 프롬프트에 데모를 추가하여 LLM이 ICL을 통해 계획을 수립하도록 안내한다. 이러한 방식으로 일부 작업은 계획할 때 추가 도구 또는 모델을 통합하는 것을 추가로 고려한다. 예를 들어 ToolFormer [80]은 먼저 LLMs를 사용하여 잠재적인 API 호출이 있는 사전 훈련 코퍼스에 주석을 달고, 그 위에 LLMs를 미세 조정함으로써 LLMs가 API 호출 시기와 방법을 학습하고 생성 중에 API에 의해 반환되는 결과를 통합할 수 있다. HuggingGPT [444]는 HuggingFace에서 사용할 수 있는 모델을 소개하고 LLM을 컨트롤러로 간주하여 설명을 기반으로 적합한 모델을 선택하고 결과를 최종 솔루션으로 집계합니다.

**코드 기반 접근법** 텍스트 기반 접근법이 직관적으로 들리지만 계획의 충실한 실행을 보장할 수 없으므로 계획이 건전하더라도 실패할 수 있습니다. 이 문제를 해결하기 위해 프로그래밍 언어, _예:_ Python 또는 PDDL에서 실행 가능한 코드 형태로 보다 검증 가능한 계획을 생성하기 위한 코드 기반 접근법이 제안되었다. 이러한 방식으로, LLMs는 먼저 프로그램을 생성하도록 프롬프트되고, 그 후 이를 실행하기 위해 결정론적 해결기를 활용한다. 예를 들어, FaithVolCt[442]와 PAL[443]은 추론 작업을 두 단계로 분해한다: 첫 번째 단계에서는 LLM이 질의에 조건화된 계획을 생성하고, 두 번째 단계에서는 결정론적 풀이자가 계획을 실행하여 최종 답변을 도출한다. 또한, 코드 기반 접근법이 유사한 방식으로 구현된 에이전트에 적용될 수 있다. 예를 들어, PROGPROMPT[530] 및 LLM+P[531]은 먼저 LLM을 활용하여 python 함수 또는 PDDL 파일 형태의 플랜을 생성한 후 가상 에이전트 또는 클래식 플래너를 활용하여 코드 기반 플랜에 따라 문제를 해결한다.

#### 6.4.3 피드백 획득

생성된 계획을 실행한 후, 환경은 LLM 기반 태스크 플래너에 피드백 신호를 생성하며, 이는 더 나은 결과를 위해 초기 계획을 구체화하는 데 사용될 수 있다. 기존 작업에서는 일반적으로 LLM 기반 작업 플래너와의 관계에 따라 내부(_즉,_LLM 자체) 및 외부(_예:_도구 또는 가상 세계) 피드백의 두 가지 피드백 소스가 있다.

**내부 피드백.** LLM 자체를 피드백 공급자로 활용할 수 있습니다. 한 가지 간단한 방법은 프롬프트를 통해 생성된 계획의 품질을 직접 평가하는 것입니다. 예를 들어, RAP[447]은 각 후보 계획이 과제 성공으로 이어질 수 있는 가능성을 평가하는 반면, 생각의 나무[527]는 그들 간의 비교를 통해 계획 전체에 투표할 것을 제안한다. 또한, LLMs는 계획 실행기의 중간 결과에 기초하여 피드백을 제공할 수 있다. 예를 들어, Reflexion[450]은 LLMs을 활용하여 희소 결과 신호(_예를 들어,_성공 또는 실패)를 구체적인 텍스트 기반 피드백(_예를 들어,_"공포 영화 대신 사용자가 질의에서 언급하는 코미디를 추천해야 한다_")으로 변환하고, 향후 계획을 위해 이 피드백을 장기 메모리에 저장한다.

**외부 피드백.** LLM 외에도 외부 개체는 피드백 신호를 제공할 수도 있습니다. 예를 들어, 코드 해석기들과 같은 툴들은 실시간 에러 메시지들을 제공하기 위한 프로그래밍 태스크들에서 널리 사용되고[450], 안정적인 확산[532]과 같은 모델들은 시각적 인지를 제공하기 위한 멀티모달 태스크들에서 사용될 수 있고[446], 마인크래프트와 같은 가상 세계들은 몰입형 경험들을 제공할 수 있다[528]. 또한, 일부 작업(예를 들어, 생성 에이전트[533])은 시뮬레이션된 환경에서 다중 에이전트 협업을 탐색하며, 여기서 각 에이전트는 환경과의 상호작용뿐만 아니라 다른 에이전트와의 통신으로부터 피드백을 받는다.

#### 6.4.4 계획 개선

환경으로부터의 피드백에 대한 액세스를 통해, 태스크 플래너는 그에 따라 자신의 현재 계획을 정제하고 더 나은 결과를 위해 "_planning - 실행 - 정제_" 루프를 반복적으로 거칠 수 있다. 이 부분에서는 기존 작업에서 세 가지 주요 정제 접근법을 요약한다.

**추론.** 환경의 피드백 데이터는 관련 없는 정보를 포함하거나 언어가 아닌 양식을 취하는 것과 같은 계획 개선을 위해 LLM에서 활용하기에 직접 적합하지 않을 수 있습니다. 이를 해결하기 위해, 일부 작업은 피드백으로부터 중요한 정보를 추출하기 위해 명시적 추론 프로세스를 추가한다[448, 449]. 예를 들어, React [449]는 피드백을 통해 추론 트레이스를 생성하기 위해 시연으로 LLMs를 프롬프트한다. 다양한 사용자 요청을 해결하기 위한 초기 계획을 수정하기 위해 관찰된 피드백을 자동으로 추론할 수 있는 AutoGPT[534]와 같은 자율 에이전트 프로젝트에서 널리 사용되었다. 그러나 이러한 접근 방식은 일반적으로 추론과 계획의 순서를 고정합니다. 더 나은 성능을 위해 두 프로세스 간의 유연한 전환을 지원하기 위해 ChatCoT[448]는 도구 증강 추론 프로세스를 LLM 기반 태스크 플래너와 도구 기반 환경 간의 다중 전환 대화로 더욱 통합한다.

**역추적** 초기 방법은 주로 기존 계획을 유지하면서 전진 작업을 계획하는 것을 고려하므로 단기 평가를 기반으로 로컬 최적 계획으로 이어질 수 있습니다. 이를 해결하기 위해 Three of Thoughts [527]은 전역 계획을 만들기 위해 너비 우선 탐색 및 깊이 우선 탐색과 같은 탐색 알고리즘으로 역추적을 허용한다. 초기 계획에서 마지막 상태로 역추적하고 다음 미개척 행동을 선택하여 계획을 단계별로 구체화한다. 또한 일부 연구[535, 446]에서는 피드백 신호를 활용하여 전체 계획을 수정하고 있다. 예를 들어, DEPS[535]는 피드백 신호에 따라 더 나은 플랜을 선택하는 반면, TIP[446]은 피드백 신호를 추가하여 LLM 기반 플래너가 초기 플랜에서 각 단계를 수정하도록 프롬프트한다.

**메모리** 장기 수평 작업을 처리 하기 위해 ICL을 통해 LM의 _단기 메모리_ 를 활용 하는 것 외에도 _장기 메모리_ 를 사용 하 여 계획 개선을 지원 하는 핵심 접근법이 되었습니다. 예를 들어, Reflexion[450]은 자기반영으로부터의 피드백을 메모리에 저장하므로, 이전의 피드백은 계획 정제를 위해 검색될 수 있다. 생성 에이전트[533]는 액션 계획 및 반성을 위한 에이전트들의 핵심 컴포넌트로서 메모리 스트림 메커니즘을 설계한다. 또한, 스킬 라이브러리 메커니즘[445, 528]은 성공적인 계획을 라이브러리에 저장하기 위해 제안되며, 이는 새로운 작업에 대한 복잡한 계획으로 재사용되고 합성될 수 있다. 장기 기억 메커니즘을 구현하기 위해 벡터 데이터베이스(_e.g._, milvus[536])와 같은 도구를 사용하여 대규모로 효율적인 저장 및 검색을 위해 계획 또는 피드백을 고차원 벡터로 인코딩할 수 있다. MemoryBank[537]는 Ebbinghaus Forgetting Curve 이론에 따라 메모리 망각 및 강화를 허용하는 메모리 갱신 메커니즘을 추가로 제안한다.

## 7 용량 및 평가

LLM의 효과성과 우수성을 검토하기 위해 실증 능력 평가 및 분석을 수행하기 위한 과제와 벤치마크의 급증이 제안되었다. 이 절에서는 먼저 언어 생성 및 이해를 위한 LLM의 기본 능력 평가의 세 가지 유형을 소개하고, 더 복잡한 설정이나 목표를 가진 몇 가지 고급 능력 평가를 제시하고, 마지막으로 기존 벤치마크, 평가 접근법 및 실증 분석에 대해 논의한다.

### _Basic Ability_

이 부분에서는 주로 LLM에 대한 능력 평가의 세 가지 기본 유형, 즉 _i.e._, 언어 생성, 지식 활용 및 복잡한 추론에 중점을 둔다. 우리는 모든 관련 작업에 대한 완전한 보장을 의도하지 않고 대신 LLM에 대해 가장 널리 논의되거나 연구된 작업에만 초점을 맞춘다는 점에 주목한다. 다음으로, 이러한 작업들에 대해 상세히 소개한다.

#### 7.1.1 언어 생성

과제 정의에 따르면 언어 생성에 관한 기존의 과제는 언어 모델링, 조건부 텍스트 생성, 코드 합성 작업으로 대별될 수 있다. 코드 합성은 전형적인 NLP 작업이 아니라는 점에 유의하여, 자연 언어 텍스트와 유사한 생성 접근법에서 다수의 LLM(코드 데이터에 대해 트레이닝됨)에 의해 직접 해결될 수 있기 때문에 논의를 위해 이를 포함한다.

**언어 모델링** LLM의 가장 기본적인 기능으로 _언어 모델링_ 은 기본 언어 이해 및 생성 용량에 주로 중점을 두는 이전 토큰 [1]을 기반으로 다음 토큰을 예측하는 것을 목표로 합니다. 이러한 능력을 평가하기 위해 기존 작업이 사용하는 일반적인 언어 모델링 데이터 세트에는 Penn Treebank[538], WikiText-103[539], Pile[161]이 있으며, 여기서 _perplexity_의 메트릭은 제로 샷 설정에서 모델 성능을 평가하는 데 일반적으로 사용된다. 경험적 연구[93, 55]는 LLM이 이러한 평가 데이터 세트에 대해 이전의 최첨단 방법에 비해 상당한 성능 향상을 가져온다는 것을 보여준다. 텍스트에서 장거리 종속성의 모델링 용량을 더 잘 테스트하기 위해 LAMBADA 데이터 세트 [233]이 도입되었으며, 여기서 LLM은 문맥 단락에 기초하여 문장의 마지막 단어를 예측하는 데 필요하다. 그런 다음, 예측된 마지막 단어의 정확도와 복잡도를 사용하여 LLM을 평가한다. 기존 작업에서 볼 수 있듯이 언어 모델링 작업에 대한 성능은 일반적으로 스케일링 법칙[30]을 따르며, 이는 언어 모델을 스케일링하면 정확도가 향상되고 복잡성이 감소한다는 것을 의미한다.

**조건부 텍스트 생성** 언어 생성의 중요한 주제로서 조건부 텍스트 생성 [48]은 일반적으로 기계 번역[624], 텍스트 요약[548] 및 질의 응답[557]을 포함하여 주어진 조건에 따라 특정 작업 요구 사항을 충족하는 텍스트를 생성하는 데 중점을 둡니다. 생성된 텍스트의 품질을 측정하기 위해, 자동 메트릭들(_e.g._, Accuracy, BLEU[625] 및 ROUGE[626]) 및 인간 등급들이 성능을 평가하기 위해 전형적으로 사용되어 왔다. 강력한 언어 생성 능력으로 인해 LLM은 기존 데이터 세트와 벤치마크에서 놀라운 성능을 달성했다. 예를 들어, GPT-4는 상당한 언어적 거리를 갖는 언어의 번역 작업에도 상용 번역 제품과 유사한 성능을 나타낸다[627]. 뉴스 요약 태스크들(_i.e._, CNN/DM 및 XSUM)에서, LLM들은 또한 인간 프리랜서 작가들과 비교가능한 성능을 입증한다[628]. 모델 용량에 대한 급속한 진전에도 불구하고 조건부 텍스트 생성 작업에서 LLM의 성능을 충실히 평가하기 위한 기존 자동 메트릭의 실현 가능성에 대한 우려가 증가하고 있다[628, 629, 630]. 자동 메트릭에 대한 대안으로서, 최근의 연구들은 또한 생성된 콘텐츠의 품질을 조사하기 위해 LLMs을 생성 평가자로 통합할 것을 제안한다[631, 632, 138]. 더욱이, 연구자들은 또한 구조화된 데이터 생성[458] 및 긴 텍스트 생성[633, 46, 46]과 같은 LLMs에 대한 보다 도전적인 언어 생성 태스크를 탐색한다.

**코드 합성** 고품질 자연어 텍스트를 생성하는 것 외에도 기존 LLM은 형식 언어, 특히 _코드 합성_[635]라고 하는 특정 조건을 충족하는 컴퓨터 프로그램(_i.e._, 코드)을 생성하는 강력한 능력을 보여줍니다. 자연어 생성과 달리 생성된 코드는 해당 컴파일러 또는 인터프리터와의 실행으로 직접 확인할 수 있기 때문에 기존의 작업은 대부분 테스트 케이스인 _i.e._, pass@_k42_에 대한 합격률을 계산하여 LLM으로부터 생성된 코드의 품질을 평가한다. 최근 APPS[378], HumanEval[105] 및 MBPP[208]와 같은 LLM의 코드 합성 능력을 평가하기 위해 기능적 정확성에 초점을 맞춘 여러 코드 벤치마크가 제안되었다. 전형적인 프로그래밍 문제는 텍스트 명세 및 정답 검사를 위한 테스트 케이스 등 다양한 프로그래밍 문제로 구성된다. 그러한 능력을 향상시키기 위해, LLM들을 코드 합성 태스크들에 효과적으로 적응시킬 수 있는, 코드 데이터 상의 LLM들을 미세 조정(또는 사전 트레이닝)하는 것이 핵심이다[86]. 또한, 기존 연구는 코드를 생성하는 새로운 전략, 예를 들어, 다중 후보 솔루션을 샘플링하고 [208] 계획 유도 디코딩[636]을 제안했으며, 이는 프로그래머에 의한 버그 고정 및 코드 계획 프로세스의 모방으로 간주될 수 있다. 인상적으로, LLMs는 최근 프로그래밍 콘테스트 플랫폼 Codeforces[114]에서 사용자들 중 상위 28%의 순위를 달성함으로써 인간과 경쟁적인 성능을 보여주고 있다. 또한, 코딩 IDE(_e.g._, Visual)에서 프로그래밍을 돕기 위해 GitHub Copilot이 출시되었다.

[MISSING_PAGE_FAIL:60]

다양한 표현을 가진 의미적으로 동등한 참조. 또한, LLMs는 단일 예측을 평가하는 것[631, 642, 632] 또는 여러 후보들을 비교하는 것[643, 644, 138, 645]을 포함하는, 참조 없는 방식으로 텍스트 생성의 평가자로서 널리 채용된다. 그럼에도 불구하고, LLM은 언어 생성 평가자로서 편향(예를 들어, 인간-작성된 텍스트보다 LLM-생성된 텍스트에 대한 오더 편향 또는 선호도)을 노출시켜, 인간 평가와 비교할 때 불균형을 입증할 수 있다[646, 632, 647].

\(\bullet\)_Underperforming specialized generation._ LLM이 일관된 텍스트를 생성하기 위해 일반적인 언어 패턴을 학습했지만 특수 영역이나 작업을 처리할 때 생성 능력이 제한될 수 있다. 예를 들어, 일반 웹 기사에 대해 훈련된 언어 모델은 많은 의학 용어 및 방법을 포함하는 의학 보고서를 생성할 때 어려움에 직면할 수 있다. 직관적으로, 도메인 지식은 모델 전문화에 매우 중요해야 한다. 그러나 이러한 전문지식을 LLMs에 주입하는 것은 쉽지 않다. 최근 분석[47, 648]에서 논의한 바와 같이 LLM이 일부 영역에서 탁월할 수 있는 특정 능력을 나타내도록 훈련될 때 다른 영역에서 어려움을 겪을 수 있다. 이러한 문제는 신경망 학습에서 새로운 지식과 오래된 지식을 통합하는 갈등 현상을 의미하는 _재난적 망각_[649, 650]과 관련이 있다. 유사한 사례가 LLM의 인간 정렬에서도 발생하며, 여기서 _"정렬 세금"_[66](예를 들어, 상황 내 학습 능력의 잠재적 손실)은 인간의 가치와 요구에 정렬하기 위해 지불되어야 한다. 또한, 시퀀스 모델링 아키텍처의 한계로 인해 LLMs은 구조화된 데이터의 이해와 생성에 여전히 어려움을 겪고 있다. 결과적으로, 이들은 종종 지식-베이스 질의 응답 및 시맨틱 파싱과 같은 복잡한 구조화된 데이터 태스크들 상의 태스크-특정 모델들에 뒤처진다[651, 458]. 따라서, LLMs을 다양한 작업 시나리오에 유연하게 적용할 수 있는 동시에 원래 능력을 최대한 유지할 수 있는 효과적인 모델 특화 방법을 개발하는 것이 중요하다.

#### 7.1.2 Knowledge Utilization

지식 활용은 지식 집약적인 작업(예: 상식 질의 응답 및 사실 완성)을 지원하는 사실 증거를 기반으로 수행하는 지능형 시스템의 중요한 능력이다. 구체적으로, LLM은 사전 훈련 말뭉치로부터 풍부한 사실 지식을 적절하게 활용하거나 필요할 때 외부 데이터를 검색해야 한다. 특히, 질의 응답(QA)과 지식 완성(knowledge completion)은 이러한 능력을 평가하기 위해 일반적으로 사용되는 두 가지 과제이다. 테스트 작업(질문 응답 또는 지식 완료)과 평가 설정(_with_ 또는 _with_ 외부 리소스)에 따라 기존 지식 활용 작업을 폐쇄형 문서 QA, 개방형 문서 QA43 및 지식 완료의 세 가지 유형으로 분류한다.

각주 43: 이 부분에서 오픈북 QA는 외부 지식 자원으로부터 유용한 정보를 추출하고 활용해야 하는 QA 태스크를 말하며, 클로즈드북 QA의 반대(사전 훈련 말뭉치에서 인코딩된 정보만 사용)이다. 외부 과학 사실을 추출하고 활용하여 오픈북 QA 작업의 설정을 따르는 OpenBookQA [566]이라는 데이터 세트도 있다는 점에 유의한다.

**닫힌 책 QA.** 닫힌 책 QA 작업 [652]는 사전 훈련 말뭉치에서 LLMs의 획득된 사실 지식을 테스트하며, 여기서 LLMs는 외부 리소스를 사용하지 않고 주어진 컨텍스트에 기초하여만 질문에 답해야 합니다. 이 능력을 평가하기 위해 정확도 메트릭이 널리 채택된 자연 질문[554], 웹 질문[557] 및 트리비아QA[558]를 포함하여 활용할 수 있는 여러 데이터 세트가 있다. 경험적 결과들은 LLM들이 이 설정에서 잘 수행될 수 있고 심지어 최첨단 오픈-도메인 QA 시스템들의 성능과 일치할 수 있다는 것을 밝혀냈다[56]. 또한, 폐쇄-북 QA 태스크들에 대한 LLM들의 성능은 모델 크기 및 데이터 크기 둘 다의 관점에서 스케일링 법칙 패턴을 보여준다: 파라미터들 및 트레이닝 토큰들을 스케일링하는 것은 LLM들의 용량을 증가시킬 수 있고 그들이 사전 트레이닝 데이터로부터 더 많은 지식을 학습(또는 암기)하는 것을 도울 수 있다[56]. 또한, 유사한 파라미터 스케일 하에서, 평가된 태스크들과 관련된 더 많은 사전 트레이닝 데이터를 갖는 LLM들은 더 나은 성능을 달성할 것이다[81]. 또한, 닫힌 문서 QA 설정은 LLMs에 의해 인코딩된 사실적 지식의 정확성을 조사하기 위한 테스트베드를 제공한다. 그러나 기존 작업 [55]에서 볼 수 있듯이 LLMs은 사전 훈련 데이터에 존재하는 경우에도 세밀한 지식에 의존하는 QA 작업에 대해 덜 잘 수행할 수 있다.

**Open-Book QA.** 닫힌-Book QA와 달리 오픈-Book QA 작업에서 LLMs는 외부 지식 기반 또는 문서 모음에서 유용한 증거를 추출한 다음 추출된 증거를 기반으로 질문에 답할 수 있습니다[653, 654, 655]. 일반적인 개방형 문서 QA 데이터 세트(예:_자연 질문[554], OpenBookQA[566] 및 SQuAD[569])는 닫힌 문서 QA 데이터 세트와 겹치지만 외부 데이터 원본인 _예:_위키피디아를 통합합니다. 정확도 및 F1 점수의 메트릭은 평가를 위한 오픈북 QA 작업에서 널리 사용된다. 외부 리소스들로부터 관련 지식을 선택하기 위해, LLM들은 종종 텍스트 리트리버(또는 심지어 검색 엔진)와 페어링되며, 텍스트 리트리버는 LLM들과 독립적으로 또는 공동으로 트레이닝된다[653, 657, 81]. 또한, 이전 작업 [658, 659, 660]은 리트리버들이 추론 경로를 검증하고 정류하는 데 LLM들을 도울 수 있음을 나타냈다. 평가에서 기존의 연구들은 주로 LLM이 추출된 지식을 어떻게 활용하여 질문에 답하는지를 테스트하는 데 초점을 맞추고 있으며, 검색된 증거가 생성된 답변의 정확도를 크게 향상시킬 수 있음을 보여주며, 더 작은 LLM이 \(10\times\) 더 큰 답변을 능가할 수 있게 한다[653, 657]. 또한, 지식 정보의 최신성을 평가하기 위해 오픈북 QA 작업도 사용할 수 있다. 구식 지식 자원으로부터 사전 트레이닝 또는 검색은 LLMs로 하여금 시간에 민감한 질문들에 대한 오답들을 생성하게 할 수 있다[653].

**지식 완료** 지식 완료 작업에서 LLM은 지식 단위(_예:_, 지식 트리플)의 누락된 부분을 완료하거나 예측하는 데 활용할 수 있는 지식 베이스[576]로 간주될 수 있습니다. 이러한 작업은 LLM이 사전 훈련 데이터에서 얼마나 많은 지식과 어떤 종류의 지식을 학습했는지 조사하고 평가할 수 있다. 기존의 지식 완성 작업은 크게 지식 그래프 완성 작업(_e.g._, FB15k-237[572]와 WN18RR[574])과 사실 완성 작업(_e.g._, WikiFact[571])으로 나눌 수 있는데, 이들은 각각 특정 사실에 대한 지식 그래프와 불완전한 문장으로부터 트리플을 완성하는 것을 목표로 한다. 경험적 연구들은 기존의 LLMs들이 특정 관계 유형과 관련된 지식 완성 작업을 수행하기 어렵다는 것을 밝혀냈다[520]. WikiFact에 대한 평가 결과에서 볼 수 있듯이 LLMs은 사전 훈련 데이터(_e.g._, 통화 및 저자)에서 발생하는 여러 빈발 관계에 대해 잘 수행되는 반면 희귀한 데이터(_e.g._, discoverer_or_inventor 및 place_of_birth)에는 잘 수행되지 않는다. 흥미롭게도 동일한 평가 설정(_e.g._, in-context learning)에서 InstructGPT(_i.e._, text-davinci-002)는 WikiFact의 모든 하위 집합에서 GPT-3보다 우수하다.

**주요 문제**. LLM은 지식 정보를 포착하고 활용하는 데 핵심적인 진전을 이루었지만 아래에서 논의되는 두 가지 주요 문제로 어려움을 겪고 있다.

\(\bullet\)_환각._ 사실 텍스트를 생성할 때 어려운 문제는 _환각 세대_[661, 638]이며, 여기서 생성된 정보는 기존 소스(_내인성 환각_)와 충돌하거나 사용 가능한 소스(_외인성 환각_)에 의해 검증될 수 없으며, 이는 그림 17의 두 가지 예에 의해 예시된다. 환각은 기존 LLM에서 널리 발생하며, GPT-4[46]와 같은 가장 우수한 LLM에서도 발생한다. 또한 기존의 연구는 LLMs가 강력한 ChatGPT까지 텍스트에서 환각 콘텐츠를 인식하는 데 어려움을 겪는다는 것을 보여준다[602]. 또한 언어 작업을 넘어 최근 연구에 따르면 대형 시각 언어 모델(LVLM)도 환각, 즉 동반 이미지에 없는 객체를 생성하는 어려움에 직면해 있다[662]. 본질적으로 LLMs는 내부 또는 외부 지식의 사용을 정확하게 통제할 수 있는 능력이 여전히 부족한 과제 해결에서 지식을 "무의식적으로" 활용하는 것으로 보인다. 환각은 LLM이 원하지 않는 출력을 생성하도록 오도하고 대부분 성능을 저하시켜 실제 응용 프로그램에 LLM을 배포할 때 잠재적인 위험을 초래할 수 있다. 이 문제를 완화하기 위해, (섹션 5.2에서 논의된 바와 같이) 정렬 튜닝 전략들은 고품질 데이터에 대한 LLM 튜닝 또는 인간 피드백을 사용하는 기존 작업 [66]에서 널리 활용되어 왔다. 더욱이, 신뢰할 수 있는 정보 소스의 제공을 위한 외부 도구의 통합은 환각 문제를 완화하는 데 도움이 될 수 있다[659, 81, 602]. 또 다른 연구 라인은 환각을 식별하기 위해 LLM의 불확실성 추정을 활용한다[663, 664]. 예를 들어, 환각 사실이 다른 샘플링된 출력에 걸쳐 불일치를 나타내기 쉽다는 점을 고려하여 SelfCheckGPT [664]는 샘플링된 출력 내에서 정보 불일치를 측정하여 환각을 감지한다. 환각 문제의 평가를 위해 모델에 의해 모방된 인간의 거짓을 탐지하기 위한 일련의 환각 탐지 작업이 제안되었다. 보다 최근에 HaluEval[602]는 태스크별 및 일반 시나리오 모두에서 환각을 인식하는 언어 모델의 능력을 평가하기 위해 대규모 LLM 생성 및 인간 주석이 있는 환각 샘플을 생성한다.

**Enhucination.**

LLM은 기존 소스와 충돌하거나 사용 가능한 소스에서 확인할 수 없는 거짓 정보를 생성하는 경향이 있다. ChatGPT와 같은 가장 강력한 LLM조차도 생성된 텍스트의 환각을 이동하는 데 큰 어려움에 직면해 있다. 이 문제는 정렬 조정 및 도구 활용과 같은 특별한 접근법에 의해 부분적으로 완화될 수 있다.

\(\bullet\)_Knowledge recency._ 또 다른 주요 과제로서 LLM은 필요한 과제를 해결할 때 어려움을 겪을 것이다.

도. 17: 공개 LLM에 대한 내재적 및 외재적 환각의 예(접근일: 3월 19일, 2023). 내재적 환각의 예로서 LLM은 투입과 모순되는 신디와 Amy의 관계에 대해 상반된 판단을 내린다. 외재적 환각의 경우, 이 예에서 LLM은 LLM(이러한 맥락에서)의 의미를 올바르게 이해할 수 있지만 RLHF(인간 피드백으로부터의 강화 학습)의 의미에 대한 잘못된 이해로 보인다.

학습 데이터를 넘어서는 최신 지식 이 문제를 해결하기 위해 간단한 접근법은 LLM을 정기적으로 새 데이터로 업데이트하는 것이다. 그러나 LLM을 미세 조정하는 것은 매우 비용이 많이 들고 LLM을 점진적으로 훈련할 때 치명적인 망각 문제를 일으킬 가능성이 있다. 따라서 기존 LLMs에 새로운 지식을 통합하여 최신 상태로 만들 수 있는 효율적이고 효과적인 접근 방식을 개발할 필요가 있다. 기존의 연구들은 LLMs를 보완하기 위해 외부 지식 소스(_예를 들어,_검색 엔진)를 활용하는 방법을 탐색하였는데, LLMs[653]와 공동으로 최적화되거나 플러그 앤 플레이 모듈[659]로 사용될 수 있다. 예를 들어, ChatGPT는 최신 정보 소스에 액세스하기 위해 검색 플러그인을 이용한다[665]. 추출된 관련 정보를 컨텍스트에 통합함으로써 [666, 667, 668] LLMs는 새로운 사실적 지식을 획득하고 관련 작업에 대해 더 잘 수행할 수 있다. 그러나, 그러한 접근은 여전히 피상적인 수준인 것 같다. 또한, 기존 연구들은 내재적 지식을 갱신하기 위해 언어 모델의 편집 파라미터를 탐색하기도 한다[669, 670, 671]. 그럼에도 불구하고, 이전의 연구[672]는 몇몇 파라미터 편집 방법들이 작은 언어 모델들의 성능을 향상시킬 수 있음에도 불구하고 LLM들에 대해 잘 수행되지 않는다는 것을 보여주었다. 따라서, 내재적 지식을 직접 수정하거나 LLMs에 특정 지식을 주입하는 것은 여전히 어려운 문제이며, 이는 여전히 미해결 연구 문제로 남아 있다[672]. 최근에는 LLMs에 대한 지식 편집 연구를 용이하게 하기 위한 유용한 프레임워크 _EasyEdit_[673]이 출시되었다.

#### 7.1.3 복합 추론

복잡한 추론은 근거나 논리를 이해하고 활용하여 결론을 도출하거나 결정을 내리는 능력을 의미한다[51, 52]. 추론 과정에서 개입된 논리와 증거의 유형에 따라 기존의 평가 과제를 지식 추론, 기호 추론, 수학적 추론의 세 가지 대범주로 구분하는 것을 고려한다.

**지식 추론** 지식 추론 작업은 주어진 질문에 답하기 위해 사실적 지식에 대한 논리적 관계와 증거에 의존합니다. 기존의 연구는 주로 특정 데이터 세트를 사용하여 해당 유형의 지식의 추론 능력을 평가하는데, _예: 상식 지식 추론의 경우 CSQA[504]/StrategyQA[185], 과학 지식 추론의 경우 ScienceQA[565]를 사용한다. 예측된 결과의 정확성 외에도, 기존 작업[565]은 또한 자동 메트릭(_예를 들어, BLEU) 또는 인간 평가를 통해 생성된 추론 프로세스의 품질을 평가했다. 전형적으로, 이러한 태스크들은 LLM들이 주어진 질문에 대한 답변에 도달할 때까지, 사실적 지식에 기초하여 단계적 추론을 수행할 것을 요구한다. 단계적 추론 능력을 도출하기 위해 LLM의 복잡한 추론 능력을 향상시키기 위한 CoT 프롬프트 전략[33]이 제안되었다. 섹션 6.3에서 논의된 바와 같이, CoT는 다중 단계 추론을 수행하도록 LLM을 안내하기 위한 프롬프트에 수동으로 생성[33]되거나 자동으로 생성[674]될 수 있는 중간 추론 단계를 포함한다. 이러한 방식은 LLMs의 추론 성능을 크게 향상시켜, 여러 복잡한 지식 추론 태스크에 대한 새로운 최신 결과를 유도한다[33, 56, 526]. 또한, 지식 추론 작업을 코드 생성 작업으로 재구성한 후, 연구자들은 특히 코드에 대해 사전 훈련된 LLMs를 사용하여 LLMs의 성능을 더욱 향상시킬 수 있음을 발견했다[211]. 그러나, 지식 추론 작업의 복잡성으로 인해, 현재 LLM의 성능은 여전히 상식 추론과 같은 작업에서 인간의 결과보다 뒤처진다[675, 56, 33]. 일반적인 실수 유형으로서 LLM은 부정확한 중간 단계를 생성하여 잘못된 최종 결과를 초래할 수 있다. 이 문제를 해결하기 위해, 기존 연구는 전체 추론 체인의 정확도를 향상시키기 위해 특별한 디코딩 또는 앙상블 전략을 제안했다[436, 437].

**기호 추론 44.** 기호 추론 작업은 주로 공식 규칙 설정에서 기호를 조작하여 몇 가지 특정 목표를 달성하는 데 중점을 둡니다. [51] 여기서 연산 및 규칙은 사전 훈련 동안 LLM에서 볼 수 없었을 수 있습니다. 기존의 작업[33, 439, 505]은 마지막 문자 연결 및 동전 뒤집기의 작업에서 LLMs을 일반적으로 평가하는데, 여기서 평가 예는 컨텍스트 내 예(도메인 내 테스트_라고 함) 또는 그 이상의 단계(도메인 외 테스트_라고 함)와 동일한 추론 단계를 요구한다. 영역 외 테스트의 예에서 LLM은 문맥에서 두 개의 단어가 있는 예만 볼 수 있지만 LLM은 세 개 이상의 단어의 마지막 문자를 연결해야 한다. 일반적으로, 생성된 심볼들의 정확도는 이러한 태스크들에 대한 LLM들의 성능을 평가하기 위해 채택된다. 따라서 LLMs은 복잡한 시나리오에서 기호 연산과 그 구성 사이의 의미 관계를 이해해야 한다. 그러나, 도메인 외 설정 하에서, LLM들은 심볼릭 연산들 및 규칙들의 복잡한 구성들(예를 들어, 문맥 예들에서 연산들의 수의 두 배)을 보지 못했기 때문에, LLM들이 그들의 정확한 의미들을 포착하기 어렵다. 이 문제를 해결하기 위해 기존 연구에서는 스크래치패드[591, 676]와 튜터[677] 전략을 통합하여 LLM이 기호 연산을 더 잘 조작할 수 있도록 지원하여 더 길고 복잡한 추론 프로세스를 생성한다. 또 다른 연구 분야는 정규 프로그래밍 언어를 사용하여 기호 연산과 규칙을 표현하는데, LLM은 코드를 생성하고 외부 해석자와 함께 실행하여 추론 과정을 수행해야 한다. 이러한 방식은 복잡한 추론 과정을 각각 LLM 및 인터프리터에 대한 코드 합성 및 프로그램 실행으로 분해하여, 보다 정확한 결과를 갖는 단순화된 추론 과정으로 이어질 수 있다[443].

각주 44: [33]에 이어서, 우리는 주로 LLMs를 평가하기 위해 특별히 설계된 기호 추론 과제에 대해 논의한다. 우리는 KBQA의 지식 그래프로부터 논리 규칙을 추론하는 것과 같은 전통적인 NLP 작업에서 기호 추론 방법을 고려하지 않는다.

**수학적 추론.** 수학적 추론 과제는 문제를 해결하거나 증명문을 생성하기 위해 수학적 지식, 논리 및 계산을 종합적으로 활용해야 합니다. 기존의 수학적 추론 과제는 주로 수학 문제 해결과 자동화된 정리 증명으로 범주화할 수 있다. 수학 문제 해결 과제의 경우, SVAMP[592], GSM8k[184] 및 MATH[364] 데이터 세트가 평가에 일반적으로 사용되며, 여기서 LLM은 수학적 문제에 답하기 위해 정확한 구체적인 숫자 또는 방정식을 생성해야 한다. 이러한 태스크들 또한 다단계 추론을 요구하기 때문에, CoT 프롬프트 전략은 추론 성능을 향상시키기 위해 LLMs에 널리 채택되었다[33]. 또 다른 실용적인 전략으로서, 대규모 수학적 코퍼라에 대한 LLM들을 지속적으로 사전 트레이닝하는 것은 수학적 추론 과제들에 대한 그들의 성능을 크게 향상시킬 수 있다[678, 35, 203]. 또한, 서로 다른 언어의 수학 문제들이 동일한 수학적 논리를 공유하기 때문에, 연구자들 또한 LLM의 다국어 수학적 추론 능력을 평가하기 위해 다국어 수학 단어 문제 벤치마크 [524]를 제안한다. 또 다른 도전 과제로서, 자동화된 정리 증명(ATP) [679, 600, 598]은 추론 모델이 추론 논리와 수학적 기술을 엄격하게 따라야 한다. 이 작업에 대한 성능을 평가하기 위해 PISA [599] 및 miniF2F [600]은 평가 메트릭으로 _증명 성공률_ 을 사용하는 두 개의 전형적인 ATP 데이터 세트입니다. 전형적인 접근법으로서, ATP에 대한 기존의 작업은 LLM을 활용하여 린, 메타매스 및 이사벨[680, 681, 682]과 같은 상호작용 정리 증명(ITP)을 사용하여 증명을 찾는 것을 돕는다. ATP 연구의 주요 한계는 공식 언어에서 관련 말뭉치의 부족이다. 이를 해결하기 위해 여러 연구에서 LLMs을 활용하여 새로운 데이터를 보강하기 위해 비공식 진술을 공식 증명으로 변환하거나[683] 증명의 검색 공간을 줄이기 위해 초안 및 증명 스케치를 생성한다[684].

**주요 문제** 발전에도 불구하고 LLM에는 복잡한 추론 작업을 해결하는 데 몇 가지 제한 사항이 여전히 있습니다.

* _일관성을 추론합니다._ 개선된 추론 전략(예를 들어, CoT 프롬프트)으로, LLMs는 지원 로직 및 증거에 기초하여 단계적 추론을 수행함으로써, 몇몇 복잡한 추론 태스크들을 해결할 수 있다. 효과성에도 불구하고, _추론 불일치_ 문제는 분해된 추론 과정에서 종종 발생한다. 구체적으로, LLMs는 잘못된 추론 경로에 따라 정답을 생성하거나, 올바른 추론 과정[33, 442] 후에 오답을 생성할 수 있으며, 이는 도출된 답변과 추론 과정의 불일치로 이어질 수 있다. 이 문제를 완화하기 위해 기존 연구에서는 외부 도구 또는 모델을 통해 LLM의 전체 생성 프로세스를 안내하고 [636, 451, 437], 잠재적인 오류를 수정하기 위한 추론 프로세스 및 최종 답변을 다시 확인하거나 [685, 686, 687] 프로세스 기반 피드백으로 LLM을 미세 조정하도록 제안했다[688, 689]. 예를 들어, _생각 나무(ToT)_[451]는 LLM이 다양한 추론 경로를 동시에 탐색하고 자체 평가함으로써 의사 결정 프로세스에 참여할 수 있도록 권한을 부여합니다. 추론 프로세스들을 정제하기 위해, Self-Refine[685]은 자체 생성된 솔루션들에 대한 LLM들로부터 피드백을 이끌어내고, 피드백에 기초한 솔루션들의 반복적 정제를 가능하게 한다. 또한, 여러 연구에서 훈련 중 과정 기반 수퍼비전의 통합을 통해 LLM의 추론 사슬의 일관성을 개선하였다[688, 689]. 유망한 해결책으로서, 최근의 접근법들은 복잡한 추론 태스크들을 코드 생성 태스크들로 재형성하는데, 여기서 생성된 코드의 엄격한 실행은 추론 프로세스와 결과 사이의 일관성을 보장한다. 또한, 유사한 입력들을 갖는 태스크들 사이에 불일치가 존재할 수 있다는 것이 밝혀졌는데, 여기서 태스크 디스크립션의 작은 변화들은 모델이 상이한 결과들을 생성하게 할 수 있다[592, 49]. 이 문제를 완화하기 위해, 자기 일관성[436]은 LLM의 디코딩 프로세스를 향상시키기 위해 다중 추론 경로의 앙상블을 채택한다.

* **Resoning Irconsistency.** LLM은 잘못된 추론 경로에 따라 정답을 생성하거나 올바른 추론 프로세스 후에 잘못된 답변을 생성하여 파생된 답변과 추론 프로세스 간의 불일치를 초래할 수 있습니다. 이 문제는 프로세스 수준의 피드백으로 LLM을 미세 조정하고 다양한 추론 경로의 앙상블을 사용하고 자기 성찰 또는 외부 피드백으로 추론 프로세스를 정제함으로써 완화될 수 있다.
* **수치 계산** 복잡한 추론 작업의 경우 LLM은 특히 많은 수를 사용하는 산술[677, 690, 49]과 같이 사전 훈련 중에 거의 발생하지 않는 기호에 대해 관련된 수치 계산에서 여전히 어려움에 직면합니다. 이 문제를 해결하기 위해, 직접적인 방법은 합성된 산술 문제에서 LLM을 조정하는 것이다[691, 361]. 또한, 연구의 급증은 훈련 및 추론 단계에서 중간 계산 단계를 추적함으로써 수치 계산 성능을 향상시킨다[676, 692, 361], _예를 들어_ 긁힌 추적이다. 또한, 기존의 작업[80]은 특히 산술 연산을 처리하기 위한 외부 도구(_예를 들어,_계산기)도 통합하였다. 보다 최근에, ChatGPT는 외부 툴들을 사용하기 위한 플러그인 메커니즘을 제공하였다[665]. 이러한 방식으로 LLM은 도구를 적절하게 조작하는 방법을 배워야 한다. 이를 위해 연구자들은 LLM을 조정하기 위한 도구(심지어 LLM 자체도)를 사용하여 예를 강화하거나[80, 693] 상황 내 학습을 위한 명령 및 예를 고안했다[443]. 외부 도구의 도움 외에도, 최근의 연구들은 숫자들을 개별 토큰들로 토큰화하는 것(_예를 들어,_LLMA 및 Galactica tokenizers)이 LLMs의 고유한 연산 능력을 향상시키는 유용한 접근법이라는 것을 발견한다[690, 361]. 한 가지 가능한 설명은 서브워드 토큰화 기술들이 숫자들을 토큰화할 때 일관되지 않은 시퀀스들을 초래할 수 있다는 것이다. 예를 들어, 서브워드 토큰화기를 사용하여 정수 7481은 \(7\_481\)로 토큰화될 수 있는 반면, 74815는 \(748\_15\)(다른 분할을 갖는 동일한 숫자 부분 문자열)로 토큰화될 수 있다[361]. 비교로서, 숫자에 대한 디지트 기반 토큰화는 이러한 불일치를 피할 수 있고, 따라서 LLM의 수치 계산 능력을 향상시킬 수 있다.
* **수치 계산.** LLM은 특히 사전 훈련 중에 거의 발생하지 않는 기호에 대해 수치 계산의 어려움에 직면합니다. 수학적 도구를 사용하는 것 외에도 숫자를 개별 토큰으로 토큰화하는 것도 LLM의 연산 능력을 향상시키기 위한 효과적인 설계 선택이다.

### _Advanced Ability_

LLM은 위의 기본 평가 과제 외에도 평가를 위한 특별한 고려가 필요한 몇 가지 우수한 능력도 보인다. 이 부분에서 우리는 인간의 정렬, 외부 환경과의 상호 작용, 도구 조작을 포함한 몇 가지 대표적인 고급 능력과 그에 상응하는 평가 접근법에 대해 논의한다. 다음으로 이러한 고급 능력에 대해 자세히 논의합니다.

#### 7.2.1 Human Alignment

LLM이 실제 응용 프로그램에서 LLM을 광범위하게 사용할 수 있는 핵심 능력인 인간의 가치와 요구, 즉 인간 정렬에 잘 부합할 수 있기를 바란다.

이러한 능력을 평가하기 위해, 기존 연구들은 유용성, 정직성, 안전성과 같은 인간 정렬에 대한 여러 기준을 고려한다[368, 46, 170]. 도움 및 정직성을 위해, 적대적 질의 응답 태스크들(_e.g._, TruthfulQA[556])은 텍스트에서 가능한 거짓을 검출하는 LLM의 능력을 조사하는 데 활용될 수 있다[36, 81]. 또한, 무해성은 여러 기존 벤치마크, _예:_, CrowS-Pairs[603] 및 Winogender[604]에 의해서도 평가될 수 있다. 위의 데이터 세트를 사용한 자동 평가에도 불구하고, 인간 평가는 여전히 LLM의 인간 정렬 능력을 효과적으로 테스트하는 보다 직접적인 방법이다. OpenAI는 위험 콘텐츠를 접했을 때 GPT-4의 행동을 평가하고 개선하기 위해 AI 위험과 관련된 도메인의 많은 전문가를 초대한다[46]. 또한, 인간 정렬의 다른 측면들(_e.g._, 진실성)에 대해, 몇몇 연구들은 특정 명령어들을 사용하고 주석 프로세스를 안내하기 위해 주석 규칙들을 고안할 것을 제안한다[81]. 경험적 연구에 따르면 이러한 전략은 LLM[170]의 인간 정렬 능력을 크게 향상시킬 수 있다. 예를 들어, 전문가와의 상호 작용을 통해 수집된 데이터에 대한 정렬 튜닝 후, GPT-4가 민감하거나 허용되지 않는 프롬프트를 다룰 때 GPT-4의 잘못된 동작 속도가 크게 감소될 수 있다. 또한, 고품질의 사전 트레이닝 데이터는 정렬에 필요한 노력을 감소시킬 수 있다[46]. 예를 들어, 은하계는 과학 말뭉치의 덜 편향된 내용 때문에 잠재적으로 더 무해하다[35].

#### 7.2.2 외부 환경과의 상호 작용

LLM은 표준 평가 작업 외에도 외부 환경으로부터 피드백을 받고 행동 지시에 따라 행동을 수행할 수 있는 능력을 가지고 있으며, _예를 들어_ 자연어로 행동 계획을 생성하여 에이전트를 조작할 수 있다[694, 695]. 그러한 능력은 또한 상세하고 매우 현실적인 액션 플랜들을 생성할 수 있는 LLM들에서 출현하는 반면, 더 작은 모델들(_e.g._, GPT-2)은 더 짧거나 무의미한 플랜들을 생성하는 경향이 있다[694].

이 능력을 테스트하기 위해 몇 가지 구현된 AI 환경과 벤치마크를 평가에 사용할 수 있으며 다음과 같이 설명한다. VirtualHome[606]은 청소, 요리 등의 가사 작업을 위한 3D 시뮬레이터를 구축하는데, 에이전트는 LLMs에 의해 생성된 자연어 동작을 실행할 수 있다. ALFRED [608]은 LLM들이 구성 목표들을 달성하도록 요구하는 더 도전적인 작업들을 포함한다. BEHAVIOR [607]은 시뮬레이션 환경에서 일상적인 집안일에 초점을 맞추고 LLM이 객체의 내부 상태를 변경하는 복잡한 솔루션을 생성해야 합니다. 가정 작업과 같은 제한된 환경 외에도, 일련의 연구 작업은 마인크래프트 및 인터넷과 같은 열린 세계 환경을 탐구하기 위한 LLM 기반 에이전트의 숙련도를 조사한다[696, 697]. 보이저[697]는 LLM이 환경의 피드백을 기반으로 새로운 기술을 지속적으로 습득할 수 있도록 하는 자동 커리큘럼 모듈을 소개한다. GITM [696]은 LLM을 기반으로 한 마인크래프트에서 인터페이스의 작업 분해, 계획 및 호출을 통해 다양한 문제를 해결하는 데 중점을 둔다. 생성된 액션 플랜 또는 작업 완료를 기반으로 기존 작업은 벤치마크에서 정규 메트릭(_예:_, 생성된 액션 플랜의 실행 가능성 및 정확성) [694]를 채택하거나 실제 실험을 직접 수행하고 성공률을 측정하여 그러한 능력을 평가한다[698]. LLM들은 외부 환경과 상호작용하고 정확한 액션 플랜들을 생성할 수 있는 것으로 나타났다[699]. 최근에, LLM의 상호 작용 능력을 향상시키고, _예를 들어, 코드-유사 프롬프트들을 설계하고, 실제 접지(698)를 제공하기 위한 몇 가지 개선 방법들이 제안되었다.

또한, 최근의 연구는 시뮬레이션 환경에서 LLM을 기반으로 하는 멀티 에이전트 협업을 탐구한다[700, 701, 533]. 이러한 연구는 모래상자 환경에서 관찰, 계획 및 기억으로 여러 LLM 기반 에이전트를 인스턴스화하여 인간의 사회적 행동을 시뮬레이션한다. 통제된 평가에서 탐색, 계획, 사고에 대한 생성적 에이전트의 능력은 인터뷰와 같은 방식으로 인간에 의해 평가된다. 또한, 그들은 또한 긴급한 사회적 행동을 조사하기 위해 시뮬레이션된 환경 내에서 여러 에이전트에 대한 기술 측정을 수행한다.

#### 7.2.3 도구 조작

복잡한 문제를 해결할 때 LLM은 필요하다고 판단되면 외부 도구로 눈을 돌릴 수 있다. API 호출로 사용 가능한 도구를 캡슐화함으로써 기존 작업에는 여러 특정 작업에 대한 LLM의 성능을 향상시키기 위해 다양한 외부 도구 _예:_, 검색 엔진 [81], 계산기 [80], 컴파일러 [443]가 포함되어 있습니다. 최근 OpenAI는 언어 모델링을 넘어 LLM에 더 넓은 용량을 탑재할 수 있는 ChatGPT [665]의 플러그인 사용을 지원했다. 예를 들어, 웹 브라우저 플러그인은 ChatGPT가 새로운 정보에 액세스할 수 있게 한다. 또한, 타사 플러그인을 통합하는 것은 LLM을 기반으로 하는 애플리케이션의 번영한 생태계를 만드는 데 특히 중요하다.

도구 조작 능력을 검토하기 위해, 기존의 작업은 대부분 수학적 문제 해결(_e.g._, GSM8k[184] 및 SVAMP[592]) 또는 지식 질의 응답(_e.g._, TruthfulQA[556])과 같은 평가를 위한 복잡한 추론 작업을 채택하는데, 여기서 도구의 성공적인 활용은 LLM이 (_e.g._, 수치 계산에서 불가능하다는 요구 기술을 향상시키는데 매우 중요하다. 이러한 방식으로 이러한 작업에 대한 평가된 성능은 도구 조작에서 LLM의 능력을 반영할 수 있다. LLM이 도구를 활용하도록 가르치기 위해 기존 연구에서는 LLM을 이끌어내기 위해 컨텍스트에서 도구를 사용하는 예제를 추가하거나 도구 활용에 대한 시뮬레이션 데이터에서 LLM을 미세 조정한다[693, 80]. 도구의 도움으로 LLM은 자신이 잘하지 못하는 문제, 예를 들어 방정식 계산 및 적시에 질문에 답하는 문제를 더 잘 처리할 수 있는 것으로 나타났다[80, 448]. 그러나 사용 가능한 도구의 수가 증가함에 따라 LLM의 제한된 컨텍스트 길이는 광범위한 도구 API를 설명하고 입증하는 데 어려움을 겪을 수 있다. 이 문제를 해결하기 위해 기존 작업은 관련 도구의 사용, 또는 도구 정보를 임베딩 공간 내의 토큰으로 인코딩한다[702, 703, 704].

인간이 개발한 기존의 도구 외에도 LLMs는 자율적으로 특정 업무에 대한 도구를 스스로 만들 수 있는 능력을 갖추고 있다[705]. 이를 통해 모델은 이러한 자체 제작 도구를 독립적으로 탐색하고 조작할 수 있으므로 광범위한 실제 작업을 해결하는 데 있어 자율 탐색의 잠재력을 확장할 수 있다.

_요약__ 위의 세 가지 능력은 인간의 가치와 선호에 순응(인간 정렬), 현실 세계 시나리오에서 적절하게 행동(외부 환경과의 상호 작용), 능력 범위 확장(도구 조작) 등 LLM의 실제 수행에 큰 가치가 있다. 위의 세 가지 고급 능력 외에도 LLM은 일부 작업(_e.g._, 데이터 주석[486]) 또는 학습 메커니즘(_e.g._, 자기 개선[706])과 특별히 관련된 다른 능력도 보여줄 수 있다. 이러한 새롭게 부상하는 능력을 발견하고 측정하고 평가하여 LLM을 더 잘 활용하고 개선할 수 있는 열린 방향이 될 것이다.

### _Benchmarks and Evaluation Approaches_

위에서 우리는 LLM의 기본 능력과 고급 능력에 대해 논의했다. 다음으로, 우리는 기존의 평가 벤치마크들과 접근법들을 소개할 것이다[733, 734].

#### 7.3.1 종합 평가 벤치마크

최근에 LLM의 평가를 위한 몇 가지 포괄적인 벤치마크[70, 364, 520]가 발표되었다. 이 부분에서는 널리 사용되는 여러 벤치마크, 즉 _i.e._, MMLU, BIG-bench, HELM 및 일련의 인간 시험 벤치마크를 소개한다.

\(\bullet\)_MMLU_[364]는 수학 및 컴퓨터 과학에서 인문사회 과학에 이르기까지 광범위한 지식 영역을 포괄하는 다중 작업 지식 이해의 대규모 평가를 위한 다목적 벤치마크이다. 이러한 업무의 어려움은 기초부터 고급까지 다양하다. 기존 작업에서 볼 수 있듯이 LLM은 대부분 이 벤치마크[56, 57, 56, 69]에서 상당한 차이로 작은 모델을 능가하며, 이는 모델 크기의 스케일링 법칙을 보여준다. 보다 최근에, GPT-4는 MMLU에서 주목할 만한 기록(5-샷 설정에서 86.4%)을 달성하는데, 이는 이전의 최첨단 모델들[46]보다 상당히 우수하다.

\(\bullet\)_BIG-bench_[70]은 다양한 측면에서 기존의 LLM들을 탐사하기 위한 협업 벤치마크이다. 언어학, 아동기 발달, 수학, 상식 추론, 생물학, 물리학, 사회적 편견, 소프트웨어 개발 등을 포함한 광범위한 주제를 포함하는 204개의 과제로 구성된다. 모델 크기를 스케일링함으로써 LLMs은 BIG-bench[56]에서 작업의 65%에 대한 소수의 샷 설정 하에서 평균 인간 성능을 능가할 수 있다. 전체 벤치마크의 높은 평가 비용을 고려하여, 경량 벤치마크 BIG-bench-Lite가 제안되었는데, 이는 BIG-bench로부터 24개의 작지만 다양하고 도전적인 과제를 포함하고 있다. 또한, BIG-bench hard (BBH) 벤치마크 [365]는 LLMs가 인간에 비해 열등한 성능을 나타내는 도전적인 태스크를 선택하여 LLMs의 현재 해결 불가능한 태스크를 조사하는 데 집중하도록 제안되었다. BBH가 더 어려워지기 때문에, 소형 모델들은 대부분 랜덤에 가까운 성능을 달성한다. 비교로서, CoT 프롬프트는 BBH에서 평균 인간 성능을 초과하더라도 성능을 향상시키기 위한 단계적 추론을 수행하는 LLM의 능력을 이끌어낼 수 있다.

\(\bullet\)_HELM_[520]은 현재 16개 시나리오의 핵심 집합과 7개 범주의 메트릭을 구현하는 포괄적인 벤치마크이다. 그것은 언어 모델에 대한 전체론적 평가를 수행하는 많은 선행 연구 위에 구축되어 있다. HELM의 실험 결과에서 보듯이 명령어 튜닝은 정확도, 견고성, 공정성 측면에서 LLM의 성능을 지속적으로 높일 수 있다. 또한, 추론 작업을 위해 코드 코퍼스에 사전 훈련된 LLM이 우수한 성능을 보인다.

\(\bullet\)_인간 수준 테스트 벤치마크_는 AGIEval [708], MMCU [709], M3KE [710], C-Eval [711] 및 Xiezhi [712]와 같이 인간을 테스트하기 위해 설계된 질문으로 LLM들의 포괄적인 능력을 평가하는 것을 목표로 한다. 이러한 벤치마크는 LLM의 일반적인 능력에 대한 포괄적인 평가를 제공하기 위해 광범위한 도메인, 난이도 및 언어를 포함한다. 공개된 모델에 비해 API 서비스(_e.g._, GPT-4, ChatGPT, Claude)를 제공하는 모델은 이러한 평가 벤치마크에서 공개된 모델에 비해 우수한 성능을 보여준다. 평가에서 가장 성능이 좋은 모델로서, GPT-4는 AGIEval의 평균 인간 성능을 능가한다[708]. 그러나 여전히 이러한 도전적인 벤치마크에서 최고의 인간 성능보다 뒤처져 있습니다. 따라서 LLM, 특히 공개적으로 액세스할 수 있는 모델의 전반적인 능력을 추가로 향상시킬 수 있는 충분한 여지가 남아 있다.

위의 벤치마크는 LLM의 평가를 위한 다양한 주류 평가 과제와 실제 인간 시험 문제를 다룬다. 또한 다국어 지식 활용을 위한 TyDiQA[735], 다국어 수학적 추론을 위한 MSSM[524]와 같이 LLM의 특정 능력을 평가하는 데 중점을 둔 벤치마크가 몇 개 있다. 평가를 수행하기 위해 특정 목표에 따라 적합한 벤치마크를 선택할 수 있다. 또한, 연구자들이 기존의 벤치마크 상에서 LLM을 평가하거나, 언어 모델 평가 하니스[736] 및 OpenAI 평가[46]와 같은 맞춤형 평가를 위한 새로운 작업을 확장하기 위한 몇 가지 오픈 소스 평가 프레임워크도 존재한다. 또한, 일부 연구자들은 또한 오픈 LLM 리더보드와 같은 기존 LLM들의 성능을 비교하기 위해, 대표적인 벤치마크들을 집계함으로써 지속적으로 업데이트된 리더보드들을 구성한다[707]. 위의 벤치마크 및 리더보드는 LLM의 기본 및 고급 능력을 입증하는 중요한 참조를 제공한다. 우리는 7.3.2절에서 평가 접근법에 대한 찬반 논의를 좀 더 깊게 할 것이다.

#### 7.3.2 평가 접근법

기존 벤치마크를 도입한 후, 이 부분에서 LLM의 성능을 평가하기 위한 기존 평가 접근법을 검토할 것이다. 논의를 정리하기 위해 LLM을 세 가지 유형으로 분류합니다. _기본 LLM_(미리 훈련된 모델 체크포인트), _미세 조정 LLM_(명령 또는 정렬 미세 조정 모델 체크포인트) 및 _특수 LLM_(특정 작업 또는 도메인에 대한 적응 모델 체크포인트)입니다. 여기서는 LLM의 다른 목적을 구별하기 위해 미세 조정된 LLM과 특수 LLM을 모두 유지한다: 일반 또는 특정 작업 해결사. 세 가지 유형의 LLM을 평가하기 위해 섹션 7.1 및 7.2에서 논의된 바와 같이 다른 능력(_예:_, 기본 또는 고급 능력)과 관련된 LLM 성능을 테스트할 수 있다. 일반적으로 LLM을 평가하기 위한 세 가지 주요 접근법, 즉 벤치마크 기반 접근법[364], 인간 기반 접근법[727], 모델 기반 접근법[729]이 있다. 표 15는 LLM 유형, 평가 접근법 및 테스트된 능력 간의 관계를 보여주는 그림이다. 다음으로, 다양한 유형의 LLM에 대한 평가 접근법에 대해 논의할 것이다.

**기본 LLM 평가** 기본 LLM은 사전 훈련 직후에 얻은 모델 검사점을 참조 합니다. 기본 LLM의 경우, 우리는 주로 복잡한 추론 및 지식 활용과 같은 기본 능력(섹션 7.1)을 조사하는 데 중점을 둔다. 이러한 기본 능력의 대부분은 잘 정의된 작업으로 평가할 수 있기 때문에 벤치마크 기반 접근법이 기본 LLM을 평가하는 데 널리 사용되었다. 다음으로 기본 LLMs에 대한 공통 평가 벤치마크와 평가 절차를 소개한다.

\(\bullet\)_일반적인 벤치마크._ 기본 LLM을 평가하기 위해 일반적인 벤치마크는 객관식 질문과 같은 근접 문제 형태로 설계된다. 이러한 일반적으로 사용되는 벤치마크는 크게 지식지향 벤치마크와 추론지향 벤치마크의 두 가지로 구분할 수 있다. 지식 지향 벤치마크(_e.g._, MMLU[364] 및 C-Eval[711])는 세계 지식의 능력을 평가하는 것을 목표로 하는 반면, 추론 지향 벤치마크(_e.g._, GSM8K[643], BBH[365] 및 MATH[364])는 복잡한 추론 작업을 해결하는 능력을 평가하는 데 중점을 둔다. 또한 최근에 제안된 일부 벤치마크(_e.g._, OpenCompass[713])는 포괄적인 비교를 위해 이 두 가지 유형을 결합한다.

\(\bullet\)_Benchmark 기반 평가 절차._ 벤치마크 평가를 수행하기 위해, 각각의 문제는 먼저 LLM이 결과 텍스트를 생성하기 위한 프롬프트로 포맷될 것이다. 그런 다음 생성된 결과 텍스트를 사람이 작성한 규칙으로 구문 분석하여 예측된 답변을 얻는다. 최종적으로, LLM의 성능은 예측된 답변과 지상-진실 답변을 비교함으로써 정확도와 같은 표준 메트릭을 사용하여 자동으로 계산될 수 있다. 평가 접근법은 소수의 샷 또는 제로 샷 설정에서 수행될 수 있으며, 이는 상이한 평가 결과 또는 순위를 초래할 수 있다. 베이스 LLM은 (상대적으로 약한 태스크 일반화 능력으로) 지시 미세 조정되지 않았기 때문에, 종종 소수의 샷 설정이 평가에 더 적합하다. 일부 복잡한 추론 태스크의 경우, CoT 프롬프트는 또한 평가 동안 용량을 완전히 나타내기 위해 사용될 필요가 있다. 또 다른 메모는 이 평가 접근법이 미세 조정된 LLM의 능력을 평가하는 데에도 적용될 수 있다는 것이다. 실제로 여러 리더보드(_예._, Open LLM 리더보드[707])가 기본 및 미세 조정된 LLM을 모두 평가하는 이 접근법에 기반한다.

**Fine-tuned LLMs의 평가** 이 부분의 Fine-tuned LLMs는 미리 훈련된 모델 가중치 45에 기초하여 명령어 튜닝 또는 정렬 튜닝 후에 획득된 모델 체크포인트를 참조한다. 일반적으로 Fine-tuned LLMs는 테스트될 것이다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Method** & **Evaluation** & **Model Types** & **Abilities/Domain** & **Data Source** \\ \hline \hline  & MMLLU [364] & Base/Fine-tuned/Specialized & General & Human exam/practice \\  & BIG-bench [70] & Base/Fine-tuned/Specialized & General & Human annotation \\  & HELM [520] & Base/Fine-tuned/Specialized & General & Benchmark collection \\  & Drem LM Leaderboard [707] & Base/Fine-tuned/Specialized & General & Benchmark collection \\  & AGLEval [708] & Base/Fine-tuned/Specialized & General & Human exam/practice \\  & MMCU [709] & Base/Fine-tuned/Specialized & General & Human exam/practice \\  & MXK [710] & Base/Fine-tuned/Specialized & General & Human exam/practice \\  & C-Eval [711] & Base/Fine-tuned/Specialized & General & Human exam/practice \\  & Xiezhi [712] & Base/Fine-tuned/Specialized & General & Human exam/practice \\  & OpenCompass [713] & Base/Fine-tuned/Specialized & General & Benchmark collection \\  & Chain-of-Thought Hub [714] & Base/Fine-tuned & General & Benchmark collection \\  & KoLA [715] & Base/Fine-tuned & Knowledge utilization & Web \\  & ARB [716] & Fine-tuned & Complex reasoning & Human exam/practice \\ Benchmark & APIBench [717] & Base/Fine-tuned & Tool manipulation & Web \\  & APIBank [718] & Fine-tuned & Tool manipulation & Synthesis \\  & ToolApaca [719] & Base/Fine-tuned & Tool manipulation & Synthesis \\  & T-Bench [720] & Fine-tuned & Tool manipulation & Synthesis \\  & ToolBench [721] & Fine-tuned & Tool manipulation & Synthesis \\  & BOLAA [722] & Base/Fine-tuned & Environment interaction & Benchmark collection \\  & AgentBench [723] & Base/Fine-tuned & Environment interaction & Human annotation/Synthesis \\  & Haluk’s [602] & Base/Fine-tuned & Human alignment & Human annotation/Synthesis \\  & PromptBench [724] & Base/Fine-tuned & Robustness & Benchmark collection \\  & HumanEval [105] & Base/Fine-tuned/Specialized & Code synthesis & Human annotation \\  & MultiduceQA [356] & Specialized & Healthcare & Benchmark collection \\  & FLUE [725] & Specialized & Finance & Benchmark collection \\  & LegalBench [726] & Specialized & Legal & Human annotation \\ \hline \hline  & Chatbot Arena [727] & Base/Fine-tuned/Specialized & Human Alignment & Human annotation \\  & SciBench [728] & Fine-tuned & Complex reasoning & Human exam/practice \\ \hline \hline  & AlpacaEval [729] & Fine-tuned & Instruction following & Synthesis \\  & MT-bench [727] & Fine-tuned & Human alignment & Human annotation \\ Model & TrustCPT [720] & Base/Fine-tuned & Human alignment & Benchmark collection \\  & LMExamQA [731] & Base/Fine-tuned & Knowledge utilization & Synthesis \\  & ChatEval [732] & Base/Fine-tuned & Knowledge utilization & Benchmark collection \\ \hline \hline \end{tabular}
\end{table} TABLE XC: A category of existing evaluation work. “General” denotes that the evaluation focuses on an overall performance of multiple abilities. The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in Section 7.1 and 7.2.

다양한 능력(예: 지식 활용 및 인간 정렬)에 따라 여러 평가 접근법으로 평가되는 것이 일반적이다. 벤치마크 기반 평가 외에도 인간 기반 및 모델 기반 접근법이 미세 조정 LLM의 고급 능력을 평가하는 데 널리 사용되었다. 다음으로 두 가지 평가 방법에 대해 소개하고자 한다.

\(\bullet\)_인간 기반 평가_. 기본 능력에 대한 자동 평가와 달리 인간 평가는 일반적으로 인간 정렬 및 도구 조작과 같은 실제 사용에서 더 많은 요소 또는 능력을 고려한다. 이러한 평가 접근법에서 시험 과제는 보통 개방형 질문의 형태이며, 인간 평가자를 초청하여 LLMs에 의해 생성된 답변의 질에 대한 판단을 하게 된다. 일반적으로, 인간 평가자에 대한 채점 방법에는 쌍별 비교와 단일 응답 채점의 두 가지 주요 유형이 있다. 쌍별 비교에서 동일한 질문을 감안할 때 인간은 다른 모델에서 두 가지 답변을 할당하여 어느 것이 더 나은지 결정하는 반면 단일 응답 등급에서는 한 번에 단일 답변만 채점하면 된다. 예를 들어, HELM[520]은 요약 및 허위 정보 작업에 대해 단일 응답 등급화를 수행하기 위해 인간을 고용하는 반면, 챗봇 아레나[727]는 사용자가 두 개의 익명 채팅 LLM과의 대화에 참여하고 쌍별 비교 결과를 보고할 수 있도록 하는 크라우드소싱 플랫폼을 구축한다.

\(\bullet\)_모델 기반 평가_. 인간 기반 평가는 비용이 많이 들고 시간이 많이 소요되기 때문에, 일부 연구는 ChatGPT 및 GPT-4와 같은 강력한 폐쇄 소스 LLM을 인간 평가자의 대리인으로 활용하는 것을 제안했다[727, 729]. 예를 들어, AlpacaEval[729]는 명령어들의 세트를 수집하고, 기준 출력들에 대한 쌍별 비교를 수행하기 위해 판단자로서 가능한 LLM(_예를 들어,_GPT-4)을 활용한다. 또한, MT-bench[727]는 평가를 위한 멀티턴 질문 세트를 수집하고 ICL 및 CoT와 같은 방법을 통해 LLM 기반 평가자의 신뢰성을 향상시킨다. 인간 평가자와 비교하여 ChatGPT 및 GPT-4와 같은 LLM은 소규모 수공예 및 대규모 크라우드소싱 평가 과제 모두에서 인간과 높은 일치를 달성할 수 있다. 그럼에도 불구하고 이러한 폐쇄 소스 LLM은 액세스가 제한적이며 데이터 누출의 잠재적 위험이 있다. 이를 해결하기 위해, 최근 작업[727]은 인간 평가자들로부터의 스코어링 데이터를 사용하여 모델 평가자로서 오픈 소스 LLM들(_예를 들어,_Vicuna[138])을 미세 조정함으로써 강력한 폐쇄 소스 LLM들(_예를 들어,_GPT-4)과의 격차를 좁혔다.

**전문화된 LLM 평가** 전문화된 LLM은 의료[356] 및 금융[737]과 같은 일부 도메인 또는 애플리케이션에 특별히 적응된 모델 체크포인트를 나타냅니다. 특수 과제 해결사로서, 전문화된 LLM은 일반적인 능력(예를 들어, 복잡한 추론과 같은 기본 능력 및 인간 정렬과 같은 고급 능력)뿐만 아니라 지정된 도메인 또는 애플리케이션과 관련된 특정 능력에 대해 테스트될 것이다. 이를 위해 대상 도메인 또는 애플리케이션에 맞게 조정된 특정 벤치마크를 구성해야 하는 경우가 많습니다. 그런 다음 이러한 도메인별 벤치마크를 일반 벤치마크와 결합하여 전문화된 LLM에 대한 포괄적 및 표적적 평가를 모두 수행할 수 있다. 예를 들어, MultiMedQA [356]은 건강 검진 및 건강 관리 질문을 포함하는 건강 관리의 특정 벤치마크이다. 이 작업[356]에서 MultiMedQA는 MMLU[364]와 결합되어 Med-PaLM[356]과 같은 의료용 특수 LLM의 성능을 평가했다. 마찬가지로 FLUE[737]은 금융 감정 분석에서 질문 답변에 이르기까지 금융에 대한 벤치마크를 구축한다. BBBH [365]와 협력하여 BloombergGPT [360]과 같은 임상 LLM을 평가했다.

**다른 평가 접근법의 장단점** 이상에서 우리는 LLM의 능력을 평가하기 위한 다양한 평가 접근법에 대해 논의했다. 다음으로, 각 평가 접근법의 장단점을 간단히 분석한다.

\(\bullet\)_Benchmark 기반 접근법_. 이 평가 접근법은 LLM의 성능을 평가하기 위해 기존 벤치마크를 활용할 수 있다. 이러한 벤치마크에 관련된 작업은 종종 핵심 능력(예를 들어, 추론)을 측정하기에 충분한 테스트 샘플을 포함한다. 전체 평가 절차는 (거의) 자동적일 수 있으며, 다양한 기본 LLM에 대한 테스트 실험을 수행하는 것이 편리하며, 특히 사전 훈련 동안 모델 체크포인트의 성능을 모니터링하는 데 유용하다. 그러나 LLM은 질문 프롬프트, 제로 샷 또는 소수 샷 테스트, 답변 구문 분석 방법 등 평가 설정에 민감한 경우가 많다. 따라서 평가 실험을 수행할 때 가능한 영향 요인을 고려해야 한다. 평가 결과는 채택된 평가 설정과 함께 유의해야 한다. 또 다른 문제는 데이터 오염[56, 738], 즉 테스트 데이터 자체 또는 관련 콘텐츠가 사전 훈련 코퍼라에 포함되어 있다는 것이다. 이러한 현상은 LLM 개발을 위해 점점 더 많은 오픈 데이터가 수집되었기 때문에 점점 더 심각해졌다.

\(\bullet\)_인간 기반 접근법_. 인간 평가는 실제 작업을 해결하기 위한 LLM의 기능을 평가할 때 몇 가지 이점을 제공한다. 주요 이점 중 하나는 LLM의 실제 능력을 직접 반영하는 능력이다. 인간 평가는 실제 사용자의 피드백과 경험을 기반으로 실제 시나리오에서 LLM의 성능을 보다 직접적으로 측정할 수 있다. 또한 인간 평가자를 기반으로 보다 유연하고 다양한 평가 작업을 수행할 수 있다. 예를 들어, 사용자는 다양한 쿼리를 제출하고 자신의 작업 인지에 따라 LLM의 능력을 테스트할 수 있다. 이는 다양한 유형의 작업과 컨텍스트에 걸쳐 LLM의 장단점을 깊이 이해할 수 있다. 그러나 인간 평가는 정확성과 일관성에 잠재적으로 영향을 미칠 수 있는 고유한 한계를 가지고 있다. 개인화된 취향과 평가자 간의 다양한 교육 수준과 같은 요소는 평가 과정에서 편향되거나 불일치를 도입할 수 있다. 경우에 따라 사용자의 판단이 주관적일 가능성이 있으며, 이는 LLM의 진정한 능력을 반영하지 못할 수 있다. 더욱이, 견고하고 신뢰할 수 있는 인간 평가를 수행하는 것은 종종 많은 수의 평가자를 필요로 하며, 이는 매우 비싸고 시간이 많이 소요될 수 있다. 또한, 인간 평가는 재현할 수 없는 경우가 많아 기존의 평가 결과를 확장하거나 LLM의 진행 상황을 추적하는 것이 불가능하다.

\(\bullet\)_모델 기반 접근법_입니다. 모델 기반 접근법은 인간 기반 접근법의 대리인으로서 인간 개입에 대한 의존도를 줄이고 보다 효율적이고 확장 가능한 평가를 가능하게 하는 역할을 한다. 또한, LLMs은 할당된 평가 점수에 대한 의미 있는 설명을 제공하여 평가의 해석 가능성을 높일 수 있다. 확장성과 설명력에도 불구하고 모델 기반 접근법은 위치, 장황성 및 자기 향상 편향과 같은 몇 가지 문제를 겪는 것으로 나타났다[727]. 특히, 위치 편향(즉, 응답 제시 순서)은 LLM이 다른 응답보다 특정 위치에서 응답에 대해 높은 점수를 부여하는 경향이 있다는 사실을 의미하며, 장황성 편향은 LLM이 짧은 응답에 비해 품질이 부족하더라도 장황한 답변을 선호한다는 것을 의미하며, 자기 향상 편향은 LLM이 종종 자신의 세대에서 과대평가된다는 것을 나타낸다. 또한, LLM은 복잡한 추론 문제를 해결하는 데 제한된 능력을 가지고 있기 때문에, 일부 어려운 과제(예를 들어, 수학적 추론)에 대해 자격을 갖춘 평가자 역할을 할 수 없다. 이러한 한계는 특정 신속한 엔지니어링 및 미세 조정 전략에 의해 어느 정도 완화될 수 있다[727].

요약하면, LLM 평가에 대한 기존 작업의 범주화(표 X)는 주로 평가 방법론과 모델 유형의 두 가지 주요 차원에 기초하며, 이는 테스트 능력과 함께 추가로 확장된다. LLM 평가를 위한 기존 작업의 범주화 또는 분류법에 대해서도 논의한 최근 작업[733, 734]이 있다.

### _Empirical Evaluation_

위의 평가 벤치마크와 접근 방식은 주로 LLM의 전반적인 능력을 평가하는 데 사용된다. 이 부분에서는 7.1절과 7.2절에서 논의된 능력에 대한 세밀한 평가를 수행한다. 각 능력에 대해 LLM의 해당 성능을 조사하기 위해 평가 실험을 수행하기 위한 대표적인 작업과 데이터 세트를 선택한다.

#### 7.4.1 실험 설정

이 부분에서는 우리의 평가를 위한 실험 설정을 소개한다.

**평가 모델** 평가를 수행 하기 위해 다음과 같이 오픈 소스 모델에서 폐쇄 소스 API 액세스 모델에 대 한 대표적인 LLM을 고려 합니다.

\(\bullet\)_오픈 소스 모델_입니다. 기존의 오픈소스 모델은 베이스 모델과 명령어 튜닝 모델로 분류할 수 있다. 베이스 모델은 언어 모델링 목적을 가진 대규모 범용 코퍼스에서 사전 훈련될 뿐, 더 이상의 감독된 미세 조정은 없다. 평가에서는 LLaMA (7B) [57], LLaMA 2 (7B) [99], Pythia (7B and 12B) [96], Falcon (7B) [747]46의 네 가지 대표적인 기본 모델을 선택한다. 명령 조정 모델은 명령(_즉,_작업 데이터 세트, 일일 채팅 또는 합성 명령)을 사용하여 미세 조정된다. 실험에서는 Vicuna (7B 및 13B) [138], Alpaca (7B) [137], ChatGLM (6B) [93] 등 4개의 대표적인 명령어 조정 모델을 선택하였다. 또한 비교를 위해 LLaMA 2-Chat (7B) [99]를 포함하였으며, LLaMA 2 (7B)를 기반으로 명령어 튜닝 및 RLHF를 통해 인간과 정렬된 대표적인 모델이다.

각주 46: 더 큰 모델들을 갖는 실험들은 계산 자원들의 한계로 인해 여전히 스케줄에 있다.

* _닫힌 원본 모델입니다._ 오픈 소스 모델 외에도 API를 통해서만 접근할 수 있는 폐쇄 소스 모델도 있어 개발자와 연구자 모두에게 많은 관심을 받고 있다. 여기에서 우리는 텍스트-다빈치-002/003(짧은 _다빈치002/003_), ChatGPT, Claude 및 Claude 2를 포함한 4개의 대표적인 폐쇄 소스 모델을 선택하며, 여기서 처음 3개의 모델은 OpenAI에 의해 개발되고 나머지 2개는 Anthropic에 의해 개발된다.

**작업 및 데이터 세트** 다음으로 섹션 7.1 및 섹션 7.2에서 논의한 능력에 대한 평가 작업 및 데이터 세트를 설정합니다. 이러한 데이터 세트에 대한 LLM의 제로샷 성능을 주로 평가합니다. 제로샷 방식(예: 수학적 추론 및 도구 조작)으로 해결하기 어려운 보다 복잡한 작업을 위해 오픈 소스 모델의 컨텍스트 길이 제한을 고려하여 주로 3-샷 성능을 보고한다.

\(\bullet\)_언어 생성_. 앞서 논의한 바와 같이 언어 생성을 위해 언어 모델링, 조건부 텍스트 생성 및 코드 합성의 세 가지 태스크를 평가하는 것을 고려한다. 특히, 일반적으로 사용되는 LAMBADA[233](언어 모델링), WMT'22[545](기계 번역), XSum[549](텍스트 요약), HumanEval[105](코드 합성)의 네 가지 데이터 세트를 선별한다. WMT'22에서는 기계 번역에서 LLM의 평균 성능을 조사하기 위해 원래 대규모 테스트 세트에서 각 언어 쌍에 대해 1000개의 예를 선택하여 새로운 평가 세트를 구성한다. 이러한 데이터셋에 대한 LLM의 제로샷 성능을 평가하고, LAMBADA, WMT'22의 경우 _BLEU-4_, XSum의 경우 _ROUGE-L_, HumanEval의 경우 _pass_\(@10\) 예측 단어의 정확도를 계산한다.

\(\bullet\)_Knowledge utilization_. 지식 활용 능력을 평가하기 위해 4개의 질의 응답 데이터 세트(_i.e.,_ TriviaQA [558], Natural Questions [554], Web Questions [557], ARC [555])와 사실 추출 데이터 세트인 WikiFact [571)를 선택한다. 또한 이러한 데이터 세트에 대한 LLM의 제로 샷 성능을 보고하고 ARC에 대한 _정확도_ 및 다른 데이터 세트에 대한 _정확한 일치_를 계산합니다.

\(\bullet\)_Complex reasoning_. 복잡한 추론의 경우 OpenbookQA[566], HelaSwag[582], SocialIQA[581], 기호추론의 경우 표 [70]의 Colored Objects[70]와 Penguins, 수학적 추론의 경우 GSM8k[184]와 MATH[364]의 비교 모델을 평가한다. OpenbookQA, HellaSwag 및 SocialIQA에 대한 _정확도_를 계산하며, 표의 유색 개체 및 펭귄에 대한 _해결 속도_ 및 GSM8k 및 MATH에 대한 _정확도_를 계산합니다. 지식 추론 작업은 모두 제로 샷 설정에서 풀 수 있는 QA 작업이기 때문에 제로 샷 성능을 평가한다. 복잡한 기호 추론과 수학적 추론 과제를 위해 3-shot in-context 예제를 활용하여 LLMs을 더 잘 이끌어내어 이를 달성한다. 기존의 작업[443, 33]에 이어서, 우리는 또한 수학적 추론 과제를 더 잘 해결하기 위해 사고 연쇄 프롬프트 전략을 활용한다.

\(\bullet\)_인간 정렬_. 인간 정렬을 위해 TruthfulQA [556]을 선택하여 LLM이 질문에 대한 답변을 생성하는 데 진실성 여부를 측정하고, CrowS-Pairs [603] 및 WinoSender [604]를 선택하여 LLM에서 고정관념을 평가하고, RealIoxicityPromps [605]를 선택하여 LLM이 독성 언어를 생성하는 정도를 평가하고, HaluEval [602]를 선택하여 LLM이 환각을 인식하는 능력을 테스트한다. 실제 독성 프롬프트의 테스트 세트가 너무 크기 때문에 평가를 위해 10000개의 예를 무작위로 샘플링한다. 우리는 LLaMA [57]을 따라 제로샷 성능을 보고하고, TruthfulQA에 대해 참으로 클레임을 식별하는 _정확도_, CrowS-Pairs에 대해 편향된 문장(높은 복잡도)을 인식하는 _정확도_, WinoGender에 대해 _상관 분해 정확도(the/she/they)_, RealToxicityPrompt에 대해 _독성 점수_ 및 HaluEval에 대해 환각을 인식하는 _평균 정확도_를 계산한다. TruthfulQA의 경우 텍스트 다빈치-003을 사용하여 채점을 위해 사람을 대체하는 기존 작업 [57]을 따른다. CrowS-Pairs와 WinoGender의 경우, 우리는 복잡도와 상호참조 해상도 점수를 계산하기 위해 LLaMA [57]의 실험 설정을 따른다. RealToxicityPrompts의 경우 독성 평가를 위해 Perspective-API47을 활용한다.

각주 47: [https://perspectiveapi.com/](https://perspectiveapi.com/)

\(\bullet\)_환경과의 상호 작용_. 이 능력을 테스트하기 위해 가정 및 전자 상거래 환경과 같은 실제 시나리오를 시뮬레이션하는 평가를 위해 ALFWorld[609] 및 WebShop[610]을 선택한다. WebShop 및 ALFWorld에서 LLM의 1-샷 및 2-샷 성능을 각각 평가하는 ReAct [449]의 설정을 따르며, com

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multicolumn{5}{c}{**Language Generation**} & \multicolumn{5}{c}{**Knowledge Utilization**} \\ \cline{2-10}  & LBD\(\uparrow\) & WMT\(\uparrow\) & XSum\(\uparrow\) & HumanEval\(\uparrow\) & TriviaQA\(\uparrow\) & NaturalQ\(\uparrow\) & WebQ\(\uparrow\) & ARC\(\uparrow\) & WikiFact\(\uparrow\) \\ \hline ChatGPT & 55.81 & **36.44** & 21.71 & 27.88 & **54.54** & 21.52 & 17.77 & 93.69 & 29.25 \\ Claude & **64.47** & 31.23 & 18.63 & 51.22 & 40.92 & 13.77 & 14.57 & 66.62 & **34.34** \\ Claude 2 & 45.20 & 12.93 & 19.13 & 27.04 & **54.30** & 21.30 & **21.06** & 79.97 & **35.83** \\ Davinci003 & **69.98** & **37.46** & 18.19 & 67.07 & 51.51 & 17.76 & 16.68 & 88.47 & 28.29 \\ Davinci002 & 58.85 & 35.11 & **19.15** & 56.70 & 52.11 & 20.47 & **18.45** & **89.23** & 29.15 \\ \hline LLaMA 2-Chat (7B) & 56.12 & 12.62 & 16.00 & 11.59 & 38.93 & **12.96** & 11.32 & **72.35** & 23.37 \\ Vicuna (13B) & 62.45 & 20.49 & **17.87** & 20.73 & 29.04 & 10.75 & **11.52** & 20.69 & **28.76** \\ Vicuna (7B) & 63.90 & 19.95 & 13.59 & 12.07 & 28.58 & 9.17 & 6.64 & 16.96 & 26.95 \\ Alpaca (7B) & 63.35 & 21.52 & 8.74 & 13.41 & 17.14 & 3.24 & 3.00 & 49.75 & 26.05 \\ ChatGLM (6B) & 33.34 & 16.58 & 13.48 & 13.42 & 13.42 & 4.40 & 9.20 & 55.39 & 16.01 \\ \hline LLaMA 2 (7B) & **66.39** & 11.57 & 11.57 & 17.07 & 30.92 & 5.15 & 2.51 & 24.16 & **28.06** \\ LLaMA (7B) & **67.68** & 13.84 & 8.77 & 15.24 & **34.62** & 7.92 & **11.12** & 4.88 & 19.78 \\ Falcon (7B) & 66.89 & 4.05 & 10.00 & 10.37 & 28.74 & **10.78** & 8.46 & 4.08 & 23.91 \\ Pythia (12B) & 61.19 & 5.43 & 8.87 & 14.63 & 15.73 & 1.99 & 4.72 & 11.66 & 20.57 \\ Pythia (7B) & 56.96 & 3.68 & 8.23 & 9.15 & 10.16 & 1.77 & 3.74 & 11.03 & 15.75 \\ \hline \hline \end{tabular}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multicolumn{5}{c}{**Knowledge Reasoning**} & \multicolumn{5}{c}{**Symbolic Reasoning**} & \multicolumn{5}{c}{**Mathematical Reasoning**} & \multicolumn{5}{c}{**Interaction with Environment**} \\ \cline{2-10}  & ORQ\(\uparrow\) & HellaSwag\(\uparrow\) & SocialQAT\(\uparrow\) & C-Objects\(\uparrow\) & Penguins\(\uparrow\) & GSM\(\uparrow\) & MATH\(\uparrow\) & ALFW\(\uparrow\) & WebShop\(\uparrow\) \\ \hline ChatGPT & 81.20 & **61.43** & **73.23** & 53.20 & 40.27 & **78.47** & **33.28** & 58.96 & 45.12 /15.60 \\ Claude & 81.80 & 54.95 & **73.23** & 59.95 & 47.65 & 70.81 & 20.18 & 76.87 & **47.27 /22.00** \\ Claude 2 & 71.60 & 50.75 & 58.34 & **86.76** & **74.50** & **82.87** & **32.24** & **72.81** & 34.96 /19.20 \\ Davinci003 & 74.40 & **62.65** & 69.70 & **64.60** & 61.07 & 57.16 & 17.66 & 65.67 & **64.08 /82.00** \\ Davinci002 & _69.80_ & 47.81 & 57.01 & 62.55 & **67.11** & 49.96 & 14.28 & 76.87 & 29.66 /15.20 \\ \hline LLaMA 2-Chat (7B) & **45.62** & **74.01** & 43.84 & 43.40 & 38.93 & 9.63 & 2.22 & **11.19** & **24.51**/5.60 \\ Vicuna (13B) & 43.65 & 70.51 & 45.97 & 53.55 & 36.91 & 18.50 & 3.72 & 8.96 & 22.74 /2.50 \\ Vicuna (7B) & 43.84 & 69.25 & 46.27 & **44.25** & 36.24 & **14.03** & 3.54 & 1.49 & 6.90 /1.40 \\ Alpaca (7B) & **47.82** & 69.81 & **47.58** & 39.35 & **40.27** & 4.93 & **4.16** & 4.48 & 0.00 /0.00 \\ ChatGLM (6B) & 30.42 & 29.27 & 33.18 & 14.05 & 14.09 & 3.41 & 1.10 & 0.00 & 0.00 /0.00 \\ \hline LLaMA 2 (7B) & 44.81 & **74.25** & 41.72

[MISSING_PAGE_FAIL:71]

ALFWorld의 경우 _성공률_ 을, WebShop의 경우 _평균 점수/성공률_ 을 계산합니다. 또한 입력 프롬프트의 길이를 줄이고 줄 바꿈을 EOS 토큰으로 활용하기 위해 ReAct [449]를 따른다.

\(\bullet\)_Tool manipulation._ 도구 조작을 위해 검색 엔진과 모델 인터페이스를 포함한 두 종류의 도구를 고려한다. 따라서 두 가지 도구 조작 벤치마크인 _즉_ HotpotQA[579]와 고릴라[617]를 채택한다. HotpotQA는 LLM이 검색 엔진을 사용하여 웹에서 문서를 검색하고, 고릴라가 TorchHub, TensorHub 및 HuggingFace의 세 허브에서 모델 API를 호출하도록 요구한다. HotpotQA에 대한 _정확한 일치_ 및 고릴라에 대한 _정확도_를 계산합니다. HotpotQA의 경우 ReAct [449]를 따라 3샷 성능을 보고합니다. 고릴라의 경우 논문 [617]에서 발표한 코드를 따르고 제로샷 성능을 평가한다.

**구현 세부 정보** 각 작업 및 데이터 세트에 대해 기존 작업에서 제공 하거나 경험적 경험에 따라 설계 된 동일한 프롬프트 및 결과 구문 분석 방법 (_i.e.,_ TruthfulQA, HotPotQA, Gorilla, HaluEval)을 사용 하 여 비교 된 LLM을 평가 합니다 (_i.e.,_ TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxicityPrompt). 특히, 폐쇄 소스 모델에 대한 모든 실험은 공식 API를 호출하는 것을 기반으로 하는 반면, 오픈 소스 모델의 경우 공개적으로 사용 가능한 코드와 모델 매개변수를 활용하고 8개의 A800-80G GPU에 대한 추론을 수행한다. 트리비아QA, OpenbookQA, HellaSwag, SocialIQA의 경우 테스트 세트가 공개되지 않았기 때문에 개발 세트에 대해 실험한다. 다른 데이터 세트의 경우 테스트 세트에 대해 실험합니다. 실험을 재현하기 위해 실험 코드와 데이터를 [https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments](https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments)에서 공개적으로 공개합니다.

#### 7.4.2 결과 분석 및 결과

실험 결과를 Table X에 보고하고, 그 결과를 다음과 같이 분석한다.

**Closed-Source 모델 분석** 4개의 폐쇄 소스 모델(_i.e.,_ ChatGPT, Claude, Davinci003 및 Davinci002)에 대한 분석 및 결과를 다음과 같이 요약합니다.

\(\bullet\)_이 5개의 폐쇄 소스 모델은 ChatGPT가 대부분 최상의 성능을 발휘하는 범용 작업 해결기로서 유망한 결과를 달성한다. ChatGPT, Claude, Claude 2, Davinci003 및 Davinci002는 복잡한 태스크들(예를 들어,_GSM8k)을 포함하는 대부분의 태스크들에서 잘 수행되며, 이는 범용 태스크 해결자가 될 수 있는 큰 잠재력을 보여주었다. 이 중 ChatGPT는 평가 과제에서 보다 우수한 모델 역량을 발휘하여 모든 과제에서 가장 많이 수상하였다. 일부 평가 작업에서 ChatGPT와 다른 폐쇄 소스 모델 간의 성능 차이는 매우 크며, 특히 복잡한 작업 _예:_ 78.47(ChatGPT)_vs._ 49.96(Davinci002) on GSM8k, 79.88(ChatGPT) _vs._ 51.22 (Claude) on HumanEval

\(\bullet\)_Claude 2, ChatGPT 및 Davinci003은 환경 및 도구 조작 작업과의 상호 작용에서 더 나은 성능을 보인다._ 두 가지 평가 과제인 Claude 2, ChatGPT 및 Davinci003은 다른 모델보다 큰 마진(예:_36.40(Claude 2)_vs_)에서 더 나은 성능을 보인다. 26.00 (Davinci002) on HotpotQA, 44.53 (ChatGPT) _vs._ 7.74(Claude) on Gorilla-TF, 72.58(Davinci003) _vs._ 22.04 (Claude) on Gorilla-TH 가능한 이유는 이 세 가지 모델이 외부 플러그인 사용을 지원하는 것과 같은 이러한 고급 기능에 특별히 최적화되었기 때문이다.

\(\bullet\)_모든 비교 모델들은 매우 어려운 추론 작업들에서 잘 수행되지 않는다._ MATH 및 HotpotQA에서 모든 모델(ChatGPT 포함)은 성능이 좋지 않습니다. 두 과제는 풀이가 매우 어려워 복잡한 수학적 지식에 대한 정확한 이해와 문서 간 멀티홉 추론을 각각 수행해야 한다. 또한, 이러한 모델들은 또한 기계 번역 작업(WMT)에 대해 상대적으로 약한 성능을 갖는다. 가능한 이유는 WMT가 소수 언어의 평가 예도 많이 포함하고 있기 때문에 이러한 LLM의 사전 훈련 데이터에서 잘 다루지 않을 수 있다.

**오픈 소스 모델 분석** 다음으로 8개의 오픈 소스 모델(_i.e.,_ LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM, LLaMA 2, LLaMA, Pythia 및 Falcon)에 대한 분석 및 결과를 다음과 같이 계속 보여줍니다.

\(\bullet\)_명령 조정 모델은 대부분 기본 모델보다 성능이 우수합니다._ 비교된 모든 오픈 소스 방법 중 명령어 조정 모델(_i.e.,_ LLaMA 2-Chat, Vicuna, Alpaca and ChatGLM)은 명령어 조정 모델(_i.e.,_ LLaMA 2, LLaMA, Pythia and Falcon)보다 대부분 더 나은 성능을 보인다. 이는 명령어 튜닝이 일반적으로 다양한 태스크를 해결하는 데 있어 LLM의 소수 샷 또는 제로 샷 능력을 향상시킬 수 있음을 나타낸다. 그러나 명령어 튜닝 후 Vicuna (7B)와 Alpaca (7B)는 언어 모델링 작업인 LAMBADA에서 성능 저하를 겪는다. 그 이유는 명령 데이터가 주로 LLM이 인간의 명령을 따를 수 있게 하는 것에 초점을 맞추고 있기 때문일 수 있으며, 이는 일반적인 언어 생성 작업에 항상 유용한 것은 아니다.

\(\bullet\)_이러한 소규모 오픈 소스 모델은 수학적 추론, 환경과의 상호 작용 및 도구 조작 작업에서 잘 수행되지 않는다._ 수학적 추론, 환경과의 상호 작용 및 도구 조작의 작업에서 평가된 모든 오픈 소스 모델은 명령어 조정 모델을 포함하여 잘 수행되지 않는다. 가능한 이유는 이러한 모델을 미세 조정하기 위한 명령 데이터가 이러한 작업에 대해 특별히 설계되지 않았기 때문이다. 또한, 이러한 폐쇄 소스 모델은 작은 모델 크기로 인해 제한된 모델 용량을 가질 수 있다.

\(\bullet\)_최상위 성능 모델은 인간 정렬 작업에 따라 다릅니다._ 서로 다른 인간 정렬 작업에 대해, 우리는 이러한 모델들이 일관성 없는 성능 순위를 달성한다는 것을 알 수 있다. 예를 들어, LLaMA 2-Chat(7B)는 TruthfulQA에서 비교된 오픈 소스 모델 중 가장 잘 수행하는 반면, Vicuna(13B)는 CrowS-Pair에서 가장 잘 수행한다. 가능한 이유는 이들 태스크들이 인간 정렬의 상이한 양태들을 평가하기 위한 특정 목적들로 설계되고, 이들 모델들은 동일한 모델의 변형들(_예를 들어,_피티아(7B) 및 피티아(12B))에 대해서도 상이한 태스크들에 대해 다양한 성능을 나타내기 때문이다. 더 자세한 결과를 밝히기 위해서는 인간 정렬 평가에 대한 더 많은 실험과 분석이 필요하다.

최근에 발표된 모델인 LLaMA 2 (7B)는 특히 복잡한 추론 작업에서 좋은 성능을 보인다._ 복잡한 추론 작업의 경우 LLaMA 2(7B)는 대부분 다른 기본 모델보다 더 나은 성능을 발휘합니다. _예:_ 43.95(LLaMA 2(7B))_vs_ 29.80 (Falcon (7B)) in C-Objects. 다른 작업(_e.g._, 언어 생성 및 지식 활용)의 경우, LLaMA 2(7B)는 또한 가장 성능이 좋은 기본 모델로서 비교 가능한 성능을 달성할 수 있다. 주로 우수한 성과에 기여하는 사전 훈련(_i.e._, 약 2조 토큰)에 더 많은 데이터를 활용했다. 또한 보다 강력한 데이터 클리닝 프로세스를 수행합니다.

각주 48: 우리는 모든 관련 연구 방향이나 영역을 다루는 것을 목표로 하지 않고 대신 이러한 선택된 예를 통해 LLM의 사용이나 영향을 입증하는 것을 목표로 한다.

\(\bullet\)_오픈 소스 모드의 크기 조정은 성능을 일관되게 향상시킬 수 있습니다._ 비쿠나(7B)와 비쿠나(13B), 피티아(7B)와 피티아(13B)의 성능을 비교한 결과, 규모가 큰 모델이 작은 모델보다 대부분 더 나은 성능을 보여 모델 크기를 확장하는 효과가 있음을 알 수 있다. 다양한 태스크들에 걸쳐, 스케일링 모델은 더 복잡한 태스크들(예컨대,_e.g._, 심볼릭 및 수학적 추론)에 더 유익하며, 여기서 더 큰 모델들은 큰 여백에서 더 작은 모델들을 대부분 능가한다.

독자들은 오픈 소스 언어 모델에 대한 이러한 발견은 모델 크기에 제한된다는 점에 유의해야 한다. 우리는 이러한 모델의 더 큰 버전의 결과를 포함하여 이 부분을 지속적으로 업데이트하고 더 많은 실험을 위한 계산 자원의 지원을 요청할 것이다.

## 8 Applications

이 섹션에서는 LLM의 적용에 대한 최근 진행 상황을 두 가지 측면, 즉 연구 커뮤니티와 대표 도메인에 미치는 영향에 대해 간략하게 검토한다. 그림 18은 이 섹션49의 콘텐츠 구성을 보여준다.

각주 49: 우리는 모든 관련 연구 방향이나 영역을 다루는 것을 목표로 하지 않고 대신 이러한 선택된 예를 통해 LLM의 사용이나 영향을 입증하는 것을 목표로 한다.

### _LLM for Research Community_

LLM이 AI 알고리즘을 개발하는 방식에 혁명을 일으켰기 때문에 연구 커뮤니티에 상당한 영향을 미친다. 이 부분에서는 대표적인 몇 가지 연구 방향에 대해 LLM이 주도한 발전을 간략하게 검토한다.

#### 8.1.1 LLM for Classic NLP Tasks

사전 훈련된 언어 모델(_e.g._, BERT)이 NLP 분야에서 시작됨에 따라 언어 모델의 기술적 발전은 NLP 연구에 중요한 영향을 미친다. 이 부분에서는 기존의 많은 NLP 시스템과 응용의 기초가 된 단어 수준, 문장 수준, 서열 태깅, 관계 추출, 텍스트 생성 작업 등 5가지 고전적 NLP 작업에 LLM의 적용에 대해 논의한다. 우리는 모든 NLP 작업을 포괄적으로 다루려는 것이 아니라 기본 작업을 통해 기본 NLP 연구를 위한 LLM의 영향을 분석하려고 한다. 우리는 또한 이 조사에서 초기에 논의된 여러 작업(예: 언어 모델링)에 대한 논의를 생략했다.

**단어/문장 수준 작업** 오랜 NLP 작업으로서 단어 수준(_예:_, 단어 군집링[748] 및 의미 명확화[749]) 및 문장 수준 작업(문장 일치[750] 및 감정 분류[751])은 문헌에서 널리 연구되었으며 실제 플랫폼에서 적용되었습니다. 이러한 과제를 해결하기 위해서는 단어 또는 문장에 대한 의미 정보를 정확하게 이해하는 것이 관건이다. 지금까지 이러한 작업에 대한 풍부한 고품질 라벨링된 데이터가 축적됨에 따라 기존 작업[23, 39]은 작은 언어 모델이 미세 조정함으로써 매우 우수한 성능을 달성할 수 있음을 발견했다. 최근의 연구들 [752, 55]은 또한 이러한 태스크들에 대한 LLM들의 성능을 테스트했는데, 이는 LLM들이 인-컨텍스트 학습을 통해서도 잘 수행될 수 있다는 것을 보여준다(매우 적은 예들이 있음). 반면에 작은 모델은 특정 작업 요구 사항과 도메인 지식을 학습하기 위해 이러한 작업에 특별히 최적화될 수 있으므로 전체 데이터 미세 조정 작은 모델은 여러 고전 작업 [753, 754], _예:_, 시맨틱 매칭 및 감성 분석에 대한 인컨텍스트 학습을 사용하여 LLM을 대부분 능가할 수 있다.

**서열 태그 지정** 서열 태그 지정 작업 _예._, 명명된 엔터티 인식(NER) [755] 및 품사(POS) 태그 지정 [756]도 기본 작업입니다. 일반적으로 이러한 작업은 입력 시퀀스의 각 토큰에 적절한 의미 범주 레이블, _e.g._, NER 작업에 대한 클래식 B-I-O(_Beginning, Inside_ 및 _Outside_) 태그 지정 체계를 할당해야 합니다. 딥러닝 시대에 초기 노력[757, 758]은 주로 학습된 시퀀스 표현(_e.g._, CNN, LSTM 및 BERT를 사용하여)을 구조적 예측에 기반한 태깅 작업을 수행하는 고전적 조건부 랜덤 필드 모델(CRF)에 통합한다. 최근 연구자들은 서열 태깅 태스크에서 LLM의 성능을 테스트했지만, 특히 모호하거나 희귀한 이름을 가진 특수 범주, _e.g._, "MISC"(_기타 개체_) 및 "ORG"(_조직_) 클래스에 대해 인컨텍스트 학습[753]을 사용하여 LLM을 해결하는 데 여전히 어려움을 겪고 있음을 관찰했다. 가능한 이유는 LLMs가 인간 주석이 있는 데이터 세트에서 이러한 클래스의 의미를 오해할 수 있기 때문에 명령어와 제한된 예에 따라 해당 클래스의 의미를 정확하게 이해하기가 어렵기 때문이다.

**정보 추출** 정보 추출 작업은 관계 추출 [759] 및 이벤트 추출 [760]과 같은 비정형 텍스트 데이터에서 유용한 정형 정보를 자동으로 추출하는 데 중점을 둡니다. 일반적으로 이전 연구에서는 이 작업을 텍스트 분류 작업 또는 순차적 레이블링 작업으로 공식화한다. 정보 추출이 종종 복잡한 의미 관계(한 문장 내의 다중 관계)를 정확하게 이해하고 처리할 필요가 있기 때문에, LLMs를 사용한 인-컨텍스트 학습은 전형적으로 최첨단 풀-데이터 미세 조정 방법을 저수행한다[761, 762]. 반면, LLM과 소형 모델 간의 협업을 가능하게 하는 것은 특정 태스크의 성능을 더욱 향상시킬 수 있음을 보여준다[763, 762]. 또한, 최근의 연구[425]에서도 LLMs이 2단계 워크플로우를 사용하여 정보 추출을 위한 경쟁적인 제로 샷 성능을 달성할 수 있음을 보여주며, 이러한 접근 방식은 향후 응용 분야에서 매력적이다.

**텍스트 생성.** 텍스트 생성 작업 _예:_, 기계 번역 [624] 및 자동 요약 [548]은 널리 연구된 오랜 NLP 작업이며 미세 조정된 작은 모델을 기반으로 하는 많은 배포된 제품 및 시스템이 있습니다. LLM의 사전 훈련은 텍스트 예측에 대해 확립되기 때문에, 적절한 프롬프트의 도움으로 상용 제품[627] 및 인간[628]으로서 강력한 언어 생성 능력을 나타낸다[765, 766]. 또한 LLM은 실제 응용 프로그램 시나리오, _예:_, 문서 수준 변환에서 특수 요구 사항을 효과적으로 처리 하도록 유연 하 고 사용자와의 자연 언어 상호 작용을 사용 하 여 생성 품질을 더욱 향상 시킬 수 있습니다. 위의 성공에도 불구하고 최근 연구에서는 LLM이 다른 언어에 걸쳐 불균형한 훈련 데이터로 인해 자원이 적은 언어 및 도메인, 예를 들어 마라티-영문 번역 [769]에 대한 생성 작업을 잘 해결하지 못한다는 것을 보여준다.

**요약**. 이상의 논의를 바탕으로 고전적 NLP 작업에서 LLMs 사용에 대한 제안과 향후 방향을 다음과 같이 요약한다.

* _제안_: LLM 및 소형 모델은 다양한 측면에서 고유한 장점을 가지고 있습니다. LLM은 다양한 NLP 작업에 대한 통일된 솔루션을 제공하고 경쟁 성능을 달성할 수 있는 반면(특히 제로/페우 샷 설정에서), 소형 모델은 개발하기에 경제적이고 목표 작업에 따라 특별히 조정될 수 있으며, 이는 충분한 고품질 라벨링된 데이터로 좋은 성능을 달성할 수 있습니다[770, 771, 754, 770, 771]. 응용 프로그램에서는 유연성, 데이터 가용성, 훈련 컴퓨팅 및 효율성을 종합적으로 고려하여 실제 필요에 따라 적절한 선택을 할 수 있습니다.
* _미래 방향:_ 뛰어난 일반 용량에도 불구하고 LLM은 리소스가 적은 도메인, 예를 들어 마이너 언어 변환에서 NLP 작업을 효과적으로 처리할 수 없습니다. 이러한 작업을 해결하기 위해서는 미세 조정 또는 프롬프트를 통해 필요한 작업 정보 또는 도메인별 지식을 LLMs에 주입하는 효과적인 접근 방식을 개발해야 한다. 또한, LLM이 고전적인 NLP 태스크(_e.g._, 중첩 엔티티 추출)에서 복잡한 의미 관계를 처리하는 것은 여전히 어려우며, 이는 LLM의 기본 작업 메커니즘으로부터 더 많은 탐색의 가치가 있다. 또한 고전적인 NLP 작업의 복잡한 경우를 해결하는 데 있어 상호 보완을 위해 LLM과 미세 조정된 작은 언어 모델을 결합하는 것이 유망하다[772]. 또 다른 유망한 방향은 LLM이 인간의 명령을 효과적으로 이해하고 의미 있는 응답을 할 수 있기 때문에 NLP 작업에 대한 인간-기계 협력 연구(예를 들어, 대화 번역[768])를 수행하는 것이다.

#### 8.1.2 LLM for Information Retrieval

정보 검색 시스템의 목표는 사용자가 이상적인 정보 자원(일반적으로 문서)을 발견하고 정보 과부하 문제를 완화할 수 있도록 지원하는 것이다. 전형적으로, 현대 IR 시스템들은 검색-then-rerank 파이프라인 프레임워크를 채택한다[54]. 이 프레임워크 내에서, 리트리버는 초기에 대규모 코퍼스로부터 관련 정보를 검색하고, 리랭커는 후속적으로 다단계 랭킹 절차를 수행하여 가장 관련성이 높은 정보를 획득한다[773]. LLM의 출현은 정보 접근 방식에 상당한 영향을 미치기 때문에 우리는 IR 모델로서의 LLM과 LLM 강화 IR 모델의 두 가지 주요 측면에서 IR의 발전을 어떻게 발전시키는지에 대해 논의한다.

**LLMs as IR Models.** 기존 IR 모델은 _sparse 모델_ (용어 기반 어휘 유사성에 의존함) 및 _dense 모델_ (기반 의미 유사성 임베딩에 의존함)로 전체적으로 분류될 수 있습니다. [740]. 특히, 밀집 모델은 주로 미세 조정된 PLM(_e.g._, BERT)에 의해 구현된다. PLM에 비해 LLM은 텍스트 의미론을 캡처하는 데 더 강력한 모델 용량을 가지므로 기존 조밀한 IR 모델을 개선할 가능성이 있다. 그러나 LLM의 높은 오버헤드로 인해 대부분의 연구는 검색된 후보의 순위를 정제하는 것을 목표로 LLM을 재순위자로 사용하는 데 중점을 둔다. 이를 달성하기 위해, 최근의 노력들은 종종 LLM들이 제공된 후보 문서들의 작은 세트에 대해 재순위화를 수행할 수 있게 하는 특별한 명령들을 공식화한다. 전형적으로, 그러한 접근법은 모델 트레이닝을 필요로 하지 않으며, 잘 트레이닝된 재순위화 방법들과 비교하여 유망한 결과들을 달성한다[774, 775]. 특히, LLM 기반 재순위화 접근법은 점 방향(_쿼리-문서 쌍들에 대한 관련성 스코어 추정_) [776], 쌍 방향(_관련성 순서 결정_)을 포함하는 제로-샷 또는 소수-샷 명령에 의해 상이한 방식으로 구현될 수 있다.

도. 18: 대표적인 연구 방향 및 다운스트림 도메인에서의 LLM의 적용.

의 두 문서_) [775], 또는 리스트별 랭킹(_후보 문서의 서브세트를 정렬_) [777]. 이러한 방법의 본질은 문서 목록에 대한 슬라이딩 윈도우 전략[774, 778], 세밀한 관련성 레이블 통합[779], 쌍별 비교 프롬프트[775]와 같은 텍스트 재순위화를 위한 명령어의 특별한 설계에 있다. 또한, 최근의 노력들은 LLMs을 사용하여 적은 샷 데모를 사용하여 검색 결과로서 중간 텍스트들(_e.g._, URL들)을 생성한다[781]. 모델 성능을 더욱 향상시키기 위해 LLMs는 전통적인 PLM 기반 IR 모델[782]에 대한 미세 조정 프로세스와 유사하게 재순위화[782, 783] 또는 검색(밀집 검색[54] 및 모델 기반 검색[784, 785] 포함)을 위한 백본으로서 특별히 미세 조정될 수 있다. 그러나 IR 모델로서 LLM을 미세 조정하는 것은 LLM의 거대한 매개변수 규모를 고려할 때 상당한 비용을 수반한다.

**LLM 강화 IR 모델** 다른 주요 연구 방향으로 LLM을 사용하여 기존 IR 모델(예: 작은 모델)을 개선할 수 있습니다. 기존 IR 모델이 직면한 일반적인 과제는 관련 판단 주석의 부족이다[786, 787]. 이 문제를 해결하기 위해, LLMs는 주어진 쿼리에 대해 긍정 또는 부정 문서에 주석을 달도록 지시받을 수 있다[788], 또는 몇 가지 데모를 참조하여 코퍼스 내의 문서 세트에 기초하여 대응하는 쿼리를 생성하도록 지시받을 수 있다[789, 790]. 학습 데이터 증강 외에도 LLM은 쿼리와 문서 모두의 검색 지향 정보성을 개선함으로써 기존 IR 모델을 개선할 수 있는 잠재력을 가지고 있다. IR 시스템에서 입력 쿼리는 사용자의 인지적 및 문화적 역량에 의해 제한될 수 있어 실제 의도를 정확하게 표현하기가 어렵고 문서에 존재하는 관련 없는 내용도 쿼리와의 관련성 평가에 영향을 미칠 수 있다. 해결책으로 LLM은 잘 설계된 명령어를 통해 질의 의도에 대한 이해를 높이고 추가 지식을 질의에 통합하기 위해 질의를 재작성하는 데 활용될 수 있다. 재작성된 쿼리는 원본 쿼리의 개선된 버전[791], 쿼리와 관련된 코퍼스 내의 문서[792], 또는 의사 생성된 문서와 연결된 쿼리의 확장[793]의 형태를 취할 수 있다. 또한, 문서들은 또한 컨텍스트 확장을 위해 LLMs를 사용하여 원본 문서들에 기초하여 생성되는 쿼리들로 확장될 수 있다[794].

**나머지 문제** 이 부분에서는 IR 시스템을 개선하기 위해 LLM을 적용하는 몇 가지 중요한 문제에 대해 추가로 논의합니다. 첫째, LLM들이 범용 태스크 해결자로서 가능할 수 있지만, 그것들은 기존의 IR 시스템들에 직접적으로 잘 맞지 않는다: 그들은 추론을 위한 높은 오버헤드를 필요로 하고[774, 782], 긴 텍스트들 또는 문서 목록들을 모델링하는데 한계가 있고[778], 그리고 텍스트 랭킹 태스크를 수행하기 위해 특별한 적응(_e.g._, 명령어 튜닝)이 필요하다[795]. 따라서 현대 IR 시스템에 LLM을 적용하기 위한 보다 체계적인 접근법이 조사되어야 하며, 이점을 활용하고 이러한 한계를 극복해야 한다. 둘째, LLMs의 출현은 새로운 정보 탐색 방법(_e.g._, New Bing)의 개발을 조명한다. LLM의 용량과 기존 IR 시스템의 장점을 통합하여 IR의 아키텍처와 패러다임을 재구성하는 방법을 탐구하는 것은 의미가 있다[796]. 셋째, 기존의 연구는 주로 텍스트 검색에 초점을 맞추고 있으며, 멀티모달 정보원에 대한 종합적인 고려가 부족하다. 8.1.4절에서 논의될 바와 같이, 멀티모달 대형 언어 모델[797]도 널리 연구되어, 보다 강력한 멀티미디어 검색 시스템을 개발하는 것이 가능하다.

#### 8.1.3 LLM for Recommender Systems

사용자 검색 쿼리를 분석하여 관련 문서를 검색하는 IR 시스템과 달리, 추천 시스템(RS)은 기본 사용자 선호도를 포착하고 사용자에게 적절한 정보 자원을 제공하는 것을 목표로 한다[798, 799, 800, 801]. 전형적으로, 기존 연구들은 추천 모델(클래식 또는 딥 러닝 모델 중 하나)을 사용자의 로그된 데이터(_e.g._, 클릭 데이터) 위에 피팅함으로써 트레이닝한다[745, 802]. 그러나 이러한 모델은 종종 _예:_, 콜드 스타트 권장 사항, 도메인 이전 및 설명력 저하와 같은 일련의 기술적 문제로 인해 어려움을 겪습니다. 최근 LLMs은 도메인 일반화 및 언어 생성의 강력한 능력으로 인해 추천 모델[803, 357, 804]의 이러한 문제를 완화할 수 있는 잠재력을 입증했다. 이 부분에서는 추천 모델로서의 LLMs, LLM 강화 추천 모델 및 추천 시뮬레이터로서의 LLMs의 세 가지 측면에서 추천 시스템에서 LLM의 최근 진행 상황을 간략하게 검토한다.

**추천 모델로서의 LLM.** 특정 메서드 또는 메커니즘을 사용 하 여 LLM을 추천 모델 역할을 하도록 조정할 수 있습니다. 이 노선에 따른 기존 작업은 일반적으로 크게 두 가지로 나눌 수 있다. 먼저, 일부 방법들은 제로-샷 패러다임(_i.e._, 파라미터 튜닝 없음)에서 추천 태스크를 완료하기 위한 LLM들을 프롬프트한다[805, 806]. 최근 집중 및 상황 학습과 같은 일련의 신속한 엔지니어링 방법은 추천 성능을 개선하고 잠재적인 모델 편향을 완화하기 위해 도입된다[807, 808]. 둘째, 또 다른 범주의 연구는 명령어 튜닝을 통해 개인화된 추천을 위한 LLM을 전문화하는 것을 목표로 한다[357, 809]. 특히, 높은 품질의 명령어 데이터는 휴리스틱 템플릿과의 사용자-아이템 상호 작용을 기반으로 구성될 수 있는 추천 태스크에 LLM을 적응시키는 핵심이다. 명령어 다양성을 더욱 향상시키기 위해, InstructRec[357]은 제품 검색 및 개인화된 추천과 같은 다양한 시나리오에서 많은 양의 잠재적인 사용자 명령을 시뮬레이션하기 위해 자체 명령 기법을 채용한다. 각 항목을 텍스트 설명에 의해 표현하는 것 외에도, 협력적 의미론을 LLM에 통합하기 위해 추천 시스템 [810, 811]에서 의미 식별자로 LLM의 어휘를 확장하는 것에 대한 관심이 증가하고 있다.

**LLM 강화 추천 모델** LLM이 권장 사항을 직접 제공하도록 지시하는 것 외에도 연구자는 LLM에 인코딩된 보편적인 지식을 활용하여 전통적인 추천 시스템을 개선하는 것도 제안한다. 이 라인의 기존 접근 방식은 크게 세 가지로 나눌 수 있다. 첫 번째 범주는 LLM을 사용하여 사용자의 과거 상호 작용 데이터에서 사용자의 잠재적 의도를 추론한다. 또한, 전통적인 추천/검색 모델은 관련 항목의 검색을 개선하기 위해 추론된 의도를 사용한다[812, 813]. 또한 여러 연구에서 LLM을 특징 인코더로 사용하는 방법을 탐구한다. 이들은 LLMs을 사용하여 아이템 및 사용자의 부가 정보(_e.g._, 아이템의 설명 및 사용자의 리뷰)를 인코딩하여 사용자 및 아이템의 보다 유익한 표현을 유도한다. 그런 다음, 이러한 표현들은 증강 입력으로서 전통적인 추천 시스템들로 공급된다[814, 815]. 다른 대안적 접근법으로서, 몇몇 연구들[816, 817]은 전통적인 추천자들(_i.e._, 소형 모델들)을 개선하기 위해 LLM들의 용량들(_e.g._, 시맨틱 인코딩)을 전달하는 증류-유사 방식을 채택한다. 특히, LLM과 전통적인 추천 모델의 숨겨진 상태를 합동 훈련을 통해 정렬한다. 교육 후에는 향상된 소형 모델만 온라인으로 배포되기 때문에 온라인 서비스에서 LLM의 막대한 오버헤드를 피할 수 있다.

**추천 시뮬레이터로서의 LLM.** 최근 자율 AI 에이전트의 성공에서 영감을 받아 [818] LLM은 추천 시뮬레이터 [820, 819](recAgent [819]에 의해 예시됨)를 개발하는 데에도 활용되어 추천 시스템에서 사용자 실제 행동을 시뮬레이션할 수 있는 큰 잠재력을 보여준다[821, 822, 819]. 구체적으로, 개인화된 시뮬레이션을 만들기 위해, 에이전트는 관련 신원 정보를 포괄하는 프로파일링 모듈을 구비할 것이다. 그런 다음 에이전트의 과거 상호 작용 경험을 저장하기 위해 메모리 모듈이 도입된다. 시뮬레이션의 프로세스 동안, 에이전트들은 그들의 근본적인 사용자 선호도를 포착하기 위해, 그들의 과거 경험들에 기초하여 자기 반성을 수행하도록 추가로 프롬프트된다. 기존의 추천 시뮬레이터는 상호작용 과정에서 항목을 명시적으로 모델링하지 않고 사용자 중심의 방식으로 수행되는 경우가 대부분이다. 이를 해결하기 위해, AgentCF [821]은 사용자와 아이템 모두를 에이전트로 모델링하고, 사용자와 아이템 사이의 양면 관계를 캡처하기 위해 사용자-아이템 상호작용을 시뮬레이션하기 위한 협력적 반사를 더 용이하게 한다.

**나머지 문제** 이러한 노력에도 불구하고 추천 시스템에서 LLM을 적용할 때 해결해야 할 몇 가지 문제가 여전히 있습니다. 먼저, 기존 연구들은 제로/퓨샷 설정에서의 LLM 기반 추천 모델들이 전통적인 ID 기반 추천기들보다 더 나쁜 성능을 발휘하는 경향이 있다는 것을 보여주었다[806, 807]. 이는 LLM이 개인화된 사용자 행동 및 도메인별 협력 의미론에 대한 이해가 부족할 수 있음을 나타낸다. 명령어 튜닝은 이 문제를 어느 정도 완화시키지만 [809, 357] LLM과 추천 시스템 사이의 의미적 격차를 완전히 줄일 수 없으며 높은 튜닝 비용도 부담한다. 또한, 추천 시스템은 메모리 오버헤드뿐만 아니라 LLM의 추론 속도에도 문제가 되는 낮은 자원 환경(예: 전화)에서 사용자의 경험을 향상시키기 위해 추론 지연 시간을 최소화하는 것을 우선시한다. 따라서, 실제 추천 시스템에서 LLMs을 효율적이고 효과적으로 배치하기 위해 효율적인 튜닝 및 양자화 방법과 같은 개선 기술을 탐색하는 것이 중요하다. 또한, 기존의 LLM들은 긴 컨텍스트 모델링에서 제한된 용량으로 인해 방대한 양의 사용자-아이템 상호작용 데이터를 처리하는데 어려움이 있다. 긴 상호작용 시퀀스에서 LLM의 모델링 능력을 향상시키기 위해 개선된 컨텍스트 길이 확장 및 컨텍스트 정보 활용 접근법이 개발되어야 한다.

#### 8.1.4 멀티모달 대형 언어 모델

기존 문헌[823, 824]에서 멀티모달 모델들은 주로 입력으로부터 다양한 모달리티들(_e.g._, 텍스트, 이미지, 및 오디오)의 정보를 처리 및 통합하고, 특정 모달리티들에서 대응하는 출력을 더 생성할 수 있는 모델들을 지칭한다. 이 부분에서는 주로 비텍스트 양식, 특히 _멀티모달 대형 언어 모델(MLLMs)_[797]49라는 비전 양식 정보의 모델링을 가능하게 함으로써 LLM의 멀티모달 확장에 중점을 둔다. 논의를 시작하기 위해 입력은 텍스트 이미지 쌍으로, 출력은 텍스트 응답으로 지정한다. 다른 양식, _예:_ 언어 오디오 모델[825]에 대해서도 유사한 논의가 이루어질 수 있으며, 이는 여기에서 우리의 범위를 벗어난다. 본질적으로, MLLM은 세계 텍스트를 기반으로 학습되는 LLM의 우수한 모델 용량을 활용하기 위해 다른 모달리티의 정보를 텍스트 모달리티에 적용하여 개발된다. 전형적으로, MLLM은 비전 및 언어 표현을 정렬하는 연결 모듈에 의해 연관된, 이미지 인코딩을 위한 이미지 인코더 및 텍스트 생성을 위한 LLM을 포함한다. 생성 동안, 이미지는 먼저 패치들로 분할되고, 이어서 이미지 인코더 및 연결 모듈에 의해 패치 임베딩들로 변환되어, LLM에 의해 이해될 수 있는 시각적 표현을 유도한다. 이어서, 패치 임베딩들 및 텍스트 임베딩들이 연접되고, MLLM에 피드백되어, 언어 모델이 응답을 자동으로 생성할 수 있게 한다. 이하에서는 능력 있는 MLLM을 개발하기 위한 훈련, 평가 및 핵심 사항에 대해 논의할 것이다.

각주 49: 기존 작업에서, 대형 비전 언어 모델들(LVLMs) [662]은 또한 LLMs에 기초하여 개발된 그러한 바이모달 모델들을 지칭하기 위해 사용된다. 우리는 기존 문헌에서 널리 사용되기 때문에 이 부분에서 MLLM의 이름을 사용한다.

**훈련 프로세스** MLLM의 훈련 프로세스에는 비전 언어 정렬 사전 훈련 및 시각적 명령 튜닝의 두 가지 주요 단계가 포함됩니다.

* _비전 언어 정렬 사전 훈련_ MLLM을 개발하기 위해, 기존의 작업은 대부분 비전 인코더와 LLM을 사전 훈련된 모델들로 초기화한다[149, 826, 150]. 이 모델은 뛰어난 비전과 언어 능력을 보유하지만 서로 다른 의미 공간에 걸쳐 있습니다. 따라서, 비전-언어 정렬 사전 트레이닝(_i.e._, 첫 단계 트레이닝)의 목표는 대규모 이미지-텍스트 쌍들에 대한 종단간 트레이닝을 통해 비전 인코더와 LLM을 정렬하는 것이다[827, 828]. 그러나 이미지-텍스트 쌍에서 이 두 모델을 직접 튜닝하면 원래 표현 능력이 저하될 수 있다. 정렬 성능을 향상시키기 위해서는 효과적인 훈련 전략을 설계하고 적절한 사전 훈련 데이터를 선택하는 것이 중요하다[829, 830]. 기존의 작업은 주로 교차 모달리티 정렬을 위해 다음과 같은 전략을 사용한다: (1) 이미지-텍스트 쌍들의 수가 충분히 크지 않으면(_e.g._, 1M 미만), 종종 연결 모듈만 업데이트하도록 제안된다[831]; (2) 트레이닝 데이터가 고품질 텍스트 코퍼스[832] 또는 미세-결정된 주석들을 갖는 이미지-텍스트 쌍들을 포함하는 경우[833], LLM을 미세-조정하여 성능을 높일 수 있다; (3) 이미지-텍스트 쌍들의 수가 매우 큰 경우(_e.g._, 약 1B), 비전 인코더를 미세-조정하는 것도 그럴듯하지만 [829, 830] 이점은 추가적인 검증으로 남아 있다.
* _시각적 명령어 튜닝_ 비전 언어 사전 훈련 후 2단계 훈련인 _i.e._, 시각적 수업 튜닝은 MLLM의 수업 후속 및 과제 해결 능력을 향상시키는 것을 목표로 한다. 일반적으로 비주얼 명령어 튜닝의 입력은 이미지와 태스크 디스크립션으로 구성되며, 태스크는 해당 텍스트 출력을 생성하는 것이다. 성능 향상을 위해 고품질 시각적 명령 데이터는 MLLM의 능력을 이끌어내고 향상시키는 데 핵심이다. 따라서 대부분의 연구는 다양한 시각적 명령어 데이터셋을 구축하는 데 전념하고 있다. 기본 접근법으로 초기 연구에서는 GPT-4 [149]에서 증류하거나 비전 언어 작업 데이터 세트를 재구성하여 시각적 지침을 구성한다[151]. 인스트럭션 데이터의 품질을 향상시키기 위해, 최근의 연구는 명령어 다이버시티를 증가시키고[834], 세밀한 정보(_e.g._, 객체의 좌표)를 명령어에 통합하거나[833], 또는 복잡한 시각적 추론 명령어들을 합성함으로써 개선된 전략들을 추가로 제안한다[835].

**MLLM 평가** MLLM 개발에 대한 접근 방식을 도입한 후 다음 세 가지 측면에서 MLLM의 다중 모드 기능을 효과적으로 평가하는 방법에 대해 추가로 논의합니다.

\(\bullet\)_평가 관점._ MLLM에 대한 평가 작업은 크게 두 가지 유형, 즉 _지각_ 과제와 _인지_ 과제로 분류할 수 있다. 구체적으로, _지각_ 과제는 이미지 콘텐츠의 기본 의미론을 이해하는 모델의 능력을 평가하는 것을 목표로 하는 반면, _인지_ 과제는 지각 결과를 기반으로 추론이 필요한 보다 복잡한 작업을 가진 모델을 평가한다. 인식 능력은 일반적으로 이미지(_e.g._, 토픽 및 스타일) 및 객체(_e.g._, 존재 및 색상) 또는 OCR 관련 작업의 속성에 대한 분류 작업을 통해 평가되며, 이는 인간 또는 LLMs에 의한 주석이 있는 기존 이미지로부터 도출된 기존 데이터 세트 또는 새로운 데이터 세트를 기반으로 한다[836, 837, 838, 839]. 눈에 띄는 인식 문제는 환각[840]이며, 여기서 모델의 응답은 이미지와 일치하지 않는 내용을 포함한다. MLLMs[841, 834, 842]의 환각에 대한 기존 연구 중 대상 환각에 대한 [843]은 많은 연구 관심을 받았다. 객체 환각에 대한 안정적이고 강력한 평가를 수행하기 위해 POPE[844]는 객체 인식을 일련의 이진 질문으로 변환하는 폴링 기반 객체 프로빙 접근법을 제안하며, 그 결과는 현재 MLLM이 종종 객체 환각에 어려움을 겪는다는 것을 나타낸다. 반면에 인지 과제는 영상 지각에 기반한 추론을 수행하기 위해 MLLM이 필요하다. 일반적인 추론 과제는 시각적 질문 응답(VQA)이며, 여기서 모델들은 공간 관계들에 대한 추론을 요구하는 이미지들에 대한 질문들에 답한다[845], 일반적인 지식[846], 또는 장면 텍스트[847]. MLLM의 기능을 완전히 탐색하기 위해 HallusionBench [848]은 200개의 정교한 시각적 종속 또는 보충 질문을 수집하며, 이 질문에서 LLaVA-1.5 [831] 및 GPT-4V [133]과 같은 가장 진보된 MLLM조차도 좋은 성능을 달성하지 못합니다.

\(\bullet\)_평가 패러다임._ MLLM의 응답은 폐쇄형 또는 개방형 방식으로 평가할 수 있다. 전통적인 다중 모드 작업은 종종 폐쇄형 평가 프레임워크에 의존하며, 여기서 평가는 모델의 응답과 지상-진실 답변 사이의 정확한 일치를 기반으로 한다. 예들은 시각적 질의 응답 태스크들에 대한 VQA 점수[849] 및 캡션링 태스크들에 대한 CIDEr[850] 점수를 포함한다. 그러나 MLLM은 개방형 방식으로 응답을 생성하며, 이는 정답을 포함할 수 있지만 그라운드-진실과 완벽하게 일치하지 않는다. 이러한 불일치는 이전 평가 패러다임에서 모델의 성능을 과소평가할 수 있다. 이 문제를 해결하기 위해, 최근의 접근법들은 인간 또는 LLMs을 평가자로 통합하였다[829]. 예를 들어, MMBench [838]은 ChatGPT를 사용하여 모델 응답을 객관식 질문 세트에서 가장 관련성이 높은 옵션과 정렬합니다. 유사하게, LLaVA [851]은 MLLM들의 출력을 평가하기 위해 GPT-4를 활용하는데, 여기서 GPT-4는 평가를 위한 시각적 입력들로서 생성된 이미지 캡션들 및 객체 바운딩 박스들을 취한다. 이러한 개방형 평가 방법은 인간 또는 LLM의 개입으로 인해 더 높은 비용을 발생시키면서 평가 정확도를 향상시킬 수 있다.

\(\bullet\)_평가 벤치마크._ MLLM의 보다 철저한 평가를 용이하게 하기 위해 다양한 벤치마크가 개발되었다. 그 중 일부는 종합적인 평가를 위해 기존의 비전 언어 과제를 수집한다. 예를 들어 LVLM-eHub [852]는 47개의 기존 텍스트 관련 시각적 작업을 집계하여 MLLM의 6가지 고유한 기능을 평가하고 Reform-Eval [853]은 기존 벤치마크의 질문을 균일한 형식으로 표준화하고 백본 모델이 MLLM의 성능에 어떻게 영향을 미치는지 논의함으로써 한 단계 더 나아간다. 기존 작업을 통합하는 것 외에도 여러 작업은 인간이 주석을 달거나 LLM의 도움으로 새로운 질문을 도출한다. MME [839]는 인식 및 인지 평가를 위해 수동으로 수집된 텍스트 명령어와 공개 소스의 이미지를 페어링하여 데이터 세트를 생성한다. MMBench[838]은 이러한 명령어를 객관식 문항으로 변형하고 CircularEval을 도입하여 평가의 일관성을 확보한다. SEED-Bench[854]는 시간적 이해 과제를 추가로 고려하고 LLM의 도움을 받아 평가 척도를 19K 객관식 문항으로 확대한다. MM-Vet[855]는 MLLM들의 통합된 멀티모달 능력들을 평가하기 위해 보다 복잡한 작업들을 제시한다. 그것은 여섯 가지 필수 복합 능력을 정의하는 것으로 시작하여 여러 능력을 결합하여 복잡한 질문을 만든다. 요약하면, 위의 벤치마크는 MLLM의 종합 평가 및 개선된 개발에 집합적으로 기여한다.

**MLLM 개선을 위한 핵심 사항** 가능한 MLLM을 개발하려면 명령 데이터, 교육 전략, 안전 및 정렬의 관점에서 모델 용량을 개선하기 위한 세 가지 핵심 사항에 대해 계속 논의합니다.

\(\bullet\)_Visual instruction data._ 광범위한 작업[856, 831]은 시각적 명령의 양과 질 모두가 MLLM의 모델 성능에 중요한 영향을 미친다는 것을 경험적으로 발견했다. 시각적 명령어들을 구성하는 하나의 기본적인 방법은 이미지들의 텍스트 설명들에 기초하여 명령어들을 합성하기 위해 LLM들의 예외적인 능력을 활용하는 것이다[851]. 명령어의 품질을 더욱 향상시키기 위해, 인간 주석의 도움으로 세밀한 시각적 명령어를 구성하거나[857, 833] 주의 깊게 설계된 프롬프트를 통해 보다 복잡한 데이터를 합성할 수 있다[835]. 위의 LLM 기반 접근법의 효과에도 불구하고, LLM(_i.e._, 어떤 이미지에 대한 훈련 없이 텍스트 생성 모델)이 구두화된 시각적 정보(_e.g._, 캡션 및 좌표)만을 기반으로 충분히 좋은 시각적 명령을 생성할 수 있는지에 대한 한 가지 주요 질문이 나타난다. 특히, 기존의 연구는 LLMs에 의해 생성된 시각적 지시가 시각적 정보, _예:_, 대상 환각에 대한 잘못된 해석을 포함하는 경우가 있음을 보여주었다[844]. 따라서 LLMs에 의해 생성된 명령어 데이터의 품질을 제어하기 위한 효과적인 검증 방법을 설계하는 것이 중요하다[835]. 또한, 시각적 지시가 좋은 시각적 지시가 무엇이며 시각적 지시가 MLLM에서 특정 멀티모달 능력을 유도하는 방법에 대한 더 많은 조사가 여전히 필요하다.

\(\bullet\)_Model training._ LLM과 달리 MLLM은 처음부터 훈련되지 않고 사전 훈련된 언어 및 비전 모델을 기반으로 개발된다. 기존 작업은 MLLM을 훈련하기 위한 전형적인 2단계 접근법, 즉 비전 언어 정렬 사전 훈련 및 시각적 명령 튜닝을 사용한다. 본질적으로 기존의 MLLM은 (1) LLM의 고유한 기능과 파라메트릭 지식을 최대한 보존하고, (2) 사전 훈련된 LLM과 비주얼 인코더를 활용하여 멀티모달 태스크에 효과적으로 적응하는 것을 목표로 한다. 위의 두 가지 목표를 달성하기 위해, 연결 모듈[151]만을 최적화하거나 커넥터 모듈과 LLM 구성 요소[851] 모두를 미세 조정하는 두 가지 전형적인 훈련 전략이 시각적 명령 튜닝을 위해 종종 사용된다. 우리가 볼 수 있듯이, 전자는 LLM의 원래 용량을 예약할 수 있지만 적응 성능이 약할 가능성이 있는 반면, 후자는 다중 모드 작업에 완전히 적응할 수 있지만 LLM의 원래 용량이 손실된다. 향상된 다중 모드 용량을 달성하기 위해 두 측면을 효과적으로 균형화하는 방법을 조사하기 위해 더 많은 노력이 필요하다. 또한, 기존의 MLLM들은 여전히 LLM들의 용량들에 과도하게 의존하며, 이는 많은 멀티모달 태스크들(예를 들어,_공간 포지셔닝)에 대한 한계를 제기한다. 이 과정에서 멀티모달 정보도 활용할 수 있도록 언어 모델의 개선된 훈련 접근법을 탐색하는 것은 의미가 있을 것이다.

\(\bullet\)_안전 및 정렬._ 안전성과 정렬은 기술 접근법에 의해 모델의 행동을 규제하는 것을 목표로 하는 LLM에서 널리 논의되어 왔다[66]. 이 주제는 MLLM에게도 중요하다. 매우 진보된 MLLM(_예를 들어,_ GPT-4V[133])조차도 안전 문제에 취약할 수 있다. 예를 들어 GPT-4V는 때때로 이미지에 대한 사실적 부정확성과 근거 없는 추론을 나타낼 수 있다. 경우에 따라서는 특정 개인 또는 그룹을 대상으로 하는 유해 콘텐츠까지 생성할 수도 있다[133]. 또한, 오픈 소스 MLLM은 또한 환각 반응을 생성하기 쉽고[844] 유해 콘텐츠를 생성하도록 쉽게 조작될 수 있다[858]. 앞서 언급한 문제를 해결하기 위해 일부 연구에서는 환각의 문제를 완화하기 위해 전문적인 시각적 지침을 수집한다[834]. 또 다른 대안적인 접근법은 사후 방식으로 MLLM에 의해 생성된 환각 반응을 교정하기 위한 수정 모델을 훈련시키는 것이다[859]. 추가적으로, MLLM들을 RLHF와 정렬하는 것은 또한 MLLM들이 개선된 사실성을 갖는 응답들을 생성하는 것을 도울 수 있다[860]. 이러한 노력에도 불구하고, MLLM에 대한 기존의 정렬 기술은 정렬 기준에 대한 포괄적인 고려가 부족한 몇 가지 특정 측면(예를 들어, 환각)에 주로 집중한다. MLLM의 안전성과 정렬에 대한 연구를 촉진하기 위해 더 많은 노력이 필요하다.

#### 8.1.5 KG-Enhanced LLM

뛰어난 능력에도 불구하고 LLM은 종종 환각 콘텐츠를 생성할 가능성[602] 및 도메인별 지식 부족[861]과 같은 지식 집약적 작업에 대한 도전을 겪는다. 유망한 해결책으로, 방대한 지식을 트리플 형식인 _i.e._(_head_entity, relation, tail_entity_)로 저장하는 지식 그래프(KG)는 정확하고 필요한 지식을 제공함으로써 LLM의 작업 성능을 향상시키는 데 활용될 수 있다. 일반적으로 지식 향상 접근법은 다른 형태의 구조화된 데이터(예: 테이블 및 데이터베이스)[862]로 확장될 수 있지만, 우리는 LLM을 개선하기 위한 KG의 통합으로 논의를 제한하며, 이는 검색 강화 LLM과 시너지 강화 LLM이라는 두 가지 측면에서 자세히 설명되어 있다.

**검색 강화 LLM** KG의 엄청난 양의 사실 기록으로 인해 기존 작업은 일반적으로 검색 모델을 채택하여 먼저 KG에서 상대적으로 작은 하위 그래프를 얻은 다음 이를 활용하여 관련 지식을 풍부하게 하여 LLM을 향상시킵니다. LLM들이 출현하기 전에, 검색된 서브그래프들은 종종 파라미터 학습을 통해 지식 정보를 PLM들에 주입하면서 트레이닝 데이터에 보충된다[863, 864, 865]. 대조적으로, 검색된 지식을 활용하기 위해 LLM은 매개 변수 업데이트 없이 주로 프롬프트의 일부로 통합한다. 이 방법을 구현하려면 두 가지 주요 기술적 문제, 즉 KG에서 관련 지식을 검색하는 방법과 LLM에서 구조화된 데이터를 더 잘 사용하는 방법이 있다. 첫 번째 이슈(_즉,_관련 지식 검색)에 대해, 전형적인 접근법은 질문-관련 사실 트리플들을 식별하기 위해 작은 언어 모델(_예를 들어,_RoBERTa)을 트레이닝하는 것이다[866]. 검색 성능을 더욱 향상시키기 위해 여러 연구에서 반복적 읽기-뒤 추론 프레임워크를 제안하여 LLM이 KG와 여러 번 상호 작용하고 필요한 지식을 보다 정확한 방법으로 습득할 수 있도록 한다[458]. 두 번째 이슈(즉, 검색된 지식을 활용하는_)에 대해, 간단한 접근법은 검색된 서브그래프를 직렬화하고 특정 프롬프트를 크래프트하여 LLMs의 입력으로 포함시키는 것이다[471, 651]. 그러나 지식 직렬화에서 구조화된 정보의 손실로 인해 LLM은 원래 KG가 전달하는 구조적 의미를 완전히 포착할 수 없다. 이 문제를 해결하기 위해, 몇몇 모델 기반 접근법들은 서브그래프를 자연 언어 텍스트로 변환하기 위해 특화된 언어 모델(_예를 들어,_T5)을 트레이닝한다[867]. 변환 정확도를 보장하기 위해, 충분한 트레이닝 쌍들(종종 감독되지 않은 구성된)[868] 및 우수한 모델 능력[869]에 의존한다.

**시너지 강화 LLM.** 복잡한 작업(예: 다중 홉 질문 응답 [656])을 해결하려면 체계적인 솔루션 계획에 따라 LLM에서 KG를 여러 번 쿼리해야 하는 경우가 많습니다. 우리는 LLM _시너지 증강 LLM_을 강화하기 위한 이러한 다중 회전 상호 작용 접근법이라고 한다. LLM과 KG를 보완적으로 더 잘 시너지시키기 위해 최근 연구에서는 복잡한 작업을 여러 하위 목표로 분해하고 KG의 필요한 지식을 활용하여 각각을 반복적으로 해결하는 것을 제안한다[870, 871, 458]. 이 과정에서 LLM은 자율 에이전트(섹션 8.1.6에서 자세히 설명됨)로 간주할 수 있으며, 이는 계획을 자동으로 생성하고 KG 환경과의 상호 작용을 통해 실행합니다[870]. 특히, 주류의 접근법들은 통상적으로 현재 단계에서 이용가능한 지식 정보를 이용하여 후보들을 열거하는 것으로 시작하여, 그 다음 단계에 대한 가장 적절한 후보들을 질문에 따라 검색한다[871, 870]. 위의 두 단계를 반복함으로써 LLMs는 점진적으로 관련 증거[871, 870]를 수집하고, 최종적으로 올바른 해결책에 접근할 수 있다. 효과에도 불구하고, KG에 대한 후보들의 열거는 방대한 검색 공간으로 이어질 것이다[872]. 이를 해결하기 위해 StructGPT [458]은 KG에 특화된 인터페이스를 사용하여 지식 정보에 접근하는 보다 효율적인 방법을 제안한다. 특히, 효율적이고 정확한 데이터 추출을 위해 KG(_e.g._, 관계 추출 및 트리플 추출) 상의 공통 데이터 연산에 따라 특화된 인터페이스를 신중하게 설계한다. 이러한 방식으로, LLMs는 KG들의 구조적 정보를 더 잘 조작하고 처리하도록 지시될 수 있고, 따라서 향상된 태스크 수행을 달성한다.

향후 방향 위의 접근법 외에도 KG 강화 LLM에 대한 몇 가지 유망한 방향이 아직 연구되지 않았다. 첫째, 구조화된 데이터의 다양성으로 인해 LLM이 다양한 종류의 지식 소스, _예:_, 도메인별 KG를 직접 활용하는 것은 여전히 어렵다. 따라서 LLM에 의해 서로 다른 지식 소스를 조작하고 활용할 수 있는 통일된 방법을 탐색하는 것이 필수적이다. 잠재적인 해결책으로 LLMs가 정확한 지식을 습득하기 위해 특정 지식 소스에서 제공하는 액세스 인터페이스를 이해하고 사용할 수 있도록 효과적인 접근법을 개발하는 것이 유망하며 [458] 데이터 다양성에 비용 효율적인 방식으로 적응하는 방법을 조사하기 위한 더 많은 노력이 필요하다. 둘째, 실세계 정보의 진화에 따라 LLMs에 저장된 지식은 구식이 되거나 부정확해질 수 있다. 업데이트된 지식을 비용 효율적인 방식으로 LLM에 동기화시키는 방법을 탐색할 필요가 있다[873, 874]. 셋째, LLM의 환각을 줄이는 데 도움이 될 수 있는 보다 충실한 콘텐츠 [875, 876] 생성에 LLM을 정렬하기 위해 KG의 사실 정보의 사용을 조사하는 것이 유망하다.

KG 강화 LLM을 탐색하는 것 외에도 LLM을 활용하여 KG 측면(_i.e._, LLM4KG)[861, 877]의 작업을 개선하는 것도 의미가 있다. 대표적인 예는 LLM이 KG를 보완하거나 구성하는 데 도움이 될 수 있다는 것이다. 이 부분에 대한 논의는 우리의 범위를 벗어났기 때문에 생략한다.

#### 8.1.6 LLM 기반 에이전트

AI에서 에이전트에 대한 연구는 환경을 인식하고, 결정을 내리고, 특정 목표를 달성하기 위해 행동을 취할 수 있는 개체를 개발하는 것을 목표로 한다[878]. 그러나, 전통적인 에이전트들은 종종 휴리스틱 규칙들 또는 특정 환경들로 제한되며, 이는 그들의 일반화를 오픈-도메인 시나리오들로 제약한다[879]. LLM은 복잡한 작업을 해결하는 데 탁월한 능력을 가지고 있다는 점을 감안할 때 에이전트의 핵심 계산 단위 역할을 하는 유망한 솔루션으로 빠르게 부상했다[818]. 이 부분에서는 먼저 LLM 기반 에이전트의 프레임워크를 소개하고 그 응용에 대해 논의할 것이다.

전체 프레임워크.다음으로, 먼저 LLM 기반 에이전트의 주요 구성 요소에 대해 자세히 설명하고 일반적인 워크플로우를 제시한다.

\(\bullet\)_Components._ 일반적으로 LLM 기반 에이전트에는 _메모리, 계획 50_ 및 _실행_의 세 가지 주요 구성 요소가 있습니다. 구체적으로, _메모리_ 컴포넌트는 환경으로부터 인지되는 정보를 저장하는 것을 목적으로 하며, 의사 결정을 지원하는 데 활용될 수 있다. 특히 LLM 기반 에이전트는 일반적으로 읽기 및 쓰기 동작으로 단기 기억과 장기 기억 모두에서 정보를 유지한다. 단기 기억은 일반적으로 LLMs(_i.e._, 입력)의 내부 컨텍스트 창을 참조하며, 여기서 LLMs는 추론과 같은 동작을 통해 읽고 쓸 수 있다[880]. 장기 메모리는 벡터 데이터베이스와 같이 외부 저장소에 매핑될 수 있지만 [537], 여기서 LLM은 검색을 통해 읽고 반사를 통해 쓸 수 있다[686]. 특히, 프로파일은 일반적으로 장기 메모리로 구현되는데, 이는 그 역할과 기능을 지정하는 에이전트의 중요한 기능이다[818]. _planning_ 컴포넌트는 메모리 컴포넌트로부터의 정보에 기초하여 액션 플랜을 생성하는 것을 담당한다. 데이터 포맷에서, 계획은 보통 텍스트 기반 명령어[441] 또는 코드 기반 프로그램[443]의 형태를 취한다. 이를 생성하기 위해, LLM-기반 에이전트들은 먼저 여러 후보들을 제안하고 그 중에서 더 적합한 후보를 선택할 것이다[436]. 초기 계획은 환경으로부터의 실행 피드백으로 더 정제될 수 있다[528]. [실행] 구성 요소는 내부 LLM [441] 또는 외부 도구 [880]에서 수행할 수 있는 계획 구성 요소에서 계획을 수행하는 것을 담당합니다.

각주 50: 섹션 6.4에서는 LLM에 대한 활용 접근법으로 계획을 소개하는 반면, 이 섹션에서는 LLM 기반 에이전트에서 기능적 구성 요소로서의 활용을 설명한다.

* _워크플로._ 위에서 언급한 세 가지 구성 요소를 사용하여 LLM 기반 에이전트의 일반적인 워크플로는 다음과 같습니다. 먼저 환경으로부터 정보를 받아 단기 기억에 기록한다. 그리고, 에이전트는 새로 수신된 정보를 단기 메모리에 처리한다. 이러한 프로세스는 장기 기억으로부터 검색된 정보로 향상될 수 있다. 이어서, 계획 컴포넌트는 다음 계획을 생성하기 위해 단기 메모리로부터 처리된 정보를 활용한다. 마지막으로, 실행 컴포넌트는 계획 컴포넌트로부터 생성된 계획을 실행하며, 이는 외부 툴들을 추가로 보조할 수 있다. 앞서 언급한 과정을 반복함으로써 LLM 기반 에이전트는 환경의 피드백에 대응하여 자율적으로 행동을 조정하고 궁극적으로 목표를 달성할 수 있다. LLM 기반 에이전트는 사용자 요청을 수신하거나 목표를 할당받으면 위의 워크플로우를 따라 환경과의 다중 전환 상호 작용을 통해 작업을 수행합니다.

요약하자면, LLM 기반 에이전트에서 LLM은 핵심 계산 단위 역할을 하며 _메모리, 계획,_ 및 _실행_을 포함한 구성 요소가 장착되어 있다. 이러한 구성 요소는 환경과의 상호 작용 동안 LLM 제어 하에 체계적인 방식으로 통합된다. 자세한 내용은 LLM 기반 AI 에이전트에 대한 포괄적인 조사를 참조할 수 있다[818].

응용.최근 LLM 기반 에이전트는 복잡한 작업을 자율적으로 해결하는 데 큰 잠재력을 보여 특정 도메인 또는 작업에 대한 유능한 응용 프로그램을 빠르게 개발할 수 있다. 이 섹션에서는 단일 에이전트 및 다중 에이전트 시나리오의 응용 프로그램에 대해 논의합니다.

* _단일 에이전트 기반 응용 프로그램_ 단일 에이전트 모드를 기반으로 하는 애플리케이션은 주로 사용자 요청을 자율적으로 완료할 수 있는 능력 있는 작업 해결기를 개발하는 것을 목표로 한다. 범용 과제 해결에 초점을 맞춘 다수의 단일 에이전트 프로젝트가 개발되었다. 대표적인 프로젝트로 AutoGPT[534]는 장기/단기 메모리 관리 및 검색 엔진과 같은 외부 도구를 사용하여 LLMs에 권한을 부여합니다. AutoGPT는 사용자 요청을 자율적으로 처리하기 위해 자신의 기억과 추론과 같은 행동으로 요청을 이해하고, 세부 계획으로 분해하고, 도구의 도움으로 계획을 단계별로 실행하며, 환경의 피드백을 바탕으로 나머지 계획을 구체화한다. 이러한 반복 프로세스는 사용자 요청이 성공적으로 해결될 때까지 계속된다. 다른 유사한 프로젝트들은 GPT-Engineer[881] 및 XAgent[882]를 포함한다. 또한 웹 브라우징 환경의 경우 WebGPT[81], 실생활 환경의 경우 ProgPrompt[530], 마인크래프트 환경의 경우 Voyager[697] 등 특정 도메인에 대한 자율 에이전트 개발을 목표로 하는 작업도 있다.

\(\bullet\)_Multi-agent 기반 응용 프로그램_ 에이전트가 독립적으로 작동하는 단일 에이전트 시스템과 달리 다중 에이전트 시스템은 공동 작업을 통해 집단 지성을 발휘합니다. 전형적으로, 다수의 에이전트들은 각각의 역할들 및 기능들을 갖는, 동일하거나 상이한 LLM들로부터 인스턴스화될 수 있다. 이러한 에이전트 간의 조정 전략에 따라 다중 에이전트 시스템은 협력 기반과 경쟁 기반 두 가지 범주로 나눌 수 있다. 협력 기반 모드에서는 정보를 공유하고 에이전트 간의 협력 행위를 모색하기 위해 자유 형식의 대화[883], 구조화된 문서[884], 데이터 임베딩[885] 등 다양한 통신 프로토콜이 제안되었다. 통신 프로토콜에 기초하여, 에이전트들은 소프트웨어 엔지니어링[884], 사용자 행동 분석[821, 819], 및 사회 시뮬레이션[533]과 같은 다운스트림 애플리케이션들에 대해 효과적으로 조직될 수 있다. 경쟁 기반 모드에서 논쟁은 발산적 사고를 촉진하고 에이전트 간의 귀중한 외부 피드백을 이끌어내기 위한 인기 있는 커뮤니케이션 프로토콜 중 하나이다. 이러한 방법은 수학적 추론[886] 및 평가[732]와 같이 정확한 의사 결정 및 정확한 응답을 요구하는 도메인에 유익하다.

**나머지 문제** 큰 성공에도 불구하고 LLM 기반 에이전트의 개발 및 응용 프로그램을 제한 하는 몇 가지 문제가 여전히 있습니다. 첫째, 모델 규모의 폭발적인 성장과 함께 LLM 기반 에이전트의 시간 및 메모리 오버헤드를 포함한 효율성은 대규모 배치, 특히 LLM 인스턴스가 많은 다중 에이전트 시스템에서 중요한 문제가 된다. 둘째, LLM 기반 에이전트 수의 축소에 따라 에이전트 간의 조정 복잡도 증가를 지원하기 위해 보다 효과적이고 효율적인 통신 프로토콜 및 아키텍처가 필요하다. 또한, 유능한 에이전트를 구축하는 것은 명령 후속 및 긴 텍스트 모델링과 같은 LLM의 능력에 대한 기술적 문제를 제기한다. 기존의 LLM은 인스턴스화 에이전트에 특별히 최적화되어 있지 않기 때문에 LLaMA와 같은 대부분의 공개 소스 LLM은 에이전트 개발을 효과적으로 촉진할 수 없다. 따라서 에이전트의 핵심 계산 단위 역할을 수행할 수 있는 전문화된 모델을 개발하는 것이 중요하다.

#### 8.1.7 LLM for Evaluation

인간 평가는 일반적으로 신뢰할 수 있는 품질 평가를 제공할 수 있지만 높은 주석 비용, 상당한 시간 요구 사항 및 주석 불일치로 인해 종종 방해를 받는다[887]. 이와는 대조적으로, 자동 평가는 인간 평가에 대한 확장 가능한 대안으로 채택될 수 있다. 전통적인 자동 평가는 참조 기반 메트릭(_e.g._, BLEU 및 ROUGE)에 의존해 왔다. 최근 LLM이 일반 과제 해결자로 등장하면서 자동 평가자로서의 잠재력을 부각시켜 LLM 기반 평가를 수행할 것을 약속하고 있다[647, 727]. 다음 부분에서는 평가 형식, 방법, 메타 평가 및 나머지 문제를 포함하여 평가를 위한 LLM에 대한 최근 진행 상황을 소개할 것이다.

**평가 형식** 평가 결과의 유형에 따라 평가 형식은 _점수 기반 평가_ 및 _언어 기반 평가_로 분류할 수 있습니다. 점수 기반 평가는 평가된 텍스트에 대한 품질 점수(_e.g._, 등급 또는 순위)를 할당하기 위해 측정 가능한 메트릭을 사용한다. 일반적인 방법은 쌍대 비교를 수행하는 것인데, 여기서 LLM은 특정 지침[354, 647, 727]에 따라 후보 텍스트의 부분 순서 관계를 결정하는 데 사용되며, 이는 평가 작업을 크게 단순화한다. 그러나, 후보들의 수를 스케일링할 때 비효율성 문제에 직면할 수 있다[727]. 평가 중에 고품질 참조 텍스트가 이용 가능할 때, LLMs는 참조에 의해 제공된 안내 하에 텍스트 점수를 매기도록 지시될 수 있다[727, 728, 716]. 한편, 언어 기반 평가는 단순한 정량적 점수를 넘어 정성적인 설명을 제공하는 비평 및 제안을 생성하는 것에 초점을 맞춘다[889, 890, 371, 891, 893, 894, 895, 372, 897, 898, 373, 899, 381, 899, 390, 391, 892, 893, 894, 895, 896, 897, 898, 999, 900, 899, 899, 1009, 910, 899, 1012, 1016, 1017, 1018, 1015, 1016, 891, 892, 893, 894, 895, 999, 1026, 1027, 1028, 1029, 1030, 1031, 1042, 1043, 1044, 1044, 1045, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 대표적인 벤치마크인 MT-벤치[727]는 LLM과 인간 판단 간의 일치를 평가하여 GPT-4가 80개의 다중 회전 질문에 대한 노타이 비교에서 인간 선호도와 밀접하게 일치함을 보여준다. 또한 주관적인 인간 평가에서 발생하는 잠재적인 편향을 해결하기 위해 LLMBar [897]은 객관적으로 더 나쁘지만 표면적으로 매력적인 출력을 수동으로 설계하여 평가자를 오도할 수 있다. 평가 결과는 가장 진보된 LLM조차도 여전히 도전적인 환경에서 인간 수준의 평가에 미치지 못한다는 것을 보여준다.

**나머지 문제** 섹션 7.1.1에서 논의한 바와 같이 최근 연구에 따르면 LLM 기반 평가자는 주문 편향, 자기 선호 편향 및 길이 편향과 같은 여러 유형의 편향을 노출한다[647, 727]. 다중 경로 앙상블 또는 다중 에이전트 협업과 같은 방법을 통해 일부 편향을 완화할 수 있지만 LLM 기반 평가자에 고유한 것으로 남아 있다. 결과적으로 모델 내에서 이러한 편향을 본질적으로 해결하는 것은 여전히 어려운 문제이다. 또한, 최근 연구에 따르면 LLM은 자체 생성 콘텐츠를 이해할 수 없어 생성 능력에 비해 이해 능력이 약할 수 있다[899]. 가장 진보된 LLMs조차도 여전히 외부 피드백 없이 그들의 추론 또는 사실적 오류를 식별하는 데 어려움을 겪고 있다[900, 901]. 따라서 현재의 LLM 기반 평가자는 최상위 LLM 또는 복잡한 작업을 평가하는 데 적합하지 않을 수 있다. 이는 특히 능력 있는 LLM과 정교한 추론, 계획 및 영역별 지식을 요구하는 복잡한 작업을 평가하기 위해 LLM 기반 평가자를 위한 개선 접근법의 중요성을 강조한다.

### _LLM for Specific Domains_

이 부분에서 우리는 의료, 교육, 법률, 재정 및 과학 연구 지원을 포함한 몇 가지 대표적인 도메인에 대한 LLM의 적용에 대해 논의한다.

**헬스케어** 는 인간의 삶과 밀접한 관련이 있는 중요한 응용 분야입니다. ChatGPT가 등장한 이후 많은 연구에서 ChatGPT 또는 기타 LLM을 의료 영역에 적용했다. LLM은 다양한 의료 업무, 예를 들어 생물학 정보 추출[763], 의료 조언 상담[902], 정신 건강 분석[903], 보고서 단순화[904]를 처리할 수 있는 것으로 나타났다. 주요 기술적 접근법으로 연구자들은 일반적으로 광범위한 의료 업무를 수행하도록 LLM을 안내하는 특정 프롬프트 또는 지침을 설계한다. 의료 영역에서 LLM의 힘을 더 활용하기 위해 연구자들은 의료 관련 LLM을 개발할 것을 제안한다[905, 356, 906]. 특히 Med-PaLM 모델[356, 905]은 미국 의료 면허 시험(USMLE)에서 전문가 수준의 성능을 달성하고 소비자의 의료 질문에 대한 의사로부터 더 큰 승인을 받는다. 그러나 LLM은 의료 잘못된 정보를 위조할 수 있습니다[904, 907], _예:_ 의료 용어를 잘못 해석하고 의료 지침과 일치하지 않는 조언을 제안합니다. 또한, 환자의 건강 정보를 LLM을 지원하는 상용 서버에 업로드하는 것도 개인 정보 보호 문제를 제기할 것이다[763].

**교육** 은 LLM이 잠재적으로 상당한 영향을 미치는 중요한 응용 프로그램 도메인이기도 합니다. 기존의 연구는 LLM이 객관식 문제와 자유응답 문제 모두에서 다양한 수학 과목(예: 물리, 컴퓨터 과학)에서 표준화된 시험[46]에 대한 학생 수준의 성과를 달성할 수 있다는 것을 발견했다. 또한, 경험적 연구에 따르면 LLMs은 교육을 위한 글쓰기 또는 읽기 보조 역할을 할 수 있다[908, 909]. 최근 연구[909]는 ChatGPT가 학문 전반에 걸쳐 논리적으로 일관된 답변을 생성할 수 있으며 깊이와 넓이의 균형을 맞출 수 있음을 보여준다. 또 다른 정량적 분석[908]은 ChatGPT를 활용하는 학생들(LLM의 결과를 자신의 답변으로 유지하거나 정제하는 것)이 컴퓨터 보안 분야의 일부 과목에서 일반 학생들보다 더 나은 성과를 보인다는 것을 보여준다. 최근 여러 관점 논문[910, 911]에서도 교사-학생 협업, 개인 맞춤형 학습, 평가 자동화와 같은 교실 수업에서 LLM의 다양한 적용 시나리오를 탐색하고 있다. 그러나 교육에 LLM을 적용하면 표절, AI 생성 콘텐츠의 잠재적 편향, LLM에 대한 과도한 의존, 비영어권 개인에 대한 불공평한 접근과 같은 일련의 실용적인 문제가 발생할 수 있다[912].

**법** 은 전문 도메인 지식을 기반으로 하는 전문 도메인입니다. 최근에는 다양한 법률 업무, 예를 들어 법률 문서 분석[913], 법률 판단 예측[914], 법률 문서 작성[915] 등을 해결하기 위해 LLMs을 적용한 연구가 다수 있다. 최근 연구[916]에 따르면 LLMs는 법적 해석과 추론의 강력한 능력을 나타낸다. 더욱이, 최신 GPT-4 모델은 모의 변호사 시험에서 인간 수험생들과 비교하여 상위 10% 점수를 달성한다[46]. 법률 영역에서 LLM의 성능을 더욱 향상시키기 위해 특별히 설계된 법률 프롬프트 엔지니어링을 사용하여 긴 법률 문서 이해 및 복잡한 법률 추론에서 향상된 성능을 산출한다[917, 918]. 진행 상황을 요약하자면, LLM은 법률 직업에 도움이 되는 보조자 역할을 할 수 있다. 진행에도 불구하고 법률에서 LLM을 사용하는 것은 저작권 문제[919], 개인 정보 유출[920], 또는 편견과 차별[921]을 포함한 법적 문제에 대한 우려를 불러일으킨다.

**금융** 은 LLM이 유망한 응용 프로그램 전망을 가지고 있는 중요한 분야입니다. LLM은 수치 클레임 탐지[922], 금융 감정 분석[923], 금융 명명 개체 인식[924], 및 금융 추론[925]과 같은 다양한 금융 관련 작업에 사용되었다. 재무 작업에서 범용 LLM이 나타내는 경쟁적인 제로 샷 성능에도 불구하고, 그들은 여전히 백만 개의 스케일 파라미터를 포함하는 도메인-특정 PLM을 저성능화한다[922]. LLM의 스케일링 효과를 활용하기 위해 연구자들은 LLM(_예:_ BloombergGPT[360], XuanYuan 2.0[926], FinGPT[927])을 지속적으로 사전 훈련하기 위해 대규모 금융 코퍼스를 수집한다. 블룸버그GPT는 범용 업무에서 경쟁적 성과를 유지하면서 다양한 금융 업무 전반에 걸쳐 괄목할 만한 성과를 보여주었다[360]. 그럼에도 불구하고, LLM에 의한 부정확하거나 유해한 콘텐츠의 생성은 금융 시장에 상당한 부정적인 영향을 미칠 수 있기 때문에 금융에서 LLM 적용의 잠재적 위험을 고려하는 것이 필수적이다[360]. 따라서 금융 분야에서 LLM의 사용에 대한 보다 엄격한 검토와 모니터링이 필요하다.

**과학적 연구** 는 LLM이 개발 진행에 권한을 부여할 수 있는 또 다른 유망한 분야입니다. 선행 연구는 지식 집약적인 과학 과제(_e.g._, PubMedQA[928], BioASQ[929])를 처리하는 데 LLMs의 효과를 입증하며, 특히 과학 관련 코퍼라[35, 203, 930]에 대해 훈련된 LLMs에 대한 것이다. 뛰어난 일반적인 능력과 광범위한 과학적 지식을 감안할 때 LLM은 과학 연구 파이프라인의 다양한 단계에 걸쳐 유용한 보조자로서 상당한 잠재력을 가지고 있다[931]. 먼저 문헌 조사 단계에서 LLMs는 특정 연구 분야의 진행 상황을 종합적으로 개관하는 데 도움을 줄 수 있다[932, 933]. 둘째, 연구 아이디어 생성 단계에서 LLM은 흥미로운 과학적 가설을 생성하는 능력을 보여준다[934]. 셋째, 데이터 분석 단계에서 LLMs을 사용하여 데이터 탐색, 시각화 및 분석 결론 도출을 포함하여 데이터 특성을 분석하는 자동 접근법을 수행할 수 있다[935, 936]. 넷째, 논문 쓰기 단계에서 연구자는 과학 글쓰기에서 LLMs의 도움을 받을 수도 있는데, LLMs는 기존 내용을 요약하고 글을 연마하는 등 다양한 수단을 통해 과학 글쓰기에 귀중한 지원을 제공할 수 있다[937, 938]. 또한, LLMs은 오류 검출, 체크리스트 검증 및 후보 랭킹과 같은 태스크를 포괄하여 자동화된 논문 검토 프로세스에 도움을 줄 수 있다[940]. 이러한 발전에도 불구하고 LLM이 생성된 과학 콘텐츠의 품질을 높이고 유해한 환각을 줄이는 데 도움이 되고 신뢰할 수 있는 과학 조수 역할을 할 수 있는 능력을 향상시킬 여지가 많다.

_요약__ 앞서 언급한 작업 외에도 LLM의 응용 프로그램은 여러 다른 도메인에서도 논의되었다. 예를 들어, 심리학적 영역에서, 일부 최근의 연구는 자기 인식, 정신 이론(ToM) 및 정의적 컴퓨팅과 같은 LLM의 인간 유사 특성을 연구했다[941, 942]. 특히, 두 개의 고전적인 거짓 신뢰 태스크에 대해 수행된 ToM에 대한 경험적 평가는 GPT-3.5 시리즈의 모델이 ToM 태스크의 9세 어린이와 유사한 성능을 달성하기 때문에 LLM이 ToM과 유사한 능력을 가질 수 있다고 추측한다[941]. 또한, 다른 작업 라인에서는 LLM을 소프트웨어 개발 도메인, _예:_, 코드 제안[943], 코드 요약[944] 및 자동화된 프로그램 복구[945]에 적용하는 것을 조사했다. 요약하자면, 실제 작업에서 LLM에 의해 인간을 돕는 것은 중요한 연구 영역이 되었다. 그러나, 그것은 또한 도전 과제들을 제시한다. LLM 생성 콘텐츠의 정확성을 보장하고 편향을 해결하며 사용자 개인 정보 보호 및 데이터 보안을 유지하는 것은 LLM을 실제 시나리오에 적용할 때 중요한 고려 사항이다.

## 9 결론 및 미래 방향

본 연구에서는 대규모 언어모델(Large Language Model, LLM)의 최근 동향을 살펴보고, LLM을 이해하고 활용하기 위한 주요 개념, 연구 결과 및 기법을 소개하였다. 기존 문헌에서 잘 다루어진 초기 사전 훈련 언어 모델(_e.g._, BERT 및 GPT-2)의 내용은 제외하면서 크기가 큰 모델(_i.e._, 10B보다 큰 크기)에 초점을 맞춘다. 특히, 설문조사는 LLM의 네 가지 중요한 측면, 즉 _i.e._, 사전 훈련, 적응, 활용 및 평가에 대해 논의했다. 각 측면에 대해 LLM의 성공에 핵심이 되는 기술이나 결과를 강조한다. 또한 LLMs 개발을 위한 가용 자원을 요약하고 LLMs 재생을 위한 중요한 구현 지침에 대해 논의한다. 이 조사는 LLMs에 대한 가장 최근의 문헌을 다루려고 하며 연구자와 엔지니어 모두에게 이 주제에 대한 좋은 참조 자료를 제공한다.

다음으로, 본 조사의 논의를 요약하고 LLM에 대한 도전과제와 향후 방향을 다음과 같은 측면에서 소개한다.

**기본 사항 및 원칙** 특정 작업 목표에 대한 교육 대신 LLM은 대규모 텍스트 데이터에 대한 감독되지 않은 사전 교육에서 학습합니다. 이것은 충분한 일반화를 달성하기 위해 훈련 과제를 가능한 한 확장하는 것을 목표로 하는 이전의 다중 작업 학습 접근법과는 상당히 다르다. 따라서 LLM의 능력의 기초를 확립하는 기본 원칙이나 요소를 밝히는 것이 필수적이다. 언어 모델의 기본 아이디어는 직관적이지만, 단순한 언어 모델링 목표(_예:_, 다음 토큰 예측)에 의해 훈련된 LLM이 다양한 실제 작업을 해결할 수 있게 되는 이유를 공식적으로 설명하는 것은 여전히 어렵다. 이 문제를 조사하기 위해, LLM의 모델 용량은 사전 훈련 데이터에 크게 의존하기 때문에, 비지도 사전 훈련에 기초한 용량 학습(또는 선택) 메커니즘을 연구하는 유망한 접근법이 있다. 또한, _스케일링_은 LLM[31, 55, 64]의 용량을 향상시키는 데 중요한 역할을 하며, 대형 모델의 행동이 소형 모델의 행동과 어떻게 관련되는지, _예를 들어, 대형 모델의 행동은 소형 모델에서 추론할 수 있고 실제로 예측할 수 없는 것에 대해 보다 이론적인 분석을 수행하는 것이 매우 유용하다. 또 다른 연구 방향은 LLM이 사전 훈련 데이터에 의해 인코딩된 지식을 넘어 일반화될 수 있는지에 대한 우려가 제기되었기 때문에 LLM에 대한 모델 일반화에 대한 보다 심층적인 분석을 탐구하는 것이다. 또한, 데이터 오염은 LLMs(738)의 성능을 공정하게 평가하기 위해 심각한 문제가 되었으며, 따라서 적절한 평가 프로토콜을 설정하는 것은 LLMs의 모델 용량을 조사하고 분석하는 기초가 될 것이다.

**모델 아키텍처.** 확장성과 효과성으로 인해 Transformer는 LLM을 빌드하는 사실상의 아키텍처가 되었습니다. 신경망 구성 및 확장 가능한 병렬 훈련과 같은 이 아키텍처의 성능을 개선하기 위한 다양한 전략이 제안되었다(섹션 4.2.2의 논의 참조). 그러나 트랜스포머는 여전히 높은 훈련 비용과 느린 추론 속도로 인해 어려움을 겪고 있다. 대규모 사전 훈련을 위한 개선된 모델 아키텍처를 개발하기 위한 더 많은 노력[251, 252]이 여전히 필요하다. 특히, 시스템 레벨 또는 하드웨어 레벨 최적화(_e.g._, FlashAttention[284])는 트랜스포머 아키텍처의 효율성을 향상시키기 위해 더 많은 탐색의 가치가 있다. 또한, 중요한 기본 용량으로서, 기존의 LLM들은 통상적으로 긴 컨텍스트 윈도우를 유지한다. 예를 들어, 가장 최근의 GPT-4 Turbo는 128K 토큰의 긴 컨텍스트를 가능하게 하고, Claude 2.1도 최대 200K 토큰의 입력을 지원한다. LLM[264, 291]의 긴 컨텍스트 모델링 능력을 향상시키기 위한 많은 노력이 있었지만, 결과 모델들은 여전히 컨텍스트 윈도우에서 정보를 잘 처리하지 못한다[299]. 이 문제를 해결하기 위해서는 긴 컨텍스트 정보의 모델링 및 활용을 향상시키기 위한 특정 아키텍처 적응 또는 알고리즘이 필요할 수 있다. 또 다른 우려되는 것은 기존 작업이 대부분 디코더 전용 트랜스포머로 LLM을 훈련하는 데 초점을 맞추고 있다는 것이다. 효과성에도 불구하고 대체 모델 아키텍처에 대한 보다 광범위하고 다양한 탐색을 심각하게 제한한다.

**모델 교육.** 사전 교육을 위해서는 데이터 수집, 데이터 정리, 데이터 혼합 및 데이터 커리큘럼의 체계적인 프로세스를 효과적으로 지원할 수 있는 LLM 최적화를 위한 데이터 중심 인프라 및 교육 절차를 구축하는 것이 필수적입니다. 또한 컴퓨팅 클러스터에서 리소스를 더 잘 구성하고 활용할 수 있도록 하드웨어 지원 또는 리소스 스케줄의 유연한 메커니즘을 요구합니다. 실제적으로, 큰 컴퓨팅 소비와 데이터 품질 및 훈련 트릭에 대한 민감성 때문에, 가능한 LLM을 사전 훈련시키는 것은 매우 어렵다[78, 93]. 따라서, LLM, _e.g._, 예측가능한 스케일링[46] 및 프록시 모델 트레이닝[59]을 최적화하기 위한 시스템적이고 경제적인 사전 트레이닝 접근법들을 개발하는 것이 특히 중요해진다. 대규모 모델 최적화에서 열화 또는 실패의 잠재적 위험을 줄이기 위해 더 많은 훈련 레시피 또는 원칙을 조사하고 공유해야 한다. 점점 더 많은 모델 체크포인트와 청소된 데이터 세트가 출시되었지만 사전 훈련 데이터 준비(_e.g._, 세부 청소 전략) 및 데이터 스케줄링(_e.g._, 데이터 혼합 및 커리큘럼)에 대한 재현 가능한 작업이 여전히 부족하다. LLM을 처음부터 사전 훈련하는 것은 매우 비용이 많이 들기 때문에 공개적으로 사용 가능한 모델 체크포인트(_e.g._, LLaMA[57] 및 Flan-T5[69])를 기반으로 LLM을 지속적으로 사전 훈련하거나 미세 조정하기 위한 적절한 메커니즘을 설계하는 것이 중요하다. 이를 위해 많은 기술적 문제, 예를 들어 치명적인 망각 및 작업 전문화를 해결해야 합니다. 또한 특정 지식을 효과적으로 주입하거나 편집하고 오래된 사실을 수정하는 효과적인 튜닝 전략을 개발하는 것도 유용합니다.

**모델 활용** 자연어 인터페이스를 기반으로 _prompting_ 는 LLM을 사용 하 여 다양한 작업을 해결 하는 데 탁월한 접근 방식이 되었습니다. 태스크 설명 및 데모 예시를 프롬프트에 결합함으로써, 인-컨텍스트 학습(ICL)은 LLM들이 새로운 태스크들에 대해 잘 수행할 수 있는 능력을 부여하며, 심지어 일부 경우들에서 전체-데이터 미세 조정 모델들을 능가한다. 복잡한 추론의 능력을 향상시키기 위해 중간 추론 단계를 프롬프트로 포함하는 CoT(Chain-of-thought) 전략으로 예시된 고급 프롬프트 기술이 제안되었다. 또한, 계획은 복잡한 작업을 해결하기 위한 유망한 접근법이며, 도구 사용 용량을 활용하여 LLM을 반복적으로 호출한다. 이러한 노력에도 불구하고, 프롬프트와 관련된 몇 가지 기본적인 문제들은 여전히 탐구되지 않고 있다: 왜 좋은 프롬프트는 정답을 이끌어낼 수 있지만 나쁜 프롬프트는 이끌어낼 수 없는지, 진보된 프롬프트 방법(_e.g._, ICL 및 CoT)의 작동 원리를 밝히고 이러한 기존 접근 방식을 더욱 개선하는 방법, 그리고 특정 작업에서 LLMs에 대한 효과적인 프롬프트를 효율적으로 찾는 방법. 더욱이, 실용적인 관점에서, 특히 대규모 배치에서 LLM의 추론 비용을 줄이는 것은 근본적인 과제가 되었다. 또 다른 인기 있는 연구 방향은 검색 증강 생성으로, 지원 소스에서 검색된 컨텍스트가 과제 해결을 위한 프롬프트에 포함된다. 검색 증강은 지식 경계를 확장하고 질의 응답 능력을 향상시킬 수 있지만 [461] LLMs에 의한 긴 컨텍스트 활용의 효율성에 어려움을 겪을 수 있음을 보였다[299].

**안전 및 정렬** 용량에도 불구하고 LLM은 실제 사용 시 큰 안전 문제에 직면해 있습니다. 확률적 모델링 성격의 근본적인 문제로서, LLMs는 그럴듯해 보이지만 사실적으로 부정확할 수 있는 텍스트를 참조하여 환각을 생성하는 경향을 나타낸다[638]. 더 나쁜 것은 LLMs가 악의적인 시스템에 대한 유해, 편향 또는 독성 텍스트를 생성하도록 의도적인 지침에 의해 유발되어 오용의 잠재적 위험을 초래할 수 있다는 것이다[66, 55]. LLM의 안전 문제(_e.g._, 프라이버시, 과잉 의존, 허위 정보 및 영향 작업)에 대한 자세한 논의를 위해 독자는 GPT-3/4 기술 보고서[55, 46]를 참조할 수 있다. 이러한 문제를 피하기 위한 주요 기술적 접근법으로서, 정렬 방법(_e.g._, RLHF)[116, 66]은 잘 정렬된 LLM을 개발하기 위해 인간 피드백을 활용하여 널리 사용되어 왔다. 그러나 RLHF는 전문 레이블러의 고품질 인간 피드백 데이터에 크게 의존하며, 이는 자격을 갖춘 인간 주석자를 모집하는 데 비용과 시간이 많이 소요된다. 따라서 인간 레이블러의 노력을 줄이기 위한 RLHF 프레임워크를 개선하고 데이터 품질이 보장된 보다 효율적인 주석 접근법을 모색해야 하며, 레이블링 작업을 지원하기 위해 LLM을 사용할 수 있다. 또한, RLHF의 훈련 난이도 및 불안정성을 줄이기 위해 정렬을 위한 단순화된 최적화 알고리즘[386, 389]을 개발할 것을 제안한다. 또 다른 실용적인 접근법으로서, 적색 학습[369, 132]은 LLM의 모델 안전성을 향상시키기 위해 채택되었으며, 이는 수집된 적대적 프롬프트를 활용하여 LLM(_i.e._, 적색 학습으로부터의 공격을 회피함)을 개선한다. 또한, 도메인별 데이터로 LLM을 미세 조정할 때 프라이버시 우려도 고려해야 하며, 따라서 연합 기반 학습[946]은 프라이버시 제한 시나리오에서 유용할 수 있다.

**애플리케이션 및 생태계** LLM은 다양한 작업을 해결하는 데 강력한 능력을 보여주었기 때문에 광범위한 실제 애플리케이션(_i.e._, 작업별 자연어 지침에 따라)에서 적용할 수 있습니다. 놀라운 진전으로 ChatGPT는 인간이 정보에 액세스하는 방식을 잠재적으로 변경했으며 이는 _New Bing_ 릴리스에 추가로 통합되었습니다. 일반적으로 가까운 미래에는 LLM이 검색 엔진과 추천 시스템을 포함한 정보 검색 기술에 상당한 영향을 미칠 것으로 예측할 수 있다. 또한, LLM은 실제 시나리오에서 다양한 복잡한 작업을 처리하기 위해 보다 지능적인 시스템(예: 자율 AI 에이전트)을 개발하는 것을 가능하게 한다. 특히, Assistants API는 OpenAI(명령어, 지식 및 도구 사용 기능)에 의해 출시되어 애플리케이션 내에서 에이전트와 유사한 어시스턴트를 빠르게 개발할 수 있다. 이러한 기술 혁신의 물결은 인간의 삶과 밀접한 관련이 있는 LLM 기반 애플리케이션(_e.g._, OpenAI의 GPT Store)의 생태계로 이어질 것이다. 마지막으로 LLMs의 부상은 인공지능(AGI)의 탐구를 조명한다. 어느 때보다 스마트 AI 시스템 개발이 유망하다. 그러나 이 개발 과정에서 AI 안전은 AI가 인류에 좋으나 나쁘지는 않게 만드는 주요 관심사 중 하나가 되어야 한다[40].

### Coda

이 긴 설문 조사를 작성하고 시기적절한 작업으로 내용을 업데이트하는 것은 쉬운 일이 아니다. 먼저 독자와 저희 팀원들의 성원에 진심으로 감사드립니다. 우리는 이 조사에 매우 열심히 노력하며 그것이 LLM에 대한 포괄적이고 시기적절한 참조를 제시할 수 있기를 바란다.

**설문 작성**. 이 조사는 우리 연구팀이 개최한 토론회에서 계획되었으며, 최근 대규모 언어 모델의 발전을 팀 구성원에게 매우 읽기 쉬운 보고서로 요약하는 것을 목표로 했다. 첫 번째 초안은 2023년 3월 13일에 완료되었으며, 우리 팀원들은 LLM에 대한 관련 연구를 비교적 객관적이고 포괄적인 방식으로 포함하도록 최선을 다했다. 그런 다음 몇 번의 패스로 글과 내용을 광범위하게 수정했습니다. 공간 한계로 인해 선택 기준을 설정하면 그림 3과 표 1에서 기존 LLM의 일부만 포함할 수 있다. 그러나 정기적으로 유지되는 GitHub 페이지([https://github.com/RUCAlBox/LLMSurvey](https://github.com/RUCAlBox/LLMSurvey))에서 모델 선택에 대한 보다 완화된 기준을 설정합니다. 2023년 3월 31일 초판, 2023년 6월 29일 대판, 2023년 9월 10일 2판, 2023년 11월 23일 이 최신판(대판)을 발표한다.

**조언을 찾습니다* * 우리의 모든 노력에도 불구하고, 이 조사는 여전히 완벽과는 거리가 멀다: 우리는 중요한 참고 자료나 주제를 놓칠 가능성이 있고, 또한 비강조적인 표현이나 토론을 할 수도 있다. 이번 설문조사는 지속적으로 업데이트하여 최대한 품질을 향상시키도록 하겠습니다. 우리에게 설문 작성은 LLMs에 대한 학습 과정이기도 합니다. 이 설문 조사를 개선하기 위한 건설적인 제안이 있는 독자의 경우 설문 조사의 깃허브 페이지에 의견을 남기거나 작성자에게 직접 이메일을 보낼 수 있습니다. 향후 버전에서 받은 의견이나 제안에 따라 수정하고, 설문조사에서 건설적인 제안에 기여한 독자들에게 인정한다.

**로그 업데이트**. 이 부분에서는 arXiv에 대한 이 설문 조사의 제출에 대한 업데이트 로그를 정기적으로 유지한다.

* 2023년 3월 31일 첫 릴리스: 초기 버전입니다.
* 2023년 4월 9일 업데이트: 소속 정보 추가, 그림 3 및 표 1 수정 및 LLMs에 대한 해당 선택 기준 명확화, 쓰기 개선 및 일부 사소한 오류 수정.
* 2023년 4월 11일에 업데이트: 라이브러리 리소스에 대한 오류를 수정합니다.
* 2023년 4월 12일 업데이트: 그림 3 및 표 1을 수정하고 LLM의 출시 날짜를 명확히 합니다.
* 2023년 4월 16일 업데이트: GPT 시리즈 모델의 기술 진화에 대한 새로운 섹션 2.2를 추가합니다.
* 2023년 4월 24일 업데이트: 스케일링 법칙에 대한 논의를 추가하고 창발 능력에 대한 모델 크기에 대한 몇 가지 설명을 추가합니다(섹션 2.1); 그림 9의 다양한 아키텍처에 대한 주의 패턴에 대한 예시적인 그림을 추가하고 표 6의 세부 공식을 추가합니다.
* 2023년 4월 25일 업데이트: 그림 및 테이블의 일부 복사 오류를 수정합니다.
* 2023년 4월 27일 업데이트: 섹션 5.3에서 효율적인 튜닝을 추가합니다.
* 2023년 4월 28일 업데이트: 섹션 5.3을 수정합니다.
* 2023년 5월 7일 업데이트: 표 1, 표 2 및 일부 사소한 사항을 수정합니다.
* Update on June 29, 2023 (major revision):
* 섹션 1: arXiv에서 출판된 LLM 논문의 동향에 대해 그림 1을 추가함;
* 섹션 2: GPT의 진화 및 대응하는 논의를 위해 도 4를 추가한다;
* 섹션 3: LLMA 패밀리에 대한 도 5 및 대응하는 논의를 추가함;
* 섹션 5: 섹션 5.1.1의 명령어 튜닝의 합성 데이터 포맷팅, 섹션 5.1.4의 명령어 튜닝에 대한 경험적 분석, 섹션 5.3의 파라미터-효율적 모델 적응 및 섹션 5.4의 메모리-효율적 적응에 대한 최신 논의를 추가함;
* 섹션 6: 섹션 6.4에서 복잡한 과제 해결을 위한 계획인 LCL 6.2.3의 기본 메커니즘에 대한 최신 논의를 추가합니다.
* 섹션 7: LLMs의 고급 능력을 평가하기 위한 대표적인 데이터 세트에 대한 업데이트 표 14 및 섹션 7.4의 경험적 능력 평가;
* 섹션 6.1.1: 프롬프트 설계를 추가합니다.
* 섹션 8: 재무 및 과학 연구 도메인에서 LLM의 적용에 대한 논의를 추가함;
* Update on September 10, 2023 (major revision):
* 이 문서의 그림 및 표의 저작권을 청구합니다.
* 섹션 3, 섹션 4, 섹션 5, 섹션 6 및 섹션 7에서 최신 LLM, 기술 및 그 설명을 추가합니다.
* 섹션 4: 섹션 4.2.5의 디코딩 전략에 대한 최신 논의를 추가함;
* 섹션 5: 섹션 5.1.2에서 명령어 튜닝을 위한 실용적인 요령, 섹션 5.1.4에서 명령어 튜닝을 위한 LLMA(13B)에 대한 경험적 분석, 섹션 5.2.3에서 RLHF에 대한 실용적인 전략, 섹션 5.2.4에서 RLHF가 없는 정렬 및 섹션 5.2.5에서 SFT 및 RLHF에 대한 비고를 추가함;
* 섹션 6: 섹션 6.4에서 복잡한 과제 해결을 위한 계획에 대한 내용을 업데이트하고;
* 7절: 기존 평가 작업의 범주에 대해 7.3.2절, 표 15절의 평가 접근법에 대한 논의를 추가하고, 7.4절의 경험적 능력 평가와 표 16의 결과를 업데이트하고;
* 섹션 6.1.1: 표 12에서 새로운 프롬프트 예를 추가함;
* Update on November 23, 2023 (this version):
* 섹션 1: 4세대 언어 모델의 진화 프로세스에 대해 그림 2를 추가함;
* 섹션 2: 스케일링 법칙 및 출현 능력이 스케일링 법칙과 어떻게 관련되는지에 대한 더 많은 논의를 추가함;
* 섹션 3: 그림 3의 최신 LLMs 및 표 1, 섹션 3.1의 최신 API, 일반적으로 사용되는 데이터 세트 추가
3.3절의 명령어 튜닝 및 정렬 튜닝, 3.4절의 여러 라이브러리에 대해;
* 섹션 4: 섹션 4.1.3의 데이터 혼합물 및 데이터 커리큘럼을 포함하는 데이터 스케줄링에 대한 최신 논의 추가; 섹션 4.1.4의 데이터 준비 요약 추가; 섹션 4.2.4의 긴 컨텍스트 모델링에 대한 논의 추가; 디코딩 효율 문제에 대한 논의 추가 및 섹션 4.2.5의 최신 디코딩 전략 추가;
* 섹션 5: 섹션 5.1의 인스턴스 구성 및 튜닝 전략에 대한 최신 토론 추가; 섹션 5.2.3의 프로세스 감독 RLHF에 대한 최신 토론 추가 및 섹션 5.4.3의 양자화된 LLaMA 모델(7B 및 13B)에 대한 경험적 연구;
* 섹션 6: 섹션 6.1.2에서 프롬프트 최적화에 대한 최신 논의를 추가하고, 섹션 6.3에서 연쇄적 사고 프롬프트에 대한 콘텐츠를 업데이트하고;
* 섹션 8: 섹션 8.1의 연구 방향을 위해 LLM에 대한 최신 논의를 추가합니다.
* 섹션 9: 여러 측면에서 내용을 수정합니다.

**콘텐츠 계획** 이 설문 조사에 새 콘텐츠를 정기적으로 포함하여 보다 자체적이고 최신 상태로 만들 것입니다. 여기에서 다음 주요 버전에서 나타날 수 있는 몇 가지 잠재적인 주제를 나열한다: (1) 수업 조정과 능력 평가를 위해 더 큰 언어 모델을 사용한 더 많은 실험; (2) 더 자세한 프롬프트 연습; (3) 훈련 레시피; (4) 더 많은 이론적 분석과 토론; (5) 응용 프로그램에 대한 더 많은 토론.

**실험에 대한 설명**. 이 버전에서는 수업 조정(표 9), 전반적인 능력 평가(표 16), 프롬프트 엔지니어링(표 17)에 대한 수많은 실험을 포함했다. 계산 자원의 한계로 인해 우리의 실험은 완전하지 않으며 작은 크기의 모델이나 몇 가지 비교에 국한된다. 그럼에도 불구하고, 우리는 부분적인 결과를 대중에게 공유하는 것이 의미가 있을 수 있다고 생각합니다. 우리는 더 큰 모델의 결측 결과나 더 많은 비교 결과를 미래 버전에 포함시키려고 노력할 것이다. **또한 보다 포괄적인 실험을 수행하기 위해 컴퓨팅 파워의 지원을 요청합니다.* *

**중국어 버전**. 또한 링크에서 이 설문지의 번역된 중국어 버전(첫 번째 릴리스에 해당)을 제공합니다. [https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf](https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf). 자원봉사자 4명은 내용을 점검하고 수정하는 데 기여하며, 이웬후, 신덩, 신명호우, 옌빈음, 잔쇼조(기여순)이다. 중국어 버전도 계속 업데이트하겠지만 최신 영어 버전만큼 시기적절하지는 않을 수 있습니다.

## Acknowledgments

저자들은 이 논문을 교정해 준 얀카이 린과 유타오 주에게 감사하고 싶다. 이 논문이 처음 발표된 이후, 우리는 독자들로부터 많은 귀중한 논평을 받았습니다. 타일러 수어드, 다마이 다이, 량 딩, 스텔라 바이더만, 케빈 그레이, 제이 알라마르, 유보 펑, 마크 홀름스트롬, 싱동 류, 오일석, 이팅 류, 샤오준 왕, 가오얀 우, 토드 모릴, 하오 류, 젠유 장, 신린 장 등 건설적인 제안과 논평으로 저희에게 편지를 보내 주신 독자들에게 진심으로 감사드립니다.

v11 버전(2023년 6월 29일) 이후로 우리는 많은 수의 실험과 신속한 관행을 추가했다. 이러한 새로운 콘텐츠는 우리 팀의 많은 자원 봉사자에 의해 완성됩니다. 여기, 우리는 이 부분에 대해 매우 열심히 노력한 모든 학생들에게 감사를 표하기 위해 특별한 부분을 추가합니다(우리 작가 목록에 있는 학생들도 포함).

**실험에 대한 기여**. 표 16과 같은 실험에 참여하신 여러분들의 노고에 진심으로 감사드립니다.

\(\bullet\) Xiaoxue Cheng: 언어 생성과 HaluEval 과제에 대한 평가를 위한 실험을 구현하였다.

\(\bullet\) Yuhao Wang: 환경 과제와의 상호 작용에 대한 평가를 위한 실험을 구현한다.

\(\bullet\) Bowen Zheng: 도구 조작 작업에 대한 평가를 위한 실험을 구현한다.

**팁에 대한 기여**. 우리는 표 12의 프롬프트를 설계하기 위한 제공된 팁의 해당 수에 대한 기여에 대해 다음 사람을 나열한다.

\(\bullet\) Xiaolei Wang: T3, O3

\(\총구\) 베이첸 장: D2, D5

\(\bullet\) Zhipeng Chen : D3, D4

\(\bullet\) Junjie Zhang: D6

\(\bullet\) Bowen Zheng: D7

\(\bullet\) Zican Dong: D8

\(\총알\) 신유탕: C2

\(\bullet\) Yifan Du: T4

\(\bullet\)Tianyi Tang : O6, O7, D9

\(\bullet\) 유풍호우: O8, C3

\(\bullet\) Salvatore Raieli: C4

## References

* [1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, "A neural probabilistic language model," _J. Mach. Learn. Res._, vol. 3, pp. 1137-1155, 2003.
* [2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa, "Natural language processing (almost) from scratch," _J. Mach. Learn. Res._, vol. 12, pp. 2493-2537, 2011.
* [3] S. Pinker, _The Language Instinct: How the Mind Creates Language_. Brilliance Audio; Unabridged edition, 2014.
* [4] M. D. Hauser, N. Chomsky, and W. T. Fitch, "The faculty of language: what is it, who has it, and how did it evolve?" _science_, vol. 298, no. 5598, pp. 1569-1579, 2002.
* [5] A. M. Turing, "Computing machinery and intelligence," _Mind_, vol. LIX, no. 236, pp. 433-460, 1950.
* [6] F. Jelinek, _Statistical Methods for Speech Recognition_. MIT Press, 1998.
* [7] J. Gao and C. Lin, "Introduction to the special issue on statistical language modeling," _ACM Trans. Asian Lang. Inf. Process._, vol. 3, no. 2, pp. 87-93, 2004.
* [8] R. Rosenfeld, "Two decades of statistical language modeling: Where do we go from here?" _Proceedings of the IEEE_, vol. 88, no. 8, pp. 1270-1278, 2000.

A. Stolcke, "Srilm-an extensible language modeling toolkit" in _7th international conference on spoken language processing_, 2002.
* [10] X. Liu and W. B. Croft, "Statistical language modeling for information retrieval," _Annu. Rev. Inf. Sci. Technol._, vol. 39, no. 1, pp. 1-31, 2005.
* [11] C. Zhai, _Statistical Language Models for Information Retrieval_, ser. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, 2008.
* [12] S. M. Thede and M. P. Harper, "A second-order hidden markov model for part-of-speech tagging," in _27th Annual Meeting of the Association for Computational Linguistics, University of Maryland, College Park, Maryland, USA, 20-26 June 1999_, R. Dale and K. W. Church, Eds. ACL, 1999, pp. 175-182.
* [13] L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer, "A tree-based statistical language model for natural language speech recognition," _IEEE Transactions on Acoustics, Speech, and Signal Processing_, vol. 37, no. 7, pp. 1001-1008, 1989.
* [14] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean, "Large language models in machine translation," in _EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic_, J. Eisner, Ed. ACL, 2007, pp. 858-867.
* [15] S. M. Katz, "Estimation of probabilities from sparse data for the language model component of a speech recognizer," _IEEE Trans. Acoust. Speech Signal Process._, vol. 35, no. 3, pp. 400-401, 1987.
* [16] W. A. Gale and G. Sampson, "Good-turing frequency estimation without tears," _J. Quant. Linguistics_, vol. 2, no. 3, pp. 217-237, 1995.
* [17] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur, "Recurrent neural network based language model," in _INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010_, T. Kobayashi, K. Hirose, and S. Nakamura, Eds. ISCA, 2010, pp. 1045-1048.
* [18] S. Kombrink, T. Mikolov, M. Karafiat, and L. Burget, "Recurrent neural network based language modeling in meeting recognition," in _INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association, Florence, Italy, August 27-31, 2011_. ISCA, 2011, pp. 2877-2880.
* [19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," in _Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States_, C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013, pp. 3111-3119.
* [20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," in _1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings_, Y. Bengio and Y. LeCun, Eds., 2013.
* [21] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, "Deep contextualized word representations," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)_, M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 2227-2237.
* [22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, 2017, pp. 5998-6008.
* [23] J. Devlin, M. Chang, K. Lee, and K. Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4171-4186.
* [24] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, 2020, pp. 7871-7880.
* [25] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," _J. Mach. Learn. Res_, pp. 1-40, 2021.
* [26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, p. 9, 2019.
* [27] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized BERT pretraining approach," _CoRR_, vol. abs/1907.11692, 2019.
* [28] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechala, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, "Multitask prompted training enables zero-shot task generalization," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [29] T. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W. Chung, I. Beltagy, J. Launay, and C. Raffel, "What language model architecture and pretraining objective works best for zero-shot generalization?" in _International Conference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA_, ser. Proceedings of Machine Learning Research, vol. 162, 2022, pp. 22 964-22 984.
* [30] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," _CoRR_, vol. abs/2001.08361, 2020.
* [31] J. Wei, Y. Tay, R. Bommassani, C. Raffel, B. Zoph, S. Borgoard, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, "Emergent abilities of large language models," _CoRR_, vol. abs/2206.07682, 2022.
* [32] M. Shanahan, "Talking about large language models," _CoRR_, vol. abs/2212.03551, 2022.
* [33] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou, "Chain of thought prompting elicits reasoning in large language models," _CoRR_, vol. abs/2201.11903, 2022.
* [34] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, "Training compute-optimal large language models," vol. abs/2203.15556, 2022.
* [35] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, "Galactica: A large language model for science," _CoRR_, vol. abs/2211.09085, 2022.
* [36] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," _ACM Comput. Surv._, pp. 195:1-195:35, 2023.
* [37] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He, H. Peng, J. Li, J. Wu, Z. Liu, P. Xie, C. Xiong, J. Pei, P. S. Yu, and L. Sun, "A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt," _CoRR_, vol. abs/2302.09419, 2023.
* [38] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang, W. Han, M. Huang, Q. Jin, Y. Lan, Y. Liu, Z. Liu, Z. Lu, X. Qiu, R. Song, J. Tang, J. Wen, J. Yuan, W. X. Zhao, and J. Zhu, "Pre-trained models: Past, present and future," _AI Open_, vol. 2, pp. 225-250, 2021.
* [39] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," _CoRR_, vol. abs/2003.08271, 2020.
* [40] S. Altman, "Planning for agi and beyond," _OpenAI Blog_, February 2023.
* [41] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, "Sparks of artificial general intelligence: Early experiments with gpt-4," vol. abs/2303.12712, 2023.
* [42] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei, "Language is not all you need: Aligning perception with language models," _CoRR_, vol. abs/2302.14045, 2023.
* [43] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt," _arXiv preprint arXiv:2303.04226_, 2023.
* [44] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu _et al._, "Palm-e: An embodied multimodal language model," _arXiv preprint arXiv:2303.03378_, 2023.
* [45] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, "Visual chatgpt: Talking, drawing and editing with visual foundation models," _arXiv preprint arXiv:2303.04671_, 2023.
* [46] OpenAI, "Gpt-4 technical report," _OpenAI_, 2023.
* [47] Y. Fu, H. Peng, and T. Khot, "How does gpt obtain its ability? tracing emergent abilities of language models to their sources," _Yao Fu's Notion_, Dec 2022.
* [48] J. Li, T. Tang, W. X. Zhao, and J. Wen, "Pretrained language model for text generation: A survey," in _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, Z. Zhou, Ed. ijcai.org, 2021, pp. 4492-4499.
* [49] P. Lu, L. Qiu, W. Yu, S. Welleck, and K. Chang, "A survey of deep learning for mathematical reasoning," _CoRR_, vol. abs/2212.10535, 2022.
* [50] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li, and Z. Sui, "A survey for in-context learning," _CoRR_, vol. abs/2301.00234, 2023.
* [51] J. Huang and K. C. Chang, "Towards reasoning in large language models: A survey," _CoRR_, vol. abs/2212.10403, 2022.
* [52] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang, and H. Chen, "Reasoning with language model prompting: A survey," _CoRR_, vol. abs/2212.09597, 2022.
* [53] J. Zhou, P. Ke, X. Qiu, M. Huang, and J. Zhang, "Chatgpt: potential, prospects, and limitations," in _Frontiers of Information Technology & Electronic Engineering_, 2023, pp. 1-6.
* [54] W. X. Zhao, J. Liu, R. Ren, and J. Wen, "Dense text retrieval based on pretrained language models: A survey," _CoRR_, vol. abs/2211.14876, 2022.
* [55] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few-shot learners," in _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020.
* [56] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsyvashchenko, J. Maynez, A. Rao, P. Barnes,Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pilai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, "Palm: Scaling language modeling with pathways," _CoRR_, vol. abs/2204.02311, 2022.
* [57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient foundation language models," _CoRR_, 2023.
* [58] T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray _et al._, "Scaling laws for autoregressive generative modeling," _arXiv preprint arXiv:2010.14701_, 2020.
* [59] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, and A. W. Yu, "Doremi: Optimizing data mixtures speeds up language model pretraining," _arXiv preprint arXiv:2305.10429_, 2023.
* [60] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho, "Will we run out of data? an analysis of the limits of scaling datasets in machine learning," _CoRR_, vol. abs/2211.04325, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2211.04325](https://doi.org/10.48550/arXiv.2211.04325)
* [61] N. Muennighoff, A. M. Rush, B. Barak, T. L. Scao, A. Piktus, N. Tazi, S. Pyysalo, T. Wolf, and C. Raffel, "Scaling data-constrained language models," _arXiv preprint arXiv:2305.16264_, 2023.
* [62] I. McKenzie, A. Lyzhov, A. Parrish, A. Prabhu, A. Mueller, N. Kim, S. Bowman, and E. Perez, "The inverse scaling prize," 2022. [Online]. Available: [https://github.com/inverse-scaling/prize](https://github.com/inverse-scaling/prize)
* [63] B. A. Huberman and T. Hogg, "Phase transitions in artificial intelligence systems," _Artificial Intelligence_, vol. 33, no. 2, pp. 155-171, 1987.
* [64] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, H. F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassier, R. Powell, G. van den Driessche, L. A. Hendricks, M. Raub, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, "Scaling language models: Methods, analysis & insights from training gopher," _CoRR_, vol. abs/2112.11446, 2021.
* [65] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei, "Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers," _CoRR_, vol. abs/2212.10559, 2022.
* [66] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," _CoRR_, vol. abs/2203.02155, 2022.
* [67] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Fine-tuned language models are zero-shot learners," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [68] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C. Chang, I. Krivokon, W. Rusch, M. Pickett, K. S. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Sorkaer, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. H. Chi, and Q. Le, "Lambda: Language models for dialog applications," _CoRR_, vol. abs/2201.08239, 2022.
* [69] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Y. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei, "Scaling instruction-finetuned language models," _CoRR_, vol. abs/2210.11416, 2022.
* [70] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tzarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane, A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlmuller, A. M. Dai, A. La, A. K. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas, and et al., "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," _CoRR_, vol. abs/2206.04615, 2022.
* [71] R. Schaeffer, B. Miranda, and S. Koyejo, "Are emergent abilities of large language models a mirage?" _arXiv preprint arXiv:2304.15004_, 2023.

* [72] S. Hu, X. Liu, X. Han, X. Zhang, C. He, W. Zhao, Y. Lin, N. Ding, Z. Ou, G. Zeng, Z. Liu, and M. Sun, "Unlock predictable scaling from emergent abilities," 2023.
* [73] A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra, "Grokking: Generalization beyond overfitting on small algorithmic datasets," _arXiv preprint arXiv:2201.02177_, 2022.
* [74] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters," in _KDD_, 2020, pp. 3505-3506.
* [75] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-Im: Training multi-billion parameter language models using model parallelism," _CoRR_, vol. abs/1909.08053, 2019.
* [76] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia, "Efficient large-scale language model training on GPU clusters using megatron-lm," in _International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Missouri, USA, November 14-19, 2021_. ACM, 2021, p. 58.
* [77] V. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Anderson, M. Shoeybi, and B. Catanzaro, "Reducing activation recomputation in large transformer models," _CoRR_, vol. abs/2205.05198, 2022.
* [78] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamachi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurencon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simh, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emeze, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al., "BLOOM: A 176b-parameter open-access multilingual language model," _CoRR_, vol. abs/2211.05100, 2022.
* [79] P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, "Deep reinforcement learning from human preferences," in _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 4299-4307.
* [80] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," _CoRR_, vol. abs/2302.04761, 2023.
* [81] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman, "Webgpt: Browser-assisted question-answering with human feedback," _CoRR_, vol. abs/2112.09332, 2021.
* [82] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," _J. Mach. Learn. Res._, pp. 140:1-140:67, 2020.
* [83] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, "mt5: A massively multilingual pre-trained text-to-text transformer," in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, 2021, pp. 483-498.
* [84] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, C. Li, Z. Gong, Y. Yao, X. Huang, J. Wang, J. Yu, Q. Guo, Y. Yu, Y. Zhang, J. Wang, H. Tao, D. Yan, Z. Yi, F. Peng, F. Jiang, H. Zhang, L. Deng, Y. Zhang, Z. Lin, C. Zhang, S. Zhang, M. Guo, S. Gu, G. Fan, Y. Wang, X. Jin, Q. Liu, and Y. Tian, "Pangu-\(\alpha\): Large-scale autoregressive pretrained chinese language models with auto-parallel computation," _CoRR_, vol. abs/2104.12369, 2021.
* [85] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke, Y. Cai, G. Zeng, Z. Tan, Z. Liu, M. Huang, W. Han, Y. Liu, X. Zhu, and M. Sun, "CPM-2: large-scale cost-effective pre-trained language models," _CoRR_, vol. abs/2106.10715, 2021.
* [86] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, "Codegen: An open large language model for code with multi-turn program synthesis," _arXiv preprint arXiv:2203.13474_, 2022.
* [87] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach, "Gpt-neox-20b: An open-source autoregressive language model," _CoRR_, vol. abs/2204.06745, 2022.
* [88] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. R. A, S. Patro, T. Divit, and X. Shen, "Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, 2022, pp. 5085-5109.
* [89] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, H. Zheng, D. Zhou, N. Houlsby, and D. Metzler, "U12: Unifying language learning paradigms," 2022.
* [90] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. T. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,"OPT: open pre-trained transformer language models," _CoRR_, vol. abs/2205.01068, 2022.
* [91] M. R. Costa-jussa, J. Cross, O. Celebi, M. Elbayad, K. Heafield, K. Heffern, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzman, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang, "No language left behind: Scaling human-centered machine translation," _CoRR_, vol. abs/2207.04672, 2022.
* [92] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li _et al._, "Codgegex: A pre-trained model for code generation with multilingual evaluations on humaneval-x," _arXiv preprint arXiv:2303.17568_, 2023.
* [93] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, P. Zhang, Y. Dong, and J. Tang, "GLM-130B: an open bilingual pre-trained model," vol. abs/2210.02414, 2022.
* [94] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z. X. Yong, H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie, Z. Alyafeai, A. Webson, E. Raff, and C. Raffel, "Crosslingual generalization through multitask finentuning," _CoRR_, vol. abs/2211.01786, 2022.
* [95] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, B. O'Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and V. Stoyanov, "OPT-IML: scaling language model instruction meta learning through the lens of generalization," _CoRR_, vol. abs/2212.12017, 2022.
* [96] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff _et al._, "Pythia: A suite for analyzing large language models across training and scaling," _arXiv preprint arXiv:2304.01373_, 2023.
* [97] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, "Codegen2: Lessons for training lms on programming and natural languages," _CoRR_, vol. abs/2305.02309, 2023.
* [98] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocektor, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. M. V, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timov, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries, "Starcoder: may the source be with you!" _CoRR_, vol. abs/2305.06161, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.06161](https://doi.org/10.48550/arXiv.2305.06161)
* [99] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale _et al._, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.
* [100] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang _et al._, "Baichuan 2: Open large-scale language models," _arXiv preprint arXiv:2309.10305_, 2023.
* [101] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang _et al._, "Qwen technical report," _arXiv preprint arXiv:2309.16609_, 2023.
* [102] X. Li, Y. Yao, X. Jiang, X. Fang, X. Meng, S. Fan, P. Han, J. Li, L. Du, B. Qin _et al._, "Flim-101b: An open llm and how to train it with 5100 k budget," _arXiv preprint arXiv:2309.03852_, 2023.
* [103] T. Wei, L. Zhao, L. Zhang, B. Zhu, L. Wang, H. Yang, B. Li, C. Cheng, W. Lu, R. Hu _et al._, "Skywork: A more open bilingual foundation model," _arXiv preprint arXiv:2310.19341_, 2023.
* [104] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic sharding," in _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.
* [105] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tilllet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," _CoRR_, vol. abs/2107.03374, 2021.
* [107] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang -J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu, W. Gong, J. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, "ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation," _CoRR_, vol. abs/2107.02137, 2021.
* [108] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, "Jurassic-1: Technical details and evaluation," _White Paper. Al21 Labs_, vol. 1, 2021.
* [109] B. Kim, H. Kim, S. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park, S. Kim, S. Kim, D. Seo, H. Lee, M. Jeong, S. Lee, M. Kim, S. Ko, S. Kim, T. Park, J. Kim, S. Kang, N. Ryu, K. M. Yoo, M. Chang, S. Suh, S. In, J. Park, K. Kim, H. Kim, J. Jeong, Y. G. Yeo, D. Ham, D. Park, M. Y. Lee, J. Kang, I. Kang, J. Ha, W. Park, and N. Sung, "What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cama, Dominican Republic, 7-11 November, 2021_. Association for Computational Linguistics, 2021.
* [109] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo, L. Xu _et al._, "Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning," _arXiv preprint arXiv:2110.04725_, 2021.
* [110] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-Sarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, "A general language assistant as a laboratory for alignment," _CoRR_, vol. abs/2112.00861, 2021.
* [111] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang, Y. Zhao, C. Pang, J. Liu, X. Chen, Y. Lu, W. Liu, X. Wang, Y. Bai, Q. Chen, L. Zhao, S. Li, P. Sun, D. Yu, Y. Ma, H. Tian, H. Wu, T. Wu, W. Zeng, G. Li, W. Gao, and H. Wang, "ERNIE 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation," _CoRR_, vol. abs/2112.12731, 2021.
* [112] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui, "Glam: Efficient scaling of language models with mixture-of-experts," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022, pp. 5547-5569.
* [113] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, E. Zheng, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi, Y. He, M. Houston, S. Tiwary, and B. Catanzaro, "Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model," _CoRR_, vol. abs/2201.11990, 2022.
* [114] Y. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gilmeno, A. D. Lago, T. Hubert, P. Choy, C. de Masson d'Autume, I. Babuschkin, X. Chen, P. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals, "Competition-level code generation with alphacode," _Science_, 2022.
* [115] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, C. S. Prakash, M. Sridhar, F. Triefenbach, A. Verma, G. Tur, and P. Natarajan, "Alexattn 20b: Few-shot learning using a large-scale multilingual seq2seq model," _CoRR_, vol. abs/2208.01448, 2022.
* [116] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham, J. Uessato, P. Huang, R. Comanescu, F. Yang, A. See, S. Dathathtri, R. Greig, C. Chen, D. Fritz, J. S. Elias, R. Green, S. Mokra, N. Fernando, B. Wu, R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis, K. Kavukcuoglu, L. A. Hendricks, and G. Irving, "Improving alignment of dialogue agents via targeted human judgements," _CoRR_, vol. abs/2209.14375, 2022.
* [117] H. Su, X. Zhou, H. Yu, Y. Chen, Z. Zhu, Y. Yu, and J. Zhou, "Welm: A well-read pre-trained language model for chinese," _CoRR_, vol. abs/2209.10372, 2022.
* [118] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng, J. Rao, A. Chowdhery, D. Zhou, D. Metzler, S. Petrov, N. Houlsby, Q. V. Le, and M. Dehghani, "Transcending scaling laws with 0.1% extra compute," _CoRR_, vol. abs/2210.11399, 2022.
* [119] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, A. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao, "Pangu-\(\Sigma\): Towards trillion parameter language model with sparse heterogeneous computing," _CoRR_, vol. abs/2303.10845, 2023.
* [120] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen _et al._, "Palm 2. technical report," _arXiv preprint arXiv:2305.10403_, 2023.
* [121] A. Radford, R. Jozefowicz, and I. Sutskever, "Learning to generate reviews and discovering sentiment," _CoRR_, vol. abs/1704.01444, 2017.
* [122] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever _et al._, "Improving language understanding by generative pre-training," 2018.
* [123] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, "The natural language decathlon: Multitask learning as question answering," _CoRR_, vol. abs/1806.08730, 2018.
* [124] Y. Zhang, S. Sun, M. Galley, Y. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan, "DIALOGPT : Large-scale generative pre-training for conversational response generation," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020_, A. Celikyilmaz and T. Wen, Eds. Association for Computational Linguistics, 2020, pp. 270-278.
* [125] D. Ham, J. Lee, Y. Jang, and K. Kim, "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_. Association for Computational Linguistics, 2020, pp. 583-592.
* [126] I. Drori, S. Tran, R. Wang, N. Cheng, K. Liu, L. Tang, E. Ke, N. Singh, T. L. Patti, J. Lynch, A. Shporer, N. Verma, E. Wu, and G. Strang, "A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more," _CoRR_, vol. abs/2112.15594, 2021.
* [127] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, J. Heidecke, P. Shyam, B. Power, T. E. Nekoul,G. Sastry, G. Krueger, D. Schnurr, F. P. Such, K. Hsu, M. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng (2022) Text and code embeddings by contrastive pre-training. CoRRabs/2201.10005. External Links: Link, 2012.10005 Cited by: SS1.
*[130]J. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 인용: SS1.
* [131]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
* [132]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 인용: SS1.
*[133]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[134]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[135]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[136]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[137]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[138]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
* [139]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[140]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[141]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
* [142]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[143]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[144]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[145]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[146]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[147]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[148]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 인용: SS1.
* [149]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 인용: SS1.
*[150]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[151]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[152]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 인용: SS1.
*[153]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2010.0005. External Links: Link, 1709.01325 Cited by: SS1.
*[154]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[155]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.
*[156]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
* [157]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 인용: SS1.
*[158]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.
* [159]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
* [160]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.
*[161]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.
* [162]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.
*[163]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov (2017) 심층 신경망의 성능 개선 및 최적화. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.
*[164]S. 설만 오양지글러 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.
*[165]S. 슐만, F. 월스키, P. 다리왈, A. 래드포드, O. Klimov(2017) Deep Neural NetworksOpenWebTextCorpus, 2019의 성능 개선 및 최적화.
* [158] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn, "The pushshift reddit dataset," in _Proceedings of the Fourteenth International AAAI Conference on Web and Social Media, ICWSM 2020, Held virtually, Original Venue: Atlanta, Georgia, USA, June 8-11, 2020_. AAAI Press, 2020, pp. 830-839.
* [159] "Wikipedia." [Online]. Available: [https://en.wikipedia.org/wiki/Main_Page](https://en.wikipedia.org/wiki/Main_Page)
* [160] "Bigquery dataset." [Online]. Available: [https://cloud.google.com/bigquery?hl=zh-cn](https://cloud.google.com/bigquery?hl=zh-cn)
* [161] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy, "The pile: An 800gb dataset of diverse text for language modeling," _CoRR_, vol. abs/2101.00027, 2021.
* [162] H. Laurencon, L. Saulnier, T. Wang, C. Akiki, A. V. del Moral, T. Le Scao, L. Von Werra, C. Mou, E. G. Ponferrada, H. Nguyen _et al._, "The bigscience roots corpus: A 1.6 tb composite multilingual dataset," in _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [163] "Common crawl." [Online]. Available: [https://commoncrawl.org/](https://commoncrawl.org/)
* [164] "A reproduction version of cc-stories on hugging face." [Online]. Available: [https://huggingface.co/datasets/spacemanidol/cc-stories](https://huggingface.co/datasets/spacemanidol/cc-stories)
* [165] B. Wang and A. Komatsuzaki, "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model," [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax), 2021.
* [166] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, "Cross-task generalization via natural language crowdsourcing instructions," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds., 2022, pp. 3470-3487.
* [167] S. H. Bach, V. Sanh, Z. X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, Z. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David, C. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S. AlShaibani, S. Sharma, U. Thakker, K. Almubarak, X. Tang, D. R. Radev, M. T. Jiang, and A. M. Rush, "Promptsource: An integrated development environment and repository for natural language prompts," in _ACL (demo)_. Association for Computational Linguistics, 2022, pp. 93-104.
* [168] T. Tang, J. Li, W. X. Zhao, and J. Wen, "MVP: multi-task supervised pre-training for natural language generation," _CoRR_, vol. abs/2206.12131, 2022.
* [169] H. Nguyen, S. Suri, K. Tsui, Shahules786, T. team, and C. Schuhmann, "The oig dataset," [https://laion.ai/blog/oig-dataset/](https://laion.ai/blog/oig-dataset/), 2023.
* [170] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. E. Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan, "Training a helpful and harmless assistant with reinforcement learning from human feedback," _CoRR_, vol. abs/2204.05862, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2204.05862](https://doi.org/10.48550/arXiv.2204.05862)
* [171] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, "How close is chatgpt to human experts? comparison corpus, evaluation, and detection," _arXiv preprint arXiv:2301.07597_, 2023.
* [172] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. (2023) Free dolly: Introducing the world's first truly open instruction-tuned llm.
* [173] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi _et al._, "Openassistant conversations-democratizing large language model alignment," _arXiv preprint arXiv:2304.07327_, 2023.
* 자연 언어 적응형 상황 인식 전언어 출력을 위한 생성 범용 비서," [https://guanaco-model.github.io/](https://guanaco-model.github.io/), 2023.
* [175] C. Xu, D. Guo, N. Duan, and J. McAuley, "Baize: An open-source chat model with parameter-efficient tuning on self-chat data," _arXiv preprint arXiv:2304.01196_, 2023.
* [176] Y. Ji, Y. Gong, Y. Deng, Y. Peng, Q. Niu, B. Ma, and X. Li, "Towards better instruction following language models for chinese: Investigating the impact of training data and evaluation," _arXiv preprint arXiv:2304.07854_, 2023.
* [177] K. Ethayarajh, Y. Choi, and S. Swayamdipta, "Understanding dataset difficulty with \(\mathcal{V}\)-usable information," in _Proceedings of the 39th International Conference on Machine Learning_, 2022, pp. 5988-6008.
* [178] N. Lambert, L. Tunstall, N. Rajani, and T. Thrush. (2023) Huggingface h4 stack exchange preference dataset. [Online]. Available: [https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)
* [179] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi, "Training socially aligned language models in simulated human society," _CoRR_, vol. abs/2305.16960, 2023.
* [180] G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P. Yi, X. Gao, J. Sang, R. Zhang, J. Zhang, C. Peng, F. Huang, and J. Zhou, "Cvalues: Measuring the values of chinese large language models from safety to responsibility," 2023.
* [181] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang, "Safe rlhf: Safe reinforcement learning from human feedback," _arXiv preprint arXiv:2310.12773_, 2023.
* [182] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawdden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, "Multitask prompted training enables zero-shot task generalization," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [183] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei _et al._, "The fan collection: Designing data and methods for effective instruction tuning," _arXiv preprint arXiv:2301.13688_, 2023.
* [184] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, "Training verifiers to solve math word problems," _CoRR_, vol. abs/2110.14168, 2021.
* [185] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant, "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 346-361, 2021.
* [186] O. Camburu, B. Shillingford, P. Minervini, T. Lukasiewicz, and P. Blunsom, "Make up your mind! adversarial generation of inconsistent natural language explanations," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 4157-4165.
* Demos, Online, November 16-20, 2020_. Association for Computational Linguistics, 2020, pp. 38-45.
* [188] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, "JAX: composable transformations of Python+NumPy programs," 2018. [Online]. Available: [http://github.com/google/jax](http://github.com/google/jax)
* [189] Z. Bian, H. Liu, B. Wang, H. Huang, Y. Li, C. Wang, F. Cui, and Y. You, "Colossal-ai: A unified deep learning system for large-scale parallel training," _CoRR_, vol. abs/2110.14883, 2021.
* [190] J. Fang, Y. Yu, S. Li, Y. You, and J. Zhou, "Patrickar: Parallel training of pre-trained models via a chunk-based memory management," _CoRR_, vol. abs/2108.05818, 2021.
* [191] "Bmtrain: Effient training for big models." [Online]. Available: [https://github.com/OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain)
* [192] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, "Fastmoe: A fast mixture-of-expert training system," _CoRR_, vol. abs/2103.13262, 2021.
* [193] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, "Efficient memory management for large language model serving with pagedattention," in _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_, 2023.
* [194] (2023) Deepspeed-mi. [Online]. Available: [https://github.com/microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)
* [195] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulinier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mistral 7b," 2023.
* [196] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A. Awan, J. Rasley, M. Zhang, C. Li, C. Holmes, Z. Zhou, M. Wyatt, M. Smith, L. Kurilenko, H. Qin, M. Tanaka, S. Che, S. L. Song, and Y. He, "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales," _arXiv preprint arXiv:2308.01320_, 2023.
* [197] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, "Pytorch: An imperative style, high-performance deep learning library," in _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 8024-8035.
* [198] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, "Tensorflow: A system for large-scale machine learning," in _12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016_, K. Keeton and T. Roscoe, Eds. USENIX Association, 2016, pp. 265-283.
* [199] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang, "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems," _CoRR_, vol. abs/1512.01274, 2015.
* [200] Y. Ma, D. Yu, T. Wu, and H. Wang, "Paddlepaddle: An open-source deep learning platform from industrial practice," _Frontiers of Data and Computing_, vol. 1, no. 1, p. 105, 2019.
* [201] J. Yuan, X. Li, C. Cheng, J. Liu, R. Guo, S. Cai, C. Yao, F. Yang, X. Yi, C. Wu, H. Zhang, and J. Zhao, "One-flow: Redesign the distributed deep learning framework from scratch," _CoRR_, vol. abs/2110.15032, 2021.
* 23, 2021_, 2021, pp. 300-325.
* [203] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra, "Solving quantitative reasoning problems with language models," _CoRR_, vol.

abs/2206.14858, 2022.
* [204] T. Saier, J. Krause, and M. Farber, "unaxtive 2022: All arxiv publications pre-processed for nlp, including structured full-text and citation network," _arXiv preprint arXiv:2303.14957_, 2023.
* [205] H. A. Simon, "Experiments with a heuristic compiler," _J. ACM_, vol. 10, no. 4, pp. 493-506, 1963.
* [206] Z. Manna and R. J. Waldinger, "Toward automatic program synthesis," _Commun. ACM_, vol. 14, no. 3, pp. 151-165, 1971.
* [207] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, "Codebert: A pre-trained model for programming and natural languages," in _Findings of EMNLP_, 2020.
* [208] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton, "Program synthesis with large language models," _CoRR_, vol. abs/2108.07732, 2021.
* [209] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow," 2021.
* [210] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, "A systematic evaluation of large language models of code," in _MAPS@PLDI_, 2022.
* [211] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig, "Language models of code are few-shot commonsense learners," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Eminates, December 7-11, 2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics_, 2022, pp. 1384-1403.
* [212] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno _et al._, "A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity," _arXiv preprint arXiv:2305.13169_, 2023.
* [213] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge, D. Gao, Y. Xie, Z. Liu, J. Gao, Y. Li, B. Ding, and J. Zhou, "Data-juicer: A one-stop data processing system for large language models," 2023.
* [214] D. Hernandez, T. B. Brown, T. Conerly, N. DasSarma, D. Drain, S. E. Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume, S. Johnston, B. Mann, C. Olah, C. Olsson, D. Amodei, N. Joseph, J. Kaplan, and S. McCandlish, "Scaling laws and interpretability of learning from repeated data," _CoRR_, vol. abs/2205.10487, 2022.
* [215] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," in _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [216] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, "Deduplicating training data makes language models better," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, 2022, pp. 8424-8445.
* [217] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang, "Quantifying memorization across neural language models," _CoRR_, 2022.
* [218] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel, "Extracting training data from large language models," in _30th IJCSENIX Security Symposium, USENIX Security 2021, August 11-13, 2021_, 2021, pp. 2633-2650.
* [219] N. Kandpal, E. Wallace, and C. Raffel, "Deduplicating training data mitigates privacy risks in language models," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA._ PMLR, 2022, pp. 10 697-10 707.
* July 1, 2001_, C. E. Brodley and A. P. Danyluk, Eds. Morgan Kaufmann, 2001, pp. 282-289.
* [221] P. Gage, "A new algorithm for data compression," C _Users Journal_, vol. 12, no. 2, pp. 23-38, 1994.
* [222] R. Sennrich, B. Haddow, and A. Birch, "Neural machine translation of rare words with subword units," in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers_. The Association for Computer Linguistics, 2016.
* [223] M. Schuster and K. Nakajima, "Japanese and korean voice search," in _2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)_. IEEE, 2012, pp. 5149-5152.
* [224] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, "Google's neural machine translation system: Bridging the gap between human and machine translation," _CoRR_, vol. abs/1609.08144, 2016.
* [225] T. Kudo, "Subword regularization: Improving neural network translation models with multiple subword candidates," in _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume I: Long Papers_, I. Gurevych and Y. Miyao, Eds. Association for Computational Linguistics, 2018, pp. 66-75.
* 11월 4일, 2018_, E. Blanco and W. Lu, Eds. Association for Computational Linguistics, 2018.
* [227] M. Davis and M. Durst, "Unicode normalization forms," 2001.
* [228] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak,and I. Sutskever, "Deep double descent: Where bigger models and more data hurt," in _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [229] K. Tirumala, D. Simig, A. Aghajanyan, and A. S. Morcos, "D4: Improving llm pretraining via document de-duplication and diversification," _arXiv preprint arXiv:2308.12284_, 2023.
* [230] Z. Shen, T. Tao, L. Ma, W. Neiswanger, J. Hestness, N. Vassilieva, D. Soboleva, and E. Xing, "Slimpajama-dc: Understanding data combinations for llm training," _arXiv preprint arXiv:2309.10818_, 2023.
* [231] S. M. Xie, S. Santurkar, T. Ma, and P. Liang, "Data selection for language models via importance resampling," _arXiv preprint arXiv:2302.03169_, 2023.
* [232] X. Wang, W. Zhou, Q. Zhang, J. Zhou, S. Gao, J. Wang, M. Zhang, X. Gao, Y. Chen, and T. Gui, "Farewell to amless large-scale pretraining: Influential subset selection for language model," _arXiv preprint arXiv:2305.12816_, 2023.
* [233] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez, "The LAMBADA dataset: Word prediction requiring a broad discourse context," in _ACL (1)_. The Association for Computer Linguistics, 2016.
* [234] M. F. Chen, N. Roberts, K. Bhatia, J. Wang, C. Zhang, F. Sala, and C. Re, "Skill-itl a data-driven skills framework for understanding and training language models," _arXiv preprint arXiv:2307.14430_, 2023.
* [235] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. Defossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve, "Code llama: Open foundation models for code," _CoRR_, vol. abs/2308.12950, 2023.
* [236] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in _ICML_, 2009, pp. 41-48.
* [237] C. Xu, C. Rosset, L. Del Corro, S. Mahajan, J. McAuley, J. Neville, A. H. Awadallah, and N. Rao, "Contrastive post-training large language models on data curriculum," _arXiv preprint arXiv:2310.02263_, 2023.
* [238] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski, and P. Milos, "Focused transformer: Contrastive training for context scaling," _CoRR_, vol. abs/2307.03170, 2023.
* [239] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck, "Llemma: An open language model for mathematics," _arXiv preprint arXiv:2310.10631_, 2023.
* [240] S. Chen, S. Wong, L. Chen, and Y. Tian, "Extending context window of large language models via positional interpolation," _CoRR_, vol. abs/2306.15595, 2023.
* [241] G. Wenzek, M.-A. Lachaux, A. Conneau, V. Chaudhary, F. Guzman, A. Joulin, and E. Grave, "Cnet: Extracting high quality monolingual datasets from web crawl data," in _Proceedings of the Twelfth Language Resources and Evaluation Conference_, 2020, pp. 4003-4012.
* [242] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, "Bag of tricks for efficient text classification," in _EACL_, 2017, pp. 427-431.
* [243] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge, D. Gao, Y. Xie, Z. Liu, J. Gao _et al._, "Data-juicer: A one-step data processing system for large language models," _arXiv preprint arXiv:2309.02033_, 2023.
* [244] B. Zhang, B. Ghorbani, A. Bapna, Y. Cheng, X. Garcia, J. Shen, and O. Firat, "Examining scaling and transfer of language model architectures for machine translation," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022, pp. 26 176-26 192.
* [245] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H. Hon, "Unified language model pre-training for natural language understanding and generation," in _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, 2019, pp. 13 042-13 054.
* [246] A. Clark, D. de Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. A. Hechtman, T. Cai, S. Borgeaud, G. van den Driessche, E. Rutherford, T. Hennigan, M. J. Johnson, A. Cassiner, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, M. Ranzato, J. W. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan, "Unified scaling laws for routed language models," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022, pp. 4057-4086.
* [247] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. [Online]. Available: [https://openreview.net/forum?id=uYLFoz1vlAC](https://openreview.net/forum?id=uYLFoz1vlAC)
* [248] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, "Long range language modeling via gated state spaces," _CoRR_, vol. abs/2206.13947, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2206.13947](https://doi.org/10.48550/arXiv.2206.13947)
* [249] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, "Hungry hungry hungry hippos: Towards language modeling with state space models," _CoRR_, vol. abs/2212.14052, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2212.14052](https://doi.org/10.48550/arXiv.2212.14052)
* [250] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re, "Hyena hierarchy: Towards larger convolutional language models," in _ICML_, 2023.
* [251] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. G. V., X. He, H. Hou, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom, A. Saito, X. Tang, B. Wang, J. S. Wind, S. Wozniak, R. Zhang, Z. Zhang, Q. Zhao, P. Zhou, J. Zhu, and R. Zhu, "RWK: reinventire rms for the transformer era," _CoRR_, vol. abs/2305.13048, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.13048](https://doi.org/10.48550/arXiv.2305.13048)
* [252] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue,

[MISSING_PAGE_FAIL:97]

nov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," _Advances in neural information processing systems_, vol. 32, 2019.
* [276] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, "Yarm: Efficient context window extension of large language models," _CoRR_, vol. abs/2309.00071, 2023.
* [277] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benham, V. Chaudhary, X. Song, and F. Wei, "A length-extrapolatable transformer," _CoRR_, vol. abs/2212.10554, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2212.10554](https://doi.org/10.48550/arXiv.2212.10554)
* [278] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, "Random feature attention," in _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_.
* [279] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed, "Big bird: Transformers for longer sequences," in _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [280] R. Child, S. Gray, A. Radford, and I. Sutskever, "Generating long sequences with sparse transformers," _CoRR_, vol. abs/1904.10509, 2019.
* [281] N. Shazeer, "Fast transformer decoding: One write-head is all you need," _CoRR_, vol. abs/1911.02150, 2019. [Online]. Available: [http://arxiv.org/abs/1911.02150](http://arxiv.org/abs/1911.02150)
* [282] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai, "Gqa: Training generalized multi-query transformer models from multi-head checkpoints," _arXiv preprint arXiv:2305.13245_, 2023.
* [283] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast and memory-efficient exact attention with IO-awareness," in _NeurIPS_, 2022.
* [284] T. Dao, "Flashattention-2: Faster attention with better parallelism and work partitioning," _arXiv preprint arXiv:2307.08691_, 2023.
* [285] "vllm: Easy, fast, and cheap llm serving with pagedattention." [Online]. Available: [https://vllm.ai/](https://vllm.ai/)
* [286] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, "Wordcraft: story writing with large language models," in _27th International Conference on Intelligent User Interfaces_, 2022, pp. 841-852.
* [287] A. Kazemmejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy, "The impact of positional encoding on length generalization in transformers," _CoRR_, vol. abs/2305.19466, 2023.
* [288] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma, "Effective long-context scaling of foundation models," _CoRR_, vol. abs/2309.16039, 2023.
* [289] kaikendev, "Things I'm learning while training superphot." 2023.
* [290] Z. Dong, T. Tang, J. Li, W. X. Zhao, and J. Wen, "BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models," _CoRR_, vol. abs/2309.13345, 2023.
* [291] J. Su. (2023) Transformer upgrade path: 12, infinite extrapolation of rerope?
* [292] X. Liu, H. Yan, S. Zhang, C. An, X. Qiu, and D. Lin, "Scaling laws of rope-based extrapolation," _CoRR_, vol. abs/2310.05209, 2023.
* [293] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and S. Naidu, "Giraffe: Adventures in expanding context lengths in lms," _CoRR_, vol. abs/2308.10882, 2023.
* 23, 2021_. Association for Computational Linguistics, 2021, pp. 874-880.
* [295] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend, E. Karpas, A. Shashua, K. Leyton-Brown, and Y. Shoham, "Parallel context windows for large language models," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_. Association for Computational Linguistics, 2023, pp. 6383-6402.
* [296] Y. Hao, Y. Sun, L. Dong, Z. Han, Y. Gu, and F. Wei, "Structured prompting: Scaling in-context learning to 1, 000 examples," _CoRR_, 2022.
* [297] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long-document transformer," _CoRR_, vol. abs/2004.05150, 2020.
* [298] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, "Efficient streaming language models with attention sinks," _CoRR_, vol. abs/2309.17453, 2023.
* [300] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, "Lost in the middle: How language models use long contexts," _CoRR_, vol. abs/2307.03172, 2023.
* [301] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang, "Lm-infinite: Simple on-the-fly length generalization for large language models," _CoRR_, vol. abs/2308.16137, 2023.
* [302] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, "Unlimiformer: Long-range transformers with unlimited length input," _CoRR_, vol. abs/2305.01625, 2023.
* [303] Y. Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, "Memorizing transformers," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [304] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz, "Walking down the memory maze: Beyond context limit through interactive reading," _CoRR_, vol. abs/2310.05029, 2023.
* [305] W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, and M. Sachan, "Recurrentopt: Interactive generation of (arbitrarily) long text," _CoRR_, vol. abs/2305.13304, 2023.
* [306] C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gonzalez, "Memgpt: Towards lms as operating systems," _CoRR_, vol. abs/2310.08560, 2023.
* [307] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro, "Retrieval meets long context large language models," _CoRR_, vol. abs/2310.03025, 2023.
* [307] K. Murray and D. Chiang, "Correcting length bias in neural machine translation," in _WMT_. Association for Computational Linguistics, 2018, pp. 212-223.
* [308] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," in _ICLR_, 2020.
* [309] C.-M. U. P. P. D. O. C. SCIENCE, _Speech Understanding Systems. Summary of Results of the Five-Year Research Effort at Carnegie-Mellon University_, 1977.
* [310] P. Koehn and R. Knowles, "Six challenges for neural machine translation," in _NMT@ACL_. Association for Computational Linguistics, 2017, pp. 28-39.
* [311] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikum, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, "Google's neural machine translation system: Bridging the gap between human and machine translation," _CoRR_, vol. abs/1609.08144, 2016.
* [312] R. Paulus, C. Xiong, and R. Socher, "A deep reinforced model for abstractive summarization," in _ICLR (Poster)_. OpenReview.net, 2018.
* [313] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. J. Crandall, and D. Batra, "Diverse beam search: Decoding diverse solutions from neural sequence models," _CoRR_, vol. abs/1610.02424, 2016.
* [314] A. Fan, M. Lewis, and Y. N. Dauphin, "Hierarchical neural story generation," in _ACL (1)_. Association for Computational Linguistics, 2018, pp. 889-898.
* [315] J. Hewitt, C. D. Manning, and P. Liang, "Truncation sampling as language model desmoothing," in _EMNLP (Findings)_. Association for Computational Linguistics, 2022, pp. 3414-3427.
* [316] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and N. Collier, "A contrastive framework for neural text generation," in _NeurIPS_, 2022.
* [317] C. Meister, T. Pimentel, G. Wier, and R. Cotterell, "Locally typical sampling," _Trans. Assoc. Comput. Linguistics_, 2023.
* [318] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis, "Contrastive decoding: Open-ended text generation as optimization," in _ACL (1)_. Association for Computational Linguistics, 2023, pp. 12 286-12 312.
* [319] Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and P. He, "Dola: Decoding by contrasting layers improves factuality in large language models," _CoRR_, vol. abs/2309.03883, 2023.
* [320] L. Chen, "Dissecting batching effects in gpt inference," 2023. [Online]. Available: [https://le.qun.ch/en/blog/2023/05/13/transformer-batching/](https://le.qun.ch/en/blog/2023/05/13/transformer-batching/)
* [321] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Re, I. Stoica, and C. Zhang, "Flexgen: High-throughput generative inference of large language models with a single GPU," in _ICML_, ser. Proceedings of Machine Learning Research, vol. 202. PMLR, 2023, pp. 31 094-31 116.
* [322] T. Dao, D. Haziza, F. Massa, and G. Sizov, "Flash-decoding for long-context inference," [https://crfm.stanford.edu/2023/10/12/flashdecoding.html](https://crfm.stanford.edu/2023/10/12/flashdecoding.html), 2023.
* [323] Y. Leviathan, M. Kalman, and Y. Matias, "Fast inference from transformers via speculative decoding," in _International Conference on Machine Learning_, 2023.
* [324] C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre, and J. Jumper, "Accelerating large language model decoding with speculative sampling," _CoRR_, vol. abs/2302.01318, 2023.
* [325] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia, "Specinfer: Accelerating generative LLM serving with speculative inference and token tree verification," _CoRR_, vol. abs/2305.09781, 2023.
* [326] B. Spector and C. Re, "Accelerating LLM inference with staged speculative decoding," _CoRR_, vol. abs/2308.04623, 2023.
* [327] L. D. Corro, A. D. Giorno, S. Agarwal, B. Yu, A. H. Awadallah, and S. Mukherjee, "Skipdecode: Autoregressive skip decoding with batching and caching for efficient LLM inference," _CoRR_, vol. abs/2307.02628, 2023.
* [328] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, Y. Bengio and Y. LeCun, Eds., 2015.
* [329] I. Loshchilov and F. Hutter, "Fixing weight decay regularization in adam," _CoRR_, vol. abs/1711.05101, 2017.
* [330] N. Shazeer and M. Stern, "Adafactor: Adaptive learning rates with sublinear memory cost," in _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, ser. Proceedings of Machine Learning Research, J. G. Dy and A. Krause, Eds., vol. 80. PMLR, 2018, pp. 4603-4611.
* [331] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. X. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, "Gpipe: Efficient training of giant neural networks using pipeline parallelism," in _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 103-112.
* [332] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, and P. B. Gibbons, "Pipedream: Fast and efficient pipeline parallel DNN training," _CoRR_, vol. abs/1806.03377, 2018.
* [333] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, "Zero: memory optimizations toward training trillion parameter models," in _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020_, C. Cuicchi, I. Qualters, and W. T. Kramer, Eds. IEEE/ACM, 2020, p. 20.
* [334] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu, "Mixed precision training," _CoRR_, vol. abs/1710.03740, 2017.
* [335] Q. Xu, S. Li, C. Gong, and Y. You, "An efficient 2d method for training super-large deep learning models," _CoRR_, vol. abs/2104.05343, 2021.
* 2022년 9월 1일_. ACM, 2022년
* [337] Z. Bian, Q. Xu, B. Wang, and Y. You, "Maximizing parallelism in distributed training for huge neural networks," _CoRR_, vol. abs/2105.14450, 2021.
* [338] S. Li, F. Xue, C. Baranwal, Y. Li, and Y. You, "Sequence parallelism: Long sequence training from system perspective," _arXiv e-prints_, pp. arXiv-2105, 2021.
* [339] FairScale authors, "Fairscale: A general purpose modular pytorch library for high performance and large scale training," [https://github.com/facebookresearch/fairscale](https://github.com/facebookresearch/fairscale), 2021.
* [340] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen, Y. Huang, Y. Wang, Y. Xu, D. Zhuo, E. P. Xing _et al._, "Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning," in _OSDI_, 2022, pp. 559-578.
* [341] T. Chen, B. Xu, C. Zhang, and C. Guestrin, "Training deep nets with sublinear memory cost," _CoRR_, vol. abs/1604.06174, 2016.
* [342] R. Lou, K. Zhang, and W. Yin, "Is prompt all you need? no. A comprehensive and broader view of instruction learning," _CoRR_, vol. abs/2303.10475, 2023.
* [343] X. Liu, P. He, W. Chen, and J. Gao, "Multi-task deep neural networks for natural language understanding," in _ACL (1)_. Association for Computational Linguistics, 2019, pp. 4487-4496.
* [344] A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen, L. Zettlemoyer, and S. Gupta, "Muppet: Massive multi-task representations with pre-finetuning," in _EMNLP (1)_. Association for Computational Linguistics, 2021, pp. 5799-5811.
* [345] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts, "The fian collection: Designing data and methods for effective instruction tuning," _CoRR_, vol. abs/2301.13688, 2023.
* [346] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang, "Wizardlm: Empowering large language models to follow complex instructions," _CoRR_, vol. abs/2304.12244, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.12244](https://doi.org/10.48550/arXiv.2304.12244)
* [347] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, "Principle-driven self-alignment of language models from scratch with minimal human supervision," _arXiv preprint arXiv:2305.03047_, 2023.
* [348] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis, "Self-alignment with instruction backtranslation," _CoRR_, vol. abs/2308.06259, 2023.
* [349] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu _et al._, "Lima: Less is more for alignment," _arXiv preprint arXiv:2305.11206_, 2023.
* [350] L. Chen, S. Li, J. Yan, H. Wang, K. Gunaratna, V. Yadav, Z. Tang, V. Srinivasan, T. Zhou, H. Huang, and H. Jin, "Alpagasus: Training A better alpaca with fewer data," _CoRR_, vol. abs/2307.08701, 2023.
* [351] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. H. Awadallah, "Orca: Progressive learning from complex explanation traces of GPT-4," _CoRR_, vol. abs/2306.02707, 2023.
* [352] YuLan-Chat-Team, "Yulan-chat: An open-source bilingual chatbot," [https://github.com/RUC-GSAI/YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat), 2023.
* [353] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy, and H. Hajishirzi, "How far can camels go? exploring the state of instruction tuning on open resources," _CoRR_, vol. abs/2306.04751, 2023.
* [354] B. Peng, C. Li, P. He, M. Galley, and J. Gao, "Instruction tuning with GPT-4," _CoRR_, vol. abs/2304.03277, 2023.
* [355] M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon, "Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance," _arXiv preprint arXiv:2107.02027_, 2021.
* [356] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl _et al._, "Large language models encode clinical knowledge," _arXiv preprint arXiv:2212.13138_, 2022.
* [357] J. Zhang, R. Xie, Y. Hou, W. X. Zhao, L. Lin, and J. Wen, "Recommendation as instruction following: A large language model empowered recommendation approach," _CoRR_, vol. abs/2305.07001, 2023.
* [358] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu, "Huatuo: Tuning llama model with chinese medical knowledge," _arXiv preprint arXiv:2304.06975_, 2023.
* [359] Q. Huang, M. Tao, Z. An, C. Zhang, C. Jiang, Z. Chen, Z. Wu, and Y. Feng, "Lawyer llama technical report," _arXiv preprint arXiv:2305.15062_, 2023.
* [360] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, "Bloomberggpt: A large language model for finance," _arXiv preprint arXiv:2303.17564_, 2023.
* [361] T. Liu and B. K. H. Low, "Goat: Fine-tuned llama outperforms gpt+4 on arithmetic tasks," _arXiv preprint arXiv:2305.14201_, 2023.
* [362] T. Sun, X. Zhang, Z. He, P. Li, Q. Cheng, H. Yan, X. Liu, Y. Shao, Q. Tang, X. Zhao, K. Chen, Y. Zheng, Z. Zhou, R. Li, J. Zhan, Y. Zhou, L. Li, X. Yang, L. Wu, Z. Yin, X. Huang, and X. Qiu, "Moss: Training conversational language models from synthetic data," 2023.
* [363] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto, "Alpacafarm: A simulation framework for methods that learn from human feedback," _CoRR_, vol. abs/2305.14387, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.14387](https://doi.org/10.48550/arXiv.2305.14387)
* [364] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, "Measuring massive multitask language understanding," in _ICLR_.

OpenReview.net, 2021.
* [365] M. Suzgun, N. Scales, N. Scharli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei, "Challenging big-bench tasks and whether chain-of-thought can solve them," _CoRR_, vol. abs/2210.09261, 2022.
* [366] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving, "Alignment of language agents," _CoRR_, vol. abs/2103.14659, 2021.
* [367] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. F. Christiano, and G. Irving, "Fine-tuning language models from human preferences," _CoRR_, vol. abs/1909.08593, 2019.
* [368] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-Sarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, "A general language assistant as a laboratory for alignment," _CoRR_, vol. abs/2112.00861, 2021.
* [369] E. Perez, S. Huang, H. F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving, "Red teaming language models with language models," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emitates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 3419-3448.
* [370] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, H. F. Song, M. Chadwick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, and N. McAleese, "Teaching language models to support answers with verified quotes," _CoRR_, vol. abs/2203.11147, 2022.
* [371] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosiute, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan, "Constitutional AI: harmlessness from AI feedback," _CoRR_, vol. abs/2212.08073, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)
* [372] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune, and A. Rastogi, "RLAIF: scaling reinforcement learning from human feedback with AI feedback," _CoRR_, vol. abs/2309.00267, 2023.
* [373] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang, "RAFT: reward ranked fine-tuning for generative foundation model alignment," _CoRR_, vol. abs/2304.06767, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.06767](https://doi.org/10.48550/arXiv.2304.06767)
* [374] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-Sarma _et al._, "A general language assistant as a laboratory for alignment," _arXiv preprint arXiv:2112.00861_, 2021.
* [375] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y. Liu, S. Jin, Q. Liu, L. Xiong, L. Chen _et al._, "Secrets of rlhf in large language models part i: Ppo," _arXiv preprint arXiv:2307.04964_, 2023.
* [376] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins, "Solving math word problems with process- and outcome-based feedback," _CoRR_, vol. abs/2211.14275, 2022.
* [377] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, "Let's verify step by step," _CoRR_, vol. abs/2305.20050, 2023.
* [378] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, "Measuring coding challenge competence with APPS," in _NeurIPS Datasets and Benchmarks_, 2021.
* [379] Q. Ma, H. Zhou, T. Liu, J. Yuan, P. Liu, Y. You, and H. Yang, "Let's reward step by step: Step-level reward model as the navigators for reasoning," _CoRR_, vol. abs/2310.10080, 2023.
* [380] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, "Mastering the game of go without human knowledge," _Nat._, pp. 354-359, 2017.
* [381] T. Anthony, Z. Tian, and D. Barber, "Thinking fast and slow with deep learning and tree search," in _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, 2017, pp. 5360-5370.
* [382] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang, "Wizard-math: Empowering mathematical reasoning for large language models via reinforced evol-instruct," _CoRR_, vol. abs/2308.09583, 2023.
* [383] R. Liu, C. Jia, G. Zhang, Z. Zhuang, T. X. Liu, and S. Vosoughi, "Second thoughts are best: Learning to re-align with human values from text edits," in _NeurIPS_, 2022.
* [384] X. Lu, S. Welleck, J. Hessel, L. Jiang, L. Qin, P. West, P. Ammanabrolu, and Y. Choi, "QUARK: controllable text generation with reinforced unlearning," in _NeurIPS_, 2022.
* [385] J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan, A. Chen, K. Cho, and E. Perez, "Training language models with language feedback at scale," _CoRR_, vol. abs/2303.16755, 2023.
* [386] G. Guo, R. Zhao, T. Tang, W. X. Zhao, and J.-R. Wen, "Beyond imitation: Leveraging fine-grained quality signals for alignment," _arXiv preprint arXiv:2311.04072_, 2023.
* [387] R. Krishna, D. Lee, L. Fei-Fei, and M. S. Bernstein, "Socially situated artificial intelligence enables learning from human interaction," _Proceedings of the National Academy of Sciences of the United Statesof America_, vol. 119, 2022. [Online]. Available: [https://api.semanticscholar.org/CorpusID:252381954](https://api.semanticscholar.org/CorpusID:252381954)
* [388] H. Liu, C. Sferrazza, and P. Abbeel, "Chain of hindsight aligns language models with feedback," _CoRR_, vol. abs/2302.02676, 2023.
* [389] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, "Direct preference optimization: Your language model is secretly a reward model," _CoRR_, vol. abs/2305.18290, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.18290](https://doi.org/10.48550/arXiv.2305.18290)
* [390] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang, "RRHF: rank responses to align language models with human feedback without tears," _CoRR_, vol. abs/2304.05302, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.05302](https://doi.org/10.48550/arXiv.2304.05302)
* [391] Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu, "Slic-hf: Sequence likelihood calibration with human feedback," _CoRR_, vol. abs/2305.10425, 2023.
* [392] T. Zhang, F. Liu, J. Wong, P. Abbeel, and J. E. Gonzalez, "The wisdom of hindsight makes language models better instruction followers," _CoRR_, vol. abs/2302.05206, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2302.05206](https://doi.org/10.48550/arXiv.2302.05206)
* [393] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, "Imitation learning: A survey of learning methods," _ACM Comput. Surv._, vol. 50, no. 2, apr 2017. [Online]. Available: [https://doi.org/10.1145/3054912](https://doi.org/10.1145/3054912)
* [394] S. Levine, "Should i imitate or reinforce," 2022. [Online]. Available: [https://www.youtube.com/watch?v=sVPm?zOrBxM](https://www.youtube.com/watch?v=sVPm?zOrBxM)
* [395] J. Schulman, "Reinforcement learning from human feedback: Progress and challenges," 2023. [Online]. Available: [https://www.youtube.com/watch?v=hiLw5Q_UFg](https://www.youtube.com/watch?v=hiLw5Q_UFg)
* [396] X. L. Li and P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," in _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, C. Zong, F. Xia, W. Li, and R. Navigli, Eds. Association for Computational Linguistics, 2021, pp. 4582-4597.
* [397] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 3045-3059.
* [398] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for NLP," in _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, 2019, pp. 2790-2799.
* [399] Z. Hu, Y. Lan, L. Wang, W. Xu, E. Lim, R. K. Lee, L. Bing, and S. Poria, "Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models," _CoRR_, vol. abs/2304.01933, 2023.
* [400] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, "Towards a unified view of parameter-efficient transfer learning," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [401] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks," _CoRR_, vol. abs/2110.07602, 2021.
* [402] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, "GPT understands, too," _CoRR_, vol. abs/2103.10385, 2021.
* [403] Y. Gu, X. Han, Z. Liu, and M. Huang, "Ppt: Pre-trained prompt tuning for few-shot learning," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 8410-8423.
* [404] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, "How can we know what language models know?" _Transactions of the Association for Computational Linguistics_, vol. 8, pp. 423-438, 2020.
* [405] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, "Autoprompt: Eliciting knowledge from language models with automatically generated prompts," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2020, pp. 4222-4235.
* [406] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, "Adaptive budget allocation for parameter-efficient fine-tuning," _CoRR_, vol. abs/2303.10512, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2303.10512](https://doi.org/10.48550/arXiv.2303.10512)
* [407] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation," _CoRR_, vol. abs/2210.07558, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2210.07558](https://doi.org/10.48550/arXiv.2210.07558)
* [408] N. Ding, Y. Qin, G. Yang, F. Wei, Y. Zonghan, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H.-T. Zheng, J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun, "Parameter-efficient fine-tuning of large-scale pre-trained language models," _Nature Machine Intelligence_, vol. 5, pp. 1-16, 03 2023.
* [409] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, "Llama-adapter: Efficient fine-tuning of language models with zero-init attention," _CoRR_, vol. abs/2303.16199, 2023.
* [410] J. Pfeiffer, I. Vulic, I. Gurevych, and S. Ruder, "MAD-X: an adapter-based framework for multi-task cross-lingual transfer," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 7654-7673.
* [411] S. Mangurikkar, S. Gugger, L. Debut, Y. Belkada, and S. Paul, "Peft: State-of-the-art parameter-efficient fine-tuning methods," [https://github.com/huggingface/peft](https://github.com/huggingface/peft), 2022.
* [412] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W.

Mahoney, K. Keutzer, "A survey of quantization methods for efficient neural network inference," _CoRR_, vol. abs/2103.13630, 2021. [Online]. 사용 가능한: [https://arxiv.org/abs/2103.13630](https://arxiv.org/abs/2103.13630)
* [413] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm.int8(0): 8-bit matrix multiplication for transformers at scale," _CoRR_, vol. abs/2208.07339, 2022.
* [414] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han, "Smoothquant: Accurate and efficient post-training quantization for large language models," _CoRR_, vol. abs/2211.10438, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2211.10438](https://doi.org/10.48550/arXiv.2211.10438)
* [415] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He, "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers," in _NeurIPS_, 2022.
* [416] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, "Awq: Activation-aware weight quantization for llm compression and acceleration," 2023.
* [417] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "Gptq: Accurate post-training quantization for generative pre-trained transformers," _arXiv preprint arXiv:2210.17323_, 2022.
* [418] E. Frantar and D. Alistarh, "Optimal brain compression: A framework for accurate post-training quantization and pruning," in _NeurIPS_, 2022.
* [419] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Olora: Efficient finetuning of quantized llms," _arXiv preprint arXiv:2305.14314_, 2023.
* [420] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra, "Llm-qat: Data-free quantization aware training for large language models," 2023.
* [421] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, "Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation," 2023.
* [422] T. Dettmers and L. Zettlemoyer, "The case for 4-bit precision: k-bit inference scaling laws," _CoRR_, vol. abs/2212.09720, 2022.
* [423] L. Peiyu, L. Zikang, G. Ze-Feng, G. Dawei, Z. W. Xin, L. Yaliang, D. Bolin, and W. Ji-Rong, "Do emergent abilities exist in quantized large language models: An empirical study," _arXiv preprint arXiv:2307.08072_, 2023.
* [424] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm.int8(): 8-bit matrix multiplication for transformers at scale," _CoRR_, vol. abs/2208.07339, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2208.07339](https://doi.org/10.48550/arXiv.2208.07339)
* [425] X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang, S. Huang, P. Xie, J. Xu, Y. Chen, M. Zhang _et al._, "Zero-shot information extraction via chatting with chatopt," _arXiv preprint arXiv:2302.10205_, 2023.
* [426] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer, "8-bit optimizers via block-wise quantization," _9th International Conference on Learning Representations, ICLR_, 2022.
* [427] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, and N. Wong, "Compression of generative pre-trained language models via quantization," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 4821-4836.
* [428] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, "What makes good in-context examples for gpt-3?" in _Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLO@ACL 2022, Dublin, Ireland and Online, May 27, 2022_, 2022, pp. 100-114.
* [429] O. Rubin, J. Herzig, and J. Berant, "Learning to retrieve prompts for in-context learning," in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, 2022, pp. 2655-2671.
* [430] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator," _CoRR_, vol. abs/2206.08082, 2022.
* [431] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, "Large language models are human-level prompt engineers," in _Proc. of ICLR_, 2023.
* [432] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp, "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds., 2022, pp. 8086-8098.
* [433] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, "Complexity-based prompting for multi-step reasoning," _CoRR_, vol. abs/2210.00720, 2022.
* [434] Z. Zhang, A. Zhang, M. Li, and A. Smola, "Automatic chain of thought prompting in large language models," _CoRR_, vol. abs/2210.03493, 2022.
* [435] A. Creswell, M. Shanahan, and I. Higgins, "Selection-inference: Exploiting large language models for interpretable logical reasoning," _CoRR_, vol. abs/2205.09712, 2022.
* [436] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," _CoRR_, vol. abs/2203.11171, 2022.
* [437] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J. Lou, and W. Chen, "On the advance of making language models better reasoners," _CoRR_, vol. abs/2206.02336, 2022.
* [438] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, "Rationale-augmented ensembles in language models," _CoRR_, 2022.
* [439] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. H. Chi, "Least-to-most prompting enables complex reasoning in large language models," _CoRR_, vol. abs/2205.10625, 2022.
* [440] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, "Decomposed prompting: A modular approach for solving complex tasks," _CoRR_, vol. abs/2210.02406, 2022. [Online]. Available:[https://doi.org/10.48550/arXiv.2210.02406](https://doi.org/10.48550/arXiv.2210.02406)
* [441] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and E. Lim, "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models," _CoRR_, vol. abs/2305.04091, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.04091](https://doi.org/10.48550/arXiv.2305.04091)
* [442] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. Callison-Burch, "Faithful chain-of-thought reasoning," _CoRR_, vol. abs/2301.13379, 2023.
* [443] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "PAL: program-aided language models," _CoRR_, vol. abs/2211.10435, 2022.
* [444] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, "Hugginggpt: Solving ai tasks with chat-gpt and its friends in huggingface," _arXiv preprint arXiv:2303.17580_, 2023.
* [445] H. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang, "Adaplanner: Adaptive planning from feedback with language models," _arXiv preprint arXiv:2305.16653_, 2023.
* [446] Y. Lu, P. Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y. Wang, "Multimodal procedural planning via dual text-image prompting," _CoRR_, vol. abs/2305.01795, 2023.
* [447] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, "Reasoning with language model is planning with world model," _CoRR_, vol. abs/2305.14992, 2023.
* [448] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and J. Wen, "Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models," _CoRR_, vol. abs/2305.14323, 2023.
* [449] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergizing reasoning and acting in language models," _CoRR_, vol. abs/2210.03629, 2022.
* [450] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao, "Reflexion: Language agents with verbal reinforcement learning," 2023.
* [451] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," _CoRR_, vol. abs/2305.10601, 2023.
* [452] V. Liu and L. B. Chilton, "Design guidelines for prompt engineering text-to-image generative models," in _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, 2022, pp. 1-23.
* [453] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern catalog to enhance prompt engineering with chatgpt," _arXiv preprint arXiv:2302.11382_, 2023.
* [454] S. K. K. Santu and D. Feng, "Teler: A general taxonomy of LLM prompts for benchmarking complex tasks," _CoRR_, vol. abs/2305.11430, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.11430](https://doi.org/10.48550/arXiv.2305.11430)
* [455] OpenAI, "Gpt best practices," _OpenAI_, 2023. [Online]. Available: [https://platform.openai.com/docs/guides/gpt-best-practices](https://platform.openai.com/docs/guides/gpt-best-practices)
* [456] Contributors, "Ai short," 2023. [Online]. Available: [https://www.aishort.top/](https://www.aishort.top/)
* [457] ----, "Awesome chatgpt prompts," _Github_, 2023. [Online]. Available: [https://github.com/f/awesome-chatgpt-prompts/](https://github.com/f/awesome-chatgpt-prompts/)
* [458] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J. Wen, "Struckpt: A general framework for large language model to reason over structured data," _CoRR_, vol. abs/2305.09645, 2023.
* [459] L. Beurer-Kellner, M. Fischer, and M. Vechev, "Prompting is programming: A query language for large language models," _Proceedings of the ACM on Programming Languages_, vol. 7, no. PLDI, pp. 1946-1969, 2023.
* [460] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao, "Chameleon: Plug-and-play compositional reasoning with large language models," _arXiv preprint arXiv:2304.09842_, 2023.
* [461] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-R. Wen, and H. Wang, "Investigating the factual knowledge boundary of large language models with retrieval augmentation," _arXiv preprint arXiv:2307.11019_, 2023.
* [462] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley, and W. X. Zhao, "Large language models are zero-shot rankers for recommender systems," _CoRR_, vol. abs/2305.08845, 2023.
* [463] S. Chang and E. Fosler-Lussier, "How to prompt lms for text-to-sql: A study in zero-shot, single-domain, and cross-domain settings," _CoRR_, vol. abs/2305.11853, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.11853](https://doi.org/10.48550/arXiv.2305.11853)
* [464] Y. Wen, N. Jain, J. Kirchenbauer, M. Goldblum, J. Geiping, and T. Goldstein, "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery," _CoRR_, vol. abs/2302.03668, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2302.03668](https://doi.org/10.48550/arXiv.2302.03668)
* [465] T. Gao, A. Fisch, and D. Chen, "Making pre-trained language models better few-shot learners," in _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/JCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, C. Zong, F. Xia, W. Li, and R. Navigli, Eds. Association for Computational Linguistics, 2021, pp. 3816-3830.
* [466] L. Chen, J. Chen, T. Goldstein, H. Huang, and T. Zhou, "Instructzero: Efficient instruction optimization for black-box large language models," _CoRR_, vol. abs/2306.03082, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2306.03082](https://doi.org/10.48550/arXiv.2306.03082)
* [467] M. Deng, J. Wang, C. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu, "Rlprompt: Optimizing discrete text prompts with reinforcement learning," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 3369-3391.

* [468] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez, "TEMPERA: test-time prompt editing via reinforcement learning," in _The Eleventh International Conference on Learning Representations, ICLR 2023, Kiagali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.
* [469] H. Xu, Y. Chen, Y. Du, N. Shao, Y. Wang, H. Li, and Z. Yang, "GPS: genetic prompt search for efficient few-shot learning," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 8162-8171.
* [470] A. Prasad, P. Hase, X. Zhou, and M. Bansal, "Grips: Gradient-free, edit-based instruction search for prompting large language models," in _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, A. Vlachos and I. Augenstein, Eds. Association for Computational Linguistics, 2023, pp. 3827-3846.
* [471] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, "Large language models are human-level prompt engineers," in _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.
* [472] R. Przyant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, "Automatic prompt optimization with "gradient descent" and beam search," _CoRR_, vol. abs/2305.03495, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.03495](https://doi.org/10.48550/arXiv.2305.03495)
* [473] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, "Large language models as optimizers," _CoRR_, vol. abs/2309.03409, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2309.03409](https://doi.org/10.48550/arXiv.2309.03409)
* [474] X. Wang, C. Li, Z. Wang, F. Bai, H. Luo, J. Zhang, N. Jojic, E. P. Xing, and Z. Hu, "Promptagent: Strategic planning with language models enables expert-level prompt optimization," _CoRR_, vol. abs/2310.16427, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2310.16427](https://doi.org/10.48550/arXiv.2310.16427)
* [475] T. Tang, J. Li, W. X. Zhao, and J. Wen, "Context-tuning: Learning contextualized prompts for natural language generation," in _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P. Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na, Eds. International Committee on Computational Linguistics, 2022, pp. 6340-6354.
* [476] T. Vu, B. Lester, N. Constant, R. Al-Rfou', and D. Cer, "Spot: Better frozen model adaptation through soft prompt transfer," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 5039-5059.
* [477] J. Li, T. Tang, J. Nie, J. Wen, and X. Zhao, "Learning to transfer prompts for text generation," in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds. Association for Computational Linguistics, 2022, pp. 3506-3518.
* [478] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, "Rethinking the role of demonstrations: What makes in-context learning work?" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_. Association for Computational Linguistics, 2022, pp. 11 048-11 064.
* [479] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, "Calibrate before use: Improving few-shot performance of language models," in _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, M. Meila and T. Zhang, Eds., 2021, pp. 12 697-12 706.
* [480] Y. Lee, C. Lim, and H. Choi, "Does GPT-3 generate empathetic dialogues? A novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation," in _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P. Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na, Eds. International Committee on Computational Linguistics, 2022, pp. 669-683.
* [481] I. Levy, B. Bogin, and J. Berant, "Diverse demonstrations improve in-context compositional generalization," _CoRR_, vol. abs/2212.06800, 2022.
* [482] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and T. Yu, "Selective annotation makes language models better few-shot learners," _CoRR_, 2022.
* [483] X. Ye, S. Iyer, A. Celikyilmaz, V. Stoyanov, G. Durrett, and R. Pasunuru, "Complementary explanations for effective in-context learning," _CoRR_, 2022.
* [484] X. Li and X. Qiu, "Finding supporting examples for in-context learning," _CoRR_, 2023.
* [485] Y. Zhang, S. Feng, and C. Tan, "Active example selection for in-context learning," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, 2022, pp. 9134-9148.
* [486] F. Gilardi, M. Alizadeh, and M. Kubli, "Chatgpt outperforms crowd-workers for text-annotation tasks," 2023.
* [487] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstrating generator," _CoRR_, vol. abs/2206.08082, 2022.
* [488] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma, "An explanation of in-context learning as implicit bayesian inference," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_, 2022.
* [489] Z. Wu, Y. Wang, J. Ye, and L. Kong, "Self-adaptive in-context learning," _CoRR_, vol. abs/2212.10375, 2022.
* [490] Y. Gu, L. Dong, F. Wei, and M. Huang, "Pre-training to learn in context," _CoRR_, vol. abs/2305.09137, 2023.
* [491] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi, "Metaicl: Learning to learn in context," in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds., 2022, pp. 2791-2809.
* [492] M. Hahn and N. Goyal, "A theory of emergent in-context learning as implicit structure induction," _CoRR_, vol. abs/2303.07971, 2023.
* [493] J. Pan, T. Gao, H. Chen, and D. Chen, "What in-context learning "learns" in-context: Disentangling task recognition and task learning," _CoRR_, vol. abs/2305.09731, 2023.
* [494] N. Wies, Y. Levine, and A. Shashua, "The learnability of in-context learning," _CoRR_, vol. abs/2303.07895, 2023.
* [495] A. Webson and E. Pavlick, "Do prompt-based models really understand the meaning of their prompts?" in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, 2022, pp. 2300-2344.
* [496] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov, "Transformers learn in-context by gradient descent," _CoRR_, vol. abs/2212.07677, 2022.
* [497] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah, "In-context learning and induction heads," _CoRR_, vol. abs/2209.11895, 2022.
* [498] E. Akyurek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou, "What learning algorithm is in-context learning? investigations with linear models," _CoRR_, vol. abs/2211.15661, 2022.
* [499] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou _et al._, "Larger language models do in-context learning differently," _arXiv preprint arXiv:2303.03846_, 2023.
* [500] J. Coda-Forno, M. Binz, Z. Akata, M. M. Botvinick, J. X. Wang, and E. Schulz, "Meta-in-context learning in large language models," _CoRR_, vol. abs/2305.12907, 2023.
* [501] J. W. Wei, L. Hou, A. K. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le, "Symbol tuning improves in-context learning in language models," _CoRR_, vol. abs/2305.08298, 2023.
* [502] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang, W. Peng, M. Liu, B. Qin, and T. Liu, "A survey of chain of thought reasoning: Advances, frontiers and future," _CoRR_, vol. abs/2309.15402, 2023.
* [503] S. Miao, C. Liang, and K. Su, "A diverse corpus for evaluating and developing english math word problem solvers," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 975-984.
* [504] A. Talmor, J. Herzig, N. Lourie, and J. Berant, "Comonsenseqa: A question answering challenge targeting commonsense knowledge," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4149-4158.
* [505] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," _CoRR_, vol. abs/2205.11916, 2022.
* [506] W. Chen, X. Ma, X. Wang, and W. W. Cohen, "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks," _CoRR_, vol. abs/2211.12588, 2022.
* [507] L. Gao, A. Madan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "PAL: program-aided language models," in _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., 2023.
* [508] X. Zhao, Y. Xie, K. Kawaguchi, J. He, and Q. Xie, "Automatic model selection with large language models for reasoning," _CoRR_, vol. abs/2305.14333, 2023.
* [509] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen, "Making large language models better reasomers with step-aware verifier," 2023.
* [510] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch, and J. Berant, "Answering questions by meta-reasoning over multiple chains of thought," _CoRR_, vol. abs/2304.13007, 2023.
* [511] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memisevic, and H. Su, "Deductive verification of chain-of-thought reasoning," _CoRR_, vol. abs/2306.03872, 2023.
* [512] T. Xue, Z. Wang, Z. Wang, C. Han, P. Yu, and H. Ji, "RCOT: detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought," _CoRR_, vol. abs/2305.11499, 2023.
* [513] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and J. Zhao, "Large language models are better reasoners with self-verification," _CoRR, abs/2212.09561_, 2023.
* [514] W. Jiang, H. Shi, L. Yu, Z. Liu, Y. Zhang, Z. Li, and J. T. Kwok, "Forward-backward reasoning in large language models for mathematical verification," 2023.
* [515] J. Long, "Large language model guided tree-of-thought," _CoRR_, vol. abs/2305.08291, 2023.
* [516] S. Mo and M. Xin, "Tree of uncertain thoughts reasoning for large language models," _CoRR_, vol. abs/2309.07694, 2023.
* [517] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski,H. Niewiadomski, P. Nyczyk, and T. Hoefler, "Graph of thoughts: Solving elaborate problems with large language models," _CoRR_, vol. abs/2308.09687, 2023.
* [518] B. Lei, P. Lin, C. Liao, and C. Ding, "Boosting logical reasoning in large language models through a new framework: The graph of thought," _CoRR_, vol. abs/2308.08614, 2023.
* [519] R. Ding, C. Zhang, L. Wang, Y. Xu, M. Ma, W. Zhang, S. Qin, S. Rajmohan, Q. Lin, and D. Zhang, "Everything of thoughts: Defying the law of penrose triangle for thought generation," _arXiv preprint arXiv:2311.04254_, 2023.
* [520] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Re, D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhaman, L. J. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. S. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda, "Holistic evaluation of language models," _CoRR_, vol. abs/2211.09110, 2022.
* [521] Z. Bi, N. Zhang, Y. Jiang, S. Deng, G. Zheng, and H. Chen, "When do program-of-thoughts work for reasoning?" _CoRR_, vol. abs/2308.15452, 2023.
* [522] A. Madan and A. Yazdanbakhsh, "Text and patterns: For effective chain of thought, it takes two to tango," _CoRR_, vol. abs/2209.07686, 2022.
* [523] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, "Multimodal chain-of-thought reasoning in language models," _CoRR_, vol. abs/2302.00923, 2023.
* [524] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei, "Language models are multilingual chain-of-thought reasoners," _CoRR_, vol. abs/2210.03057, 2022.
* [525] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, "Limitations of language models in arithmetic and symbolic induction," _CoRR_, vol. abs/2208.05051, 2022.
* [526] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He, "ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models," _CoRR_, 2023.
* [527] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," _CoRR_, vol. abs/2305.10601, 2023.
* [528] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, "Voyager: An open-ended embodied agent with large language models," _arXiv preprint arXiv:2305.16291_, 2023.
* [529] X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li, "Self-planning code generation with large language model," _CoRR_, vol. abs/2303.06689, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2303.06689](https://doi.org/10.48550/arXiv.2303.06689) prompt: Generating situated robot task plans using large language models," _CoRR_, vol. abs/2209.11302, 2022.
* [530] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, "LLM+P: empowering large language models with optimal planning proficiency," _CoRR_, vol. abs/2304.11477, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.11477](https://doi.org/10.48550/arXiv.2304.11477)
* [531] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, 2022, pp. 10 674-10 685.
* [532] J. S. Park, J. C. O'Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, "Generative agents: Interactive simulacra of human behavior," _CoRR_, vol. abs/2304.03442, 2023.
* [533] 2023. [Online]. Available: [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)
* [534] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents," _CoRR_, vol. abs/2302.01560, 2023.
* [535] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li, X. Xu _et al._, "Milvus: A purpose-built vector data management system," in _Proceedings of the 2021 International Conference on Management of Data_, 2021, pp. 2614-2627.
* [536] W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang, "Memorybank: Enhancing large language models with long-term memory," _CoRR_, vol. abs/2305.10250, 2023.
* [537] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, "Building a large annotated corpus of english: The penn treebank," _Comput. Linguistics_, vol. 19, no. 2, pp. 313-330, 1993.
* [538] S. Merity, C. Xiong, J. Bradbury, and R. Socher, "Pointer sentinel mixture models," in _ICLR (Poster)_. OpenReview.net, 2017.
* [539] O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, and A. Tamchyna, "Findings of the 2014 workshop on statistical machine translation," in _WMT@ACL_. The Association for Computer Linguistics, 2014, pp. 12-58.
* [540] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck, A. Jimeno-Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Neveol, M. L. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri, "Findings of the 2016 conference on machine translation," in _WMT_. The Association for Computer Linguistics, 2016, pp. 131-198.
* Volume 2: 공유 작업 페이퍼, Day 1_, O. 보자르 채터지 C. 페더만 피셸 그레이엄 B. 해도 헉
A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. 흑인, A. 네벌, M. L. 네베스, M. 포스트 엠. Turchi, K. Verspoor (2019) A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. 흑인, A. 네벌, M. L. 네베스, M. 포스트 엠. Turchi, K. 불쌍한 놈 (2019) A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. 흑인, A. 네벌, M. L. 네베스, M. 포스트 엠. Turchi, K. 불쌍한 놈 (2020) A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M.

- August 4, Volume 1: Long Papers_, 2017, pp. 1601-1611.
* [559] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, "PIQA: reasoning about physical commonsense in natural language," in _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 2020_, pp. 7432-7439.
* ISWC 2019
- 18th International Semantic Web Conference, Auckland, New Zealand, October 26-30, 2019, Proceedings, Part II_, 2019, pp. 69-78.
* [561] Y. Gu, S. Kase, M. Vanni, B. M. Sadler, P. Liang, X. Yan, and Y. Su, "Beyond I.I.D.: three levels of generalization for question answering on knowledge bases," in _WWW '21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021_, 2021, pp. 3477-3488.
* [562] S. Cao, J. Shi, L. Pan, L. Nie, Y. Xiang, L. Hou, J. Li, B. He, and H. Zhang, "KQA pro: A dataset with explicit compositional programs for complex question answering over knowledge base," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, 2022, pp. 6101-6119.
* [563] X. Hu, X. Wu, Y. Shu, and Y. Qu, "Logical form generation via multi-task learning for complex question answering over knowledge bases," in _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, 2022, pp. 1687-1696.
* [564] S. Longpre, Y. Lu, and J. Daiber, "MKQA: A linguistically diverse benchmark for multilingual open domain question answering," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 1389-1406, 2021.
* [565] T. Saikh, T. Ghosal, A. Mittal, A. Ekbal, and P. Bhattacharyya, "Scienceqa: a novel resource for question answering on scholarly articles," _Int. J. Digit. Libr._, vol. 23, no. 3, pp. 289-301, 2022.
* November 4, 2018_, 2018, pp. 2381-2391.
* [567] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, "MS MARCO: A human generated machine reading comprehension dataset," in _Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, 2016.
* [568] T. Khot, P. Clark, M. Guerquin, P. Jansen, and A. Sabharwal, "QASC: A dataset for question answering via sentence composition," in _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, 2020, pp. 8082-8090.
* [569] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, "Squad: 100, 000+ questions for machine comprehension of text," in _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016_, 2016, pp. 2383-2392.
* [570] A. H. Miller, A. Fisch, J. Dodge, A. Karimi, A. Bordes, and J. Weston, "Key-value memory networks for directly reading documents," in _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016_, 2016, pp. 1400-1409.
* [571] B. Goodrich, V. Rao, P. J. Liu, and M. Saleh, "Assessing the factual accuracy of generated text," in _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019_, 2019, pp. 166-175.
* [572] K. Toutanova and D. Chen, "Observed versus latent features for knowledge base and text inference," in _Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, CVSC 2015, Beijing, China, July 26-31, 2015_, 2015, pp. 57-66.
* [573] K. D. Bollacker, C. Evans, P. K. Paritosh, T. Sturge, and J. Taylor, "Freebase: a collaboratively created graph database for structuring human knowledge," in _Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008_, 2008, pp. 1247-1250.
* [574] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel, "Convolutional 2d knowledge graph embeddings," in _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, 2018, pp. 1811-1818.
* [575] G. A. Miller, "Wordnet: A lexical database for english," _Commun. ACM_, pp. 39-41, 1995.
* [576] F. Petroni, T. Rocktaschel, S. Riedel, P. S. H. Lewis, A. Bakhtin, Y. Wu, and A. H. Miller, "Language models as knowledge bases?" in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, 2019, pp. 2463-2473.
* [577] F. Mahdisoltani, J. Biega, and F. M. Suchanek, "YAGO3: A knowledge base from multilingual wikipedias," in _Seventh Biennial Conference on Innovative Data Systems Research, CIDR 2015, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings_, 2015.
* [578] F. M. Suchanek, G. Kasneci, and G. Weikum, "Yago: a core of semantic knowledge," in _Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007_, 2007, pp.

697-706.
* 11월 4일, 2018_. Association for Computational Linguistics, 2018, pp. 2369-2380.
* [580] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, "Boolq: Exploring the surprising difficulty of natural yes/no questions," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2924-2936.
* [581] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, "Socialiqa: Commonsense reasoning about social interactions," _CoRR_, vol. abs/1904.09728, 2019.
* [582] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "Hellaswag: Can a machine really finish your sentence?" in _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL, 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, A. Korhonen, D. R. Traum, and L. Marquez, Eds. Association for Computational Linguistics, 2019, pp. 4791-4800.
* [583] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, "Winogrande: An adversarial winograd schema challenge at scale," in _AAAI_. AAAI Press, 2020, pp. 8732-8740.
* [584] M. Roemmele, C. A. Bejan, and A. S. Gordon, "Choice of plausible alternatives: An evaluation of commonsense causal reasoning," in _Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011_. AAAI, 2011.
* [585] K. Sakaguchi, C. Bhagavatula, R. L. Bras, N. Tandon, P. Clark, and Y. Choi, "proscript: Partially ordered scripts generation," in _Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 2138-2149.
* [586] B. Dalvi, L. Huang, N. Tandon, W. Yih, and P. Clark, "Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)_, M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 1595-1604.
* [587] S. Saha, P. Yadav, L. Bauer, and M. Bansal, "Explagraphs: An explanation graph generation task for structured commonsense reasoning," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 7716-7740.
* [588] O. Tafjord, B. Dalvi, and P. Clark, "Proofwriter: Generating implications, proofs, and abductive statements over natural language," in _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, ser. Findings of ACL, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 3621-3634.
* [589] B. Dalvi, P. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark, "Explaining answers with entailment trees," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 7358-7370.
* [590] A. Saparov and H. He, "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought," _CoRR_, vol. abs/2210.01240, 2022.
* [591] C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gur-Ari, E. Dyer, and B. Neyshabur, "Exploring length generalization in large language models," _CoRR_, vol. abs/2207.04901, 2022.
* [592] A. Patel, S. Bhattacharya, and N. Goyal, "Are NLP models really able to solve simple math word problems?" in _NAACL-HLT_. Association for Computational Linguistics, 2021, pp. 2080-2094.
* [593] S. Roy and D. Roth, "Solving general arithmetic word problems," in _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015_, L. Marquez, C. Callison-Burch, J. Su, D. Pighin, and Y. Marton, Eds. The Association for Computational Linguistics, 2015, pp. 1743-1752.
* [594] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi, "Mathqa: Towards interpretable math word problem solving with operation-based formalisms," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2357-2367.
* August 4, Volume 1: Long Papers_, R. Barzilay와 M. Kan, Eds. Association for Computational Linguistics, 2017, pp. 158-167.
* [596] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Hajishirzi, "Mawps: A math word problem repository," in _Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies_, 2016, pp. 1152-1157.
* [597] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner, "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, 2019, pp. 2368-2378.
* [598] S. Welleck, J. Liu, R. L. Bras, H. Hajishirzi, Y. Choi, and K. Cho, "Naturalproofs: Mathematical theorem proving in natural language," in _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_, J. Vanschoren and S. Yeung, Eds., 2021.
* [599] A. Q. Jiang, W. Li, J. M. Han, and Y. Wu, "Lisa: Language models of isabelle proofs," in _6th Conference on Artificial Intelligence and Theorem Proving_, 2021, pp. 378-392.
* [600] K. Zheng, J. M. Han, and S. Polu, "mini2f: a cross-system benchmark for formal olympiad-level mathematics," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [601] Z. Azerbayev, B. Piotrowski, H. Schoelkopf, E. W. Ayers, D. Radev, and J. Avigad, "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics," _CoRR_, vol. abs/2302.12433, 2023.
* [602] J. Li, X. Cheng, W. X. Zhao, J. Nie, and J. Wen, "Halueval: A large-scale hallucination evaluation benchmark for large language models," _CoRR_, vol. abs/2305.11747, 2023.
* [603] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman, "Crows-pairs: A challenge dataset for measuring social biases in masked language models," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, 2020, pp. 1953-1967.
* [604] R. Rudinger, J. Naradowsky, B. Leonard, and B. V. Durme, "Gender bias in coreference resolution," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers)_, 2018, pp. 8-14.
* [605] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, "Realtoxicityprompts: Evaluating neural toxic degeneration in language models," in _Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020_, ser. Findings of ACL, T. Cohn, Y. He, and Y. Liu, Eds., vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 3356-3369.
* [606] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba, "Virtualhome: Simulating household activities via programs," in _CVPR_. Computer Vision Foundation / IEEE Computer Society, 2018, pp. 8494-8502.
* [607] S. Srivastava, C. Li, M. Lingelbach, R. Martin-Martin, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen, S. Buch, C. K. Liu, S. Savarese, H. Gweon, J. Wu, and L. Fei-Fei, "BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments," in _CoRL_, ser. Proceedings of Machine Learning Research, vol. 164. PMLR, 2021, pp. 477-490.
* [608] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks," in _CVPR_. Computer Vision Foundation / IEEE, 2020, pp. 10 737-10 746.
* [609] M. Shridhar, X. Yuan, M. Cote, Y. Bisk, A. Trischler, and M. J. Hausknecht, "Alfworld: Aligning text and embodied environments for interactive learning," in _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [610] S. Yao, H. Chen, J. Yang, and K. Narasimhan, "Webshop: Towards scalable real-world web interaction with grounded language agents," in _NeurIPS_, 2022.
* [611] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su, "Mind2web: Towards a generalist agent for the web," _CoRR_, vol. abs/2306.06070, 2023.
* [612] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov, "Minerl: A large-scale dataset of minecraft demonstrations," in _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019_, S. Kraus, Ed. ijcai.org, 2019, pp. 2442-2448.
* [613] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D. Huang, Y. Zhu, and A. Anandkumar, "Minedjo: Building open-ended embodied agents with internet-scale knowledge," in _NeurIPS_, 2022.
* [614] P. Lu, L. Qiu, K. Chang, Y. N. Wu, S. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning," _CoRR_, vol. abs/2209.14610, 2022.
* [615] B. Zhang, K. Zhou, X. Wei, W. X. Zhao, J. Sha, S. Wang, and J. rong Wen, "Evaluating and improving tool-augmented computation-intensive math reasoning," _CoRR_, vol. abs/2306.02408, 2023.
* [616] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan, "Gpt4tools: Teaching large language model to use tools via self-instruction," _CoRR_, vol. abs/2305.18752, 2023.
* [617] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, "Gorilla: Large language model connected with massive apis," _CoRR_, vol. abs/2305.15334, 2023.
* [618] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh, "The value of semantic parse labeling for knowledge base question answering," in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers_. The Association for Computer Linguistics, 2016.

* [619] H. Puerto, G. G. Sahin, and I. Gurevych, "Metaqa: Combining expert agents for multi-skill question answering," in _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, A. Vlachos and I. Augenstein, Eds. Association for Computational Linguistics, 2023, pp. 3548-3562.
* [620] P. Pasupat and P. Liang, "Compositional semantic parsing on semi-structured tables," in _Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers_. The Association for Computer Linguistics, 2015, pp. 1470-1480.
* [621] V. Zhong, C. Xiong, and R. Socher, "Seq2sql: Generating structured queries from natural language using reinforcement learning," _CoRR_, vol. abs/1709.00103, 2017.
* [622] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang, "Tabfact: A large-scale dataset for table-based fact verification," in _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* November 4, 2018_, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsuji, Eds. Association for Computational Linguistics, 2018, pp. 3911-3921.
* [624] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," in _ICLR_, 2015.
* [625] K. Papineni, S. Roukos, T. Ward, and W. Zhu, "Bleu: a method for automatic evaluation of machine translation," in _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA_. ACL, 2002, pp. 311-318.
* [626] C.-Y. Lin, "ROUGE: A package for automatic evaluation of summaries," in _Text Summarization Branches Out_. Association for Computational Linguistics, Jul. 2004, pp. 74-81.
* [627] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, "Is chatgpt a good translator? a preliminary study," _arXiv preprint arXiv:2301.08745_, 2023.
* [628] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. R. McKeown, and T. B. Hashimoto, "Benchmarking large language models for news summarization," _CoRR_, vol. abs/2301.13848, 2023.
* [629] T. Goyal, J. J. Li, and G. Durrett, "News summarization and evaluation in the era of GPT-3," _CoRR_, vol. abs/2209.12356, 2022.
* [630] S. Gehrmann, E. Clark, and T. Sellam, "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text," _CoRR_, vol. abs/2202.06935, 2022.
* [631] J. Wang, Y. Liang, F. Meng, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou, "Is chatgpt a good NLG evaluator? A preliminary study," _CoRR_, vol. abs/2303.04048, 2023.
* [632] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, "G-eval: NLG evaluation using GPT-4 with better human alignment," _CoRR_, vol. abs/2303.16634, 2023.
* [633] K. Yang, Y. Tian, N. Peng, and D. Klein, "Re3: Generating longer stories with recursive repropting and revision," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emitrates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 4393-4479.
* [634] W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, and M. Sachan, "Recurrentpt: Interactive generation of (arbitrarily) long text," _CoRR_, vol. abs/2305.13304, 2023.
* [635] S. Gulwani, O. Polozov, and R. Singh, "Program synthesis," _Found. Trends Program. Lang._, vol. 4, no. 1-2, pp. 1-119, 2017.
* [636] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, "Planning with large language models for code generation," 2023.
* [637] M. Welsh, "The end of programming," _Commun. ACM_, vol. 66, no. 1, pp. 34-35, 2023.
* [638] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. WiliE, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung, "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity," _CoRR_, vol. abs/2302.04023, 2023.
* [639] Y. Liu, A. R. Fabbri, P. Liu, Y. Zhao, L. Nan, R. Han, S. Han, S. R. Joty, C. Wu, C. Xiong, and D. Radev, "Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation," _CoRR_, vol. abs/2212.07981, 2022.
* [640] A. R. Fabbri, W. Kryscinski, B. McCann, C. Xiong, R. Socher, and D. R. Radev, "Summeval: Re-evaluating summarization evaluation," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 391-409, 2021.
* [641] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, and F. Wei, "Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing," _CoRR_, vol. abs/2305.15067, 2023.
* [642] X. Wang, X. Tang, W. X. Zhao, J. Wang, and J. Wen, "Rethinking the evaluation for conversational recommendation in the era of large language models," _CoRR_, vol. abs/2305.13112, 2023.
* [643] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan, "Human-like summarization evaluation with chatgpt," _CoRR_, vol. abs/2304.02554, 2023.
* [644] Y. Ji, Y. Gong, Y. Peng, C. Ni, P. Sun, D. Pan, B. Ma, and X. Li, "Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences," _CoRR_, vol. abs/2303.07610, 2023.
* [645] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, "Benchmarking foundation models with language-model-as-an-examiner," _CoRR_, vol. abs/2306.04181,2023.
* [646] Y. Liu, S. Feng, D. Wang, Y. Zhang, and H. Schutze, "Evaluate what you can't evaluate: Unassessable generated responses quality," _CoRR_, vol. abs/2305.14658, 2023.
* [647] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui, "Large language models are not fair evaluators," _CoRR_, vol. abs/2305.17926, 2023.
* [648] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou, C. Gong, Y. Shen, J. Zhou, S. Chen, T. Gui, Q. Zhang, and X. Huang, "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models," _arXiv preprint arXiv:2303.10420_, 2023.
* [649] M. McCloskey and N. J. Cohen, "Catastrophic interference in connectionist networks: The sequential learning problem," in _Psychology of learning and motivation_, 1989, pp. 109-165.
* [650] R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan, "Measuring catastrophic forgetting in neural networks," in _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, 2018, pp. 3390-3398.
* [651] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C. Wu, M. Zhong, P. Yin, S. I. Wang, V. Zhong, B. Wang, C. Li, C. Boyle, A. Ni, Z. Yao, D. Radev, C. Xiong, L. Kong, R. Zhang, N. A. Smith, L. Zettlemoyer, and T. Yu, "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models," in _EMNLP_. Association for Computational Linguistics, 2022, pp. 602-631.
* [652] A. Roberts, C. Raffel, and N. Shazeer, "How much knowledge can you pack into the parameters of a language model?" _in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, 2020, pp. 5418-5426.
* [653] G. Izacard, P. S. H. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, "Few-shot learning with retrieval augmented language models," _CoRR_, vol. abs/2208.03299, 2022.
* [654] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, "Retrieval augmented language model pre-training," in _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, 2020, pp. 3929-3938.
* [655] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela, "Retrieval-augmented generation for knowledge-intensive NLP tasks," in _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [656] Y. Lan, G. He, J. Jiang, J. Jiang, W. X. Zhao, and J. Wen, "Complex knowledge base question answering: A survey," _CoRR_, vol. abs/2108.06688, 2021.
* [657] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre, "Improving language models by retrieving from trillions of tokens," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 2022, pp. 2206-2240.
* [658] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, "Search-in-the-chain: Towards accurate, credible and traceable large language models for knowledge-intensive tasks," _CoRR_, vol. abs/2304.14732, 2023.
* [659] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao, "Check your facts and try again: Improving large language models with external knowledge and automated feedback," _CoRR_, vol. abs/2302.12813, 2023.
* [660] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig, "Active retrieval augmented generation," _CoRR_, vol. abs/2305.06983, 2023.
* [661] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions," _CoRR_, vol. abs/2311.05232, 2023.
* [662] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J. Wen, "Evaluating object hallucination in large vision-language models," _CoRR_, vol. abs/2305.10355, 2023.
* [663] S. Kadavath, T. Conerly, A. Askell, T. J. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Dodds, N. Das-Sarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. B. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish, C. Olah, and J. Kaplan, "Language models (mostly) know what they know," _CoRR_, vol. abs/2207.05221, 2022.
* [664] P. Manakul, A. Liuise, and M. J. F. Gales, "Selfcheck-gpt: Zero-resource black-box hallucination detection for generative large language models," _ArXiv_, vol. abs/2305.06983, 2023.
* [665] S. Agarwal, I. Akkaya, V. Balcom, M. Bavarian, G. Bernadett-Shapiro, G. Brockman, M. Brundage, J. Chan, F. Chantzis, N. Deutsch, B. Eastman, A. Eleti, N. Felix, S. P. Fishman, I. Fulford, C. Gibson, J. Gross, M. Heaton, J. Hilton, X. Hu, S. Jain, H. Jin, L. Kilpatrick, C. Kim, M. Kolhede, A. Mayne, P. McMillan, D. Medina, J. Menick, A. Mishchenko, A. Nair, R. Nayak, A. Neelakantan, R. Nuttall, J. Parish, A. T. Passos, A. Perelman, F. de Avila Belbute Peres, V. Pong, J. Schulman, E. Sigler, N. Staudacher, N. Turley, J. Tworek, R. Greene, A. Vijayvergiya, C. Voss,J. Weng, M. Wiethoff, S. Yoo, K. Yu, W. Zaremba, S. Zhao, W. Zhuk, and B. Zoph, "Chatgpt plugins," _OpenAI Blog_, March 2023.
* [666] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev, "Internet-augmented language models through few-shot prompting for open-domain question answering," _CoRR_, vol. abs/2203.05115, 2022.
* [667] H. Qian, Y. Zhu, Z. Dou, H. Gu, X. Zhang, Z. Liu, R. Lai, Z. Cao, J. Nie, and J. Wen, "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus," _CoRR_, vol. abs/2304.04358, 2023.
* [668] J. Liu, J. Jin, Z. Wang, J. Cheng, Z. Dou, and J. Wen, "RETA-LLM: A retrieval-augmented large language model toolkit," _CoRR_, vol. abs/2306.05212, 2023.
* [669] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, "Knowledge neurons in pretrained transformers," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 8493-8502.
* [670] K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov, "Locating and editing factual associations in gpt," in _Advances in Neural Information Processing Systems_, 2022.
* [671] M. Geva, R. Schuster, J. Berant, and O. Levy, "Transformer feed-forward layers are key-value memories," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 5484-5495.
* [672] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, "Editing large language models: Problems, methods, and opportunities," _CoRR_, vol. abs/2305.13172, 2023.
* [673] P. Wang, N. Zhang, X. Xie, Y. Yao, B. Tian, M. Wang, Z. Xi, S. Cheng, K. Liu, G. Zheng, and H. Chen, "Easyedit: An easy-to-use knowledge editing framework for large language models," _CoRR_, vol. abs/2308.07269, 2023.
* [674] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, "Synthetic prompting: Generating chain-of-thought demonstrations for large language models," _CoRR_, vol. abs/2302.00618, 2023.
* [675] Siftakaur, M. Singh, V. S. B, and N. Malviya, "Mind meets machine: Unravelling gpt-4's cognitive psychology," _CoRR_, vol. abs/2303.11436, 2023.
* [676] M. I. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, C. Sutton, and A. Odena, "Show your work: Scratchpads for intermediate computation with language models," _CoRR_, vol. abs/2112.00114, 2021.
* [677] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, "Limitations of language models in arithmetic and symbolic induction," _CoRR_, vol. abs/2208.05051, 2022.
*18, 2022_, A. Zhang and H. Rangwala, Eds. ACM, 2022, pp. 4571-4581.
* 11th International Conference, CICM 2018, Hagenberg, Austria, August 13-17, 2018, Proceedings_, ser. Lecture Notes in Computer Science, F. Rabe, W. M. Farmer, G. O. Passmore, and A. Youssef, Eds., vol. 11006. Springer, 2018, pp. 255-270.
* [680] S. Polu and I. Sutskever, "Generative language modeling for automated theorem proving," _CoRR_, vol. abs/2009.03393, 2020.
* [681] A. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski, T. Odrzygozdz, P. Milos, Y. Wu, and M. Jamnik, "Thor: Wielding hammers to integrate language models and automated theorem provers," _CoRR_, vol. abs/2205.10893, 2022.
* [682] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, and I. Sutskever, "Formal mathematics statement curriculum learning," _CoRR_, vol. abs/2202.01344, 2022.
* [683] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats, M. Jamnik, and C. Szegedy, "Autoformalization with large language models," _CoRR_, vol. abs/2205.12615, 2022.
* [684] A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample, "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs," _CoRR_, vol. abs/2210.12283, 2022.
* [685] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdandbakhsh, and P. Clark, "Self-refine: Iterative refinement with self-feedback," _CoRR_, vol. abs/2303.17651, 2023.
* [686] N. Shinn, B. Labash, and A. Gopinath, "Reflexion: an autonomous agent with dynamic memory and self-reflection," _CoRR_, vol. abs/2303.11366, 2023.
* [687] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan, and W. Chen, "CRITIC: large language models can self-correct with tool-interactive critiquing," _CoRR_, vol. abs/2305.11738, 2023.
* [688] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins, "Solving math word problems with process- and outcome-based feedback," _CoRR_, vol. abs/2211.14275, 2022.
* [689] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, "Let's verify step by step," _CoRR_, vol. abs/2305.20050, 2023.
* [690] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang, "How well do large language models perform in arithmetic tasks?" _CoRR_, vol. abs/2304.02015, 2023.
* [691] X. Pi, Q. Liu, B. Chen, M. Ziyadi, Z. Lin, Q. Fu, Y. Gao, J. Lou, and W. Chen, "Reasoning like program executors," in _Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Enirates, December 7-11, 2022_, 2022, pp. 761-779.
* [692] H. Zhou, A. Nova, H. Larochelle, A. C. Courville, B. Neyshabur, and H. Sedghi, "Teaching algorithmic reasoning via in-context learning," _CoRR_, vol. abs/2211.09066, 2022.
* [693] A. Parisi, Y. Zhao, and N. Fiedel, "TALM: tool augmented language models," _CoRR_, vol. abs/2205.12255, 2022.
* [694] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," in _ICML_, ser. Proceedings of Machine Learning Research, vol. 162. PMLR, 2022, pp. 9118-9147.
* [695] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P. Oudeyer, "Grounding large language models in interactive environments with online reinforcement learning," _CoRR_, vol. abs/2302.02662, 2023.
* [696] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang, and J. Dai, "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory," _CoRR_, vol. abs/2305.17144, 2023.
* [697] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, "Voyager: An open-ended embodied agent with large language models," _CoRR_, vol. abs/2305.16291, 2023.
* [698] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan, "Do as I can, not as I say: Grounding language in robotic affordances," _CoRR_, vol. abs/2204.01691, 2022.
* [699] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," _CoRR_, vol. abs/2209.07753, 2022.
* [700] Y. Fu, H. Peng, T. Khot, and M. Lapata, "Improving language model negotiation with self-play and in-context learning from AI feedback," _CoRR_, vol. abs/2305.10142, 2023.
* [701] N. Mehta, M. Teruel, P. F. Sanz, X. Deng, A. H. Awadallah, and J. Kiseleva, "Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback," _CoRR_, vol. abs/2304.10750, 2023.
* [702] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, "Gorilla: Large language model connected with massive apis," _CoRR_, vol. abs/2305.15334, 2023.
* [703] S. Hao, T. Liu, Z. Wang, and Z. Hu, "Toolfengpt: Augmenting frozen language models with massive tools via tool embeddings," _CoRR_, vol. abs/2305.11554, 2023.
* [704] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji, S. Mao, Y. Wang, L. Shou, M. Gong, and N. Duan, "Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis," _CoRR_, vol. abs/2303.16434, 2023.
* [705] T. Cai, X. Wang, T. Ma, X. Chen, and D. Zhou, "Large language models as tool makers," _CoRR_, vol. abs/2305.17126, 2023.
* [706] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, "Large language models can self-improve," _CoRR_, vol. abs/2210.11610, 2022.
* [707] E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf, "Open llm leaderboard," [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), 2023.
* [708] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, "Agieval: A human-centric benchmark for evaluating foundation models," _CoRR_, vol. abs/2304.06364, 2023.
* [709] H. Zeng, "Measuring massive multitask chinese understanding," _CoRR_, vol. abs/2304.12986, 2023.
* [710] C. Liu, R. Jin, Y. Ren, L. Yu, T. Dong, X. Peng, S. Zhang, J. Peng, P. Zhang, Q. Lyu, X. Su, Q. Liu, and D. Xiong, "M3KE: A massive multi-level multi-subject knowledge evaluation benchmark for chinese large language models," _CoRR_, vol. abs/2305.10263, 2023.
* [711] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He, "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models," _CoRR_, vol. abs/2305.08322, 2023.
* [712] Z. Gu, X. Zhu, H. Ye, L. Zhang, J. Wang, S. Jiang, Z. Xiong, Z. Li, Q. He, R. Xu, W. Huang, W. Zheng, H. Feng, and Y. Xiao, "Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation," _CoRR_, vol. abs/2306.05783, 2023.
* [713] O. Contributors, "Opencompass: A universal evaluation platform for foundation models," [https://github.com/InternLM/OpenCompass](https://github.com/InternLM/OpenCompass), 2023.
* [714] Y. Fu, L. Ou, M. Chen, Y. Wan, H. Peng, and T. Khot, "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance," _CoRR_, vol. abs/2305.17306, 2023.
* [715] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-li, X. Lv, H. Peng, Z. Yao, X. Zhang, H. Li, C. Li, Z. Zhang, Y. Bai, Y. Liu, A. Xin, N. Lin, K. Yun, L. Gong, J. Chen, Z. Wu, Y. Qi, W. Li, Y. Guan, K. Zeng, J. Qi, H. Jin, J. Liu, Y. Gu, Y. Yao, N. Ding, L. Hou, Z. Liu, B. Xu, J. Tang, and J. Li, "Kola: Carefully benchmarking world knowledge of large language models," _CoRR_, vol. abs/2306.09296, 2023.
* [716] T. Sawada, D. Paleka, A. Havrilla, P. Tadepalli, P. Vidas, A. Kranias, J. J. Nay, K. Gupta, and A. Komatuszaki, "ARB: advanced reasoning benchmark for large language models," _CoRR_, vol. abs/2307.13692, 2023.
* [717] Y. Peng, S. Li, W. Gu, Y. Li, W. Wang, C. Gao, and M. R. Lyu, "Revisiting, benchmarking and exploring API recommendation: How far are we?" _IEEE Trans. Software Eng._, vol. 49, no. 4, pp. 1876-1897, 2023.

* [718] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y. Li, "Api-bank: A benchmark for tool-augmented llms," _CoRR_, vol. abs/2304.08244, 2023.
* [719] Q. Tang, Z. Deng, H. Lin, X. Han, Q. Liang, and L. Sun, "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases," _CoRR_, vol. abs/2306.05301, 2023.
* [720] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, and J. Zhang, "On the tool manipulation capability of open-source large language models," _CoRR_, vol. abs/2305.16504, 2023.
* [721] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun, "Toolllm: Facilitating large language models to master 16000+ real-world apis," _CoRR_, vol. abs/2307.16789, 2023.
* [722] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng, Z. Chen, J. C. Niebles, D. Arpit, R. Xu, P. Mui, H. Wang, C. Xiong, and S. Savarese, "BOLAA: benchmarking and orchestrating llm-augmented autonomous agents," _CoRR_, vol. abs/2308.05960, 2023.
* [723] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang, "Agenthenech: Evaluating llms as agents," _CoRR_, vol. abs/2308.03688, 2023.
* [724] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, N. Z. Gong, Y. Zhang, and X. Xie, "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts," _CoRR_, vol. abs/2306.04528, 2023.
* [725] R. S. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du, S. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang, "WHEN FLUE MEETS FLANG: benchmarks and large pre-trained language model for financial domain," _CoRR_, vol. abs/2211.00803, 2022.
* [726] N. Guha, D. E. Ho, J. Nyarko, and C. Re, "Legalbench: Prototyping a collaborative benchmark for legal reasoning," _CoRR_, vol. abs/2209.06120, 2022.
* [727] L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica, "Judging llm-as-a-judge with mt-bench and chatbot arena," _CoRR_, vol. abs/2306.05685, 2023.
* [728] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Leonba, S. Zhang, Y. Sun, and W. Wang, "Scibench: Evaluating college-level scientific problem-solving abilities of large language models," _CoRR_, vol. abs/2307.10635, 2023.
* [729] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto, "Alpacaeval: An automatic evaluator of instruction-following models," [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.
* [730] Y. Huang, Q. Zhang, P. S. Yu, and L. Sun, "Trustgpt: A benchmark for trustworthy and responsible large language models," _CoRR_, vol. abs/2306.11507, 2023.
* [731] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, "Benchmarking foundation models with language-model-as-an-examiner," _CoRR_, vol. abs/2306.04181, 2023.
* [732] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu, "Chateval: Towards better llm-based evaluators through multi-agent debate," _CoRR_, vol. abs/2308.07201, 2023.
* [733] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, "A survey on evaluation of large language models," _CoRR_, vol. abs/2307.03109, 2023.
* [734] Z. Zhuang, Q. Chen, L. Ma, M. Li, Y. Han, Y. Qian, H. Bai, Z. Feng, W. Zhang, and T. Liu, "Through the lens of core competency: Survey on evaluation of large language models," _CoRR_, vol. abs/2308.07902, 2023.
* [735] J. H. Clark, J. Palomaki, V. Nikolaev, E. Choi, D. Garrette, M. Collins, and T. Kwiatkowski, "Tyld QA: A benchmark for information-seeking question answering in typologically diverse languages," _Trans. Assoc. Comput. Linguistics_, vol. 8, pp. 454-470, 2020.
* [736] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, "A framework for few-shot language model evaluation," Sep. 2021.
* [737] R. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du, S. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang, "When flue meets flang: Benchmarks and large pre-trained language model for financial domain," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 2322-2335.
* [738] K. Zhou, Y. Zhu, Z. Chen, W. Chen, W. X. Zhao, X. Chen, Y. Lin, J.-R. Wen, and J. Han, "Don't make your llm an evaluation benchmark cheater," _arXiv preprint arXiv:2311.01964_, 2023.
* [739] C. Zan, K. Peng, L. Ding, B. Qiu, B. Liu, S. He, Q. Lu, Z. Zhang, C. Liu, W. Liu, Y. Zhan, and D. Tao, "Vegamt: The JD explore academy machine translation system for WMT22," in _Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022_, P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussa, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes, T. Kocni, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Neveol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, Eds. Association for Computational Linguistics, 2022, pp. 411-422.
* [740] Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P. J. Liu, "Calibrating sequence likelihood improves conditional language generation," _CoRR_, vol. abs/2210.00045, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2210.00045](https://doi.org/10.48550/arXiv.2210.00045)
* [741] D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi, "Unifiedqa: Crossing format boundaries with a single QA system," in _EMNLP (Findings)_, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 1896-1907.

* [742] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang, "Solving math word problem via cooperative reasoning induced language models," _arXiv preprint arXiv:2210.16257_, 2022.
* [743] A. Nguyen, N. Karampatziakis, and W. Chen, "Meet in the middle: A new pre-training paradigm," _CoRR_, vol. abs/2303.07295, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2303.07295](https://doi.org/10.48550/arXiv.2303.07295)
* [744] H. Li, J. Zhang, C. Li, and H. Chen, "RESDSQL: decoupling schema linking and skeleton parsing for text-to-sql," _CoRR_, vol. abs/2302.05965, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2302.05965](https://doi.org/10.48550/arXiv.2302.05965)
* [745] W. Kang and J. J. McAuley, "Self-attentive sequential recommendation," in _IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018_. IEEE Computer Society, 2018, pp. 197-206.
* [746] B. Yang, C. Han, Y. Li, L. Zuo, and Z. Yu, "Improving conversational recommendation systems' quality with context-aware item meta-information," in _Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds. Association for Computational Linguistics, 2022, pp. 38-48.
* [747] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo, "Falcon-40B: an open large language model with state-of-the-art performance," 2023.
* [748] S. Martin, J. Liermann, and H. Ney, "Algorithms for bigram and trigram word clustering," _Speech communication_, vol. 24, no. 1, pp. 19-37, 1998.
* [749] R. Navigli, "Word sense disambiguation: A survey," _ACM computing surveys (CSUR)_, vol. 41, no. 2, pp. 1-69, 2009.
* [750] W. H. Gomaa, A. A. Fahmy _et al._, "A survey of text similarity approaches," _international journal of Computer Applications_, vol. 68, no. 13, pp. 13-18, 2013.
* [751] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao, "Deep learning-based text classification: a comprehensive review," _ACM computing surveys (CSUR)_, vol. 54, no. 3, pp. 1-40, 2021.
* [752] N. Alex, E. Lifland, L. Tunstall, A. Thakur, P. Maham, C. J. Riedel, E. Hine, C. Ashurst, P. Sedille, A. Carlier, M. Noetel, and A. Stuhlmuller, "RAFT: A real-world few-shot text classification benchmark," in _NeurIPS Datasets and Benchmarks_, 2021.
* [753] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, "Is chatgpt a general-purpose natural language processing task solver?" _CoRR_, vol. abs/2302.06476, 2023.
* [754] X. Chen, J. Ye, C. Zu, N. Xu, R. Zheng, M. Peng, J. Zhou, T. Gui, Q. Zhang, and X. Huang, "How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks," 2023.
* [755] D. Nadeau and S. Sekine, "A survey of named entity recognition and classification," _Linguistic Investigationes_, vol. 30, no. 1, pp. 3-26, 2007.
* [756] A. Ratnaparkhi, "A maximum entropy model for part-of-speech tagging," in _Conference on empirical methods in natural language processing_, 1996.
* [757] V. Yadav and S. Bethard, "A survey on recent advances in named entity recognition from deep learning models," in _Proceedings of the 27th International Conference on Computational Linguistics_, 2018, pp. 2145-2158.
* [758] F. Souza, R. Nogueira, and R. Lotufo, "Portuguese named entity recognition using bert-crf," _arXiv preprint arXiv:1909.10649_, 2019.
* [759] S. Pawar, G. K. Palshikar, and P. Bhattacharyya, "Relation extraction: A survey," _arXiv preprint arXiv:1712.05191_, 2017.
* [760] C. Walker and et al., "Ace 2005 multilingual training corpus ldc2006t06," Philadelphia, 2006.
* [761] J. Gao, H. Zhao, C. Yu, and R. Xu, "Exploring the feasibility of chatgpt for event extraction," _CoRR_, vol. abs/2303.03836, 2023.
* [762] Y. Ma, Y. Cao, Y. Hong, and A. Sun, "Large language model is not a good few-shot information extractor, but a good reranker for hard samples!" _CoRR_, vol. abs/2303.08559, 2023.
* [763] R. Tang, X. Han, X. Jiang, and X. Hu, "Does synthetic data generation of l1ms help clinical text mining?" _arXiv preprint arXiv:2303.04360_, 2023.
* [764] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar _et al._, "Tensor2tensor for neural machine translation," in _Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)_, 2018, pp. 193-199.
* [765] B. Zhang, B. Haddow, and A. Birch, "Prompting large language model for machine translation: A case study," _arXiv preprint arXiv:2301.07069_, 2023.
* [766] M. Ghazvininejad, H. Gonen, and L. Zettlemoyer, "Dictionary-based phrase-level prompting of large language models for machine translation," _arXiv preprint arXiv:2302.07856_, 2023.
* [767] L. Wang, C. Lyu, T. Ji, Z. Zhang, D. Yu, S. Shi, and Z. Tu, "Document-level machine translation with large language models," _arXiv preprint arXiv:2304.02210_, 2023.
* [768] W. Jiao, J.-t. Huang, W. Wang, X. Wang, S. Shi, and Z. Tu, "Parrot: Translating during chat using large language models," _arXiv preprint arXiv:2304.02426_, 2023.
* [769] W. Yang, C. Li, J. Zhang, and C. Zong, "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages," _arXiv preprint arXiv:2305.18098_, 2023.
* [770] J. Kocon, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydlo, J. Baran, J. Bielaniewicz, M. Gruza, A. Janz, K. Kanclerz, A. Kocon, B. Koptyra, W. Mieleszczenko-Kowszewicz, P. Milkowski, M. Oleksky, M. Piasecki, L. Radlinski, K. Wojtasik, S. Wozniak, and P. Kazienko, "Chatgpt: Jack of all trades, master of none," _CoRR_, vol. abs/2302.10724, 2023.
* [771] Q. Zhong, L. Ding, J. Liu, B. Du, and D. Tao, "Can chatgpt understand too? A comparative study on chatgpt and fine-tuned BERT," _CoRR_, vol. abs/2302.10198, 2023.
* [772] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun, F. Wei, D. Deng, and Q. Zhang, "Uprise:Universal prompt retrieval for improving zero-shot evaluation," _arXiv preprint arXiv:2303.08518_, 2023.
* [773] R. Ren, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu, H. Wang, and J.-R. Wen, "Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021, pp. 2825-2835.
* [774] W. Sun, L. Yan, X. Ma, P. Ren, D. Yin, and Z. Ren, "Is chatgpt good at search? investigating large language models as re-ranking agent," _arXiv preprint arXiv:2304.09542_, 2023.
* [775] Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen, T. Liu, J. Liu, D. Metzler, X. Wang _et al._, "Large language models are effective text rankers with pairwise ranking prompting," _arXiv preprint arXiv:2306.17563_, 2023.
* [776] S. Cho, S. Jeong, J. Seo, and J. C. Park, "Discrete prompt optimization via constrained generation for zero-shot re-ranker," _arXiv preprint arXiv:2305.13729_, 2023.
* [777] R. Tang, X. Zhang, X. Ma, J. Lin, and F. Ture, "Found in the middle: Permutation self-consistency improves listwise ranking in large language models," _arXiv preprint arXiv:2310.07712_, 2023.
* [778] X. Ma, X. Zhang, R. Pradeep, and J. Lin, "Zero-shot listwise document reranking with a large language model," _arXiv preprint arXiv:2305.02156_, 2023.
* [779] S. Zhuang, H. Zhuang, B. Koopman, and G. Zuccon, "A setwise approach for effective and highly efficient zero-shot ranking with large language models," _arXiv preprint arXiv:2310.09497_, 2023.
* [780] H. Zhuang, Z. Qin, K. Hui, J. Wu, L. Yan, X. Wang, and M. Berdersky, "Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels," _arXiv preprint arXiv:2310.14122_, 2023.
* [781] N. Ziems, W. Yu, Z. Zhang, and M. Jiang, "Large language models are built-in autoregressive search engines," _arXiv preprint arXiv:2305.09612_, 2023.
* [782] X. Ma, L. Wang, N. Yang, F. Wei, and J. Lin, "Finetuning llama for multi-stage text retrieval," _arXiv preprint arXiv:2310.08319_, 2023.
* [783] R. Pradeep, S. Sharifymoghaddam, and J. Lin, "Rankvicuna: Zero-shot listwise document reranking with open-source large language models," _arXiv preprint arXiv:2309.15088_, 2023.
* [784] Y. Tay, V. Q. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui, Z. Zhao, J. Gupta _et al._, "Transformer memory as a differentiable search index," in _Advances in Neural Information Processing Systems_, 2022.
* [785] R. Ren, W. X. Zhao, J. Liu, H. Wu, J.-R. Wen, and H. Wang, "TOME: A two-stage approach for model-based retrieval," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Association for Computational Linguistics, 2023, pp. 6102-6114. [Online]. Available: [https://aclanthology.org/2023.acl-long.336](https://aclanthology.org/2023.acl-long.336)
* [786] Y. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang, "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering," in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021, pp. 5835-5847.
* [787] R. Ren, S. Lv, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu, H. Wang, and J.-R. Wen, "Pair: Leveraging passage-centric similarity relation for improving dense passage retrieval," in _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, 2021, pp. 2173-2183.
* [788] Z. Peng, X. Wu, and Y. Fang, "Soft prompt tuning for augmenting dense retrieval with large language models," _arXiv preprint arXiv:2307.08303_, 2023.
* [789] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. Hall, and M.-W. Chang, "Promptagator: Few-shot dense retrieval from 8 examples," in _The Eleventh International Conference on Learning Representations_, 2023.
* [790] A. Askari, M. Aliannejadi, E. Kanoulas, and S. Verberne, "Generating synthetic documents for cross-encoder re-rankers: A comparative study of chatgpt and human experts," _arXiv preprint arXiv:2305.02320_, 2023.
* [791] K. Mao, Z. Dou, H. Chen, F. Mo, and H. Qian, "Large language models know your contextual search intent: A prompting framework for conversational search," _arXiv preprint arXiv:2303.06573_, 2023.
* [792] L. Gao, X. Ma, J. Lin, and J. Callan, "Precise zero-shot dense retrieval without relevance labels," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Association for Computational Linguistics, 2023, pp. 1762-1777.
* [793] L. Wang, N. Yang, and F. Wei, "Query2doc: Query expansion with large language models," _arXiv preprint arXiv:2303.07678_, 2023.
* [794] G. Ma, X. Wu, P. Wang, Z. Lin, and S. Hu, "Pre-training with large language model-based document expansion for dense passage retrieval," _arXiv preprint arXiv:2308.08285_, 2023.
* [795] W. Sun, Z. Chen, X. Ma, L. Yan, S. Wang, P. Ren, Z. Chen, D. Yin, and Z. Ren, "Instruction distillation makes large language models efficient zero-shot rankers," _arXiv preprint arXiv:2311.01555_, 2023.
* [796] X. Wang, W. Zhu, and W. Y. Wang, "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning," _CoRR_, vol. abs/2301.11916, 2023.
* [797] C. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao, "Multimodal foundation models: From specialists to general-purpose assistants," _CoRR_, vol. abs/2309.10020, 2023.
* [798] W. X. Zhao, S. Mu, Y. Hou, Z. Lin, Y. Chen, X. Pan, K. Li, Y. Lu, H. Wang, C. Tian, Y. Min, Z. Feng, X. Fan, X. Chen, P. Wang, W. Ji, Y. Li, X. Wang, and J. Wen, "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms," in _CIKM_, G. Demartini, G. Zuccon, J. S. Culpepper, Z. Huang, and H. Tong, Eds. ACM, 2021, pp. 4653-4664.

* [799] K. Zhou, H. Wang, W. X. Zhao, Y. Zhu, S. Wang, F. Zhang, Z. Wang, and J. Wen, "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization," in _CIKM_, M. d'Aquin, S. Dietze, C. Hauff, E. Curry, and P. Cudre-Mauroux, Eds. ACM, 2020, pp. 1893-1902.
* [800] W. X. Zhao, Y. Hou, X. Pan, C. Yang, Z. Zhang, Z. Lin, J. Zhang, S. Bian, J. Tang, W. Sun, Y. Chen, L. Xu, G. Zhang, Z. Tian, C. Tian, S. Mu, X. Fan, X. Chen, and J. Wen, "Recbole 2.0: Towards a more up-to-date recommendation library," in _CIKM_, M. A. Hasan and L. Xiong, Eds. ACM, 2022, pp. 4722-4726.
* [801] L. Xu, Z. Tian, G. Zhang, J. Zhang, L. Wang, B. Zheng, Y. Li, J. Tang, Z. Zhang, Y. Hou, X. Pan, W. X. Zhao, X. Chen, and J. Wen, "Towards a more user-friendly and easy-to-use benchmark library for recommender systems," in _SIGIR_, H. Chen, W. E. Duh, H. Huang, M. P. Kato, J. Mothe, and B. Poblete, Eds. ACM, 2023, pp. 2837-2847.
* [802] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, "BPR: bayesian personalized ranking from implicit feedback," _CoRR_, vol. abs/1205.2618, 2012.
* [803] W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, J. Tang, and Q. Li, "Recommender systems in the era of large language models (IIms)," _CoRR_, 2023.
* [804] L. Wu, Z. Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu, H. Xiong, and E. Chen, "A survey on large language models for recommendation," _CoRR_, 2023.
* [805] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, "Chat-rec: Towards interactive and explainable llms-augmented recommender system," _CoRR_, vol. abs/2303.14524, 2023.
* [806] S. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun, X. Zhang, and J. Xu, "Uncovering chatgpt's capabilities in recommender systems," in _RecSys_, J. Zhang, L. Chen, S. Berkovsky, M. Zhang, T. D. Noia, J. Basilico, L. Pizzato, and Y. Song, Eds. ACM, 2023, pp. 1126-1132.
* [807] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley, and W. X. Zhao, "Large language models are zero-shot rankers for recommender systems," _CoRR_, 2023.
* [808] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, "Is chatgpt a good recommender? A preliminary study," _CoRR_, vol. abs/2304.10149, 2023.
* [809] K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He, "Tallrec: An effective and efficient tuning framework to align large language model with recommendation," in _RecSys_, J. Zhang, L. Chen, S. Berkovsky, M. Zhang, T. D. Noia, J. Basilico, L. Pizzato, and Y. Song, Eds. ACM, 2023, pp. 1007-1014.
* [810] Y. Zhu, L. Wu, Q. Guo, L. Hong, and J. Li, "Collaborative large language model for recommender systems," _arXiv preprint arXiv:2311.01343_, 2023.
* [811] B. Zheng, Y. Hou, H. Lu, Y. Chen, W. X. Zhao, and J.-R. Wen, "Adapting large language models by integrating collaborative semantics for recommendation," 2023. [Online]. Available: [https://api.semanticscholar.org/CorpusID:265213194](https://api.semanticscholar.org/CorpusID:265213194)
* [812] Y. Xi, W. Liu, J. Lin, J. Zhu, B. Chen, R. Tang, W. Zhang, R. Zhang, and Y. Yu, "Towards open-world recommendation with knowledge augmentation from large language models," _CoRR_, vol. abs/2306.10933, 2023.
* [813] Q. Liu, N. Chen, T. Sakai, and X. Wu, "A first look at llm-powered generative news recommendation," _CoRR_, vol. abs/2305.06566, 2023.
* [814] R. Li, W. Deng, Y. Cheng, Z. Yuan, J. Zhang, and F. Yuan, "Exploring the upper limits of text-based collaborative filtering using large language models: Discoveries and insights," _CoRR_, vol. abs/2305.11700, 2023.
* [815] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang, "Llmrec: Large language models with graph augmentation for recommendation," _CoRR_, vol. abs/2311.00423, 2023.
* [816] X. Li, B. Chen, L. Hou, and R. Tang, "Ctrl: Connect tabular and language model for ctr prediction," _arXiv preprint arXiv:2306.02841_, 2023.
* [817] A. Muhamed, I. Keivanloo, S. Perera, J. Mracek, Y. Xu, Q. Cui, S. Rajagopalan, B. Zeng, and T. Chilimbi, "Ctr-bert: Cost-effective knowledge distillation for billion-parameter teacher models," in _NeurIPS Efficient Natural Language and Speech Processing Workshop_, 2021.
* [818] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, "A survey on large language model based autonomous agents," _CoRR_, vol. abs/2308.11432, 2023.
* [819] L. Wang, J. Zhang, X. Chen, Y. Lin, R. Song, W. X. Zhao, and J. Wen, "Recagent: A novel simulation paradigm for recommender systems," _CoRR_, vol. abs/2306.02552, 2023.
* [820] E. Ie, C. Hsu, M. Mladenov, V. Jain, S. Narvekar, J. Wang, R. Wu, and C. Boutilier, "Recsim: A configurable simulation platform for recommender systems," _CoRR_, vol. abs/1909.04847, 2019.
* [821] J. Zhang, Y. Hou, R. Xie, W. Sun, J. J. McAuley, W. X. Zhao, L. Lin, and J. Wen, "Agentet: Collaborative learning with autonomous language agents for recommender systems," _CoRR_, vol. abs/2310.09233, 2023.
* [822] A. Zhang, L. Sheng, Y. Chen, H. Li, Y. Deng, X. Wang, and T. Chua, "On generative agents in recommendation," _CoRR_, vol. abs/2310.10108, 2023.
* [823] Y. Du, Z. Liu, J. Li, and W. X. Zhao, "A survey of vision-language pre-trained models," in _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, L. D. Raedt, d. ijcai.org, 2022, pp. 5436-5443.
* [824] Z. Gan, L. Li, C. Li, L. Wang, Z. Liu, and J. Gao, "Vision-language pre-training: Basics, recent advances, and future trends," _Found. Trends Comput. Graph. Vis._, vol. 14, no. 3-4, pp. 163-352, 2022.
* [825] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. de Chaumont Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov _et al._, "Autoploam: A large language model that can speak and listen," _CoRR_, 2023.
* [826] J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick,S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, "Flamingo: a visual language model for few-shot learning," in _NeurIPS_, 2022.
* [827] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Chertl, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmareczyk, and J. Jitsev, "LAION-5B: an open large-scale dataset for training next generation image-text models," in _NeurIPS_, 2022.
* [828] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts," in _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_. Computer Vision Foundation _ _IEEE_, 2021, pp. 3558-3568.
* [829] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang, "mplug-owl: Modularization empowers large language models with multimodality," _CoRR_, vol. abs/2304.14178, 2023.
* [830] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, "Owen-vl: A frontier large vision-language model with versatile abilities," _CoRR_, vol. abs/2308.12966, 2023.
* [831] H. Liu, C. Li, Y. Li, and Y. J. Lee, "Improved baselines with visual instruction tuning," _CoRR_, vol. abs/2310.03744, 2023.
* [832] P. Zhang, X. Dong, B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang, H. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He, X. Zhang, Y. Qiao, D. Lin, and J. Wang, "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition," _CoRR_, vol. abs/2309.15112, 2023.
* [833] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, "Shixra: Unleashing multimodal llm's referential dialogue magic," _CoRR_, vol. abs/2306.15195, 2023.
* [834] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, "Aligning large multi-modal model with robust instruction tuning," _CoRR_, vol. abs/2306.14565, 2023.
* [835] Y. Du, H. Guo, K. Zhou, W. X. Zhao, J. Wang, C. Wang, M. Cai, R. Song, and J.-R. Wen, "What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning," 2023.
* [836] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, "Vizwiz grand challenge: Answering visual questions from blind people," in _CVPR_. Computer Vision Foundation / IEEE Computer Society, 2018, pp. 3608-3617.
* [837] A. Mishra, K. Alahari, and C. V. Jawahar, "Top-down and bottom-up cues for scene text recognition," in _CVPR_. IEEE Computer Society, 2012, pp. 2687-2694.
* [838] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin, "Mmbench: Is your multi-modal model an all-around player?" _CoRR_, vol. abs/2307.06281, 2023.
* [839] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji, "MME: A comprehensive evaluation benchmark for multimodal large language models," _CoRR_, vol. abs/2306.13394, 2023.
* [840] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, "Sien's song in the AI ocean: A survey on hallucination in large language models," _CoRR_, vol. abs/2309.01219, 2023.
* [841] A. Gunjal, J. Yin, and E. Bas, "Detecting and preventing hallucinations in large vision language models," _CoRR_, vol. abs/2308.06394, 2023.
* [842] J. Lu, J. Rao, K. Chen, X. Guo, Y. Zhang, B. Sun, C. Yang, and J. Yang, "Evaluation and mitigation of agnosis in multimodal large language models," _CoRR_, vol. abs/2309.04041, 2023.
* [843] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, "Object hallucination in image captioning," in _EMNLP_. Association for Computational Linguistics, 2018, pp. 4035-4045.
* [844] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, "Evaluating object hallucination in large vision-language models," in _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023. [Online]. Available: [https://openreview.net/forum?id=xozJw0kZXF](https://openreview.net/forum?id=xozJw0kZXF)
* [845] D. A. Hudson and C. D. Manning, "GQA: A new dataset for real-world visual reasoning and compositional question answering," in _CVPR_. Computer Vision Foundation / IEEE, 2019, pp. 6700-6709.
* [846] P. Lu, S. Mishra, T. Xia, L. Qiu, K. Chang, S. Zhu, O. Tafjord, P. Clark, and A. Kalyan, "Learn to explain: Multimodal reasoning via thought chains for science question answering," in _NeurIPS_, 2022.
* [847] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach, "Towards vqa models that can read," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2019, pp. 8317-8326.
* [848] F. Liu, T. Guan, Z. Li, L. Chen, Y. Yacoob, D. Manocha, and T. Zhou, "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5, and other multi-modality models," _CoRR_, vol. abs/2310.14566, 2023.
* [849] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, "VQA: visual question answering," in _ICCV_. IEEE Computer Society, 2015, pp. 2425-2433.
* [850] R. Vedantam, C. L. Zitnick, and D. Parikh, "Cider: Consensus-based image description evaluation," in _CVPR_. IEEE Computer Society, 2015, pp. 4566-4575.
* [851] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," _CoRR_, vol. abs/2304.08485, 2023.
* [852] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo, "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models," _CoRR_, vol. abs/2306.09265, 2023.
* [853] Z. Li, Y. Wang, M. Du, Q. Liu, B. Wu, J. Zhang, C. Zhou, Z. Fan, J. Fu, J. Chen, X. Huang, and Z. Wei,"Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks," _CoRR_, vol. abs/2310.02569, 2023.
* [854] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, "Seed-bench: Benchmarking multimodal llms with generative comprehension," _CoRR_, vol. abs/2307.16125, 2023.
* [855] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, "Mm-vet: Evaluating large multimodal models for integrated capabilities," _CoRR_, vol. abs/2308.02490, 2023.
* [856] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y. Jiang, "To see is to believe: Prompting GPT-4V for better visual instruction tuning," _CoRR_, vol. abs/2311.07574, 2023.
* [857] Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang, and T. Sun, "Llavar: Enhanced visual instruction tuning for text-rich image understanding," _arXiv preprint arXiv:2306.17107_, 2023.
* [858] X. Qi, K. Huang, A. Panda, M. Wang, and P. Mittal, "Visual adversarial examples jailbreak aligned large language models," in _The Second Workshop on New Frontiers in Adversarial Machine Learning_, 2023.
* [859] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, "Analyzing and mitigating object hallucination in large vision-language models," _arXiv preprint arXiv:2310.00754_, 2023.
* [860] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang _et al._, "Aligning large multimodal models with factually augmented rlhf," _arXiv preprint arXiv:2309.14525_, 2023.
* [861] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, "Unifying large language models and knowledge graphs: A roadmap," _CoRR_, vol. abs/2306.08302, 2023.
* 17th International Conference, ESWC 2020, Heraklion, Crete, Greece, May 31-June 4, 2020, Proceedings_, ser. Lecture Notes in Computer Science, vol. 12123. Springer, 2020, pp. 514-530.
* [863] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu, W. Gong, J. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, "ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation," _CoRR_, vol. abs/2107.02137, 2021. [Online]. Available: [https://arxiv.org/abs/2107.02137](https://arxiv.org/abs/2107.02137)
* [864] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, "ERNIE: enhanced language representation with informative entities," in _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_. Association for Computational Linguistics, 2019, pp. 1441-1451.
* [865] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang, "KEPLER: A unified model for knowledge embedding and pre-trained language representation," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 176-194, 2021.
* [866] J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, "Subgraph retrieval enhanced model for multi-hop knowledge base question answering," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_. Association for Computational Linguistics, 2022, pp. 5773-5784.
* [867] P. Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and M. Huang, "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs," in _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, ser. Findings of ACL, vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 2526-2538.
* [868] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, "Large scale knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training," _CoRR_, vol. abs/2010.12688, 2020.
* [869] W. Chen, Y. Su, X. Yan, and W. Y. Wang, "KGPT: knowledge-grounded pre-training for data-to-text generation," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_. Association for Computational Linguistics, 2020, pp. 8635-8648.
* [870] Y. Gu, X. Deng, and Y. Su, "Don't generate, discriminate: A proposal for grounding language models to real-world environments," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_. Association for Computational Linguistics, 2023, pp. 4928-4949.
* [871] L. Luo, Y. Li, G. Haffari, and S. Pan, "Reasoning on graphs: Faithful and interpretable large language model reasoning," _CoRR_, vol. abs/2310.01061, 2023.
* [872] Y. Lan and J. Jiang, "Query graph generation for answering multi-hop complex questions from knowledge bases," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, D. J. and, Ed. Association for Computational Linguistics, 2020, pp. 969-974.
* [873] P. Wang, N. Zhang, X. Xie, Y. Yao, B. Tian, M. Wang, Z. Xi, S. Cheng, K. Liu, G. Zheng, and H. Chen, "Easyedit: An easy-to-use knowledge editing framework for large language models," _CoRR_, vol. abs/2308.07269, 2023.
* [874] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, "Editing large language models: Problems, methods, and opportunities," _CoRR_, vol. abs/2305.13172, 2023.
* [875] S. Choi, T. Fang, Z. Wang, and Y. Song, "KCTS: knowledge-constrained tree search decoding with token-level hallucination detection," _CoRR_, vol. abs/2310.09044, 2023.
* [876] S. Zhang, L. Pan, J. Zhao, and W. Y. Wang, "Mitigating language model hallucination with interactive question-knowledge alignment," _CoRR_, vol. abs/2305.13669, 2023.

* [877] Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, and N. Zhang, "Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities," _CoRR_, vol. abs/2305.13168, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.13168](https://doi.org/10.48550/arXiv.2305.13168)
* [878] S. Russell and P. Norvig, _Artificial Intelligence: A Modern Approach (4th Edition)_. Pearson, 2020. [Online]. Available: [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)
* [879] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, "Building machines that learn and think like people," _CoRR_, vol. abs/1604.00289, 2016.
* [880] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergizing reasoning and acting in language models," _CoRR_, vol. abs/2210.03629, 2022.
* [881] 2023. [Online]. Available: [https://github.com/AntonCiska/gpt-engineer](https://github.com/AntonCiska/gpt-engineer)
* [882] X. Team, "Xagent: An autonomous agent for complex task solving," 2023.
* [883] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, "CAMEL: communicative agents for "mind" exploration of large scale language model society," _CoRR_, vol. abs/2303.17760, 2023.
* [884] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, and C. Wu, "Metagpt: Meta programming for multi-agent collaborative framework," _CoRR_, vol. abs/2308.00352, 2023.
* [885] C. Pham, B. Liu, Y. Yang, Z. Chen, T. Liu, J. Yuan, B. A. Plummer, Z. Wang, and H. Yang, "Let models speak ciphers: Multiagent debate through embeddings," _CoRR_, vol. abs/2310.06272, 2023.
* [886] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, "Improving factuality and reasoning in language models through multiagent debate," _CoRR_, vol. abs/2305.14325, 2023.
* [887] M. Karpinska, N. Akoury, and M. Iyyer, "The perils of using mechanical turk to evaluate open-ended text generation," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 1265-1285.
* [888] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune, and A. Rastogi, "RLAF: scaling reinforcement learning from human feedback with AI feedback," _CoRR_, vol. abs/2309.00267, 2023.
* [889] T. Wang, P. Yu, X. E. Tan, S. O'Brien, R. Pasunuru, J. Dwivedi-Yu, O. Golovneva, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz, "Shepherd: A critic for language model generation," _CoRR_, vol. abs/2308.04592, 2023.
* [890] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun, "Ultrafeedback: Boosting language models with high-quality feedback," _CoRR_, vol. abs/2310.01377, 2023.
* [891] X. Wang, Z. Wang, J. Liu, Y. Chen, L. Yuan, H. Peng, and H. Ji, "MINT: evaluating llms in multi-turn interaction with tools and language feedback," _CoRR_, vol. abs/2309.10691, 2023.
* [892] S. Saha, O. Levy, A. Celikyilmaz, M. Bansal, J. Weston, and X. Li, "Branch-solve-merge improves large language model evaluation and generation," _CoRR_, vol. abs/2310.15123, 2023.
* [893] X. Zhang, B. Yu, H. Yu, Y. Lv, T. Liu, F. Huang, H. Xu, and Y. Li, "Wider and deeper LLM networks are fairer LLM evaluators," _CoRR_, vol. abs/2308.01862, 2023.
* [894] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu, "Chateval: Towards better llm-based evaluators through multi-agent debate," _CoRR_, vol. abs/2308.07201, 2023.
* [895] R. Li, T. Patel, and X. Du, "PRD: peer rank and discussion improve large language model based evaluations," _CoRR_, vol. abs/2307.02762, 2023.
* [896] L. Zhu, X. Wang, and X. Wang, "Judgelm: Fine-tuned large language models are scalable judges," _CoRR_, vol. abs/2310.17631, 2023.
* [897] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen, "Evaluating large language models at evaluating instruction following," _CoRR_, vol. abs/2310.07641, 2023.
* [898] R. Koo, M. Lee, V. Raheja, J. I. Park, Z. M. Kim, and D. Kang, "Benchmarking cognitive biases in large language models as evaluators," _CoRR_, vol. abs/2309.17012, 2023.
* [899] P. West, X. Lu, N. Dziri, F. Brahman, L. Li, J. D. Hwang, L. Jiang, J. Fisher, A. Ravichander, K. Chandu, B. Newman, P. W. Koh, A. Ettinger, and Y. Choi, "The generative AI paradox: "what it can create, it may not understand"," _CoRR_, vol. abs/2311.00059, 2023.
* [900] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, "Large language models cannot self-correct reasoning yet," _CoRR_, vol. abs/2310.01798, 2023.
* [901] K. Stechly, M. Marquez, and S. Kambhampati, "GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems," _CoRR_, vol. abs/2310.12397, 2023.
* [902] O. Nov, N. Singh, and D. M. Mann, "Putting chatgpt's medical advice to the (turing) test," _CoRR_, vol. abs/2301.10035, 2023.
* [903] K. Yang, S. Ji, T. Zhang, Q. Xie, and S. Ananiadou, "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis," _CoRR_, vol. abs/2304.03347, 2023.
* [904] K. Jeblick, B. Schachtner, J. Deal, A. Mittermeier, A. T. Stiuber, J. Topalis, T. Weber, P. Wesp, B. O. Sabel, J. Ricke, and M. Ingrisch, "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports," _CoRR_, vol. abs/2212.14882, 2022.
* [905] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, M. Schaekermann, A. Wang, M. Amin, S. Lachgar, P. A. Mansfield, S. Prakash, B. Green, E. Dominowska, B. A. y Arcas, N. Tomasev, Y. Liu, R. Wong, C. Sembturs, S. S. Mahdavi, J. K. Barral, D. R. Webster, G. S. Corrado, Y. Matias, S. Azizi, A. Karthikesalingam, and V. Natarajan, "Towards expert-level medical question answering with large language models," _CoRR_, vol. abs/2305.09617, 2023.

* [906] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and H. Zan, "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue," _CoRR_, vol. abs/2308.03549, 2023.
* [907] S. Chen, B. H. Kann, M. B. Foote, H. J. Aerts, G. K. Savvao, R. H. Mak, and D. S. Bitterman, "The utility of chatgpt for cancer treatment information," _medRxiv_, 2023.
* [908] K. Malinka, M. Peresini, A. Firc, O. Hujnak, and F. Janus, "On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?" _CoRR_, vol. abs/2303.11146, 2023.
* [909] T. Susnjak, "Chatgpt: The end of online exam integrity?" _CoRR_, vol. abs/2212.09292, 2022.
* [910] K. Tan, T. Pang, and C. Fan, "Towards applying powerful large ai models in classroom teaching: Opportunities, challenges and prospects," 2023.
* [911] F. Kamalov and I. Gurrib, "A new era of artificial intelligence in education: A multifaceted revolution," _CoRR_, vol. abs/2305.18303, 2023.
* [912] E. Kasneci, K. Sessler, S. Kuchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Gunnemann, E. Hullermeier _et al._, "Chatgpt for good? on opportunities and challenges of large language models for education," _Learning and Individual Differences_, vol. 103, p. 102274, 2023.
* [913] A. Blair-Stanek, N. Holzenberger, and B. V. Durme, "Can GPT-3 perform statutory reasoning?" _CoRR_, vol. abs/2302.06100, 2023.
* [914] D. Trautmann, A. Petrova, and F. Schilder, "Legal prompt engineering for multilingual legal judgement prediction," _CoRR_, vol. abs/2212.02199, 2022.
* [915] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, "Chatgpt goes to law school," _Available at SSRN_, 2023.
* [916] J. J. Nay, "Law informs code: A legal informatics approach to aligning artificial intelligence with humans," _CoRR_, vol. abs/2209.13020, 2022.
* [917] F. Yu, L. Quartey, and F. Schilder, "Legal prompting: Teaching a language model to think like a lawyer," _CoRR_, vol. abs/2212.01326, 2022.
* [918] D. Trautmann, A. Petrova, and F. Schilder, "Legal prompt engineering for multilingual legal judgement prediction," _CoRR_, vol. abs/2212.02199, 2022.
* [919] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, "Understanding the capabilities, limitations, and societal impact of large language models," _CoRR_, vol. abs/2102.02503, 2021.
* [920] Z. Sun, "A short survey of viewing large language models in legal aspect," _CoRR_, vol. abs/2303.09136, 2023.
* [921] A. Abid, M. Farooqi, and J. Zou, "Persistent anti-muslim bias in large language models," in _AIES '21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021_, M. Fourcade, B. Kuipers, S. Lazar, and D. K. Mulligan, Eds. ACM, 2021, pp. 298-306.
* [922] A. Shah and S. Chava, "Zero is not hero yet: Benchmarking zero-shot performance of llms for financial tasks," _CoRR_, vol. abs/2305.16633, 2023.
* [923] D. Araci, "Finbert: Financial sentiment analysis with pre-trained language models," _CoRR_, vol. abs/1908.10063, 2019.
*9, 2015_, B. Hachey and K. Webster, Eds. ACL, 2015, pp. 84-90.
* [925] G. Son, H. Jung, M. Hahm, K. Na, and S. Jin, "Beyond classification: Financial reasoning in state-of-the-art language models," _CoRR_, vol. abs/2305.01505, 2023.
* [926] X. Zhang, Q. Yang, and D. Xu, "Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters," _arXiv preprint arXiv:2305.12002_, 2023.
* [927] H. Yang, X.-Y. Liu, and C. D. Wang, "Fingpt: Open-source financial large language models," _CoRR_, vol. abs/2306.06031, 2023.
* [928] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, "Pubmedqa: A dataset for biomedical research question answering," in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, 2019, pp. 2567-2577.
* [929] A. Krikhara, A. Nentidis, K. Bougiatiotis, and G. Paliouras, "Bioasq-qa: A manually curated corpus for biomedical question answering," 2022.
* [930] Z. Bi, N. Zhang, Y. Xue, Y. Ou, D. Ji, G. Zheng, and H. Chen, "Oceangpt: A large language model for ocean science tasks," _CoRR_, vol. abs/2310.02031, 2023.
* [931] C. Zhang, C. Zhang, C. Li, Y. Qiao, S. Zheng, S. K. Dam, M. Zhang, J. U. Kim, S. T. Kim, J. Choi, G. Park, S. Bae, L. Lee, P. Hui, I. S. Kweon, and C. S. Hong, "One small step for generative ai, one giant leap for AGI: A complete survey on chatgpt in AIGC era," _CoRR_, vol. abs/2304.06488, 2023.
* [932] M. Haman and M. Skolnik, "Using chatgpt to conduct a literature review." _Accontability in research_, 2023.
* [933] O. Aydin and E. Karaarslan, "Openai chatgpt generated literature review: Digital twin in healthcare," _SSRN Electronic Journal_, 2022.
* [934] Y. J. Park, D. Kaplan, Z. Ren, C. Hsu, C. Li, H. Xu, S. Li, and J. Li, "Can chatgpt be used to generate scientific hypotheses?" _CoRR_, vol. abs/2304.12208, 2023.
* [935] M. M. Hassan, R. A. Knipper, and S. K. K. Santu, "Chatgpt as your personal data scientist," _CoRR_, vol. abs/2305.13657, 2023.
* [936] L. Cheng, X. Li, and L. Bing, "Is GPT-4 a good data analyst?" _CoRR_, vol. abs/2305.15038, 2023.
* [937] S. I. M. Hussam Alkaissi, "Artificial hallucinations in chatgpt: Implications in scientific writing," _PubMed_, 2023.
* 전문가용," _CoRR_, vol. abs/2306.03102, 2023.
* [939] O. O. Buruk, "Academic writing with GPT-3.5: reflections on practices, efficacy and transparency," _CoRR_, vol. abs/2304.11079, 2023.
* [940] R. Liu and N. B. Shah, "Reviewerpt? an exploratory study on using large language models for paper re viewing," _CoRR_, vol. abs/2306.00622, 2023.
* [941] M. Kosinski, "Theory of mind may have spontaneously emerged in large language models," _CoRR_, vol. abs/2302.02083, 2023.
* [942] M. M. Amin, E. Cambria, and B. W. Schuller, "Will affective computing emerge from foundation models and general ai? A first evaluation on chatgpt," _CoRR_, vol. abs/2303.03186, 2023.
* [943] G. Sridhara, R. H. G., and S. Mazumdar, "Chatgpt: A study on its utility for ubiquitous software engineering tasks," _CoRR_, vol. abs/2305.16837, 2023.
* [944] W. Sun, C. Fang, Y. You, Y. Miao, Y. Liu, Y. Li, G. Deng, S. Huang, Y. Chen, Q. Zhang, H. Qian, Y. Liu, and Z. Chen, "Automatic code summarization via chatgpt: How far are we?" _CoRR_, vol. abs/2305.12865, 2023.
* [945] C. S. Xia and L. Zhang, "Conversational automated program repair," _CoRR_, vol. abs/2301.13246, 2023.
* [946] W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li, B. Ding, and J. Zhou, "Federatedscope-Ilm: A comprehensive package for fine-tuning large language models in federated learning," 2023.

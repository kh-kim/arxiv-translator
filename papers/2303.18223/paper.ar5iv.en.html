<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.18223] A Survey of Large Language Models</title><meta property="og:description" content="Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine.
Language is essentially a complex, intricate system of human expressions governed by grammat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey of Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey of Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.18223">

<!--Generated on Thu Feb 29 17:19:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Large Language Models; Emergent Abilities; Adaptation Tuning;
Utilization; Alignment; Capacity Evaluation
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey of Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen
</span><span class="ltx_author_notes">
Version: v13 (major update on November 23, 2023).
GitHub link: <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey</a>
Chinese version link: <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf</a>
* K. Zhou and J. Li contribute equally to this work.
The authors are mainly with Gaoling School of Artificial Intelligence and School of Information, Renmin University of China, Beijing, China; Jian-Yun Nie is with DIRO, Université de Montréal, Canada. 
<br class="ltx_break">Contact e-mail: batmanfly@gmail.com
<span id="id1.1.id1" class="ltx_text" style="color:#FF0000;">The authors of this survey paper reserve all the copyrights of the figures/tables, and any use of these materials for publication purpose must be officially granted by the survey authors.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine.
Language is essentially a complex, intricate system of human expressions governed by grammatical rules.
It poses a significant challenge to develop capable artificial intelligence&nbsp;(AI) algorithms for comprehending and grasping a language.
As a major approach,
<em id="id2.id1.1" class="ltx_emph ltx_font_italic">language modeling</em>
has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models.
Recently, pre-trained language models&nbsp;(PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing&nbsp;(NLP) tasks.
Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (<em id="id2.id1.2" class="ltx_emph ltx_font_italic">e.g.,</em> in-context learning) that are not present in small-scale language models (<em id="id2.id1.3" class="ltx_emph ltx_font_italic">e.g.,</em> BERT).
To discriminate the language models in different parameter scales,
the research community has coined the term <em id="id2.id1.4" class="ltx_emph ltx_font_italic">large language models&nbsp;(LLM)</em> for the PLMs of significant size (<em id="id2.id1.5" class="ltx_emph ltx_font_italic">e.g.,</em> containing tens or hundreds of billions of parameters).
Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress
is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society.
The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Large Language Models; Emergent Abilities; Adaptation Tuning;
Utilization; Alignment; Capacity Evaluation

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.2.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.1" class="ltx_logical-block">
<div id="S1.1.p1" class="ltx_para">
<p id="S1.1.p1.1" class="ltx_p ltx_align_right"><span id="S1.1.p1.1.1" class="ltx_text ltx_font_italic">“The limits of my language mean the limits of my world.”</span></p>
<p id="S1.1.p1.2" class="ltx_p ltx_align_right">—<em id="S1.1.p1.2.1" class="ltx_emph ltx_font_italic">Ludwig Wittgenstein</em></p>
</div>
</div>
<figure id="S1.F1" class="ltx_figure">
<div id="S1.F1.5" class="ltx_block">
<figure id="S1.F1.2.fig1" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:195.1pt;"><img src="/html/2303.18223/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="266" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption">(a) Query=”Language Model”</figcaption>
</figure>
<figure id="S1.F1.4.fig1" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:195.1pt;"><img src="/html/2303.18223/assets/x2.png" id="S1.F1.3.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="269" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption">(b) Query=”Large Language Model”</figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The trends of the cumulative numbers of arXiv papers that contain the keyphrases “<em id="S1.F1.9.1" class="ltx_emph ltx_font_italic">language model</em>” (since June 2018) and “<em id="S1.F1.10.2" class="ltx_emph ltx_font_italic">large language model</em>” (since October 2019), respectively. The statistics are calculated using exact match by querying the keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because “language models” have been explored at an earlier time. We label the points corresponding to important landmarks in the research progress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers that contain “<em id="S1.F1.11.3" class="ltx_emph ltx_font_italic">large language model</em>” in title or abstract goes from 0.40 per day to 8.58 per day (Figure&nbsp;<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b)). </figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2303.18223/assets/x3.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An evolution process of the four generations of language models&nbsp;(LM) from the perspective of task solving capacity. Note that the time period for each stage may not be very accurate, and we set the time mainly according to the publish date of the most representative studies at each stage. For neural language models, we abbreviate the paper titles of two representative studies to name the two approaches: NPLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> (“<em id="S1.F2.3.1" class="ltx_emph ltx_font_italic">A neural probabilistic language model</em>”) and NLPS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (“<em id="S1.F2.4.2" class="ltx_emph ltx_font_italic">Natural language processing (almost) from scratch</em>”). Due to the space limitation, we don’t list all representative studies in this figure. </figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Language is a prominent ability in human beings to express and communicate, which develops in early childhood and evolves over a lifetime&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Machines, however, cannot naturally grasp the abilities of understanding and communicating in the form of human language, unless equipped with powerful artificial intelligence&nbsp;(AI) algorithms. It has been a longstanding research challenge to achieve this goal, to enable machines to read, write, and communicate like humans&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Technically, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">language modeling&nbsp;(LM)</em> is one of the major approaches to advancing language intelligence of machines.
In general, LM aims to model the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens.
The research of LM has received extensive attention in the literature, which can be divided into four major development stages:</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.3" class="ltx_p"><math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S1.p3.3.1" class="ltx_emph ltx_font_italic">Statistical language models&nbsp;(SLM)</em>. SLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> are developed based on <em id="S1.p3.3.2" class="ltx_emph ltx_font_italic">statistical learning</em> methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, <em id="S1.p3.3.3" class="ltx_emph ltx_font_italic">e.g.,</em> predicting the next word based on the most recent context. The SLMs with a fixed context length <math id="S1.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p3.2.m2.1a"><mi id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">n</annotation></semantics></math> are also called <math id="S1.p3.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p3.3.m3.1a"><mi id="S1.p3.3.m3.1.1" xref="S1.p3.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p3.3.m3.1b"><ci id="S1.p3.3.m3.1.1.cmml" xref="S1.p3.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.3.m3.1c">n</annotation></semantics></math>-gram language models, <em id="S1.p3.3.4" class="ltx_emph ltx_font_italic">e.g.,</em> bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval&nbsp;(IR)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and natural language processing&nbsp;(NLP)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated.
Thus, specially designed smoothing strategies such as back-off estimation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and Good–Turing estimation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have been introduced to alleviate the data sparsity problem.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><math id="S1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p4.1.m1.1a"><mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">Neural language models&nbsp;(NLM)</em>. NLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> characterize the probability of word sequences by neural networks, <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> multi-layer perceptron&nbsp;(MLP) and recurrent neural networks&nbsp;(RNNs).
As a remarkable contribution, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> introduced the concept of <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">distributed representation</em> of words and built the word prediction function conditioned on the aggregated context features (<em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> the distributed word vectors).
By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for various NLP tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Furthermore, word2vec&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks.
These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><math id="S1.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">Pre-trained language models&nbsp;(PLM)</em>. As an early attempt, ELMo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM&nbsp;(biLSTM) network (instead of learning fixed word representations) and then fine-tuning the biLSTM network according to specific downstream tasks. Furthermore, based on the highly parallelizable Transformer
architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with self-attention mechanisms,
BERT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora. These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the “<em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">pre-training</em> and <em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">fine-tuning</em>” learning paradigm.
Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> (<em id="S1.p5.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and BART&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) or improved pre-training strategies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><math id="S1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">Large language models&nbsp;(LLM)</em>. Researchers find that scaling PLM (<em id="S1.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> scaling model size or data size) often leads to an improved model capacity on downstream tasks (<em id="S1.p6.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> following the scaling law&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>). A number of studies have explored the performance limit by training an ever larger PLM (<em id="S1.p6.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> the 175B-parameter GPT-3 and the 540B-parameter PaLM). Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (<em id="S1.p6.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> 330M-parameter BERT and 1.5B-parameter GPT-2) and show surprising abilities (called <em id="S1.p6.1.6" class="ltx_emph ltx_font_italic">emergent abilities</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>) in solving a series of complex tasks.
For example, GPT-3 can solve few-shot tasks through <em id="S1.p6.1.7" class="ltx_emph ltx_font_italic">in-context learning</em>, whereas GPT-2 cannot do well.
Thus, the research community coins the term “<em id="S1.p6.1.8" class="ltx_emph ltx_font_italic">large language models&nbsp;(LLM)</em>”<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that a LLM is not necessarily more capable than a small PLM, and emergent abilities may not occur in some LLMs. </span></span></span> for these large-sized PLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which attract increasing research attention (See Figure&nbsp;<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
A remarkable application of LLMs is <em id="S1.p6.1.9" class="ltx_emph ltx_font_italic">ChatGPT</em><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://openai.com/blog/chatgpt/</span></span></span> that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the arXiv papers that are related to LLMs after the release of ChatGPT in Figure&nbsp;<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">As discussed before, language model is not a new technical concept specially for LLMs, but has evolved with the advance of artificial intelligence over the decades. Early language models mainly aim to model and generate text data, while latest language models (<em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-4) focus on complex task solving. From <em id="S1.p7.1.2" class="ltx_emph ltx_font_italic">language modeling</em> to <em id="S1.p7.1.3" class="ltx_emph ltx_font_italic">task solving</em>, it is an important leap in scientific thinking, which is the key to understand the development of language models in the research history.
From the perspective of task solving, the four generations of language models have exhibited different levels of model capacities.
In Figure&nbsp;<a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we describe the evolution process of language models in terms of the task solving capacity.
At first, statistical language models mainly assisted in some specific tasks (<em id="S1.p7.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> retrieval or speech tasks), in which the predicted or estimated probabilities can enhance the performance of task-specific approaches.
Subsequently, neural language models focused on learning task-agnostic representations (<em id="S1.p7.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> features), aiming to reduce the efforts for human feature engineering. Furthermore, pre-trained language models learned context-aware representations that can be optimized according to downstream tasks. For the latest generation of language model, LLMs are enhanced by exploring the scaling effect on model capacity, which can be considered as general-purpose task solvers. To summarize, in the evolution process, the task scope that can be solved by language models have been greatly extended, and the task performance attained by language models have been significantly enhanced.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In the existing literature, PLMs have been widely discussed and surveyed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, while LLMs are seldom reviewed in a systematic way. To motivate our survey, we first highlight three major differences between LLMs and PLMs.
First, LLMs display some surprising emergent abilities that may not be observed in previous smaller PLMs. These abilities are key to the performance of language models on complex tasks, making AI algorithms unprecedently powerful and effective.
Second, LLMs would revolutionize the way that humans develop and use AI algorithms.
Unlike small PLMs, the major approach to accessing LLMs is through the prompting interface (<em id="S1.p8.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-4 API). Humans have to understand how LLMs work and format their tasks in a way that LLMs can follow.
Third, the development of LLMs no longer draws a clear distinction between research and engineering. The training of LLMs requires extensive practical experiences in large-scale data processing and distributed parallel training.
To develop capable LLMs, researchers have to solve complicated engineering issues, working with engineers or being engineers.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Nowadays, LLMs are posing a significant impact on the AI community, and the advent of ChatGPT and GPT-4 leads to the rethinking of the possibilities of artificial general intelligence&nbsp;(AGI). OpenAI has published a technical article entitled “<em id="S1.p9.1.1" class="ltx_emph ltx_font_italic">Planning for AGI and beyond</em>”, which discusses the short-term and long-term plans to approach AGI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, and a more recent paper has argued that GPT-4 might be considered as an early version of an AGI system&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
The research areas of AI are being revolutionized by the rapid progress of LLMs.
In the field of NLP, LLMs can serve as a general-purpose language task solver (to some extent), and the research paradigm has been shifting towards the use of LLMs.
In the field of IR, traditional search engines are challenged by the new information seeking way through AI chatbots (<em id="S1.p9.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> ChatGPT), and <em id="S1.p9.1.3" class="ltx_emph ltx_font_italic">New Bing</em><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.bing.com/new</span></span></span> presents an initial attempt that enhances the search results based on LLMs.
In the field of CV, the researchers try to develop ChatGPT-like vision-language models that can better serve multimodal dialogues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, and GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> has supported multimodal input by integrating the visual information.
This new wave of technology would potentially lead to a prosperous
ecosystem of real-world applications based on LLMs.
For instance, Microsoft 365 is being empowered by LLMs (<em id="S1.p9.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> Copilot) to automate the office work, and OpenAI supports the use of plugins in ChatGPT for implementing special functions.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Despite the progress and impact, the underlying principles of LLMs are still not well explored. Firstly, it is mysterious why emergent abilities occur in LLMs, instead of smaller PLMs. As a more general issue, there lacks a deep, detailed investigation of the key factors that contribute to the superior abilities of LLMs.
It is important to study when and how LLMs obtain such abilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Although there are some meaningful discussions about this problem&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, more principled investigations are needed to uncover the “<em id="S1.p10.1.1" class="ltx_emph ltx_font_italic">secrets</em>“ of LLMs.
Secondly, it is difficult for the research community to train capable LLMs.
Due to the huge demand of computation resources, it is very costly to carry out repetitive, ablating studies for investigating the effect of various strategies for training LLMs.
Indeed, LLMs are mainly trained by industry, where many important training details (<em id="S1.p10.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> data collection and cleaning) are not revealed to the public.
Thirdly, it is challenging to align LLMs with human values or preferences. Despite the capacities, LLMs are also likely to produce toxic, fictitious, or harmful contents. It requires effective and efficient control approaches to eliminating the potential risk of the use of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">Faced with both opportunities and challenges, it needs more attention on the research and development of LLMs.
In order to provide a basic understanding of LLMs,
this survey conducts a literature review of the recent advances in LLMs
from four major aspects, including <em id="S1.p11.1.1" class="ltx_emph ltx_font_italic">pre-training</em> (how to pre-train a capable LLM), <em id="S1.p11.1.2" class="ltx_emph ltx_font_italic">adaptation</em> (how to effectively adapt pre-trained LLMs for better use), <em id="S1.p11.1.3" class="ltx_emph ltx_font_italic">utilization</em> (how to use LLMs for solving various downstream tasks) and
<em id="S1.p11.1.4" class="ltx_emph ltx_font_italic">capability evaluation</em> (how to evaluate the abilities of LLMs and existing empirical findings).
We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs.
For this survey, we also
create a GitHub project website by collecting the supporting resources for LLMs, at the link <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey</a>.
We are also aware of several related review articles on PLMs or LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. These papers either discuss PLMs or some specific (or general) aspects of LLMs.
Compared with them, we focus on the techniques and methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects of LLMs.</p>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">The remainder of this survey is organized as follows: Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization of available resources for developing LLMs in Section 3. Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation, utilization, and capacity evaluation, respectively.
Then, Section 8 discusses the practical guide for prompt design, and Section 9 reviews the applications of LLMs in several representative domains.
Finally, we conclude the survey in Section 10 by summarizing the major findings and discuss the remaining issues for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Overview</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we present an overview about the background of LLMs and then summarize the
technical evolution of the GPT-series models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Background for LLMs</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Typically, <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">large language models</em>&nbsp;(LLMs) refer to Transformer language models
that contain hundreds of billions&nbsp;(or more) of parameters<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>In existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute. In this survey, we take a slightly loose definition of LLMs, and mainly focus on discussing language models with a model size larger than 10B. </span></span></span>, which are trained on massive text data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, such as GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. LLMs exhibit strong capacities to understand natural language and solve complex tasks (via text generation). To have a quick understanding of how LLMs work, this part introduces the basic background for LLMs, including scaling laws, emergent abilities and key techniques.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Formulation of Scaling Laws for LLMs</span>. Currently, LLMs are mainly built upon the Transformer architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, where multi-head attention layers are stacked in a very deep neural network.
Existing LLMs adopt similar Transformer architectures and pre-training objectives (<em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> language modeling) as small language models.
However, LLMs significantly extend the model size, data size, and total compute (orders of magnification). Extensive research has shown that scaling can largely improve the model capacity of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Thus, it is useful to establish a quantitative approach to characterizing the scaling effect.
Next, we introduce two representative scaling laws for Transformer language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.5" class="ltx_p"><math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mo id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p3.5.1" class="ltx_emph ltx_font_italic">KM scaling law</em><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Since there was not a model trained following this law in the original paper, we took the last names of the two co-first authors to name this scaling law.
</span></span></span>. In 2020, Kaplan et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (<math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">N</annotation></semantics></math>), dataset size (<math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">D</annotation></semantics></math>), and the amount of training compute (<math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">C</annotation></semantics></math>), for neural language models. Given a compute budget <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mi id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><ci id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">c</annotation></semantics></math>, they empirically presented three basic formulas for the scaling law<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Here, <math id="footnote6.m1.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="footnote6.m1.1b"><msub id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml"><mi id="footnote6.m1.1.1.2" xref="footnote6.m1.1.1.2.cmml">N</mi><mi id="footnote6.m1.1.1.3" xref="footnote6.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><apply id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1"><csymbol cd="ambiguous" id="footnote6.m1.1.1.1.cmml" xref="footnote6.m1.1.1">subscript</csymbol><ci id="footnote6.m1.1.1.2.cmml" xref="footnote6.m1.1.1.2">𝑁</ci><ci id="footnote6.m1.1.1.3.cmml" xref="footnote6.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">N_{c}</annotation></semantics></math>, <math id="footnote6.m2.1" class="ltx_Math" alttext="D_{c}" display="inline"><semantics id="footnote6.m2.1b"><msub id="footnote6.m2.1.1" xref="footnote6.m2.1.1.cmml"><mi id="footnote6.m2.1.1.2" xref="footnote6.m2.1.1.2.cmml">D</mi><mi id="footnote6.m2.1.1.3" xref="footnote6.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="footnote6.m2.1c"><apply id="footnote6.m2.1.1.cmml" xref="footnote6.m2.1.1"><csymbol cd="ambiguous" id="footnote6.m2.1.1.1.cmml" xref="footnote6.m2.1.1">subscript</csymbol><ci id="footnote6.m2.1.1.2.cmml" xref="footnote6.m2.1.1.2">𝐷</ci><ci id="footnote6.m2.1.1.3.cmml" xref="footnote6.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m2.1d">D_{c}</annotation></semantics></math> and <math id="footnote6.m3.1" class="ltx_Math" alttext="C_{c}" display="inline"><semantics id="footnote6.m3.1b"><msub id="footnote6.m3.1.1" xref="footnote6.m3.1.1.cmml"><mi id="footnote6.m3.1.1.2" xref="footnote6.m3.1.1.2.cmml">C</mi><mi id="footnote6.m3.1.1.3" xref="footnote6.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="footnote6.m3.1c"><apply id="footnote6.m3.1.1.cmml" xref="footnote6.m3.1.1"><csymbol cd="ambiguous" id="footnote6.m3.1.1.1.cmml" xref="footnote6.m3.1.1">subscript</csymbol><ci id="footnote6.m3.1.1.2.cmml" xref="footnote6.m3.1.1.2">𝐶</ci><ci id="footnote6.m3.1.1.3.cmml" xref="footnote6.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m3.1d">C_{c}</annotation></semantics></math> are measured in the number of non-embedding parameters, the number of training tokens and the number of FP-days, respectively. According to the original paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, <math id="footnote6.m4.1" class="ltx_Math" alttext="C_{c}" display="inline"><semantics id="footnote6.m4.1b"><msub id="footnote6.m4.1.1" xref="footnote6.m4.1.1.cmml"><mi id="footnote6.m4.1.1.2" xref="footnote6.m4.1.1.2.cmml">C</mi><mi id="footnote6.m4.1.1.3" xref="footnote6.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="footnote6.m4.1c"><apply id="footnote6.m4.1.1.cmml" xref="footnote6.m4.1.1"><csymbol cd="ambiguous" id="footnote6.m4.1.1.1.cmml" xref="footnote6.m4.1.1">subscript</csymbol><ci id="footnote6.m4.1.1.2.cmml" xref="footnote6.m4.1.1.2">𝐶</ci><ci id="footnote6.m4.1.1.3.cmml" xref="footnote6.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m4.1d">C_{c}</annotation></semantics></math> and <math id="footnote6.m5.1" class="ltx_Math" alttext="C" display="inline"><semantics id="footnote6.m5.1b"><mi id="footnote6.m5.1.1" xref="footnote6.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="footnote6.m5.1c"><ci id="footnote6.m5.1.1.cmml" xref="footnote6.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m5.1d">C</annotation></semantics></math> should be denoted by <math id="footnote6.m6.1" class="ltx_Math" alttext="C_{c}^{min}" display="inline"><semantics id="footnote6.m6.1b"><msubsup id="footnote6.m6.1.1" xref="footnote6.m6.1.1.cmml"><mi id="footnote6.m6.1.1.2.2" xref="footnote6.m6.1.1.2.2.cmml">C</mi><mi id="footnote6.m6.1.1.2.3" xref="footnote6.m6.1.1.2.3.cmml">c</mi><mrow id="footnote6.m6.1.1.3" xref="footnote6.m6.1.1.3.cmml"><mi id="footnote6.m6.1.1.3.2" xref="footnote6.m6.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="footnote6.m6.1.1.3.1" xref="footnote6.m6.1.1.3.1.cmml">​</mo><mi id="footnote6.m6.1.1.3.3" xref="footnote6.m6.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="footnote6.m6.1.1.3.1b" xref="footnote6.m6.1.1.3.1.cmml">​</mo><mi id="footnote6.m6.1.1.3.4" xref="footnote6.m6.1.1.3.4.cmml">n</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="footnote6.m6.1c"><apply id="footnote6.m6.1.1.cmml" xref="footnote6.m6.1.1"><csymbol cd="ambiguous" id="footnote6.m6.1.1.1.cmml" xref="footnote6.m6.1.1">superscript</csymbol><apply id="footnote6.m6.1.1.2.cmml" xref="footnote6.m6.1.1"><csymbol cd="ambiguous" id="footnote6.m6.1.1.2.1.cmml" xref="footnote6.m6.1.1">subscript</csymbol><ci id="footnote6.m6.1.1.2.2.cmml" xref="footnote6.m6.1.1.2.2">𝐶</ci><ci id="footnote6.m6.1.1.2.3.cmml" xref="footnote6.m6.1.1.2.3">𝑐</ci></apply><apply id="footnote6.m6.1.1.3.cmml" xref="footnote6.m6.1.1.3"><times id="footnote6.m6.1.1.3.1.cmml" xref="footnote6.m6.1.1.3.1"></times><ci id="footnote6.m6.1.1.3.2.cmml" xref="footnote6.m6.1.1.3.2">𝑚</ci><ci id="footnote6.m6.1.1.3.3.cmml" xref="footnote6.m6.1.1.3.3">𝑖</ci><ci id="footnote6.m6.1.1.3.4.cmml" xref="footnote6.m6.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m6.1d">C_{c}^{min}</annotation></semantics></math> and <math id="footnote6.m7.1" class="ltx_Math" alttext="C_{min}" display="inline"><semantics id="footnote6.m7.1b"><msub id="footnote6.m7.1.1" xref="footnote6.m7.1.1.cmml"><mi id="footnote6.m7.1.1.2" xref="footnote6.m7.1.1.2.cmml">C</mi><mrow id="footnote6.m7.1.1.3" xref="footnote6.m7.1.1.3.cmml"><mi id="footnote6.m7.1.1.3.2" xref="footnote6.m7.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="footnote6.m7.1.1.3.1" xref="footnote6.m7.1.1.3.1.cmml">​</mo><mi id="footnote6.m7.1.1.3.3" xref="footnote6.m7.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="footnote6.m7.1.1.3.1b" xref="footnote6.m7.1.1.3.1.cmml">​</mo><mi id="footnote6.m7.1.1.3.4" xref="footnote6.m7.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="footnote6.m7.1c"><apply id="footnote6.m7.1.1.cmml" xref="footnote6.m7.1.1"><csymbol cd="ambiguous" id="footnote6.m7.1.1.1.cmml" xref="footnote6.m7.1.1">subscript</csymbol><ci id="footnote6.m7.1.1.2.cmml" xref="footnote6.m7.1.1.2">𝐶</ci><apply id="footnote6.m7.1.1.3.cmml" xref="footnote6.m7.1.1.3"><times id="footnote6.m7.1.1.3.1.cmml" xref="footnote6.m7.1.1.3.1"></times><ci id="footnote6.m7.1.1.3.2.cmml" xref="footnote6.m7.1.1.3.2">𝑚</ci><ci id="footnote6.m7.1.1.3.3.cmml" xref="footnote6.m7.1.1.3.3">𝑖</ci><ci id="footnote6.m7.1.1.3.4.cmml" xref="footnote6.m7.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m7.1d">C_{min}</annotation></semantics></math>, corresponding to the optimal use of compute. We use the simplified notations for ease of discussions. </span></span></span>:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<table id="Sx2.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\displaystyle L(N)" display="inline"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.2" xref="S2.E1.m1.1.2.cmml"><mi mathsize="90%" id="S2.E1.m1.1.2.2" xref="S2.E1.m1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.2.1" xref="S2.E1.m1.1.2.1.cmml">​</mo><mrow id="S2.E1.m1.1.2.3.2" xref="S2.E1.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.E1.m1.1.2.3.2.1" xref="S2.E1.m1.1.2.cmml">(</mo><mi mathsize="90%" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">N</mi><mo maxsize="90%" minsize="90%" id="S2.E1.m1.1.2.3.2.2" xref="S2.E1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.2.cmml" xref="S2.E1.m1.1.2"><times id="S2.E1.m1.1.2.1.cmml" xref="S2.E1.m1.1.2.1"></times><ci id="S2.E1.m1.1.2.2.cmml" xref="S2.E1.m1.1.2.2">𝐿</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle L(N)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E1.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E1.m2.1a"><mo mathsize="90%" id="S2.E1.m2.1.1" xref="S2.E1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E1.m2.1b"><eq id="S2.E1.m2.1.1.cmml" xref="S2.E1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m3.3" class="ltx_Math" alttext="\displaystyle\bigg{(}\frac{N_{c}}{N}\bigg{)}^{\alpha_{N}},\text{~{}~{}~{}}\alpha_{N}\sim 0.076,N_{c}\sim 8.8\times 10^{13}" display="inline"><semantics id="S2.E1.m3.3a"><mrow id="S2.E1.m3.3.3.2" xref="S2.E1.m3.3.3.3.cmml"><mrow id="S2.E1.m3.2.2.1.1" xref="S2.E1.m3.2.2.1.1.cmml"><mrow id="S2.E1.m3.2.2.1.1.2.2" xref="S2.E1.m3.2.2.1.1.2.3.cmml"><msup id="S2.E1.m3.2.2.1.1.1.1.1" xref="S2.E1.m3.2.2.1.1.1.1.1.cmml"><mrow id="S2.E1.m3.2.2.1.1.1.1.1.2.2" xref="S2.E1.m3.1.1.cmml"><mo maxsize="210%" minsize="210%" id="S2.E1.m3.2.2.1.1.1.1.1.2.2.1" xref="S2.E1.m3.1.1.cmml">(</mo><mstyle displaystyle="true" id="S2.E1.m3.1.1" xref="S2.E1.m3.1.1.cmml"><mfrac id="S2.E1.m3.1.1a" xref="S2.E1.m3.1.1.cmml"><msub id="S2.E1.m3.1.1.2" xref="S2.E1.m3.1.1.2.cmml"><mi mathsize="90%" id="S2.E1.m3.1.1.2.2" xref="S2.E1.m3.1.1.2.2.cmml">N</mi><mi mathsize="90%" id="S2.E1.m3.1.1.2.3" xref="S2.E1.m3.1.1.2.3.cmml">c</mi></msub><mi mathsize="90%" id="S2.E1.m3.1.1.3" xref="S2.E1.m3.1.1.3.cmml">N</mi></mfrac></mstyle><mo maxsize="210%" minsize="210%" id="S2.E1.m3.2.2.1.1.1.1.1.2.2.2" xref="S2.E1.m3.1.1.cmml">)</mo></mrow><msub id="S2.E1.m3.2.2.1.1.1.1.1.3" xref="S2.E1.m3.2.2.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S2.E1.m3.2.2.1.1.1.1.1.3.2" xref="S2.E1.m3.2.2.1.1.1.1.1.3.2.cmml">α</mi><mi mathsize="90%" id="S2.E1.m3.2.2.1.1.1.1.1.3.3" xref="S2.E1.m3.2.2.1.1.1.1.1.3.3.cmml">N</mi></msub></msup><mo mathsize="90%" id="S2.E1.m3.2.2.1.1.2.2.3" xref="S2.E1.m3.2.2.1.1.2.3.cmml">,</mo><mrow id="S2.E1.m3.2.2.1.1.2.2.2" xref="S2.E1.m3.2.2.1.1.2.2.2.cmml"><mtext mathsize="90%" id="S2.E1.m3.2.2.1.1.2.2.2.2" xref="S2.E1.m3.2.2.1.1.2.2.2.2a.cmml">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E1.m3.2.2.1.1.2.2.2.1" xref="S2.E1.m3.2.2.1.1.2.2.2.1.cmml">​</mo><msub id="S2.E1.m3.2.2.1.1.2.2.2.3" xref="S2.E1.m3.2.2.1.1.2.2.2.3.cmml"><mi mathsize="90%" id="S2.E1.m3.2.2.1.1.2.2.2.3.2" xref="S2.E1.m3.2.2.1.1.2.2.2.3.2.cmml">α</mi><mi mathsize="90%" id="S2.E1.m3.2.2.1.1.2.2.2.3.3" xref="S2.E1.m3.2.2.1.1.2.2.2.3.3.cmml">N</mi></msub></mrow></mrow><mo mathsize="90%" id="S2.E1.m3.2.2.1.1.3" xref="S2.E1.m3.2.2.1.1.3.cmml">∼</mo><mn mathsize="90%" id="S2.E1.m3.2.2.1.1.4" xref="S2.E1.m3.2.2.1.1.4.cmml">0.076</mn></mrow><mo mathsize="90%" id="S2.E1.m3.3.3.2.3" xref="S2.E1.m3.3.3.3a.cmml">,</mo><mrow id="S2.E1.m3.3.3.2.2" xref="S2.E1.m3.3.3.2.2.cmml"><msub id="S2.E1.m3.3.3.2.2.2" xref="S2.E1.m3.3.3.2.2.2.cmml"><mi mathsize="90%" id="S2.E1.m3.3.3.2.2.2.2" xref="S2.E1.m3.3.3.2.2.2.2.cmml">N</mi><mi mathsize="90%" id="S2.E1.m3.3.3.2.2.2.3" xref="S2.E1.m3.3.3.2.2.2.3.cmml">c</mi></msub><mo mathsize="90%" id="S2.E1.m3.3.3.2.2.1" xref="S2.E1.m3.3.3.2.2.1.cmml">∼</mo><mrow id="S2.E1.m3.3.3.2.2.3" xref="S2.E1.m3.3.3.2.2.3.cmml"><mn mathsize="90%" id="S2.E1.m3.3.3.2.2.3.2" xref="S2.E1.m3.3.3.2.2.3.2.cmml">8.8</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.E1.m3.3.3.2.2.3.1" xref="S2.E1.m3.3.3.2.2.3.1.cmml">×</mo><msup id="S2.E1.m3.3.3.2.2.3.3" xref="S2.E1.m3.3.3.2.2.3.3.cmml"><mn mathsize="90%" id="S2.E1.m3.3.3.2.2.3.3.2" xref="S2.E1.m3.3.3.2.2.3.3.2.cmml">10</mn><mn mathsize="90%" id="S2.E1.m3.3.3.2.2.3.3.3" xref="S2.E1.m3.3.3.2.2.3.3.3.cmml">13</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m3.3b"><apply id="S2.E1.m3.3.3.3.cmml" xref="S2.E1.m3.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m3.3.3.3a.cmml" xref="S2.E1.m3.3.3.2.3">formulae-sequence</csymbol><apply id="S2.E1.m3.2.2.1.1.cmml" xref="S2.E1.m3.2.2.1.1"><csymbol cd="latexml" id="S2.E1.m3.2.2.1.1.3.cmml" xref="S2.E1.m3.2.2.1.1.3">similar-to</csymbol><list id="S2.E1.m3.2.2.1.1.2.3.cmml" xref="S2.E1.m3.2.2.1.1.2.2"><apply id="S2.E1.m3.2.2.1.1.1.1.1.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m3.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1">superscript</csymbol><apply id="S2.E1.m3.1.1.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1.2.2"><divide id="S2.E1.m3.1.1.1.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1.2.2"></divide><apply id="S2.E1.m3.1.1.2.cmml" xref="S2.E1.m3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.2.1.cmml" xref="S2.E1.m3.1.1.2">subscript</csymbol><ci id="S2.E1.m3.1.1.2.2.cmml" xref="S2.E1.m3.1.1.2.2">𝑁</ci><ci id="S2.E1.m3.1.1.2.3.cmml" xref="S2.E1.m3.1.1.2.3">𝑐</ci></apply><ci id="S2.E1.m3.1.1.3.cmml" xref="S2.E1.m3.1.1.3">𝑁</ci></apply><apply id="S2.E1.m3.2.2.1.1.1.1.1.3.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m3.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m3.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1.3.2">𝛼</ci><ci id="S2.E1.m3.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E1.m3.2.2.1.1.1.1.1.3.3">𝑁</ci></apply></apply><apply id="S2.E1.m3.2.2.1.1.2.2.2.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2"><times id="S2.E1.m3.2.2.1.1.2.2.2.1.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.1"></times><ci id="S2.E1.m3.2.2.1.1.2.2.2.2a.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.2"><mtext mathsize="90%" id="S2.E1.m3.2.2.1.1.2.2.2.2.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.2">&nbsp;</mtext></ci><apply id="S2.E1.m3.2.2.1.1.2.2.2.3.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m3.2.2.1.1.2.2.2.3.1.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.3">subscript</csymbol><ci id="S2.E1.m3.2.2.1.1.2.2.2.3.2.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.3.2">𝛼</ci><ci id="S2.E1.m3.2.2.1.1.2.2.2.3.3.cmml" xref="S2.E1.m3.2.2.1.1.2.2.2.3.3">𝑁</ci></apply></apply></list><cn type="float" id="S2.E1.m3.2.2.1.1.4.cmml" xref="S2.E1.m3.2.2.1.1.4">0.076</cn></apply><apply id="S2.E1.m3.3.3.2.2.cmml" xref="S2.E1.m3.3.3.2.2"><csymbol cd="latexml" id="S2.E1.m3.3.3.2.2.1.cmml" xref="S2.E1.m3.3.3.2.2.1">similar-to</csymbol><apply id="S2.E1.m3.3.3.2.2.2.cmml" xref="S2.E1.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m3.3.3.2.2.2.1.cmml" xref="S2.E1.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.E1.m3.3.3.2.2.2.2.cmml" xref="S2.E1.m3.3.3.2.2.2.2">𝑁</ci><ci id="S2.E1.m3.3.3.2.2.2.3.cmml" xref="S2.E1.m3.3.3.2.2.2.3">𝑐</ci></apply><apply id="S2.E1.m3.3.3.2.2.3.cmml" xref="S2.E1.m3.3.3.2.2.3"><times id="S2.E1.m3.3.3.2.2.3.1.cmml" xref="S2.E1.m3.3.3.2.2.3.1"></times><cn type="float" id="S2.E1.m3.3.3.2.2.3.2.cmml" xref="S2.E1.m3.3.3.2.2.3.2">8.8</cn><apply id="S2.E1.m3.3.3.2.2.3.3.cmml" xref="S2.E1.m3.3.3.2.2.3.3"><csymbol cd="ambiguous" id="S2.E1.m3.3.3.2.2.3.3.1.cmml" xref="S2.E1.m3.3.3.2.2.3.3">superscript</csymbol><cn type="integer" id="S2.E1.m3.3.3.2.2.3.3.2.cmml" xref="S2.E1.m3.3.3.2.2.3.3.2">10</cn><cn type="integer" id="S2.E1.m3.3.3.2.2.3.3.3.cmml" xref="S2.E1.m3.3.3.2.2.3.3.3">13</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m3.3c">\displaystyle\bigg{(}\frac{N_{c}}{N}\bigg{)}^{\alpha_{N}},\text{~{}~{}~{}}\alpha_{N}\sim 0.076,N_{c}\sim 8.8\times 10^{13}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex1.m1.1" class="ltx_Math" alttext="\displaystyle L(D)" display="inline"><semantics id="S2.Ex1.m1.1a"><mrow id="S2.Ex1.m1.1.2" xref="S2.Ex1.m1.1.2.cmml"><mi mathsize="90%" id="S2.Ex1.m1.1.2.2" xref="S2.Ex1.m1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.2.1" xref="S2.Ex1.m1.1.2.1.cmml">​</mo><mrow id="S2.Ex1.m1.1.2.3.2" xref="S2.Ex1.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.Ex1.m1.1.2.3.2.1" xref="S2.Ex1.m1.1.2.cmml">(</mo><mi mathsize="90%" id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">D</mi><mo maxsize="90%" minsize="90%" id="S2.Ex1.m1.1.2.3.2.2" xref="S2.Ex1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><apply id="S2.Ex1.m1.1.2.cmml" xref="S2.Ex1.m1.1.2"><times id="S2.Ex1.m1.1.2.1.cmml" xref="S2.Ex1.m1.1.2.1"></times><ci id="S2.Ex1.m1.1.2.2.cmml" xref="S2.Ex1.m1.1.2.2">𝐿</ci><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">\displaystyle L(D)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.Ex1.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.Ex1.m2.1a"><mo mathsize="90%" id="S2.Ex1.m2.1.1" xref="S2.Ex1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.Ex1.m2.1b"><eq id="S2.Ex1.m2.1.1.cmml" xref="S2.Ex1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex1.m3.3" class="ltx_Math" alttext="\displaystyle\bigg{(}\frac{D_{c}}{D}\bigg{)}^{\alpha_{D}},\text{~{}~{}~{}}\alpha_{D}\sim 0.095,D_{c}\sim 5.4\times 10^{13}" display="inline"><semantics id="S2.Ex1.m3.3a"><mrow id="S2.Ex1.m3.3.3.2" xref="S2.Ex1.m3.3.3.3.cmml"><mrow id="S2.Ex1.m3.2.2.1.1" xref="S2.Ex1.m3.2.2.1.1.cmml"><mrow id="S2.Ex1.m3.2.2.1.1.2.2" xref="S2.Ex1.m3.2.2.1.1.2.3.cmml"><msup id="S2.Ex1.m3.2.2.1.1.1.1.1" xref="S2.Ex1.m3.2.2.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m3.2.2.1.1.1.1.1.2.2" xref="S2.Ex1.m3.1.1.cmml"><mo maxsize="210%" minsize="210%" id="S2.Ex1.m3.2.2.1.1.1.1.1.2.2.1" xref="S2.Ex1.m3.1.1.cmml">(</mo><mstyle displaystyle="true" id="S2.Ex1.m3.1.1" xref="S2.Ex1.m3.1.1.cmml"><mfrac id="S2.Ex1.m3.1.1a" xref="S2.Ex1.m3.1.1.cmml"><msub id="S2.Ex1.m3.1.1.2" xref="S2.Ex1.m3.1.1.2.cmml"><mi mathsize="90%" id="S2.Ex1.m3.1.1.2.2" xref="S2.Ex1.m3.1.1.2.2.cmml">D</mi><mi mathsize="90%" id="S2.Ex1.m3.1.1.2.3" xref="S2.Ex1.m3.1.1.2.3.cmml">c</mi></msub><mi mathsize="90%" id="S2.Ex1.m3.1.1.3" xref="S2.Ex1.m3.1.1.3.cmml">D</mi></mfrac></mstyle><mo maxsize="210%" minsize="210%" id="S2.Ex1.m3.2.2.1.1.1.1.1.2.2.2" xref="S2.Ex1.m3.1.1.cmml">)</mo></mrow><msub id="S2.Ex1.m3.2.2.1.1.1.1.1.3" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S2.Ex1.m3.2.2.1.1.1.1.1.3.2" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3.2.cmml">α</mi><mi mathsize="90%" id="S2.Ex1.m3.2.2.1.1.1.1.1.3.3" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3.3.cmml">D</mi></msub></msup><mo mathsize="90%" id="S2.Ex1.m3.2.2.1.1.2.2.3" xref="S2.Ex1.m3.2.2.1.1.2.3.cmml">,</mo><mrow id="S2.Ex1.m3.2.2.1.1.2.2.2" xref="S2.Ex1.m3.2.2.1.1.2.2.2.cmml"><mtext mathsize="90%" id="S2.Ex1.m3.2.2.1.1.2.2.2.2" xref="S2.Ex1.m3.2.2.1.1.2.2.2.2a.cmml">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.Ex1.m3.2.2.1.1.2.2.2.1" xref="S2.Ex1.m3.2.2.1.1.2.2.2.1.cmml">​</mo><msub id="S2.Ex1.m3.2.2.1.1.2.2.2.3" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3.cmml"><mi mathsize="90%" id="S2.Ex1.m3.2.2.1.1.2.2.2.3.2" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3.2.cmml">α</mi><mi mathsize="90%" id="S2.Ex1.m3.2.2.1.1.2.2.2.3.3" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3.3.cmml">D</mi></msub></mrow></mrow><mo mathsize="90%" id="S2.Ex1.m3.2.2.1.1.3" xref="S2.Ex1.m3.2.2.1.1.3.cmml">∼</mo><mn mathsize="90%" id="S2.Ex1.m3.2.2.1.1.4" xref="S2.Ex1.m3.2.2.1.1.4.cmml">0.095</mn></mrow><mo mathsize="90%" id="S2.Ex1.m3.3.3.2.3" xref="S2.Ex1.m3.3.3.3a.cmml">,</mo><mrow id="S2.Ex1.m3.3.3.2.2" xref="S2.Ex1.m3.3.3.2.2.cmml"><msub id="S2.Ex1.m3.3.3.2.2.2" xref="S2.Ex1.m3.3.3.2.2.2.cmml"><mi mathsize="90%" id="S2.Ex1.m3.3.3.2.2.2.2" xref="S2.Ex1.m3.3.3.2.2.2.2.cmml">D</mi><mi mathsize="90%" id="S2.Ex1.m3.3.3.2.2.2.3" xref="S2.Ex1.m3.3.3.2.2.2.3.cmml">c</mi></msub><mo mathsize="90%" id="S2.Ex1.m3.3.3.2.2.1" xref="S2.Ex1.m3.3.3.2.2.1.cmml">∼</mo><mrow id="S2.Ex1.m3.3.3.2.2.3" xref="S2.Ex1.m3.3.3.2.2.3.cmml"><mn mathsize="90%" id="S2.Ex1.m3.3.3.2.2.3.2" xref="S2.Ex1.m3.3.3.2.2.3.2.cmml">5.4</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.Ex1.m3.3.3.2.2.3.1" xref="S2.Ex1.m3.3.3.2.2.3.1.cmml">×</mo><msup id="S2.Ex1.m3.3.3.2.2.3.3" xref="S2.Ex1.m3.3.3.2.2.3.3.cmml"><mn mathsize="90%" id="S2.Ex1.m3.3.3.2.2.3.3.2" xref="S2.Ex1.m3.3.3.2.2.3.3.2.cmml">10</mn><mn mathsize="90%" id="S2.Ex1.m3.3.3.2.2.3.3.3" xref="S2.Ex1.m3.3.3.2.2.3.3.3.cmml">13</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m3.3b"><apply id="S2.Ex1.m3.3.3.3.cmml" xref="S2.Ex1.m3.3.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m3.3.3.3a.cmml" xref="S2.Ex1.m3.3.3.2.3">formulae-sequence</csymbol><apply id="S2.Ex1.m3.2.2.1.1.cmml" xref="S2.Ex1.m3.2.2.1.1"><csymbol cd="latexml" id="S2.Ex1.m3.2.2.1.1.3.cmml" xref="S2.Ex1.m3.2.2.1.1.3">similar-to</csymbol><list id="S2.Ex1.m3.2.2.1.1.2.3.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2"><apply id="S2.Ex1.m3.2.2.1.1.1.1.1.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m3.2.2.1.1.1.1.1.1.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1">superscript</csymbol><apply id="S2.Ex1.m3.1.1.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1.2.2"><divide id="S2.Ex1.m3.1.1.1.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1.2.2"></divide><apply id="S2.Ex1.m3.1.1.2.cmml" xref="S2.Ex1.m3.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m3.1.1.2.1.cmml" xref="S2.Ex1.m3.1.1.2">subscript</csymbol><ci id="S2.Ex1.m3.1.1.2.2.cmml" xref="S2.Ex1.m3.1.1.2.2">𝐷</ci><ci id="S2.Ex1.m3.1.1.2.3.cmml" xref="S2.Ex1.m3.1.1.2.3">𝑐</ci></apply><ci id="S2.Ex1.m3.1.1.3.cmml" xref="S2.Ex1.m3.1.1.3">𝐷</ci></apply><apply id="S2.Ex1.m3.2.2.1.1.1.1.1.3.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m3.2.2.1.1.1.1.1.3.1.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex1.m3.2.2.1.1.1.1.1.3.2.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3.2">𝛼</ci><ci id="S2.Ex1.m3.2.2.1.1.1.1.1.3.3.cmml" xref="S2.Ex1.m3.2.2.1.1.1.1.1.3.3">𝐷</ci></apply></apply><apply id="S2.Ex1.m3.2.2.1.1.2.2.2.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2"><times id="S2.Ex1.m3.2.2.1.1.2.2.2.1.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.1"></times><ci id="S2.Ex1.m3.2.2.1.1.2.2.2.2a.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.2"><mtext mathsize="90%" id="S2.Ex1.m3.2.2.1.1.2.2.2.2.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.2">&nbsp;</mtext></ci><apply id="S2.Ex1.m3.2.2.1.1.2.2.2.3.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m3.2.2.1.1.2.2.2.3.1.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3">subscript</csymbol><ci id="S2.Ex1.m3.2.2.1.1.2.2.2.3.2.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3.2">𝛼</ci><ci id="S2.Ex1.m3.2.2.1.1.2.2.2.3.3.cmml" xref="S2.Ex1.m3.2.2.1.1.2.2.2.3.3">𝐷</ci></apply></apply></list><cn type="float" id="S2.Ex1.m3.2.2.1.1.4.cmml" xref="S2.Ex1.m3.2.2.1.1.4">0.095</cn></apply><apply id="S2.Ex1.m3.3.3.2.2.cmml" xref="S2.Ex1.m3.3.3.2.2"><csymbol cd="latexml" id="S2.Ex1.m3.3.3.2.2.1.cmml" xref="S2.Ex1.m3.3.3.2.2.1">similar-to</csymbol><apply id="S2.Ex1.m3.3.3.2.2.2.cmml" xref="S2.Ex1.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m3.3.3.2.2.2.1.cmml" xref="S2.Ex1.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.Ex1.m3.3.3.2.2.2.2.cmml" xref="S2.Ex1.m3.3.3.2.2.2.2">𝐷</ci><ci id="S2.Ex1.m3.3.3.2.2.2.3.cmml" xref="S2.Ex1.m3.3.3.2.2.2.3">𝑐</ci></apply><apply id="S2.Ex1.m3.3.3.2.2.3.cmml" xref="S2.Ex1.m3.3.3.2.2.3"><times id="S2.Ex1.m3.3.3.2.2.3.1.cmml" xref="S2.Ex1.m3.3.3.2.2.3.1"></times><cn type="float" id="S2.Ex1.m3.3.3.2.2.3.2.cmml" xref="S2.Ex1.m3.3.3.2.2.3.2">5.4</cn><apply id="S2.Ex1.m3.3.3.2.2.3.3.cmml" xref="S2.Ex1.m3.3.3.2.2.3.3"><csymbol cd="ambiguous" id="S2.Ex1.m3.3.3.2.2.3.3.1.cmml" xref="S2.Ex1.m3.3.3.2.2.3.3">superscript</csymbol><cn type="integer" id="S2.Ex1.m3.3.3.2.2.3.3.2.cmml" xref="S2.Ex1.m3.3.3.2.2.3.3.2">10</cn><cn type="integer" id="S2.Ex1.m3.3.3.2.2.3.3.3.cmml" xref="S2.Ex1.m3.3.3.2.2.3.3.3">13</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m3.3c">\displaystyle\bigg{(}\frac{D_{c}}{D}\bigg{)}^{\alpha_{D}},\text{~{}~{}~{}}\alpha_{D}\sim 0.095,D_{c}\sim 5.4\times 10^{13}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2.m1.1" class="ltx_Math" alttext="\displaystyle L(C)" display="inline"><semantics id="S2.Ex2.m1.1a"><mrow id="S2.Ex2.m1.1.2" xref="S2.Ex2.m1.1.2.cmml"><mi mathsize="90%" id="S2.Ex2.m1.1.2.2" xref="S2.Ex2.m1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.2.1" xref="S2.Ex2.m1.1.2.1.cmml">​</mo><mrow id="S2.Ex2.m1.1.2.3.2" xref="S2.Ex2.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.Ex2.m1.1.2.3.2.1" xref="S2.Ex2.m1.1.2.cmml">(</mo><mi mathsize="90%" id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">C</mi><mo maxsize="90%" minsize="90%" id="S2.Ex2.m1.1.2.3.2.2" xref="S2.Ex2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><apply id="S2.Ex2.m1.1.2.cmml" xref="S2.Ex2.m1.1.2"><times id="S2.Ex2.m1.1.2.1.cmml" xref="S2.Ex2.m1.1.2.1"></times><ci id="S2.Ex2.m1.1.2.2.cmml" xref="S2.Ex2.m1.1.2.2">𝐿</ci><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">\displaystyle L(C)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.Ex2.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.Ex2.m2.1a"><mo mathsize="90%" id="S2.Ex2.m2.1.1" xref="S2.Ex2.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.Ex2.m2.1b"><eq id="S2.Ex2.m2.1.1.cmml" xref="S2.Ex2.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2.m3.3" class="ltx_Math" alttext="\displaystyle\bigg{(}\frac{C_{c}}{C}\bigg{)}^{\alpha_{C}},\text{~{}~{}~{}}\alpha_{C}\sim 0.050,C_{c}\sim 3.1\times 10^{8}" display="inline"><semantics id="S2.Ex2.m3.3a"><mrow id="S2.Ex2.m3.3.3.2" xref="S2.Ex2.m3.3.3.3.cmml"><mrow id="S2.Ex2.m3.2.2.1.1" xref="S2.Ex2.m3.2.2.1.1.cmml"><mrow id="S2.Ex2.m3.2.2.1.1.2.2" xref="S2.Ex2.m3.2.2.1.1.2.3.cmml"><msup id="S2.Ex2.m3.2.2.1.1.1.1.1" xref="S2.Ex2.m3.2.2.1.1.1.1.1.cmml"><mrow id="S2.Ex2.m3.2.2.1.1.1.1.1.2.2" xref="S2.Ex2.m3.1.1.cmml"><mo maxsize="210%" minsize="210%" id="S2.Ex2.m3.2.2.1.1.1.1.1.2.2.1" xref="S2.Ex2.m3.1.1.cmml">(</mo><mstyle displaystyle="true" id="S2.Ex2.m3.1.1" xref="S2.Ex2.m3.1.1.cmml"><mfrac id="S2.Ex2.m3.1.1a" xref="S2.Ex2.m3.1.1.cmml"><msub id="S2.Ex2.m3.1.1.2" xref="S2.Ex2.m3.1.1.2.cmml"><mi mathsize="90%" id="S2.Ex2.m3.1.1.2.2" xref="S2.Ex2.m3.1.1.2.2.cmml">C</mi><mi mathsize="90%" id="S2.Ex2.m3.1.1.2.3" xref="S2.Ex2.m3.1.1.2.3.cmml">c</mi></msub><mi mathsize="90%" id="S2.Ex2.m3.1.1.3" xref="S2.Ex2.m3.1.1.3.cmml">C</mi></mfrac></mstyle><mo maxsize="210%" minsize="210%" id="S2.Ex2.m3.2.2.1.1.1.1.1.2.2.2" xref="S2.Ex2.m3.1.1.cmml">)</mo></mrow><msub id="S2.Ex2.m3.2.2.1.1.1.1.1.3" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S2.Ex2.m3.2.2.1.1.1.1.1.3.2" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3.2.cmml">α</mi><mi mathsize="90%" id="S2.Ex2.m3.2.2.1.1.1.1.1.3.3" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3.3.cmml">C</mi></msub></msup><mo mathsize="90%" id="S2.Ex2.m3.2.2.1.1.2.2.3" xref="S2.Ex2.m3.2.2.1.1.2.3.cmml">,</mo><mrow id="S2.Ex2.m3.2.2.1.1.2.2.2" xref="S2.Ex2.m3.2.2.1.1.2.2.2.cmml"><mtext mathsize="90%" id="S2.Ex2.m3.2.2.1.1.2.2.2.2" xref="S2.Ex2.m3.2.2.1.1.2.2.2.2a.cmml">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2.m3.2.2.1.1.2.2.2.1" xref="S2.Ex2.m3.2.2.1.1.2.2.2.1.cmml">​</mo><msub id="S2.Ex2.m3.2.2.1.1.2.2.2.3" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3.cmml"><mi mathsize="90%" id="S2.Ex2.m3.2.2.1.1.2.2.2.3.2" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3.2.cmml">α</mi><mi mathsize="90%" id="S2.Ex2.m3.2.2.1.1.2.2.2.3.3" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3.3.cmml">C</mi></msub></mrow></mrow><mo mathsize="90%" id="S2.Ex2.m3.2.2.1.1.3" xref="S2.Ex2.m3.2.2.1.1.3.cmml">∼</mo><mn mathsize="90%" id="S2.Ex2.m3.2.2.1.1.4" xref="S2.Ex2.m3.2.2.1.1.4.cmml">0.050</mn></mrow><mo mathsize="90%" id="S2.Ex2.m3.3.3.2.3" xref="S2.Ex2.m3.3.3.3a.cmml">,</mo><mrow id="S2.Ex2.m3.3.3.2.2" xref="S2.Ex2.m3.3.3.2.2.cmml"><msub id="S2.Ex2.m3.3.3.2.2.2" xref="S2.Ex2.m3.3.3.2.2.2.cmml"><mi mathsize="90%" id="S2.Ex2.m3.3.3.2.2.2.2" xref="S2.Ex2.m3.3.3.2.2.2.2.cmml">C</mi><mi mathsize="90%" id="S2.Ex2.m3.3.3.2.2.2.3" xref="S2.Ex2.m3.3.3.2.2.2.3.cmml">c</mi></msub><mo mathsize="90%" id="S2.Ex2.m3.3.3.2.2.1" xref="S2.Ex2.m3.3.3.2.2.1.cmml">∼</mo><mrow id="S2.Ex2.m3.3.3.2.2.3" xref="S2.Ex2.m3.3.3.2.2.3.cmml"><mn mathsize="90%" id="S2.Ex2.m3.3.3.2.2.3.2" xref="S2.Ex2.m3.3.3.2.2.3.2.cmml">3.1</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.Ex2.m3.3.3.2.2.3.1" xref="S2.Ex2.m3.3.3.2.2.3.1.cmml">×</mo><msup id="S2.Ex2.m3.3.3.2.2.3.3" xref="S2.Ex2.m3.3.3.2.2.3.3.cmml"><mn mathsize="90%" id="S2.Ex2.m3.3.3.2.2.3.3.2" xref="S2.Ex2.m3.3.3.2.2.3.3.2.cmml">10</mn><mn mathsize="90%" id="S2.Ex2.m3.3.3.2.2.3.3.3" xref="S2.Ex2.m3.3.3.2.2.3.3.3.cmml">8</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m3.3b"><apply id="S2.Ex2.m3.3.3.3.cmml" xref="S2.Ex2.m3.3.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m3.3.3.3a.cmml" xref="S2.Ex2.m3.3.3.2.3">formulae-sequence</csymbol><apply id="S2.Ex2.m3.2.2.1.1.cmml" xref="S2.Ex2.m3.2.2.1.1"><csymbol cd="latexml" id="S2.Ex2.m3.2.2.1.1.3.cmml" xref="S2.Ex2.m3.2.2.1.1.3">similar-to</csymbol><list id="S2.Ex2.m3.2.2.1.1.2.3.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2"><apply id="S2.Ex2.m3.2.2.1.1.1.1.1.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m3.2.2.1.1.1.1.1.1.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1">superscript</csymbol><apply id="S2.Ex2.m3.1.1.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1.2.2"><divide id="S2.Ex2.m3.1.1.1.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1.2.2"></divide><apply id="S2.Ex2.m3.1.1.2.cmml" xref="S2.Ex2.m3.1.1.2"><csymbol cd="ambiguous" id="S2.Ex2.m3.1.1.2.1.cmml" xref="S2.Ex2.m3.1.1.2">subscript</csymbol><ci id="S2.Ex2.m3.1.1.2.2.cmml" xref="S2.Ex2.m3.1.1.2.2">𝐶</ci><ci id="S2.Ex2.m3.1.1.2.3.cmml" xref="S2.Ex2.m3.1.1.2.3">𝑐</ci></apply><ci id="S2.Ex2.m3.1.1.3.cmml" xref="S2.Ex2.m3.1.1.3">𝐶</ci></apply><apply id="S2.Ex2.m3.2.2.1.1.1.1.1.3.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m3.2.2.1.1.1.1.1.3.1.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m3.2.2.1.1.1.1.1.3.2.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3.2">𝛼</ci><ci id="S2.Ex2.m3.2.2.1.1.1.1.1.3.3.cmml" xref="S2.Ex2.m3.2.2.1.1.1.1.1.3.3">𝐶</ci></apply></apply><apply id="S2.Ex2.m3.2.2.1.1.2.2.2.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2"><times id="S2.Ex2.m3.2.2.1.1.2.2.2.1.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.1"></times><ci id="S2.Ex2.m3.2.2.1.1.2.2.2.2a.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.2"><mtext mathsize="90%" id="S2.Ex2.m3.2.2.1.1.2.2.2.2.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.2">&nbsp;</mtext></ci><apply id="S2.Ex2.m3.2.2.1.1.2.2.2.3.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex2.m3.2.2.1.1.2.2.2.3.1.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3">subscript</csymbol><ci id="S2.Ex2.m3.2.2.1.1.2.2.2.3.2.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3.2">𝛼</ci><ci id="S2.Ex2.m3.2.2.1.1.2.2.2.3.3.cmml" xref="S2.Ex2.m3.2.2.1.1.2.2.2.3.3">𝐶</ci></apply></apply></list><cn type="float" id="S2.Ex2.m3.2.2.1.1.4.cmml" xref="S2.Ex2.m3.2.2.1.1.4">0.050</cn></apply><apply id="S2.Ex2.m3.3.3.2.2.cmml" xref="S2.Ex2.m3.3.3.2.2"><csymbol cd="latexml" id="S2.Ex2.m3.3.3.2.2.1.cmml" xref="S2.Ex2.m3.3.3.2.2.1">similar-to</csymbol><apply id="S2.Ex2.m3.3.3.2.2.2.cmml" xref="S2.Ex2.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m3.3.3.2.2.2.1.cmml" xref="S2.Ex2.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.Ex2.m3.3.3.2.2.2.2.cmml" xref="S2.Ex2.m3.3.3.2.2.2.2">𝐶</ci><ci id="S2.Ex2.m3.3.3.2.2.2.3.cmml" xref="S2.Ex2.m3.3.3.2.2.2.3">𝑐</ci></apply><apply id="S2.Ex2.m3.3.3.2.2.3.cmml" xref="S2.Ex2.m3.3.3.2.2.3"><times id="S2.Ex2.m3.3.3.2.2.3.1.cmml" xref="S2.Ex2.m3.3.3.2.2.3.1"></times><cn type="float" id="S2.Ex2.m3.3.3.2.2.3.2.cmml" xref="S2.Ex2.m3.3.3.2.2.3.2">3.1</cn><apply id="S2.Ex2.m3.3.3.2.2.3.3.cmml" xref="S2.Ex2.m3.3.3.2.2.3.3"><csymbol cd="ambiguous" id="S2.Ex2.m3.3.3.2.2.3.3.1.cmml" xref="S2.Ex2.m3.3.3.2.2.3.3">superscript</csymbol><cn type="integer" id="S2.Ex2.m3.3.3.2.2.3.3.2.cmml" xref="S2.Ex2.m3.3.3.2.2.3.3.2">10</cn><cn type="integer" id="S2.Ex2.m3.3.3.2.2.3.3.3.cmml" xref="S2.Ex2.m3.3.3.2.2.3.3.3">8</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m3.3c">\displaystyle\bigg{(}\frac{C_{c}}{C}\bigg{)}^{\alpha_{C}},\text{~{}~{}~{}}\alpha_{C}\sim 0.050,C_{c}\sim 3.1\times 10^{8}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.1" class="ltx_p">where <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="L(\cdot)" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><mrow id="S2.SS1.p5.1.m1.1.2" xref="S2.SS1.p5.1.m1.1.2.cmml"><mi id="S2.SS1.p5.1.m1.1.2.2" xref="S2.SS1.p5.1.m1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.2.1" xref="S2.SS1.p5.1.m1.1.2.1.cmml">​</mo><mrow id="S2.SS1.p5.1.m1.1.2.3.2" xref="S2.SS1.p5.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p5.1.m1.1.2.3.2.1" xref="S2.SS1.p5.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS1.p5.1.m1.1.2.3.2.2" xref="S2.SS1.p5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><apply id="S2.SS1.p5.1.m1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.2"><times id="S2.SS1.p5.1.m1.1.2.1.cmml" xref="S2.SS1.p5.1.m1.1.2.1"></times><ci id="S2.SS1.p5.1.m1.1.2.2.cmml" xref="S2.SS1.p5.1.m1.1.2.2">𝐿</ci><ci id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">L(\cdot)</annotation></semantics></math> denotes the cross entropy loss in nats, and a follow-up study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely <em id="S2.SS1.p5.1.1" class="ltx_emph ltx_font_italic">irreducible loss</em> (the entropy of the true data distribution) and <em id="S2.SS1.p5.1.2" class="ltx_emph ltx_font_italic">reducible loss</em> (an estimate of the KL divergence between the true and model distributions). The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768M to 1.5B non-embedding parameters) and training compute, under some assumptions (<em id="S2.SS1.p5.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><math id="S2.SS1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p6.1.m1.1a"><mo id="S2.SS1.p6.1.m1.1.1" xref="S2.SS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.1.m1.1b"><ci id="S2.SS1.p6.1.m1.1.1.cmml" xref="S2.SS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p6.1.1" class="ltx_emph ltx_font_italic">Chinchilla scaling law</em>. As another representative study, Hoffmann et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> (the Google DeepMind team)
proposed an alternative form for scaling laws to instruct the compute-optimal training for LLMs. They conducted rigorous experiments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients as below&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="L(N,D)=E+\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}}," display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2" xref="S2.E2.m1.3.3.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2.1" xref="S2.E2.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.2.3.2" xref="S2.E2.m1.3.3.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.2.3.2.1" xref="S2.E2.m1.3.3.1.1.2.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">N</mi><mo id="S2.E2.m1.3.3.1.1.2.3.2.2" xref="S2.E2.m1.3.3.1.1.2.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">D</mi><mo stretchy="false" id="S2.E2.m1.3.3.1.1.2.3.2.3" xref="S2.E2.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.3.2" xref="S2.E2.m1.3.3.1.1.3.2.cmml">E</mi><mo id="S2.E2.m1.3.3.1.1.3.1" xref="S2.E2.m1.3.3.1.1.3.1.cmml">+</mo><mfrac id="S2.E2.m1.3.3.1.1.3.3" xref="S2.E2.m1.3.3.1.1.3.3.cmml"><mi id="S2.E2.m1.3.3.1.1.3.3.2" xref="S2.E2.m1.3.3.1.1.3.3.2.cmml">A</mi><msup id="S2.E2.m1.3.3.1.1.3.3.3" xref="S2.E2.m1.3.3.1.1.3.3.3.cmml"><mi id="S2.E2.m1.3.3.1.1.3.3.3.2" xref="S2.E2.m1.3.3.1.1.3.3.3.2.cmml">N</mi><mi id="S2.E2.m1.3.3.1.1.3.3.3.3" xref="S2.E2.m1.3.3.1.1.3.3.3.3.cmml">α</mi></msup></mfrac><mo id="S2.E2.m1.3.3.1.1.3.1a" xref="S2.E2.m1.3.3.1.1.3.1.cmml">+</mo><mfrac id="S2.E2.m1.3.3.1.1.3.4" xref="S2.E2.m1.3.3.1.1.3.4.cmml"><mi id="S2.E2.m1.3.3.1.1.3.4.2" xref="S2.E2.m1.3.3.1.1.3.4.2.cmml">B</mi><msup id="S2.E2.m1.3.3.1.1.3.4.3" xref="S2.E2.m1.3.3.1.1.3.4.3.cmml"><mi id="S2.E2.m1.3.3.1.1.3.4.3.2" xref="S2.E2.m1.3.3.1.1.3.4.3.2.cmml">D</mi><mi id="S2.E2.m1.3.3.1.1.3.4.3.3" xref="S2.E2.m1.3.3.1.1.3.4.3.3.cmml">β</mi></msup></mfrac></mrow></mrow><mo id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1"><eq id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"></eq><apply id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"><times id="S2.E2.m1.3.3.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2.1"></times><ci id="S2.E2.m1.3.3.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2">𝐿</ci><interval closure="open" id="S2.E2.m1.3.3.1.1.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.2.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑁</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐷</ci></interval></apply><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"><plus id="S2.E2.m1.3.3.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3.1"></plus><ci id="S2.E2.m1.3.3.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.2">𝐸</ci><apply id="S2.E2.m1.3.3.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3"><divide id="S2.E2.m1.3.3.1.1.3.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3.3"></divide><ci id="S2.E2.m1.3.3.1.1.3.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.3.2">𝐴</ci><apply id="S2.E2.m1.3.3.1.1.3.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3.3.3">superscript</csymbol><ci id="S2.E2.m1.3.3.1.1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.3.3.2">𝑁</ci><ci id="S2.E2.m1.3.3.1.1.3.3.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3.3.3">𝛼</ci></apply></apply><apply id="S2.E2.m1.3.3.1.1.3.4.cmml" xref="S2.E2.m1.3.3.1.1.3.4"><divide id="S2.E2.m1.3.3.1.1.3.4.1.cmml" xref="S2.E2.m1.3.3.1.1.3.4"></divide><ci id="S2.E2.m1.3.3.1.1.3.4.2.cmml" xref="S2.E2.m1.3.3.1.1.3.4.2">𝐵</ci><apply id="S2.E2.m1.3.3.1.1.3.4.3.cmml" xref="S2.E2.m1.3.3.1.1.3.4.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.3.4.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3.4.3">superscript</csymbol><ci id="S2.E2.m1.3.3.1.1.3.4.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.4.3.2">𝐷</ci><ci id="S2.E2.m1.3.3.1.1.3.4.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.4.3.3">𝛽</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">L(N,D)=E+\frac{A}{N^{\alpha}}+\frac{B}{D^{\beta}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p6.6" class="ltx_p">where <math id="S2.SS1.p6.2.m1.2" class="ltx_Math" alttext="E=1.69,A=406.4,B=410.7" display="inline"><semantics id="S2.SS1.p6.2.m1.2a"><mrow id="S2.SS1.p6.2.m1.2.2.2" xref="S2.SS1.p6.2.m1.2.2.3.cmml"><mrow id="S2.SS1.p6.2.m1.1.1.1.1" xref="S2.SS1.p6.2.m1.1.1.1.1.cmml"><mi id="S2.SS1.p6.2.m1.1.1.1.1.2" xref="S2.SS1.p6.2.m1.1.1.1.1.2.cmml">E</mi><mo id="S2.SS1.p6.2.m1.1.1.1.1.1" xref="S2.SS1.p6.2.m1.1.1.1.1.1.cmml">=</mo><mn id="S2.SS1.p6.2.m1.1.1.1.1.3" xref="S2.SS1.p6.2.m1.1.1.1.1.3.cmml">1.69</mn></mrow><mo id="S2.SS1.p6.2.m1.2.2.2.3" xref="S2.SS1.p6.2.m1.2.2.3a.cmml">,</mo><mrow id="S2.SS1.p6.2.m1.2.2.2.2.2" xref="S2.SS1.p6.2.m1.2.2.2.2.3.cmml"><mrow id="S2.SS1.p6.2.m1.2.2.2.2.1.1" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.cmml"><mi id="S2.SS1.p6.2.m1.2.2.2.2.1.1.2" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.2.cmml">A</mi><mo id="S2.SS1.p6.2.m1.2.2.2.2.1.1.1" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.1.cmml">=</mo><mn id="S2.SS1.p6.2.m1.2.2.2.2.1.1.3" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.3.cmml">406.4</mn></mrow><mo id="S2.SS1.p6.2.m1.2.2.2.2.2.3" xref="S2.SS1.p6.2.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S2.SS1.p6.2.m1.2.2.2.2.2.2" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p6.2.m1.2.2.2.2.2.2.2" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.2.cmml">B</mi><mo id="S2.SS1.p6.2.m1.2.2.2.2.2.2.1" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.1.cmml">=</mo><mn id="S2.SS1.p6.2.m1.2.2.2.2.2.2.3" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.3.cmml">410.7</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.2.m1.2b"><apply id="S2.SS1.p6.2.m1.2.2.3.cmml" xref="S2.SS1.p6.2.m1.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.2.m1.2.2.3a.cmml" xref="S2.SS1.p6.2.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.SS1.p6.2.m1.1.1.1.1.cmml" xref="S2.SS1.p6.2.m1.1.1.1.1"><eq id="S2.SS1.p6.2.m1.1.1.1.1.1.cmml" xref="S2.SS1.p6.2.m1.1.1.1.1.1"></eq><ci id="S2.SS1.p6.2.m1.1.1.1.1.2.cmml" xref="S2.SS1.p6.2.m1.1.1.1.1.2">𝐸</ci><cn type="float" id="S2.SS1.p6.2.m1.1.1.1.1.3.cmml" xref="S2.SS1.p6.2.m1.1.1.1.1.3">1.69</cn></apply><apply id="S2.SS1.p6.2.m1.2.2.2.2.3.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.2.m1.2.2.2.2.3a.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.SS1.p6.2.m1.2.2.2.2.1.1.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1"><eq id="S2.SS1.p6.2.m1.2.2.2.2.1.1.1.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.1"></eq><ci id="S2.SS1.p6.2.m1.2.2.2.2.1.1.2.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.2">𝐴</ci><cn type="float" id="S2.SS1.p6.2.m1.2.2.2.2.1.1.3.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.1.1.3">406.4</cn></apply><apply id="S2.SS1.p6.2.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2"><eq id="S2.SS1.p6.2.m1.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.1"></eq><ci id="S2.SS1.p6.2.m1.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.2">𝐵</ci><cn type="float" id="S2.SS1.p6.2.m1.2.2.2.2.2.2.3.cmml" xref="S2.SS1.p6.2.m1.2.2.2.2.2.2.3">410.7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.2.m1.2c">E=1.69,A=406.4,B=410.7</annotation></semantics></math>, <math id="S2.SS1.p6.3.m2.1" class="ltx_Math" alttext="\alpha=0.34" display="inline"><semantics id="S2.SS1.p6.3.m2.1a"><mrow id="S2.SS1.p6.3.m2.1.1" xref="S2.SS1.p6.3.m2.1.1.cmml"><mi id="S2.SS1.p6.3.m2.1.1.2" xref="S2.SS1.p6.3.m2.1.1.2.cmml">α</mi><mo id="S2.SS1.p6.3.m2.1.1.1" xref="S2.SS1.p6.3.m2.1.1.1.cmml">=</mo><mn id="S2.SS1.p6.3.m2.1.1.3" xref="S2.SS1.p6.3.m2.1.1.3.cmml">0.34</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.3.m2.1b"><apply id="S2.SS1.p6.3.m2.1.1.cmml" xref="S2.SS1.p6.3.m2.1.1"><eq id="S2.SS1.p6.3.m2.1.1.1.cmml" xref="S2.SS1.p6.3.m2.1.1.1"></eq><ci id="S2.SS1.p6.3.m2.1.1.2.cmml" xref="S2.SS1.p6.3.m2.1.1.2">𝛼</ci><cn type="float" id="S2.SS1.p6.3.m2.1.1.3.cmml" xref="S2.SS1.p6.3.m2.1.1.3">0.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.3.m2.1c">\alpha=0.34</annotation></semantics></math> and <math id="S2.SS1.p6.4.m3.1" class="ltx_Math" alttext="\beta=0.28" display="inline"><semantics id="S2.SS1.p6.4.m3.1a"><mrow id="S2.SS1.p6.4.m3.1.1" xref="S2.SS1.p6.4.m3.1.1.cmml"><mi id="S2.SS1.p6.4.m3.1.1.2" xref="S2.SS1.p6.4.m3.1.1.2.cmml">β</mi><mo id="S2.SS1.p6.4.m3.1.1.1" xref="S2.SS1.p6.4.m3.1.1.1.cmml">=</mo><mn id="S2.SS1.p6.4.m3.1.1.3" xref="S2.SS1.p6.4.m3.1.1.3.cmml">0.28</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.4.m3.1b"><apply id="S2.SS1.p6.4.m3.1.1.cmml" xref="S2.SS1.p6.4.m3.1.1"><eq id="S2.SS1.p6.4.m3.1.1.1.cmml" xref="S2.SS1.p6.4.m3.1.1.1"></eq><ci id="S2.SS1.p6.4.m3.1.1.2.cmml" xref="S2.SS1.p6.4.m3.1.1.2">𝛽</ci><cn type="float" id="S2.SS1.p6.4.m3.1.1.3.cmml" xref="S2.SS1.p6.4.m3.1.1.3">0.28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.4.m3.1c">\beta=0.28</annotation></semantics></math>. By optimizing the loss <math id="S2.SS1.p6.5.m4.2" class="ltx_Math" alttext="L(N,D)" display="inline"><semantics id="S2.SS1.p6.5.m4.2a"><mrow id="S2.SS1.p6.5.m4.2.3" xref="S2.SS1.p6.5.m4.2.3.cmml"><mi id="S2.SS1.p6.5.m4.2.3.2" xref="S2.SS1.p6.5.m4.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.5.m4.2.3.1" xref="S2.SS1.p6.5.m4.2.3.1.cmml">​</mo><mrow id="S2.SS1.p6.5.m4.2.3.3.2" xref="S2.SS1.p6.5.m4.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p6.5.m4.2.3.3.2.1" xref="S2.SS1.p6.5.m4.2.3.3.1.cmml">(</mo><mi id="S2.SS1.p6.5.m4.1.1" xref="S2.SS1.p6.5.m4.1.1.cmml">N</mi><mo id="S2.SS1.p6.5.m4.2.3.3.2.2" xref="S2.SS1.p6.5.m4.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p6.5.m4.2.2" xref="S2.SS1.p6.5.m4.2.2.cmml">D</mi><mo stretchy="false" id="S2.SS1.p6.5.m4.2.3.3.2.3" xref="S2.SS1.p6.5.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.5.m4.2b"><apply id="S2.SS1.p6.5.m4.2.3.cmml" xref="S2.SS1.p6.5.m4.2.3"><times id="S2.SS1.p6.5.m4.2.3.1.cmml" xref="S2.SS1.p6.5.m4.2.3.1"></times><ci id="S2.SS1.p6.5.m4.2.3.2.cmml" xref="S2.SS1.p6.5.m4.2.3.2">𝐿</ci><interval closure="open" id="S2.SS1.p6.5.m4.2.3.3.1.cmml" xref="S2.SS1.p6.5.m4.2.3.3.2"><ci id="S2.SS1.p6.5.m4.1.1.cmml" xref="S2.SS1.p6.5.m4.1.1">𝑁</ci><ci id="S2.SS1.p6.5.m4.2.2.cmml" xref="S2.SS1.p6.5.m4.2.2">𝐷</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.5.m4.2c">L(N,D)</annotation></semantics></math> under the constraint <math id="S2.SS1.p6.6.m5.1" class="ltx_Math" alttext="C\approx 6ND" display="inline"><semantics id="S2.SS1.p6.6.m5.1a"><mrow id="S2.SS1.p6.6.m5.1.1" xref="S2.SS1.p6.6.m5.1.1.cmml"><mi id="S2.SS1.p6.6.m5.1.1.2" xref="S2.SS1.p6.6.m5.1.1.2.cmml">C</mi><mo id="S2.SS1.p6.6.m5.1.1.1" xref="S2.SS1.p6.6.m5.1.1.1.cmml">≈</mo><mrow id="S2.SS1.p6.6.m5.1.1.3" xref="S2.SS1.p6.6.m5.1.1.3.cmml"><mn id="S2.SS1.p6.6.m5.1.1.3.2" xref="S2.SS1.p6.6.m5.1.1.3.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p6.6.m5.1.1.3.1" xref="S2.SS1.p6.6.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p6.6.m5.1.1.3.3" xref="S2.SS1.p6.6.m5.1.1.3.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.6.m5.1.1.3.1a" xref="S2.SS1.p6.6.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p6.6.m5.1.1.3.4" xref="S2.SS1.p6.6.m5.1.1.3.4.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.6.m5.1b"><apply id="S2.SS1.p6.6.m5.1.1.cmml" xref="S2.SS1.p6.6.m5.1.1"><approx id="S2.SS1.p6.6.m5.1.1.1.cmml" xref="S2.SS1.p6.6.m5.1.1.1"></approx><ci id="S2.SS1.p6.6.m5.1.1.2.cmml" xref="S2.SS1.p6.6.m5.1.1.2">𝐶</ci><apply id="S2.SS1.p6.6.m5.1.1.3.cmml" xref="S2.SS1.p6.6.m5.1.1.3"><times id="S2.SS1.p6.6.m5.1.1.3.1.cmml" xref="S2.SS1.p6.6.m5.1.1.3.1"></times><cn type="integer" id="S2.SS1.p6.6.m5.1.1.3.2.cmml" xref="S2.SS1.p6.6.m5.1.1.3.2">6</cn><ci id="S2.SS1.p6.6.m5.1.1.3.3.cmml" xref="S2.SS1.p6.6.m5.1.1.3.3">𝑁</ci><ci id="S2.SS1.p6.6.m5.1.1.3.4.cmml" xref="S2.SS1.p6.6.m5.1.1.3.4">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.6.m5.1c">C\approx 6ND</annotation></semantics></math>, they showed that the optimal allocation of compute budget to model size and data size can be derived as follows:</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<table id="Sx2.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E3.m1.5" class="ltx_Math" alttext="\displaystyle N_{opt}(C)=G\bigg{(}\frac{C}{6}\bigg{)}^{a},\text{~{}~{}~{}}D_{opt}(C)=G^{-1}\bigg{(}\frac{C}{6}\bigg{)}^{b}," display="inline"><semantics id="S2.E3.m1.5a"><mrow id="S2.E3.m1.5.5.1"><mrow id="S2.E3.m1.5.5.1.1.2" xref="S2.E3.m1.5.5.1.1.3.cmml"><mrow id="S2.E3.m1.5.5.1.1.1.1" xref="S2.E3.m1.5.5.1.1.1.1.cmml"><mrow id="S2.E3.m1.5.5.1.1.1.1.2" xref="S2.E3.m1.5.5.1.1.1.1.2.cmml"><msub id="S2.E3.m1.5.5.1.1.1.1.2.2" xref="S2.E3.m1.5.5.1.1.1.1.2.2.cmml"><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.2.2.2" xref="S2.E3.m1.5.5.1.1.1.1.2.2.2.cmml">N</mi><mrow id="S2.E3.m1.5.5.1.1.1.1.2.2.3" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.2.2.3.2" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.1.1.2.2.3.1" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.2.2.3.3" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.1.1.2.2.3.1a" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.2.2.3.4" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.4.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.1.1.2.1" xref="S2.E3.m1.5.5.1.1.1.1.2.1.cmml">​</mo><mrow id="S2.E3.m1.5.5.1.1.1.1.2.3.2" xref="S2.E3.m1.5.5.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.E3.m1.5.5.1.1.1.1.2.3.2.1" xref="S2.E3.m1.5.5.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">C</mi><mo maxsize="90%" minsize="90%" id="S2.E3.m1.5.5.1.1.1.1.2.3.2.2" xref="S2.E3.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.1" xref="S2.E3.m1.5.5.1.1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.5.5.1.1.1.1.3" xref="S2.E3.m1.5.5.1.1.1.1.3.cmml"><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.3.2" xref="S2.E3.m1.5.5.1.1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.1.1.3.1" xref="S2.E3.m1.5.5.1.1.1.1.3.1.cmml">​</mo><msup id="S2.E3.m1.5.5.1.1.1.1.3.3" xref="S2.E3.m1.5.5.1.1.1.1.3.3.cmml"><mrow id="S2.E3.m1.5.5.1.1.1.1.3.3.2.2" xref="S2.E3.m1.2.2.cmml"><mo maxsize="210%" minsize="210%" id="S2.E3.m1.5.5.1.1.1.1.3.3.2.2.1" xref="S2.E3.m1.2.2.cmml">(</mo><mstyle displaystyle="true" id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><mfrac id="S2.E3.m1.2.2a" xref="S2.E3.m1.2.2.cmml"><mi mathsize="90%" id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml">C</mi><mn mathsize="90%" id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml">6</mn></mfrac></mstyle><mo maxsize="210%" minsize="210%" id="S2.E3.m1.5.5.1.1.1.1.3.3.2.2.2" xref="S2.E3.m1.2.2.cmml">)</mo></mrow><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.1.1.3.3.3" xref="S2.E3.m1.5.5.1.1.1.1.3.3.3.cmml">a</mi></msup></mrow></mrow><mo mathsize="90%" id="S2.E3.m1.5.5.1.1.2.3" xref="S2.E3.m1.5.5.1.1.3a.cmml">,</mo><mrow id="S2.E3.m1.5.5.1.1.2.2" xref="S2.E3.m1.5.5.1.1.2.2.cmml"><mrow id="S2.E3.m1.5.5.1.1.2.2.2" xref="S2.E3.m1.5.5.1.1.2.2.2.cmml"><mtext mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.2" xref="S2.E3.m1.5.5.1.1.2.2.2.2a.cmml">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.2.2.2.1" xref="S2.E3.m1.5.5.1.1.2.2.2.1.cmml">​</mo><msub id="S2.E3.m1.5.5.1.1.2.2.2.3" xref="S2.E3.m1.5.5.1.1.2.2.2.3.cmml"><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.3.2" xref="S2.E3.m1.5.5.1.1.2.2.2.3.2.cmml">D</mi><mrow id="S2.E3.m1.5.5.1.1.2.2.2.3.3" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.cmml"><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.3.3.2" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.2.2.2.3.3.1" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.3.3.3" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.2.2.2.3.3.1a" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.3.3.4" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.4.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.2.2.2.1a" xref="S2.E3.m1.5.5.1.1.2.2.2.1.cmml">​</mo><mrow id="S2.E3.m1.5.5.1.1.2.2.2.4.2" xref="S2.E3.m1.5.5.1.1.2.2.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.4.2.1" xref="S2.E3.m1.5.5.1.1.2.2.2.cmml">(</mo><mi mathsize="90%" id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml">C</mi><mo maxsize="90%" minsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.4.2.2" xref="S2.E3.m1.5.5.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.1" xref="S2.E3.m1.5.5.1.1.2.2.1.cmml">=</mo><mrow id="S2.E3.m1.5.5.1.1.2.2.3" xref="S2.E3.m1.5.5.1.1.2.2.3.cmml"><msup id="S2.E3.m1.5.5.1.1.2.2.3.2" xref="S2.E3.m1.5.5.1.1.2.2.3.2.cmml"><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.3.2.2" xref="S2.E3.m1.5.5.1.1.2.2.3.2.2.cmml">G</mi><mrow id="S2.E3.m1.5.5.1.1.2.2.3.2.3" xref="S2.E3.m1.5.5.1.1.2.2.3.2.3.cmml"><mo mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.3.2.3a" xref="S2.E3.m1.5.5.1.1.2.2.3.2.3.cmml">−</mo><mn mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.3.2.3.2" xref="S2.E3.m1.5.5.1.1.2.2.3.2.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.1.1.2.2.3.1" xref="S2.E3.m1.5.5.1.1.2.2.3.1.cmml">​</mo><msup id="S2.E3.m1.5.5.1.1.2.2.3.3" xref="S2.E3.m1.5.5.1.1.2.2.3.3.cmml"><mrow id="S2.E3.m1.5.5.1.1.2.2.3.3.2.2" xref="S2.E3.m1.4.4.cmml"><mo maxsize="210%" minsize="210%" id="S2.E3.m1.5.5.1.1.2.2.3.3.2.2.1" xref="S2.E3.m1.4.4.cmml">(</mo><mstyle displaystyle="true" id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml"><mfrac id="S2.E3.m1.4.4a" xref="S2.E3.m1.4.4.cmml"><mi mathsize="90%" id="S2.E3.m1.4.4.2" xref="S2.E3.m1.4.4.2.cmml">C</mi><mn mathsize="90%" id="S2.E3.m1.4.4.3" xref="S2.E3.m1.4.4.3.cmml">6</mn></mfrac></mstyle><mo maxsize="210%" minsize="210%" id="S2.E3.m1.5.5.1.1.2.2.3.3.2.2.2" xref="S2.E3.m1.4.4.cmml">)</mo></mrow><mi mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.3.3.3" xref="S2.E3.m1.5.5.1.1.2.2.3.3.3.cmml">b</mi></msup></mrow></mrow></mrow><mo mathsize="90%" id="S2.E3.m1.5.5.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.5b"><apply id="S2.E3.m1.5.5.1.1.3.cmml" xref="S2.E3.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.3a.cmml" xref="S2.E3.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E3.m1.5.5.1.1.1.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1"><eq id="S2.E3.m1.5.5.1.1.1.1.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.1"></eq><apply id="S2.E3.m1.5.5.1.1.1.1.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2"><times id="S2.E3.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.1"></times><apply id="S2.E3.m1.5.5.1.1.1.1.2.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.1.1.2.2.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E3.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2.2">𝑁</ci><apply id="S2.E3.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3"><times id="S2.E3.m1.5.5.1.1.1.1.2.2.3.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.1"></times><ci id="S2.E3.m1.5.5.1.1.1.1.2.2.3.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.2">𝑜</ci><ci id="S2.E3.m1.5.5.1.1.1.1.2.2.3.3.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.3">𝑝</ci><ci id="S2.E3.m1.5.5.1.1.1.1.2.2.3.4.cmml" xref="S2.E3.m1.5.5.1.1.1.1.2.2.3.4">𝑡</ci></apply></apply><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">𝐶</ci></apply><apply id="S2.E3.m1.5.5.1.1.1.1.3.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3"><times id="S2.E3.m1.5.5.1.1.1.1.3.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.1"></times><ci id="S2.E3.m1.5.5.1.1.1.1.3.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.2">𝐺</ci><apply id="S2.E3.m1.5.5.1.1.1.1.3.3.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.3">superscript</csymbol><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.3.2.2"><divide id="S2.E3.m1.2.2.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.3.2.2"></divide><ci id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2">𝐶</ci><cn type="integer" id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3">6</cn></apply><ci id="S2.E3.m1.5.5.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.5.5.1.1.1.1.3.3.3">𝑎</ci></apply></apply></apply><apply id="S2.E3.m1.5.5.1.1.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2"><eq id="S2.E3.m1.5.5.1.1.2.2.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.1"></eq><apply id="S2.E3.m1.5.5.1.1.2.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2"><times id="S2.E3.m1.5.5.1.1.2.2.2.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.1"></times><ci id="S2.E3.m1.5.5.1.1.2.2.2.2a.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.2"><mtext mathsize="90%" id="S2.E3.m1.5.5.1.1.2.2.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.2">&nbsp;</mtext></ci><apply id="S2.E3.m1.5.5.1.1.2.2.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.2.2.2.3.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3">subscript</csymbol><ci id="S2.E3.m1.5.5.1.1.2.2.2.3.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3.2">𝐷</ci><apply id="S2.E3.m1.5.5.1.1.2.2.2.3.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3"><times id="S2.E3.m1.5.5.1.1.2.2.2.3.3.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.1"></times><ci id="S2.E3.m1.5.5.1.1.2.2.2.3.3.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.2">𝑜</ci><ci id="S2.E3.m1.5.5.1.1.2.2.2.3.3.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.3">𝑝</ci><ci id="S2.E3.m1.5.5.1.1.2.2.2.3.3.4.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.3.3.4">𝑡</ci></apply></apply><ci id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3">𝐶</ci></apply><apply id="S2.E3.m1.5.5.1.1.2.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3"><times id="S2.E3.m1.5.5.1.1.2.2.3.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.1"></times><apply id="S2.E3.m1.5.5.1.1.2.2.3.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.2.2.3.2.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.2">superscript</csymbol><ci id="S2.E3.m1.5.5.1.1.2.2.3.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.2.2">𝐺</ci><apply id="S2.E3.m1.5.5.1.1.2.2.3.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.2.3"><minus id="S2.E3.m1.5.5.1.1.2.2.3.2.3.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.2.3"></minus><cn type="integer" id="S2.E3.m1.5.5.1.1.2.2.3.2.3.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.2.3.2">1</cn></apply></apply><apply id="S2.E3.m1.5.5.1.1.2.2.3.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.2.2.3.3.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.3">superscript</csymbol><apply id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.3.2.2"><divide id="S2.E3.m1.4.4.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.3.2.2"></divide><ci id="S2.E3.m1.4.4.2.cmml" xref="S2.E3.m1.4.4.2">𝐶</ci><cn type="integer" id="S2.E3.m1.4.4.3.cmml" xref="S2.E3.m1.4.4.3">6</cn></apply><ci id="S2.E3.m1.5.5.1.1.2.2.3.3.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.3.3.3">𝑏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.5c">\displaystyle N_{opt}(C)=G\bigg{(}\frac{C}{6}\bigg{)}^{a},\text{~{}~{}~{}}D_{opt}(C)=G^{-1}\bigg{(}\frac{C}{6}\bigg{)}^{b},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p8" class="ltx_para ltx_noindent">
<p id="S2.SS1.p8.9" class="ltx_p">where <math id="S2.SS1.p8.1.m1.1" class="ltx_Math" alttext="a=\frac{\alpha}{\alpha+\beta}" display="inline"><semantics id="S2.SS1.p8.1.m1.1a"><mrow id="S2.SS1.p8.1.m1.1.1" xref="S2.SS1.p8.1.m1.1.1.cmml"><mi id="S2.SS1.p8.1.m1.1.1.2" xref="S2.SS1.p8.1.m1.1.1.2.cmml">a</mi><mo id="S2.SS1.p8.1.m1.1.1.1" xref="S2.SS1.p8.1.m1.1.1.1.cmml">=</mo><mfrac id="S2.SS1.p8.1.m1.1.1.3" xref="S2.SS1.p8.1.m1.1.1.3.cmml"><mi id="S2.SS1.p8.1.m1.1.1.3.2" xref="S2.SS1.p8.1.m1.1.1.3.2.cmml">α</mi><mrow id="S2.SS1.p8.1.m1.1.1.3.3" xref="S2.SS1.p8.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p8.1.m1.1.1.3.3.2" xref="S2.SS1.p8.1.m1.1.1.3.3.2.cmml">α</mi><mo id="S2.SS1.p8.1.m1.1.1.3.3.1" xref="S2.SS1.p8.1.m1.1.1.3.3.1.cmml">+</mo><mi id="S2.SS1.p8.1.m1.1.1.3.3.3" xref="S2.SS1.p8.1.m1.1.1.3.3.3.cmml">β</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.1.m1.1b"><apply id="S2.SS1.p8.1.m1.1.1.cmml" xref="S2.SS1.p8.1.m1.1.1"><eq id="S2.SS1.p8.1.m1.1.1.1.cmml" xref="S2.SS1.p8.1.m1.1.1.1"></eq><ci id="S2.SS1.p8.1.m1.1.1.2.cmml" xref="S2.SS1.p8.1.m1.1.1.2">𝑎</ci><apply id="S2.SS1.p8.1.m1.1.1.3.cmml" xref="S2.SS1.p8.1.m1.1.1.3"><divide id="S2.SS1.p8.1.m1.1.1.3.1.cmml" xref="S2.SS1.p8.1.m1.1.1.3"></divide><ci id="S2.SS1.p8.1.m1.1.1.3.2.cmml" xref="S2.SS1.p8.1.m1.1.1.3.2">𝛼</ci><apply id="S2.SS1.p8.1.m1.1.1.3.3.cmml" xref="S2.SS1.p8.1.m1.1.1.3.3"><plus id="S2.SS1.p8.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p8.1.m1.1.1.3.3.1"></plus><ci id="S2.SS1.p8.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p8.1.m1.1.1.3.3.2">𝛼</ci><ci id="S2.SS1.p8.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p8.1.m1.1.1.3.3.3">𝛽</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.1.m1.1c">a=\frac{\alpha}{\alpha+\beta}</annotation></semantics></math>, <math id="S2.SS1.p8.2.m2.1" class="ltx_Math" alttext="b=\frac{\beta}{\alpha+\beta}" display="inline"><semantics id="S2.SS1.p8.2.m2.1a"><mrow id="S2.SS1.p8.2.m2.1.1" xref="S2.SS1.p8.2.m2.1.1.cmml"><mi id="S2.SS1.p8.2.m2.1.1.2" xref="S2.SS1.p8.2.m2.1.1.2.cmml">b</mi><mo id="S2.SS1.p8.2.m2.1.1.1" xref="S2.SS1.p8.2.m2.1.1.1.cmml">=</mo><mfrac id="S2.SS1.p8.2.m2.1.1.3" xref="S2.SS1.p8.2.m2.1.1.3.cmml"><mi id="S2.SS1.p8.2.m2.1.1.3.2" xref="S2.SS1.p8.2.m2.1.1.3.2.cmml">β</mi><mrow id="S2.SS1.p8.2.m2.1.1.3.3" xref="S2.SS1.p8.2.m2.1.1.3.3.cmml"><mi id="S2.SS1.p8.2.m2.1.1.3.3.2" xref="S2.SS1.p8.2.m2.1.1.3.3.2.cmml">α</mi><mo id="S2.SS1.p8.2.m2.1.1.3.3.1" xref="S2.SS1.p8.2.m2.1.1.3.3.1.cmml">+</mo><mi id="S2.SS1.p8.2.m2.1.1.3.3.3" xref="S2.SS1.p8.2.m2.1.1.3.3.3.cmml">β</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.2.m2.1b"><apply id="S2.SS1.p8.2.m2.1.1.cmml" xref="S2.SS1.p8.2.m2.1.1"><eq id="S2.SS1.p8.2.m2.1.1.1.cmml" xref="S2.SS1.p8.2.m2.1.1.1"></eq><ci id="S2.SS1.p8.2.m2.1.1.2.cmml" xref="S2.SS1.p8.2.m2.1.1.2">𝑏</ci><apply id="S2.SS1.p8.2.m2.1.1.3.cmml" xref="S2.SS1.p8.2.m2.1.1.3"><divide id="S2.SS1.p8.2.m2.1.1.3.1.cmml" xref="S2.SS1.p8.2.m2.1.1.3"></divide><ci id="S2.SS1.p8.2.m2.1.1.3.2.cmml" xref="S2.SS1.p8.2.m2.1.1.3.2">𝛽</ci><apply id="S2.SS1.p8.2.m2.1.1.3.3.cmml" xref="S2.SS1.p8.2.m2.1.1.3.3"><plus id="S2.SS1.p8.2.m2.1.1.3.3.1.cmml" xref="S2.SS1.p8.2.m2.1.1.3.3.1"></plus><ci id="S2.SS1.p8.2.m2.1.1.3.3.2.cmml" xref="S2.SS1.p8.2.m2.1.1.3.3.2">𝛼</ci><ci id="S2.SS1.p8.2.m2.1.1.3.3.3.cmml" xref="S2.SS1.p8.2.m2.1.1.3.3.3">𝛽</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.2.m2.1c">b=\frac{\beta}{\alpha+\beta}</annotation></semantics></math> and <math id="S2.SS1.p8.3.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.SS1.p8.3.m3.1a"><mi id="S2.SS1.p8.3.m3.1.1" xref="S2.SS1.p8.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.3.m3.1b"><ci id="S2.SS1.p8.3.m3.1.1.cmml" xref="S2.SS1.p8.3.m3.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.3.m3.1c">G</annotation></semantics></math> is a scaling coefficient that can be computed by <math id="S2.SS1.p8.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS1.p8.4.m4.1a"><mi id="S2.SS1.p8.4.m4.1.1" xref="S2.SS1.p8.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.4.m4.1b"><ci id="S2.SS1.p8.4.m4.1.1.cmml" xref="S2.SS1.p8.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.4.m4.1c">A</annotation></semantics></math>, <math id="S2.SS1.p8.5.m5.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS1.p8.5.m5.1a"><mi id="S2.SS1.p8.5.m5.1.1" xref="S2.SS1.p8.5.m5.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.5.m5.1b"><ci id="S2.SS1.p8.5.m5.1.1.cmml" xref="S2.SS1.p8.5.m5.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.5.m5.1c">B</annotation></semantics></math>, <math id="S2.SS1.p8.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS1.p8.6.m6.1a"><mi id="S2.SS1.p8.6.m6.1.1" xref="S2.SS1.p8.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.6.m6.1b"><ci id="S2.SS1.p8.6.m6.1.1.cmml" xref="S2.SS1.p8.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.6.m6.1c">\alpha</annotation></semantics></math> and <math id="S2.SS1.p8.7.m7.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S2.SS1.p8.7.m7.1a"><mi id="S2.SS1.p8.7.m7.1.1" xref="S2.SS1.p8.7.m7.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.7.m7.1b"><ci id="S2.SS1.p8.7.m7.1.1.cmml" xref="S2.SS1.p8.7.m7.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.7.m7.1c">\beta</annotation></semantics></math>. As analyzed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, given an increase in compute budget, the KM scaling law favors a larger budget allocation in model size than the data size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales, <em id="S2.SS1.p8.9.1" class="ltx_emph ltx_font_italic">i.e.,</em> having similar values for <math id="S2.SS1.p8.8.m8.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS1.p8.8.m8.1a"><mi id="S2.SS1.p8.8.m8.1.1" xref="S2.SS1.p8.8.m8.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.8.m8.1b"><ci id="S2.SS1.p8.8.m8.1.1.cmml" xref="S2.SS1.p8.8.m8.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.8.m8.1c">a</annotation></semantics></math> and <math id="S2.SS1.p8.9.m9.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS1.p8.9.m9.1a"><mi id="S2.SS1.p8.9.m9.1.1" xref="S2.SS1.p8.9.m9.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p8.9.m9.1b"><ci id="S2.SS1.p8.9.m9.1.1.cmml" xref="S2.SS1.p8.9.m9.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p8.9.m9.1c">b</annotation></semantics></math> in Equation&nbsp;(<a href="#S2.E3" title="In 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="S2.SS1.p9" class="ltx_para ltx_noindent">
<p id="S2.SS1.p9.1" class="ltx_p"><span id="S2.SS1.p9.1.1" class="ltx_text ltx_font_bold">Discussion on Scaling Laws</span>. After introducing the formulations, we continue to discuss scaling law in the following two aspects, to enhance its understanding:</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p"><math id="S2.SS1.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p10.1.m1.1a"><mo id="S2.SS1.p10.1.m1.1.1" xref="S2.SS1.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p10.1.m1.1b"><ci id="S2.SS1.p10.1.m1.1.1.cmml" xref="S2.SS1.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p10.1.1" class="ltx_emph ltx_font_italic">Predictable scaling</em>. In practice, scaling law can be used to instruct the training of LLMs, and
it has been proven feasible to reliably estimate the performance of larger models based on that of smaller models, called <em id="S2.SS1.p10.1.2" class="ltx_emph ltx_font_italic">predictable scaling</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
The benefits of predictable scaling for training LLMs are mainly twofold.
Firstly, for large models, it is infeasible to rigorously examine various training tricks or variants, and it would be very helpful if experiences gained from small models could also apply to large models.
For instance, small proxy models can be trained to find the optimal schedule of the data mixture for large models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
Secondly, the training of large-scale models takes a long time, often suffering from issues such as training loss spike, and scaling law can be employed to monitor the training status of LLMs, <em id="S2.SS1.p10.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> identifying abnormal performance at an early time. Despite that scaling law characterizes a smooth trend of performance increase (or loss decrease), it also indicates that <em id="S2.SS1.p10.1.4" class="ltx_emph ltx_font_italic">diminishing returns</em><span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://en.wikipedia.org/wiki/Diminishing_returns</span></span></span> might occur as model scaling. An empirical study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> from the OpenAI team has shown that
representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns (<em id="S2.SS1.p10.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> approaching the irreducible loss)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. This finding suggests that training large models are promising for improving the performance of downstream tasks.
To further explore scaling effect, a potential issue is that the amount of available data for training LLMs is actually limited. With the ever-increasing model scale, the public text data would be soon “exhausted” for LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. Thus, it will be meaningful to study how scaling laws apply to a data-constrained regime&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, where data repetition or augmentation might be useful to alleviate data scarcity.</p>
</div>
<div id="S2.SS1.p11" class="ltx_para">
<p id="S2.SS1.p11.1" class="ltx_p"><math id="S2.SS1.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p11.1.m1.1a"><mo id="S2.SS1.p11.1.m1.1.1" xref="S2.SS1.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p11.1.m1.1b"><ci id="S2.SS1.p11.1.m1.1.1.cmml" xref="S2.SS1.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p11.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p11.1.1" class="ltx_emph ltx_font_italic">Task-level predictability</em>.
Existing research of scaling laws are mostly conducted in terms of language modeling loss (<em id="S2.SS1.p11.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> per-token cross-entropy loss in nats&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>), while in practice we are more concerned about the performance of LLMs on actual tasks.
Thus, a basic problem is that how the decrease of language modeling loss translates into the improvement of task performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.
Intuitively, a model with a smaller language modeling loss tends to yield a better performance on downstream tasks, since language modeling loss can be considered as a general measure of the overall model capacity.
GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> has reported that some capabilities (<em id="S2.SS1.p11.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> coding ability) can be accurately predicted via scaling law. Despite that,
readers should be aware that a direct decrease in language modeling loss does not always indicate an improvement of model performance on downstream tasks. Specially, the phenomenon of <em id="S2.SS1.p11.1.4" class="ltx_emph ltx_font_italic">inverse scaling</em> would occur for some tasks, where task performance surprisingly becomes worse as the language modeling loss decreases&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Overall, it is more difficult to explore and characterize task-level scaling laws, since it might be also dependent on task-related information (task metric, task difficulty, etc.). Furthermore, some capacities (<em id="S2.SS1.p11.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> in-context learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>) are unpredictable according to the scaling law, which can be observed only when the model size exceeds a certain level (as discussed below).</p>
</div>
<div id="S2.SS1.p12" class="ltx_para ltx_noindent">
<p id="S2.SS1.p12.1" class="ltx_p"><span id="S2.SS1.p12.1.1" class="ltx_text ltx_font_bold">Emergent Abilities of LLMs</span>. In the literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, <em id="S2.SS1.p12.1.2" class="ltx_emph ltx_font_italic">emergent abilities</em> of LLMs are formally defined as “the abilities that are not present in small models but arise in large models”, which is one of the most prominent features that distinguish LLMs from previous PLMs.
It further introduces a notable characteristic when emergent abilities occur&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>: performance
rises significantly above random when the scale reaches a certain level. By analogy, such an emergent pattern has close connections with the phenomenon of <em id="S2.SS1.p12.1.3" class="ltx_emph ltx_font_italic">phase transition</em> in physics&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. In principle, emergent abilities can be defined in relation to some complex tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, while we are more concerned with general abilities that can be applied to solve a variety of tasks. Here, we briefly introduce three typical emergent abilities for LLMs and representative models that possess such an ability<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>It is difficult to accurately examine the critical size for emergent abilities of LLMs (<em id="footnote8.1" class="ltx_emph ltx_font_italic">i.e.,</em> the minimum size to possess an ability), since it might vary for different models or tasks. Also, existing studies often test emergent abilities on very limited model sizes for a specific LLM. For example, PaLM is often tested with three sizes of 8B, 62B and 540B. It is unclear about the model performance of the untested sizes.</span></span></span>.</p>
</div>
<div id="S2.SS1.p13" class="ltx_para">
<p id="S2.SS1.p13.1" class="ltx_p"><math id="S2.SS1.p13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p13.1.m1.1a"><mo id="S2.SS1.p13.1.m1.1.1" xref="S2.SS1.p13.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p13.1.m1.1b"><ci id="S2.SS1.p13.1.m1.1.1.cmml" xref="S2.SS1.p13.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p13.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p13.1.1" class="ltx_emph ltx_font_italic">In-context learning.</em>
The in-context learning&nbsp;(ICL) ability is formally introduced by GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>: assuming that the language model has been provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient update<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>In a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, it also shows that in-context learning implicitly performs meta-optimization through the attention mechanism.</span></span></span>.
Among the GPT-series models, the 175B GPT-3 model exhibited a strong ICL ability in general, but not the GPT-1 and GPT-2 models.
Such an ability also depends on the specific downstream task. For example, the ICL ability can emerge on the arithmetic tasks (<em id="S2.SS1.p13.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> the 3-digit addition and subtraction) for the 13B GPT-3, but 175B GPT-3 even cannot work well on the Persian QA task&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S2.SS1.p14" class="ltx_para">
<p id="S2.SS1.p14.1" class="ltx_p"><math id="S2.SS1.p14.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p14.1.m1.1a"><mo id="S2.SS1.p14.1.m1.1.1" xref="S2.SS1.p14.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p14.1.m1.1b"><ci id="S2.SS1.p14.1.m1.1.1.cmml" xref="S2.SS1.p14.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p14.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p14.1.1" class="ltx_emph ltx_font_italic">Instruction following.</em>
By fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions (called <em id="S2.SS1.p14.1.2" class="ltx_emph ltx_font_italic">instruction tuning</em>),
LLMs are shown to perform well on unseen tasks that are also described in the form of instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
With instruction tuning, LLMs are enabled to follow
the task instructions for new tasks without using explicit examples, thus having an improved generalization ability.
According to the experiments in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, instruction-tuned LaMDA-PT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> started to significantly outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (<em id="S2.SS1.p14.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (<em id="S2.SS1.p14.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> MMLU).</p>
</div>
<div id="S2.SS1.p15" class="ltx_para">
<p id="S2.SS1.p15.3" class="ltx_p"><math id="S2.SS1.p15.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p15.1.m1.1a"><mo id="S2.SS1.p15.1.m1.1.1" xref="S2.SS1.p15.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p15.1.m1.1b"><ci id="S2.SS1.p15.1.m1.1.1.cmml" xref="S2.SS1.p15.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p15.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p15.3.1" class="ltx_emph ltx_font_italic">Step-by-step reasoning.</em>
For small language models, it is usually difficult to solve complex tasks that involve multiple reasoning steps, <em id="S2.SS1.p15.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> mathematical word problems.
In contrast, with the chain-of-thought&nbsp;(CoT) prompting strategy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, LLMs can solve such tasks by utilizing the prompting mechanism that involves intermediate reasoning steps for deriving the final answer.
This ability is speculated to be potentially obtained by training on code&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
An empirical study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> has shown that CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B.
Furthermore, the performance improvement with CoT prompting seems to be also varied for different tasks, <em id="S2.SS1.p15.3.3" class="ltx_emph ltx_font_italic">e.g.,</em> GSM8K <math id="S2.SS1.p15.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.SS1.p15.2.m2.1a"><mo id="S2.SS1.p15.2.m2.1.1" xref="S2.SS1.p15.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p15.2.m2.1b"><gt id="S2.SS1.p15.2.m2.1.1.cmml" xref="S2.SS1.p15.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p15.2.m2.1c">&gt;</annotation></semantics></math> MAWPS <math id="S2.SS1.p15.3.m3.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.SS1.p15.3.m3.1a"><mo id="S2.SS1.p15.3.m3.1.1" xref="S2.SS1.p15.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p15.3.m3.1b"><gt id="S2.SS1.p15.3.m3.1.1.cmml" xref="S2.SS1.p15.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p15.3.m3.1c">&gt;</annotation></semantics></math> SWAMP for PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S2.SS1.p16" class="ltx_para ltx_noindent">
<p id="S2.SS1.p16.1" class="ltx_p"><span id="S2.SS1.p16.1.1" class="ltx_text ltx_font_bold">How Emergent Abilities Relate to Scaling Laws</span>. In existing literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, scaling laws and emergent abilities provide two perspectives to understand the advantage of large models over small models. In general, scaling law (often measured by <em id="S2.SS1.p16.1.2" class="ltx_emph ltx_font_italic">language modeling loss</em>) describes predictable performance relation with the potential effect of diminishing returns, while emergent abilities (often measured by <em id="S2.SS1.p16.1.3" class="ltx_emph ltx_font_italic">task performance</em>) are unpredictable but very profitable once such abilities actually emerge.
Since the two perspectives reflect different performance trends (continuous improvement <em id="S2.SS1.p16.1.4" class="ltx_emph ltx_font_italic">v.s.</em> sharp performance leap), they might lead to misaligned findings or observations.
There are also extensive debates on the rationality of emergent abilities.
A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (<em id="S2.SS1.p16.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> the discontinuous evaluation metrics)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>: when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear.
However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way.
For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones.
More recently, a study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. Despite these efforts, more fundamental research (<em id="S2.SS1.p16.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> grokking<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Grokking refers that “a pattern in the data, improving generalization performance from random chance level to perfect generalization”, quoted from the original paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>.</span></span></span>) about the working mechanism of LLMs is still in need to understand the emergence of certain abilities. The subtle relation between scaling law and emergent abilities can be explained by analogy with the ability acquisition of human<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>This explanation is only for ease of understanding, and there is not direct evidence to connect the two points. </span></span></span>.
Take the speaking ability as an example. For children, language development (especially infants) can be also considered as a multi-level process where “emergent abilities” occur. Specially, the language ability would relatively stable within a time interval, but qualitative change only occurs when evolving into another ability level (<em id="S2.SS1.p16.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> from speaking simple words to speaking simple sentences). Such a learning process is essentially not <em id="S2.SS1.p16.1.8" class="ltx_emph ltx_font_italic">smooth</em> and <em id="S2.SS1.p16.1.9" class="ltx_emph ltx_font_italic">stable</em> (<em id="S2.SS1.p16.1.10" class="ltx_emph ltx_font_italic">i.e.,</em> language ability does not develop at a constant rate over time), though a child actually grows every day. It is interesting that young parents would be often surprised by unexpected progress of the speaking ability exhibited by their babies.</p>
</div>
<div id="S2.SS1.p17" class="ltx_para ltx_noindent">
<p id="S2.SS1.p17.1" class="ltx_p"><span id="S2.SS1.p17.1.1" class="ltx_text ltx_font_bold">Key Techniques for LLMs</span>. It has been a long way that LLMs evolve into the current state: <em id="S2.SS1.p17.1.2" class="ltx_emph ltx_font_italic">general</em> and <em id="S2.SS1.p17.1.3" class="ltx_emph ltx_font_italic">capable</em> learners.
In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs.
Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.</p>
</div>
<div id="S2.SS1.p18" class="ltx_para">
<p id="S2.SS1.p18.1" class="ltx_p"><math id="S2.SS1.p18.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p18.1.m1.1a"><mo id="S2.SS1.p18.1.m1.1.1" xref="S2.SS1.p18.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p18.1.m1.1b"><ci id="S2.SS1.p18.1.m1.1.1.cmml" xref="S2.SS1.p18.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p18.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p18.1.1" class="ltx_emph ltx_font_italic">Scaling</em>. As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively. Since compute budget is usually limited, scaling laws can be further employed to conduct a more compute-efficient allocation of the compute resources.
For example, Chinchilla (with more training tokens) outperforms its
counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
In addition, data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity.</p>
</div>
<div id="S2.SS1.p19" class="ltx_para">
<p id="S2.SS1.p19.1" class="ltx_p"><math id="S2.SS1.p19.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p19.1.m1.1a"><mo id="S2.SS1.p19.1.m1.1.1" xref="S2.SS1.p19.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p19.1.m1.1b"><ci id="S2.SS1.p19.1.m1.1.1.cmml" xref="S2.SS1.p19.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p19.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p19.1.1" class="ltx_emph ltx_font_italic">Training</em>. Due to the huge model size, it is very challenging to successfully train a capable LLM. Distributed training algorithms are needed to learn
the network parameters of LLMs, in which various parallel strategies are often jointly utilized. To support distributed training, several optimization frameworks have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> and Megatron-LM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. Also, optimization tricks are also important for training stability and model performance, <em id="S2.SS1.p19.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> restart to overcome training loss spike&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and mixed precision training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.
More recently, GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models.</p>
</div>
<div id="S2.SS1.p20" class="ltx_para">
<p id="S2.SS1.p20.1" class="ltx_p"><math id="S2.SS1.p20.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p20.1.m1.1a"><mo id="S2.SS1.p20.1.m1.1.1" xref="S2.SS1.p20.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p20.1.m1.1b"><ci id="S2.SS1.p20.1.m1.1.1.cmml" xref="S2.SS1.p20.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p20.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p20.1.1" class="ltx_emph ltx_font_italic">Ability eliciting</em>. After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general-purpose task solvers.
These abilities might not be explicitly exhibited when LLMs perform some specific tasks. As the technical approach, it is useful to design suitable task instructions or specific in-context learning strategies to elicit such abilities.
For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps.
Furthermore, we can perform instruction tuning on LLMs with task descriptions expressed in natural language, for improving the generalizability of LLMs on unseen tasks.
These eliciting techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models.</p>
</div>
<div id="S2.SS1.p21" class="ltx_para">
<p id="S2.SS1.p21.1" class="ltx_p"><math id="S2.SS1.p21.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p21.1.m1.1a"><mo id="S2.SS1.p21.1.m1.1.1" xref="S2.SS1.p21.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p21.1.m1.1b"><ci id="S2.SS1.p21.1.m1.1.1.cmml" xref="S2.SS1.p21.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p21.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p21.1.1" class="ltx_emph ltx_font_italic">Alignment tuning</em>. Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans. It is necessary to align LLMs with human values, <em id="S2.SS1.p21.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> <em id="S2.SS1.p21.1.3" class="ltx_emph ltx_font_italic">helpful</em>, <em id="S2.SS1.p21.1.4" class="ltx_emph ltx_font_italic">honest</em>, and <em id="S2.SS1.p21.1.5" class="ltx_emph ltx_font_italic">harmless</em>. For this purpose, InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of <em id="S2.SS1.p21.1.6" class="ltx_emph ltx_font_italic">reinforcement learning with human feedback</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. It incorporates human in the training loop with elaborately designed labeling strategies.
ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, <em id="S2.SS1.p21.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> rejecting to answer insulting questions.</p>
</div>
<div id="S2.SS1.p22" class="ltx_para">
<p id="S2.SS1.p22.1" class="ltx_p"><math id="S2.SS1.p22.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS1.p22.1.m1.1a"><mo id="S2.SS1.p22.1.m1.1.1" xref="S2.SS1.p22.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p22.1.m1.1b"><ci id="S2.SS1.p22.1.m1.1.1.cmml" xref="S2.SS1.p22.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p22.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS1.p22.1.1" class="ltx_emph ltx_font_italic">Tools manipulation</em>. In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best expressed in the form of text (<em id="S2.SS1.p22.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> numerical computation). In addition, their capacities are also limited to the pre-training data, <em id="S2.SS1.p22.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> the inability to capture up-to-date information. To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. For example, LLMs can utilize the calculator for accurate computation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and employ search engines to retrieve unknown information&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.
More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps)<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://openai.com/blog/chatgpt-plugins</span></span></span>, which are by analogy with the “<em id="S2.SS1.p22.1.4" class="ltx_emph ltx_font_italic">eyes and ears</em>” of LLMs. Such a mechanism can broadly expand the scope of capacities for LLMs.</p>
</div>
<div id="S2.SS1.p23" class="ltx_para">
<p id="S2.SS1.p23.1" class="ltx_p">In addition, many other factors (<em id="S2.SS1.p23.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> the upgrade of hardware) also contribute to the success of LLMs. Currently, we limit our discussion to the major technical approaches and key findings for developing LLMs.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2303.18223/assets/x4.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A timeline of existing large language models (having a size larger than 10B) in recent years. The timeline was established mainly according to the release date (<em id="S2.F3.2.1" class="ltx_emph ltx_font_italic">e.g.,</em> the submission date to arXiv) of the technical paper for a model. If there was not a corresponding paper, we set the date of a model as the earliest time of its public release or announcement. We mark the LLMs with publicly available model checkpoints in yellow color. Due to the space limit of the figure, we only include the LLMs with publicly reported evaluation results. </figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Statistics of large language models (having a size larger than 10B in this survey) in recent years, including the capacity evaluation, pre-training data scale (either in the number of tokens or storage size) and hardware resource costs. In this table, we only include LLMs with a public paper about the technical details.
Here, “Release Time” indicates the date when the corresponding paper was officially released. “Publicly Available” means that the model checkpoints can be publicly accessible while “Closed Source” means the opposite.
“Adaptation” indicates whether the model has been with subsequent fine-tuning: IT denotes instruction tuning and RLHF denotes reinforcement learning with human feedback.
“Evaluation” indicates whether the model has been evaluated with corresponding abilities in their original paper: ICL denotes in-context learning and CoT denotes chain-of-thought. “*” denotes the largest publicly available version.
</figcaption>
<table id="S2.T1.90" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.90.91" class="ltx_tr">
<td id="S2.T1.90.91.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.4" class="ltx_td ltx_nopad_l ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S2.T1.90.91.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Adaptation</span></td>
<td id="S2.T1.90.91.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.91.11" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S2.T1.90.91.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Evaluation</span></td>
</tr>
<tr id="S2.T1.90.92" class="ltx_tr">
<td id="S2.T1.90.92.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.92.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></td>
<td id="S2.T1.90.92.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.3.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.3.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.3.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Release</span></span></span>
<span id="S2.T1.90.92.3.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.3.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.3.1.1.2.1.1" class="ltx_text ltx_font_bold">Time</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.4.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.4.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.4.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Size</span></span></span>
<span id="S2.T1.90.92.4.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.4.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.4.1.1.2.1.1" class="ltx_text ltx_font_bold">(B)</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.5.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.5.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Base</span></span></span>
<span id="S2.T1.90.92.5.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.5.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.5.1.1.2.1.1" class="ltx_text ltx_font_bold">Model</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IT</span></td>
<td id="S2.T1.90.92.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RLHF</span></td>
<td id="S2.T1.90.92.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.8.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.8.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.8.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.8.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.8.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-train</span></span></span>
<span id="S2.T1.90.92.8.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.8.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.8.1.1.2.1.1" class="ltx_text ltx_font_bold">Data Scale</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.9.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.9.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.9.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.9.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.9.1.1.1.1.1" class="ltx_text ltx_font_bold">Latest Data</span></span></span>
<span id="S2.T1.90.92.9.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.9.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.9.1.1.2.1.1" class="ltx_text ltx_font_bold">Timestamp</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.10.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.10.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.10.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.10.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.10.1.1.1.1.1" class="ltx_text ltx_font_bold">Hardware</span></span></span>
<span id="S2.T1.90.92.10.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.10.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.10.1.1.2.1.1" class="ltx_text ltx_font_bold">(GPUs / TPUs)</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.11.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.92.11.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.92.11.1.1.1" class="ltx_tr">
<span id="S2.T1.90.92.11.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.11.1.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S2.T1.90.92.11.1.1.2" class="ltx_tr">
<span id="S2.T1.90.92.11.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.11.1.1.2.1.1" class="ltx_text ltx_font_bold">Time</span></span></span>
</span></span></td>
<td id="S2.T1.90.92.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ICL</span></td>
<td id="S2.T1.90.92.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.92.13.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CoT</span></td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">T5&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib82" title="" class="ltx_ref">82</a><span id="S2.T1.1.1.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.4.1" class="ltx_text" style="font-size:80%;">Oct-2019</span></td>
<td id="S2.T1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.5.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S2.T1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.1.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.9.1" class="ltx_text" style="font-size:80%;">1T tokens</span></td>
<td id="S2.T1.1.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.10.1" class="ltx_text" style="font-size:80%;">Apr-2019</span></td>
<td id="S2.T1.1.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.11.1" class="ltx_text" style="font-size:80%;">1024 TPU v3</span></td>
<td id="S2.T1.1.1.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.1.1.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.1.1.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">mT5&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.2.2.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib83" title="" class="ltx_ref">83</a><span id="S2.T1.2.2.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.4.1" class="ltx_text" style="font-size:80%;">Oct-2020</span></td>
<td id="S2.T1.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.5.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S2.T1.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.2.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.9.1" class="ltx_text" style="font-size:80%;">1T tokens</span></td>
<td id="S2.T1.2.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.2.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.2.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.2.2.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.2.2.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><ci id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.2.2.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.2.2.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<td id="S2.T1.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.3.3.1.1" class="ltx_text" style="font-size:80%;">PanGu-</span><math id="S2.T1.3.3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.T1.3.3.1.m1.1a"><mi mathsize="80%" id="S2.T1.3.3.1.m1.1.1" xref="S2.T1.3.3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.m1.1b"><ci id="S2.T1.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.m1.1c">\alpha</annotation></semantics></math><span id="S2.T1.3.3.1.2" class="ltx_text" style="font-size:80%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.3.3.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib84" title="" class="ltx_ref">84</a><span id="S2.T1.3.3.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.4.1" class="ltx_text" style="font-size:80%;">Apr-2021</span></td>
<td id="S2.T1.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.5.1" class="ltx_text" style="font-size:80%;">13*</span></td>
<td id="S2.T1.4.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.4.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.4.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.4.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.9.1" class="ltx_text" style="font-size:80%;">1.1TB</span></td>
<td id="S2.T1.4.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.4.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.11.1" class="ltx_text" style="font-size:80%;">2048 Ascend 910</span></td>
<td id="S2.T1.4.4.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.4.4.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.4.4.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.4.4.2.m1.1.1" xref="S2.T1.4.4.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.2.m1.1b"><ci id="S2.T1.4.4.2.m1.1.1.cmml" xref="S2.T1.4.4.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.4.4.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.4.4.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.90.93" class="ltx_tr">
<td id="S2.T1.90.93.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.93.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.90.93.2.1" class="ltx_text" style="font-size:80%;">CPM-2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.90.93.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib85" title="" class="ltx_ref">85</a><span id="S2.T1.90.93.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.90.93.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.3.1" class="ltx_text" style="font-size:80%;">Jun-2021</span></td>
<td id="S2.T1.90.93.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.4.1" class="ltx_text" style="font-size:80%;">198</span></td>
<td id="S2.T1.90.93.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.8.1" class="ltx_text" style="font-size:80%;">2.6TB</span></td>
<td id="S2.T1.90.93.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.93.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.93.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.6.6" class="ltx_tr">
<td id="S2.T1.6.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.6.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.6.6.4.1" class="ltx_text" style="font-size:80%;">T0&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.6.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S2.T1.6.6.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.6.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.5.1" class="ltx_text" style="font-size:80%;">Oct-2021</span></td>
<td id="S2.T1.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.6.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S2.T1.6.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.7.1" class="ltx_text" style="font-size:80%;">T5</span></td>
<td id="S2.T1.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.5.5.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.5.5.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.5.5.1.m1.1.1" xref="S2.T1.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b"><ci id="S2.T1.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.6.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.6.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.6.6.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.6.6.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.11.1" class="ltx_text" style="font-size:80%;">512 TPU v3</span></td>
<td id="S2.T1.6.6.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.12.1" class="ltx_text" style="font-size:80%;">27 h</span></td>
<td id="S2.T1.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.6.6.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.6.6.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.6.6.2.m1.1.1" xref="S2.T1.6.6.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.2.m1.1b"><ci id="S2.T1.6.6.2.m1.1.1.cmml" xref="S2.T1.6.6.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.6.6.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.6.6.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.7.7" class="ltx_tr">
<td id="S2.T1.7.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.7.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.7.7.3.1" class="ltx_text" style="font-size:80%;">CodeGen&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.7.7.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="S2.T1.7.7.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.7.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.4.1" class="ltx_text" style="font-size:80%;">Mar-2022</span></td>
<td id="S2.T1.7.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.5.1" class="ltx_text" style="font-size:80%;">16</span></td>
<td id="S2.T1.7.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.7.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.7.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.9.1" class="ltx_text" style="font-size:80%;">577B tokens</span></td>
<td id="S2.T1.7.7.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.7.7.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.7.7.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.7.7.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.7.7.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.7.7.1.m1.1.1" xref="S2.T1.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.m1.1b"><ci id="S2.T1.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.7.7.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.7.7.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.8.8" class="ltx_tr">
<td id="S2.T1.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.8.8.3.1" class="ltx_text" style="font-size:80%;">GPT-NeoX-20B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.8.8.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="S2.T1.8.8.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.8.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.4.1" class="ltx_text" style="font-size:80%;">Apr-2022</span></td>
<td id="S2.T1.8.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.5.1" class="ltx_text" style="font-size:80%;">20</span></td>
<td id="S2.T1.8.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.8.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.8.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.8.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.9.1" class="ltx_text" style="font-size:80%;">825GB</span></td>
<td id="S2.T1.8.8.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.8.8.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.11.1" class="ltx_text" style="font-size:80%;">96 40G A100</span></td>
<td id="S2.T1.8.8.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.8.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.8.8.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.8.8.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.8.8.1.m1.1.1" xref="S2.T1.8.8.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.1.m1.1b"><ci id="S2.T1.8.8.1.m1.1.1.cmml" xref="S2.T1.8.8.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.8.8.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.8.8.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.10.10" class="ltx_tr">
<td id="S2.T1.10.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.10.10.4.1" class="ltx_text" style="font-size:80%;">Tk-Instruct&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.10.10.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib88" title="" class="ltx_ref">88</a><span id="S2.T1.10.10.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.10.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.5.1" class="ltx_text" style="font-size:80%;">Apr-2022</span></td>
<td id="S2.T1.10.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.6.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S2.T1.10.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.7.1" class="ltx_text" style="font-size:80%;">T5</span></td>
<td id="S2.T1.9.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.9.9.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.9.9.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.9.9.1.m1.1.1" xref="S2.T1.9.9.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.1.m1.1b"><ci id="S2.T1.9.9.1.m1.1.1.cmml" xref="S2.T1.9.9.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.10.10.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.10.10.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.10.10.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.10.10.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.11.1" class="ltx_text" style="font-size:80%;">256 TPU v3</span></td>
<td id="S2.T1.10.10.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.12.1" class="ltx_text" style="font-size:80%;">4 h</span></td>
<td id="S2.T1.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.10.10.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.10.10.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.10.10.2.m1.1.1" xref="S2.T1.10.10.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.2.m1.1b"><ci id="S2.T1.10.10.2.m1.1.1.cmml" xref="S2.T1.10.10.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.10.10.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.10.10.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.12.12" class="ltx_tr">
<td id="S2.T1.12.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.12.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.12.12.4.1" class="ltx_text" style="font-size:80%;">UL2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.12.12.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a><span id="S2.T1.12.12.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.12.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.5.1" class="ltx_text" style="font-size:80%;">May-2022</span></td>
<td id="S2.T1.12.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.6.1" class="ltx_text" style="font-size:80%;">20</span></td>
<td id="S2.T1.12.12.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.12.12.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.12.12.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.12.12.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.10.1" class="ltx_text" style="font-size:80%;">1T tokens</span></td>
<td id="S2.T1.12.12.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.11.1" class="ltx_text" style="font-size:80%;">Apr-2019</span></td>
<td id="S2.T1.12.12.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.12.1" class="ltx_text" style="font-size:80%;">512 TPU v4</span></td>
<td id="S2.T1.12.12.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.12.12.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.11.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.11.11.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.11.11.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.11.11.1.m1.1.1" xref="S2.T1.11.11.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.1.m1.1b"><ci id="S2.T1.11.11.1.m1.1.1.cmml" xref="S2.T1.11.11.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.12.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.12.12.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.12.12.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.12.12.2.m1.1.1" xref="S2.T1.12.12.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.2.m1.1b"><ci id="S2.T1.12.12.2.m1.1.1.cmml" xref="S2.T1.12.12.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.13.13" class="ltx_tr">
<td id="S2.T1.13.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.13.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.13.13.3.1" class="ltx_text" style="font-size:80%;">OPT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.13.13.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib90" title="" class="ltx_ref">90</a><span id="S2.T1.13.13.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.13.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.4.1" class="ltx_text" style="font-size:80%;">May-2022</span></td>
<td id="S2.T1.13.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.5.1" class="ltx_text" style="font-size:80%;">175</span></td>
<td id="S2.T1.13.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.13.13.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.13.13.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.13.13.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.9.1" class="ltx_text" style="font-size:80%;">180B tokens</span></td>
<td id="S2.T1.13.13.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.13.13.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.11.1" class="ltx_text" style="font-size:80%;">992 80G A100</span></td>
<td id="S2.T1.13.13.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.13.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.13.13.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.13.13.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.13.13.1.m1.1.1" xref="S2.T1.13.13.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.1.m1.1b"><ci id="S2.T1.13.13.1.m1.1.1.cmml" xref="S2.T1.13.13.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.13.13.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.13.13.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.14.14" class="ltx_tr">
<td id="S2.T1.14.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.14.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.14.14.3.1" class="ltx_text" style="font-size:80%;">NLLB&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.14.14.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib91" title="" class="ltx_ref">91</a><span id="S2.T1.14.14.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.14.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.4.1" class="ltx_text" style="font-size:80%;">Jul-2022</span></td>
<td id="S2.T1.14.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.5.1" class="ltx_text" style="font-size:80%;">54.5</span></td>
<td id="S2.T1.14.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.14.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.14.14.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.14.14.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.14.14.1.m1.1.1" xref="S2.T1.14.14.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.1.m1.1b"><ci id="S2.T1.14.14.1.m1.1.1.cmml" xref="S2.T1.14.14.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.14.14.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.14.14.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.15.15" class="ltx_tr">
<td id="S2.T1.15.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.15.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.15.15.3.1" class="ltx_text" style="font-size:80%;">CodeGeeX&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.15.15.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib92" title="" class="ltx_ref">92</a><span id="S2.T1.15.15.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.15.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.4.1" class="ltx_text" style="font-size:80%;">Sep-2022</span></td>
<td id="S2.T1.15.15.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.5.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S2.T1.15.15.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.15.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.15.15.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.15.15.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.9.1" class="ltx_text" style="font-size:80%;">850B tokens</span></td>
<td id="S2.T1.15.15.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.15.15.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.11.1" class="ltx_text" style="font-size:80%;">1536 Ascend 910</span></td>
<td id="S2.T1.15.15.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.12.1" class="ltx_text" style="font-size:80%;">60 d</span></td>
<td id="S2.T1.15.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.15.15.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.15.15.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.15.15.1.m1.1.1" xref="S2.T1.15.15.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.1.m1.1b"><ci id="S2.T1.15.15.1.m1.1.1.cmml" xref="S2.T1.15.15.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.15.15.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.15.15.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.16.16" class="ltx_tr">
<td id="S2.T1.16.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.16.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.16.16.3.1" class="ltx_text" style="font-size:80%;">GLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.16.16.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib93" title="" class="ltx_ref">93</a><span id="S2.T1.16.16.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.16.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.4.1" class="ltx_text" style="font-size:80%;">Oct-2022</span></td>
<td id="S2.T1.16.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.5.1" class="ltx_text" style="font-size:80%;">130</span></td>
<td id="S2.T1.16.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.16.16.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.16.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.16.16.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.9.1" class="ltx_text" style="font-size:80%;">400B tokens</span></td>
<td id="S2.T1.16.16.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.16.16.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.11.1" class="ltx_text" style="font-size:80%;">768 40G A100</span></td>
<td id="S2.T1.16.16.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.12.1" class="ltx_text" style="font-size:80%;">60 d</span></td>
<td id="S2.T1.16.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.16.16.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.16.16.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.16.16.1.m1.1.1" xref="S2.T1.16.16.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.1.m1.1b"><ci id="S2.T1.16.16.1.m1.1.1.cmml" xref="S2.T1.16.16.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.16.16.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.16.16.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.19.19" class="ltx_tr">
<td id="S2.T1.19.19.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.19.19.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.19.19.5.1" class="ltx_text" style="font-size:80%;">Flan-T5&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.19.19.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib69" title="" class="ltx_ref">69</a><span id="S2.T1.19.19.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.19.19.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.6.1" class="ltx_text" style="font-size:80%;">Oct-2022</span></td>
<td id="S2.T1.19.19.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.7.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S2.T1.19.19.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.8.1" class="ltx_text" style="font-size:80%;">T5</span></td>
<td id="S2.T1.17.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.17.17.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.17.17.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.17.17.1.m1.1.1" xref="S2.T1.17.17.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.1.m1.1b"><ci id="S2.T1.17.17.1.m1.1.1.cmml" xref="S2.T1.17.17.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.19.19.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.19.19.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.19.19.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.19.19.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.19.19.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.19.19.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.18.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.18.18.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.18.18.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.18.18.2.m1.1.1" xref="S2.T1.18.18.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.2.m1.1b"><ci id="S2.T1.18.18.2.m1.1.1.cmml" xref="S2.T1.18.18.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.19.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.19.19.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.19.19.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.19.19.3.m1.1.1" xref="S2.T1.19.19.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.19.19.3.m1.1b"><ci id="S2.T1.19.19.3.m1.1.1.cmml" xref="S2.T1.19.19.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.19.19.3.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.20.20" class="ltx_tr">
<td id="S2.T1.20.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.20.20.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.20.20.3.1" class="ltx_text" style="font-size:80%;">BLOOM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.20.20.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="S2.T1.20.20.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.20.20.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.4.1" class="ltx_text" style="font-size:80%;">Nov-2022</span></td>
<td id="S2.T1.20.20.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.5.1" class="ltx_text" style="font-size:80%;">176</span></td>
<td id="S2.T1.20.20.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.20.20.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.20.20.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.20.20.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.9.1" class="ltx_text" style="font-size:80%;">366B tokens</span></td>
<td id="S2.T1.20.20.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.20.20.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.11.1" class="ltx_text" style="font-size:80%;">384 80G A100</span></td>
<td id="S2.T1.20.20.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.12.1" class="ltx_text" style="font-size:80%;">105 d</span></td>
<td id="S2.T1.20.20.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.20.20.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.20.20.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.20.20.1.m1.1.1" xref="S2.T1.20.20.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.20.20.1.m1.1b"><ci id="S2.T1.20.20.1.m1.1.1.cmml" xref="S2.T1.20.20.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.20.20.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.20.20.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.20.20.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.22.22" class="ltx_tr">
<td id="S2.T1.22.22.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.22.22.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.22.22.4.1" class="ltx_text" style="font-size:80%;">mT0&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.22.22.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib94" title="" class="ltx_ref">94</a><span id="S2.T1.22.22.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.22.22.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.5.1" class="ltx_text" style="font-size:80%;">Nov-2022</span></td>
<td id="S2.T1.22.22.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.6.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S2.T1.22.22.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.7.1" class="ltx_text" style="font-size:80%;">mT5</span></td>
<td id="S2.T1.21.21.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.21.21.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.21.21.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.21.21.1.m1.1.1" xref="S2.T1.21.21.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.21.21.1.m1.1b"><ci id="S2.T1.21.21.1.m1.1.1.cmml" xref="S2.T1.21.21.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.21.21.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.22.22.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.22.22.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.22.22.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.22.22.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.22.22.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.22.22.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.22.22.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.22.22.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.22.22.2.m1.1.1" xref="S2.T1.22.22.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.22.22.2.m1.1b"><ci id="S2.T1.22.22.2.m1.1.1.cmml" xref="S2.T1.22.22.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.22.22.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.22.22.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.22.22.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.24.24" class="ltx_tr">
<td id="S2.T1.24.24.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.24.24.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.24.24.4.1" class="ltx_text" style="font-size:80%;">Galactica&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.24.24.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.24.24.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.24.24.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.5.1" class="ltx_text" style="font-size:80%;">Nov-2022</span></td>
<td id="S2.T1.24.24.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.6.1" class="ltx_text" style="font-size:80%;">120</span></td>
<td id="S2.T1.24.24.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.24.24.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.24.24.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.24.24.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.10.1" class="ltx_text" style="font-size:80%;">106B tokens</span></td>
<td id="S2.T1.24.24.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.24.24.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.24.24.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.24.24.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.23.23.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.23.23.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.23.23.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.23.23.1.m1.1.1" xref="S2.T1.23.23.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.23.23.1.m1.1b"><ci id="S2.T1.23.23.1.m1.1.1.cmml" xref="S2.T1.23.23.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.23.23.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.24.24.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.24.24.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.24.24.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.24.24.2.m1.1.1" xref="S2.T1.24.24.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.24.24.2.m1.1b"><ci id="S2.T1.24.24.2.m1.1.1.cmml" xref="S2.T1.24.24.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.24.24.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.26.26" class="ltx_tr">
<td id="S2.T1.26.26.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.26.26.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.26.26.4.1" class="ltx_text" style="font-size:80%;">BLOOMZ&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.26.26.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib94" title="" class="ltx_ref">94</a><span id="S2.T1.26.26.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.26.26.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.5.1" class="ltx_text" style="font-size:80%;">Nov-2022</span></td>
<td id="S2.T1.26.26.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.6.1" class="ltx_text" style="font-size:80%;">176</span></td>
<td id="S2.T1.26.26.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.7.1" class="ltx_text" style="font-size:80%;">BLOOM</span></td>
<td id="S2.T1.25.25.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.25.25.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.25.25.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.25.25.1.m1.1.1" xref="S2.T1.25.25.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.25.25.1.m1.1b"><ci id="S2.T1.25.25.1.m1.1.1.cmml" xref="S2.T1.25.25.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.25.25.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.26.26.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.26.26.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.26.26.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.26.26.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.26.26.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.26.26.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.26.26.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.26.26.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.26.26.2.m1.1.1" xref="S2.T1.26.26.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.26.26.2.m1.1b"><ci id="S2.T1.26.26.2.m1.1.1.cmml" xref="S2.T1.26.26.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.26.26.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.26.26.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.26.26.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.29.29" class="ltx_tr">
<td id="S2.T1.29.29.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.29.29.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.29.29.5.1" class="ltx_text" style="font-size:80%;">OPT-IML&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.29.29.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib95" title="" class="ltx_ref">95</a><span id="S2.T1.29.29.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.29.29.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.6.1" class="ltx_text" style="font-size:80%;">Dec-2022</span></td>
<td id="S2.T1.29.29.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.7.1" class="ltx_text" style="font-size:80%;">175</span></td>
<td id="S2.T1.29.29.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.8.1" class="ltx_text" style="font-size:80%;">OPT</span></td>
<td id="S2.T1.27.27.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.27.27.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.27.27.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.27.27.1.m1.1.1" xref="S2.T1.27.27.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.27.27.1.m1.1b"><ci id="S2.T1.27.27.1.m1.1.1.cmml" xref="S2.T1.27.27.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.27.27.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.29.29.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.29.29.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.29.29.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.29.29.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.12.1" class="ltx_text" style="font-size:80%;">128 40G A100</span></td>
<td id="S2.T1.29.29.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.29.29.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.28.28.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.28.28.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.28.28.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.28.28.2.m1.1.1" xref="S2.T1.28.28.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.28.28.2.m1.1b"><ci id="S2.T1.28.28.2.m1.1.1.cmml" xref="S2.T1.28.28.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.28.28.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.29.29.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.29.29.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.29.29.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.29.29.3.m1.1.1" xref="S2.T1.29.29.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.29.29.3.m1.1b"><ci id="S2.T1.29.29.3.m1.1.1.cmml" xref="S2.T1.29.29.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.29.29.3.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.30.30" class="ltx_tr">
<td id="S2.T1.30.30.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.30.30.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.30.30.3.1" class="ltx_text" style="font-size:80%;">LLaMA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.30.30.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S2.T1.30.30.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.30.30.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.4.1" class="ltx_text" style="font-size:80%;">Feb-2023</span></td>
<td id="S2.T1.30.30.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.5.1" class="ltx_text" style="font-size:80%;">65</span></td>
<td id="S2.T1.30.30.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.30.30.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.30.30.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.30.30.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.9.1" class="ltx_text" style="font-size:80%;">1.4T tokens</span></td>
<td id="S2.T1.30.30.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.30.30.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.11.1" class="ltx_text" style="font-size:80%;">2048 80G A100</span></td>
<td id="S2.T1.30.30.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.12.1" class="ltx_text" style="font-size:80%;">21 d</span></td>
<td id="S2.T1.30.30.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.30.30.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.30.30.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.30.30.1.m1.1.1" xref="S2.T1.30.30.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.30.30.1.m1.1b"><ci id="S2.T1.30.30.1.m1.1.1.cmml" xref="S2.T1.30.30.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.30.30.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.30.30.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.30.30.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.31.31" class="ltx_tr">
<td id="S2.T1.31.31.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.31.31.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.31.31.3.1" class="ltx_text" style="font-size:80%;">Pythia&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.31.31.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib96" title="" class="ltx_ref">96</a><span id="S2.T1.31.31.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.31.31.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.4.1" class="ltx_text" style="font-size:80%;">Apr-2023</span></td>
<td id="S2.T1.31.31.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.5.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S2.T1.31.31.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.31.31.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.31.31.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.31.31.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.9.1" class="ltx_text" style="font-size:80%;">300B tokens</span></td>
<td id="S2.T1.31.31.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.31.31.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.11.1" class="ltx_text" style="font-size:80%;">256 40G A100</span></td>
<td id="S2.T1.31.31.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.31.31.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.31.31.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.31.31.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.31.31.1.m1.1.1" xref="S2.T1.31.31.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.31.31.1.m1.1b"><ci id="S2.T1.31.31.1.m1.1.1.cmml" xref="S2.T1.31.31.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.31.31.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.31.31.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.31.31.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.32.32" class="ltx_tr">
<td id="S2.T1.32.32.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.32.32.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.32.32.3.1" class="ltx_text" style="font-size:80%;">CodeGen2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.32.32.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib97" title="" class="ltx_ref">97</a><span id="S2.T1.32.32.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.32.32.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.4.1" class="ltx_text" style="font-size:80%;">May-2023</span></td>
<td id="S2.T1.32.32.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.5.1" class="ltx_text" style="font-size:80%;">16</span></td>
<td id="S2.T1.32.32.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.32.32.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.32.32.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.32.32.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.9.1" class="ltx_text" style="font-size:80%;">400B tokens</span></td>
<td id="S2.T1.32.32.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.32.32.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.32.32.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.32.32.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.32.32.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.32.32.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.32.32.1.m1.1.1" xref="S2.T1.32.32.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.32.32.1.m1.1b"><ci id="S2.T1.32.32.1.m1.1.1.cmml" xref="S2.T1.32.32.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.32.32.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.32.32.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.32.32.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.34.34" class="ltx_tr">
<td id="S2.T1.34.34.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.34.34.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.34.34.4.1" class="ltx_text" style="font-size:80%;">StarCoder&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.34.34.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib98" title="" class="ltx_ref">98</a><span id="S2.T1.34.34.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.34.34.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.5.1" class="ltx_text" style="font-size:80%;">May-2023</span></td>
<td id="S2.T1.34.34.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.6.1" class="ltx_text" style="font-size:80%;">15.5</span></td>
<td id="S2.T1.34.34.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.34.34.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.34.34.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.34.34.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.10.1" class="ltx_text" style="font-size:80%;">1T tokens</span></td>
<td id="S2.T1.34.34.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.34.34.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.12.1" class="ltx_text" style="font-size:80%;">512 40G A100</span></td>
<td id="S2.T1.34.34.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.34.34.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.33.33.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.33.33.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.33.33.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.33.33.1.m1.1.1" xref="S2.T1.33.33.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.33.33.1.m1.1b"><ci id="S2.T1.33.33.1.m1.1.1.cmml" xref="S2.T1.33.33.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.33.33.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.34.34.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.34.34.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.34.34.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.34.34.2.m1.1.1" xref="S2.T1.34.34.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.34.34.2.m1.1b"><ci id="S2.T1.34.34.2.m1.1.1.cmml" xref="S2.T1.34.34.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.34.34.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.37.37" class="ltx_tr">
<td id="S2.T1.37.37.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.37.37.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.37.37.5.1" class="ltx_text" style="font-size:80%;">LLaMA2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.37.37.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib99" title="" class="ltx_ref">99</a><span id="S2.T1.37.37.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.37.37.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.6.1" class="ltx_text" style="font-size:80%;">Jul-2023</span></td>
<td id="S2.T1.37.37.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.7.1" class="ltx_text" style="font-size:80%;">70</span></td>
<td id="S2.T1.37.37.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.35.35.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.35.35.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.35.35.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.35.35.1.m1.1.1" xref="S2.T1.35.35.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.35.35.1.m1.1b"><ci id="S2.T1.35.35.1.m1.1.1.cmml" xref="S2.T1.35.35.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.35.35.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.36.36.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.36.36.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.36.36.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.36.36.2.m1.1.1" xref="S2.T1.36.36.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.36.36.2.m1.1b"><ci id="S2.T1.36.36.2.m1.1.1.cmml" xref="S2.T1.36.36.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.36.36.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.37.37.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.9.1" class="ltx_text" style="font-size:80%;">2T tokens</span></td>
<td id="S2.T1.37.37.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.37.37.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.11.1" class="ltx_text" style="font-size:80%;">2000 80G A100</span></td>
<td id="S2.T1.37.37.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.37.37.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.37.37.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.37.37.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.37.37.3.m1.1.1" xref="S2.T1.37.37.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.37.37.3.m1.1b"><ci id="S2.T1.37.37.3.m1.1.1.cmml" xref="S2.T1.37.37.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.37.37.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.37.37.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.37.37.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.40.40" class="ltx_tr">
<td id="S2.T1.40.40.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.40.40.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.40.40.5.1" class="ltx_text" style="font-size:80%;">Baichuan2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.40.40.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib100" title="" class="ltx_ref">100</a><span id="S2.T1.40.40.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.40.40.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.6.1" class="ltx_text" style="font-size:80%;">Sep-2023</span></td>
<td id="S2.T1.40.40.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.7.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S2.T1.40.40.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.38.38.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.38.38.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.38.38.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.38.38.1.m1.1.1" xref="S2.T1.38.38.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.38.38.1.m1.1b"><ci id="S2.T1.38.38.1.m1.1.1.cmml" xref="S2.T1.38.38.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.38.38.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.39.39.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.39.39.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.39.39.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.39.39.2.m1.1.1" xref="S2.T1.39.39.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.39.39.2.m1.1b"><ci id="S2.T1.39.39.2.m1.1.1.cmml" xref="S2.T1.39.39.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.39.39.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.40.40.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.9.1" class="ltx_text" style="font-size:80%;">2.6T tokens</span></td>
<td id="S2.T1.40.40.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.40.40.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.11.1" class="ltx_text" style="font-size:80%;">1024 A800</span></td>
<td id="S2.T1.40.40.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.40.40.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.40.40.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.40.40.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.40.40.3.m1.1.1" xref="S2.T1.40.40.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.40.40.3.m1.1b"><ci id="S2.T1.40.40.3.m1.1.1.cmml" xref="S2.T1.40.40.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.40.40.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.40.40.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.40.40.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.43.43" class="ltx_tr">
<td id="S2.T1.43.43.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.43.43.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.43.43.5.1" class="ltx_text" style="font-size:80%;">QWEN&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.43.43.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib101" title="" class="ltx_ref">101</a><span id="S2.T1.43.43.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.43.43.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.6.1" class="ltx_text" style="font-size:80%;">Sep-2023</span></td>
<td id="S2.T1.43.43.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.7.1" class="ltx_text" style="font-size:80%;">14</span></td>
<td id="S2.T1.43.43.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.41.41.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.41.41.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.41.41.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.41.41.1.m1.1.1" xref="S2.T1.41.41.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.41.41.1.m1.1b"><ci id="S2.T1.41.41.1.m1.1.1.cmml" xref="S2.T1.41.41.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.41.41.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.42.42.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.42.42.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.42.42.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.42.42.2.m1.1.1" xref="S2.T1.42.42.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.42.42.2.m1.1b"><ci id="S2.T1.42.42.2.m1.1.1.cmml" xref="S2.T1.42.42.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.42.42.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.43.43.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.9.1" class="ltx_text" style="font-size:80%;">3T tokens</span></td>
<td id="S2.T1.43.43.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.43.43.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.43.43.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.43.43.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.43.43.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.43.43.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.43.43.3.m1.1.1" xref="S2.T1.43.43.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.43.43.3.m1.1b"><ci id="S2.T1.43.43.3.m1.1.1.cmml" xref="S2.T1.43.43.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.43.43.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.43.43.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.43.43.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.45.45" class="ltx_tr">
<td id="S2.T1.45.45.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.45.45.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.45.45.4.1" class="ltx_text" style="font-size:80%;">FLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.45.45.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib102" title="" class="ltx_ref">102</a><span id="S2.T1.45.45.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.45.45.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.5.1" class="ltx_text" style="font-size:80%;">Sep-2023</span></td>
<td id="S2.T1.45.45.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.6.1" class="ltx_text" style="font-size:80%;">101</span></td>
<td id="S2.T1.45.45.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.44.44.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.44.44.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.44.44.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.44.44.1.m1.1.1" xref="S2.T1.44.44.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.44.44.1.m1.1b"><ci id="S2.T1.44.44.1.m1.1.1.cmml" xref="S2.T1.44.44.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.44.44.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.45.45.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.45.45.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.9.1" class="ltx_text" style="font-size:80%;">311B tokens</span></td>
<td id="S2.T1.45.45.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.45.45.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.11.1" class="ltx_text" style="font-size:80%;">192 A800</span></td>
<td id="S2.T1.45.45.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.12.1" class="ltx_text" style="font-size:80%;">22 d</span></td>
<td id="S2.T1.45.45.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.45.45.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.45.45.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.45.45.2.m1.1.1" xref="S2.T1.45.45.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.45.45.2.m1.1b"><ci id="S2.T1.45.45.2.m1.1.1.cmml" xref="S2.T1.45.45.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.45.45.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.45.45.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.45.45.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.46.46" class="ltx_tr">
<td id="S2.T1.46.46.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.2.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.46.46.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.46.46.2.1.1.1" class="ltx_tr">
<span id="S2.T1.46.46.2.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Publicly</span></span>
<span id="S2.T1.46.46.2.1.1.2" class="ltx_tr">
<span id="S2.T1.46.46.2.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Available</span></span>
</span></span></td>
<td id="S2.T1.46.46.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.46.46.3.1" class="ltx_text" style="font-size:80%;">Skywork&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.46.46.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib103" title="" class="ltx_ref">103</a><span id="S2.T1.46.46.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.46.46.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.4.1" class="ltx_text" style="font-size:80%;">Oct-2023</span></td>
<td id="S2.T1.46.46.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.5.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S2.T1.46.46.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.46.46.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.46.46.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.46.46.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.9.1" class="ltx_text" style="font-size:80%;">3.2T tokens</span></td>
<td id="S2.T1.46.46.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.46.46.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.11.1" class="ltx_text" style="font-size:80%;">512 80G A800</span></td>
<td id="S2.T1.46.46.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.46.46.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.46.46.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.46.46.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.46.46.1.m1.1.1" xref="S2.T1.46.46.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.46.46.1.m1.1b"><ci id="S2.T1.46.46.1.m1.1.1.cmml" xref="S2.T1.46.46.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.46.46.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.46.46.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.46.46.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.47.47" class="ltx_tr">
<td id="S2.T1.47.47.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.47.47.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.47.47.3.1" class="ltx_text" style="font-size:80%;">GPT-3&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.47.47.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S2.T1.47.47.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.47.47.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.4.1" class="ltx_text" style="font-size:80%;">May-2020</span></td>
<td id="S2.T1.47.47.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.5.1" class="ltx_text" style="font-size:80%;">175</span></td>
<td id="S2.T1.47.47.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.47.47.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.47.47.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.47.47.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.9.1" class="ltx_text" style="font-size:80%;">300B tokens</span></td>
<td id="S2.T1.47.47.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.47.47.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.47.47.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.47.47.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.47.47.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.47.47.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.47.47.1.m1.1.1" xref="S2.T1.47.47.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.47.47.1.m1.1b"><ci id="S2.T1.47.47.1.m1.1.1.cmml" xref="S2.T1.47.47.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.47.47.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.47.47.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.47.47.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.90.94" class="ltx_tr">
<td id="S2.T1.90.94.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.94.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.90.94.2.1" class="ltx_text" style="font-size:80%;">GShard&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.90.94.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib104" title="" class="ltx_ref">104</a><span id="S2.T1.90.94.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.90.94.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.3.1" class="ltx_text" style="font-size:80%;">Jun-2020</span></td>
<td id="S2.T1.90.94.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.4.1" class="ltx_text" style="font-size:80%;">600</span></td>
<td id="S2.T1.90.94.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.94.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.94.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.94.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.8.1" class="ltx_text" style="font-size:80%;">1T tokens</span></td>
<td id="S2.T1.90.94.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.94.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.10.1" class="ltx_text" style="font-size:80%;">2048 TPU v3</span></td>
<td id="S2.T1.90.94.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.11.1" class="ltx_text" style="font-size:80%;">4 d</span></td>
<td id="S2.T1.90.94.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.94.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.94.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.48.48" class="ltx_tr">
<td id="S2.T1.48.48.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.48.48.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.48.48.3.1" class="ltx_text" style="font-size:80%;">Codex&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.48.48.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib105" title="" class="ltx_ref">105</a><span id="S2.T1.48.48.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.48.48.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.4.1" class="ltx_text" style="font-size:80%;">Jul-2021</span></td>
<td id="S2.T1.48.48.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.5.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S2.T1.48.48.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.6.1" class="ltx_text" style="font-size:80%;">GPT-3</span></td>
<td id="S2.T1.48.48.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.48.48.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.48.48.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.9.1" class="ltx_text" style="font-size:80%;">100B tokens</span></td>
<td id="S2.T1.48.48.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.10.1" class="ltx_text" style="font-size:80%;">May-2020</span></td>
<td id="S2.T1.48.48.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.48.48.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.48.48.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.48.48.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.48.48.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.48.48.1.m1.1.1" xref="S2.T1.48.48.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.48.48.1.m1.1b"><ci id="S2.T1.48.48.1.m1.1.1.cmml" xref="S2.T1.48.48.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.48.48.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.48.48.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.48.48.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.49.49" class="ltx_tr">
<td id="S2.T1.49.49.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.49.49.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.49.49.3.1" class="ltx_text" style="font-size:80%;">ERNIE 3.0&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.49.49.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib106" title="" class="ltx_ref">106</a><span id="S2.T1.49.49.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.49.49.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.4.1" class="ltx_text" style="font-size:80%;">Jul-2021</span></td>
<td id="S2.T1.49.49.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.5.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S2.T1.49.49.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.49.49.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.49.49.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.49.49.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.9.1" class="ltx_text" style="font-size:80%;">375B tokens</span></td>
<td id="S2.T1.49.49.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.49.49.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.11.1" class="ltx_text" style="font-size:80%;">384 V100</span></td>
<td id="S2.T1.49.49.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.49.49.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.49.49.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.49.49.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.49.49.1.m1.1.1" xref="S2.T1.49.49.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.49.49.1.m1.1b"><ci id="S2.T1.49.49.1.m1.1.1.cmml" xref="S2.T1.49.49.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.49.49.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.49.49.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.49.49.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.50.50" class="ltx_tr">
<td id="S2.T1.50.50.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.50.50.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.50.50.3.1" class="ltx_text" style="font-size:80%;">Jurassic-1&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.50.50.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib107" title="" class="ltx_ref">107</a><span id="S2.T1.50.50.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.50.50.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.4.1" class="ltx_text" style="font-size:80%;">Aug-2021</span></td>
<td id="S2.T1.50.50.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.5.1" class="ltx_text" style="font-size:80%;">178</span></td>
<td id="S2.T1.50.50.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.50.50.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.50.50.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.50.50.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.9.1" class="ltx_text" style="font-size:80%;">300B tokens</span></td>
<td id="S2.T1.50.50.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.50.50.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.11.1" class="ltx_text" style="font-size:80%;">800 GPU</span></td>
<td id="S2.T1.50.50.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.50.50.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.50.50.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.50.50.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.50.50.1.m1.1.1" xref="S2.T1.50.50.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.50.50.1.m1.1b"><ci id="S2.T1.50.50.1.m1.1.1.cmml" xref="S2.T1.50.50.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.50.50.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.50.50.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.50.50.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.51.51" class="ltx_tr">
<td id="S2.T1.51.51.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.51.51.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.51.51.3.1" class="ltx_text" style="font-size:80%;">HyperCLOVA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.51.51.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib108" title="" class="ltx_ref">108</a><span id="S2.T1.51.51.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.51.51.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.4.1" class="ltx_text" style="font-size:80%;">Sep-2021</span></td>
<td id="S2.T1.51.51.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.5.1" class="ltx_text" style="font-size:80%;">82</span></td>
<td id="S2.T1.51.51.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.51.51.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.51.51.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.51.51.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.9.1" class="ltx_text" style="font-size:80%;">300B tokens</span></td>
<td id="S2.T1.51.51.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.51.51.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.11.1" class="ltx_text" style="font-size:80%;">1024 A100</span></td>
<td id="S2.T1.51.51.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.12.1" class="ltx_text" style="font-size:80%;">13.4 d</span></td>
<td id="S2.T1.51.51.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.51.51.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.51.51.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.51.51.1.m1.1.1" xref="S2.T1.51.51.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.51.51.1.m1.1b"><ci id="S2.T1.51.51.1.m1.1.1.cmml" xref="S2.T1.51.51.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.51.51.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.51.51.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.51.51.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.53.53" class="ltx_tr">
<td id="S2.T1.53.53.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.53.53.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.53.53.4.1" class="ltx_text" style="font-size:80%;">FLAN&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.53.53.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S2.T1.53.53.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.53.53.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.5.1" class="ltx_text" style="font-size:80%;">Sep-2021</span></td>
<td id="S2.T1.53.53.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.6.1" class="ltx_text" style="font-size:80%;">137</span></td>
<td id="S2.T1.53.53.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.7.1" class="ltx_text" style="font-size:80%;">LaMDA-PT</span></td>
<td id="S2.T1.52.52.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.52.52.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.52.52.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.52.52.1.m1.1.1" xref="S2.T1.52.52.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.52.52.1.m1.1b"><ci id="S2.T1.52.52.1.m1.1.1.cmml" xref="S2.T1.52.52.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.52.52.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.53.53.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.53.53.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.53.53.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.53.53.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.11.1" class="ltx_text" style="font-size:80%;">128 TPU v3</span></td>
<td id="S2.T1.53.53.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.12.1" class="ltx_text" style="font-size:80%;">60 h</span></td>
<td id="S2.T1.53.53.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.53.53.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.53.53.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.53.53.2.m1.1.1" xref="S2.T1.53.53.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.53.53.2.m1.1b"><ci id="S2.T1.53.53.2.m1.1.1.cmml" xref="S2.T1.53.53.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.53.53.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.53.53.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.53.53.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.54.54" class="ltx_tr">
<td id="S2.T1.54.54.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.54.54.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.54.54.3.1" class="ltx_text" style="font-size:80%;">Yuan 1.0&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.54.54.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib109" title="" class="ltx_ref">109</a><span id="S2.T1.54.54.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.54.54.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.4.1" class="ltx_text" style="font-size:80%;">Oct-2021</span></td>
<td id="S2.T1.54.54.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.5.1" class="ltx_text" style="font-size:80%;">245</span></td>
<td id="S2.T1.54.54.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.54.54.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.54.54.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.54.54.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.9.1" class="ltx_text" style="font-size:80%;">180B tokens</span></td>
<td id="S2.T1.54.54.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.54.54.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.11.1" class="ltx_text" style="font-size:80%;">2128 GPU</span></td>
<td id="S2.T1.54.54.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.54.54.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.54.54.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.54.54.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.54.54.1.m1.1.1" xref="S2.T1.54.54.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.54.54.1.m1.1b"><ci id="S2.T1.54.54.1.m1.1.1.cmml" xref="S2.T1.54.54.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.54.54.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.54.54.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.54.54.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.55.55" class="ltx_tr">
<td id="S2.T1.55.55.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.55.55.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.55.55.3.1" class="ltx_text" style="font-size:80%;">Anthropic&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.55.55.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib110" title="" class="ltx_ref">110</a><span id="S2.T1.55.55.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.55.55.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.4.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S2.T1.55.55.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.5.1" class="ltx_text" style="font-size:80%;">52</span></td>
<td id="S2.T1.55.55.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.55.55.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.55.55.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.55.55.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.9.1" class="ltx_text" style="font-size:80%;">400B tokens</span></td>
<td id="S2.T1.55.55.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.55.55.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.55.55.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.55.55.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.55.55.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.55.55.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.55.55.1.m1.1.1" xref="S2.T1.55.55.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.55.55.1.m1.1b"><ci id="S2.T1.55.55.1.m1.1.1.cmml" xref="S2.T1.55.55.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.55.55.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.55.55.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.55.55.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.57.57" class="ltx_tr">
<td id="S2.T1.57.57.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.57.57.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.57.57.4.1" class="ltx_text" style="font-size:80%;">WebGPT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.57.57.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="S2.T1.57.57.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.57.57.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.5.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S2.T1.57.57.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.6.1" class="ltx_text" style="font-size:80%;">175</span></td>
<td id="S2.T1.57.57.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.7.1" class="ltx_text" style="font-size:80%;">GPT-3</span></td>
<td id="S2.T1.57.57.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.56.56.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.56.56.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.56.56.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.56.56.1.m1.1.1" xref="S2.T1.56.56.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.56.56.1.m1.1b"><ci id="S2.T1.56.56.1.m1.1.1.cmml" xref="S2.T1.56.56.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.56.56.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.57.57.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.57.57.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.57.57.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.57.57.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.57.57.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.57.57.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.57.57.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.57.57.2.m1.1.1" xref="S2.T1.57.57.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.57.57.2.m1.1b"><ci id="S2.T1.57.57.2.m1.1.1.cmml" xref="S2.T1.57.57.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.57.57.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.57.57.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.57.57.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.58.58" class="ltx_tr">
<td id="S2.T1.58.58.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.58.58.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.58.58.3.1" class="ltx_text" style="font-size:80%;">Gopher&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.58.58.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib64" title="" class="ltx_ref">64</a><span id="S2.T1.58.58.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.58.58.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.4.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S2.T1.58.58.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.5.1" class="ltx_text" style="font-size:80%;">280</span></td>
<td id="S2.T1.58.58.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.58.58.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.58.58.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.58.58.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.9.1" class="ltx_text" style="font-size:80%;">300B tokens</span></td>
<td id="S2.T1.58.58.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.58.58.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.11.1" class="ltx_text" style="font-size:80%;">4096 TPU v3</span></td>
<td id="S2.T1.58.58.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.12.1" class="ltx_text" style="font-size:80%;">920 h</span></td>
<td id="S2.T1.58.58.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.58.58.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.58.58.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.58.58.1.m1.1.1" xref="S2.T1.58.58.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.58.58.1.m1.1b"><ci id="S2.T1.58.58.1.m1.1.1.cmml" xref="S2.T1.58.58.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.58.58.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.58.58.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.58.58.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.59.59" class="ltx_tr">
<td id="S2.T1.59.59.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.59.59.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.59.59.3.1" class="ltx_text" style="font-size:80%;">ERNIE 3.0 Titan&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.59.59.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib111" title="" class="ltx_ref">111</a><span id="S2.T1.59.59.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.59.59.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.4.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S2.T1.59.59.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.5.1" class="ltx_text" style="font-size:80%;">260</span></td>
<td id="S2.T1.59.59.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.59.59.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.59.59.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.59.59.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.59.59.1.m1.1.1" xref="S2.T1.59.59.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.59.59.1.m1.1b"><ci id="S2.T1.59.59.1.m1.1.1.cmml" xref="S2.T1.59.59.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.59.59.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.59.59.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.59.59.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.60.60" class="ltx_tr">
<td id="S2.T1.60.60.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.60.60.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.60.60.3.1" class="ltx_text" style="font-size:80%;">GLaM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.60.60.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib112" title="" class="ltx_ref">112</a><span id="S2.T1.60.60.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.60.60.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.4.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S2.T1.60.60.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.5.1" class="ltx_text" style="font-size:80%;">1200</span></td>
<td id="S2.T1.60.60.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.60.60.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.60.60.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.60.60.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.9.1" class="ltx_text" style="font-size:80%;">280B tokens</span></td>
<td id="S2.T1.60.60.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.60.60.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.11.1" class="ltx_text" style="font-size:80%;">1024 TPU v4</span></td>
<td id="S2.T1.60.60.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.12.1" class="ltx_text" style="font-size:80%;">574 h</span></td>
<td id="S2.T1.60.60.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.60.60.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.60.60.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.60.60.1.m1.1.1" xref="S2.T1.60.60.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.60.60.1.m1.1b"><ci id="S2.T1.60.60.1.m1.1.1.cmml" xref="S2.T1.60.60.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.60.60.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.60.60.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.60.60.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.90.95" class="ltx_tr">
<td id="S2.T1.90.95.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.95.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.90.95.2.1" class="ltx_text" style="font-size:80%;">LaMDA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.90.95.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib68" title="" class="ltx_ref">68</a><span id="S2.T1.90.95.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.90.95.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.3.1" class="ltx_text" style="font-size:80%;">Jan-2022</span></td>
<td id="S2.T1.90.95.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.4.1" class="ltx_text" style="font-size:80%;">137</span></td>
<td id="S2.T1.90.95.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.95.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.95.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.95.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.8.1" class="ltx_text" style="font-size:80%;">768B tokens</span></td>
<td id="S2.T1.90.95.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.95.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.10.1" class="ltx_text" style="font-size:80%;">1024 TPU v3</span></td>
<td id="S2.T1.90.95.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.11.1" class="ltx_text" style="font-size:80%;">57.7 d</span></td>
<td id="S2.T1.90.95.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.95.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.95.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.61.61" class="ltx_tr">
<td id="S2.T1.61.61.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.61.61.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.61.61.3.1" class="ltx_text" style="font-size:80%;">MT-NLG&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.61.61.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib113" title="" class="ltx_ref">113</a><span id="S2.T1.61.61.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.61.61.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.4.1" class="ltx_text" style="font-size:80%;">Jan-2022</span></td>
<td id="S2.T1.61.61.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.5.1" class="ltx_text" style="font-size:80%;">530</span></td>
<td id="S2.T1.61.61.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.61.61.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.61.61.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.61.61.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.9.1" class="ltx_text" style="font-size:80%;">270B tokens</span></td>
<td id="S2.T1.61.61.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.61.61.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.11.1" class="ltx_text" style="font-size:80%;">4480 80G A100</span></td>
<td id="S2.T1.61.61.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.61.61.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.61.61.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.61.61.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.61.61.1.m1.1.1" xref="S2.T1.61.61.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.61.61.1.m1.1b"><ci id="S2.T1.61.61.1.m1.1.1.cmml" xref="S2.T1.61.61.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.61.61.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.61.61.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.61.61.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.90.96" class="ltx_tr">
<td id="S2.T1.90.96.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.90.96.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.90.96.2.1" class="ltx_text" style="font-size:80%;">AlphaCode&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.90.96.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib114" title="" class="ltx_ref">114</a><span id="S2.T1.90.96.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.90.96.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.3.1" class="ltx_text" style="font-size:80%;">Feb-2022</span></td>
<td id="S2.T1.90.96.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.4.1" class="ltx_text" style="font-size:80%;">41</span></td>
<td id="S2.T1.90.96.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.96.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.96.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.96.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.8.1" class="ltx_text" style="font-size:80%;">967B tokens</span></td>
<td id="S2.T1.90.96.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.9.1" class="ltx_text" style="font-size:80%;">Jul-2021</span></td>
<td id="S2.T1.90.96.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.96.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.96.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.96.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.96.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.64.64" class="ltx_tr">
<td id="S2.T1.64.64.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.64.64.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.64.64.5.1" class="ltx_text" style="font-size:80%;">InstructGPT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.64.64.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib66" title="" class="ltx_ref">66</a><span id="S2.T1.64.64.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.64.64.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.6.1" class="ltx_text" style="font-size:80%;">Mar-2022</span></td>
<td id="S2.T1.64.64.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.7.1" class="ltx_text" style="font-size:80%;">175</span></td>
<td id="S2.T1.64.64.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.8.1" class="ltx_text" style="font-size:80%;">GPT-3</span></td>
<td id="S2.T1.62.62.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.62.62.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.62.62.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.62.62.1.m1.1.1" xref="S2.T1.62.62.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.62.62.1.m1.1b"><ci id="S2.T1.62.62.1.m1.1.1.cmml" xref="S2.T1.62.62.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.62.62.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.63.63.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.63.63.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.63.63.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.63.63.2.m1.1.1" xref="S2.T1.63.63.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.63.63.2.m1.1b"><ci id="S2.T1.63.63.2.m1.1.1.cmml" xref="S2.T1.63.63.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.63.63.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.64.64.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.64.64.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.64.64.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.64.64.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.64.64.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.64.64.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.64.64.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.64.64.3.m1.1.1" xref="S2.T1.64.64.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.64.64.3.m1.1b"><ci id="S2.T1.64.64.3.m1.1.1.cmml" xref="S2.T1.64.64.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.64.64.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.64.64.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.64.64.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.65.65" class="ltx_tr">
<td id="S2.T1.65.65.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.65.65.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.65.65.3.1" class="ltx_text" style="font-size:80%;">Chinchilla&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.65.65.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S2.T1.65.65.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.65.65.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.4.1" class="ltx_text" style="font-size:80%;">Mar-2022</span></td>
<td id="S2.T1.65.65.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.5.1" class="ltx_text" style="font-size:80%;">70</span></td>
<td id="S2.T1.65.65.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.65.65.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.65.65.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.65.65.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.9.1" class="ltx_text" style="font-size:80%;">1.4T tokens</span></td>
<td id="S2.T1.65.65.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.65.65.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.65.65.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.65.65.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.65.65.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.65.65.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.65.65.1.m1.1.1" xref="S2.T1.65.65.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.65.65.1.m1.1b"><ci id="S2.T1.65.65.1.m1.1.1.cmml" xref="S2.T1.65.65.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.65.65.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.65.65.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.65.65.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.67.67" class="ltx_tr">
<td id="S2.T1.67.67.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.67.67.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.67.67.4.1" class="ltx_text" style="font-size:80%;">PaLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.67.67.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S2.T1.67.67.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.67.67.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.5.1" class="ltx_text" style="font-size:80%;">Apr-2022</span></td>
<td id="S2.T1.67.67.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.6.1" class="ltx_text" style="font-size:80%;">540</span></td>
<td id="S2.T1.67.67.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.67.67.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.67.67.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.67.67.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.10.1" class="ltx_text" style="font-size:80%;">780B tokens</span></td>
<td id="S2.T1.67.67.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.67.67.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.12.1" class="ltx_text" style="font-size:80%;">6144 TPU v4</span></td>
<td id="S2.T1.67.67.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.67.67.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.66.66.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.66.66.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.66.66.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.66.66.1.m1.1.1" xref="S2.T1.66.66.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.66.66.1.m1.1b"><ci id="S2.T1.66.66.1.m1.1.1.cmml" xref="S2.T1.66.66.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.66.66.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.67.67.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.67.67.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.67.67.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.67.67.2.m1.1.1" xref="S2.T1.67.67.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.67.67.2.m1.1b"><ci id="S2.T1.67.67.2.m1.1.1.cmml" xref="S2.T1.67.67.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.67.67.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.69.69" class="ltx_tr">
<td id="S2.T1.69.69.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.69.69.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.69.69.4.1" class="ltx_text" style="font-size:80%;">AlexaTM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.69.69.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib115" title="" class="ltx_ref">115</a><span id="S2.T1.69.69.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.69.69.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.5.1" class="ltx_text" style="font-size:80%;">Aug-2022</span></td>
<td id="S2.T1.69.69.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.6.1" class="ltx_text" style="font-size:80%;">20</span></td>
<td id="S2.T1.69.69.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.69.69.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.69.69.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.69.69.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.10.1" class="ltx_text" style="font-size:80%;">1.3T tokens</span></td>
<td id="S2.T1.69.69.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.69.69.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.12.1" class="ltx_text" style="font-size:80%;">128 A100</span></td>
<td id="S2.T1.69.69.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.69.69.13.1" class="ltx_text" style="font-size:80%;">120 d</span></td>
<td id="S2.T1.68.68.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.68.68.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.68.68.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.68.68.1.m1.1.1" xref="S2.T1.68.68.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.68.68.1.m1.1b"><ci id="S2.T1.68.68.1.m1.1.1.cmml" xref="S2.T1.68.68.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.68.68.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.69.69.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.69.69.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.69.69.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.69.69.2.m1.1.1" xref="S2.T1.69.69.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.69.69.2.m1.1b"><ci id="S2.T1.69.69.2.m1.1.1.cmml" xref="S2.T1.69.69.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.69.69.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.71.71" class="ltx_tr">
<td id="S2.T1.71.71.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.71.71.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.71.71.4.1" class="ltx_text" style="font-size:80%;">Sparrow&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.71.71.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib116" title="" class="ltx_ref">116</a><span id="S2.T1.71.71.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.71.71.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.5.1" class="ltx_text" style="font-size:80%;">Sep-2022</span></td>
<td id="S2.T1.71.71.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.6.1" class="ltx_text" style="font-size:80%;">70</span></td>
<td id="S2.T1.71.71.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.71.71.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.70.70.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.70.70.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.70.70.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.70.70.1.m1.1.1" xref="S2.T1.70.70.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.70.70.1.m1.1b"><ci id="S2.T1.70.70.1.m1.1.1.cmml" xref="S2.T1.70.70.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.70.70.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.71.71.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.71.71.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.71.71.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.11.1" class="ltx_text" style="font-size:80%;">64 TPU v3</span></td>
<td id="S2.T1.71.71.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.71.71.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.71.71.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.71.71.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.71.71.2.m1.1.1" xref="S2.T1.71.71.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.71.71.2.m1.1b"><ci id="S2.T1.71.71.2.m1.1.1.cmml" xref="S2.T1.71.71.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.71.71.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.71.71.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.71.71.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.72.72" class="ltx_tr">
<td id="S2.T1.72.72.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.72.72.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.72.72.3.1" class="ltx_text" style="font-size:80%;">WeLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.72.72.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib117" title="" class="ltx_ref">117</a><span id="S2.T1.72.72.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.72.72.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.4.1" class="ltx_text" style="font-size:80%;">Sep-2022</span></td>
<td id="S2.T1.72.72.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.5.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S2.T1.72.72.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.72.72.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.72.72.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.72.72.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.9.1" class="ltx_text" style="font-size:80%;">300B tokens</span></td>
<td id="S2.T1.72.72.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.72.72.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.11.1" class="ltx_text" style="font-size:80%;">128 A100 40G</span></td>
<td id="S2.T1.72.72.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.12.1" class="ltx_text" style="font-size:80%;">24 d</span></td>
<td id="S2.T1.72.72.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.72.72.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.72.72.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.72.72.1.m1.1.1" xref="S2.T1.72.72.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.72.72.1.m1.1b"><ci id="S2.T1.72.72.1.m1.1.1.cmml" xref="S2.T1.72.72.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.72.72.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.72.72.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.72.72.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.74.74" class="ltx_tr">
<td id="S2.T1.74.74.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.74.74.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.74.74.4.1" class="ltx_text" style="font-size:80%;">U-PaLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.74.74.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib118" title="" class="ltx_ref">118</a><span id="S2.T1.74.74.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.74.74.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.5.1" class="ltx_text" style="font-size:80%;">Oct-2022</span></td>
<td id="S2.T1.74.74.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.6.1" class="ltx_text" style="font-size:80%;">540</span></td>
<td id="S2.T1.74.74.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.7.1" class="ltx_text" style="font-size:80%;">PaLM</span></td>
<td id="S2.T1.74.74.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.74.74.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.74.74.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.74.74.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.74.74.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.12.1" class="ltx_text" style="font-size:80%;">512 TPU v4</span></td>
<td id="S2.T1.74.74.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.74.74.13.1" class="ltx_text" style="font-size:80%;">5 d</span></td>
<td id="S2.T1.73.73.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.73.73.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.73.73.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.73.73.1.m1.1.1" xref="S2.T1.73.73.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.73.73.1.m1.1b"><ci id="S2.T1.73.73.1.m1.1.1.cmml" xref="S2.T1.73.73.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.73.73.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.74.74.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.74.74.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.74.74.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.74.74.2.m1.1.1" xref="S2.T1.74.74.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.74.74.2.m1.1b"><ci id="S2.T1.74.74.2.m1.1.1.cmml" xref="S2.T1.74.74.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.74.74.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.77.77" class="ltx_tr">
<td id="S2.T1.77.77.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.77.77.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.77.77.5.1" class="ltx_text" style="font-size:80%;">Flan-PaLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.77.77.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib69" title="" class="ltx_ref">69</a><span id="S2.T1.77.77.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.77.77.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.6.1" class="ltx_text" style="font-size:80%;">Oct-2022</span></td>
<td id="S2.T1.77.77.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.7.1" class="ltx_text" style="font-size:80%;">540</span></td>
<td id="S2.T1.77.77.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.8.1" class="ltx_text" style="font-size:80%;">PaLM</span></td>
<td id="S2.T1.75.75.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.75.75.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.75.75.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.75.75.1.m1.1.1" xref="S2.T1.75.75.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.75.75.1.m1.1b"><ci id="S2.T1.75.75.1.m1.1.1.cmml" xref="S2.T1.75.75.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.75.75.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.77.77.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.77.77.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.77.77.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.77.77.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.12.1" class="ltx_text" style="font-size:80%;">512 TPU v4</span></td>
<td id="S2.T1.77.77.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.77.77.13.1" class="ltx_text" style="font-size:80%;">37 h</span></td>
<td id="S2.T1.76.76.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.76.76.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.76.76.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.76.76.2.m1.1.1" xref="S2.T1.76.76.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.76.76.2.m1.1b"><ci id="S2.T1.76.76.2.m1.1.1.cmml" xref="S2.T1.76.76.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.76.76.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.77.77.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.77.77.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.77.77.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.77.77.3.m1.1.1" xref="S2.T1.77.77.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.77.77.3.m1.1b"><ci id="S2.T1.77.77.3.m1.1.1.cmml" xref="S2.T1.77.77.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.77.77.3.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.80.80" class="ltx_tr">
<td id="S2.T1.80.80.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.80.80.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.80.80.5.1" class="ltx_text" style="font-size:80%;">Flan-U-PaLM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.80.80.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib69" title="" class="ltx_ref">69</a><span id="S2.T1.80.80.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.80.80.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.6.1" class="ltx_text" style="font-size:80%;">Oct-2022</span></td>
<td id="S2.T1.80.80.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.7.1" class="ltx_text" style="font-size:80%;">540</span></td>
<td id="S2.T1.80.80.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.8.1" class="ltx_text" style="font-size:80%;">U-PaLM</span></td>
<td id="S2.T1.78.78.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.78.78.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.78.78.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.78.78.1.m1.1.1" xref="S2.T1.78.78.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.78.78.1.m1.1b"><ci id="S2.T1.78.78.1.m1.1.1.cmml" xref="S2.T1.78.78.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.78.78.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.80.80.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.80.80.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.80.80.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.80.80.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.80.80.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.80.80.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.79.79.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.79.79.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.79.79.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.79.79.2.m1.1.1" xref="S2.T1.79.79.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.79.79.2.m1.1b"><ci id="S2.T1.79.79.2.m1.1.1.cmml" xref="S2.T1.79.79.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.79.79.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.80.80.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.80.80.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.80.80.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.80.80.3.m1.1.1" xref="S2.T1.80.80.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.80.80.3.m1.1b"><ci id="S2.T1.80.80.3.m1.1.1.cmml" xref="S2.T1.80.80.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.80.80.3.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.84.84" class="ltx_tr">
<td id="S2.T1.84.84.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.84.84.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.84.84.6.1" class="ltx_text" style="font-size:80%;">GPT-4&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.84.84.6.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S2.T1.84.84.6.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.84.84.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.7.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
<td id="S2.T1.84.84.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.84.84.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.81.81.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.81.81.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.81.81.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.81.81.1.m1.1.1" xref="S2.T1.81.81.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.81.81.1.m1.1b"><ci id="S2.T1.81.81.1.m1.1.1.cmml" xref="S2.T1.81.81.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.81.81.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.82.82.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.82.82.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.82.82.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.82.82.2.m1.1.1" xref="S2.T1.82.82.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.82.82.2.m1.1b"><ci id="S2.T1.82.82.2.m1.1.1.cmml" xref="S2.T1.82.82.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.82.82.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.84.84.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.84.84.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.84.84.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.84.84.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.84.84.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.83.83.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.83.83.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.83.83.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.83.83.3.m1.1.1" xref="S2.T1.83.83.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.83.83.3.m1.1b"><ci id="S2.T1.83.83.3.m1.1.1.cmml" xref="S2.T1.83.83.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.83.83.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.84.84.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.84.84.4.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.84.84.4.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.84.84.4.m1.1.1" xref="S2.T1.84.84.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.84.84.4.m1.1b"><ci id="S2.T1.84.84.4.m1.1.1.cmml" xref="S2.T1.84.84.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.84.84.4.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.87.87" class="ltx_tr">
<td id="S2.T1.87.87.4" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S2.T1.85.85.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.85.85.1.1" class="ltx_text" style="font-size:80%;">PanGu-</span><math id="S2.T1.85.85.1.m1.1" class="ltx_Math" alttext="\Sigma" display="inline"><semantics id="S2.T1.85.85.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.85.85.1.m1.1.1" xref="S2.T1.85.85.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S2.T1.85.85.1.m1.1b"><ci id="S2.T1.85.85.1.m1.1.1.cmml" xref="S2.T1.85.85.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.85.85.1.m1.1c">\Sigma</annotation></semantics></math><span id="S2.T1.85.85.1.2" class="ltx_text" style="font-size:80%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.85.85.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib119" title="" class="ltx_ref">119</a><span id="S2.T1.85.85.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.87.87.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.5.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
<td id="S2.T1.87.87.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.6.1" class="ltx_text" style="font-size:80%;">1085</span></td>
<td id="S2.T1.86.86.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.86.86.2.1" class="ltx_text" style="font-size:80%;">PanGu-</span><math id="S2.T1.86.86.2.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.T1.86.86.2.m1.1a"><mi mathsize="80%" id="S2.T1.86.86.2.m1.1.1" xref="S2.T1.86.86.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.T1.86.86.2.m1.1b"><ci id="S2.T1.86.86.2.m1.1.1.cmml" xref="S2.T1.86.86.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.86.86.2.m1.1c">\alpha</annotation></semantics></math>
</td>
<td id="S2.T1.87.87.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.87.87.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.87.87.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.9.1" class="ltx_text" style="font-size:80%;">329B tokens</span></td>
<td id="S2.T1.87.87.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.10.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.87.87.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.11.1" class="ltx_text" style="font-size:80%;">512 Ascend 910</span></td>
<td id="S2.T1.87.87.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.12.1" class="ltx_text" style="font-size:80%;">100 d</span></td>
<td id="S2.T1.87.87.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.87.87.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.87.87.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.87.87.3.m1.1.1" xref="S2.T1.87.87.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.87.87.3.m1.1b"><ci id="S2.T1.87.87.3.m1.1.1.cmml" xref="S2.T1.87.87.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.87.87.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.87.87.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.87.87.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S2.T1.90.90" class="ltx_tr">
<td id="S2.T1.90.90.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.4.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.90.90.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.90.90.4.1.1.1" class="ltx_tr">
<span id="S2.T1.90.90.4.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Closed</span></span>
<span id="S2.T1.90.90.4.1.1.2" class="ltx_tr">
<span id="S2.T1.90.90.4.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Source</span></span>
</span></span></td>
<td id="S2.T1.90.90.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S2.T1.90.90.5.1" class="ltx_text" style="font-size:80%;">PaLM2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.90.90.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib120" title="" class="ltx_ref">120</a><span id="S2.T1.90.90.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S2.T1.90.90.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.6.1" class="ltx_text" style="font-size:80%;">May-2023</span></td>
<td id="S2.T1.90.90.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.7.1" class="ltx_text" style="font-size:80%;">16</span></td>
<td id="S2.T1.90.90.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.8.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.88.88.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.88.88.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.88.88.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.88.88.1.m1.1.1" xref="S2.T1.88.88.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.88.88.1.m1.1b"><ci id="S2.T1.88.88.1.m1.1.1.cmml" xref="S2.T1.88.88.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.88.88.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.90.90.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.90.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.10.1" class="ltx_text" style="font-size:80%;">100B tokens</span></td>
<td id="S2.T1.90.90.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.11.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.90.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.12.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.90.90.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S2.T1.90.90.13.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S2.T1.89.89.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.89.89.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.89.89.2.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.89.89.2.m1.1.1" xref="S2.T1.89.89.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.89.89.2.m1.1b"><ci id="S2.T1.89.89.2.m1.1.1.cmml" xref="S2.T1.89.89.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.89.89.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S2.T1.90.90.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><math id="S2.T1.90.90.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S2.T1.90.90.3.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.90.90.3.m1.1.1" xref="S2.T1.90.90.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S2.T1.90.90.3.m1.1b"><ci id="S2.T1.90.90.3.m1.1.1.cmml" xref="S2.T1.90.90.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.90.90.3.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
</tbody></table>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2303.18223/assets/x5.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A brief illustration for the technical evolution of GPT-series models. We plot this figure mainly based on the papers, blog articles and official APIs from OpenAI. Here, <em id="S2.F4.4.1" class="ltx_emph ltx_font_italic">solid lines</em> denote that there exists an explicit evidence (<em id="S2.F4.5.2" class="ltx_emph ltx_font_italic">e.g.,</em> the official statement that a new model is developed based on a base model) on the evolution path between two models, while <em id="S2.F4.6.3" class="ltx_emph ltx_font_italic">dashed lines</em> denote a relatively weaker evolution relation. </figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Technical Evolution of GPT-series Models</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Due to the excellent capacity in communicating with humans, ChatGPT has ignited the excitement of the AI community since its release.
ChatGPT is developed based on the powerful GPT model with specially optimized conversation capacities.
Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure&nbsp;<a href="#S2.F4" title="Figure 4 ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The basic principle underlying GPT models is to compress the world knowledge into the decoder-only Transformer model by language modeling, such that it can recover (or memorize) the semantics of world knowledge and serve as a general-purpose task solver. Two key points to the success are (I) training decoder-only Transformer language models that can <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">accurately predict the next word</em> and (II) <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">scaling up the size of language models</em>.
Overall, the research of OpenAI on LLMs can be roughly divided into the following stages<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Note that the discussion of this part can be somewhat subjective. The overall viewpoints and summaries are made based on the understanding of the survey authors by reading the papers, blog articles, interview reports and APIs released by OpenAI. </span></span></span>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Early Explorations</span>. According to one interview with Ilya Sutskever<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a target="_blank" href="https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai</a></span></span></span> (a co-founder and chief scientist of OpenAI), the idea of approaching intelligent systems with language models was already explored in the early days of OpenAI, while it was attempted with recurrent neural networks&nbsp;(RNN)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. With the advent of Transformer, OpenAI developed two initial GPT models, namely GPT-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> and GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which can be considered as the foundation to more powerful models subsequently <em id="S2.SS2.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> GPT-3 and GPT-4.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mo id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">GPT-1</em>. In 2017, the Transformer model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> was introduced by Google, and the OpenAI team quickly adapted their language modeling work to this new neural network architecture. They released the first GPT model in 2018, <em id="S2.SS2.p3.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> GPT-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, and coined the abbreviation term <em id="S2.SS2.p3.1.3" class="ltx_emph ltx_font_italic">GPT</em> as the model name, standing for <em id="S2.SS2.p3.1.4" class="ltx_emph ltx_font_italic">Generative Pre-Training</em>. GPT-1 was developed based on a generative, decoder-only Transformer architecture, and adopted a hybrid approach of unsupervised pretraining and supervised fine-tuning.
GPT-1 has set up the core architecture for the GPT-series models and established the underlying principle to model natural language text, <em id="S2.SS2.p3.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> predicting the next word.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.2" class="ltx_p"><math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mo id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p4.2.1" class="ltx_emph ltx_font_italic">GPT-2</em>. Following a similar architecture of GPT-1, GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> increased the parameter scale to 1.5B, which was trained with a large webpage dataset WebText. As claimed in the paper of GPT-2, it sought to perform tasks via unsupervised language modeling, without explicit fine-tuning using labeled data. To motivate the approach, they introduced a probabilistic form for multi-task solving, <em id="S2.SS2.p4.2.2" class="ltx_emph ltx_font_italic">i.e.,</em> <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="p(output|input,task)" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><mrow id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml"><mi id="S2.SS2.p4.2.m2.1.1.3" xref="S2.SS2.p4.2.m2.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.2" xref="S2.SS2.p4.2.m2.1.1.2.cmml">​</mo><mrow id="S2.SS2.p4.2.m2.1.1.1.1" xref="S2.SS2.p4.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p4.2.m2.1.1.1.1.2" xref="S2.SS2.p4.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.p4.2.m2.1.1.1.1.1" xref="S2.SS2.p4.2.m2.1.1.1.1.1.cmml"><mrow id="S2.SS2.p4.2.m2.1.1.1.1.1.4" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.cmml"><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.4.2" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.4.1" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.4.3" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.4.1a" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.4.4" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.4.1b" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.4.5" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.4.1c" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.4.6" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.4.1d" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.4.7" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.7.cmml">t</mi></mrow><mo fence="false" id="S2.SS2.p4.2.m2.1.1.1.1.1.3" xref="S2.SS2.p4.2.m2.1.1.1.1.1.3.cmml">|</mo><mrow id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.3.cmml"><mrow id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.2" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.3" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1a" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.4" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1b" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.5" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1c" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.6" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.6.cmml">t</mi></mrow><mo id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.3" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.cmml"><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.2" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.3" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1a" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.4" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1b" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1.cmml">​</mo><mi id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.5" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.5.cmml">k</mi></mrow></mrow></mrow><mo stretchy="false" id="S2.SS2.p4.2.m2.1.1.1.1.3" xref="S2.SS2.p4.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><apply id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1"><times id="S2.SS2.p4.2.m2.1.1.2.cmml" xref="S2.SS2.p4.2.m2.1.1.2"></times><ci id="S2.SS2.p4.2.m2.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.3">𝑝</ci><apply id="S2.SS2.p4.2.m2.1.1.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1"><csymbol cd="latexml" id="S2.SS2.p4.2.m2.1.1.1.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.3">conditional</csymbol><apply id="S2.SS2.p4.2.m2.1.1.1.1.1.4.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4"><times id="S2.SS2.p4.2.m2.1.1.1.1.1.4.1.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.1"></times><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.4.2.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.2">𝑜</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.4.3.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.3">𝑢</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.4.4.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.4">𝑡</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.4.5.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.5">𝑝</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.4.6.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.6">𝑢</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.4.7.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.4.7">𝑡</ci></apply><list id="S2.SS2.p4.2.m2.1.1.1.1.1.2.3.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2"><apply id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1"><times id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.1"></times><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.2">𝑖</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.3">𝑛</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.4.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.4">𝑝</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.5.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.5">𝑢</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.6.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.1.1.1.6">𝑡</ci></apply><apply id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2"><times id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.1"></times><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.2.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.2">𝑡</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.3.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.3">𝑎</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.4.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.4">𝑠</ci><ci id="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.5.cmml" xref="S2.SS2.p4.2.m2.1.1.1.1.1.2.2.2.5">𝑘</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">p(output|input,task)</annotation></semantics></math> (similar approaches have been adopted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>), which predicts the output conditioned on the input and task information. To model this conditional probability, language text can be naturally employed as a unified way to format input, output and task information. In this way, the process of solving a task can be cast as a word prediction problem for generating the solution text. Further, they introduced a more formal claim for this idea: “Since the (task-specific) supervised objective is the same as the unsupervised (language modeling) objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective (for various tasks)”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite><span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>To better understand this sentence, we put some explanation words in parentheses.</span></span></span>.
A basic understanding of this claim is that each (NLP) task can be considered as the word prediction problem based on a subset of the world text. Thus, unsupervised language modeling could be capable in solving various tasks, if it was trained to have sufficient capacity in recovering the world text.
These early discussion in GPT-2’s paper echoed in the interview of Ilya Sutskever by Jensen Huang: “What the neural network learns is some representation of the process that produced the text. This text is actually a projection of the world…the more accurate you are in predicting the next word, the higher the fidelity, the more resolution you get in this process…”<span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a target="_blank" href="https://lifearchitect.ai/ilya/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lifearchitect.ai/ilya/</a></span></span></span>.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Capacity Leap</span>. Although GPT-2 is intended to be an “unsupervised multitask learner”, it overall has an inferior performance compared with supervised fine-tuning state-of-the-art methods.
Because it has a relatively small model size, it has been widely fine-tuned in downstream tasks, especially the dialog tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>. Based on GPT-2, GPT-3 demonstrates a key capacity leap by scaling of the (nearly same) generative pre-training architecture.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p"><math id="S2.SS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p6.1.m1.1a"><mo id="S2.SS2.p6.1.m1.1.1" xref="S2.SS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.1.m1.1b"><ci id="S2.SS2.p6.1.m1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p6.1.1" class="ltx_emph ltx_font_italic">GPT-3</em>. GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> was released in 2020, which scaled the model parameters to an ever larger size of 175B. In the GPT-3’s paper, it formally introduced the concept of in-context learning&nbsp;(ICL)<span id="footnote17" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>GPT-2 essentially used ICL for unsupervised task learning, though it wasn’t called ICL at that time. </span></span></span>, which utilizes LLMs in a few-shot or zero-shot way. ICL can teach (or instruct) LLMs to understand the tasks in the form of natural language text.
With ICL, the pre-training and utilization of LLMs converge to the same language modeling paradigm: pre-training predicts the following text sequence conditioned on the context, while ICL predicts the correct task solution, which can be also formatted as a text sequence, given the task description and demonstrations.
GPT-3 not only demonstrates very excellent performance in a variety of NLP tasks, but also on a number of specially designed tasks that require the abilities of reasoning or domain adaptation. Although the GPT-3’s paper does not explicitly discuss the emergent abilities of LLMs, we can observe large performance leap that might transcend the basic scaling law&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, <em id="S2.SS2.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> larger models have significantly stronger ICL ability (illustrated in the original Figure&nbsp;1.2 of the GPT-3’s paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>). Overall, GPT-3 can be viewed as a remarkable landmark in the journey evolving from PLMs to LLMs. It has empirically proved that scaling the neural networks to a significant size can lead to a huge increase in model capacity.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para ltx_noindent">
<p id="S2.SS2.p7.1" class="ltx_p"><span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold">Capacity Enhancement</span>. Due to the strong capacities, GPT-3 has been the base model to develop even more capable LLMs for OpenAI. Overall, OpenAI has explored two major approaches to further improving the GPT-3 model, <em id="S2.SS2.p7.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> training on code data and alignment with human preference, which are detailed as follows.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p"><math id="S2.SS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p8.1.m1.1a"><mo id="S2.SS2.p8.1.m1.1.1" xref="S2.SS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.1.m1.1b"><ci id="S2.SS2.p8.1.m1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p8.1.1" class="ltx_emph ltx_font_italic">Training on code data</em>. A major limitation of the original GPT-3 model (pre-trained on plain text) lies in the lack of the reasoning ability on complex tasks, <em id="S2.SS2.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> completing the code and solving math problems. To enhance this ability, Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> was introduced by OpenAI in July 2021, which was a GPT model fine-tuned on
a large corpus of GitHub code. It demonstrated that Codex can solve very difficult programming problems, and also lead to a significant performance improvement in solving math problems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. Further, a contrastive approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> to training text and code embedding was reported in January 2022, which was shown to improve a series of related tasks (<em id="S2.SS2.p8.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> linear-probe classification, text search and code search). Actually, the GPT-3.5 models are developed based on a code-based GPT model (<em id="S2.SS2.p8.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> <span id="S2.SS2.p8.1.5" class="ltx_text ltx_font_typewriter">code-davinci-002</span>), which indicates that training on code data is a very useful practice to improve the model capacity of GPT models, especially the reasoning ability.
Furthermore, there is also a speculation that training on code data can greatly increase the chain-of-thought prompting abilities of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, while it is still worth further investigation with more thorough verification.</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p"><math id="S2.SS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p9.1.m1.1a"><mo id="S2.SS2.p9.1.m1.1.1" xref="S2.SS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p9.1.m1.1b"><ci id="S2.SS2.p9.1.m1.1.1.cmml" xref="S2.SS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p9.1.1" class="ltx_emph ltx_font_italic">Human alignment</em>. The related research of human alignment can be dated back to the year 2017 (or earlier) for OpenAI: a blog article entitled “learning from human preferences”<span id="footnote18" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a target="_blank" href="https://openai.com/research/learning-from-human-preferences" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/learning-from-human-preferences</a></span></span></span> was posted on the OpenAI blog describing a work that applied reinforcement learning&nbsp;(RL) to learn from the <em id="S2.SS2.p9.1.2" class="ltx_emph ltx_font_italic">preference comparisons</em> annotated by humans&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> (similar to the <em id="S2.SS2.p9.1.3" class="ltx_emph ltx_font_italic">reward training</em> step in the aligning algorithm of InstructGPT in Figure&nbsp;<a href="#S5.F12" title="Figure 12 ‣ 5.2.3 Reinforcement Learning from Human Feedback ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>).
Shortly after the release of this RL paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, the paper of the Proximal Policy Optimization&nbsp;(PPO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> was published in July 2017, which now has been the foundational RL algorithm for learning from human preferences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
Later in January 2020, GPT-2 was fine-tuned using the aforementioned RL algorithms&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>, which leveraged human preferences to improve the capacities of GPT-2 on NLP tasks. In the same year, another work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> trained a summarization model for optimizing human preferences in a similar way.
Based on these prior work, InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> was proposed in January 2022 to improve the GPT-3 model for human alignment, which formally established a three-stage <em id="S2.SS2.p9.1.4" class="ltx_emph ltx_font_italic">reinforcement learning from human feedback&nbsp;(RLHF)</em> algorithm.
Note that it seems that the wording of “<em id="S2.SS2.p9.1.5" class="ltx_emph ltx_font_italic">instruction tuning</em>” has seldom been used in OpenAI’s paper and documentation, which is substituted by <em id="S2.SS2.p9.1.6" class="ltx_emph ltx_font_italic">supervised fine-tuning on human demonstrations</em> (<em id="S2.SS2.p9.1.7" class="ltx_emph ltx_font_italic">i.e.,</em> the first step of the RLHF algorithm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>).
In addition to improving the instruction following capacity, the RLHF algorithm is particularly useful to mitigate the issues of generating harm or toxic content for LLMs, which is key to the safe deployment of LLMs in practice.
OpenAI describes their approach to alignment research in a technical article&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>, which has summarized three promising directions: “training AI systems to use human feedback, to assist human evaluation and to do alignment research”.</p>
</div>
<div id="S2.SS2.p10" class="ltx_para">
<p id="S2.SS2.p10.1" class="ltx_p">These enhancement techniques lead to the improved GPT-3 models with stronger capacities, which are called GPT-3.5 models by OpenAI (see the discussion about the OpenAI API in Section&nbsp;<a href="#S3.SS1" title="3.1 Publicly Available Model Checkpoints or APIs ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
<div id="S2.SS2.p11" class="ltx_para ltx_noindent">
<p id="S2.SS2.p11.1" class="ltx_p"><span id="S2.SS2.p11.1.1" class="ltx_text ltx_font_bold">The Milestones of Language Models</span>.
Based on all the exploration efforts, two major milestones have been achieved by OpenAI, namely ChatGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> and GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, which have largely raised the capacity bar of existing AI systems.</p>
</div>
<div id="S2.SS2.p12" class="ltx_para">
<p id="S2.SS2.p12.1" class="ltx_p"><math id="S2.SS2.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p12.1.m1.1a"><mo id="S2.SS2.p12.1.m1.1.1" xref="S2.SS2.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p12.1.m1.1b"><ci id="S2.SS2.p12.1.m1.1.1.cmml" xref="S2.SS2.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p12.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p12.1.1" class="ltx_emph ltx_font_italic">ChatGPT</em>. In November 2022, OpenAI released the conversation model ChatGPT, based on the GPT models (GPT-3.5 and GPT-4). As the official blog article introduced&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>, ChatGPT was trained in a similar way as InstructGPT (called “a sibling model to InstructGPT” in the original post), while specially optimized for dialogue.
They reported a difference between the training of ChatGPT and InstructGPT in the data collection setup: human-generated conversations (playing both the roles of user and AI) are combined with the InstructGPT dataset in a dialogue format for training ChatGPT.
ChatGPT exhibited superior capacities in communicating with humans: possessing a vast store of knowledge, skill at reasoning on mathematical problems, tracing the context accurately in multi-turn dialogues, and aligning well with human values for safe use. Later on, the plugin mechanism has been supported in ChatGPT, which further extends the capacities of ChatGPT with existing tools or apps.
So far, it seems to be the ever most powerful chatbot in the AI history. The launch of ChatGPT has a significant impact on the AI research in the future, which sheds light on the exploration of human-like AI systems.</p>
</div>
<div id="S2.SS2.p13" class="ltx_para">
<p id="S2.SS2.p13.1" class="ltx_p"><math id="S2.SS2.p13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p13.1.m1.1a"><mo id="S2.SS2.p13.1.m1.1.1" xref="S2.SS2.p13.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p13.1.m1.1b"><ci id="S2.SS2.p13.1.m1.1.1.cmml" xref="S2.SS2.p13.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p13.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p13.1.1" class="ltx_emph ltx_font_italic">GPT-4</em>. As another remarkable progress, GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> was released in March 2023, which extended the text input to multimodal signals.
Overall, GPT-4 has stronger capacities in solving complex tasks than GPT-3.5, showing a large performance improvement on many evaluation tasks.
A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> investigated the capacities of GPT-4 by conducting qualitative tests with human-generated problems, spanning a diverse range of difficult tasks, and showed that GPT-4 can achieve more superior performance than prior GPT models such as ChatGPT.
Furthermore, GPT-4 responds more safely to malicious or provocative queries, due to a six-month iterative alignment (with an additional safety reward signal in the RLHF training).
In the technical report, OpenAI has emphasized how to safely develop GPT-4 and applied a number of intervention strategies to mitigate the possible issues of LLMs, such as hallucinations, privacy and overreliance. For example, they introduced the mechanism called <em id="S2.SS2.p13.1.2" class="ltx_emph ltx_font_italic">red teaming</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> to reduce the harm or toxic content generation.
As another important aspect, GPT-4 has been developed on a well-established deep learning infrastructure with improved optimization methods. They introduced a new mechanism called <em id="S2.SS2.p13.1.3" class="ltx_emph ltx_font_italic">predictable scaling</em> that can accurately predict the final performance with a small proportion of compute during model training.</p>
</div>
<div id="S2.SS2.p14" class="ltx_para">
<p id="S2.SS2.p14.1" class="ltx_p"><math id="S2.SS2.p14.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.SS2.p14.1.m1.1a"><mo id="S2.SS2.p14.1.m1.1.1" xref="S2.SS2.p14.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p14.1.m1.1b"><ci id="S2.SS2.p14.1.m1.1.1.cmml" xref="S2.SS2.p14.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p14.1.m1.1c">\bullet</annotation></semantics></math> <em id="S2.SS2.p14.1.1" class="ltx_emph ltx_font_italic">GPT-4V, GPT-4 turbo, and beyond</em>. Based on the work done for GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, OpenAI further released GPT-4V in September 2023,
which focused on the safe deployment of the vision capabilities of GPT-4. In the GPT-4V’s system card&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, it has extensively discussed the assessment and mitigation of risks related to visually augmented inputs.
Specially, GPT-4V exhibited strong vision capacities in various application scenarios, showing the great potential as a powerful multimodal learning system.
More recently, in November 2023, OpenAI
released an upgraded generation of GPT-4 model at DevDay, named <em id="S2.SS2.p14.1.2" class="ltx_emph ltx_font_italic">GPT-4 Turbo</em>, with a series of technical improvements.
GPT-4 Turbo is featured by the improved model capacity (more capable than GPT-4), the extended knowledge source (up to April 2023), long context window (up to 128k tokens), optimized model performance (cheaper price), and other useful functionality updates (function call, reproducible outputs, etc.).
At the same time, Assistants API was launched to ease the rapid development of agent-like assistants. With this API, developers can easily create goal-oriented assistants within their applications, by leveraging specific instruction, extra knowledge and tool use. Furthermore, multimodal capacities (see, hear, and speak) were also enhanced in this new release, supported by GPT-4 Turbo with vision, DALL·E 3, Text-to-speech&nbsp;(TTS), and Listen to voice samples.
These improvements have greatly extended the capacity scope and enhanced the task performance of GPT models. More importantly, the application ecosystem will be greatly strengthened with the technology upgrade in improved models, APIs, and functionalities.</p>
</div>
<div id="S2.SS2.p15" class="ltx_para">
<p id="S2.SS2.p15.1" class="ltx_p">Despite the huge progress, there are still limitations with these superior LLMs, <em id="S2.SS2.p15.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> generating hallucinations with factual errors or potentially risky response within some specific context&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. More limitations or issues of LLMs will be discussed in Section&nbsp;<a href="#S7" title="7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
It poses long-standing research challenges to develop more capable, safer LLMs.
From the perspective of engineering, OpenAI has adopted an iterative deployment strategy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> to develop the models and products by following a five-stage development and deployment life-cycle, which aims to effectively reduce the potential risks of using the models.
In the following, we will dive into the technical details in order to have a specific understanding of how they have been developed.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2303.18223/assets/x6.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>An evolutionary graph of the research work conducted on LLaMA. Due to the huge number, we cannot include all the LLaMA variants in this figure, even much excellent work. To support incremental update, we share the source file of this figure, and welcome the readers to include the desired models by submitting the pull requests on our GitHub page. </figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Resources of LLMs</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">It is by no means an easy job to develop or reproduce LLMs, considering the challenging technical issues and huge demands of computation resources.
A feasible way is to learn experiences from existing LLMs and reuse publicly available resources for incremental development or experimental study. In this section, we briefly summarize the publicly available resources for developing LLMs, including model checkpoints (or APIs), corpora and libraries.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Publicly Available Model Checkpoints or APIs</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Given the huge cost of model pre-training, well-trained model checkpoints are critical to the study and development of LLMs for the research community. Since the parameter scale is a key factor to consider for using LLMs, we categorize these public models into two scale levels (<em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">tens of billions of parameters</em> and <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">hundreds of billions of parameters</em>),
which is useful for users to identify the suitable resources according to their resource budget. In addition, for inference, we can directly employ public APIs to perform our tasks, without running the model
locally. Next, we introduce the publicly available model checkpoints and APIs.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.3" class="ltx_p"><span id="S3.SS1.p2.3.1" class="ltx_text ltx_font_bold">Models with Tens of Billions of Parameters</span>.
Most of the models in this category have a parameter scale ranging from 10B to 20B, except LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and LLaMA2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> (containing 70B parameters in the largest version), NLLB&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> (containing 54.5B parameters in the largest version), and Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> (containing 40B parameters in the largest version). 
Other models within this range include mT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, PanGu-<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\alpha</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, T0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, GPT-NeoX-20B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>, CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, UL2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, Flan-T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, and mT0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.
Among them, Flan-T5&nbsp;(11B version) can serve as a premier model for research on instruction tuning, since it explores the instruction tuning from three aspects&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>: increasing the number of tasks, scaling the model size, and fine-tuning with chain-of-thought prompting data.
Besides, CodeGen&nbsp;(11B version), as an autoregressive language model designed for generating code, can be considered as a good candidate for exploring the code generation ability.
It also introduces a new benchmark MTPB&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> specially for multi-turn program synthesis, which is composed by 115 expert-generated problems. To solve these problems, it requires LLMs to acquire sufficient programming knowledge (<em id="S3.SS1.p2.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> math, array operations, and algorithms). More recently, CodeGen2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> has been released to explore the impact of choices in model architecture, learning algorithms, and data distributions on the model. As another LLM specialized in coding abilities, StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> has also achieved excellent results.
As for multilingual tasks, mT0&nbsp;(13B version) might be a good candidate model, which has been fine-tuned on multilingual tasks with multilingual prompts.
Furthermore, PanGu-<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\alpha</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> shows good performance in Chinese downstream tasks in zero-shot or few-shot settings, which is developed based on the deep learning framework MindSpore&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite>.
Note that PanGu-<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\alpha</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> holds multiple versions of models (up to 200B parameters), while the largest public version has 13B parameters. 
As a popular LLM, LLaMA&nbsp;(65B version)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, which contains approximately five times as many parameters as other models, has exhibited superior performance in tasks related to instruction following. Compared to LLaMA, LLaMA2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> has made more explorations in reinforcement learning from human feedback&nbsp;(RLHF) and developed a chat-oriented version called <em id="S3.SS1.p2.3.3" class="ltx_emph ltx_font_italic">LLaMA-chat</em>, which generally outperforms existing open-source models across a range of helpfulness and safety benchmarks.
Due to the openness and effectiveness,
LLaMA has attracted significant attention from the research community, and many efforts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> have been devoted to fine-tuning or continually pre-training its different model versions for implementing new models or tools.  More recently, Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>, as another open-source LLM, has also achieved very excellent performance on open benchmarks. It is featured by a more careful data cleaning process to prepare the pre-training data (with a publicly shared dataset <em id="S3.SS1.p2.3.4" class="ltx_emph ltx_font_italic">RefinedWeb&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>). </em>
Typically, pre-training models at this scale require hundreds or even thousands of GPUs or TPUs. For instance, GPT-NeoX-20B uses 12 supermicro servers, each equipped with 8 NVIDIA A100-SXM4-40GB GPUs, while LLaMA utilizes 2,048 A100-80G GPUs as reported in their original publications. To accurately estimate the computation resources needed, it is suggested to use the metrics measuring the number of involved computations such as <em id="S3.SS1.p2.3.5" class="ltx_emph ltx_font_italic">FLOPS</em> (<em id="S3.SS1.p2.3.6" class="ltx_emph ltx_font_italic">i.e.,</em> FLoating point number Operations Per Second)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Models with Hundreds of Billions of Parameters</span>.
For models in this category, only a handful of models have been publicly released. For example, OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, OPT-IML&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, and BLOOMZ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> have nearly the same number of parameters as GPT-3&nbsp;(175B version), while GLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> and Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> have 130B and 120B parameters, respectively.
Among them, OPT&nbsp;(175B version), with the instruction-tuned version OPT-IML, has been specially motivated for open sharing, which aims to enable researchers to carry out reproducible research at scale. 
For research in cross-lingual generalization, BLOOM&nbsp;(176B version) and BLOOMZ&nbsp;(176B version) can be used as base models, due to the competence in multilingual language modeling tasks.
As a bilingual LLM, GLM has also provided a popular small-sized Chinese chat model ChatGLM2-6B
(a updated version for ChatGLM-6B), which is featured with many improvements in efficiency and capacity (<em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> quantization, 32K-length context, fast inference rate).
Models of this scale typically require thousands of GPUs or TPUs to train. For instance, OPT&nbsp;(175B version) used 992 A100-80GB GPUs, while GLM&nbsp;(130B version) used a cluster of 96 NVIDIA DGX-A100 (8x40G) GPU nodes.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">LLaMA Model Family</span>. The collection of LLaMA models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
were introduced by Meta AI in February, 2023, consisting of four sizes (7B, 13B, 30B and 65B).
Since released, LLaMA has attracted extensive attention from both research and industry communities.
LLaMA models have achieved very excellent performance on various open benchmarks, which have become the most popular open language models thus far.
A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining. In particular, instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.
To effectively adapt LLaMA models in non-English languages, it often needs to extend the original vocabulary (trained mainly on English corpus) or fine-tune it with instructions or data in the target language.
Among these extended models, Stanford Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> is the first open instruct-following model fine-tuned based on LLaMA&nbsp;(7B). It is trained by 52K instruction-following demonstrations generated via self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> using <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_typewriter">text-davinci-003</span>.
The instruction data, named <em id="S3.SS1.p4.1.3" class="ltx_emph ltx_font_italic">Alpaca-52K</em>, and training code have been extensively adopted in subsequent work, such as Alpaca-LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite> (a reproduction of Stanford Alpaca using LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>), Koala&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite>, and BELLE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>. In addition, Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite> is another popular LLaMA variant, trained upon user-shared conversations collected from ShareGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>.
Due to the excellent performance and availability of the LLaMA model family, many multimodal models incorporate them as the base language models, to achieve strong language understanding and generation abilities.
Compared with other variants, Vicuna is more preferred in multimodal language models, which have led to the emergence of a variety of popular models, including LLaVA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite>, MiniGPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite>, InstructBLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>, and PandaGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>. The release of LLaMA has greatly advanced the research progress of LLMs. To summarize the research work conducted on LLaMA, we present a brief evolutionary graph in Figure&nbsp;<a href="#S2.F5" title="Figure 5 ‣ 2.2 Technical Evolution of GPT-series Models ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Public API of LLMs</span>.

Instead of directly using the model copies, APIs provide a more convenient way for common users to use LLMs, without the need of running the model locally. As a representative interface for using LLMs, the APIs for the GPT-series models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> have been widely used for both academia and industry<span id="footnote19" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>https://platform.openai.com/docs/api-reference/introduction</span></span></span>. OpenAI has provided seven major interfaces to the models in GPT-3 series: <span id="S3.SS1.p5.1.2" class="ltx_text ltx_font_typewriter">ada</span>, <span id="S3.SS1.p5.1.3" class="ltx_text ltx_font_typewriter">babbage</span>, <span id="S3.SS1.p5.1.4" class="ltx_text ltx_font_typewriter">curie</span>, <span id="S3.SS1.p5.1.5" class="ltx_text ltx_font_typewriter">davinci</span> (the most powerful version in GPT-3 series), <span id="S3.SS1.p5.1.6" class="ltx_text ltx_font_typewriter">text-ada-001</span>, <span id="S3.SS1.p5.1.7" class="ltx_text ltx_font_typewriter">text-babbage-001</span>, and <span id="S3.SS1.p5.1.8" class="ltx_text ltx_font_typewriter">text-curie-001</span>. Among them, the first four interfaces can be further fine-tuned on the host server of OpenAI.
In particular, <span id="S3.SS1.p5.1.9" class="ltx_text ltx_font_typewriter">babbage</span>, <span id="S3.SS1.p5.1.10" class="ltx_text ltx_font_typewriter">curie</span>, and <span id="S3.SS1.p5.1.11" class="ltx_text ltx_font_typewriter">davinci</span> correspond to the GPT-3&nbsp;(1B), GPT-3&nbsp;(6.7B), and GPT-3&nbsp;(175B) models, respectively&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
In addition, there are also two APIs related to Codex&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, called <span id="S3.SS1.p5.1.12" class="ltx_text ltx_font_typewriter">code-cushman-001</span> (a powerful and multilingual version of the Codex&nbsp;(12B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>) and <span id="S3.SS1.p5.1.13" class="ltx_text ltx_font_typewriter">code-davinci-002</span>.
Further, GPT-3.5 series include one base model <span id="S3.SS1.p5.1.14" class="ltx_text ltx_font_typewriter">code-davinci-002</span> and three enhanced versions, namely <span id="S3.SS1.p5.1.15" class="ltx_text ltx_font_typewriter">text-davinci-002</span>, <span id="S3.SS1.p5.1.16" class="ltx_text ltx_font_typewriter">text-davinci-003</span>, and <span id="S3.SS1.p5.1.17" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span>.
As more powerful alternatives, in this year, OpenAI has released the model interfaces for GPT-4 series, including <span id="S3.SS1.p5.1.18" class="ltx_text ltx_font_typewriter">gpt-4</span>, <span id="S3.SS1.p5.1.19" class="ltx_text ltx_font_typewriter">gpt-4-32k</span>, <span id="S3.SS1.p5.1.20" class="ltx_text ltx_font_typewriter">gpt-4-1106-preview</span>&nbsp;(<em id="S3.SS1.p5.1.21" class="ltx_emph ltx_font_italic">i.e.,</em> GPT-4 Turbo) and <span id="S3.SS1.p5.1.22" class="ltx_text ltx_font_typewriter">gpt-4-vision-preview</span>&nbsp;(<em id="S3.SS1.p5.1.23" class="ltx_emph ltx_font_italic">i.e.,</em> GPT-4 Turbo with vision, a multimodal model).
It is worth noting that OpenAI has been maintaining and upgrading these model interfaces (<span id="S3.SS1.p5.1.24" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span>, <span id="S3.SS1.p5.1.25" class="ltx_text ltx_font_typewriter">gpt-4</span>, <span id="S3.SS1.p5.1.26" class="ltx_text ltx_font_typewriter">gpt-4-32k</span>), so the API name will actually point to the latest version. 
Currently, ChatGPT can be powered by either GPT-3.5 or GPT-4 models. Overall, one select the suitable model interface based on the specific application scenarios and response requirements.
The detailed usage can be found on their project websites<span id="footnote20" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span>https://platform.openai.com/docs/models/overview</span></span></span>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Statistics of commonly-used data sources. </figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Corpora</span></td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Size</span></td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Source</span></td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Latest Update Time</span></td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.2.1.1" class="ltx_text" style="font-size:80%;">BookCorpus&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib153" title="" class="ltx_ref">153</a><span id="S3.T2.1.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.2.2.1" class="ltx_text" style="font-size:80%;">5GB</span></td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.2.3.1" class="ltx_text" style="font-size:80%;">Books</span></td>
<td id="S3.T2.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.2.4.1" class="ltx_text" style="font-size:80%;">Dec-2015</span></td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.3.1.1" class="ltx_text" style="font-size:80%;">Gutenberg&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib154" title="" class="ltx_ref">154</a><span id="S3.T2.1.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.3.3.1" class="ltx_text" style="font-size:80%;">Books</span></td>
<td id="S3.T2.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.3.4.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.4.1.1" class="ltx_text" style="font-size:80%;">C4&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib82" title="" class="ltx_ref">82</a><span id="S3.T2.1.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.4.2.1" class="ltx_text" style="font-size:80%;">800GB</span></td>
<td id="S3.T2.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.4.3.1" class="ltx_text" style="font-size:80%;">CommonCrawl</span></td>
<td id="S3.T2.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.4.4.1" class="ltx_text" style="font-size:80%;">Apr-2019</span></td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.5.1.1" class="ltx_text" style="font-size:80%;">CC-Stories-R&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib155" title="" class="ltx_ref">155</a><span id="S3.T2.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.5.2.1" class="ltx_text" style="font-size:80%;">31GB</span></td>
<td id="S3.T2.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.5.3.1" class="ltx_text" style="font-size:80%;">CommonCrawl</span></td>
<td id="S3.T2.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.5.4.1" class="ltx_text" style="font-size:80%;">Sep-2019</span></td>
</tr>
<tr id="S3.T2.1.6" class="ltx_tr">
<td id="S3.T2.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.6.1.1" class="ltx_text" style="font-size:80%;">CC-NEWS&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.T2.1.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.6.2.1" class="ltx_text" style="font-size:80%;">78GB</span></td>
<td id="S3.T2.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.6.3.1" class="ltx_text" style="font-size:80%;">CommonCrawl</span></td>
<td id="S3.T2.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.6.4.1" class="ltx_text" style="font-size:80%;">Feb-2019</span></td>
</tr>
<tr id="S3.T2.1.7" class="ltx_tr">
<td id="S3.T2.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.7.1.1" class="ltx_text" style="font-size:80%;">REALNEWs&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib156" title="" class="ltx_ref">156</a><span id="S3.T2.1.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.7.2.1" class="ltx_text" style="font-size:80%;">120GB</span></td>
<td id="S3.T2.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.7.3.1" class="ltx_text" style="font-size:80%;">CommonCrawl</span></td>
<td id="S3.T2.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.7.4.1" class="ltx_text" style="font-size:80%;">Apr-2019</span></td>
</tr>
<tr id="S3.T2.1.8" class="ltx_tr">
<td id="S3.T2.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.8.1.1" class="ltx_text" style="font-size:80%;">OpenWebText&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib157" title="" class="ltx_ref">157</a><span id="S3.T2.1.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.8.2.1" class="ltx_text" style="font-size:80%;">38GB</span></td>
<td id="S3.T2.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.8.3.1" class="ltx_text" style="font-size:80%;">Reddit links</span></td>
<td id="S3.T2.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.8.4.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
</tr>
<tr id="S3.T2.1.9" class="ltx_tr">
<td id="S3.T2.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.9.1.1" class="ltx_text" style="font-size:80%;">Pushift.io&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib158" title="" class="ltx_ref">158</a><span id="S3.T2.1.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.9.2.1" class="ltx_text" style="font-size:80%;">2TB</span></td>
<td id="S3.T2.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.9.3.1" class="ltx_text" style="font-size:80%;">Reddit links</span></td>
<td id="S3.T2.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.9.4.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
</tr>
<tr id="S3.T2.1.10" class="ltx_tr">
<td id="S3.T2.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.10.1.1" class="ltx_text" style="font-size:80%;">Wikipedia&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib159" title="" class="ltx_ref">159</a><span id="S3.T2.1.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.10.2.1" class="ltx_text" style="font-size:80%;">21GB</span></td>
<td id="S3.T2.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.10.3.1" class="ltx_text" style="font-size:80%;">Wikipedia</span></td>
<td id="S3.T2.1.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.10.4.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
</tr>
<tr id="S3.T2.1.11" class="ltx_tr">
<td id="S3.T2.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.11.1.1" class="ltx_text" style="font-size:80%;">BigQuery&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.11.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib160" title="" class="ltx_ref">160</a><span id="S3.T2.1.11.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.11.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.11.3.1" class="ltx_text" style="font-size:80%;">Codes</span></td>
<td id="S3.T2.1.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.11.4.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
</tr>
<tr id="S3.T2.1.12" class="ltx_tr">
<td id="S3.T2.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.12.1.1" class="ltx_text" style="font-size:80%;">the Pile&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib161" title="" class="ltx_ref">161</a><span id="S3.T2.1.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.12.2.1" class="ltx_text" style="font-size:80%;">800GB</span></td>
<td id="S3.T2.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.12.3.1" class="ltx_text" style="font-size:80%;">Other</span></td>
<td id="S3.T2.1.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.12.4.1" class="ltx_text" style="font-size:80%;">Dec-2020</span></td>
</tr>
<tr id="S3.T2.1.13" class="ltx_tr">
<td id="S3.T2.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T2.1.13.1.1" class="ltx_text" style="font-size:80%;">ROOTS&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib162" title="" class="ltx_ref">162</a><span id="S3.T2.1.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.13.2.1" class="ltx_text" style="font-size:80%;">1.6TB</span></td>
<td id="S3.T2.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.13.3.1" class="ltx_text" style="font-size:80%;">Other</span></td>
<td id="S3.T2.1.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T2.1.13.4.1" class="ltx_text" style="font-size:80%;">Jun-2022</span></td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Commonly Used Corpora for Pre-training</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In contrast to earlier PLMs, LLMs which consist of a significantly larger number of parameters require a higher volume of training data that covers a broad range of content. For this need, there are increasingly more accessible training datasets that have been released for research.
In this section, we will briefly summarize several widely used corpora for training LLMs. 
Based on their content types, we categorize these corpora into six groups: Books, CommonCrawl, Reddit links, Wikipedia, Code, and others.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Books.</span> BookCorpus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite> is a commonly used dataset in previous small-scale models (<em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> and GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>), consisting of over 11,000 books covering a wide range of topics and genres (<em id="S3.SS2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> novels and biographies).
Another large-scale book corpus is Project Gutenberg&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>, consisting of over 70,000 literary books including novels, essays, poetry, drama, history, science, philosophy, and other types of works in the public domain. It is currently one of the largest open-source book collections, which is used in training of MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. As for Books1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and Books2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> used in GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, they are much larger than BookCorpus but have not been publicly released so far.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">CommonCrawl.</span> CommonCrawl&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite> is one of the largest open-source web crawling databases, containing a petabyte-scale data volume, which has been widely used as training data for existing LLMs.
As the whole dataset is very large, existing studies mainly extract subsets of web pages from it within a specific period.
However, due to the widespread existence of noisy and low-quality information in web data, it is necessary to perform data preprocessing before usage. Based on CommonCrawl, there are four filtered datasets that are commonly used in existing work: C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, CC-Stories&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>, CC-News&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and RealNews&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>. The Colossal Clean Crawled Corpus (C4) includes five variants<span id="footnote21" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span>https://www.tensorflow.org/datasets/catalog/c4 </span></span></span>, namely en&nbsp;(806G), en.noclean&nbsp;(6T), realnewslike&nbsp;(36G), webtextlike&nbsp;(17G), and multilingual&nbsp;(38T). The <em id="S3.SS2.p3.1.2" class="ltx_emph ltx_font_italic">en</em> version has been utilized for pre-training T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, and UL2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>. The multilingual C4, also called mC4, has been used in mT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>.
CC-Stories&nbsp;(31G) is composed of a subset of CommonCrawl data, in which the contents are made in a story-like way.
Because the original source of CC-Stories is not available now, we include
a reproduction version, <em id="S3.SS2.p3.1.3" class="ltx_emph ltx_font_italic">CC-Stories-R</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>, in Table&nbsp;<a href="#S3.T2" title="TABLE II ‣ 3.1 Publicly Available Model Checkpoints or APIs ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
Moreover, two news corpora extracted from CommonCrawl, <em id="S3.SS2.p3.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> REALNEWS&nbsp;(120G) and CC-News&nbsp;(76G), are also commonly used as the pre-training data.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Reddit Links.</span> Reddit is a social media platform that enables users to submit links and text posts, which can be voted on by others through “upvotes” or “downvotes”.
Highly upvoted posts are often considered useful, and can be utilized to create high-quality datasets.
WebText&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is a well-known corpus composed of highly upvoted links from Reddit, but it is not publicly available.
As a surrogate, there is a readily accessible open-source alternative called OpenWebText&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>.
Another corpus extracted from Reddit is PushShift.io&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite>, a real-time updated dataset that consists of historical data from Reddit since its creation day.
Pushshift provides not only monthly data dumps but also useful utility tools to support users in searching, summarizing, and conducting preliminary investigations on the entire dataset. This makes it easy for users to collect and process Reddit data.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Wikipedia.</span>
Wikipedia&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> is an online encyclopedia containing a large volume of high-quality articles on diverse topics.
Most of these articles are composed in an expository style of writing (with supporting references), covering a wide range of languages and fields. 
Typically, the English-only filtered versions of Wikipedia are widely used in most LLMs (<em id="S3.SS2.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>). Wikipedia is available in multiple languages, so it can be used in multilingual settings.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Code.</span> To collect code data, existing work mainly crawls open-source licensed codes from the Internet.
Two major sources are public code repositories under open-source licenses (<em id="S3.SS2.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GitHub) and code-related question-answering platforms (<em id="S3.SS2.p6.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> StackOverflow).
Google has publicly released the BigQuery dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>, which includes a substantial number of open-source licensed code snippets in various programming languages, serving as a representative code dataset. CodeGen has utilized BIGQUERY&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi).</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p"><span id="S3.SS2.p7.1.1" class="ltx_text ltx_font_bold">Others.</span> The Pile&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> is a large-scale, diverse, and open-source text dataset consisting of over 800GB of data from multiple sources, including books, websites, codes, scientific papers, and social media platforms. It is constructed from 22 diverse high-quality subsets.
The Pile dataset is widely used in models with different parameter scales, such as GPT-J&nbsp;(6B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>, CodeGen&nbsp;(16B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, and Megatron-Turing NLG&nbsp;(530B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>.
ROOTS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite> is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure&nbsp;<a href="#S3.F6" title="Figure 6 ‣ 3.4 Library Resource ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), instead of a single corpus.
Therefore, existing studies commonly mix several ready-made datasets (<em id="S3.SS2.p8.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> C4, OpenWebText, and the Pile), and then perform further processing to obtain the pre-training corpus.
Furthermore, to train the LLMs that are adaptive to specific applications, it is also important to extract data from relevant sources (<em id="S3.SS2.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Wikipedia and BigQuery) for enriching the corresponding information in pre-training data.
To have a quick reference of the data sources used in existing LLMs, we present the pre-training corpora of three representative LLMs:</p>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<p id="S3.SS2.p9.1" class="ltx_p"><math id="S3.SS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.p9.1.m1.1a"><mo id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><ci id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS2.p9.1.1" class="ltx_emph ltx_font_italic">GPT-3</em> (175B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> was trained on a mixed dataset of 300B tokens, including CommonCrawl&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, WebText2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, Books1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, Books2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and Wikipedia&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite>.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para">
<p id="S3.SS2.p10.1" class="ltx_p"><math id="S3.SS2.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.p10.1.m1.1a"><mo id="S3.SS2.p10.1.m1.1.1" xref="S3.SS2.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.1.m1.1b"><ci id="S3.SS2.p10.1.m1.1.1.cmml" xref="S3.SS2.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS2.p10.1.1" class="ltx_emph ltx_font_italic">PaLM</em> (540B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> uses a pre-training dataset of 780B tokens, which is sourced from social media conversations, filtered webpages, books, Github, multilingual Wikipedia, and news.</p>
</div>
<div id="S3.SS2.p11" class="ltx_para">
<p id="S3.SS2.p11.1" class="ltx_p"><math id="S3.SS2.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.p11.1.m1.1a"><mo id="S3.SS2.p11.1.m1.1.1" xref="S3.SS2.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p11.1.m1.1b"><ci id="S3.SS2.p11.1.m1.1.1.cmml" xref="S3.SS2.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p11.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS2.p11.1.1" class="ltx_emph ltx_font_italic">LLaMA</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> extracts training data from various sources, including CommonCrawl, C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, Github, Wikipedia, books, ArXiv, and StackExchange. The training data size for LLaMA&nbsp;(6B) and LLaMA&nbsp;(13B) is 1.0T tokens, while 1.4T tokens are used for LLaMA&nbsp;(32B) and LLaMA&nbsp;(65B).</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>A detailed list of available collections for instruction tuning. </figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Categories</span></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Collections</span></td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Time</span></td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">#Examples</span></td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="7"><span id="S3.T3.1.2.1.1" class="ltx_text" style="font-size:80%;">Task</span></td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.2.2.1" class="ltx_text" style="font-size:80%;">Nat. Inst.&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib166" title="" class="ltx_ref">166</a><span id="S3.T3.1.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.2.3.1" class="ltx_text" style="font-size:80%;">Apr-2021</span></td>
<td id="S3.T3.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.2.4.1" class="ltx_text" style="font-size:80%;">193K</span></td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.3.1.1" class="ltx_text" style="font-size:80%;">FLAN&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S3.T3.1.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.3.2.1" class="ltx_text" style="font-size:80%;">Sep-2021</span></td>
<td id="S3.T3.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.3.3.1" class="ltx_text" style="font-size:80%;">4.4M</span></td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.4.1.1" class="ltx_text" style="font-size:80%;">P3&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib167" title="" class="ltx_ref">167</a><span id="S3.T3.1.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.4.2.1" class="ltx_text" style="font-size:80%;">Oct-2021</span></td>
<td id="S3.T3.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.4.3.1" class="ltx_text" style="font-size:80%;">12.1M</span></td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.5.1.1" class="ltx_text" style="font-size:80%;">Super Nat. Inst.&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib88" title="" class="ltx_ref">88</a><span id="S3.T3.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.5.2.1" class="ltx_text" style="font-size:80%;">Apr-2022</span></td>
<td id="S3.T3.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.5.3.1" class="ltx_text" style="font-size:80%;">5M</span></td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.6.1.1" class="ltx_text" style="font-size:80%;">MVPCorpus&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib168" title="" class="ltx_ref">168</a><span id="S3.T3.1.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.6.2.1" class="ltx_text" style="font-size:80%;">Jun-2022</span></td>
<td id="S3.T3.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.6.3.1" class="ltx_text" style="font-size:80%;">41M</span></td>
</tr>
<tr id="S3.T3.1.7" class="ltx_tr">
<td id="S3.T3.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.7.1.1" class="ltx_text" style="font-size:80%;">xP3&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib94" title="" class="ltx_ref">94</a><span id="S3.T3.1.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.7.2.1" class="ltx_text" style="font-size:80%;">Nov-2022</span></td>
<td id="S3.T3.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.7.3.1" class="ltx_text" style="font-size:80%;">81M</span></td>
</tr>
<tr id="S3.T3.1.8" class="ltx_tr">
<td id="S3.T3.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.8.1.1" class="ltx_text" style="font-size:80%;">OIG</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib169" title="" class="ltx_ref">169</a><span id="S3.T3.1.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.8.2.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
<td id="S3.T3.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.8.3.1" class="ltx_text" style="font-size:80%;">43M</span></td>
</tr>
<tr id="S3.T3.1.9" class="ltx_tr">
<td id="S3.T3.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="5"><span id="S3.T3.1.9.1.1" class="ltx_text" style="font-size:80%;">Chat</span></td>
<td id="S3.T3.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.9.2.1" class="ltx_text" style="font-size:80%;">HH-RLHF&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.9.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib170" title="" class="ltx_ref">170</a><span id="S3.T3.1.9.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.9.3.1" class="ltx_text" style="font-size:80%;">Apr-2022</span></td>
<td id="S3.T3.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.9.4.1" class="ltx_text" style="font-size:80%;">160K</span></td>
</tr>
<tr id="S3.T3.1.10" class="ltx_tr">
<td id="S3.T3.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.10.1.1" class="ltx_text" style="font-size:80%;">HC3&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib171" title="" class="ltx_ref">171</a><span id="S3.T3.1.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.10.2.1" class="ltx_text" style="font-size:80%;">Jan-2023</span></td>
<td id="S3.T3.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.10.3.1" class="ltx_text" style="font-size:80%;">87K</span></td>
</tr>
<tr id="S3.T3.1.11" class="ltx_tr">
<td id="S3.T3.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.11.1.1" class="ltx_text" style="font-size:80%;">ShareGPT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.11.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib148" title="" class="ltx_ref">148</a><span id="S3.T3.1.11.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.11.2.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
<td id="S3.T3.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.11.3.1" class="ltx_text" style="font-size:80%;">90K</span></td>
</tr>
<tr id="S3.T3.1.12" class="ltx_tr">
<td id="S3.T3.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.12.1.1" class="ltx_text" style="font-size:80%;">Dolly&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib172" title="" class="ltx_ref">172</a><span id="S3.T3.1.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.12.2.1" class="ltx_text" style="font-size:80%;">Apr-2023</span></td>
<td id="S3.T3.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.12.3.1" class="ltx_text" style="font-size:80%;">15K</span></td>
</tr>
<tr id="S3.T3.1.13" class="ltx_tr">
<td id="S3.T3.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.13.1.1" class="ltx_text" style="font-size:80%;">OpenAssistant&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib173" title="" class="ltx_ref">173</a><span id="S3.T3.1.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.13.2.1" class="ltx_text" style="font-size:80%;">Apr-2023</span></td>
<td id="S3.T3.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.13.3.1" class="ltx_text" style="font-size:80%;">161K</span></td>
</tr>
<tr id="S3.T3.1.14" class="ltx_tr">
<td id="S3.T3.1.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="5"><span id="S3.T3.1.14.1.1" class="ltx_text" style="font-size:80%;">Synthetic</span></td>
<td id="S3.T3.1.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.14.2.1" class="ltx_text" style="font-size:80%;">Self-Instruct&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.14.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib143" title="" class="ltx_ref">143</a><span id="S3.T3.1.14.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.14.3.1" class="ltx_text" style="font-size:80%;">Dec-2022</span></td>
<td id="S3.T3.1.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.14.4.1" class="ltx_text" style="font-size:80%;">82K</span></td>
</tr>
<tr id="S3.T3.1.15" class="ltx_tr">
<td id="S3.T3.1.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.15.1.1" class="ltx_text" style="font-size:80%;">Alpaca&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.15.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib137" title="" class="ltx_ref">137</a><span id="S3.T3.1.15.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.15.2.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
<td id="S3.T3.1.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.15.3.1" class="ltx_text" style="font-size:80%;">52K</span></td>
</tr>
<tr id="S3.T3.1.16" class="ltx_tr">
<td id="S3.T3.1.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.16.1.1" class="ltx_text" style="font-size:80%;">Guanaco&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.16.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib174" title="" class="ltx_ref">174</a><span id="S3.T3.1.16.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.16.2.1" class="ltx_text" style="font-size:80%;">Mar-2023</span></td>
<td id="S3.T3.1.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.16.3.1" class="ltx_text" style="font-size:80%;">535K</span></td>
</tr>
<tr id="S3.T3.1.17" class="ltx_tr">
<td id="S3.T3.1.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.17.1.1" class="ltx_text" style="font-size:80%;">Baize&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.17.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib175" title="" class="ltx_ref">175</a><span id="S3.T3.1.17.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.17.2.1" class="ltx_text" style="font-size:80%;">Apr-2023</span></td>
<td id="S3.T3.1.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.17.3.1" class="ltx_text" style="font-size:80%;">158K</span></td>
</tr>
<tr id="S3.T3.1.18" class="ltx_tr">
<td id="S3.T3.1.18.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T3.1.18.1.1" class="ltx_text" style="font-size:80%;">BELLE&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.18.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib176" title="" class="ltx_ref">176</a><span id="S3.T3.1.18.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T3.1.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.18.2.1" class="ltx_text" style="font-size:80%;">Apr-2023</span></td>
<td id="S3.T3.1.18.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T3.1.18.3.1" class="ltx_text" style="font-size:80%;">1.5M</span></td>
</tr>
</tbody></table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>A list of available collections for alignment. </figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Release Time</span></td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">#Examples</span></td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.2.1.1" class="ltx_text" style="font-size:80%;">Summarize from Feedback&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib129" title="" class="ltx_ref">129</a><span id="S3.T4.1.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.2.2.1" class="ltx_text" style="font-size:80%;">Sep-2020</span></td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.2.3.1" class="ltx_text" style="font-size:80%;">193K</span></td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.3.1.1" class="ltx_text" style="font-size:80%;">SHP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib177" title="" class="ltx_ref">177</a><span id="S3.T4.1.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.3.2.1" class="ltx_text" style="font-size:80%;">Oct-2021</span></td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.3.3.1" class="ltx_text" style="font-size:80%;">385K</span></td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.4.1.1" class="ltx_text" style="font-size:80%;">WebGPT Comparisons&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="S3.T4.1.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.4.2.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.4.3.1" class="ltx_text" style="font-size:80%;">19K</span></td>
</tr>
<tr id="S3.T4.1.5" class="ltx_tr">
<td id="S3.T4.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.5.1.1" class="ltx_text" style="font-size:80%;">Stack Exchange Preferences&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib178" title="" class="ltx_ref">178</a><span id="S3.T4.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.5.2.1" class="ltx_text" style="font-size:80%;">Dec-2021</span></td>
<td id="S3.T4.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.5.3.1" class="ltx_text" style="font-size:80%;">10M</span></td>
</tr>
<tr id="S3.T4.1.6" class="ltx_tr">
<td id="S3.T4.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.6.1.1" class="ltx_text" style="font-size:80%;">HH-RLHF&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib170" title="" class="ltx_ref">170</a><span id="S3.T4.1.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.6.2.1" class="ltx_text" style="font-size:80%;">Apr-2022</span></td>
<td id="S3.T4.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.6.3.1" class="ltx_text" style="font-size:80%;">169K</span></td>
</tr>
<tr id="S3.T4.1.7" class="ltx_tr">
<td id="S3.T4.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.7.1.1" class="ltx_text" style="font-size:80%;">Sandbox Alignment Data&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib179" title="" class="ltx_ref">179</a><span id="S3.T4.1.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.7.2.1" class="ltx_text" style="font-size:80%;">May-2023</span></td>
<td id="S3.T4.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.7.3.1" class="ltx_text" style="font-size:80%;">169K</span></td>
</tr>
<tr id="S3.T4.1.8" class="ltx_tr">
<td id="S3.T4.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.8.1.1" class="ltx_text" style="font-size:80%;">CValues&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib180" title="" class="ltx_ref">180</a><span id="S3.T4.1.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.8.2.1" class="ltx_text" style="font-size:80%;">Jul-2023</span></td>
<td id="S3.T4.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.8.3.1" class="ltx_text" style="font-size:80%;">145K</span></td>
</tr>
<tr id="S3.T4.1.9" class="ltx_tr">
<td id="S3.T4.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S3.T4.1.9.1.1" class="ltx_text" style="font-size:80%;">PKU-SafeRLHF&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib181" title="" class="ltx_ref">181</a><span id="S3.T4.1.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T4.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.9.2.1" class="ltx_text" style="font-size:80%;">Oct-2023</span></td>
<td id="S3.T4.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S3.T4.1.9.3.1" class="ltx_text" style="font-size:80%;">330K</span></td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">Commonly Used Datasets for Fine-tuning</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">After pre-training,
it requires further fine-tuning LLMs to enhance the model capacity, which often involve two major steps, namely instruction tuning (supervised fine-tuning) and alignment tuning. In this section, we mainly focus on discussing the related available datasets for the two kinds of tuning approaches, and more algorithm details can be found in Section&nbsp;<a href="#S5" title="5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Instruction Tuning Datasets</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">After pre-training, instruction tuning (<em id="S3.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">a.k.a.,</em> supervised fine-tuning) is an important method to enhance or unlock specific abilities of LLMs (<em id="S3.SS3.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> instruction following). In this part, we introduce several widely used datasets for instruction tuning, and categorize them into three main types based on the construction method of formatted instruction instances, namely NLP task datasets, daily chat datasets and synthetic datasets.
We show their details in Table&nbsp;<a href="#S3.T3" title="TABLE III ‣ 3.2 Commonly Used Corpora for Pre-training ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p"><span id="S3.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">NLP Task Datasets.</span> This kind of datasets are formatted based on collected NLP task datasets&nbsp;(<em id="S3.SS3.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> text classification and summarization) with corresponding natural language task descriptions. In this category, P3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> and FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> are two widely used datasets for instruction tuning.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p"><math id="S3.SS3.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p3.1.m1.1a"><mo id="S3.SS3.SSS1.p3.1.m1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.1.m1.1b"><ci id="S3.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">P3</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> is composed of 170 English NLP datasets and 2,052 English prompt templates, where the input and output of each data example have been formatted with specific prompt templates for composing the training instance.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p"><math id="S3.SS3.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p4.1.m1.1a"><mo id="S3.SS3.SSS1.p4.1.m1.1.1" xref="S3.SS3.SSS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p4.1.m1.1b"><ci id="S3.SS3.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">FLAN</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> consists of 62 widely used NLP benchmarks in its original version. Recently, FLAN-v2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> is also proposed, which expands FLAN by mixing additional instruction datasets, including Muffin&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, NIV2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, T0-SF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>, <a href="#bib.bib185" title="" class="ltx_ref">185</a>, <a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>. Muffin contains 62 tasks from the original FLAN and additional 26 tasks, including conversation and code synthesis tasks. T0-SF is extracted from T0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> while ensuring no overlap with Muffin. NIV2 refers to the Natural-Instructions v2 dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, and CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>, <a href="#bib.bib185" title="" class="ltx_ref">185</a>, <a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> is a combination of nine reasoning tasks with corresponding chain-of-thought prompts and outputs.</p>
</div>
<div id="S3.SS3.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS1.p5.1" class="ltx_p"><span id="S3.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Daily Chat Datasets.</span> This kind of datasets are constructed based on real user conversations where queries are posed by humans and responses are mainly generated by human labelers or LLMs&nbsp;(<em id="S3.SS3.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> ChatGPT, GPT-4). The conversation types include open-ended generation, question answering, brainstorming, and chatting. In this category, ShareGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>, OpenAssistant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> and Dolly&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite> are three commonly used datasets for LLM fine-tuning.</p>
</div>
<div id="S3.SS3.SSS1.p6" class="ltx_para">
<p id="S3.SS3.SSS1.p6.1" class="ltx_p"><math id="S3.SS3.SSS1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p6.1.m1.1a"><mo id="S3.SS3.SSS1.p6.1.m1.1.1" xref="S3.SS3.SSS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p6.1.m1.1b"><ci id="S3.SS3.SSS1.p6.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">ShareGPT</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> is collected from a data collection platform where users can upload their conversations with ChatGPT or GPT-4 through the ShareGPT API. Currently, this dataset consists of approximately 90,000 conversations, including real instructions or inquiries from human and responses from ChatGPT.</p>
</div>
<div id="S3.SS3.SSS1.p7" class="ltx_para">
<p id="S3.SS3.SSS1.p7.1" class="ltx_p"><math id="S3.SS3.SSS1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p7.1.m1.1a"><mo id="S3.SS3.SSS1.p7.1.m1.1.1" xref="S3.SS3.SSS1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p7.1.m1.1b"><ci id="S3.SS3.SSS1.p7.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p7.1.1" class="ltx_emph ltx_font_italic">OpenAssistant</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> is a multilingual corpus containing 66,497 real-world conversation trees between human and AI assistant. Each conversation tree consists of multiple nodes, and each node represents the information generated by a role in the dialogue.
It spans 35 languages and includes 461,292 manually annotated quality ratings of responses.</p>
</div>
<div id="S3.SS3.SSS1.p8" class="ltx_para">
<p id="S3.SS3.SSS1.p8.1" class="ltx_p"><math id="S3.SS3.SSS1.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p8.1.m1.1a"><mo id="S3.SS3.SSS1.p8.1.m1.1.1" xref="S3.SS3.SSS1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p8.1.m1.1b"><ci id="S3.SS3.SSS1.p8.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p8.1.1" class="ltx_emph ltx_font_italic">Dolly</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite> is an English dataset comprising 15,000 human-generated data instances (prompt-response pairs) from Databricks.
This dataset covers seven domains outlined in the InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, including brainstorming, classification, closed-book quality assurance, generation, information extraction, open-book quality assurance, and summarization.</p>
</div>
<div id="S3.SS3.SSS1.p9" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS1.p9.1" class="ltx_p"><span id="S3.SS3.SSS1.p9.1.1" class="ltx_text ltx_font_bold">Synthetic Datasets.</span> This kind of datasets are typically constructed by instructing LLMs, based on pre-defined guidance rules or methods.
In this category,
Self-Instruct-52K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>, Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> and Baize&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> are three commonly used synthetic datasets for LLMs.</p>
</div>
<div id="S3.SS3.SSS1.p10" class="ltx_para">
<p id="S3.SS3.SSS1.p10.1" class="ltx_p"><math id="S3.SS3.SSS1.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p10.1.m1.1a"><mo id="S3.SS3.SSS1.p10.1.m1.1.1" xref="S3.SS3.SSS1.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p10.1.m1.1b"><ci id="S3.SS3.SSS1.p10.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p10.1.1" class="ltx_emph ltx_font_italic">Self-Instruct-52K</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> is an instruction dataset generated through the self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> method, consisting of 82,000 instances with 52,000 instructions. Concretely, the authors construct 175 seed instances, and then iteratively prompt the LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> to synthesize additional instructions based on randomly selected 8 instructions as reference.
Subsequently, the LLM is further instructed to generate instance inputs and their corresponding outputs based on the synthetic instructions, and finally obtain the Self-Instruct-52K dataset.</p>
</div>
<div id="S3.SS3.SSS1.p11" class="ltx_para">
<p id="S3.SS3.SSS1.p11.1" class="ltx_p"><math id="S3.SS3.SSS1.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p11.1.m1.1a"><mo id="S3.SS3.SSS1.p11.1.m1.1.1" xref="S3.SS3.SSS1.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p11.1.m1.1b"><ci id="S3.SS3.SSS1.p11.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p11.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p11.1.1" class="ltx_emph ltx_font_italic">Alpaca</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> is also a synthetic dataset based on the self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> method. It utilizes the <span id="S3.SS3.SSS1.p11.1.2" class="ltx_text ltx_font_typewriter">text-davinci-003</span> model on the 175 seed datasets from Self-Instruct-52K to obtain 52,000 new instructions and corresponding inputs and outputs. Moreover, 60% of the examples are pure instructions without the input part in the final dataset.</p>
</div>
<div id="S3.SS3.SSS1.p12" class="ltx_para">
<p id="S3.SS3.SSS1.p12.1" class="ltx_p"><math id="S3.SS3.SSS1.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS1.p12.1.m1.1a"><mo id="S3.SS3.SSS1.p12.1.m1.1.1" xref="S3.SS3.SSS1.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p12.1.m1.1b"><ci id="S3.SS3.SSS1.p12.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p12.1.m1.1c">\bullet</annotation></semantics></math> <em id="S3.SS3.SSS1.p12.1.1" class="ltx_emph ltx_font_italic">Baize</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances. To create Baize, a method called “self-chat”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Alignment Datasets</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences&nbsp;(<em id="S3.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> helpfulness, honesty, and harmlessness). In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, SHP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>, PKU-SafeRLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, Stack Exchange Preferences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> and Sandbox Alignment Data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>. We show their details in Table&nbsp;<a href="#S3.T4" title="TABLE IV ‣ 3.2 Commonly Used Corpora for Pre-training ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p"><math id="S3.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS2.p2.1.m1.1a"><mo id="S3.SS3.SSS2.p2.1.m1.1.1" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.1b"><ci id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">HH-RLHF</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> consists of around 169K instances, and can be divided into two parts that focus on the helpfulness and harmlessness of LLMs, respectively.
Each instance is an open-ended conversation between a crowdworker and a chat model, about seeking assistance, advice, or task completion.
The chat model provides two responses to each user query, and the more helpful or harmful responses will be chosen as the annotations.</p>
</div>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.1" class="ltx_p"><math id="S3.SS3.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS2.p3.1.m1.1a"><mo id="S3.SS3.SSS2.p3.1.m1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.1.m1.1b"><ci id="S3.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_bold">SHP</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> focuses on the helpfulness of responses. It comprises 385K collective human preferences over responses to questions/instructions across 18 diverse subject areas, spanning topics from cooking to legal advice. Each instance is a Reddit post containing a question or instruction and a pair of top-level comments, one of which is deemed as more preferable by Reddit users and the other one is deemed as less helpful.
Different from HH-RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, the data in SHP consists of naturally occurring and human-written responses.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<p id="S3.SS3.SSS2.p4.1" class="ltx_p"><math id="S3.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS2.p4.1.m1.1a"><mo id="S3.SS3.SSS2.p4.1.m1.1.1" xref="S3.SS3.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p4.1.m1.1b"><ci id="S3.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.SSS2.p4.1.1" class="ltx_text ltx_font_bold">PKU-SafeRLHF</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> encompasses more than 330K instances of expert comparison data, concentrating on the helpfulness and harmlessness. Each instance in the dataset includes a question and two responses, accompanied by safety labels for each response and two preference annotations between the two responses according to helpfulness and harmlessness.
The harmlessness of a response indicates its classification as risk-neutral across all 14 harm categories, while the helpfulness of a response is evaluated based on its effectiveness in addressing the question.</p>
</div>
<div id="S3.SS3.SSS2.p5" class="ltx_para">
<p id="S3.SS3.SSS2.p5.1" class="ltx_p"><math id="S3.SS3.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS2.p5.1.m1.1a"><mo id="S3.SS3.SSS2.p5.1.m1.1.1" xref="S3.SS3.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p5.1.m1.1b"><ci id="S3.SS3.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Stack Exchange Preferences</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> focuses on the helpfulness of answers. It comprises about 10M questions and answers from Stack Overflow. Each instance consists of a question and more than two corresponding answers. Each answer is annotated with a score calculated based on its votes and a label denoting whether it is selected.</p>
</div>
<div id="S3.SS3.SSS2.p6" class="ltx_para">
<p id="S3.SS3.SSS2.p6.1" class="ltx_p"><math id="S3.SS3.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.SSS2.p6.1.m1.1a"><mo id="S3.SS3.SSS2.p6.1.m1.1.1" xref="S3.SS3.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p6.1.m1.1b"><ci id="S3.SS3.SSS2.p6.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Sandbox Alignment Data</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> is an alignment dataset containing feedback from LLMs rather than human. It comes from a virtual interaction environment called SANDBOX, where the model simulates social interactions with other models and revise responses according to the feedback from other models. The dataset contains 169K instances, and each instance consists of a societal query, several responses, and corresponding ratings from other models.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span id="S3.SS4.1.1" class="ltx_text ltx_font_italic">Library Resource</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this part, we briefly introduce a series of available libraries for developing LLMs.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mo id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Transformers</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite> is an open-source Python library for building models using the Transformer architecture, which is developed and maintained by Hugging Face.
It has a simple and user-friendly API, making it easy to use and customize various pre-trained models. It is a powerful library with a large and active community of users and developers who regularly update and improve the models and algorithms.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mo id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">DeepSpeed</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> is a deep learning optimization library (compatible with PyTorch) developed by Microsoft, which has been used to train a number of LLMs, such as MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> and BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. It provides the support of various optimization techniques for distributed training, such as memory optimization (ZeRO technique, gradient checkpointing), and pipeline parallelism.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><mo id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">Megatron-LM</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> is a deep learning library developed by NVIDIA for training large-scale language models. It also provides rich optimization techniques for distributed training, including model and data parallelism, mixed-precision training, and FlashAttention. These optimization techniques can largely improve the training efficiency and speed, enabling efficient distributed training across GPUs.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p"><math id="S3.SS4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p5.1.m1.1a"><mo id="S3.SS4.p5.1.m1.1.1" xref="S3.SS4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.1.m1.1b"><ci id="S3.SS4.p5.1.m1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p5.1.1" class="ltx_text ltx_font_bold">JAX</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite> is a Python library for high-performance machine learning algorithms developed by Google, allowing users to easily perform computations on arrays with hardware acceleration (<em id="S3.SS4.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPU or TPU). It enables efficient computation on various devices and also supports several featured functions, such as automatic differentiation and just-in-time compilation.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p"><math id="S3.SS4.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p6.1.m1.1a"><mo id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><ci id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p6.1.1" class="ltx_text ltx_font_bold">Colossal-AI</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite> is a deep learning library developed by HPC-AI Tech for training large-scale AI models. It is implemented based on PyTorch and supports a rich collection of parallel training strategies.
Furthermore, it can also optimize heterogeneous memory management with methods proposed by PatrickStar&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>.
Recently, a ChatGPT-like model called ColossalChat&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> has been publicly released with two versions (7B and 13B), which are developed
using Colossal-AI based on LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.1" class="ltx_p"><math id="S3.SS4.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p7.1.m1.1a"><mo id="S3.SS4.p7.1.m1.1.1" xref="S3.SS4.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.1.m1.1b"><ci id="S3.SS4.p7.1.m1.1.1.cmml" xref="S3.SS4.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p7.1.1" class="ltx_text ltx_font_bold">BMTrain</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite> is an efficient library developed by OpenBMB for training models with large-scale parameters in a distributed manner, which emphasizes code simplicity, low resource, and high availability. BMTrain has already incorporated several common LLMs (<em id="S3.SS4.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Flan-T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> and GLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>) into its ModelCenter, where developers can use these models directly.</p>
</div>
<div id="S3.SS4.p8" class="ltx_para">
<p id="S3.SS4.p8.1" class="ltx_p"><math id="S3.SS4.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p8.1.m1.1a"><mo id="S3.SS4.p8.1.m1.1.1" xref="S3.SS4.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.1.m1.1b"><ci id="S3.SS4.p8.1.m1.1.1.cmml" xref="S3.SS4.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p8.1.1" class="ltx_text ltx_font_bold">FastMoE</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> is a specialized training library for MoE (<em id="S3.SS4.p8.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> mixture-of-experts) models. It is developed based on PyTorch, prioritizing both efficiency and user-friendliness in its design. FastMoE simplifies the process of transferring Transformer models to MoE models and supports both data parallelism and model parallelism during training.</p>
</div>
<div id="S3.SS4.p9" class="ltx_para">
<p id="S3.SS4.p9.1" class="ltx_p"><math id="S3.SS4.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p9.1.m1.1a"><mo id="S3.SS4.p9.1.m1.1.1" xref="S3.SS4.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p9.1.m1.1b"><ci id="S3.SS4.p9.1.m1.1.1.cmml" xref="S3.SS4.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p9.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p9.1.1" class="ltx_text ltx_font_bold">vLLM</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite> is a fast, memory efficient, and easy-to-use library for LLM inference and serving.
To enable fast inference, it is specially optimized with high serving throughput, effective attention memory management using PagedAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite>, continuous batching, and optimized CUDA kernels.
Furthermore, vLLM also supports various decoding algorithms, tensor parallelism and streaming outputs.
To ease the integration with other systems, vLLM is friendly to the use of HuggingFace models, and also provide OpenAI-compatible API servers.</p>
</div>
<div id="S3.SS4.p10" class="ltx_para">
<p id="S3.SS4.p10.1" class="ltx_p"><math id="S3.SS4.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p10.1.m1.1a"><mo id="S3.SS4.p10.1.m1.1.1" xref="S3.SS4.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p10.1.m1.1b"><ci id="S3.SS4.p10.1.m1.1.1.cmml" xref="S3.SS4.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p10.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p10.1.1" class="ltx_text ltx_font_bold">DeepSpeed-MII</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> is also a memory efficient Python library developed by DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. It aims to democratize LLMs inference by prioritizing high throughput, low latency, and cost-effectiveness. DeepSpeed-MII achieves accelerated text generation inference by leveraging four essential technologies: blocked KV caching, continuous batching, dynamic SplitFuse, and high-performance CUDA Kernels. It currently supports over 13,000 models across three popular model architectures, such as LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, Mistral&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite>, and OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>.</p>
</div>
<div id="S3.SS4.p11" class="ltx_para">
<p id="S3.SS4.p11.1" class="ltx_p"><math id="S3.SS4.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS4.p11.1.m1.1a"><mo id="S3.SS4.p11.1.m1.1.1" xref="S3.SS4.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p11.1.m1.1b"><ci id="S3.SS4.p11.1.m1.1.1.cmml" xref="S3.SS4.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p11.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS4.p11.1.1" class="ltx_text ltx_font_bold">DeepSpeed-Chat</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> is a fast, cost-effective, and easy-to-use system framework that enables the integration of the complete RLHF process during model training. It is featured by three major functionalities: (1) it simplifies the training and inference process for ChatGPT-like models, enabling using a simple script to implement multiple training or inference steps;
(2) it replicates the training mode of InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and provides a complete pipeline for three training steps&nbsp;(<em id="S3.SS4.p11.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> SFT, reward model fine-tuning, and RLHF); (3) it integrates the training engine and inference engine of Deepspeed into a unified hybrid engine&nbsp;(Deepspeed HE) for RLHF training, which enables seamless switch between training and inference modes, and leveraging various optimizations from DeepSpeed Inference.</p>
</div>
<div id="S3.SS4.p12" class="ltx_para">
<p id="S3.SS4.p12.1" class="ltx_p">In addition to the above library resources, existing deep learning frameworks (<em id="S3.SS4.p12.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> PyTorch&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>, TensorFlow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>, MXNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>, PaddlePaddle&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite>, MindSpore&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> and OneFlow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>) have also provided the support for parallel algorithms, which are commonly used for training large-scale models.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2303.18223/assets/x7.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Ratios of various data sources in the pre-training data for existing LLMs. </figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Pre-training</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Pre-training establishes the basis of the abilities of LLMs. By pre-training on large-scale corpora, LLMs can acquire essential language understanding and generation skills&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
In this process, the scale and quality of the pre-training corpus are critical for LLMs to attain powerful capabilities.
Furthermore, to effectively pre-train LLMs, model architectures, acceleration methods, and optimization techniques need to be well designed. In what follows, we first discuss the data collection and processing in Section&nbsp;<a href="#S4.SS1" title="4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, then introduce the commonly used model architectures in Section&nbsp;<a href="#S4.SS2" title="4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, and finally present the training techniques to stably and efficiently optimize LLMs in Section&nbsp;<a href="#S4.SS3" title="4.3 Model Training ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Data Collection and Preparation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Compared with small-scale language models, LLMs have a stronger demand for high-quality data for model pre-training, and their model capacities largely rely on the pre-training corpus and how it has been preprocessed. In this part, we discuss the collection and processing of pre-training data, including data sources, preprocessing methods, and important analysis of how pre-training data affects the performance of LLMs.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Data Source</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">To develop a capable LLM, it is key to collect a large amount of natural language corpus from various data sources.
Existing LLMs mainly leverage a mixture of diverse public textual datasets as the pre-training corpus.
Figure&nbsp;<a href="#S3.F6" title="Figure 6 ‣ 3.4 Library Resource ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the distribution of the sources of pre-training data for a number of representative LLMs.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The source of pre-training corpus can be broadly categorized into two types: general data and specialized data. General data, such as webpages, books, and conversational text, is utilized by most LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> due to its large, diverse, and accessible nature, which can enhance the language modeling and generalization abilities of LLMs. In light of the impressive generalization capabilities exhibited by LLMs, there are also studies that extend their pre-training corpus to more specialized datasets, such as multilingual data, scientific data, and code, endowing LLMs with specific task-solving capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>. In what follows, we describe these two types of pre-training data sources and their effects on LLMs. For a detailed introduction to the commonly used corpus, one can refer to Section&nbsp;<a href="#S3.SS2" title="3.2 Commonly Used Corpora for Pre-training ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p"><span id="S4.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">General Text Data.</span>
As we can see in Figure&nbsp;<a href="#S3.F6" title="Figure 6 ‣ 3.4 Library Resource ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the vast majority of LLMs adopt general-purpose pre-training data, such as webpages, books, and conversational text, which provides rich text sources on a variety of topics.
Next, we briefly summarize three important kinds of general data.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para">
<p id="S4.SS1.SSS1.p4.1" class="ltx_p"><math id="S4.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS1.p4.1.m1.1a"><mo id="S4.SS1.SSS1.p4.1.m1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.1.m1.1b"><ci id="S4.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Webpages.</em> Owing to the proliferation of the Internet, various types of data have been created, which enables LLMs to gain diverse linguistic knowledge and enhance their generalization capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. For convenient use of these data resources, a large amount of data is crawled from the web in previous work, such as CommonCrawl&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. However, the crawled web data tends to contain both high-quality text, such as Wikipedia and low-quality text, like spam mail, thus it is important to filter and process webpages for improving the data quality.</p>
</div>
<div id="S4.SS1.SSS1.p5" class="ltx_para">
<p id="S4.SS1.SSS1.p5.1" class="ltx_p"><math id="S4.SS1.SSS1.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS1.p5.1.m1.1a"><mo id="S4.SS1.SSS1.p5.1.m1.1.1" xref="S4.SS1.SSS1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p5.1.m1.1b"><ci id="S4.SS1.SSS1.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">Conversation text.</em> Conversation data can enhance the conversational competence of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> and potentially improve their performance on a range of question-answering tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Researchers can utilize subsets of public conversation corpus (<em id="S4.SS1.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> PushShift.io Reddit corpus)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> or collect conversation data from online social media.
Since online conversational data often involves discussions among multiple participants, an effective processing way is to transform a conversation into a tree structure, where the utterance is linked to the one it responds to.
In this way, the multi-party conversation tree can be divided into multiple sub-conversations, which can be collected in the pre-training corpus.
Furthermore, a potential risk is that the excessive integration of dialogue data into LLMs may result in a side effect&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>: declarative instructions and direct interrogatives are erroneously perceived as the beginning of conversations, thus leading to a decline in the efficacy of the instructions.</p>
</div>
<div id="S4.SS1.SSS1.p6" class="ltx_para">
<p id="S4.SS1.SSS1.p6.1" class="ltx_p"><math id="S4.SS1.SSS1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS1.p6.1.m1.1a"><mo id="S4.SS1.SSS1.p6.1.m1.1.1" xref="S4.SS1.SSS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p6.1.m1.1b"><ci id="S4.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">Books.</em> Compared to other corpus, books provide an important source of formal long texts, which are potentially beneficial for LLMs to learn linguistic knowledge, model long-term dependency, and generate narrative and coherent texts.
To obtain open-source book data, existing studies usually adopt the Books3 and Bookcorpus2 datasets, which are available in the Pile dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>.</p>
</div>
<div id="S4.SS1.SSS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p7.1" class="ltx_p"><span id="S4.SS1.SSS1.p7.1.1" class="ltx_text ltx_font_bold">Specialized Text Data.</span> Specialized datasets are useful to improve the specific capabilities of LLMs on downstream tasks.
Next, we introduce three kinds of specialized data.</p>
</div>
<div id="S4.SS1.SSS1.p8" class="ltx_para">
<p id="S4.SS1.SSS1.p8.1" class="ltx_p"><math id="S4.SS1.SSS1.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS1.p8.1.m1.1a"><mo id="S4.SS1.SSS1.p8.1.m1.1.1" xref="S4.SS1.SSS1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p8.1.m1.1b"><ci id="S4.SS1.SSS1.p8.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS1.p8.1.1" class="ltx_emph ltx_font_italic">Multilingual text.</em>
In addition to the text in the target language,
integrating a multilingual corpus can enhance the multilingual abilities of language understanding and generation.
For example, BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> and PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora. FLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> mixes Chinese and English corpora in nearly equal proportions. These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-the-art models that are fine-tuned on the corpus in the target language(s).</p>
</div>
<div id="S4.SS1.SSS1.p9" class="ltx_para">
<p id="S4.SS1.SSS1.p9.1" class="ltx_p"><math id="S4.SS1.SSS1.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS1.p9.1.m1.1a"><mo id="S4.SS1.SSS1.p9.1.m1.1.1" xref="S4.SS1.SSS1.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p9.1.m1.1b"><ci id="S4.SS1.SSS1.p9.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS1.p9.1.1" class="ltx_emph ltx_font_italic">Scientific text.</em>
The exploration of science by humans has been witnessed by the increasing growth of scientific publications.
In order to enhance the understanding of scientific knowledge for LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>,
it is useful to incorporate a scientific corpus for model pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>. By pre-training on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite>.
To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources.
Due to the complex nature of data in scientific fields, such as mathematical symbols and protein sequences, specific tokenization and preprocessing techniques are usually required to transform these different formats of data into a unified form that can be processed by language models.</p>
</div>
<div id="S4.SS1.SSS1.p10" class="ltx_para">
<p id="S4.SS1.SSS1.p10.1" class="ltx_p"><math id="S4.SS1.SSS1.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS1.p10.1.m1.1a"><mo id="S4.SS1.SSS1.p10.1.m1.1.1" xref="S4.SS1.SSS1.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p10.1.m1.1b"><ci id="S4.SS1.SSS1.p10.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS1.p10.1.1" class="ltx_emph ltx_font_italic">Code.</em> Program synthesis has been widely studied in the research community&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib205" title="" class="ltx_ref">205</a>, <a href="#bib.bib206" title="" class="ltx_ref">206</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>, especially the use of PLMs trained on code&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>.
However, it remains challenging for these PLMs (<em id="S4.SS1.SSS1.p10.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-J&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>) to generate high-quality and accurate programs.
Recent studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite> have found that training LLMs on a vast code corpus can lead to a substantial improvement in the quality of the synthesized programs. The generated programs can successfully pass expert-designed unit-test cases&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> or solve competitive programming questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>. In general, two types of code corpora are commonly used for pre-training LLMs. The first source is from programming question answering communities like Stack Exchange&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite>. The second source is from public software repositories such as GitHub&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, where code data (including comments and docstrings) are collected for utilization.
Compared to natural language text, code is in the format of a programming language, corresponding to long-range dependencies and accurate execution logic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>.
A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> also speculates that training on code might be a source of complex reasoning abilities (<em id="S4.SS1.SSS1.p10.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> chain-of-thought ability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>).
Furthermore, it has been shown that formatting reasoning tasks into code can help LLMs generate more accurate results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Data Preprocessing</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">After collecting a large amount of text data, it is essential to preprocess the data for constructing the pre-training corpus, especially removing noisy, redundant, irrelevant, and potentially toxic data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>, which may largely affect the capacity and performance of LLMs.
To facilitate the data processing,
a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> proposes a useful data processing system for LLMs, named Data-Juicer, which provides over 50 processing operators and tools.
In this part, we review the detailed data preprocessing strategies to improve the quality of the collected data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.
A typical pipeline of preprocessing the pre-training data for LLMs has been illustrated in Figure&nbsp;<a href="#S4.F7" title="Figure 7 ‣ 4.1.2 Data Preprocessing ‣ 4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2303.18223/assets/x8.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>An illustration of a typical data preprocessing pipeline for pre-training large language models. </figcaption>
</figure>
<div id="S4.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p"><span id="S4.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Quality Filtering.</span>
To remove low-quality data from the collected corpus, existing work generally adopts two approaches: (1) classifier-based, and (2) heuristic-based.
The former approach trains a selection classifier based on high-quality texts and leverages it to identify and filter out low-quality data.
Typically, these methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> train a binary classifier with well-curated data (<em id="S4.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Wikipedia pages) as positive instances and sample candidate data as negative instances, and predict the score that measures the quality of each data example.
However, several studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> find that a classifier-based approach may result in the unintentional removal of high-quality texts in dialectal, colloquial, and sociolectal languages, which potentially leads to bias in the pre-training corpus and diminishes the corpus diversity.
As the second approach, several studies, such as BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> and Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, employ heuristic-based approaches to eliminate low-quality texts through a set of well-designed rules, which can be summarized as follows:</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><em id="S4.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Language based filtering.</em> If a LLM would be mainly used in the tasks of certain languages, the text in other languages can be filtered.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Metric based filtering.</em> Evaluation metrics about the generated texts, <em id="S4.I1.i2.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> perplexity, can be employed to detect and remove unnatural sentences.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><em id="S4.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Statistic based filtering.</em> Statistical features of a corpus, <em id="S4.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> the punctuation distribution, symbol-to-word ratio, and sentence length, can be utilized to measure the text quality and filter the low-quality data.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><em id="S4.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Keyword based filtering.</em> Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p"><span id="S4.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">De-duplication.</span>
Existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> has found that duplicate data in a corpus would reduce the diversity of language models, which may cause the training process to become unstable and thus affect the model performance.
Therefore, it is necessary to de-duplicate the pre-training corpus.
Specially, de-duplication can be performed at different granularities, including sentence-level, document-level, and dataset-level de-duplication.
First, low-quality sentences that contain repeated words and phrases should be removed, as they may introduce repetitive patterns in language modeling&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite>. At the document level, existing studies mostly rely on the overlap ratio of surface features (<em id="S4.SS1.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> words and <math id="S4.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS1.SSS2.p4.1.m1.1a"><mi id="S4.SS1.SSS2.p4.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.m1.1b"><ci id="S4.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.m1.1c">n</annotation></semantics></math>-grams overlap) between documents to detect and remove duplicate documents containing similar contents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite>.
Furthermore, to avoid the dataset contamination problem, it is also crucial to prevent the overlap between the training and evaluation sets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, by removing the possible duplicate texts from the training set.
It has been shown that the three levels of de-duplication are useful to improve the training of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, which should be jointly used in practice.</p>
</div>
<div id="S4.SS1.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p5.1" class="ltx_p"><span id="S4.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Privacy Reduction.</span>
The majority of pre-training text data is obtained from web sources, including user-generated content involving sensitive or personal information, which may increase the risk of privacy breaches&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite>.
Thus, it is necessary to remove the <em id="S4.SS1.SSS2.p5.1.2" class="ltx_emph ltx_font_italic">personally identifiable information&nbsp;(PII)</em> from the pre-training corpus. One direct and effective approach is to employ rule-based methods, such as keyword spotting, to detect and remove PII such as names, addresses, and phone numbers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>. Furthermore, researchers also find that the vulnerability of LLMs under privacy attacks can be attributed to the presence of duplicate PII data in the pre-training corpus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite>. Therefore, de-duplication can also reduce privacy risks to some extent.</p>
</div>
<div id="S4.SS1.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p6.1" class="ltx_p"><span id="S4.SS1.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Tokenization.</span>
Tokenization is also a crucial step for data preprocessing.
It aims to segment raw text into sequences of individual tokens, which are subsequently used as the inputs of LLMs. In traditional NLP research (<em id="S4.SS1.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> sequence labeling with conditional random fields&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib220" title="" class="ltx_ref">220</a>]</cite>), word-based tokenization is the predominant approach, which is more aligned with human’s language cognition. However, word-based tokenization can yield different segmentation results for the same input in some languages (<em id="S4.SS1.SSS2.p6.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> Chinese word segmentation), generate a huge word vocabulary containing many low-frequency words, and also suffer from the “<em id="S4.SS1.SSS2.p6.1.4" class="ltx_emph ltx_font_italic">out-of-vocabulary</em>” issue. Thus, several neural network models employ <em id="S4.SS1.SSS2.p6.1.5" class="ltx_emph ltx_font_italic">character</em> as the minimum unit to derive the word representation (<em id="S4.SS1.SSS2.p6.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> a CNN word encoder in ELMo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>). Recently, <em id="S4.SS1.SSS2.p6.1.7" class="ltx_emph ltx_font_italic">subword tokenizers</em> have been widely used in Transformer based language models, typically including Byte-Pair Encoding tokenization, WordPiece tokenization and Unigram tokenization. HuggingFace has maintained an excellent online NLP course on tokenizer<span id="footnote22" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span>https://huggingface.co/learn/nlp-course/chapter6</span></span></span> with running examples, and we refer to the beginners to this course. Next, we briefly describe the three representative tokenization methods.</p>
</div>
<div id="S4.SS1.SSS2.p7" class="ltx_para">
<p id="S4.SS1.SSS2.p7.1" class="ltx_p"><math id="S4.SS1.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS2.p7.1.m1.1a"><mo id="S4.SS1.SSS2.p7.1.m1.1.1" xref="S4.SS1.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p7.1.m1.1b"><ci id="S4.SS1.SSS2.p7.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Byte-Pair Encoding&nbsp;(BPE) tokenization</em>. BPE was originally proposed as a general data compression algorithm in 1994&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib221" title="" class="ltx_ref">221</a>]</cite>, and then adapted to NLP for tokenization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>. It starts with a set of basic symbols (<em id="S4.SS1.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> the alphabets and boundary characters), and iteratively combine frequent pairs of two consecutive tokens in the corpus as new tokens (called <em id="S4.SS1.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">merge</em>). For each merge, the selection criterion is based on the co-occurrence frequency of two contiguous tokens: the top frequent pair would be selected. The merge process continues until it reaches the predefined size.
Further, Byte-level BPE has been used to improve the tokenization quality for multilingual corpus (<em id="S4.SS1.SSS2.p7.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> the text containing non-ASCII characters) by considering <em id="S4.SS1.SSS2.p7.1.5" class="ltx_emph ltx_font_italic">bytes</em> as the basic symbols for merge. Representative language models with this tokenization approach include GPT-2, BART, and LLaMA.</p>
</div>
<div id="S4.SS1.SSS2.p8" class="ltx_para">
<p id="S4.SS1.SSS2.p8.1" class="ltx_p"><math id="S4.SS1.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS2.p8.1.m1.1a"><mo id="S4.SS1.SSS2.p8.1.m1.1.1" xref="S4.SS1.SSS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p8.1.m1.1b"><ci id="S4.SS1.SSS2.p8.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS2.p8.1.1" class="ltx_emph ltx_font_italic">WordPiece tokenization</em>. WordPiece was a Google internal subword tokenization algorithm.
It was originally proposed by Google in developing voice search systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite>. Then, it was used in the neural machine translation system in 2016&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite>, and was adopted as the word tokenizer for BERT in 2018&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. WordPiece has a very similar idea with BPE by iteratively merging consecutive tokens, whereas taking a slightly different selection criterion for the merge. To conduct the merge, it first trains a language model and employs it to score all possible pairs. Then, at each merge, it selects the pair that leads to the most increase in the likelihood of training data.
Since Google has’t released the official implementation of the WordPiece algorithm, HuggingFace gives a more intuitive selection measure in its online NLP course: a pair is scored by dividing the co-occurrence count by the product of the occurrence counts of two tokens in the pair based on training corpus.</p>
</div>
<div id="S4.SS1.SSS2.p9" class="ltx_para">
<p id="S4.SS1.SSS2.p9.1" class="ltx_p"><math id="S4.SS1.SSS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS2.p9.1.m1.1a"><mo id="S4.SS1.SSS2.p9.1.m1.1.1" xref="S4.SS1.SSS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p9.1.m1.1b"><ci id="S4.SS1.SSS2.p9.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS2.p9.1.1" class="ltx_emph ltx_font_italic">Unigram tokenization</em>.
Unlike BPE and WordPiece, Unigram tokenization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite> starts with a sufficiently large set of possible substrings or subtokens for a corpus, and iteratively removes the tokens in the current vocabulary until the expected vocabulary size is reached.
As the selection criterion, it calculates the yielded increase in the likelihood of training corpus by assuming that some token was removed from current vocabulary.
This step is conducted based on a trained unigram language model. To estimate the unigram language model, it adopts
an expectation–maximization&nbsp;(EM) algorithm: at each iteration, we first find the currently optimal tokenization of words based on the old language model, and then re-estimate the probabilities of unigrams to update the language model. During this procedure, dynamic programming algorithms (<em id="S4.SS1.SSS2.p9.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> the Viterbi algorithm) are used to efficiently find the optimal decomposition way of a word given the language model.
Representative models that adopt this tokenization approach include T5 and mBART.</p>
</div>
<div id="S4.SS1.SSS2.p10" class="ltx_para">
<p id="S4.SS1.SSS2.p10.1" class="ltx_p">Although it is expedient to leverage an existing tokenizer (<em id="S4.SS1.SSS2.p10.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> and GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> utilize the tokenizer of GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>), using a tokenizer specially designed for the pre-training corpus can be highly beneficial&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, especially for the corpus that consists of diverse domains, languages, and formats.
Therefore, recent LLMs often train the customized tokenizers specially for the pre-training corpus with the SentencePiece library&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite>, which includes Byte-level BPE and Unigram tokenization.
A note is that normalization techniques in BPE, such as NFKC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite>, may degrade the tokenization performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. 
When extending existing LLMs (<em id="S4.SS1.SSS2.p10.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> continual pre-training or instruction tuning), we should be also aware of the potential side effect with customized tokenizers.
For example, LLaMA trains
the BPE tokenizer based on a pre-training corpus mainly consisting of English texts, and the derived vocabulary might be less capable in processing non-English data, <em id="S4.SS1.SSS2.p10.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> taking longer inference latency to generate Chinese texts.</p>
</div>
<div id="S4.SS1.SSS2.p11" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p11.1" class="ltx_p"><span id="S4.SS1.SSS2.p11.1.1" class="ltx_text ltx_font_bold">Discussion on Effect of Data Quality.</span> For pre-training, the quality of pre-training data is vital to the model capacities of LLMs. Existing work has shown that pre-training on the low-quality corpus, such as noisy, toxic, and duplicate data, would largely hurt the performance of models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib216" title="" class="ltx_ref">216</a>, <a href="#bib.bib219" title="" class="ltx_ref">219</a>, <a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite>.
Recent studies, such as T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, GLaM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, and Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, have investigated the influence of data quality on the LLMs’ capacities.
By comparing the performance of models trained on the filtered and unfiltered corpus, they have reached the similar conclusion that pre-training LLMs on cleaned data can improve the model performance. More specifically, the duplication of data may result in “<em id="S4.SS1.SSS2.p11.1.2" class="ltx_emph ltx_font_italic">double descent</em>” (referring to the phenomenon of performance initially deteriorating and subsequently improving)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>, <a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite>, or even overwhelm the training process&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite>.
In addition, it has been shown that duplicate data degrades the ability of LLMs to copy from the context, which might further affect the generalization capacity of LLMs using in-context learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite>.
Therefore, as suggested in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>, it is essential to utilize preprocessing methods like quality filtering, toxic filtering and deduplication to carefully clean the pre-training corpus (as illustrated in Section&nbsp;<a href="#S4.SS1.SSS2" title="4.1.2 Data Preprocessing ‣ 4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>),  to improve stability of the training process and avoid affecting the model performance.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2303.18223/assets/x9.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>An illustration of data scheduling for pre-training LLMs. </figcaption>
</figure>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Data Scheduling</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">After data preprocessing, it is essential to design suitable strategies to schedule these multi-source data for pre-training a capable LLM. Generally, two key aspects should be paid close attention for data scheduling: the proportion of each data source (<em id="S4.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">data mixture</em>), and the order in which each data source is scheduled for training (<em id="S4.SS1.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">data curriculum</em>).
Next, we discuss the two aspects in detail. An illustration of data scheduling has been presented in Figure&nbsp;<a href="#S4.F8" title="Figure 8 ‣ 4.1.2 Data Preprocessing ‣ 4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p"><span id="S4.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Data Mixture.</span>
Since each kind of data source is closely related to the development of certain capacities for LLMs (referring to the discussions in Section&nbsp;<a href="#S4.SS1" title="4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), it is important to set a suitable distribution to mix these data.
The data mixture is generally set in a global level (<em id="S4.SS1.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> the distribution of the entire pre-training data), and can be also locally set to varied proportions at different training stages.
During pre-training, data samples from different sources would be selected according to the mixture proportions: more data will be sampled from a data source with a larger weight.
Typically, existing LLMs such as LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> may employ upsampling or downsampling on the full data of each source to create specific data mixtures as pre-training data.
As Figure&nbsp;<a href="#S3.F6" title="Figure 6 ‣ 3.4 Library Resource ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates, existing LLMs use different data mixtures to construct the pre-training data.
As a representative model, the pre-training data of LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> mainly consists of webpages (over 80%), alongside 6.5% of code-heavy data from GitHub and StackExchange, 4.5% from books, and 2.5% of scientific data sourced from arXiv, which has become an important reference for training general-purpose LLMs.
Furthermore, special data mixtures can be used to facilitate different purposes. For example, Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> is trained on pure webpages, and CodeGen&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> largely increases the amount of code data.
In practice, data mixture is often determined empirically, and we summarize several common strategies for finding an effective data mixture as follows:</p>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p"><math id="S4.SS1.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS3.p3.1.m1.1a"><mo id="S4.SS1.SSS3.p3.1.m1.1.1" xref="S4.SS1.SSS3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p3.1.m1.1b"><ci id="S4.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS3.p3.1.1" class="ltx_emph ltx_font_italic">Increasing the diversity of data sources.</em>
Recent studies have empirically shown that training on excessive data about a certain domain would degrade the generalization capability of LLMs on other domains&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. In contrast, increasing the data source heterogeneity (<em id="S4.SS1.SSS3.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> including diverse data sources) is critical for improving the downstream performance of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>, <a href="#bib.bib229" title="" class="ltx_ref">229</a>, <a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite>.
To further examine the effect of different data sources, some studies have conducted ablation experiments by removing each data source one by one, and pre-train LLMs with specially curated datasets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>. It has been shown that dropping data sources with high heterogeneity (<em id="S4.SS1.SSS3.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> webpages) impacts LLM’s abilities more severely than dropping sources with low heterogeneity (<em id="S4.SS1.SSS3.p3.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> academic corpus).</p>
</div>
<div id="S4.SS1.SSS3.p4" class="ltx_para">
<p id="S4.SS1.SSS3.p4.1" class="ltx_p"><math id="S4.SS1.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS3.p4.1.m1.1a"><mo id="S4.SS1.SSS3.p4.1.m1.1.1" xref="S4.SS1.SSS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p4.1.m1.1b"><ci id="S4.SS1.SSS3.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS3.p4.1.1" class="ltx_emph ltx_font_italic">Optimizing data mixtures.</em>
In addition to manually setting the data mixtures, several studies have proposed to optimize the data mixtures for improving the model pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib231" title="" class="ltx_ref">231</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> or those that provide positive influences on downstream task performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>]</cite>. Further, to reduce the reliance of target tasks, DoReMi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood between the two models are observed. Finally, the learned domain weights of the proxy model are applied to train a much larger LLM.
In a more simple way, one can train several small language models with different data mixtures, and select the data mixture that leads to the most desirable performance.
However, an assumption made in this approach is, when trained in a similar way, small models would resemble with large models in model abilities or behaviors, which may not always hold in practice.</p>
</div>
<div id="S4.SS1.SSS3.p5" class="ltx_para">
<p id="S4.SS1.SSS3.p5.1" class="ltx_p"><math id="S4.SS1.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS3.p5.1.m1.1a"><mo id="S4.SS1.SSS3.p5.1.m1.1.1" xref="S4.SS1.SSS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p5.1.m1.1b"><ci id="S4.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS3.p5.1.1" class="ltx_emph ltx_font_italic">Specializing the targeted abilities.</em>
The model capacities of LLMs heavily rely on data selection and mixture, and one can boost the proportions of specific data sources to enhance certain model abilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>.
For example,
the mathematical reasoning and coding abilities can be specially enhanced by training with more mathematical texts and code data, respectively.
Furthermore, experimental results on the LAMBADA dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> show that increasing the proportion of books data can improve the model capacity in capturing long-term dependencies from text, and increasing the proportion of the C4 dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> leads to performance improvement on the C4 validation dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>.
Generally, it is important to identify more implicit relations between data sources and model abilities. 
To enhance specific skills such as mathematics and coding in LLMs, or to develop specialized LLMs, a practical way is to employ a multi-stage training approach, <em id="S4.SS1.SSS3.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> general and skill-specific data can be scheduled at two consecutive stages. This approach of training LLMs on varying sources or proportions of data across multiple stages is also known as “data curriculum”, which will be introduced below.</p>
</div>
<div id="S4.SS1.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p6.1" class="ltx_p"><span id="S4.SS1.SSS3.p6.1.1" class="ltx_text ltx_font_bold">Data Curriculum.</span>

After preparing the data mixture, it is important to schedule the order that specific data is presented to LLMs for pre-training.
It has been shown that, in some cases, to learn a certain skill, learning in a skill-set sequence (<em id="S4.SS1.SSS3.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> basic skills <math id="S4.SS1.SSS3.p6.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p6.1.m1.1a"><mo stretchy="false" id="S4.SS1.SSS3.p6.1.m1.1.1" xref="S4.SS1.SSS3.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p6.1.m1.1b"><ci id="S4.SS1.SSS3.p6.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p6.1.m1.1c">\rightarrow</annotation></semantics></math> target skill) outperforms direct learning from a corpus focused solely on the target skill&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>, <a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite>.
Following the idea of curriculum learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib236" title="" class="ltx_ref">236</a>]</cite>, <em id="S4.SS1.SSS3.p6.1.3" class="ltx_emph ltx_font_italic">data curriculum</em> has been proposed and widely used in model pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>, <a href="#bib.bib237" title="" class="ltx_ref">237</a>, <a href="#bib.bib235" title="" class="ltx_ref">235</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite>. It aims to organize different parts of pre-training data for LLMs in a specific order, <em id="S4.SS1.SSS3.p6.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> starting with easy/general examples and progressively introducing more challenging/specialized ones.
More generally,
it can broadly refer to the adaptive adjustment of data proportions for different sources during pre-training.
Existing work about data curriculum mainly focuses on continual pre-training, such as specialized coding LLMs (<em id="S4.SS1.SSS3.p6.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> CodeLLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite>) or long context LLMs (<em id="S4.SS1.SSS3.p6.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> LongLLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite>).
However, it still lacks of more detailed report about data curriculum for general-purpose LLMs (<em id="S4.SS1.SSS3.p6.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> LLaMA) in the literature.
To determine data curriculum, a practical approach is to monitor the development of key abilities of LLMs based on specially constructed evaluation benchmarks, and then adaptively adjust the data mixture during pre-training.
Next, we take three common abilities as examples to introduce how the concept of data curriculum<span id="footnote23" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span>We utilize the symbol “<math id="footnote23.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="footnote23.m1.1b"><mo stretchy="false" id="footnote23.m1.1.1" xref="footnote23.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote23.m1.1c"><ci id="footnote23.m1.1.1.cmml" xref="footnote23.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote23.m1.1d">\rightarrow</annotation></semantics></math>” to represent the data order in data curriculum. For example, “2T webpage tokens <math id="footnote23.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="footnote23.m2.1b"><mo stretchy="false" id="footnote23.m2.1.1" xref="footnote23.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote23.m2.1c"><ci id="footnote23.m2.1.1.cmml" xref="footnote23.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote23.m2.1d">\rightarrow</annotation></semantics></math> 500B code tokens” means that the LLM is firstly trained with 2T webpage tokens and subsequently with 500B code data tokens. </span></span></span> applies in continual pre-training.</p>
</div>
<div id="S4.SS1.SSS3.p7" class="ltx_para">
<p id="S4.SS1.SSS3.p7.4" class="ltx_p"><math id="S4.SS1.SSS3.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS3.p7.1.m1.1a"><mo id="S4.SS1.SSS3.p7.1.m1.1.1" xref="S4.SS1.SSS3.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p7.1.m1.1b"><ci id="S4.SS1.SSS3.p7.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS3.p7.4.1" class="ltx_emph ltx_font_italic">Coding</em>. To improve the coding ability of LLMs, CodeLLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite> is developed based on LLaMA 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
(2T general tokens <math id="S4.SS1.SSS3.p7.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p7.2.m2.1a"><mo stretchy="false" id="S4.SS1.SSS3.p7.2.m2.1.1" xref="S4.SS1.SSS3.p7.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p7.2.m2.1b"><ci id="S4.SS1.SSS3.p7.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p7.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p7.2.m2.1c">\rightarrow</annotation></semantics></math> 500B code-heavy tokens), aiming to improve the code generation ability and retain natural language understanding skills. CodeLLaMA also provides a version that is further specialized to a certain programming language, namely CodeLLaMA-Python (2T general tokens <math id="S4.SS1.SSS3.p7.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p7.3.m3.1a"><mo stretchy="false" id="S4.SS1.SSS3.p7.3.m3.1.1" xref="S4.SS1.SSS3.p7.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p7.3.m3.1b"><ci id="S4.SS1.SSS3.p7.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p7.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p7.3.m3.1c">\rightarrow</annotation></semantics></math> 500B code-heavy tokens <math id="S4.SS1.SSS3.p7.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p7.4.m4.1a"><mo stretchy="false" id="S4.SS1.SSS3.p7.4.m4.1.1" xref="S4.SS1.SSS3.p7.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p7.4.m4.1b"><ci id="S4.SS1.SSS3.p7.4.m4.1.1.cmml" xref="S4.SS1.SSS3.p7.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p7.4.m4.1c">\rightarrow</annotation></semantics></math> 100B Python-heavy tokens).</p>
</div>
<div id="S4.SS1.SSS3.p8" class="ltx_para">
<p id="S4.SS1.SSS3.p8.4" class="ltx_p"><math id="S4.SS1.SSS3.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS3.p8.1.m1.1a"><mo id="S4.SS1.SSS3.p8.1.m1.1.1" xref="S4.SS1.SSS3.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p8.1.m1.1b"><ci id="S4.SS1.SSS3.p8.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS3.p8.4.1" class="ltx_emph ltx_font_italic">Mathematics.</em> Llemma&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib239" title="" class="ltx_ref">239</a>]</cite> is proposed to enhance the mathematical capacities of general-purpose LLMs.
It is developed based on CodeLLaMA.
Although CodeLLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite> mainly focuses on the coding ability, experiments have shown that it performs better than its base model LLaMA 2 on mathematics benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib239" title="" class="ltx_ref">239</a>]</cite>. Based on CodeLLaMA, Llemma is continually trained on mixtures of scientific papers, web data containing mathematical text and code (2T general tokens <math id="S4.SS1.SSS3.p8.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p8.2.m2.1a"><mo stretchy="false" id="S4.SS1.SSS3.p8.2.m2.1.1" xref="S4.SS1.SSS3.p8.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p8.2.m2.1b"><ci id="S4.SS1.SSS3.p8.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p8.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p8.2.m2.1c">\rightarrow</annotation></semantics></math> 500B code-heavy tokens <math id="S4.SS1.SSS3.p8.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p8.3.m3.1a"><mo stretchy="false" id="S4.SS1.SSS3.p8.3.m3.1.1" xref="S4.SS1.SSS3.p8.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p8.3.m3.1b"><ci id="S4.SS1.SSS3.p8.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p8.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p8.3.m3.1c">\rightarrow</annotation></semantics></math> 50<math id="S4.SS1.SSS3.p8.4.m4.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.SSS3.p8.4.m4.1a"><mo id="S4.SS1.SSS3.p8.4.m4.1.1" xref="S4.SS1.SSS3.p8.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p8.4.m4.1b"><csymbol cd="latexml" id="S4.SS1.SSS3.p8.4.m4.1.1.cmml" xref="S4.SS1.SSS3.p8.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p8.4.m4.1c">\sim</annotation></semantics></math>200B math-heavy tokens). Note that the pre-training data of Llemma also contains 5% general domain data as a form of regularization.</p>
</div>
<div id="S4.SS1.SSS3.p9" class="ltx_para">
<p id="S4.SS1.SSS3.p9.3" class="ltx_p"><math id="S4.SS1.SSS3.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS3.p9.1.m1.1a"><mo id="S4.SS1.SSS3.p9.1.m1.1.1" xref="S4.SS1.SSS3.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p9.1.m1.1b"><ci id="S4.SS1.SSS3.p9.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS3.p9.3.1" class="ltx_emph ltx_font_italic">Long context</em>. Long context modeling is an important ability for LLMs, and many studies have explored extending the context windows of LLMs via continually training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite>. With modifications on position embeddings (<em id="S4.SS1.SSS3.p9.3.2" class="ltx_emph ltx_font_italic">i.e.,</em> position interpolation) of RoPE-based LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib240" title="" class="ltx_ref">240</a>]</cite>,
CodeLLaMA further extends the context window of LLaMA 2 (2.5T tokens with 4K context window <math id="S4.SS1.SSS3.p9.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p9.2.m2.1a"><mo stretchy="false" id="S4.SS1.SSS3.p9.2.m2.1.1" xref="S4.SS1.SSS3.p9.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p9.2.m2.1b"><ci id="S4.SS1.SSS3.p9.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p9.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p9.2.m2.1c">\rightarrow</annotation></semantics></math> 20B tokens with 16K context window).
LongLLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite> also achieves longer context window
with the help of external memory and a unique training objective (1T tokens with 2K context window <math id="S4.SS1.SSS3.p9.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS3.p9.3.m3.1a"><mo stretchy="false" id="S4.SS1.SSS3.p9.3.m3.1.1" xref="S4.SS1.SSS3.p9.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p9.3.m3.1b"><ci id="S4.SS1.SSS3.p9.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p9.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p9.3.m3.1c">\rightarrow</annotation></semantics></math> 10B tokens with 8K context window).</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Summary of Data Preparation</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">In this part, we summarize the general procedure and key points to prepare pre-training data for LLMs, which are detailed in the following three aspects.</p>
</div>
<div id="S4.SS1.SSS4.p2" class="ltx_para">
<p id="S4.SS1.SSS4.p2.1" class="ltx_p"><math id="S4.SS1.SSS4.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS4.p2.1.m1.1a"><mo id="S4.SS1.SSS4.p2.1.m1.1.1" xref="S4.SS1.SSS4.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p2.1.m1.1b"><ci id="S4.SS1.SSS4.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p2.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS4.p2.1.1" class="ltx_emph ltx_font_italic">Data collection.</em> It is suggested to include diverse data sources in the pre-training data. Although Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> shows that webpages alone can be employed to train powerful LLMs, a more typical approach is to also incorporate diverse high-quality text like code, books, scientific papers, <em id="S4.SS1.SSS4.p2.1.2" class="ltx_emph ltx_font_italic">etc</em>.
If a LLM is specialized with a certain skill, the proportion of corresponding data source should be increased accordingly. For example, Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> are trained with approximately 40% of data from books. PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> use approximately 50% conversational data.</p>
</div>
<div id="S4.SS1.SSS4.p3" class="ltx_para">
<p id="S4.SS1.SSS4.p3.1" class="ltx_p"><math id="S4.SS1.SSS4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS4.p3.1.m1.1a"><mo id="S4.SS1.SSS4.p3.1.m1.1.1" xref="S4.SS1.SSS4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p3.1.m1.1b"><ci id="S4.SS1.SSS4.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS4.p3.1.1" class="ltx_emph ltx_font_italic">Data cleaning.</em> After data collection, it is crucial to clean the raw corpus to enhance its quality as possible. First, deduplication is commonly used in existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib229" title="" class="ltx_ref">229</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>.
Second, low-quality text, toxic content, and data with privacy concerns should be removed at different granularities (<em id="S4.SS1.SSS4.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> document, passage or sentence). In practice, both heuristic and classifier-based methods can be employed for quality and toxicity filtering (<em id="S4.SS1.SSS4.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> CCNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib241" title="" class="ltx_ref">241</a>]</cite>, fastText&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib242" title="" class="ltx_ref">242</a>]</cite>, and Data-Juicer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite>). Third, with the cleaned data, one can further unify or specify the format for pre-training data, and perform the tokenization by training the tokenizer on the filtered and deduplicated corpus with libraries like SentencePiece&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite>.</p>
</div>
<div id="S4.SS1.SSS4.p4" class="ltx_para">
<p id="S4.SS1.SSS4.p4.1" class="ltx_p"><math id="S4.SS1.SSS4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.SSS4.p4.1.m1.1a"><mo id="S4.SS1.SSS4.p4.1.m1.1.1" xref="S4.SS1.SSS4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p4.1.m1.1b"><ci id="S4.SS1.SSS4.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS1.SSS4.p4.1.1" class="ltx_emph ltx_font_italic">Data scheduling.</em> With the preprocessed data, the next step is to determine the data mixture and the specific order of data for pre-training LLMs.
To determine both settings, a practical way is to first train several small language models with multiple candidate plans and then select a good plan among them&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
Overall, it is more difficult to find a suitable data curriculum.
In practice, one can monitor the performance of intermediate model checkpoints on specific evaluation benchmarks, and dynamically tune the data mixture and distribution during pre-training. In this process, it is also useful to explore the potential relations between data sources and model abilities to instruct the design of data curriculum.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Architecture</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this section, we review the architecture design of LLMs, <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> mainstream architecture, pre-training objective, and detailed configuration. Table&nbsp;<a href="#S4.T5" title="TABLE V ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the model cards of several representative LLMs with public details.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, <math id="S4.T5.2.m1.1" class="ltx_Math" alttext="d_{model}" display="inline"><semantics id="S4.T5.2.m1.1b"><msub id="S4.T5.2.m1.1.1" xref="S4.T5.2.m1.1.1.cmml"><mi id="S4.T5.2.m1.1.1.2" xref="S4.T5.2.m1.1.1.2.cmml">d</mi><mrow id="S4.T5.2.m1.1.1.3" xref="S4.T5.2.m1.1.1.3.cmml"><mi id="S4.T5.2.m1.1.1.3.2" xref="S4.T5.2.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.m1.1.1.3.1" xref="S4.T5.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.2.m1.1.1.3.3" xref="S4.T5.2.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.m1.1.1.3.1b" xref="S4.T5.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.2.m1.1.1.3.4" xref="S4.T5.2.m1.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.m1.1.1.3.1c" xref="S4.T5.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.2.m1.1.1.3.5" xref="S4.T5.2.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T5.2.m1.1.1.3.1d" xref="S4.T5.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.2.m1.1.1.3.6" xref="S4.T5.2.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T5.2.m1.1c"><apply id="S4.T5.2.m1.1.1.cmml" xref="S4.T5.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.2.m1.1.1.1.cmml" xref="S4.T5.2.m1.1.1">subscript</csymbol><ci id="S4.T5.2.m1.1.1.2.cmml" xref="S4.T5.2.m1.1.1.2">𝑑</ci><apply id="S4.T5.2.m1.1.1.3.cmml" xref="S4.T5.2.m1.1.1.3"><times id="S4.T5.2.m1.1.1.3.1.cmml" xref="S4.T5.2.m1.1.1.3.1"></times><ci id="S4.T5.2.m1.1.1.3.2.cmml" xref="S4.T5.2.m1.1.1.3.2">𝑚</ci><ci id="S4.T5.2.m1.1.1.3.3.cmml" xref="S4.T5.2.m1.1.1.3.3">𝑜</ci><ci id="S4.T5.2.m1.1.1.3.4.cmml" xref="S4.T5.2.m1.1.1.3.4">𝑑</ci><ci id="S4.T5.2.m1.1.1.3.5.cmml" xref="S4.T5.2.m1.1.1.3.5">𝑒</ci><ci id="S4.T5.2.m1.1.1.3.6.cmml" xref="S4.T5.2.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.m1.1d">d_{model}</annotation></semantics></math> denotes the size of hidden states, and MCL denotes the maximum context length during training.</figcaption>
<table id="S4.T5.10" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T5.3.1" class="ltx_tr">
<td id="S4.T5.3.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.3.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T5.3.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.3.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S4.T5.3.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T5.3.1.4.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S4.T5.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.5.1" class="ltx_text ltx_font_bold">Normalization</span></td>
<td id="S4.T5.3.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.6.1" class="ltx_text ltx_font_bold">PE</span></td>
<td id="S4.T5.3.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.7.1" class="ltx_text ltx_font_bold">Activation</span></td>
<td id="S4.T5.3.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.8.1" class="ltx_text ltx_font_bold">Bias</span></td>
<td id="S4.T5.3.1.9" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T5.3.1.9.1" class="ltx_text ltx_font_bold">#L</span></td>
<td id="S4.T5.3.1.10" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T5.3.1.10.1" class="ltx_text ltx_font_bold">#H</span></td>
<td id="S4.T5.3.1.1" class="ltx_td ltx_align_right ltx_border_tt"><math id="S4.T5.3.1.1.m1.1" class="ltx_Math" alttext="d_{model}" display="inline"><semantics id="S4.T5.3.1.1.m1.1a"><msub id="S4.T5.3.1.1.m1.1.1" xref="S4.T5.3.1.1.m1.1.1.cmml"><mi id="S4.T5.3.1.1.m1.1.1.2" xref="S4.T5.3.1.1.m1.1.1.2.cmml">d</mi><mrow id="S4.T5.3.1.1.m1.1.1.3" xref="S4.T5.3.1.1.m1.1.1.3.cmml"><mi id="S4.T5.3.1.1.m1.1.1.3.2" xref="S4.T5.3.1.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T5.3.1.1.m1.1.1.3.1" xref="S4.T5.3.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.3.1.1.m1.1.1.3.3" xref="S4.T5.3.1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T5.3.1.1.m1.1.1.3.1a" xref="S4.T5.3.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.3.1.1.m1.1.1.3.4" xref="S4.T5.3.1.1.m1.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.T5.3.1.1.m1.1.1.3.1b" xref="S4.T5.3.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.3.1.1.m1.1.1.3.5" xref="S4.T5.3.1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T5.3.1.1.m1.1.1.3.1c" xref="S4.T5.3.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T5.3.1.1.m1.1.1.3.6" xref="S4.T5.3.1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T5.3.1.1.m1.1b"><apply id="S4.T5.3.1.1.m1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T5.3.1.1.m1.1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1">subscript</csymbol><ci id="S4.T5.3.1.1.m1.1.1.2.cmml" xref="S4.T5.3.1.1.m1.1.1.2">𝑑</ci><apply id="S4.T5.3.1.1.m1.1.1.3.cmml" xref="S4.T5.3.1.1.m1.1.1.3"><times id="S4.T5.3.1.1.m1.1.1.3.1.cmml" xref="S4.T5.3.1.1.m1.1.1.3.1"></times><ci id="S4.T5.3.1.1.m1.1.1.3.2.cmml" xref="S4.T5.3.1.1.m1.1.1.3.2">𝑚</ci><ci id="S4.T5.3.1.1.m1.1.1.3.3.cmml" xref="S4.T5.3.1.1.m1.1.1.3.3">𝑜</ci><ci id="S4.T5.3.1.1.m1.1.1.3.4.cmml" xref="S4.T5.3.1.1.m1.1.1.3.4">𝑑</ci><ci id="S4.T5.3.1.1.m1.1.1.3.5.cmml" xref="S4.T5.3.1.1.m1.1.1.3.5">𝑒</ci><ci id="S4.T5.3.1.1.m1.1.1.3.6.cmml" xref="S4.T5.3.1.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.1.1.m1.1c">d_{model}</annotation></semantics></math></td>
<td id="S4.T5.3.1.11" class="ltx_td ltx_align_right ltx_border_tt"><span id="S4.T5.3.1.11.1" class="ltx_text ltx_font_bold">MCL</span></td>
</tr>
<tr id="S4.T5.10.9" class="ltx_tr">
<td id="S4.T5.10.9.1" class="ltx_td ltx_align_left ltx_border_t">GPT3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S4.T5.10.9.2" class="ltx_td ltx_align_center ltx_border_t">Causal decoder</td>
<td id="S4.T5.10.9.3" class="ltx_td ltx_align_right ltx_border_t">175B</td>
<td id="S4.T5.10.9.4" class="ltx_td ltx_align_center ltx_border_t">Pre LayerNorm</td>
<td id="S4.T5.10.9.5" class="ltx_td ltx_align_center ltx_border_t">Learned</td>
<td id="S4.T5.10.9.6" class="ltx_td ltx_align_center ltx_border_t">GeLU</td>
<td id="S4.T5.10.9.7" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T5.10.9.8" class="ltx_td ltx_align_right ltx_border_t">96</td>
<td id="S4.T5.10.9.9" class="ltx_td ltx_align_right ltx_border_t">96</td>
<td id="S4.T5.10.9.10" class="ltx_td ltx_align_right ltx_border_t">12288</td>
<td id="S4.T5.10.9.11" class="ltx_td ltx_align_right ltx_border_t">2048</td>
</tr>
<tr id="S4.T5.4.2" class="ltx_tr">
<td id="S4.T5.4.2.1" class="ltx_td ltx_align_left">PanGU-&nbsp;<math id="S4.T5.4.2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T5.4.2.1.m1.1a"><mi id="S4.T5.4.2.1.m1.1.1" xref="S4.T5.4.2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T5.4.2.1.m1.1b"><ci id="S4.T5.4.2.1.m1.1.1.cmml" xref="S4.T5.4.2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.2.1.m1.1c">\alpha</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
<td id="S4.T5.4.2.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.4.2.3" class="ltx_td ltx_align_right">207B</td>
<td id="S4.T5.4.2.4" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.4.2.5" class="ltx_td ltx_align_center">Learned</td>
<td id="S4.T5.4.2.6" class="ltx_td ltx_align_center">GeLU</td>
<td id="S4.T5.4.2.7" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.4.2.8" class="ltx_td ltx_align_right">64</td>
<td id="S4.T5.4.2.9" class="ltx_td ltx_align_right">128</td>
<td id="S4.T5.4.2.10" class="ltx_td ltx_align_right">16384</td>
<td id="S4.T5.4.2.11" class="ltx_td ltx_align_right">1024</td>
</tr>
<tr id="S4.T5.10.10" class="ltx_tr">
<td id="S4.T5.10.10.1" class="ltx_td ltx_align_left">OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S4.T5.10.10.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.10.3" class="ltx_td ltx_align_right">175B</td>
<td id="S4.T5.10.10.4" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.10.10.5" class="ltx_td ltx_align_center">Learned</td>
<td id="S4.T5.10.10.6" class="ltx_td ltx_align_center">ReLU</td>
<td id="S4.T5.10.10.7" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.10.10.8" class="ltx_td ltx_align_right">96</td>
<td id="S4.T5.10.10.9" class="ltx_td ltx_align_right">96</td>
<td id="S4.T5.10.10.10" class="ltx_td ltx_align_right">12288</td>
<td id="S4.T5.10.10.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.5.3" class="ltx_tr">
<td id="S4.T5.5.3.2" class="ltx_td ltx_align_left">PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S4.T5.5.3.3" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.5.3.4" class="ltx_td ltx_align_right">540B</td>
<td id="S4.T5.5.3.5" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.5.3.6" class="ltx_td ltx_align_center">RoPE</td>
<td id="S4.T5.5.3.7" class="ltx_td ltx_align_center">SwiGLU</td>
<td id="S4.T5.5.3.1" class="ltx_td ltx_align_center"><math id="S4.T5.5.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.5.3.1.m1.1a"><mo id="S4.T5.5.3.1.m1.1.1" xref="S4.T5.5.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.3.1.m1.1b"><times id="S4.T5.5.3.1.m1.1.1.cmml" xref="S4.T5.5.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.3.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T5.5.3.8" class="ltx_td ltx_align_right">118</td>
<td id="S4.T5.5.3.9" class="ltx_td ltx_align_right">48</td>
<td id="S4.T5.5.3.10" class="ltx_td ltx_align_right">18432</td>
<td id="S4.T5.5.3.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.11" class="ltx_tr">
<td id="S4.T5.10.11.1" class="ltx_td ltx_align_left">BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S4.T5.10.11.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.11.3" class="ltx_td ltx_align_right">176B</td>
<td id="S4.T5.10.11.4" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.10.11.5" class="ltx_td ltx_align_center">ALiBi</td>
<td id="S4.T5.10.11.6" class="ltx_td ltx_align_center">GeLU</td>
<td id="S4.T5.10.11.7" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.10.11.8" class="ltx_td ltx_align_right">70</td>
<td id="S4.T5.10.11.9" class="ltx_td ltx_align_right">112</td>
<td id="S4.T5.10.11.10" class="ltx_td ltx_align_right">14336</td>
<td id="S4.T5.10.11.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.12" class="ltx_tr">
<td id="S4.T5.10.12.1" class="ltx_td ltx_align_left">MT-NLG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>
</td>
<td id="S4.T5.10.12.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.12.3" class="ltx_td ltx_align_right">530B</td>
<td id="S4.T5.10.12.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.12.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.12.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.12.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.12.8" class="ltx_td ltx_align_right">105</td>
<td id="S4.T5.10.12.9" class="ltx_td ltx_align_right">128</td>
<td id="S4.T5.10.12.10" class="ltx_td ltx_align_right">20480</td>
<td id="S4.T5.10.12.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.13" class="ltx_tr">
<td id="S4.T5.10.13.1" class="ltx_td ltx_align_left">Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
<td id="S4.T5.10.13.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.13.3" class="ltx_td ltx_align_right">280B</td>
<td id="S4.T5.10.13.4" class="ltx_td ltx_align_center">Pre RMSNorm</td>
<td id="S4.T5.10.13.5" class="ltx_td ltx_align_center">Relative</td>
<td id="S4.T5.10.13.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.13.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.13.8" class="ltx_td ltx_align_right">80</td>
<td id="S4.T5.10.13.9" class="ltx_td ltx_align_right">128</td>
<td id="S4.T5.10.13.10" class="ltx_td ltx_align_right">16384</td>
<td id="S4.T5.10.13.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.14" class="ltx_tr">
<td id="S4.T5.10.14.1" class="ltx_td ltx_align_left">Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T5.10.14.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.14.3" class="ltx_td ltx_align_right">70B</td>
<td id="S4.T5.10.14.4" class="ltx_td ltx_align_center">Pre RMSNorm</td>
<td id="S4.T5.10.14.5" class="ltx_td ltx_align_center">Relative</td>
<td id="S4.T5.10.14.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.14.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.14.8" class="ltx_td ltx_align_right">80</td>
<td id="S4.T5.10.14.9" class="ltx_td ltx_align_right">64</td>
<td id="S4.T5.10.14.10" class="ltx_td ltx_align_right">8192</td>
<td id="S4.T5.10.14.11" class="ltx_td ltx_align_right">-</td>
</tr>
<tr id="S4.T5.6.4" class="ltx_tr">
<td id="S4.T5.6.4.2" class="ltx_td ltx_align_left">Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S4.T5.6.4.3" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.6.4.4" class="ltx_td ltx_align_right">120B</td>
<td id="S4.T5.6.4.5" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.6.4.6" class="ltx_td ltx_align_center">Learned</td>
<td id="S4.T5.6.4.7" class="ltx_td ltx_align_center">GeLU</td>
<td id="S4.T5.6.4.1" class="ltx_td ltx_align_center"><math id="S4.T5.6.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.6.4.1.m1.1a"><mo id="S4.T5.6.4.1.m1.1.1" xref="S4.T5.6.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T5.6.4.1.m1.1b"><times id="S4.T5.6.4.1.m1.1.1.cmml" xref="S4.T5.6.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.4.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T5.6.4.8" class="ltx_td ltx_align_right">96</td>
<td id="S4.T5.6.4.9" class="ltx_td ltx_align_right">80</td>
<td id="S4.T5.6.4.10" class="ltx_td ltx_align_right">10240</td>
<td id="S4.T5.6.4.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.15" class="ltx_tr">
<td id="S4.T5.10.15.1" class="ltx_td ltx_align_left">LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S4.T5.10.15.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.15.3" class="ltx_td ltx_align_right">137B</td>
<td id="S4.T5.10.15.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.15.5" class="ltx_td ltx_align_center">Relative</td>
<td id="S4.T5.10.15.6" class="ltx_td ltx_align_center">GeGLU</td>
<td id="S4.T5.10.15.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.15.8" class="ltx_td ltx_align_right">64</td>
<td id="S4.T5.10.15.9" class="ltx_td ltx_align_right">128</td>
<td id="S4.T5.10.15.10" class="ltx_td ltx_align_right">8192</td>
<td id="S4.T5.10.15.11" class="ltx_td ltx_align_right">-</td>
</tr>
<tr id="S4.T5.10.16" class="ltx_tr">
<td id="S4.T5.10.16.1" class="ltx_td ltx_align_left">Jurassic-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>
</td>
<td id="S4.T5.10.16.2" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.10.16.3" class="ltx_td ltx_align_right">178B</td>
<td id="S4.T5.10.16.4" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.10.16.5" class="ltx_td ltx_align_center">Learned</td>
<td id="S4.T5.10.16.6" class="ltx_td ltx_align_center">GeLU</td>
<td id="S4.T5.10.16.7" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.10.16.8" class="ltx_td ltx_align_right">76</td>
<td id="S4.T5.10.16.9" class="ltx_td ltx_align_right">96</td>
<td id="S4.T5.10.16.10" class="ltx_td ltx_align_right">13824</td>
<td id="S4.T5.10.16.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.7.5" class="ltx_tr">
<td id="S4.T5.7.5.2" class="ltx_td ltx_align_left">LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S4.T5.7.5.3" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.7.5.4" class="ltx_td ltx_align_right">65B</td>
<td id="S4.T5.7.5.5" class="ltx_td ltx_align_center">Pre RMSNorm</td>
<td id="S4.T5.7.5.6" class="ltx_td ltx_align_center">RoPE</td>
<td id="S4.T5.7.5.7" class="ltx_td ltx_align_center">SwiGLU</td>
<td id="S4.T5.7.5.1" class="ltx_td ltx_align_center"><math id="S4.T5.7.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.7.5.1.m1.1a"><mo id="S4.T5.7.5.1.m1.1.1" xref="S4.T5.7.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T5.7.5.1.m1.1b"><times id="S4.T5.7.5.1.m1.1.1.cmml" xref="S4.T5.7.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.5.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T5.7.5.8" class="ltx_td ltx_align_right">80</td>
<td id="S4.T5.7.5.9" class="ltx_td ltx_align_right">64</td>
<td id="S4.T5.7.5.10" class="ltx_td ltx_align_right">8192</td>
<td id="S4.T5.7.5.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.8.6" class="ltx_tr">
<td id="S4.T5.8.6.2" class="ltx_td ltx_align_left">LLaMA 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S4.T5.8.6.3" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.8.6.4" class="ltx_td ltx_align_right">70B</td>
<td id="S4.T5.8.6.5" class="ltx_td ltx_align_center">Pre RMSNorm</td>
<td id="S4.T5.8.6.6" class="ltx_td ltx_align_center">RePE</td>
<td id="S4.T5.8.6.7" class="ltx_td ltx_align_center">SwiGLU</td>
<td id="S4.T5.8.6.1" class="ltx_td ltx_align_center"><math id="S4.T5.8.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.8.6.1.m1.1a"><mo id="S4.T5.8.6.1.m1.1.1" xref="S4.T5.8.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T5.8.6.1.m1.1b"><times id="S4.T5.8.6.1.m1.1.1.cmml" xref="S4.T5.8.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.6.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T5.8.6.8" class="ltx_td ltx_align_right">80</td>
<td id="S4.T5.8.6.9" class="ltx_td ltx_align_right">64</td>
<td id="S4.T5.8.6.10" class="ltx_td ltx_align_right">8192</td>
<td id="S4.T5.8.6.11" class="ltx_td ltx_align_right">4096</td>
</tr>
<tr id="S4.T5.9.7" class="ltx_tr">
<td id="S4.T5.9.7.2" class="ltx_td ltx_align_left">Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>
</td>
<td id="S4.T5.9.7.3" class="ltx_td ltx_align_center">Causal decoder</td>
<td id="S4.T5.9.7.4" class="ltx_td ltx_align_right">40B</td>
<td id="S4.T5.9.7.5" class="ltx_td ltx_align_center">Pre LayerNorm</td>
<td id="S4.T5.9.7.6" class="ltx_td ltx_align_center">RoPE</td>
<td id="S4.T5.9.7.7" class="ltx_td ltx_align_center">GeLU</td>
<td id="S4.T5.9.7.1" class="ltx_td ltx_align_center"><math id="S4.T5.9.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.9.7.1.m1.1a"><mo id="S4.T5.9.7.1.m1.1.1" xref="S4.T5.9.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T5.9.7.1.m1.1b"><times id="S4.T5.9.7.1.m1.1.1.cmml" xref="S4.T5.9.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.7.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T5.9.7.8" class="ltx_td ltx_align_right">60</td>
<td id="S4.T5.9.7.9" class="ltx_td ltx_align_right">64</td>
<td id="S4.T5.9.7.10" class="ltx_td ltx_align_right">8192</td>
<td id="S4.T5.9.7.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.17" class="ltx_tr">
<td id="S4.T5.10.17.1" class="ltx_td ltx_align_left">GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S4.T5.10.17.2" class="ltx_td ltx_align_center">Prefix decoder</td>
<td id="S4.T5.10.17.3" class="ltx_td ltx_align_right">130B</td>
<td id="S4.T5.10.17.4" class="ltx_td ltx_align_center">Post DeepNorm</td>
<td id="S4.T5.10.17.5" class="ltx_td ltx_align_center">RoPE</td>
<td id="S4.T5.10.17.6" class="ltx_td ltx_align_center">GeGLU</td>
<td id="S4.T5.10.17.7" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T5.10.17.8" class="ltx_td ltx_align_right">70</td>
<td id="S4.T5.10.17.9" class="ltx_td ltx_align_right">96</td>
<td id="S4.T5.10.17.10" class="ltx_td ltx_align_right">12288</td>
<td id="S4.T5.10.17.11" class="ltx_td ltx_align_right">2048</td>
</tr>
<tr id="S4.T5.10.8" class="ltx_tr">
<td id="S4.T5.10.8.2" class="ltx_td ltx_align_left ltx_border_bb">T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S4.T5.10.8.3" class="ltx_td ltx_align_center ltx_border_bb">Encoder-decoder</td>
<td id="S4.T5.10.8.4" class="ltx_td ltx_align_right ltx_border_bb">11B</td>
<td id="S4.T5.10.8.5" class="ltx_td ltx_align_center ltx_border_bb">Pre RMSNorm</td>
<td id="S4.T5.10.8.6" class="ltx_td ltx_align_center ltx_border_bb">Relative</td>
<td id="S4.T5.10.8.7" class="ltx_td ltx_align_center ltx_border_bb">ReLU</td>
<td id="S4.T5.10.8.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T5.10.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T5.10.8.1.m1.1a"><mo id="S4.T5.10.8.1.m1.1.1" xref="S4.T5.10.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T5.10.8.1.m1.1b"><times id="S4.T5.10.8.1.m1.1.1.cmml" xref="S4.T5.10.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.8.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T5.10.8.8" class="ltx_td ltx_align_right ltx_border_bb">24</td>
<td id="S4.T5.10.8.9" class="ltx_td ltx_align_right ltx_border_bb">128</td>
<td id="S4.T5.10.8.10" class="ltx_td ltx_align_right ltx_border_bb">1024</td>
<td id="S4.T5.10.8.11" class="ltx_td ltx_align_right ltx_border_bb">512</td>
</tr>
</tbody></table>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Typical Architectures</h4>

<figure id="S4.F9" class="ltx_figure"><img src="/html/2303.18223/assets/x10.png" id="S4.F9.g1" class="ltx_graphics ltx_img_landscape" width="461" height="173" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>A comparison of the attention patterns in three mainstream architectures. Here, the blue, green, yellow and grey rounded rectangles indicate the attention between prefix tokens, attention between prefix and target tokens, attention between target tokens, and masked attention respectively.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Due to the excellent parallelizability and capacity, the Transformer architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> has become the de facto backbone to develop various LLMs, making it possible to scale language models to hundreds or thousands of billions of parameters.
In general, the mainstream architectures of existing LLMs can be roughly categorized into three major types, namely encoder-decoder, causal decoder, and prefix decoder, as shown in Figure&nbsp;<a href="#S4.F9" title="Figure 9 ‣ 4.2.1 Typical Architectures ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p"><span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Encoder-decoder Architecture.</span>
The vanilla Transformer model is built on the encoder-decoder architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which consists of two stacks of Transformer blocks as the encoder and decoder, respectively.
The encoder adopts stacked multi-head self-attention layers to encode the input sequence for generating its latent representations, while the decoder performs cross-attention on these representations and autoregressively generates the target sequence. Encoder-decoder PLMs (<em id="S4.SS2.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> and BART&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) have shown effectiveness on a variety of NLP tasks.

So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, <em id="S4.SS2.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> Flan-T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. We leave a detailed discussion about the architecture selection in Section&nbsp;<a href="#S4.SS2.SSS6" title="4.2.6 Summary and Discussion ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.6</span></a>.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p"><span id="S4.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Causal Decoder Architecture.</span>
The causal decoder architecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself.
The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT-series models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> are developed based on the causal-decoder architecture.
In particular, GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> has successfully demonstrated the effectiveness of this architecture, also showing an amazing in-context learning capability of LLMs.
Interestingly, GPT-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> and GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> do not exhibit such superior abilities as those in GPT-3, and it seems that scaling plays an important role in increasing the model capacity of this model architecture.
So far,
the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, and Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>.

Note that both the causal decoder and prefix decoder discussed next belong to decoder-only architectures. When mentioning “decoder-only architecture”, it mainly refers to the causal decoder architecture in existing literature, unless specified.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p"><span id="S4.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Prefix Decoder Architecture.</span>
The prefix decoder architecture (<em id="S4.SS2.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">a.k.a.,</em> non-causal decoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib244" title="" class="ltx_ref">244</a>]</cite>) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib245" title="" class="ltx_ref">245</a>]</cite> and unidirectional attention only on generated tokens.
In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding.
Instead of pre-training from scratch, a practical suggestion is to continually train causal decoders and then convert them into prefix decoders for accelerating convergence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, <em id="S4.SS2.SSS1.p4.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> U-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> is derived from PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Existing representative LLMs based on prefix decoders include GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> and U-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p5.1" class="ltx_p"><span id="S4.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Mixture-of-Experts.</span> For the above three types of architectures, we can further extend them via the mixture-of-experts (MoE) scaling, in which a subset of neural network weights for each input are sparsely activated, <em id="S4.SS2.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Switch Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and GLaM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>. The major merit is that MoE is a flexible way to scale up the model parameter while maintaining a constant computational cost&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. It has been shown that substantial performance improvement can be observed by increasing either the number of experts or the total parameter size&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite>. Despite the merits, training large MoE models may suffer from instability issues due to the complex, hard-switching nature of the routing operation.
To enhance the training stability of MoE-based language models, techniques such as selectively using high-precision tensors in the routing module or initializing the model with a smaller range have been introduced&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
More recently, there is widespread speculation that GPT-4 has been developed based on the MoE architecture, but without official verification.</p>
</div>
<div id="S4.SS2.SSS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p6.1" class="ltx_p"><span id="S4.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Emergent Architectures.</span> The conventional Transformer architectures typically suffer from quadratic computational complexity. Because of this, efficiency has become an important issue when training and making inference with long inputs. To improve efficiency, some studies aim to devise new architectures for language modeling, including parameterized state space models (<em id="S4.SS2.SSS1.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> S4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite>, GSS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite>, and H3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib249" title="" class="ltx_ref">249</a>]</cite>), long convolutions like Hyena&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite>, and Transformer-like architectures that incorporate recursive update mechanisms (<em id="S4.SS2.SSS1.p6.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> RWKV&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite> and RetNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite>).
The key merits of these new architectures are twofold. First, these models can generate outputs recursively like RNNs, meaning that they only need to refer to the single previous state during decoding. It makes the decoding process more efficient as it eliminates the need to revisit all previous states as in conventional Transformers. Second, these models have the capacity to encode an entire sentence in parallel like Transformers. This contrasts with conventional RNNs which has to encode sentences on a token-by-token basis. Thus, they can benefit from the parallelism of GPUs with techniques such as Parallel Scan&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib253" title="" class="ltx_ref">253</a>, <a href="#bib.bib254" title="" class="ltx_ref">254</a>]</cite>, FFT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>, <a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite>, and Chunkwise Recurrent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite>. These techniques enable models with these new architectures to be trained in a highly parallel and efficient manner.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Detailed Configuration</h4>

<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, <math id="S4.T6.8.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.T6.8.m1.1b"><mi id="S4.T6.8.m1.1.1" xref="S4.T6.8.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T6.8.m1.1c"><ci id="S4.T6.8.m1.1.1.cmml" xref="S4.T6.8.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.m1.1d">d</annotation></semantics></math> denotes the size of hidden states, <math id="S4.T6.9.m2.1" class="ltx_Math" alttext="\mathbf{p}_{i}" display="inline"><semantics id="S4.T6.9.m2.1b"><msub id="S4.T6.9.m2.1.1" xref="S4.T6.9.m2.1.1.cmml"><mi id="S4.T6.9.m2.1.1.2" xref="S4.T6.9.m2.1.1.2.cmml">𝐩</mi><mi id="S4.T6.9.m2.1.1.3" xref="S4.T6.9.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.9.m2.1c"><apply id="S4.T6.9.m2.1.1.cmml" xref="S4.T6.9.m2.1.1"><csymbol cd="ambiguous" id="S4.T6.9.m2.1.1.1.cmml" xref="S4.T6.9.m2.1.1">subscript</csymbol><ci id="S4.T6.9.m2.1.1.2.cmml" xref="S4.T6.9.m2.1.1.2">𝐩</ci><ci id="S4.T6.9.m2.1.1.3.cmml" xref="S4.T6.9.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.m2.1d">\mathbf{p}_{i}</annotation></semantics></math> denotes position embedding at position <math id="S4.T6.10.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T6.10.m3.1b"><mi id="S4.T6.10.m3.1.1" xref="S4.T6.10.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T6.10.m3.1c"><ci id="S4.T6.10.m3.1.1.cmml" xref="S4.T6.10.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.m3.1d">i</annotation></semantics></math>, <math id="S4.T6.11.m4.1" class="ltx_Math" alttext="A_{ij}" display="inline"><semantics id="S4.T6.11.m4.1b"><msub id="S4.T6.11.m4.1.1" xref="S4.T6.11.m4.1.1.cmml"><mi id="S4.T6.11.m4.1.1.2" xref="S4.T6.11.m4.1.1.2.cmml">A</mi><mrow id="S4.T6.11.m4.1.1.3" xref="S4.T6.11.m4.1.1.3.cmml"><mi id="S4.T6.11.m4.1.1.3.2" xref="S4.T6.11.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T6.11.m4.1.1.3.1" xref="S4.T6.11.m4.1.1.3.1.cmml">​</mo><mi id="S4.T6.11.m4.1.1.3.3" xref="S4.T6.11.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T6.11.m4.1c"><apply id="S4.T6.11.m4.1.1.cmml" xref="S4.T6.11.m4.1.1"><csymbol cd="ambiguous" id="S4.T6.11.m4.1.1.1.cmml" xref="S4.T6.11.m4.1.1">subscript</csymbol><ci id="S4.T6.11.m4.1.1.2.cmml" xref="S4.T6.11.m4.1.1.2">𝐴</ci><apply id="S4.T6.11.m4.1.1.3.cmml" xref="S4.T6.11.m4.1.1.3"><times id="S4.T6.11.m4.1.1.3.1.cmml" xref="S4.T6.11.m4.1.1.3.1"></times><ci id="S4.T6.11.m4.1.1.3.2.cmml" xref="S4.T6.11.m4.1.1.3.2">𝑖</ci><ci id="S4.T6.11.m4.1.1.3.3.cmml" xref="S4.T6.11.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.11.m4.1d">A_{ij}</annotation></semantics></math> denotes the attention score between a query and a key, <math id="S4.T6.12.m5.1" class="ltx_Math" alttext="r_{i-j}" display="inline"><semantics id="S4.T6.12.m5.1b"><msub id="S4.T6.12.m5.1.1" xref="S4.T6.12.m5.1.1.cmml"><mi id="S4.T6.12.m5.1.1.2" xref="S4.T6.12.m5.1.1.2.cmml">r</mi><mrow id="S4.T6.12.m5.1.1.3" xref="S4.T6.12.m5.1.1.3.cmml"><mi id="S4.T6.12.m5.1.1.3.2" xref="S4.T6.12.m5.1.1.3.2.cmml">i</mi><mo id="S4.T6.12.m5.1.1.3.1" xref="S4.T6.12.m5.1.1.3.1.cmml">−</mo><mi id="S4.T6.12.m5.1.1.3.3" xref="S4.T6.12.m5.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T6.12.m5.1c"><apply id="S4.T6.12.m5.1.1.cmml" xref="S4.T6.12.m5.1.1"><csymbol cd="ambiguous" id="S4.T6.12.m5.1.1.1.cmml" xref="S4.T6.12.m5.1.1">subscript</csymbol><ci id="S4.T6.12.m5.1.1.2.cmml" xref="S4.T6.12.m5.1.1.2">𝑟</ci><apply id="S4.T6.12.m5.1.1.3.cmml" xref="S4.T6.12.m5.1.1.3"><minus id="S4.T6.12.m5.1.1.3.1.cmml" xref="S4.T6.12.m5.1.1.3.1"></minus><ci id="S4.T6.12.m5.1.1.3.2.cmml" xref="S4.T6.12.m5.1.1.3.2">𝑖</ci><ci id="S4.T6.12.m5.1.1.3.3.cmml" xref="S4.T6.12.m5.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.12.m5.1d">r_{i-j}</annotation></semantics></math> denotes a learnable scalar based on the offset between the query and the key, and <math id="S4.T6.13.m6.2" class="ltx_Math" alttext="\mathbf{R}_{\Theta,t}" display="inline"><semantics id="S4.T6.13.m6.2b"><msub id="S4.T6.13.m6.2.3" xref="S4.T6.13.m6.2.3.cmml"><mi id="S4.T6.13.m6.2.3.2" xref="S4.T6.13.m6.2.3.2.cmml">𝐑</mi><mrow id="S4.T6.13.m6.2.2.2.4" xref="S4.T6.13.m6.2.2.2.3.cmml"><mi mathvariant="normal" id="S4.T6.13.m6.1.1.1.1" xref="S4.T6.13.m6.1.1.1.1.cmml">Θ</mi><mo id="S4.T6.13.m6.2.2.2.4.1" xref="S4.T6.13.m6.2.2.2.3.cmml">,</mo><mi id="S4.T6.13.m6.2.2.2.2" xref="S4.T6.13.m6.2.2.2.2.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T6.13.m6.2c"><apply id="S4.T6.13.m6.2.3.cmml" xref="S4.T6.13.m6.2.3"><csymbol cd="ambiguous" id="S4.T6.13.m6.2.3.1.cmml" xref="S4.T6.13.m6.2.3">subscript</csymbol><ci id="S4.T6.13.m6.2.3.2.cmml" xref="S4.T6.13.m6.2.3.2">𝐑</ci><list id="S4.T6.13.m6.2.2.2.3.cmml" xref="S4.T6.13.m6.2.2.2.4"><ci id="S4.T6.13.m6.1.1.1.1.cmml" xref="S4.T6.13.m6.1.1.1.1">Θ</ci><ci id="S4.T6.13.m6.2.2.2.2.cmml" xref="S4.T6.13.m6.2.2.2.2">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.13.m6.2d">\mathbf{R}_{\Theta,t}</annotation></semantics></math> denotes a rotary matrix with rotation degree <math id="S4.T6.14.m7.1" class="ltx_Math" alttext="t\cdot\Theta" display="inline"><semantics id="S4.T6.14.m7.1b"><mrow id="S4.T6.14.m7.1.1" xref="S4.T6.14.m7.1.1.cmml"><mi id="S4.T6.14.m7.1.1.2" xref="S4.T6.14.m7.1.1.2.cmml">t</mi><mo lspace="0.222em" rspace="0.222em" id="S4.T6.14.m7.1.1.1" xref="S4.T6.14.m7.1.1.1.cmml">⋅</mo><mi mathvariant="normal" id="S4.T6.14.m7.1.1.3" xref="S4.T6.14.m7.1.1.3.cmml">Θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.14.m7.1c"><apply id="S4.T6.14.m7.1.1.cmml" xref="S4.T6.14.m7.1.1"><ci id="S4.T6.14.m7.1.1.1.cmml" xref="S4.T6.14.m7.1.1.1">⋅</ci><ci id="S4.T6.14.m7.1.1.2.cmml" xref="S4.T6.14.m7.1.1.2">𝑡</ci><ci id="S4.T6.14.m7.1.1.3.cmml" xref="S4.T6.14.m7.1.1.3">Θ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.14.m7.1d">t\cdot\Theta</annotation></semantics></math>.</figcaption>
<table id="S4.T6.29" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T6.29.16" class="ltx_tr">
<td id="S4.T6.29.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T6.29.16.1.1" class="ltx_text ltx_font_bold">Configuration</span></td>
<td id="S4.T6.29.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T6.29.16.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T6.29.16.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T6.29.16.3.1" class="ltx_text ltx_font_bold">Equation</span></td>
</tr>
<tr id="S4.T6.15.1" class="ltx_tr">
<td id="S4.T6.15.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T6.15.1.2.1" class="ltx_text">Normalization position</span></td>
<td id="S4.T6.15.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Post Norm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T6.15.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T6.15.1.1.m1.2" class="ltx_Math" alttext="\mathrm{Norm(}\mathbf{x}\mathrm{+Sublayer(}\mathbf{x}\mathrm{))}" display="inline"><semantics id="S4.T6.15.1.1.m1.2a"><mrow id="S4.T6.15.1.1.m1.2.2" xref="S4.T6.15.1.1.m1.2.2.cmml"><mi id="S4.T6.15.1.1.m1.2.2.3" xref="S4.T6.15.1.1.m1.2.2.3.cmml">Norm</mi><mo lspace="0em" rspace="0em" id="S4.T6.15.1.1.m1.2.2.2" xref="S4.T6.15.1.1.m1.2.2.2.cmml">​</mo><mrow id="S4.T6.15.1.1.m1.2.2.1.1" xref="S4.T6.15.1.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.T6.15.1.1.m1.2.2.1.1.2" xref="S4.T6.15.1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S4.T6.15.1.1.m1.2.2.1.1.1" xref="S4.T6.15.1.1.m1.2.2.1.1.1.cmml"><mi id="S4.T6.15.1.1.m1.2.2.1.1.1.2" xref="S4.T6.15.1.1.m1.2.2.1.1.1.2.cmml">𝐱</mi><mo id="S4.T6.15.1.1.m1.2.2.1.1.1.1" xref="S4.T6.15.1.1.m1.2.2.1.1.1.1.cmml">+</mo><mrow id="S4.T6.15.1.1.m1.2.2.1.1.1.3" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.cmml"><mi id="S4.T6.15.1.1.m1.2.2.1.1.1.3.2" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.2.cmml">Sublayer</mi><mo lspace="0em" rspace="0em" id="S4.T6.15.1.1.m1.2.2.1.1.1.3.1" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.1.cmml">​</mo><mrow id="S4.T6.15.1.1.m1.2.2.1.1.1.3.3.2" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.cmml"><mo stretchy="false" id="S4.T6.15.1.1.m1.2.2.1.1.1.3.3.2.1" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.cmml">(</mo><mi id="S4.T6.15.1.1.m1.1.1" xref="S4.T6.15.1.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.15.1.1.m1.2.2.1.1.1.3.3.2.2" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S4.T6.15.1.1.m1.2.2.1.1.3" xref="S4.T6.15.1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.15.1.1.m1.2b"><apply id="S4.T6.15.1.1.m1.2.2.cmml" xref="S4.T6.15.1.1.m1.2.2"><times id="S4.T6.15.1.1.m1.2.2.2.cmml" xref="S4.T6.15.1.1.m1.2.2.2"></times><ci id="S4.T6.15.1.1.m1.2.2.3.cmml" xref="S4.T6.15.1.1.m1.2.2.3">Norm</ci><apply id="S4.T6.15.1.1.m1.2.2.1.1.1.cmml" xref="S4.T6.15.1.1.m1.2.2.1.1"><plus id="S4.T6.15.1.1.m1.2.2.1.1.1.1.cmml" xref="S4.T6.15.1.1.m1.2.2.1.1.1.1"></plus><ci id="S4.T6.15.1.1.m1.2.2.1.1.1.2.cmml" xref="S4.T6.15.1.1.m1.2.2.1.1.1.2">𝐱</ci><apply id="S4.T6.15.1.1.m1.2.2.1.1.1.3.cmml" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3"><times id="S4.T6.15.1.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.1"></times><ci id="S4.T6.15.1.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.T6.15.1.1.m1.2.2.1.1.1.3.2">Sublayer</ci><ci id="S4.T6.15.1.1.m1.1.1.cmml" xref="S4.T6.15.1.1.m1.1.1">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.15.1.1.m1.2c">\mathrm{Norm(}\mathbf{x}\mathrm{+Sublayer(}\mathbf{x}\mathrm{))}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.16.2" class="ltx_tr">
<td id="S4.T6.16.2.2" class="ltx_td ltx_align_center ltx_border_r">Pre Norm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S4.T6.16.2.1" class="ltx_td ltx_align_left"><math id="S4.T6.16.2.1.m1.2" class="ltx_Math" alttext="\mathbf{x}+\mathrm{Sublayer}(\mathrm{Norm}(\mathbf{x}))" display="inline"><semantics id="S4.T6.16.2.1.m1.2a"><mrow id="S4.T6.16.2.1.m1.2.2" xref="S4.T6.16.2.1.m1.2.2.cmml"><mi id="S4.T6.16.2.1.m1.2.2.3" xref="S4.T6.16.2.1.m1.2.2.3.cmml">𝐱</mi><mo id="S4.T6.16.2.1.m1.2.2.2" xref="S4.T6.16.2.1.m1.2.2.2.cmml">+</mo><mrow id="S4.T6.16.2.1.m1.2.2.1" xref="S4.T6.16.2.1.m1.2.2.1.cmml"><mi id="S4.T6.16.2.1.m1.2.2.1.3" xref="S4.T6.16.2.1.m1.2.2.1.3.cmml">Sublayer</mi><mo lspace="0em" rspace="0em" id="S4.T6.16.2.1.m1.2.2.1.2" xref="S4.T6.16.2.1.m1.2.2.1.2.cmml">​</mo><mrow id="S4.T6.16.2.1.m1.2.2.1.1.1" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.16.2.1.m1.2.2.1.1.1.2" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.T6.16.2.1.m1.2.2.1.1.1.1" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml"><mi id="S4.T6.16.2.1.m1.2.2.1.1.1.1.2" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.2.cmml">Norm</mi><mo lspace="0em" rspace="0em" id="S4.T6.16.2.1.m1.2.2.1.1.1.1.1" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.1.cmml">​</mo><mrow id="S4.T6.16.2.1.m1.2.2.1.1.1.1.3.2" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.16.2.1.m1.2.2.1.1.1.1.3.2.1" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml">(</mo><mi id="S4.T6.16.2.1.m1.1.1" xref="S4.T6.16.2.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.16.2.1.m1.2.2.1.1.1.1.3.2.2" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.T6.16.2.1.m1.2.2.1.1.1.3" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.16.2.1.m1.2b"><apply id="S4.T6.16.2.1.m1.2.2.cmml" xref="S4.T6.16.2.1.m1.2.2"><plus id="S4.T6.16.2.1.m1.2.2.2.cmml" xref="S4.T6.16.2.1.m1.2.2.2"></plus><ci id="S4.T6.16.2.1.m1.2.2.3.cmml" xref="S4.T6.16.2.1.m1.2.2.3">𝐱</ci><apply id="S4.T6.16.2.1.m1.2.2.1.cmml" xref="S4.T6.16.2.1.m1.2.2.1"><times id="S4.T6.16.2.1.m1.2.2.1.2.cmml" xref="S4.T6.16.2.1.m1.2.2.1.2"></times><ci id="S4.T6.16.2.1.m1.2.2.1.3.cmml" xref="S4.T6.16.2.1.m1.2.2.1.3">Sublayer</ci><apply id="S4.T6.16.2.1.m1.2.2.1.1.1.1.cmml" xref="S4.T6.16.2.1.m1.2.2.1.1.1"><times id="S4.T6.16.2.1.m1.2.2.1.1.1.1.1.cmml" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.1"></times><ci id="S4.T6.16.2.1.m1.2.2.1.1.1.1.2.cmml" xref="S4.T6.16.2.1.m1.2.2.1.1.1.1.2">Norm</ci><ci id="S4.T6.16.2.1.m1.1.1.cmml" xref="S4.T6.16.2.1.m1.1.1">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.16.2.1.m1.2c">\mathbf{x}+\mathrm{Sublayer}(\mathrm{Norm}(\mathbf{x}))</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.17.3" class="ltx_tr">
<td id="S4.T6.17.3.2" class="ltx_td ltx_align_center ltx_border_r">Sandwich Norm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite>
</td>
<td id="S4.T6.17.3.1" class="ltx_td ltx_align_left"><math id="S4.T6.17.3.1.m1.2" class="ltx_Math" alttext="\mathbf{x}+\mathrm{Norm}(\mathrm{Sublayer}(\mathrm{Norm}(\mathbf{x})))" display="inline"><semantics id="S4.T6.17.3.1.m1.2a"><mrow id="S4.T6.17.3.1.m1.2.2" xref="S4.T6.17.3.1.m1.2.2.cmml"><mi id="S4.T6.17.3.1.m1.2.2.3" xref="S4.T6.17.3.1.m1.2.2.3.cmml">𝐱</mi><mo id="S4.T6.17.3.1.m1.2.2.2" xref="S4.T6.17.3.1.m1.2.2.2.cmml">+</mo><mrow id="S4.T6.17.3.1.m1.2.2.1" xref="S4.T6.17.3.1.m1.2.2.1.cmml"><mi id="S4.T6.17.3.1.m1.2.2.1.3" xref="S4.T6.17.3.1.m1.2.2.1.3.cmml">Norm</mi><mo lspace="0em" rspace="0em" id="S4.T6.17.3.1.m1.2.2.1.2" xref="S4.T6.17.3.1.m1.2.2.1.2.cmml">​</mo><mrow id="S4.T6.17.3.1.m1.2.2.1.1.1" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.17.3.1.m1.2.2.1.1.1.2" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.T6.17.3.1.m1.2.2.1.1.1.1" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.cmml"><mi id="S4.T6.17.3.1.m1.2.2.1.1.1.1.3" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.3.cmml">Sublayer</mi><mo lspace="0em" rspace="0em" id="S4.T6.17.3.1.m1.2.2.1.1.1.1.2" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.2" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">Norm</mi><mo lspace="0em" rspace="0em" id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.3.2.1" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mi id="S4.T6.17.3.1.m1.1.1" xref="S4.T6.17.3.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.3" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.T6.17.3.1.m1.2.2.1.1.1.3" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.17.3.1.m1.2b"><apply id="S4.T6.17.3.1.m1.2.2.cmml" xref="S4.T6.17.3.1.m1.2.2"><plus id="S4.T6.17.3.1.m1.2.2.2.cmml" xref="S4.T6.17.3.1.m1.2.2.2"></plus><ci id="S4.T6.17.3.1.m1.2.2.3.cmml" xref="S4.T6.17.3.1.m1.2.2.3">𝐱</ci><apply id="S4.T6.17.3.1.m1.2.2.1.cmml" xref="S4.T6.17.3.1.m1.2.2.1"><times id="S4.T6.17.3.1.m1.2.2.1.2.cmml" xref="S4.T6.17.3.1.m1.2.2.1.2"></times><ci id="S4.T6.17.3.1.m1.2.2.1.3.cmml" xref="S4.T6.17.3.1.m1.2.2.1.3">Norm</ci><apply id="S4.T6.17.3.1.m1.2.2.1.1.1.1.cmml" xref="S4.T6.17.3.1.m1.2.2.1.1.1"><times id="S4.T6.17.3.1.m1.2.2.1.1.1.1.2.cmml" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.2"></times><ci id="S4.T6.17.3.1.m1.2.2.1.1.1.1.3.cmml" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.3">Sublayer</ci><apply id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1"><times id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.T6.17.3.1.m1.2.2.1.1.1.1.1.1.1.2">Norm</ci><ci id="S4.T6.17.3.1.m1.1.1.cmml" xref="S4.T6.17.3.1.m1.1.1">𝐱</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.17.3.1.m1.2c">\mathbf{x}+\mathrm{Norm}(\mathrm{Sublayer}(\mathrm{Norm}(\mathbf{x})))</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.18.4" class="ltx_tr">
<td id="S4.T6.18.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T6.18.4.2.1" class="ltx_text">Normalization method</span></td>
<td id="S4.T6.18.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LayerNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib256" title="" class="ltx_ref">256</a>]</cite>
</td>
<td id="S4.T6.18.4.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T6.18.4.1.m1.2" class="ltx_math_unparsed" alttext="\frac{\mathbf{x}-\mathbf{\mu}}{\mathbf{\sigma}}\cdot\gamma+\beta,\text{~{}~{}~{}}\mathbf{\mu}=\frac{1}{d}\sum_{i=1}^{d}x_{i},\text{~{}~{}~{}}\mathbf{\sigma}=\sqrt{\frac{1}{d}\sum_{i=1}^{d}(x_{i}-\mathbf{\mu}))^{2}}" display="inline"><semantics id="S4.T6.18.4.1.m1.2a"><mrow id="S4.T6.18.4.1.m1.2.2.2"><mrow id="S4.T6.18.4.1.m1.1.1.1.1"><mrow id="S4.T6.18.4.1.m1.1.1.1.1.2.2"><mrow id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1"><mrow id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2"><mfrac id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.2"><mrow id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.2.2"><mi id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.2.2.2">𝐱</mi><mo id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.2.2.1">−</mo><mi id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.2.2.3">μ</mi></mrow><mi id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.2.3">σ</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.1">⋅</mo><mi id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.2.3">γ</mi></mrow><mo id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.1">+</mo><mi id="S4.T6.18.4.1.m1.1.1.1.1.1.1.1.3">β</mi></mrow><mo id="S4.T6.18.4.1.m1.1.1.1.1.2.2.3">,</mo><mrow id="S4.T6.18.4.1.m1.1.1.1.1.2.2.2"><mtext id="S4.T6.18.4.1.m1.1.1.1.1.2.2.2.2">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S4.T6.18.4.1.m1.1.1.1.1.2.2.2.1">​</mo><mi id="S4.T6.18.4.1.m1.1.1.1.1.2.2.2.3">μ</mi></mrow></mrow><mo id="S4.T6.18.4.1.m1.1.1.1.1.3">=</mo><mrow id="S4.T6.18.4.1.m1.1.1.1.1.4"><mfrac id="S4.T6.18.4.1.m1.1.1.1.1.4.2"><mn id="S4.T6.18.4.1.m1.1.1.1.1.4.2.2">1</mn><mi id="S4.T6.18.4.1.m1.1.1.1.1.4.2.3">d</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.T6.18.4.1.m1.1.1.1.1.4.1">​</mo><mrow id="S4.T6.18.4.1.m1.1.1.1.1.4.3"><msubsup id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1"><mo id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1.2.2">∑</mo><mrow id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1.2.3"><mi id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1.2.3.2">i</mi><mo id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1.2.3.1">=</mo><mn id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1.2.3.3">1</mn></mrow><mi id="S4.T6.18.4.1.m1.1.1.1.1.4.3.1.3">d</mi></msubsup><msub id="S4.T6.18.4.1.m1.1.1.1.1.4.3.2"><mi id="S4.T6.18.4.1.m1.1.1.1.1.4.3.2.2">x</mi><mi id="S4.T6.18.4.1.m1.1.1.1.1.4.3.2.3">i</mi></msub></mrow></mrow></mrow><mo id="S4.T6.18.4.1.m1.2.2.2.3">,</mo><mrow id="S4.T6.18.4.1.m1.2.2.2.2"><mrow id="S4.T6.18.4.1.m1.2.2.2.2.2"><mtext id="S4.T6.18.4.1.m1.2.2.2.2.2.2">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S4.T6.18.4.1.m1.2.2.2.2.2.1">​</mo><mi id="S4.T6.18.4.1.m1.2.2.2.2.2.3">σ</mi></mrow><mo id="S4.T6.18.4.1.m1.2.2.2.2.1">=</mo><msqrt id="S4.T6.18.4.1.m1.2.2.2.2.3"><mrow id="S4.T6.18.4.1.m1.2.2.2.2.3.2"><mfrac id="S4.T6.18.4.1.m1.2.2.2.2.3.2.1"><mn id="S4.T6.18.4.1.m1.2.2.2.2.3.2.1.2">1</mn><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.1.3">d</mi></mfrac><msubsup id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2"><mo rspace="0em" id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2.2.2">∑</mo><mrow id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2.2.3"><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2.2.3.2">i</mi><mo id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2.2.3.1">=</mo><mn id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2.2.3.3">1</mn></mrow><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.2.3">d</mi></msubsup><mrow id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3"><mo stretchy="false" id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.1">(</mo><msub id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.2"><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.2.2">x</mi><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.2.3">i</mi></msub><mo id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.3">−</mo><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.4">μ</mi><mo stretchy="false" id="S4.T6.18.4.1.m1.2.2.2.2.3.2.3.5">)</mo></mrow><mo stretchy="false" id="S4.T6.18.4.1.m1.2.2.2.2.3.2.4">)</mo><msup id="S4.T6.18.4.1.m1.2.2.2.2.3.2.5"><mi id="S4.T6.18.4.1.m1.2.2.2.2.3.2.5a"></mi><mn id="S4.T6.18.4.1.m1.2.2.2.2.3.2.5.1">2</mn></msup></mrow></msqrt></mrow></mrow><annotation encoding="application/x-tex" id="S4.T6.18.4.1.m1.2b">\frac{\mathbf{x}-\mathbf{\mu}}{\mathbf{\sigma}}\cdot\gamma+\beta,\text{~{}~{}~{}}\mathbf{\mu}=\frac{1}{d}\sum_{i=1}^{d}x_{i},\text{~{}~{}~{}}\mathbf{\sigma}=\sqrt{\frac{1}{d}\sum_{i=1}^{d}(x_{i}-\mathbf{\mu}))^{2}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.19.5" class="ltx_tr">
<td id="S4.T6.19.5.2" class="ltx_td ltx_align_center ltx_border_r">RMSNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib257" title="" class="ltx_ref">257</a>]</cite>
</td>
<td id="S4.T6.19.5.1" class="ltx_td ltx_align_left"><math id="S4.T6.19.5.1.m1.4" class="ltx_Math" alttext="\frac{\mathbf{x}}{\mathrm{RMS}(\mathbf{x})}\cdot\gamma,\text{~{}~{}~{}}\mathrm{RMS}(\mathbf{x})=\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_{i}^{2}}" display="inline"><semantics id="S4.T6.19.5.1.m1.4a"><mrow id="S4.T6.19.5.1.m1.4.4" xref="S4.T6.19.5.1.m1.4.4.cmml"><mrow id="S4.T6.19.5.1.m1.4.4.2.2" xref="S4.T6.19.5.1.m1.4.4.2.3.cmml"><mrow id="S4.T6.19.5.1.m1.3.3.1.1.1" xref="S4.T6.19.5.1.m1.3.3.1.1.1.cmml"><mfrac id="S4.T6.19.5.1.m1.1.1" xref="S4.T6.19.5.1.m1.1.1.cmml"><mi id="S4.T6.19.5.1.m1.1.1.3" xref="S4.T6.19.5.1.m1.1.1.3.cmml">𝐱</mi><mrow id="S4.T6.19.5.1.m1.1.1.1" xref="S4.T6.19.5.1.m1.1.1.1.cmml"><mi id="S4.T6.19.5.1.m1.1.1.1.3" xref="S4.T6.19.5.1.m1.1.1.1.3.cmml">RMS</mi><mo lspace="0em" rspace="0em" id="S4.T6.19.5.1.m1.1.1.1.2" xref="S4.T6.19.5.1.m1.1.1.1.2.cmml">​</mo><mrow id="S4.T6.19.5.1.m1.1.1.1.4.2" xref="S4.T6.19.5.1.m1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.19.5.1.m1.1.1.1.4.2.1" xref="S4.T6.19.5.1.m1.1.1.1.cmml">(</mo><mi id="S4.T6.19.5.1.m1.1.1.1.1" xref="S4.T6.19.5.1.m1.1.1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.19.5.1.m1.1.1.1.4.2.2" xref="S4.T6.19.5.1.m1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo lspace="0.222em" rspace="0.222em" id="S4.T6.19.5.1.m1.3.3.1.1.1.1" xref="S4.T6.19.5.1.m1.3.3.1.1.1.1.cmml">⋅</mo><mi id="S4.T6.19.5.1.m1.3.3.1.1.1.2" xref="S4.T6.19.5.1.m1.3.3.1.1.1.2.cmml">γ</mi></mrow><mo id="S4.T6.19.5.1.m1.4.4.2.2.3" xref="S4.T6.19.5.1.m1.4.4.2.3.cmml">,</mo><mrow id="S4.T6.19.5.1.m1.4.4.2.2.2" xref="S4.T6.19.5.1.m1.4.4.2.2.2.cmml"><mtext id="S4.T6.19.5.1.m1.4.4.2.2.2.2" xref="S4.T6.19.5.1.m1.4.4.2.2.2.2a.cmml">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S4.T6.19.5.1.m1.4.4.2.2.2.1" xref="S4.T6.19.5.1.m1.4.4.2.2.2.1.cmml">​</mo><mi id="S4.T6.19.5.1.m1.4.4.2.2.2.3" xref="S4.T6.19.5.1.m1.4.4.2.2.2.3.cmml">RMS</mi><mo lspace="0em" rspace="0em" id="S4.T6.19.5.1.m1.4.4.2.2.2.1a" xref="S4.T6.19.5.1.m1.4.4.2.2.2.1.cmml">​</mo><mrow id="S4.T6.19.5.1.m1.4.4.2.2.2.4.2" xref="S4.T6.19.5.1.m1.4.4.2.2.2.cmml"><mo stretchy="false" id="S4.T6.19.5.1.m1.4.4.2.2.2.4.2.1" xref="S4.T6.19.5.1.m1.4.4.2.2.2.cmml">(</mo><mi id="S4.T6.19.5.1.m1.2.2" xref="S4.T6.19.5.1.m1.2.2.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.19.5.1.m1.4.4.2.2.2.4.2.2" xref="S4.T6.19.5.1.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.T6.19.5.1.m1.4.4.3" xref="S4.T6.19.5.1.m1.4.4.3.cmml">=</mo><msqrt id="S4.T6.19.5.1.m1.4.4.4" xref="S4.T6.19.5.1.m1.4.4.4.cmml"><mrow id="S4.T6.19.5.1.m1.4.4.4.2" xref="S4.T6.19.5.1.m1.4.4.4.2.cmml"><mfrac id="S4.T6.19.5.1.m1.4.4.4.2.2" xref="S4.T6.19.5.1.m1.4.4.4.2.2.cmml"><mn id="S4.T6.19.5.1.m1.4.4.4.2.2.2" xref="S4.T6.19.5.1.m1.4.4.4.2.2.2.cmml">1</mn><mi id="S4.T6.19.5.1.m1.4.4.4.2.2.3" xref="S4.T6.19.5.1.m1.4.4.4.2.2.3.cmml">d</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.T6.19.5.1.m1.4.4.4.2.1" xref="S4.T6.19.5.1.m1.4.4.4.2.1.cmml">​</mo><mrow id="S4.T6.19.5.1.m1.4.4.4.2.3" xref="S4.T6.19.5.1.m1.4.4.4.2.3.cmml"><msubsup id="S4.T6.19.5.1.m1.4.4.4.2.3.1" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.cmml"><mo id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.2" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.2.cmml">∑</mo><mrow id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.cmml"><mi id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.2" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.2.cmml">i</mi><mo id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.1" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.1.cmml">=</mo><mn id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.3" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.T6.19.5.1.m1.4.4.4.2.3.1.3" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.3.cmml">d</mi></msubsup><msubsup id="S4.T6.19.5.1.m1.4.4.4.2.3.2" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.cmml"><mi id="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.2" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.2.cmml">x</mi><mi id="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.3" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.3.cmml">i</mi><mn id="S4.T6.19.5.1.m1.4.4.4.2.3.2.3" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.3.cmml">2</mn></msubsup></mrow></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.19.5.1.m1.4b"><apply id="S4.T6.19.5.1.m1.4.4.cmml" xref="S4.T6.19.5.1.m1.4.4"><eq id="S4.T6.19.5.1.m1.4.4.3.cmml" xref="S4.T6.19.5.1.m1.4.4.3"></eq><list id="S4.T6.19.5.1.m1.4.4.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.2.2"><apply id="S4.T6.19.5.1.m1.3.3.1.1.1.cmml" xref="S4.T6.19.5.1.m1.3.3.1.1.1"><ci id="S4.T6.19.5.1.m1.3.3.1.1.1.1.cmml" xref="S4.T6.19.5.1.m1.3.3.1.1.1.1">⋅</ci><apply id="S4.T6.19.5.1.m1.1.1.cmml" xref="S4.T6.19.5.1.m1.1.1"><divide id="S4.T6.19.5.1.m1.1.1.2.cmml" xref="S4.T6.19.5.1.m1.1.1"></divide><ci id="S4.T6.19.5.1.m1.1.1.3.cmml" xref="S4.T6.19.5.1.m1.1.1.3">𝐱</ci><apply id="S4.T6.19.5.1.m1.1.1.1.cmml" xref="S4.T6.19.5.1.m1.1.1.1"><times id="S4.T6.19.5.1.m1.1.1.1.2.cmml" xref="S4.T6.19.5.1.m1.1.1.1.2"></times><ci id="S4.T6.19.5.1.m1.1.1.1.3.cmml" xref="S4.T6.19.5.1.m1.1.1.1.3">RMS</ci><ci id="S4.T6.19.5.1.m1.1.1.1.1.cmml" xref="S4.T6.19.5.1.m1.1.1.1.1">𝐱</ci></apply></apply><ci id="S4.T6.19.5.1.m1.3.3.1.1.1.2.cmml" xref="S4.T6.19.5.1.m1.3.3.1.1.1.2">𝛾</ci></apply><apply id="S4.T6.19.5.1.m1.4.4.2.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.2.2.2"><times id="S4.T6.19.5.1.m1.4.4.2.2.2.1.cmml" xref="S4.T6.19.5.1.m1.4.4.2.2.2.1"></times><ci id="S4.T6.19.5.1.m1.4.4.2.2.2.2a.cmml" xref="S4.T6.19.5.1.m1.4.4.2.2.2.2"><mtext id="S4.T6.19.5.1.m1.4.4.2.2.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.2.2.2.2">&nbsp;</mtext></ci><ci id="S4.T6.19.5.1.m1.4.4.2.2.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.2.2.2.3">RMS</ci><ci id="S4.T6.19.5.1.m1.2.2.cmml" xref="S4.T6.19.5.1.m1.2.2">𝐱</ci></apply></list><apply id="S4.T6.19.5.1.m1.4.4.4.cmml" xref="S4.T6.19.5.1.m1.4.4.4"><root id="S4.T6.19.5.1.m1.4.4.4a.cmml" xref="S4.T6.19.5.1.m1.4.4.4"></root><apply id="S4.T6.19.5.1.m1.4.4.4.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2"><times id="S4.T6.19.5.1.m1.4.4.4.2.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.1"></times><apply id="S4.T6.19.5.1.m1.4.4.4.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.2"><divide id="S4.T6.19.5.1.m1.4.4.4.2.2.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.2"></divide><cn type="integer" id="S4.T6.19.5.1.m1.4.4.4.2.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.2.2">1</cn><ci id="S4.T6.19.5.1.m1.4.4.4.2.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.2.3">𝑑</ci></apply><apply id="S4.T6.19.5.1.m1.4.4.4.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3"><apply id="S4.T6.19.5.1.m1.4.4.4.2.3.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1"><csymbol cd="ambiguous" id="S4.T6.19.5.1.m1.4.4.4.2.3.1.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1">superscript</csymbol><apply id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1"><csymbol cd="ambiguous" id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1">subscript</csymbol><sum id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.2"></sum><apply id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3"><eq id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.1"></eq><ci id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.2.3.3">1</cn></apply></apply><ci id="S4.T6.19.5.1.m1.4.4.4.2.3.1.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.1.3">𝑑</ci></apply><apply id="S4.T6.19.5.1.m1.4.4.4.2.3.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2"><csymbol cd="ambiguous" id="S4.T6.19.5.1.m1.4.4.4.2.3.2.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2">superscript</csymbol><apply id="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2"><csymbol cd="ambiguous" id="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.1.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2">subscript</csymbol><ci id="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.2.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.2">𝑥</ci><ci id="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.2.3">𝑖</ci></apply><cn type="integer" id="S4.T6.19.5.1.m1.4.4.4.2.3.2.3.cmml" xref="S4.T6.19.5.1.m1.4.4.4.2.3.2.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.19.5.1.m1.4c">\frac{\mathbf{x}}{\mathrm{RMS}(\mathbf{x})}\cdot\gamma,\text{~{}~{}~{}}\mathrm{RMS}(\mathbf{x})=\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_{i}^{2}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.20.6" class="ltx_tr">
<td id="S4.T6.20.6.2" class="ltx_td ltx_align_center ltx_border_r">DeepNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite>
</td>
<td id="S4.T6.20.6.1" class="ltx_td ltx_align_left"><math id="S4.T6.20.6.1.m1.2" class="ltx_Math" alttext="\mathrm{LayerNorm}(\alpha\cdot\mathbf{x}+\mathrm{Sublayer}(\mathbf{x}))" display="inline"><semantics id="S4.T6.20.6.1.m1.2a"><mrow id="S4.T6.20.6.1.m1.2.2" xref="S4.T6.20.6.1.m1.2.2.cmml"><mi id="S4.T6.20.6.1.m1.2.2.3" xref="S4.T6.20.6.1.m1.2.2.3.cmml">LayerNorm</mi><mo lspace="0em" rspace="0em" id="S4.T6.20.6.1.m1.2.2.2" xref="S4.T6.20.6.1.m1.2.2.2.cmml">​</mo><mrow id="S4.T6.20.6.1.m1.2.2.1.1" xref="S4.T6.20.6.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.T6.20.6.1.m1.2.2.1.1.2" xref="S4.T6.20.6.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S4.T6.20.6.1.m1.2.2.1.1.1" xref="S4.T6.20.6.1.m1.2.2.1.1.1.cmml"><mrow id="S4.T6.20.6.1.m1.2.2.1.1.1.2" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.cmml"><mi id="S4.T6.20.6.1.m1.2.2.1.1.1.2.2" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.2.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S4.T6.20.6.1.m1.2.2.1.1.1.2.1" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.1.cmml">⋅</mo><mi id="S4.T6.20.6.1.m1.2.2.1.1.1.2.3" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.3.cmml">𝐱</mi></mrow><mo id="S4.T6.20.6.1.m1.2.2.1.1.1.1" xref="S4.T6.20.6.1.m1.2.2.1.1.1.1.cmml">+</mo><mrow id="S4.T6.20.6.1.m1.2.2.1.1.1.3" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.cmml"><mi id="S4.T6.20.6.1.m1.2.2.1.1.1.3.2" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.2.cmml">Sublayer</mi><mo lspace="0em" rspace="0em" id="S4.T6.20.6.1.m1.2.2.1.1.1.3.1" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.1.cmml">​</mo><mrow id="S4.T6.20.6.1.m1.2.2.1.1.1.3.3.2" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.cmml"><mo stretchy="false" id="S4.T6.20.6.1.m1.2.2.1.1.1.3.3.2.1" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.cmml">(</mo><mi id="S4.T6.20.6.1.m1.1.1" xref="S4.T6.20.6.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.20.6.1.m1.2.2.1.1.1.3.3.2.2" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S4.T6.20.6.1.m1.2.2.1.1.3" xref="S4.T6.20.6.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.20.6.1.m1.2b"><apply id="S4.T6.20.6.1.m1.2.2.cmml" xref="S4.T6.20.6.1.m1.2.2"><times id="S4.T6.20.6.1.m1.2.2.2.cmml" xref="S4.T6.20.6.1.m1.2.2.2"></times><ci id="S4.T6.20.6.1.m1.2.2.3.cmml" xref="S4.T6.20.6.1.m1.2.2.3">LayerNorm</ci><apply id="S4.T6.20.6.1.m1.2.2.1.1.1.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1"><plus id="S4.T6.20.6.1.m1.2.2.1.1.1.1.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.1"></plus><apply id="S4.T6.20.6.1.m1.2.2.1.1.1.2.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2"><ci id="S4.T6.20.6.1.m1.2.2.1.1.1.2.1.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.1">⋅</ci><ci id="S4.T6.20.6.1.m1.2.2.1.1.1.2.2.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.2">𝛼</ci><ci id="S4.T6.20.6.1.m1.2.2.1.1.1.2.3.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.2.3">𝐱</ci></apply><apply id="S4.T6.20.6.1.m1.2.2.1.1.1.3.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3"><times id="S4.T6.20.6.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.1"></times><ci id="S4.T6.20.6.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.T6.20.6.1.m1.2.2.1.1.1.3.2">Sublayer</ci><ci id="S4.T6.20.6.1.m1.1.1.cmml" xref="S4.T6.20.6.1.m1.1.1">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.20.6.1.m1.2c">\mathrm{LayerNorm}(\alpha\cdot\mathbf{x}+\mathrm{Sublayer}(\mathbf{x}))</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.21.7" class="ltx_tr">
<td id="S4.T6.21.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T6.21.7.2.1" class="ltx_text">Activation function</span></td>
<td id="S4.T6.21.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ReLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib259" title="" class="ltx_ref">259</a>]</cite>
</td>
<td id="S4.T6.21.7.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T6.21.7.1.m1.4" class="ltx_Math" alttext="\mathrm{ReLU}(\mathbf{x})=\max(\mathbf{x},\mathbf{0})" display="inline"><semantics id="S4.T6.21.7.1.m1.4a"><mrow id="S4.T6.21.7.1.m1.4.5" xref="S4.T6.21.7.1.m1.4.5.cmml"><mrow id="S4.T6.21.7.1.m1.4.5.2" xref="S4.T6.21.7.1.m1.4.5.2.cmml"><mi id="S4.T6.21.7.1.m1.4.5.2.2" xref="S4.T6.21.7.1.m1.4.5.2.2.cmml">ReLU</mi><mo lspace="0em" rspace="0em" id="S4.T6.21.7.1.m1.4.5.2.1" xref="S4.T6.21.7.1.m1.4.5.2.1.cmml">​</mo><mrow id="S4.T6.21.7.1.m1.4.5.2.3.2" xref="S4.T6.21.7.1.m1.4.5.2.cmml"><mo stretchy="false" id="S4.T6.21.7.1.m1.4.5.2.3.2.1" xref="S4.T6.21.7.1.m1.4.5.2.cmml">(</mo><mi id="S4.T6.21.7.1.m1.1.1" xref="S4.T6.21.7.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.21.7.1.m1.4.5.2.3.2.2" xref="S4.T6.21.7.1.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="S4.T6.21.7.1.m1.4.5.1" xref="S4.T6.21.7.1.m1.4.5.1.cmml">=</mo><mrow id="S4.T6.21.7.1.m1.4.5.3.2" xref="S4.T6.21.7.1.m1.4.5.3.1.cmml"><mi id="S4.T6.21.7.1.m1.2.2" xref="S4.T6.21.7.1.m1.2.2.cmml">max</mi><mo id="S4.T6.21.7.1.m1.4.5.3.2a" xref="S4.T6.21.7.1.m1.4.5.3.1.cmml">⁡</mo><mrow id="S4.T6.21.7.1.m1.4.5.3.2.1" xref="S4.T6.21.7.1.m1.4.5.3.1.cmml"><mo stretchy="false" id="S4.T6.21.7.1.m1.4.5.3.2.1.1" xref="S4.T6.21.7.1.m1.4.5.3.1.cmml">(</mo><mi id="S4.T6.21.7.1.m1.3.3" xref="S4.T6.21.7.1.m1.3.3.cmml">𝐱</mi><mo id="S4.T6.21.7.1.m1.4.5.3.2.1.2" xref="S4.T6.21.7.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.T6.21.7.1.m1.4.4" xref="S4.T6.21.7.1.m1.4.4.cmml">𝟎</mn><mo stretchy="false" id="S4.T6.21.7.1.m1.4.5.3.2.1.3" xref="S4.T6.21.7.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.21.7.1.m1.4b"><apply id="S4.T6.21.7.1.m1.4.5.cmml" xref="S4.T6.21.7.1.m1.4.5"><eq id="S4.T6.21.7.1.m1.4.5.1.cmml" xref="S4.T6.21.7.1.m1.4.5.1"></eq><apply id="S4.T6.21.7.1.m1.4.5.2.cmml" xref="S4.T6.21.7.1.m1.4.5.2"><times id="S4.T6.21.7.1.m1.4.5.2.1.cmml" xref="S4.T6.21.7.1.m1.4.5.2.1"></times><ci id="S4.T6.21.7.1.m1.4.5.2.2.cmml" xref="S4.T6.21.7.1.m1.4.5.2.2">ReLU</ci><ci id="S4.T6.21.7.1.m1.1.1.cmml" xref="S4.T6.21.7.1.m1.1.1">𝐱</ci></apply><apply id="S4.T6.21.7.1.m1.4.5.3.1.cmml" xref="S4.T6.21.7.1.m1.4.5.3.2"><max id="S4.T6.21.7.1.m1.2.2.cmml" xref="S4.T6.21.7.1.m1.2.2"></max><ci id="S4.T6.21.7.1.m1.3.3.cmml" xref="S4.T6.21.7.1.m1.3.3">𝐱</ci><cn type="integer" id="S4.T6.21.7.1.m1.4.4.cmml" xref="S4.T6.21.7.1.m1.4.4">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.21.7.1.m1.4c">\mathrm{ReLU}(\mathbf{x})=\max(\mathbf{x},\mathbf{0})</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.22.8" class="ltx_tr">
<td id="S4.T6.22.8.2" class="ltx_td ltx_align_center ltx_border_r">GeLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib260" title="" class="ltx_ref">260</a>]</cite>
</td>
<td id="S4.T6.22.8.1" class="ltx_td ltx_align_left"><math id="S4.T6.22.8.1.m1.4" class="ltx_Math" alttext="\mathrm{GeLU}(\mathbf{x})=\mathrm{0.5}\mathbf{x}\otimes[1+\mathrm{erf}(\mathbf{x}/\sqrt{2})],\text{~{}~{}~{}}\mathrm{erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt" display="inline"><semantics id="S4.T6.22.8.1.m1.4a"><mrow id="S4.T6.22.8.1.m1.4.4.2" xref="S4.T6.22.8.1.m1.4.4.3.cmml"><mrow id="S4.T6.22.8.1.m1.3.3.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.cmml"><mrow id="S4.T6.22.8.1.m1.3.3.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.3.cmml"><mi id="S4.T6.22.8.1.m1.3.3.1.1.3.2" xref="S4.T6.22.8.1.m1.3.3.1.1.3.2.cmml">GeLU</mi><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.3.3.1.1.3.1" xref="S4.T6.22.8.1.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.3.3.2" xref="S4.T6.22.8.1.m1.3.3.1.1.3.cmml"><mo stretchy="false" id="S4.T6.22.8.1.m1.3.3.1.1.3.3.2.1" xref="S4.T6.22.8.1.m1.3.3.1.1.3.cmml">(</mo><mi id="S4.T6.22.8.1.m1.1.1" xref="S4.T6.22.8.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.22.8.1.m1.3.3.1.1.3.3.2.2" xref="S4.T6.22.8.1.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.T6.22.8.1.m1.3.3.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.cmml"><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.cmml"><mn id="S4.T6.22.8.1.m1.3.3.1.1.1.3.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.3.3.1.1.1.3.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.1.cmml">​</mo><mi id="S4.T6.22.8.1.m1.3.3.1.1.1.3.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.3.cmml">𝐱</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.T6.22.8.1.m1.3.3.1.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.2.cmml">⊗</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.cmml"><mn id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.3.cmml">1</mn><mo id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.3.cmml">erf</mi><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mo id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">/</mo><msqrt id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">2</mn></msqrt></mrow><mo stretchy="false" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.3" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.T6.22.8.1.m1.4.4.2.3" xref="S4.T6.22.8.1.m1.4.4.3a.cmml">,</mo><mrow id="S4.T6.22.8.1.m1.4.4.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.cmml"><mrow id="S4.T6.22.8.1.m1.4.4.2.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.2.cmml"><mtext id="S4.T6.22.8.1.m1.4.4.2.2.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.2.2a.cmml">&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.4.4.2.2.2.1" xref="S4.T6.22.8.1.m1.4.4.2.2.2.1.cmml">​</mo><mi id="S4.T6.22.8.1.m1.4.4.2.2.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.2.3.cmml">erf</mi><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.4.4.2.2.2.1a" xref="S4.T6.22.8.1.m1.4.4.2.2.2.1.cmml">​</mo><mrow id="S4.T6.22.8.1.m1.4.4.2.2.2.4.2" xref="S4.T6.22.8.1.m1.4.4.2.2.2.cmml"><mo stretchy="false" id="S4.T6.22.8.1.m1.4.4.2.2.2.4.2.1" xref="S4.T6.22.8.1.m1.4.4.2.2.2.cmml">(</mo><mi id="S4.T6.22.8.1.m1.2.2" xref="S4.T6.22.8.1.m1.2.2.cmml">x</mi><mo stretchy="false" id="S4.T6.22.8.1.m1.4.4.2.2.2.4.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow><mo id="S4.T6.22.8.1.m1.4.4.2.2.1" xref="S4.T6.22.8.1.m1.4.4.2.2.1.cmml">=</mo><mrow id="S4.T6.22.8.1.m1.4.4.2.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.cmml"><mfrac id="S4.T6.22.8.1.m1.4.4.2.2.3.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.cmml"><mn id="S4.T6.22.8.1.m1.4.4.2.2.3.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.2.cmml">2</mn><msqrt id="S4.T6.22.8.1.m1.4.4.2.2.3.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.3.cmml"><mi id="S4.T6.22.8.1.m1.4.4.2.2.3.2.3.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.3.2.cmml">π</mi></msqrt></mfrac><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.4.4.2.2.3.1" xref="S4.T6.22.8.1.m1.4.4.2.2.3.1.cmml">​</mo><mrow id="S4.T6.22.8.1.m1.4.4.2.2.3.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.cmml"><msubsup id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.cmml"><mo id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.2.cmml">∫</mo><mn id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.3.cmml">0</mn><mi id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.3.cmml">x</mi></msubsup><mrow id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.cmml"><msup id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.cmml"><mi id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.2.cmml">e</mi><mrow id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.cmml"><mo id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3a" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.cmml">−</mo><msup id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.cmml"><mi id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.2.cmml">t</mi><mn id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.3.cmml">2</mn></msup></mrow></msup><mo lspace="0em" rspace="0em" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.1" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.1.cmml">​</mo><mrow id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.cmml"><mo rspace="0em" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.1" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.1.cmml">𝑑</mo><mi id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.2" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.2.cmml">t</mi></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.22.8.1.m1.4b"><apply id="S4.T6.22.8.1.m1.4.4.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2"><csymbol cd="ambiguous" id="S4.T6.22.8.1.m1.4.4.3a.cmml" xref="S4.T6.22.8.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S4.T6.22.8.1.m1.3.3.1.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1"><eq id="S4.T6.22.8.1.m1.3.3.1.1.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.2"></eq><apply id="S4.T6.22.8.1.m1.3.3.1.1.3.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.3"><times id="S4.T6.22.8.1.m1.3.3.1.1.3.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.3.1"></times><ci id="S4.T6.22.8.1.m1.3.3.1.1.3.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.3.2">GeLU</ci><ci id="S4.T6.22.8.1.m1.1.1.cmml" xref="S4.T6.22.8.1.m1.1.1">𝐱</ci></apply><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S4.T6.22.8.1.m1.3.3.1.1.1.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.2">tensor-product</csymbol><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.3.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3"><times id="S4.T6.22.8.1.m1.3.3.1.1.1.3.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.1"></times><cn type="float" id="S4.T6.22.8.1.m1.3.3.1.1.1.3.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.2">0.5</cn><ci id="S4.T6.22.8.1.m1.3.3.1.1.1.3.3.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.3.3">𝐱</ci></apply><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.1.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1"><plus id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.2"></plus><cn type="integer" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.3">1</cn><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1"><times id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.2"></times><ci id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.3">erf</ci><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1"><divide id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"></divide><ci id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">𝐱</ci><apply id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><root id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"></root><cn type="integer" id="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.T6.22.8.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">2</cn></apply></apply></apply></apply></apply></apply></apply><apply id="S4.T6.22.8.1.m1.4.4.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2"><eq id="S4.T6.22.8.1.m1.4.4.2.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.1"></eq><apply id="S4.T6.22.8.1.m1.4.4.2.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.2"><times id="S4.T6.22.8.1.m1.4.4.2.2.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.2.1"></times><ci id="S4.T6.22.8.1.m1.4.4.2.2.2.2a.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.2.2"><mtext id="S4.T6.22.8.1.m1.4.4.2.2.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.2.2">&nbsp;</mtext></ci><ci id="S4.T6.22.8.1.m1.4.4.2.2.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.2.3">erf</ci><ci id="S4.T6.22.8.1.m1.2.2.cmml" xref="S4.T6.22.8.1.m1.2.2">𝑥</ci></apply><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3"><times id="S4.T6.22.8.1.m1.4.4.2.2.3.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.1"></times><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2"><divide id="S4.T6.22.8.1.m1.4.4.2.2.3.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2"></divide><cn type="integer" id="S4.T6.22.8.1.m1.4.4.2.2.3.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.2">2</cn><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.3"><root id="S4.T6.22.8.1.m1.4.4.2.2.3.2.3a.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.3"></root><ci id="S4.T6.22.8.1.m1.4.4.2.2.3.2.3.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.2.3.2">𝜋</ci></apply></apply><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3"><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1"><csymbol cd="ambiguous" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1">superscript</csymbol><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1"><csymbol cd="ambiguous" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1">subscript</csymbol><int id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.2"></int><cn type="integer" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.2.3">0</cn></apply><ci id="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.1.3">𝑥</ci></apply><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2"><times id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.1"></times><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2"><csymbol cd="ambiguous" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2">superscript</csymbol><ci id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.2">𝑒</ci><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3"><minus id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3"></minus><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2"><csymbol cd="ambiguous" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2">superscript</csymbol><ci id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.2">𝑡</ci><cn type="integer" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.2.3.2.3">2</cn></apply></apply></apply><apply id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3"><csymbol cd="latexml" id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.1.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.1">differential-d</csymbol><ci id="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.2.cmml" xref="S4.T6.22.8.1.m1.4.4.2.2.3.3.2.3.2">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.22.8.1.m1.4c">\mathrm{GeLU}(\mathbf{x})=\mathrm{0.5}\mathbf{x}\otimes[1+\mathrm{erf}(\mathbf{x}/\sqrt{2})],\text{~{}~{}~{}}\mathrm{erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.23.9" class="ltx_tr">
<td id="S4.T6.23.9.2" class="ltx_td ltx_align_center ltx_border_r">Swish&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib261" title="" class="ltx_ref">261</a>]</cite>
</td>
<td id="S4.T6.23.9.1" class="ltx_td ltx_align_left"><math id="S4.T6.23.9.1.m1.2" class="ltx_Math" alttext="\mathrm{Swish}(\mathbf{x})=\mathbf{x}\otimes\mathrm{sigmoid}(\mathbf{x})" display="inline"><semantics id="S4.T6.23.9.1.m1.2a"><mrow id="S4.T6.23.9.1.m1.2.3" xref="S4.T6.23.9.1.m1.2.3.cmml"><mrow id="S4.T6.23.9.1.m1.2.3.2" xref="S4.T6.23.9.1.m1.2.3.2.cmml"><mi id="S4.T6.23.9.1.m1.2.3.2.2" xref="S4.T6.23.9.1.m1.2.3.2.2.cmml">Swish</mi><mo lspace="0em" rspace="0em" id="S4.T6.23.9.1.m1.2.3.2.1" xref="S4.T6.23.9.1.m1.2.3.2.1.cmml">​</mo><mrow id="S4.T6.23.9.1.m1.2.3.2.3.2" xref="S4.T6.23.9.1.m1.2.3.2.cmml"><mo stretchy="false" id="S4.T6.23.9.1.m1.2.3.2.3.2.1" xref="S4.T6.23.9.1.m1.2.3.2.cmml">(</mo><mi id="S4.T6.23.9.1.m1.1.1" xref="S4.T6.23.9.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.23.9.1.m1.2.3.2.3.2.2" xref="S4.T6.23.9.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S4.T6.23.9.1.m1.2.3.1" xref="S4.T6.23.9.1.m1.2.3.1.cmml">=</mo><mrow id="S4.T6.23.9.1.m1.2.3.3" xref="S4.T6.23.9.1.m1.2.3.3.cmml"><mrow id="S4.T6.23.9.1.m1.2.3.3.2" xref="S4.T6.23.9.1.m1.2.3.3.2.cmml"><mi id="S4.T6.23.9.1.m1.2.3.3.2.2" xref="S4.T6.23.9.1.m1.2.3.3.2.2.cmml">𝐱</mi><mo lspace="0.222em" rspace="0.222em" id="S4.T6.23.9.1.m1.2.3.3.2.1" xref="S4.T6.23.9.1.m1.2.3.3.2.1.cmml">⊗</mo><mi id="S4.T6.23.9.1.m1.2.3.3.2.3" xref="S4.T6.23.9.1.m1.2.3.3.2.3.cmml">sigmoid</mi></mrow><mo lspace="0em" rspace="0em" id="S4.T6.23.9.1.m1.2.3.3.1" xref="S4.T6.23.9.1.m1.2.3.3.1.cmml">​</mo><mrow id="S4.T6.23.9.1.m1.2.3.3.3.2" xref="S4.T6.23.9.1.m1.2.3.3.cmml"><mo stretchy="false" id="S4.T6.23.9.1.m1.2.3.3.3.2.1" xref="S4.T6.23.9.1.m1.2.3.3.cmml">(</mo><mi id="S4.T6.23.9.1.m1.2.2" xref="S4.T6.23.9.1.m1.2.2.cmml">𝐱</mi><mo stretchy="false" id="S4.T6.23.9.1.m1.2.3.3.3.2.2" xref="S4.T6.23.9.1.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.23.9.1.m1.2b"><apply id="S4.T6.23.9.1.m1.2.3.cmml" xref="S4.T6.23.9.1.m1.2.3"><eq id="S4.T6.23.9.1.m1.2.3.1.cmml" xref="S4.T6.23.9.1.m1.2.3.1"></eq><apply id="S4.T6.23.9.1.m1.2.3.2.cmml" xref="S4.T6.23.9.1.m1.2.3.2"><times id="S4.T6.23.9.1.m1.2.3.2.1.cmml" xref="S4.T6.23.9.1.m1.2.3.2.1"></times><ci id="S4.T6.23.9.1.m1.2.3.2.2.cmml" xref="S4.T6.23.9.1.m1.2.3.2.2">Swish</ci><ci id="S4.T6.23.9.1.m1.1.1.cmml" xref="S4.T6.23.9.1.m1.1.1">𝐱</ci></apply><apply id="S4.T6.23.9.1.m1.2.3.3.cmml" xref="S4.T6.23.9.1.m1.2.3.3"><times id="S4.T6.23.9.1.m1.2.3.3.1.cmml" xref="S4.T6.23.9.1.m1.2.3.3.1"></times><apply id="S4.T6.23.9.1.m1.2.3.3.2.cmml" xref="S4.T6.23.9.1.m1.2.3.3.2"><csymbol cd="latexml" id="S4.T6.23.9.1.m1.2.3.3.2.1.cmml" xref="S4.T6.23.9.1.m1.2.3.3.2.1">tensor-product</csymbol><ci id="S4.T6.23.9.1.m1.2.3.3.2.2.cmml" xref="S4.T6.23.9.1.m1.2.3.3.2.2">𝐱</ci><ci id="S4.T6.23.9.1.m1.2.3.3.2.3.cmml" xref="S4.T6.23.9.1.m1.2.3.3.2.3">sigmoid</ci></apply><ci id="S4.T6.23.9.1.m1.2.2.cmml" xref="S4.T6.23.9.1.m1.2.2">𝐱</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.23.9.1.m1.2c">\mathrm{Swish}(\mathbf{x})=\mathbf{x}\otimes\mathrm{sigmoid}(\mathbf{x})</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.24.10" class="ltx_tr">
<td id="S4.T6.24.10.2" class="ltx_td ltx_align_center ltx_border_r">SwiGLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib262" title="" class="ltx_ref">262</a>]</cite>
</td>
<td id="S4.T6.24.10.1" class="ltx_td ltx_align_left"><math id="S4.T6.24.10.1.m1.3" class="ltx_Math" alttext="\mathrm{SwiGLU}(\mathbf{x}_{1},\mathbf{x}_{2})=\mathrm{Swish}(\mathbf{x_{1}})\otimes\mathbf{x_{2}}" display="inline"><semantics id="S4.T6.24.10.1.m1.3a"><mrow id="S4.T6.24.10.1.m1.3.3" xref="S4.T6.24.10.1.m1.3.3.cmml"><mrow id="S4.T6.24.10.1.m1.2.2.2" xref="S4.T6.24.10.1.m1.2.2.2.cmml"><mi id="S4.T6.24.10.1.m1.2.2.2.4" xref="S4.T6.24.10.1.m1.2.2.2.4.cmml">SwiGLU</mi><mo lspace="0em" rspace="0em" id="S4.T6.24.10.1.m1.2.2.2.3" xref="S4.T6.24.10.1.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T6.24.10.1.m1.2.2.2.2.2" xref="S4.T6.24.10.1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.T6.24.10.1.m1.2.2.2.2.2.3" xref="S4.T6.24.10.1.m1.2.2.2.2.3.cmml">(</mo><msub id="S4.T6.24.10.1.m1.1.1.1.1.1.1" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1.cmml"><mi id="S4.T6.24.10.1.m1.1.1.1.1.1.1.2" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1.2.cmml">𝐱</mi><mn id="S4.T6.24.10.1.m1.1.1.1.1.1.1.3" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.T6.24.10.1.m1.2.2.2.2.2.4" xref="S4.T6.24.10.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S4.T6.24.10.1.m1.2.2.2.2.2.2" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2.cmml"><mi id="S4.T6.24.10.1.m1.2.2.2.2.2.2.2" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2.2.cmml">𝐱</mi><mn id="S4.T6.24.10.1.m1.2.2.2.2.2.2.3" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S4.T6.24.10.1.m1.2.2.2.2.2.5" xref="S4.T6.24.10.1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.T6.24.10.1.m1.3.3.4" xref="S4.T6.24.10.1.m1.3.3.4.cmml">=</mo><mrow id="S4.T6.24.10.1.m1.3.3.3" xref="S4.T6.24.10.1.m1.3.3.3.cmml"><mrow id="S4.T6.24.10.1.m1.3.3.3.1" xref="S4.T6.24.10.1.m1.3.3.3.1.cmml"><mi id="S4.T6.24.10.1.m1.3.3.3.1.3" xref="S4.T6.24.10.1.m1.3.3.3.1.3.cmml">Swish</mi><mo lspace="0em" rspace="0em" id="S4.T6.24.10.1.m1.3.3.3.1.2" xref="S4.T6.24.10.1.m1.3.3.3.1.2.cmml">​</mo><mrow id="S4.T6.24.10.1.m1.3.3.3.1.1.1" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.24.10.1.m1.3.3.3.1.1.1.2" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.cmml">(</mo><msub id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.cmml"><mi id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.2" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.2.cmml">𝐱</mi><mn id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.3" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.3.cmml">𝟏</mn></msub><mo rspace="0.055em" stretchy="false" id="S4.T6.24.10.1.m1.3.3.3.1.1.1.3" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S4.T6.24.10.1.m1.3.3.3.2" xref="S4.T6.24.10.1.m1.3.3.3.2.cmml">⊗</mo><msub id="S4.T6.24.10.1.m1.3.3.3.3" xref="S4.T6.24.10.1.m1.3.3.3.3.cmml"><mi id="S4.T6.24.10.1.m1.3.3.3.3.2" xref="S4.T6.24.10.1.m1.3.3.3.3.2.cmml">𝐱</mi><mn id="S4.T6.24.10.1.m1.3.3.3.3.3" xref="S4.T6.24.10.1.m1.3.3.3.3.3.cmml">𝟐</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.24.10.1.m1.3b"><apply id="S4.T6.24.10.1.m1.3.3.cmml" xref="S4.T6.24.10.1.m1.3.3"><eq id="S4.T6.24.10.1.m1.3.3.4.cmml" xref="S4.T6.24.10.1.m1.3.3.4"></eq><apply id="S4.T6.24.10.1.m1.2.2.2.cmml" xref="S4.T6.24.10.1.m1.2.2.2"><times id="S4.T6.24.10.1.m1.2.2.2.3.cmml" xref="S4.T6.24.10.1.m1.2.2.2.3"></times><ci id="S4.T6.24.10.1.m1.2.2.2.4.cmml" xref="S4.T6.24.10.1.m1.2.2.2.4">SwiGLU</ci><interval closure="open" id="S4.T6.24.10.1.m1.2.2.2.2.3.cmml" xref="S4.T6.24.10.1.m1.2.2.2.2.2"><apply id="S4.T6.24.10.1.m1.1.1.1.1.1.1.cmml" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.T6.24.10.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.T6.24.10.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1.2">𝐱</ci><cn type="integer" id="S4.T6.24.10.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.T6.24.10.1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S4.T6.24.10.1.m1.2.2.2.2.2.2.cmml" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.T6.24.10.1.m1.2.2.2.2.2.2.1.cmml" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S4.T6.24.10.1.m1.2.2.2.2.2.2.2.cmml" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2.2">𝐱</ci><cn type="integer" id="S4.T6.24.10.1.m1.2.2.2.2.2.2.3.cmml" xref="S4.T6.24.10.1.m1.2.2.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S4.T6.24.10.1.m1.3.3.3.cmml" xref="S4.T6.24.10.1.m1.3.3.3"><csymbol cd="latexml" id="S4.T6.24.10.1.m1.3.3.3.2.cmml" xref="S4.T6.24.10.1.m1.3.3.3.2">tensor-product</csymbol><apply id="S4.T6.24.10.1.m1.3.3.3.1.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1"><times id="S4.T6.24.10.1.m1.3.3.3.1.2.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1.2"></times><ci id="S4.T6.24.10.1.m1.3.3.3.1.3.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1.3">Swish</ci><apply id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.1.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1">subscript</csymbol><ci id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.2.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.2">𝐱</ci><cn type="integer" id="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.3.cmml" xref="S4.T6.24.10.1.m1.3.3.3.1.1.1.1.3">1</cn></apply></apply><apply id="S4.T6.24.10.1.m1.3.3.3.3.cmml" xref="S4.T6.24.10.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.T6.24.10.1.m1.3.3.3.3.1.cmml" xref="S4.T6.24.10.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.T6.24.10.1.m1.3.3.3.3.2.cmml" xref="S4.T6.24.10.1.m1.3.3.3.3.2">𝐱</ci><cn type="integer" id="S4.T6.24.10.1.m1.3.3.3.3.3.cmml" xref="S4.T6.24.10.1.m1.3.3.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.24.10.1.m1.3c">\mathrm{SwiGLU}(\mathbf{x}_{1},\mathbf{x}_{2})=\mathrm{Swish}(\mathbf{x_{1}})\otimes\mathbf{x_{2}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.25.11" class="ltx_tr">
<td id="S4.T6.25.11.2" class="ltx_td ltx_align_center ltx_border_r">GeGLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib262" title="" class="ltx_ref">262</a>]</cite>
</td>
<td id="S4.T6.25.11.1" class="ltx_td ltx_align_left"><math id="S4.T6.25.11.1.m1.3" class="ltx_Math" alttext="\mathrm{GeGLU}(\mathbf{x}_{1},\mathbf{x}_{2})=\mathrm{GeLU}(\mathbf{x_{1}})\otimes\mathbf{x_{2}}" display="inline"><semantics id="S4.T6.25.11.1.m1.3a"><mrow id="S4.T6.25.11.1.m1.3.3" xref="S4.T6.25.11.1.m1.3.3.cmml"><mrow id="S4.T6.25.11.1.m1.2.2.2" xref="S4.T6.25.11.1.m1.2.2.2.cmml"><mi id="S4.T6.25.11.1.m1.2.2.2.4" xref="S4.T6.25.11.1.m1.2.2.2.4.cmml">GeGLU</mi><mo lspace="0em" rspace="0em" id="S4.T6.25.11.1.m1.2.2.2.3" xref="S4.T6.25.11.1.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T6.25.11.1.m1.2.2.2.2.2" xref="S4.T6.25.11.1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.T6.25.11.1.m1.2.2.2.2.2.3" xref="S4.T6.25.11.1.m1.2.2.2.2.3.cmml">(</mo><msub id="S4.T6.25.11.1.m1.1.1.1.1.1.1" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1.cmml"><mi id="S4.T6.25.11.1.m1.1.1.1.1.1.1.2" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1.2.cmml">𝐱</mi><mn id="S4.T6.25.11.1.m1.1.1.1.1.1.1.3" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.T6.25.11.1.m1.2.2.2.2.2.4" xref="S4.T6.25.11.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S4.T6.25.11.1.m1.2.2.2.2.2.2" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2.cmml"><mi id="S4.T6.25.11.1.m1.2.2.2.2.2.2.2" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2.2.cmml">𝐱</mi><mn id="S4.T6.25.11.1.m1.2.2.2.2.2.2.3" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S4.T6.25.11.1.m1.2.2.2.2.2.5" xref="S4.T6.25.11.1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.T6.25.11.1.m1.3.3.4" xref="S4.T6.25.11.1.m1.3.3.4.cmml">=</mo><mrow id="S4.T6.25.11.1.m1.3.3.3" xref="S4.T6.25.11.1.m1.3.3.3.cmml"><mrow id="S4.T6.25.11.1.m1.3.3.3.1" xref="S4.T6.25.11.1.m1.3.3.3.1.cmml"><mi id="S4.T6.25.11.1.m1.3.3.3.1.3" xref="S4.T6.25.11.1.m1.3.3.3.1.3.cmml">GeLU</mi><mo lspace="0em" rspace="0em" id="S4.T6.25.11.1.m1.3.3.3.1.2" xref="S4.T6.25.11.1.m1.3.3.3.1.2.cmml">​</mo><mrow id="S4.T6.25.11.1.m1.3.3.3.1.1.1" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.25.11.1.m1.3.3.3.1.1.1.2" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.cmml">(</mo><msub id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.cmml"><mi id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.2" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.2.cmml">𝐱</mi><mn id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.3" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.3.cmml">𝟏</mn></msub><mo rspace="0.055em" stretchy="false" id="S4.T6.25.11.1.m1.3.3.3.1.1.1.3" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S4.T6.25.11.1.m1.3.3.3.2" xref="S4.T6.25.11.1.m1.3.3.3.2.cmml">⊗</mo><msub id="S4.T6.25.11.1.m1.3.3.3.3" xref="S4.T6.25.11.1.m1.3.3.3.3.cmml"><mi id="S4.T6.25.11.1.m1.3.3.3.3.2" xref="S4.T6.25.11.1.m1.3.3.3.3.2.cmml">𝐱</mi><mn id="S4.T6.25.11.1.m1.3.3.3.3.3" xref="S4.T6.25.11.1.m1.3.3.3.3.3.cmml">𝟐</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.25.11.1.m1.3b"><apply id="S4.T6.25.11.1.m1.3.3.cmml" xref="S4.T6.25.11.1.m1.3.3"><eq id="S4.T6.25.11.1.m1.3.3.4.cmml" xref="S4.T6.25.11.1.m1.3.3.4"></eq><apply id="S4.T6.25.11.1.m1.2.2.2.cmml" xref="S4.T6.25.11.1.m1.2.2.2"><times id="S4.T6.25.11.1.m1.2.2.2.3.cmml" xref="S4.T6.25.11.1.m1.2.2.2.3"></times><ci id="S4.T6.25.11.1.m1.2.2.2.4.cmml" xref="S4.T6.25.11.1.m1.2.2.2.4">GeGLU</ci><interval closure="open" id="S4.T6.25.11.1.m1.2.2.2.2.3.cmml" xref="S4.T6.25.11.1.m1.2.2.2.2.2"><apply id="S4.T6.25.11.1.m1.1.1.1.1.1.1.cmml" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.T6.25.11.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.T6.25.11.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1.2">𝐱</ci><cn type="integer" id="S4.T6.25.11.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.T6.25.11.1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S4.T6.25.11.1.m1.2.2.2.2.2.2.cmml" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.T6.25.11.1.m1.2.2.2.2.2.2.1.cmml" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S4.T6.25.11.1.m1.2.2.2.2.2.2.2.cmml" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2.2">𝐱</ci><cn type="integer" id="S4.T6.25.11.1.m1.2.2.2.2.2.2.3.cmml" xref="S4.T6.25.11.1.m1.2.2.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S4.T6.25.11.1.m1.3.3.3.cmml" xref="S4.T6.25.11.1.m1.3.3.3"><csymbol cd="latexml" id="S4.T6.25.11.1.m1.3.3.3.2.cmml" xref="S4.T6.25.11.1.m1.3.3.3.2">tensor-product</csymbol><apply id="S4.T6.25.11.1.m1.3.3.3.1.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1"><times id="S4.T6.25.11.1.m1.3.3.3.1.2.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1.2"></times><ci id="S4.T6.25.11.1.m1.3.3.3.1.3.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1.3">GeLU</ci><apply id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.1.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1">subscript</csymbol><ci id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.2.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.2">𝐱</ci><cn type="integer" id="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.3.cmml" xref="S4.T6.25.11.1.m1.3.3.3.1.1.1.1.3">1</cn></apply></apply><apply id="S4.T6.25.11.1.m1.3.3.3.3.cmml" xref="S4.T6.25.11.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.T6.25.11.1.m1.3.3.3.3.1.cmml" xref="S4.T6.25.11.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.T6.25.11.1.m1.3.3.3.3.2.cmml" xref="S4.T6.25.11.1.m1.3.3.3.3.2">𝐱</ci><cn type="integer" id="S4.T6.25.11.1.m1.3.3.3.3.3.cmml" xref="S4.T6.25.11.1.m1.3.3.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.25.11.1.m1.3c">\mathrm{GeGLU}(\mathbf{x}_{1},\mathbf{x}_{2})=\mathrm{GeLU}(\mathbf{x_{1}})\otimes\mathbf{x_{2}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.26.12" class="ltx_tr">
<td id="S4.T6.26.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T6.26.12.2.1" class="ltx_text">Position embedding</span></td>
<td id="S4.T6.26.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Absolute&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T6.26.12.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T6.26.12.1.m1.1" class="ltx_Math" alttext="\mathbf{x}_{i}=\mathbf{x}_{i}+\mathbf{p}_{i}" display="inline"><semantics id="S4.T6.26.12.1.m1.1a"><mrow id="S4.T6.26.12.1.m1.1.1" xref="S4.T6.26.12.1.m1.1.1.cmml"><msub id="S4.T6.26.12.1.m1.1.1.2" xref="S4.T6.26.12.1.m1.1.1.2.cmml"><mi id="S4.T6.26.12.1.m1.1.1.2.2" xref="S4.T6.26.12.1.m1.1.1.2.2.cmml">𝐱</mi><mi id="S4.T6.26.12.1.m1.1.1.2.3" xref="S4.T6.26.12.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S4.T6.26.12.1.m1.1.1.1" xref="S4.T6.26.12.1.m1.1.1.1.cmml">=</mo><mrow id="S4.T6.26.12.1.m1.1.1.3" xref="S4.T6.26.12.1.m1.1.1.3.cmml"><msub id="S4.T6.26.12.1.m1.1.1.3.2" xref="S4.T6.26.12.1.m1.1.1.3.2.cmml"><mi id="S4.T6.26.12.1.m1.1.1.3.2.2" xref="S4.T6.26.12.1.m1.1.1.3.2.2.cmml">𝐱</mi><mi id="S4.T6.26.12.1.m1.1.1.3.2.3" xref="S4.T6.26.12.1.m1.1.1.3.2.3.cmml">i</mi></msub><mo id="S4.T6.26.12.1.m1.1.1.3.1" xref="S4.T6.26.12.1.m1.1.1.3.1.cmml">+</mo><msub id="S4.T6.26.12.1.m1.1.1.3.3" xref="S4.T6.26.12.1.m1.1.1.3.3.cmml"><mi id="S4.T6.26.12.1.m1.1.1.3.3.2" xref="S4.T6.26.12.1.m1.1.1.3.3.2.cmml">𝐩</mi><mi id="S4.T6.26.12.1.m1.1.1.3.3.3" xref="S4.T6.26.12.1.m1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.26.12.1.m1.1b"><apply id="S4.T6.26.12.1.m1.1.1.cmml" xref="S4.T6.26.12.1.m1.1.1"><eq id="S4.T6.26.12.1.m1.1.1.1.cmml" xref="S4.T6.26.12.1.m1.1.1.1"></eq><apply id="S4.T6.26.12.1.m1.1.1.2.cmml" xref="S4.T6.26.12.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.26.12.1.m1.1.1.2.1.cmml" xref="S4.T6.26.12.1.m1.1.1.2">subscript</csymbol><ci id="S4.T6.26.12.1.m1.1.1.2.2.cmml" xref="S4.T6.26.12.1.m1.1.1.2.2">𝐱</ci><ci id="S4.T6.26.12.1.m1.1.1.2.3.cmml" xref="S4.T6.26.12.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S4.T6.26.12.1.m1.1.1.3.cmml" xref="S4.T6.26.12.1.m1.1.1.3"><plus id="S4.T6.26.12.1.m1.1.1.3.1.cmml" xref="S4.T6.26.12.1.m1.1.1.3.1"></plus><apply id="S4.T6.26.12.1.m1.1.1.3.2.cmml" xref="S4.T6.26.12.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.T6.26.12.1.m1.1.1.3.2.1.cmml" xref="S4.T6.26.12.1.m1.1.1.3.2">subscript</csymbol><ci id="S4.T6.26.12.1.m1.1.1.3.2.2.cmml" xref="S4.T6.26.12.1.m1.1.1.3.2.2">𝐱</ci><ci id="S4.T6.26.12.1.m1.1.1.3.2.3.cmml" xref="S4.T6.26.12.1.m1.1.1.3.2.3">𝑖</ci></apply><apply id="S4.T6.26.12.1.m1.1.1.3.3.cmml" xref="S4.T6.26.12.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.T6.26.12.1.m1.1.1.3.3.1.cmml" xref="S4.T6.26.12.1.m1.1.1.3.3">subscript</csymbol><ci id="S4.T6.26.12.1.m1.1.1.3.3.2.cmml" xref="S4.T6.26.12.1.m1.1.1.3.3.2">𝐩</ci><ci id="S4.T6.26.12.1.m1.1.1.3.3.3.cmml" xref="S4.T6.26.12.1.m1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.26.12.1.m1.1c">\mathbf{x}_{i}=\mathbf{x}_{i}+\mathbf{p}_{i}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.27.13" class="ltx_tr">
<td id="S4.T6.27.13.2" class="ltx_td ltx_align_center ltx_border_r">Relative&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S4.T6.27.13.1" class="ltx_td ltx_align_left"><math id="S4.T6.27.13.1.m1.1" class="ltx_Math" alttext="A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}+r_{i-j}" display="inline"><semantics id="S4.T6.27.13.1.m1.1a"><mrow id="S4.T6.27.13.1.m1.1.1" xref="S4.T6.27.13.1.m1.1.1.cmml"><msub id="S4.T6.27.13.1.m1.1.1.2" xref="S4.T6.27.13.1.m1.1.1.2.cmml"><mi id="S4.T6.27.13.1.m1.1.1.2.2" xref="S4.T6.27.13.1.m1.1.1.2.2.cmml">A</mi><mrow id="S4.T6.27.13.1.m1.1.1.2.3" xref="S4.T6.27.13.1.m1.1.1.2.3.cmml"><mi id="S4.T6.27.13.1.m1.1.1.2.3.2" xref="S4.T6.27.13.1.m1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T6.27.13.1.m1.1.1.2.3.1" xref="S4.T6.27.13.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.T6.27.13.1.m1.1.1.2.3.3" xref="S4.T6.27.13.1.m1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.T6.27.13.1.m1.1.1.1" xref="S4.T6.27.13.1.m1.1.1.1.cmml">=</mo><mrow id="S4.T6.27.13.1.m1.1.1.3" xref="S4.T6.27.13.1.m1.1.1.3.cmml"><mrow id="S4.T6.27.13.1.m1.1.1.3.2" xref="S4.T6.27.13.1.m1.1.1.3.2.cmml"><msub id="S4.T6.27.13.1.m1.1.1.3.2.2" xref="S4.T6.27.13.1.m1.1.1.3.2.2.cmml"><mi id="S4.T6.27.13.1.m1.1.1.3.2.2.2" xref="S4.T6.27.13.1.m1.1.1.3.2.2.2.cmml">𝐖</mi><mi id="S4.T6.27.13.1.m1.1.1.3.2.2.3" xref="S4.T6.27.13.1.m1.1.1.3.2.2.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.27.13.1.m1.1.1.3.2.1" xref="S4.T6.27.13.1.m1.1.1.3.2.1.cmml">​</mo><msub id="S4.T6.27.13.1.m1.1.1.3.2.3" xref="S4.T6.27.13.1.m1.1.1.3.2.3.cmml"><mi id="S4.T6.27.13.1.m1.1.1.3.2.3.2" xref="S4.T6.27.13.1.m1.1.1.3.2.3.2.cmml">𝐱</mi><mi id="S4.T6.27.13.1.m1.1.1.3.2.3.3" xref="S4.T6.27.13.1.m1.1.1.3.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.27.13.1.m1.1.1.3.2.1a" xref="S4.T6.27.13.1.m1.1.1.3.2.1.cmml">​</mo><msubsup id="S4.T6.27.13.1.m1.1.1.3.2.4" xref="S4.T6.27.13.1.m1.1.1.3.2.4.cmml"><mi id="S4.T6.27.13.1.m1.1.1.3.2.4.2.2" xref="S4.T6.27.13.1.m1.1.1.3.2.4.2.2.cmml">𝐱</mi><mi id="S4.T6.27.13.1.m1.1.1.3.2.4.2.3" xref="S4.T6.27.13.1.m1.1.1.3.2.4.2.3.cmml">j</mi><mi id="S4.T6.27.13.1.m1.1.1.3.2.4.3" xref="S4.T6.27.13.1.m1.1.1.3.2.4.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.T6.27.13.1.m1.1.1.3.2.1b" xref="S4.T6.27.13.1.m1.1.1.3.2.1.cmml">​</mo><msubsup id="S4.T6.27.13.1.m1.1.1.3.2.5" xref="S4.T6.27.13.1.m1.1.1.3.2.5.cmml"><mi id="S4.T6.27.13.1.m1.1.1.3.2.5.2.2" xref="S4.T6.27.13.1.m1.1.1.3.2.5.2.2.cmml">𝐖</mi><mi id="S4.T6.27.13.1.m1.1.1.3.2.5.2.3" xref="S4.T6.27.13.1.m1.1.1.3.2.5.2.3.cmml">k</mi><mi id="S4.T6.27.13.1.m1.1.1.3.2.5.3" xref="S4.T6.27.13.1.m1.1.1.3.2.5.3.cmml">T</mi></msubsup></mrow><mo id="S4.T6.27.13.1.m1.1.1.3.1" xref="S4.T6.27.13.1.m1.1.1.3.1.cmml">+</mo><msub id="S4.T6.27.13.1.m1.1.1.3.3" xref="S4.T6.27.13.1.m1.1.1.3.3.cmml"><mi id="S4.T6.27.13.1.m1.1.1.3.3.2" xref="S4.T6.27.13.1.m1.1.1.3.3.2.cmml">r</mi><mrow id="S4.T6.27.13.1.m1.1.1.3.3.3" xref="S4.T6.27.13.1.m1.1.1.3.3.3.cmml"><mi id="S4.T6.27.13.1.m1.1.1.3.3.3.2" xref="S4.T6.27.13.1.m1.1.1.3.3.3.2.cmml">i</mi><mo id="S4.T6.27.13.1.m1.1.1.3.3.3.1" xref="S4.T6.27.13.1.m1.1.1.3.3.3.1.cmml">−</mo><mi id="S4.T6.27.13.1.m1.1.1.3.3.3.3" xref="S4.T6.27.13.1.m1.1.1.3.3.3.3.cmml">j</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.27.13.1.m1.1b"><apply id="S4.T6.27.13.1.m1.1.1.cmml" xref="S4.T6.27.13.1.m1.1.1"><eq id="S4.T6.27.13.1.m1.1.1.1.cmml" xref="S4.T6.27.13.1.m1.1.1.1"></eq><apply id="S4.T6.27.13.1.m1.1.1.2.cmml" xref="S4.T6.27.13.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.2.1.cmml" xref="S4.T6.27.13.1.m1.1.1.2">subscript</csymbol><ci id="S4.T6.27.13.1.m1.1.1.2.2.cmml" xref="S4.T6.27.13.1.m1.1.1.2.2">𝐴</ci><apply id="S4.T6.27.13.1.m1.1.1.2.3.cmml" xref="S4.T6.27.13.1.m1.1.1.2.3"><times id="S4.T6.27.13.1.m1.1.1.2.3.1.cmml" xref="S4.T6.27.13.1.m1.1.1.2.3.1"></times><ci id="S4.T6.27.13.1.m1.1.1.2.3.2.cmml" xref="S4.T6.27.13.1.m1.1.1.2.3.2">𝑖</ci><ci id="S4.T6.27.13.1.m1.1.1.2.3.3.cmml" xref="S4.T6.27.13.1.m1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S4.T6.27.13.1.m1.1.1.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3"><plus id="S4.T6.27.13.1.m1.1.1.3.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.1"></plus><apply id="S4.T6.27.13.1.m1.1.1.3.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2"><times id="S4.T6.27.13.1.m1.1.1.3.2.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.1"></times><apply id="S4.T6.27.13.1.m1.1.1.3.2.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.2.2.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S4.T6.27.13.1.m1.1.1.3.2.2.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.2.2">𝐖</ci><ci id="S4.T6.27.13.1.m1.1.1.3.2.2.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.2.3">𝑞</ci></apply><apply id="S4.T6.27.13.1.m1.1.1.3.2.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.2.3.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S4.T6.27.13.1.m1.1.1.3.2.3.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.3.2">𝐱</ci><ci id="S4.T6.27.13.1.m1.1.1.3.2.3.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.3.3">𝑖</ci></apply><apply id="S4.T6.27.13.1.m1.1.1.3.2.4.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.2.4.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4">superscript</csymbol><apply id="S4.T6.27.13.1.m1.1.1.3.2.4.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.2.4.2.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4">subscript</csymbol><ci id="S4.T6.27.13.1.m1.1.1.3.2.4.2.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4.2.2">𝐱</ci><ci id="S4.T6.27.13.1.m1.1.1.3.2.4.2.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4.2.3">𝑗</ci></apply><ci id="S4.T6.27.13.1.m1.1.1.3.2.4.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.4.3">𝑇</ci></apply><apply id="S4.T6.27.13.1.m1.1.1.3.2.5.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.2.5.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5">superscript</csymbol><apply id="S4.T6.27.13.1.m1.1.1.3.2.5.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.2.5.2.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5">subscript</csymbol><ci id="S4.T6.27.13.1.m1.1.1.3.2.5.2.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5.2.2">𝐖</ci><ci id="S4.T6.27.13.1.m1.1.1.3.2.5.2.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5.2.3">𝑘</ci></apply><ci id="S4.T6.27.13.1.m1.1.1.3.2.5.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.2.5.3">𝑇</ci></apply></apply><apply id="S4.T6.27.13.1.m1.1.1.3.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.T6.27.13.1.m1.1.1.3.3.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3">subscript</csymbol><ci id="S4.T6.27.13.1.m1.1.1.3.3.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3.2">𝑟</ci><apply id="S4.T6.27.13.1.m1.1.1.3.3.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3.3"><minus id="S4.T6.27.13.1.m1.1.1.3.3.3.1.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3.3.1"></minus><ci id="S4.T6.27.13.1.m1.1.1.3.3.3.2.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3.3.2">𝑖</ci><ci id="S4.T6.27.13.1.m1.1.1.3.3.3.3.cmml" xref="S4.T6.27.13.1.m1.1.1.3.3.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.27.13.1.m1.1c">A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}+r_{i-j}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.28.14" class="ltx_tr">
<td id="S4.T6.28.14.2" class="ltx_td ltx_align_center ltx_border_r">RoPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite>
</td>
<td id="S4.T6.28.14.1" class="ltx_td ltx_align_left"><math id="S4.T6.28.14.1.m1.8" class="ltx_Math" alttext="A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{R}_{\Theta,i-j}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}=(\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{R}_{\Theta,i})(\mathbf{W}_{k}\mathbf{x}_{j}R_{\Theta,j})^{T}" display="inline"><semantics id="S4.T6.28.14.1.m1.8a"><mrow id="S4.T6.28.14.1.m1.8.8" xref="S4.T6.28.14.1.m1.8.8.cmml"><msub id="S4.T6.28.14.1.m1.8.8.4" xref="S4.T6.28.14.1.m1.8.8.4.cmml"><mi id="S4.T6.28.14.1.m1.8.8.4.2" xref="S4.T6.28.14.1.m1.8.8.4.2.cmml">A</mi><mrow id="S4.T6.28.14.1.m1.8.8.4.3" xref="S4.T6.28.14.1.m1.8.8.4.3.cmml"><mi id="S4.T6.28.14.1.m1.8.8.4.3.2" xref="S4.T6.28.14.1.m1.8.8.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.4.3.1" xref="S4.T6.28.14.1.m1.8.8.4.3.1.cmml">​</mo><mi id="S4.T6.28.14.1.m1.8.8.4.3.3" xref="S4.T6.28.14.1.m1.8.8.4.3.3.cmml">j</mi></mrow></msub><mo id="S4.T6.28.14.1.m1.8.8.5" xref="S4.T6.28.14.1.m1.8.8.5.cmml">=</mo><mrow id="S4.T6.28.14.1.m1.8.8.6" xref="S4.T6.28.14.1.m1.8.8.6.cmml"><msub id="S4.T6.28.14.1.m1.8.8.6.2" xref="S4.T6.28.14.1.m1.8.8.6.2.cmml"><mi id="S4.T6.28.14.1.m1.8.8.6.2.2" xref="S4.T6.28.14.1.m1.8.8.6.2.2.cmml">𝐖</mi><mi id="S4.T6.28.14.1.m1.8.8.6.2.3" xref="S4.T6.28.14.1.m1.8.8.6.2.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.6.1" xref="S4.T6.28.14.1.m1.8.8.6.1.cmml">​</mo><msub id="S4.T6.28.14.1.m1.8.8.6.3" xref="S4.T6.28.14.1.m1.8.8.6.3.cmml"><mi id="S4.T6.28.14.1.m1.8.8.6.3.2" xref="S4.T6.28.14.1.m1.8.8.6.3.2.cmml">𝐱</mi><mi id="S4.T6.28.14.1.m1.8.8.6.3.3" xref="S4.T6.28.14.1.m1.8.8.6.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.6.1a" xref="S4.T6.28.14.1.m1.8.8.6.1.cmml">​</mo><msub id="S4.T6.28.14.1.m1.8.8.6.4" xref="S4.T6.28.14.1.m1.8.8.6.4.cmml"><mi id="S4.T6.28.14.1.m1.8.8.6.4.2" xref="S4.T6.28.14.1.m1.8.8.6.4.2.cmml">𝐑</mi><mrow id="S4.T6.28.14.1.m1.2.2.2.2" xref="S4.T6.28.14.1.m1.2.2.2.3.cmml"><mi mathvariant="normal" id="S4.T6.28.14.1.m1.1.1.1.1" xref="S4.T6.28.14.1.m1.1.1.1.1.cmml">Θ</mi><mo id="S4.T6.28.14.1.m1.2.2.2.2.2" xref="S4.T6.28.14.1.m1.2.2.2.3.cmml">,</mo><mrow id="S4.T6.28.14.1.m1.2.2.2.2.1" xref="S4.T6.28.14.1.m1.2.2.2.2.1.cmml"><mi id="S4.T6.28.14.1.m1.2.2.2.2.1.2" xref="S4.T6.28.14.1.m1.2.2.2.2.1.2.cmml">i</mi><mo id="S4.T6.28.14.1.m1.2.2.2.2.1.1" xref="S4.T6.28.14.1.m1.2.2.2.2.1.1.cmml">−</mo><mi id="S4.T6.28.14.1.m1.2.2.2.2.1.3" xref="S4.T6.28.14.1.m1.2.2.2.2.1.3.cmml">j</mi></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.6.1b" xref="S4.T6.28.14.1.m1.8.8.6.1.cmml">​</mo><msubsup id="S4.T6.28.14.1.m1.8.8.6.5" xref="S4.T6.28.14.1.m1.8.8.6.5.cmml"><mi id="S4.T6.28.14.1.m1.8.8.6.5.2.2" xref="S4.T6.28.14.1.m1.8.8.6.5.2.2.cmml">𝐱</mi><mi id="S4.T6.28.14.1.m1.8.8.6.5.2.3" xref="S4.T6.28.14.1.m1.8.8.6.5.2.3.cmml">j</mi><mi id="S4.T6.28.14.1.m1.8.8.6.5.3" xref="S4.T6.28.14.1.m1.8.8.6.5.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.6.1c" xref="S4.T6.28.14.1.m1.8.8.6.1.cmml">​</mo><msubsup id="S4.T6.28.14.1.m1.8.8.6.6" xref="S4.T6.28.14.1.m1.8.8.6.6.cmml"><mi id="S4.T6.28.14.1.m1.8.8.6.6.2.2" xref="S4.T6.28.14.1.m1.8.8.6.6.2.2.cmml">𝐖</mi><mi id="S4.T6.28.14.1.m1.8.8.6.6.2.3" xref="S4.T6.28.14.1.m1.8.8.6.6.2.3.cmml">k</mi><mi id="S4.T6.28.14.1.m1.8.8.6.6.3" xref="S4.T6.28.14.1.m1.8.8.6.6.3.cmml">T</mi></msubsup></mrow><mo id="S4.T6.28.14.1.m1.8.8.7" xref="S4.T6.28.14.1.m1.8.8.7.cmml">=</mo><mrow id="S4.T6.28.14.1.m1.8.8.2" xref="S4.T6.28.14.1.m1.8.8.2.cmml"><mrow id="S4.T6.28.14.1.m1.7.7.1.1.1" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.28.14.1.m1.7.7.1.1.1.2" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.cmml">(</mo><mrow id="S4.T6.28.14.1.m1.7.7.1.1.1.1" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.cmml"><msub id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.cmml"><mi id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.2" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.2.cmml">𝐖</mi><mi id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.3" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.7.7.1.1.1.1.1" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.1.cmml">​</mo><msub id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.cmml"><mi id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.2" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.2.cmml">𝐱</mi><mi id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.3" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.7.7.1.1.1.1.1a" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.1.cmml">​</mo><msub id="S4.T6.28.14.1.m1.7.7.1.1.1.1.4" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.cmml"><mi id="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.2" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.2.cmml">𝐑</mi><mrow id="S4.T6.28.14.1.m1.4.4.2.4" xref="S4.T6.28.14.1.m1.4.4.2.3.cmml"><mi mathvariant="normal" id="S4.T6.28.14.1.m1.3.3.1.1" xref="S4.T6.28.14.1.m1.3.3.1.1.cmml">Θ</mi><mo id="S4.T6.28.14.1.m1.4.4.2.4.1" xref="S4.T6.28.14.1.m1.4.4.2.3.cmml">,</mo><mi id="S4.T6.28.14.1.m1.4.4.2.2" xref="S4.T6.28.14.1.m1.4.4.2.2.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.T6.28.14.1.m1.7.7.1.1.1.3" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.2.3" xref="S4.T6.28.14.1.m1.8.8.2.3.cmml">​</mo><msup id="S4.T6.28.14.1.m1.8.8.2.2" xref="S4.T6.28.14.1.m1.8.8.2.2.cmml"><mrow id="S4.T6.28.14.1.m1.8.8.2.2.1.1" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.2" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.cmml">(</mo><mrow id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.cmml"><msub id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.cmml"><mi id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.2" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.2.cmml">𝐖</mi><mi id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.3" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.1" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.1.cmml">​</mo><msub id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.cmml"><mi id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.2" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.2.cmml">𝐱</mi><mi id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.3" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.1a" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.1.cmml">​</mo><msub id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.cmml"><mi id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.2" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.2.cmml">R</mi><mrow id="S4.T6.28.14.1.m1.6.6.2.4" xref="S4.T6.28.14.1.m1.6.6.2.3.cmml"><mi mathvariant="normal" id="S4.T6.28.14.1.m1.5.5.1.1" xref="S4.T6.28.14.1.m1.5.5.1.1.cmml">Θ</mi><mo id="S4.T6.28.14.1.m1.6.6.2.4.1" xref="S4.T6.28.14.1.m1.6.6.2.3.cmml">,</mo><mi id="S4.T6.28.14.1.m1.6.6.2.2" xref="S4.T6.28.14.1.m1.6.6.2.2.cmml">j</mi></mrow></msub></mrow><mo stretchy="false" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.3" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.cmml">)</mo></mrow><mi id="S4.T6.28.14.1.m1.8.8.2.2.3" xref="S4.T6.28.14.1.m1.8.8.2.2.3.cmml">T</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.28.14.1.m1.8b"><apply id="S4.T6.28.14.1.m1.8.8.cmml" xref="S4.T6.28.14.1.m1.8.8"><and id="S4.T6.28.14.1.m1.8.8a.cmml" xref="S4.T6.28.14.1.m1.8.8"></and><apply id="S4.T6.28.14.1.m1.8.8b.cmml" xref="S4.T6.28.14.1.m1.8.8"><eq id="S4.T6.28.14.1.m1.8.8.5.cmml" xref="S4.T6.28.14.1.m1.8.8.5"></eq><apply id="S4.T6.28.14.1.m1.8.8.4.cmml" xref="S4.T6.28.14.1.m1.8.8.4"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.4.1.cmml" xref="S4.T6.28.14.1.m1.8.8.4">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.4.2.cmml" xref="S4.T6.28.14.1.m1.8.8.4.2">𝐴</ci><apply id="S4.T6.28.14.1.m1.8.8.4.3.cmml" xref="S4.T6.28.14.1.m1.8.8.4.3"><times id="S4.T6.28.14.1.m1.8.8.4.3.1.cmml" xref="S4.T6.28.14.1.m1.8.8.4.3.1"></times><ci id="S4.T6.28.14.1.m1.8.8.4.3.2.cmml" xref="S4.T6.28.14.1.m1.8.8.4.3.2">𝑖</ci><ci id="S4.T6.28.14.1.m1.8.8.4.3.3.cmml" xref="S4.T6.28.14.1.m1.8.8.4.3.3">𝑗</ci></apply></apply><apply id="S4.T6.28.14.1.m1.8.8.6.cmml" xref="S4.T6.28.14.1.m1.8.8.6"><times id="S4.T6.28.14.1.m1.8.8.6.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.1"></times><apply id="S4.T6.28.14.1.m1.8.8.6.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.2"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.2.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.2">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.6.2.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.2.2">𝐖</ci><ci id="S4.T6.28.14.1.m1.8.8.6.2.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.2.3">𝑞</ci></apply><apply id="S4.T6.28.14.1.m1.8.8.6.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.3"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.3.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.3">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.6.3.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.3.2">𝐱</ci><ci id="S4.T6.28.14.1.m1.8.8.6.3.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.3.3">𝑖</ci></apply><apply id="S4.T6.28.14.1.m1.8.8.6.4.cmml" xref="S4.T6.28.14.1.m1.8.8.6.4"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.4.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.4">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.6.4.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.4.2">𝐑</ci><list id="S4.T6.28.14.1.m1.2.2.2.3.cmml" xref="S4.T6.28.14.1.m1.2.2.2.2"><ci id="S4.T6.28.14.1.m1.1.1.1.1.cmml" xref="S4.T6.28.14.1.m1.1.1.1.1">Θ</ci><apply id="S4.T6.28.14.1.m1.2.2.2.2.1.cmml" xref="S4.T6.28.14.1.m1.2.2.2.2.1"><minus id="S4.T6.28.14.1.m1.2.2.2.2.1.1.cmml" xref="S4.T6.28.14.1.m1.2.2.2.2.1.1"></minus><ci id="S4.T6.28.14.1.m1.2.2.2.2.1.2.cmml" xref="S4.T6.28.14.1.m1.2.2.2.2.1.2">𝑖</ci><ci id="S4.T6.28.14.1.m1.2.2.2.2.1.3.cmml" xref="S4.T6.28.14.1.m1.2.2.2.2.1.3">𝑗</ci></apply></list></apply><apply id="S4.T6.28.14.1.m1.8.8.6.5.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.5.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5">superscript</csymbol><apply id="S4.T6.28.14.1.m1.8.8.6.5.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.5.2.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.6.5.2.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5.2.2">𝐱</ci><ci id="S4.T6.28.14.1.m1.8.8.6.5.2.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5.2.3">𝑗</ci></apply><ci id="S4.T6.28.14.1.m1.8.8.6.5.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.5.3">𝑇</ci></apply><apply id="S4.T6.28.14.1.m1.8.8.6.6.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.6.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6">superscript</csymbol><apply id="S4.T6.28.14.1.m1.8.8.6.6.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.6.6.2.1.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.6.6.2.2.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6.2.2">𝐖</ci><ci id="S4.T6.28.14.1.m1.8.8.6.6.2.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6.2.3">𝑘</ci></apply><ci id="S4.T6.28.14.1.m1.8.8.6.6.3.cmml" xref="S4.T6.28.14.1.m1.8.8.6.6.3">𝑇</ci></apply></apply></apply><apply id="S4.T6.28.14.1.m1.8.8c.cmml" xref="S4.T6.28.14.1.m1.8.8"><eq id="S4.T6.28.14.1.m1.8.8.7.cmml" xref="S4.T6.28.14.1.m1.8.8.7"></eq><share href="#S4.T6.28.14.1.m1.8.8.6.cmml" id="S4.T6.28.14.1.m1.8.8d.cmml" xref="S4.T6.28.14.1.m1.8.8"></share><apply id="S4.T6.28.14.1.m1.8.8.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2"><times id="S4.T6.28.14.1.m1.8.8.2.3.cmml" xref="S4.T6.28.14.1.m1.8.8.2.3"></times><apply id="S4.T6.28.14.1.m1.7.7.1.1.1.1.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1"><times id="S4.T6.28.14.1.m1.7.7.1.1.1.1.1.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.1"></times><apply id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.1.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2">subscript</csymbol><ci id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.2.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.2">𝐖</ci><ci id="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.3.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.2.3">𝑞</ci></apply><apply id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.1.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3">subscript</csymbol><ci id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.2.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.2">𝐱</ci><ci id="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.3.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.3.3">𝑖</ci></apply><apply id="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.1.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.4">subscript</csymbol><ci id="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.2.cmml" xref="S4.T6.28.14.1.m1.7.7.1.1.1.1.4.2">𝐑</ci><list id="S4.T6.28.14.1.m1.4.4.2.3.cmml" xref="S4.T6.28.14.1.m1.4.4.2.4"><ci id="S4.T6.28.14.1.m1.3.3.1.1.cmml" xref="S4.T6.28.14.1.m1.3.3.1.1">Θ</ci><ci id="S4.T6.28.14.1.m1.4.4.2.2.cmml" xref="S4.T6.28.14.1.m1.4.4.2.2">𝑖</ci></list></apply></apply><apply id="S4.T6.28.14.1.m1.8.8.2.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.2.2.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2">superscript</csymbol><apply id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1"><times id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.1.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.1"></times><apply id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.1.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.2">𝐖</ci><ci id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.3.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.2.3">𝑘</ci></apply><apply id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.1.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.2">𝐱</ci><ci id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.3.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.3.3">𝑗</ci></apply><apply id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.1.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4">subscript</csymbol><ci id="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.2.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.1.1.1.4.2">𝑅</ci><list id="S4.T6.28.14.1.m1.6.6.2.3.cmml" xref="S4.T6.28.14.1.m1.6.6.2.4"><ci id="S4.T6.28.14.1.m1.5.5.1.1.cmml" xref="S4.T6.28.14.1.m1.5.5.1.1">Θ</ci><ci id="S4.T6.28.14.1.m1.6.6.2.2.cmml" xref="S4.T6.28.14.1.m1.6.6.2.2">𝑗</ci></list></apply></apply><ci id="S4.T6.28.14.1.m1.8.8.2.2.3.cmml" xref="S4.T6.28.14.1.m1.8.8.2.2.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.28.14.1.m1.8c">A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{R}_{\Theta,i-j}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}=(\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{R}_{\Theta,i})(\mathbf{W}_{k}\mathbf{x}_{j}R_{\Theta,j})^{T}</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.29.15" class="ltx_tr">
<td id="S4.T6.29.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">ALiBi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite>
</td>
<td id="S4.T6.29.15.1" class="ltx_td ltx_align_left ltx_border_bb"><math id="S4.T6.29.15.1.m1.1" class="ltx_Math" alttext="A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}-m(i-j)" display="inline"><semantics id="S4.T6.29.15.1.m1.1a"><mrow id="S4.T6.29.15.1.m1.1.1" xref="S4.T6.29.15.1.m1.1.1.cmml"><msub id="S4.T6.29.15.1.m1.1.1.3" xref="S4.T6.29.15.1.m1.1.1.3.cmml"><mi id="S4.T6.29.15.1.m1.1.1.3.2" xref="S4.T6.29.15.1.m1.1.1.3.2.cmml">A</mi><mrow id="S4.T6.29.15.1.m1.1.1.3.3" xref="S4.T6.29.15.1.m1.1.1.3.3.cmml"><mi id="S4.T6.29.15.1.m1.1.1.3.3.2" xref="S4.T6.29.15.1.m1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T6.29.15.1.m1.1.1.3.3.1" xref="S4.T6.29.15.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.T6.29.15.1.m1.1.1.3.3.3" xref="S4.T6.29.15.1.m1.1.1.3.3.3.cmml">j</mi></mrow></msub><mo id="S4.T6.29.15.1.m1.1.1.2" xref="S4.T6.29.15.1.m1.1.1.2.cmml">=</mo><mrow id="S4.T6.29.15.1.m1.1.1.1" xref="S4.T6.29.15.1.m1.1.1.1.cmml"><mrow id="S4.T6.29.15.1.m1.1.1.1.3" xref="S4.T6.29.15.1.m1.1.1.1.3.cmml"><msub id="S4.T6.29.15.1.m1.1.1.1.3.2" xref="S4.T6.29.15.1.m1.1.1.1.3.2.cmml"><mi id="S4.T6.29.15.1.m1.1.1.1.3.2.2" xref="S4.T6.29.15.1.m1.1.1.1.3.2.2.cmml">𝐖</mi><mi id="S4.T6.29.15.1.m1.1.1.1.3.2.3" xref="S4.T6.29.15.1.m1.1.1.1.3.2.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.29.15.1.m1.1.1.1.3.1" xref="S4.T6.29.15.1.m1.1.1.1.3.1.cmml">​</mo><msub id="S4.T6.29.15.1.m1.1.1.1.3.3" xref="S4.T6.29.15.1.m1.1.1.1.3.3.cmml"><mi id="S4.T6.29.15.1.m1.1.1.1.3.3.2" xref="S4.T6.29.15.1.m1.1.1.1.3.3.2.cmml">𝐱</mi><mi id="S4.T6.29.15.1.m1.1.1.1.3.3.3" xref="S4.T6.29.15.1.m1.1.1.1.3.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.T6.29.15.1.m1.1.1.1.3.1a" xref="S4.T6.29.15.1.m1.1.1.1.3.1.cmml">​</mo><msubsup id="S4.T6.29.15.1.m1.1.1.1.3.4" xref="S4.T6.29.15.1.m1.1.1.1.3.4.cmml"><mi id="S4.T6.29.15.1.m1.1.1.1.3.4.2.2" xref="S4.T6.29.15.1.m1.1.1.1.3.4.2.2.cmml">𝐱</mi><mi id="S4.T6.29.15.1.m1.1.1.1.3.4.2.3" xref="S4.T6.29.15.1.m1.1.1.1.3.4.2.3.cmml">j</mi><mi id="S4.T6.29.15.1.m1.1.1.1.3.4.3" xref="S4.T6.29.15.1.m1.1.1.1.3.4.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.T6.29.15.1.m1.1.1.1.3.1b" xref="S4.T6.29.15.1.m1.1.1.1.3.1.cmml">​</mo><msubsup id="S4.T6.29.15.1.m1.1.1.1.3.5" xref="S4.T6.29.15.1.m1.1.1.1.3.5.cmml"><mi id="S4.T6.29.15.1.m1.1.1.1.3.5.2.2" xref="S4.T6.29.15.1.m1.1.1.1.3.5.2.2.cmml">𝐖</mi><mi id="S4.T6.29.15.1.m1.1.1.1.3.5.2.3" xref="S4.T6.29.15.1.m1.1.1.1.3.5.2.3.cmml">k</mi><mi id="S4.T6.29.15.1.m1.1.1.1.3.5.3" xref="S4.T6.29.15.1.m1.1.1.1.3.5.3.cmml">T</mi></msubsup></mrow><mo id="S4.T6.29.15.1.m1.1.1.1.2" xref="S4.T6.29.15.1.m1.1.1.1.2.cmml">−</mo><mrow id="S4.T6.29.15.1.m1.1.1.1.1" xref="S4.T6.29.15.1.m1.1.1.1.1.cmml"><mi id="S4.T6.29.15.1.m1.1.1.1.1.3" xref="S4.T6.29.15.1.m1.1.1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T6.29.15.1.m1.1.1.1.1.2" xref="S4.T6.29.15.1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S4.T6.29.15.1.m1.1.1.1.1.1.1" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T6.29.15.1.m1.1.1.1.1.1.1.2" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.2" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.1" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.3" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.3.cmml">j</mi></mrow><mo stretchy="false" id="S4.T6.29.15.1.m1.1.1.1.1.1.1.3" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.29.15.1.m1.1b"><apply id="S4.T6.29.15.1.m1.1.1.cmml" xref="S4.T6.29.15.1.m1.1.1"><eq id="S4.T6.29.15.1.m1.1.1.2.cmml" xref="S4.T6.29.15.1.m1.1.1.2"></eq><apply id="S4.T6.29.15.1.m1.1.1.3.cmml" xref="S4.T6.29.15.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.3.1.cmml" xref="S4.T6.29.15.1.m1.1.1.3">subscript</csymbol><ci id="S4.T6.29.15.1.m1.1.1.3.2.cmml" xref="S4.T6.29.15.1.m1.1.1.3.2">𝐴</ci><apply id="S4.T6.29.15.1.m1.1.1.3.3.cmml" xref="S4.T6.29.15.1.m1.1.1.3.3"><times id="S4.T6.29.15.1.m1.1.1.3.3.1.cmml" xref="S4.T6.29.15.1.m1.1.1.3.3.1"></times><ci id="S4.T6.29.15.1.m1.1.1.3.3.2.cmml" xref="S4.T6.29.15.1.m1.1.1.3.3.2">𝑖</ci><ci id="S4.T6.29.15.1.m1.1.1.3.3.3.cmml" xref="S4.T6.29.15.1.m1.1.1.3.3.3">𝑗</ci></apply></apply><apply id="S4.T6.29.15.1.m1.1.1.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1"><minus id="S4.T6.29.15.1.m1.1.1.1.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.2"></minus><apply id="S4.T6.29.15.1.m1.1.1.1.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3"><times id="S4.T6.29.15.1.m1.1.1.1.3.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.1"></times><apply id="S4.T6.29.15.1.m1.1.1.1.3.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.1.3.2.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.2">subscript</csymbol><ci id="S4.T6.29.15.1.m1.1.1.1.3.2.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.2.2">𝐖</ci><ci id="S4.T6.29.15.1.m1.1.1.1.3.2.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.2.3">𝑞</ci></apply><apply id="S4.T6.29.15.1.m1.1.1.1.3.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.1.3.3.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.3">subscript</csymbol><ci id="S4.T6.29.15.1.m1.1.1.1.3.3.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.3.2">𝐱</ci><ci id="S4.T6.29.15.1.m1.1.1.1.3.3.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.3.3">𝑖</ci></apply><apply id="S4.T6.29.15.1.m1.1.1.1.3.4.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.1.3.4.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4">superscript</csymbol><apply id="S4.T6.29.15.1.m1.1.1.1.3.4.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.1.3.4.2.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4">subscript</csymbol><ci id="S4.T6.29.15.1.m1.1.1.1.3.4.2.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4.2.2">𝐱</ci><ci id="S4.T6.29.15.1.m1.1.1.1.3.4.2.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4.2.3">𝑗</ci></apply><ci id="S4.T6.29.15.1.m1.1.1.1.3.4.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.4.3">𝑇</ci></apply><apply id="S4.T6.29.15.1.m1.1.1.1.3.5.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.1.3.5.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5">superscript</csymbol><apply id="S4.T6.29.15.1.m1.1.1.1.3.5.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5"><csymbol cd="ambiguous" id="S4.T6.29.15.1.m1.1.1.1.3.5.2.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5">subscript</csymbol><ci id="S4.T6.29.15.1.m1.1.1.1.3.5.2.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5.2.2">𝐖</ci><ci id="S4.T6.29.15.1.m1.1.1.1.3.5.2.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5.2.3">𝑘</ci></apply><ci id="S4.T6.29.15.1.m1.1.1.1.3.5.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.3.5.3">𝑇</ci></apply></apply><apply id="S4.T6.29.15.1.m1.1.1.1.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1"><times id="S4.T6.29.15.1.m1.1.1.1.1.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1.2"></times><ci id="S4.T6.29.15.1.m1.1.1.1.1.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1.3">𝑚</ci><apply id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1"><minus id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.1"></minus><ci id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.2">𝑖</ci><ci id="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.T6.29.15.1.m1.1.1.1.1.1.1.1.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.29.15.1.m1.1c">A_{ij}=\mathbf{W}_{q}\mathbf{x}_{i}\mathbf{x}_{j}^{T}\mathbf{W}_{k}^{T}-m(i-j)</annotation></semantics></math></td>
</tr>
</tbody></table>
</figure>
<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Since the launch of Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, various improvements have been proposed to enhance its training stability, performance, and computational efficiency. In this part, we will discuss the corresponding
configurations for
four major parts of the Transformer, including normalization, position embeddings, activation functions, and attention and bias.
To make this survey more self-contained, we present the detailed formulations for these configurations in Table&nbsp;<a href="#S4.T6" title="TABLE VI ‣ 4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Normalization Methods.</span> Training instability is a challenging issue for pre-training LLMs. To alleviate this issue, normalization is a widely adopted strategy to stabilize the training of neural networks. In the vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, LayerNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib256" title="" class="ltx_ref">256</a>]</cite> is employed. Recently, several advanced normalization techniques have been proposed as alternatives to LayerNorm, <em id="S4.SS2.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> RMSNorm, and DeepNorm.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p"><math id="S4.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p3.1.m1.1a"><mo id="S4.SS2.SSS2.p3.1.m1.1.1" xref="S4.SS2.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.1.m1.1b"><ci id="S4.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">LayerNorm.</em> In the early research, BatchNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref">265</a>]</cite> is a commonly used normalization method. However, it is difficult to deal with sequence data of variable lengths and small-batch data. Thus, LayerNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib256" title="" class="ltx_ref">256</a>]</cite> is introduced to conduct layerwise normalization. Specifically, the mean and variance over all activations per layer are calculated to re-center and re-scale the activations.</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p"><math id="S4.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p4.1.m1.1a"><mo id="S4.SS2.SSS2.p4.1.m1.1.1" xref="S4.SS2.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p4.1.m1.1b"><ci id="S4.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">RMSNorm.</em> To improve the training speed of LayerNorm (LN), RMSNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib257" title="" class="ltx_ref">257</a>]</cite> is proposed by re-scaling the activations with only the root mean square (RMS) of the summed activations, instead of the mean and variance. Related research has demonstrated its superiority in training speed and performance on Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref">266</a>]</cite>. Representative models that adopt RMSNorm include Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p5" class="ltx_para">
<p id="S4.SS2.SSS2.p5.1" class="ltx_p"><math id="S4.SS2.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p5.1.m1.1a"><mo id="S4.SS2.SSS2.p5.1.m1.1.1" xref="S4.SS2.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p5.1.m1.1b"><ci id="S4.SS2.SSS2.p5.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p5.1.1" class="ltx_emph ltx_font_italic">DeepNorm.</em> DeepNorm is proposed by Microsoft&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite> to stabilize the training of deep Transformers.
With DeepNorm as residual connections, Transformers can be scaled up to 1,000 layers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite>, which has shown the advantages of stability and good performance. It has been adopted by GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p6.1" class="ltx_p"><span id="S4.SS2.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Normalization Position.</span> In addition to the normalization method, normalization position also plays a crucial role in the LLMs. There are generally three choices for the normalization position, <em id="S4.SS2.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> post-LN, pre-LN, and sandwich-LN.</p>
</div>
<div id="S4.SS2.SSS2.p7" class="ltx_para">
<p id="S4.SS2.SSS2.p7.1" class="ltx_p"><math id="S4.SS2.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p7.1.m1.1a"><mo id="S4.SS2.SSS2.p7.1.m1.1.1" xref="S4.SS2.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p7.1.m1.1b"><ci id="S4.SS2.SSS2.p7.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Post-LN.</em> Post-LN is used in the vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which is placed between residual blocks. However, existing work has found that the training of Transformers with post-LN tends to be instable due to the large gradients near the output layer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite>. Thus, post-LN is rarely employed in existing LLMs except combined with other strategies (<em id="S4.SS2.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> combining post-LN with pre-LN in GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>).</p>
</div>
<div id="S4.SS2.SSS2.p8" class="ltx_para">
<p id="S4.SS2.SSS2.p8.1" class="ltx_p"><math id="S4.SS2.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p8.1.m1.1a"><mo id="S4.SS2.SSS2.p8.1.m1.1.1" xref="S4.SS2.SSS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p8.1.m1.1b"><ci id="S4.SS2.SSS2.p8.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p8.1.1" class="ltx_emph ltx_font_italic">Pre-LN.</em> Different from post-LN, pre-LN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite> is applied before each sub-layer, and an additional LN is placed before the final prediction. Compared with post-LN, the Transformers with pre-LN are more stable in training. However, it performs worse than the variants with post-LN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite>. Despite the decreasing performance, most LLMs still adopt pre-LN due to the training stability. 

However, one exception is that pre-LN has been found unstable in GLM when training models more than 100B parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p9" class="ltx_para">
<p id="S4.SS2.SSS2.p9.1" class="ltx_p"><math id="S4.SS2.SSS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p9.1.m1.1a"><mo id="S4.SS2.SSS2.p9.1.m1.1.1" xref="S4.SS2.SSS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p9.1.m1.1b"><ci id="S4.SS2.SSS2.p9.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p9.1.1" class="ltx_emph ltx_font_italic">Sandwich-LN.</em> Based on pre-LN, Sandwich-LN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite> adds extra LN before the residual connections to avoid the value explosion issues in Transformer layer outputs. However, it has been found that Sandwich-LN sometimes fails to stabilize the training of LLMs and may lead to the collapse of training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p10" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p10.1" class="ltx_p"><span id="S4.SS2.SSS2.p10.1.1" class="ltx_text ltx_font_bold">Activation Functions.</span>
To obtain good performance, activation functions also need to be properly set in feed-forward networks.
In existing LLMs, GeLU activations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite> are widely used. Specially, in the latest LLMs (<em id="S4.SS2.SSS2.p10.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> PaLM and LaMDA), variants of GLU activation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib271" title="" class="ltx_ref">271</a>, <a href="#bib.bib262" title="" class="ltx_ref">262</a>]</cite> have also been utilized, especially the SwiGLU and GeGLU variants, which
often achieve better performance in practice&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref">266</a>]</cite>. However, compared with GeLU, they require extra parameters (about 50%) in the feed-forward networks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib272" title="" class="ltx_ref">272</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p11" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p11.1" class="ltx_p"><span id="S4.SS2.SSS2.p11.1.1" class="ltx_text ltx_font_bold">Position Embeddings.</span>
Since the self-attention modules
in Transformer are permutation equivariant, position embeddings&nbsp;(PE) are employed to inject absolute or relative position information for modeling sequences.</p>
</div>
<div id="S4.SS2.SSS2.p12" class="ltx_para">
<p id="S4.SS2.SSS2.p12.1" class="ltx_p"><math id="S4.SS2.SSS2.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p12.1.m1.1a"><mo id="S4.SS2.SSS2.p12.1.m1.1.1" xref="S4.SS2.SSS2.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p12.1.m1.1b"><ci id="S4.SS2.SSS2.p12.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p12.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p12.1.1" class="ltx_emph ltx_font_italic">Absolute position embedding.</em> In the vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, absolute position embeddings are employed. At the bottoms of the encoder and the decoder, the absolute positional embeddings are added to the input embeddings.
There are two variants of absolute position embeddings proposed in the vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, <em id="S4.SS2.SSS2.p12.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> sinusoidal and learned position embeddings, where the latter is commonly used in existing pre-trained language models.</p>
</div>
<div id="S4.SS2.SSS2.p13" class="ltx_para">
<p id="S4.SS2.SSS2.p13.1" class="ltx_p"><math id="S4.SS2.SSS2.p13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p13.1.m1.1a"><mo id="S4.SS2.SSS2.p13.1.m1.1.1" xref="S4.SS2.SSS2.p13.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p13.1.m1.1b"><ci id="S4.SS2.SSS2.p13.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p13.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p13.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p13.1.1" class="ltx_emph ltx_font_italic">Relative position embedding.</em> Unlike absolute position embeddings, relative positional embeddings are generated according to the offsets between keys and queries&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib273" title="" class="ltx_ref">273</a>]</cite>. A popular variant of relative PE was introduced in Transformer-XL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib274" title="" class="ltx_ref">274</a>, <a href="#bib.bib275" title="" class="ltx_ref">275</a>]</cite>. The calculation of attention scores between keys and queries has been modified to introduce learnable embeddings corresponding to relative positions.
T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> further simplified relative positional embeddings, which was subsequently adopted by Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>.
Specifically, it adds learnable scalars to the attention scores, where the scalars are calculated based on the distances between the positions of the query and the key. Compared with the absolute PE, Transformers with relative position embedding can generalize to sequences longer than those sequences for training, <em id="S4.SS2.SSS2.p13.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> extrapolation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p14" class="ltx_para">
<p id="S4.SS2.SSS2.p14.10" class="ltx_p"><math id="S4.SS2.SSS2.p14.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p14.1.m1.1a"><mo id="S4.SS2.SSS2.p14.1.m1.1.1" xref="S4.SS2.SSS2.p14.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.1.m1.1b"><ci id="S4.SS2.SSS2.p14.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p14.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p14.10.1" class="ltx_emph ltx_font_italic">Rotary Position Embedding.</em>
Rotary position embedding (RoPE)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite>
sets specific rotatory matrices based on the absolute position of each key or query.
The scores between keys and queries can be computed with relative position information (Table&nbsp;<a href="#S4.T6" title="TABLE VI ‣ 4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>).
RoPE combines each consecutive pair of elements in query and key vectors as <em id="S4.SS2.SSS2.p14.10.2" class="ltx_emph ltx_font_italic">a dimension</em>, so there are <math id="S4.SS2.SSS2.p14.2.m2.1" class="ltx_Math" alttext="d/2" display="inline"><semantics id="S4.SS2.SSS2.p14.2.m2.1a"><mrow id="S4.SS2.SSS2.p14.2.m2.1.1" xref="S4.SS2.SSS2.p14.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p14.2.m2.1.1.2" xref="S4.SS2.SSS2.p14.2.m2.1.1.2.cmml">d</mi><mo id="S4.SS2.SSS2.p14.2.m2.1.1.1" xref="S4.SS2.SSS2.p14.2.m2.1.1.1.cmml">/</mo><mn id="S4.SS2.SSS2.p14.2.m2.1.1.3" xref="S4.SS2.SSS2.p14.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.2.m2.1b"><apply id="S4.SS2.SSS2.p14.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p14.2.m2.1.1"><divide id="S4.SS2.SSS2.p14.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p14.2.m2.1.1.1"></divide><ci id="S4.SS2.SSS2.p14.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p14.2.m2.1.1.2">𝑑</ci><cn type="integer" id="S4.SS2.SSS2.p14.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p14.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.2.m2.1c">d/2</annotation></semantics></math> dimensions for an original <math id="S4.SS2.SSS2.p14.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS2.SSS2.p14.3.m3.1a"><mi id="S4.SS2.SSS2.p14.3.m3.1.1" xref="S4.SS2.SSS2.p14.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.3.m3.1b"><ci id="S4.SS2.SSS2.p14.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p14.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.3.m3.1c">d</annotation></semantics></math>-length embedding. For each dimension <math id="S4.SS2.SSS2.p14.4.m4.3" class="ltx_Math" alttext="i\in\{1,\dots,d/2\}" display="inline"><semantics id="S4.SS2.SSS2.p14.4.m4.3a"><mrow id="S4.SS2.SSS2.p14.4.m4.3.3" xref="S4.SS2.SSS2.p14.4.m4.3.3.cmml"><mi id="S4.SS2.SSS2.p14.4.m4.3.3.3" xref="S4.SS2.SSS2.p14.4.m4.3.3.3.cmml">i</mi><mo id="S4.SS2.SSS2.p14.4.m4.3.3.2" xref="S4.SS2.SSS2.p14.4.m4.3.3.2.cmml">∈</mo><mrow id="S4.SS2.SSS2.p14.4.m4.3.3.1.1" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.2.cmml"><mo stretchy="false" id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.2" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.2.cmml">{</mo><mn id="S4.SS2.SSS2.p14.4.m4.1.1" xref="S4.SS2.SSS2.p14.4.m4.1.1.cmml">1</mn><mo id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.3" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.SSS2.p14.4.m4.2.2" xref="S4.SS2.SSS2.p14.4.m4.2.2.cmml">…</mi><mo id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.4" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.2.cmml">,</mo><mrow id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.cmml"><mi id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.2" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.2.cmml">d</mi><mo id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.1" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.1.cmml">/</mo><mn id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.3" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.5" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.4.m4.3b"><apply id="S4.SS2.SSS2.p14.4.m4.3.3.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3"><in id="S4.SS2.SSS2.p14.4.m4.3.3.2.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.2"></in><ci id="S4.SS2.SSS2.p14.4.m4.3.3.3.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.3">𝑖</ci><set id="S4.SS2.SSS2.p14.4.m4.3.3.1.2.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1"><cn type="integer" id="S4.SS2.SSS2.p14.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p14.4.m4.1.1">1</cn><ci id="S4.SS2.SSS2.p14.4.m4.2.2.cmml" xref="S4.SS2.SSS2.p14.4.m4.2.2">…</ci><apply id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1"><divide id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.1.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.1"></divide><ci id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.2.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.2">𝑑</ci><cn type="integer" id="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.3.cmml" xref="S4.SS2.SSS2.p14.4.m4.3.3.1.1.1.3">2</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.4.m4.3c">i\in\{1,\dots,d/2\}</annotation></semantics></math>, the pair of involved elements will rotate based on the rotation angle <math id="S4.SS2.SSS2.p14.5.m5.1" class="ltx_Math" alttext="t\cdot\theta_{i}" display="inline"><semantics id="S4.SS2.SSS2.p14.5.m5.1a"><mrow id="S4.SS2.SSS2.p14.5.m5.1.1" xref="S4.SS2.SSS2.p14.5.m5.1.1.cmml"><mi id="S4.SS2.SSS2.p14.5.m5.1.1.2" xref="S4.SS2.SSS2.p14.5.m5.1.1.2.cmml">t</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS2.p14.5.m5.1.1.1" xref="S4.SS2.SSS2.p14.5.m5.1.1.1.cmml">⋅</mo><msub id="S4.SS2.SSS2.p14.5.m5.1.1.3" xref="S4.SS2.SSS2.p14.5.m5.1.1.3.cmml"><mi id="S4.SS2.SSS2.p14.5.m5.1.1.3.2" xref="S4.SS2.SSS2.p14.5.m5.1.1.3.2.cmml">θ</mi><mi id="S4.SS2.SSS2.p14.5.m5.1.1.3.3" xref="S4.SS2.SSS2.p14.5.m5.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.5.m5.1b"><apply id="S4.SS2.SSS2.p14.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1"><ci id="S4.SS2.SSS2.p14.5.m5.1.1.1.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1.1">⋅</ci><ci id="S4.SS2.SSS2.p14.5.m5.1.1.2.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1.2">𝑡</ci><apply id="S4.SS2.SSS2.p14.5.m5.1.1.3.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p14.5.m5.1.1.3.1.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS2.p14.5.m5.1.1.3.2.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1.3.2">𝜃</ci><ci id="S4.SS2.SSS2.p14.5.m5.1.1.3.3.cmml" xref="S4.SS2.SSS2.p14.5.m5.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.5.m5.1c">t\cdot\theta_{i}</annotation></semantics></math>, where <math id="S4.SS2.SSS2.p14.6.m6.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS2.p14.6.m6.1a"><mi id="S4.SS2.SSS2.p14.6.m6.1.1" xref="S4.SS2.SSS2.p14.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.6.m6.1b"><ci id="S4.SS2.SSS2.p14.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p14.6.m6.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.6.m6.1c">t</annotation></semantics></math> denotes the position index and <math id="S4.SS2.SSS2.p14.7.m7.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S4.SS2.SSS2.p14.7.m7.1a"><msub id="S4.SS2.SSS2.p14.7.m7.1.1" xref="S4.SS2.SSS2.p14.7.m7.1.1.cmml"><mi id="S4.SS2.SSS2.p14.7.m7.1.1.2" xref="S4.SS2.SSS2.p14.7.m7.1.1.2.cmml">θ</mi><mi id="S4.SS2.SSS2.p14.7.m7.1.1.3" xref="S4.SS2.SSS2.p14.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.7.m7.1b"><apply id="S4.SS2.SSS2.p14.7.m7.1.1.cmml" xref="S4.SS2.SSS2.p14.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p14.7.m7.1.1.1.cmml" xref="S4.SS2.SSS2.p14.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p14.7.m7.1.1.2.cmml" xref="S4.SS2.SSS2.p14.7.m7.1.1.2">𝜃</ci><ci id="S4.SS2.SSS2.p14.7.m7.1.1.3.cmml" xref="S4.SS2.SSS2.p14.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.7.m7.1c">\theta_{i}</annotation></semantics></math> is the basis in the dimension. Following sinusoidal position embeddings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, RoPE defines the <em id="S4.SS2.SSS2.p14.10.3" class="ltx_emph ltx_font_italic">basis</em> <math id="S4.SS2.SSS2.p14.8.m8.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S4.SS2.SSS2.p14.8.m8.1a"><msub id="S4.SS2.SSS2.p14.8.m8.1.1" xref="S4.SS2.SSS2.p14.8.m8.1.1.cmml"><mi id="S4.SS2.SSS2.p14.8.m8.1.1.2" xref="S4.SS2.SSS2.p14.8.m8.1.1.2.cmml">θ</mi><mi id="S4.SS2.SSS2.p14.8.m8.1.1.3" xref="S4.SS2.SSS2.p14.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.8.m8.1b"><apply id="S4.SS2.SSS2.p14.8.m8.1.1.cmml" xref="S4.SS2.SSS2.p14.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p14.8.m8.1.1.1.cmml" xref="S4.SS2.SSS2.p14.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p14.8.m8.1.1.2.cmml" xref="S4.SS2.SSS2.p14.8.m8.1.1.2">𝜃</ci><ci id="S4.SS2.SSS2.p14.8.m8.1.1.3.cmml" xref="S4.SS2.SSS2.p14.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.8.m8.1c">\theta_{i}</annotation></semantics></math> as an exponentiation of the <em id="S4.SS2.SSS2.p14.10.4" class="ltx_emph ltx_font_italic">base</em> <math id="S4.SS2.SSS2.p14.9.m9.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S4.SS2.SSS2.p14.9.m9.1a"><mi id="S4.SS2.SSS2.p14.9.m9.1.1" xref="S4.SS2.SSS2.p14.9.m9.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.9.m9.1b"><ci id="S4.SS2.SSS2.p14.9.m9.1.1.cmml" xref="S4.SS2.SSS2.p14.9.m9.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.9.m9.1c">b</annotation></semantics></math> (set to <math id="S4.SS2.SSS2.p14.10.m10.1" class="ltx_Math" alttext="10000" display="inline"><semantics id="S4.SS2.SSS2.p14.10.m10.1a"><mn id="S4.SS2.SSS2.p14.10.m10.1.1" xref="S4.SS2.SSS2.p14.10.m10.1.1.cmml">10000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.10.m10.1b"><cn type="integer" id="S4.SS2.SSS2.p14.10.m10.1.1.cmml" xref="S4.SS2.SSS2.p14.10.m10.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.10.m10.1c">10000</annotation></semantics></math> by default):</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.5" class="ltx_Math" alttext="\Theta=\{\theta_{i}=b^{-2(i-1)/d}|i\in\{1,2,\dots,d/2\}\}." display="block"><semantics id="S4.E4.m1.5a"><mrow id="S4.E4.m1.5.5.1" xref="S4.E4.m1.5.5.1.1.cmml"><mrow id="S4.E4.m1.5.5.1.1" xref="S4.E4.m1.5.5.1.1.cmml"><mi mathvariant="normal" id="S4.E4.m1.5.5.1.1.4" xref="S4.E4.m1.5.5.1.1.4.cmml">Θ</mi><mo id="S4.E4.m1.5.5.1.1.3" xref="S4.E4.m1.5.5.1.1.3.cmml">=</mo><mrow id="S4.E4.m1.5.5.1.1.2.2" xref="S4.E4.m1.5.5.1.1.2.3.cmml"><mo stretchy="false" id="S4.E4.m1.5.5.1.1.2.2.3" xref="S4.E4.m1.5.5.1.1.2.3.1.cmml">{</mo><mrow id="S4.E4.m1.5.5.1.1.1.1.1" xref="S4.E4.m1.5.5.1.1.1.1.1.cmml"><msub id="S4.E4.m1.5.5.1.1.1.1.1.2" xref="S4.E4.m1.5.5.1.1.1.1.1.2.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.1.2.2" xref="S4.E4.m1.5.5.1.1.1.1.1.2.2.cmml">θ</mi><mi id="S4.E4.m1.5.5.1.1.1.1.1.2.3" xref="S4.E4.m1.5.5.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E4.m1.5.5.1.1.1.1.1.1" xref="S4.E4.m1.5.5.1.1.1.1.1.1.cmml">=</mo><msup id="S4.E4.m1.5.5.1.1.1.1.1.3" xref="S4.E4.m1.5.5.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.1.3.2" xref="S4.E4.m1.5.5.1.1.1.1.1.3.2.cmml">b</mi><mrow id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml"><mo id="S4.E4.m1.1.1.1a" xref="S4.E4.m1.1.1.1.cmml">−</mo><mrow id="S4.E4.m1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.cmml"><mrow id="S4.E4.m1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.cmml"><mn id="S4.E4.m1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S4.E4.m1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.2.cmml">/</mo><mi id="S4.E4.m1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.3.cmml">d</mi></mrow></mrow></msup></mrow><mo lspace="0em" rspace="0em" id="S4.E4.m1.5.5.1.1.2.2.4" xref="S4.E4.m1.5.5.1.1.2.3.1.cmml">|</mo><mrow id="S4.E4.m1.5.5.1.1.2.2.2" xref="S4.E4.m1.5.5.1.1.2.2.2.cmml"><mi id="S4.E4.m1.5.5.1.1.2.2.2.3" xref="S4.E4.m1.5.5.1.1.2.2.2.3.cmml">i</mi><mo id="S4.E4.m1.5.5.1.1.2.2.2.2" xref="S4.E4.m1.5.5.1.1.2.2.2.2.cmml">∈</mo><mrow id="S4.E4.m1.5.5.1.1.2.2.2.1.1" xref="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.E4.m1.5.5.1.1.2.2.2.1.1.2" xref="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml">{</mo><mn id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml">1</mn><mo id="S4.E4.m1.5.5.1.1.2.2.2.1.1.3" xref="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml">,</mo><mn id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml">2</mn><mo id="S4.E4.m1.5.5.1.1.2.2.2.1.1.4" xref="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S4.E4.m1.4.4" xref="S4.E4.m1.4.4.cmml">…</mi><mo id="S4.E4.m1.5.5.1.1.2.2.2.1.1.5" xref="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml">,</mo><mrow id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.cmml"><mi id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.2" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.2.cmml">d</mi><mo id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.1" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.1.cmml">/</mo><mn id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.3" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S4.E4.m1.5.5.1.1.2.2.2.1.1.6" xref="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml">}</mo></mrow></mrow><mo stretchy="false" id="S4.E4.m1.5.5.1.1.2.2.5" xref="S4.E4.m1.5.5.1.1.2.3.1.cmml">}</mo></mrow></mrow><mo lspace="0em" id="S4.E4.m1.5.5.1.2" xref="S4.E4.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.5b"><apply id="S4.E4.m1.5.5.1.1.cmml" xref="S4.E4.m1.5.5.1"><eq id="S4.E4.m1.5.5.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.3"></eq><ci id="S4.E4.m1.5.5.1.1.4.cmml" xref="S4.E4.m1.5.5.1.1.4">Θ</ci><apply id="S4.E4.m1.5.5.1.1.2.3.cmml" xref="S4.E4.m1.5.5.1.1.2.2"><csymbol cd="latexml" id="S4.E4.m1.5.5.1.1.2.3.1.cmml" xref="S4.E4.m1.5.5.1.1.2.2.3">conditional-set</csymbol><apply id="S4.E4.m1.5.5.1.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1"><eq id="S4.E4.m1.5.5.1.1.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1"></eq><apply id="S4.E4.m1.5.5.1.1.1.1.1.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E4.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.2.2">𝜃</ci><ci id="S4.E4.m1.5.5.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E4.m1.5.5.1.1.1.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E4.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.3.2">𝑏</ci><apply id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"><minus id="S4.E4.m1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1"></minus><apply id="S4.E4.m1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1"><divide id="S4.E4.m1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.2"></divide><apply id="S4.E4.m1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1"><times id="S4.E4.m1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.2"></times><cn type="integer" id="S4.E4.m1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.3">2</cn><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1"><minus id="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S4.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S4.E4.m1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.3">𝑑</ci></apply></apply></apply></apply><apply id="S4.E4.m1.5.5.1.1.2.2.2.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2"><in id="S4.E4.m1.5.5.1.1.2.2.2.2.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.2"></in><ci id="S4.E4.m1.5.5.1.1.2.2.2.3.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.3">𝑖</ci><set id="S4.E4.m1.5.5.1.1.2.2.2.1.2.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1"><cn type="integer" id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2">1</cn><cn type="integer" id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3">2</cn><ci id="S4.E4.m1.4.4.cmml" xref="S4.E4.m1.4.4">…</ci><apply id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1"><divide id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.1"></divide><ci id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.2.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.2">𝑑</ci><cn type="integer" id="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.2.2.2.1.1.1.3">2</cn></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.5c">\Theta=\{\theta_{i}=b^{-2(i-1)/d}|i\in\{1,2,\dots,d/2\}\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS2.p14.11" class="ltx_p">Furthermore, a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite> defines the distance required to rotate one cycle (<math id="S4.SS2.SSS2.p14.11.m1.1" class="ltx_Math" alttext="2\pi" display="inline"><semantics id="S4.SS2.SSS2.p14.11.m1.1a"><mrow id="S4.SS2.SSS2.p14.11.m1.1.1" xref="S4.SS2.SSS2.p14.11.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p14.11.m1.1.1.2" xref="S4.SS2.SSS2.p14.11.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS2.p14.11.m1.1.1.1" xref="S4.SS2.SSS2.p14.11.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS2.p14.11.m1.1.1.3" xref="S4.SS2.SSS2.p14.11.m1.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p14.11.m1.1b"><apply id="S4.SS2.SSS2.p14.11.m1.1.1.cmml" xref="S4.SS2.SSS2.p14.11.m1.1.1"><times id="S4.SS2.SSS2.p14.11.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p14.11.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS2.p14.11.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p14.11.m1.1.1.2">2</cn><ci id="S4.SS2.SSS2.p14.11.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p14.11.m1.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p14.11.m1.1c">2\pi</annotation></semantics></math>) for each dimension as wavelength:</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.2" class="ltx_Math" alttext="\lambda_{i}=2\pi b^{2(i-1)/d}=2\pi/\theta_{i}." display="block"><semantics id="S4.E5.m1.2a"><mrow id="S4.E5.m1.2.2.1" xref="S4.E5.m1.2.2.1.1.cmml"><mrow id="S4.E5.m1.2.2.1.1" xref="S4.E5.m1.2.2.1.1.cmml"><msub id="S4.E5.m1.2.2.1.1.2" xref="S4.E5.m1.2.2.1.1.2.cmml"><mi id="S4.E5.m1.2.2.1.1.2.2" xref="S4.E5.m1.2.2.1.1.2.2.cmml">λ</mi><mi id="S4.E5.m1.2.2.1.1.2.3" xref="S4.E5.m1.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S4.E5.m1.2.2.1.1.3" xref="S4.E5.m1.2.2.1.1.3.cmml">=</mo><mrow id="S4.E5.m1.2.2.1.1.4" xref="S4.E5.m1.2.2.1.1.4.cmml"><mn id="S4.E5.m1.2.2.1.1.4.2" xref="S4.E5.m1.2.2.1.1.4.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.1.1.4.1" xref="S4.E5.m1.2.2.1.1.4.1.cmml">​</mo><mi id="S4.E5.m1.2.2.1.1.4.3" xref="S4.E5.m1.2.2.1.1.4.3.cmml">π</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.1.1.4.1a" xref="S4.E5.m1.2.2.1.1.4.1.cmml">​</mo><msup id="S4.E5.m1.2.2.1.1.4.4" xref="S4.E5.m1.2.2.1.1.4.4.cmml"><mi id="S4.E5.m1.2.2.1.1.4.4.2" xref="S4.E5.m1.2.2.1.1.4.4.2.cmml">b</mi><mrow id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mn id="S4.E5.m1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.E5.m1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S4.E5.m1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.1.1.1.2" xref="S4.E5.m1.1.1.1.2.cmml">/</mo><mi id="S4.E5.m1.1.1.1.3" xref="S4.E5.m1.1.1.1.3.cmml">d</mi></mrow></msup></mrow><mo id="S4.E5.m1.2.2.1.1.5" xref="S4.E5.m1.2.2.1.1.5.cmml">=</mo><mrow id="S4.E5.m1.2.2.1.1.6" xref="S4.E5.m1.2.2.1.1.6.cmml"><mrow id="S4.E5.m1.2.2.1.1.6.2" xref="S4.E5.m1.2.2.1.1.6.2.cmml"><mn id="S4.E5.m1.2.2.1.1.6.2.2" xref="S4.E5.m1.2.2.1.1.6.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.1.1.6.2.1" xref="S4.E5.m1.2.2.1.1.6.2.1.cmml">​</mo><mi id="S4.E5.m1.2.2.1.1.6.2.3" xref="S4.E5.m1.2.2.1.1.6.2.3.cmml">π</mi></mrow><mo id="S4.E5.m1.2.2.1.1.6.1" xref="S4.E5.m1.2.2.1.1.6.1.cmml">/</mo><msub id="S4.E5.m1.2.2.1.1.6.3" xref="S4.E5.m1.2.2.1.1.6.3.cmml"><mi id="S4.E5.m1.2.2.1.1.6.3.2" xref="S4.E5.m1.2.2.1.1.6.3.2.cmml">θ</mi><mi id="S4.E5.m1.2.2.1.1.6.3.3" xref="S4.E5.m1.2.2.1.1.6.3.3.cmml">i</mi></msub></mrow></mrow><mo lspace="0em" id="S4.E5.m1.2.2.1.2" xref="S4.E5.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.2b"><apply id="S4.E5.m1.2.2.1.1.cmml" xref="S4.E5.m1.2.2.1"><and id="S4.E5.m1.2.2.1.1a.cmml" xref="S4.E5.m1.2.2.1"></and><apply id="S4.E5.m1.2.2.1.1b.cmml" xref="S4.E5.m1.2.2.1"><eq id="S4.E5.m1.2.2.1.1.3.cmml" xref="S4.E5.m1.2.2.1.1.3"></eq><apply id="S4.E5.m1.2.2.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.1.1.2.1.cmml" xref="S4.E5.m1.2.2.1.1.2">subscript</csymbol><ci id="S4.E5.m1.2.2.1.1.2.2.cmml" xref="S4.E5.m1.2.2.1.1.2.2">𝜆</ci><ci id="S4.E5.m1.2.2.1.1.2.3.cmml" xref="S4.E5.m1.2.2.1.1.2.3">𝑖</ci></apply><apply id="S4.E5.m1.2.2.1.1.4.cmml" xref="S4.E5.m1.2.2.1.1.4"><times id="S4.E5.m1.2.2.1.1.4.1.cmml" xref="S4.E5.m1.2.2.1.1.4.1"></times><cn type="integer" id="S4.E5.m1.2.2.1.1.4.2.cmml" xref="S4.E5.m1.2.2.1.1.4.2">2</cn><ci id="S4.E5.m1.2.2.1.1.4.3.cmml" xref="S4.E5.m1.2.2.1.1.4.3">𝜋</ci><apply id="S4.E5.m1.2.2.1.1.4.4.cmml" xref="S4.E5.m1.2.2.1.1.4.4"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.1.1.4.4.1.cmml" xref="S4.E5.m1.2.2.1.1.4.4">superscript</csymbol><ci id="S4.E5.m1.2.2.1.1.4.4.2.cmml" xref="S4.E5.m1.2.2.1.1.4.4.2">𝑏</ci><apply id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"><divide id="S4.E5.m1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.2"></divide><apply id="S4.E5.m1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1"><times id="S4.E5.m1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.2"></times><cn type="integer" id="S4.E5.m1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3">2</cn><apply id="S4.E5.m1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1"><minus id="S4.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E5.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S4.E5.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S4.E5.m1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.3">𝑑</ci></apply></apply></apply></apply><apply id="S4.E5.m1.2.2.1.1c.cmml" xref="S4.E5.m1.2.2.1"><eq id="S4.E5.m1.2.2.1.1.5.cmml" xref="S4.E5.m1.2.2.1.1.5"></eq><share href="#S4.E5.m1.2.2.1.1.4.cmml" id="S4.E5.m1.2.2.1.1d.cmml" xref="S4.E5.m1.2.2.1"></share><apply id="S4.E5.m1.2.2.1.1.6.cmml" xref="S4.E5.m1.2.2.1.1.6"><divide id="S4.E5.m1.2.2.1.1.6.1.cmml" xref="S4.E5.m1.2.2.1.1.6.1"></divide><apply id="S4.E5.m1.2.2.1.1.6.2.cmml" xref="S4.E5.m1.2.2.1.1.6.2"><times id="S4.E5.m1.2.2.1.1.6.2.1.cmml" xref="S4.E5.m1.2.2.1.1.6.2.1"></times><cn type="integer" id="S4.E5.m1.2.2.1.1.6.2.2.cmml" xref="S4.E5.m1.2.2.1.1.6.2.2">2</cn><ci id="S4.E5.m1.2.2.1.1.6.2.3.cmml" xref="S4.E5.m1.2.2.1.1.6.2.3">𝜋</ci></apply><apply id="S4.E5.m1.2.2.1.1.6.3.cmml" xref="S4.E5.m1.2.2.1.1.6.3"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.1.1.6.3.1.cmml" xref="S4.E5.m1.2.2.1.1.6.3">subscript</csymbol><ci id="S4.E5.m1.2.2.1.1.6.3.2.cmml" xref="S4.E5.m1.2.2.1.1.6.3.2">𝜃</ci><ci id="S4.E5.m1.2.2.1.1.6.3.3.cmml" xref="S4.E5.m1.2.2.1.1.6.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.2c">\lambda_{i}=2\pi b^{2(i-1)/d}=2\pi/\theta_{i}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS2.p14.12" class="ltx_p">Due to the excellent performance and the long-term decay property, RoPE is widely adopted in the latest LLMs, <em id="S4.SS2.SSS2.p14.12.1" class="ltx_emph ltx_font_italic">e.g.,</em> PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Based on RoPE, xPos&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib277" title="" class="ltx_ref">277</a>]</cite> further improves the translation invariance and length extrapolation of Transformer. At each dimension of the rotation angle vector, xPos adds a special exponential decay that is smaller when the basis is larger. It can alleviate the unstable phenomenon during training as the distance increases.</p>
</div>
<div id="S4.SS2.SSS2.p15" class="ltx_para">
<p id="S4.SS2.SSS2.p15.1" class="ltx_p"><math id="S4.SS2.SSS2.p15.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p15.1.m1.1a"><mo id="S4.SS2.SSS2.p15.1.m1.1.1" xref="S4.SS2.SSS2.p15.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p15.1.m1.1b"><ci id="S4.SS2.SSS2.p15.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p15.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p15.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p15.1.1" class="ltx_emph ltx_font_italic">ALiBi.</em> ALiBi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite> is proposed to improve the extrapolation of Transformer. Similar to relative position embedding, it biases attention scores with a penalty based on the distances between keys and queries.
Different from the relative positional embedding methods like T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, the penalty scores in ALiBi are pre-defined without any trainable parameters.
Empirical results in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite> have shown that ALiBi has a better extrapolation performance on sequences that are longer than those for training than several popular
position embedding methods such as sinusoidal PE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, RoPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite>, and T5 bias&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. 
In addition, it has been shown that ALiBi can also improve training stability in BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p16" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p16.1" class="ltx_p"><span id="S4.SS2.SSS2.p16.1.1" class="ltx_text ltx_font_bold">Attention.</span>
Attention mechanism is a critical component of Transformer. It allows the tokens across the sequence to interact with each other and compute the representations of the input and output sequence.</p>
</div>
<div id="S4.SS2.SSS2.p17" class="ltx_para">
<p id="S4.SS2.SSS2.p17.1" class="ltx_p"><math id="S4.SS2.SSS2.p17.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p17.1.m1.1a"><mo id="S4.SS2.SSS2.p17.1.m1.1.1" xref="S4.SS2.SSS2.p17.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p17.1.m1.1b"><ci id="S4.SS2.SSS2.p17.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p17.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p17.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p17.1.1" class="ltx_emph ltx_font_italic">Full attention</em>. In the vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, the attention mechanism is conducted in a pairwise way, considering the relations between all token pairs in a sequence. It
adopts scaled dot-product attention, in which the hidden states are mapped into queries, keys, and values.
Additionally, Transformer uses multi-head attention instead of single attention, projecting the queries, keys, and values with different projections in different heads. The concatenation of the output of each head is taken as the final output.</p>
</div>
<div id="S4.SS2.SSS2.p18" class="ltx_para">
<p id="S4.SS2.SSS2.p18.1" class="ltx_p"><math id="S4.SS2.SSS2.p18.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p18.1.m1.1a"><mo id="S4.SS2.SSS2.p18.1.m1.1.1" xref="S4.SS2.SSS2.p18.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p18.1.m1.1b"><ci id="S4.SS2.SSS2.p18.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p18.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p18.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p18.1.1" class="ltx_emph ltx_font_italic">Sparse attention</em>. A crucial challenge of full attention is the quadratic computational complexity, which becomes a burden when dealing with long sequences. Therefore, various efficient Transformer variants are proposed to reduce the computational complexity of the attention mechanism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib278" title="" class="ltx_ref">278</a>, <a href="#bib.bib279" title="" class="ltx_ref">279</a>]</cite>. For instance, locally banded sparse attention (<em id="S4.SS2.SSS2.p18.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> Factorized Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib280" title="" class="ltx_ref">280</a>]</cite> has been adopted in GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. Instead of the whole sequence, each query can only attend to a subset of tokens based on the positions.</p>
</div>
<div id="S4.SS2.SSS2.p19" class="ltx_para">
<p id="S4.SS2.SSS2.p19.1" class="ltx_p"><math id="S4.SS2.SSS2.p19.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p19.1.m1.1a"><mo id="S4.SS2.SSS2.p19.1.m1.1.1" xref="S4.SS2.SSS2.p19.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p19.1.m1.1b"><ci id="S4.SS2.SSS2.p19.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p19.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p19.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p19.1.1" class="ltx_emph ltx_font_italic">Multi-query/grouped-query attention</em>. Multi-query attention refers to the attention variant
where different heads share the same linear transformation matrices on the keys and values&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib281" title="" class="ltx_ref">281</a>]</cite>.
It achieves higher inference speed with only a minor sacrifice in model quality.
Representative models with multi-query attention include PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and StarCoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. To make a trade-off between multi-query attention and multi-head attention, grouped-query attention (GQA)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib282" title="" class="ltx_ref">282</a>]</cite> has been explored. In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices. Specially, GQA has been adopted and empirically tested in the recently released LLaMA 2 model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p20" class="ltx_para">
<p id="S4.SS2.SSS2.p20.2" class="ltx_p"><math id="S4.SS2.SSS2.p20.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p20.1.m1.1a"><mo id="S4.SS2.SSS2.p20.1.m1.1.1" xref="S4.SS2.SSS2.p20.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p20.1.m1.1b"><ci id="S4.SS2.SSS2.p20.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p20.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p20.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p20.2.1" class="ltx_emph ltx_font_italic">FlashAttention</em>. Different from most existing approximate attention methods that trade-off model quality to improve the computing efficiency, FlashAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib283" title="" class="ltx_ref">283</a>]</cite> proposes to optimize the speed and memory consumption of attention modules on GPUs from an IO-aware perspective. There exist different levels of memory on modern GPUs, <em id="S4.SS2.SSS2.p20.2.2" class="ltx_emph ltx_font_italic">e.g.,</em> SRAM with a fast IO and HBM with a relatively slow IO. FlashAttention organizes the input into blocks and introduces necessary recomputation, both to make better use of the fast memory SRAM. Implemented as a fused kernel in CUDA, FlashAttention has been integrated into PyTorch&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>, DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, and Megatron-LM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. The updated version FlashAttention-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite> further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2<math id="S4.SS2.SSS2.p20.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS2.p20.2.m2.1a"><mo id="S4.SS2.SSS2.p20.2.m2.1.1" xref="S4.SS2.SSS2.p20.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p20.2.m2.1b"><times id="S4.SS2.SSS2.p20.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p20.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p20.2.m2.1c">\times</annotation></semantics></math> speedup when compared to the original FlashAttention.</p>
</div>
<div id="S4.SS2.SSS2.p21" class="ltx_para">
<p id="S4.SS2.SSS2.p21.1" class="ltx_p"><math id="S4.SS2.SSS2.p21.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p21.1.m1.1a"><mo id="S4.SS2.SSS2.p21.1.m1.1.1" xref="S4.SS2.SSS2.p21.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p21.1.m1.1b"><ci id="S4.SS2.SSS2.p21.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p21.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p21.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS2.p21.1.1" class="ltx_emph ltx_font_italic">PagedAttention</em>. It has been observed when LLM are deployed on servers, GPU memory is largely occupied by cached attention key and value tensors (called <em id="S4.SS2.SSS2.p21.1.2" class="ltx_emph ltx_font_italic">KV cache</em>). The major reason is that the input lengths are often varied, leading to fragmentation and over-reservation issues. Inspired by the classic paging technique in operating systems, PagedAttention has been proposed to improve the memory efficiency and throughput of deployed LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref">285</a>]</cite>. In detail, PagedAttention partitions each sequence into subsequences, and the corresponding KV caches of these subsequences are allocated into non-contiguous physical blocks. The paging technique increases the GPU utilization and enables efficient memory sharing in parallel sampling.</p>
</div>
<div id="S4.SS2.SSS2.p22" class="ltx_para">
<p id="S4.SS2.SSS2.p22.1" class="ltx_p">To put all these discussions together, we summarize the suggestions from existing literature for detailed configuration.
For stronger generalization and training stability, it is suggested to choose the pre RMSNorm for layer normalization, and SwiGLU or GeGLU as the activation function. In addition, LN may not be used immediately after embedding layers, which is likely to incur performance degradation. As for position embeddings, RoPE or ALiBi is a better choice since it performs better on long sequences.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Pre-training Tasks</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Pre-training plays a key role that encodes general knowledge from large-scale corpus into the massive model parameters.
For training LLMs, there are two commonly used pre-training tasks, namely language modeling and denoising autoencoding.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS3.p2.3" class="ltx_p"><span id="S4.SS2.SSS3.p2.3.1" class="ltx_text ltx_font_bold">Language Modeling.</span>
The language modeling task (LM) is the most commonly used objective to pre-train decoder-only LLMs, <em id="S4.SS2.SSS3.p2.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Given a sequence of tokens <math id="S4.SS2.SSS3.p2.1.m1.3" class="ltx_Math" alttext="\mathbf{x}=\{x_{1},\dots,x_{n}\}" display="inline"><semantics id="S4.SS2.SSS3.p2.1.m1.3a"><mrow id="S4.SS2.SSS3.p2.1.m1.3.3" xref="S4.SS2.SSS3.p2.1.m1.3.3.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.3.3.4" xref="S4.SS2.SSS3.p2.1.m1.3.3.4.cmml">𝐱</mi><mo id="S4.SS2.SSS3.p2.1.m1.3.3.3" xref="S4.SS2.SSS3.p2.1.m1.3.3.3.cmml">=</mo><mrow id="S4.SS2.SSS3.p2.1.m1.3.3.2.2" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.3" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.3.cmml">{</mo><msub id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.2" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.3" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.4" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml">…</mi><mo id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.5" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.3.cmml">,</mo><msub id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.2" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.2.cmml">x</mi><mi id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.3" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.6" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.3b"><apply id="S4.SS2.SSS3.p2.1.m1.3.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3"><eq id="S4.SS2.SSS3.p2.1.m1.3.3.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.3"></eq><ci id="S4.SS2.SSS3.p2.1.m1.3.3.4.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.4">𝐱</ci><set id="S4.SS2.SSS3.p2.1.m1.3.3.2.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2"><apply id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1">…</ci><apply id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.2">𝑥</ci><ci id="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.3.3.2.2.2.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.3c">\mathbf{x}=\{x_{1},\dots,x_{n}\}</annotation></semantics></math>, the LM task aims to autoregressively predict the target tokens <math id="S4.SS2.SSS3.p2.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><msub id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p2.2.m2.1.1.2" xref="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml">x</mi><mi id="S4.SS2.SSS3.p2.2.m2.1.1.3" xref="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><apply id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.2">𝑥</ci><ci id="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">x_{i}</annotation></semantics></math> based on the preceding tokens <math id="S4.SS2.SSS3.p2.3.m3.1" class="ltx_Math" alttext="x_{<i}" display="inline"><semantics id="S4.SS2.SSS3.p2.3.m3.1a"><msub id="S4.SS2.SSS3.p2.3.m3.1.1" xref="S4.SS2.SSS3.p2.3.m3.1.1.cmml"><mi id="S4.SS2.SSS3.p2.3.m3.1.1.2" xref="S4.SS2.SSS3.p2.3.m3.1.1.2.cmml">x</mi><mrow id="S4.SS2.SSS3.p2.3.m3.1.1.3" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.2" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.2.cmml"></mi><mo id="S4.SS2.SSS3.p2.3.m3.1.1.3.1" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.1.cmml">&lt;</mo><mi id="S4.SS2.SSS3.p2.3.m3.1.1.3.3" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.3.m3.1b"><apply id="S4.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.2">𝑥</ci><apply id="S4.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3"><lt id="S4.SS2.SSS3.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.1"></lt><csymbol cd="latexml" id="S4.SS2.SSS3.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.2">absent</csymbol><ci id="S4.SS2.SSS3.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS3.p2.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.3.m3.1c">x_{&lt;i}</annotation></semantics></math> in a sequence. A general training objective is to maximize the following likelihood:</p>
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.2" class="ltx_Math" alttext="\mathcal{L}_{LM}(\mathbf{x})=\sum_{i=1}^{n}\log P(x_{i}|\mathbf{x}_{<i})." display="block"><semantics id="S4.E6.m1.2a"><mrow id="S4.E6.m1.2.2.1" xref="S4.E6.m1.2.2.1.1.cmml"><mrow id="S4.E6.m1.2.2.1.1" xref="S4.E6.m1.2.2.1.1.cmml"><mrow id="S4.E6.m1.2.2.1.1.3" xref="S4.E6.m1.2.2.1.1.3.cmml"><msub id="S4.E6.m1.2.2.1.1.3.2" xref="S4.E6.m1.2.2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6.m1.2.2.1.1.3.2.2" xref="S4.E6.m1.2.2.1.1.3.2.2.cmml">ℒ</mi><mrow id="S4.E6.m1.2.2.1.1.3.2.3" xref="S4.E6.m1.2.2.1.1.3.2.3.cmml"><mi id="S4.E6.m1.2.2.1.1.3.2.3.2" xref="S4.E6.m1.2.2.1.1.3.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.1.1.3.2.3.1" xref="S4.E6.m1.2.2.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E6.m1.2.2.1.1.3.2.3.3" xref="S4.E6.m1.2.2.1.1.3.2.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.1.1.3.1" xref="S4.E6.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S4.E6.m1.2.2.1.1.3.3.2" xref="S4.E6.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S4.E6.m1.2.2.1.1.3.3.2.1" xref="S4.E6.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.E6.m1.2.2.1.1.3.3.2.2" xref="S4.E6.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S4.E6.m1.2.2.1.1.2" xref="S4.E6.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E6.m1.2.2.1.1.1" xref="S4.E6.m1.2.2.1.1.1.cmml"><munderover id="S4.E6.m1.2.2.1.1.1.2" xref="S4.E6.m1.2.2.1.1.1.2.cmml"><mo movablelimits="false" id="S4.E6.m1.2.2.1.1.1.2.2.2" xref="S4.E6.m1.2.2.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E6.m1.2.2.1.1.1.2.2.3" xref="S4.E6.m1.2.2.1.1.1.2.2.3.cmml"><mi id="S4.E6.m1.2.2.1.1.1.2.2.3.2" xref="S4.E6.m1.2.2.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E6.m1.2.2.1.1.1.2.2.3.1" xref="S4.E6.m1.2.2.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E6.m1.2.2.1.1.1.2.2.3.3" xref="S4.E6.m1.2.2.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E6.m1.2.2.1.1.1.2.3" xref="S4.E6.m1.2.2.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S4.E6.m1.2.2.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.cmml"><mrow id="S4.E6.m1.2.2.1.1.1.1.3" xref="S4.E6.m1.2.2.1.1.1.1.3.cmml"><mi id="S4.E6.m1.2.2.1.1.1.1.3.1" xref="S4.E6.m1.2.2.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E6.m1.2.2.1.1.1.1.3a" xref="S4.E6.m1.2.2.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E6.m1.2.2.1.1.1.1.3.2" xref="S4.E6.m1.2.2.1.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.1.1.1.1.2" xref="S4.E6.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S4.E6.m1.2.2.1.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E6.m1.2.2.1.1.1.1.1.1.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6.m1.2.2.1.1.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">𝐱</mi><mrow id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.1" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E6.m1.2.2.1.1.1.1.1.1.3" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S4.E6.m1.2.2.1.2" xref="S4.E6.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.2b"><apply id="S4.E6.m1.2.2.1.1.cmml" xref="S4.E6.m1.2.2.1"><eq id="S4.E6.m1.2.2.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.2"></eq><apply id="S4.E6.m1.2.2.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.3"><times id="S4.E6.m1.2.2.1.1.3.1.cmml" xref="S4.E6.m1.2.2.1.1.3.1"></times><apply id="S4.E6.m1.2.2.1.1.3.2.cmml" xref="S4.E6.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.3.2.1.cmml" xref="S4.E6.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S4.E6.m1.2.2.1.1.3.2.2.cmml" xref="S4.E6.m1.2.2.1.1.3.2.2">ℒ</ci><apply id="S4.E6.m1.2.2.1.1.3.2.3.cmml" xref="S4.E6.m1.2.2.1.1.3.2.3"><times id="S4.E6.m1.2.2.1.1.3.2.3.1.cmml" xref="S4.E6.m1.2.2.1.1.3.2.3.1"></times><ci id="S4.E6.m1.2.2.1.1.3.2.3.2.cmml" xref="S4.E6.m1.2.2.1.1.3.2.3.2">𝐿</ci><ci id="S4.E6.m1.2.2.1.1.3.2.3.3.cmml" xref="S4.E6.m1.2.2.1.1.3.2.3.3">𝑀</ci></apply></apply><ci id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1">𝐱</ci></apply><apply id="S4.E6.m1.2.2.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1"><apply id="S4.E6.m1.2.2.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.1.2.1.cmml" xref="S4.E6.m1.2.2.1.1.1.2">superscript</csymbol><apply id="S4.E6.m1.2.2.1.1.1.2.2.cmml" xref="S4.E6.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.1.2.2.1.cmml" xref="S4.E6.m1.2.2.1.1.1.2">subscript</csymbol><sum id="S4.E6.m1.2.2.1.1.1.2.2.2.cmml" xref="S4.E6.m1.2.2.1.1.1.2.2.2"></sum><apply id="S4.E6.m1.2.2.1.1.1.2.2.3.cmml" xref="S4.E6.m1.2.2.1.1.1.2.2.3"><eq id="S4.E6.m1.2.2.1.1.1.2.2.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.2.2.3.1"></eq><ci id="S4.E6.m1.2.2.1.1.1.2.2.3.2.cmml" xref="S4.E6.m1.2.2.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S4.E6.m1.2.2.1.1.1.2.2.3.3.cmml" xref="S4.E6.m1.2.2.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E6.m1.2.2.1.1.1.2.3.cmml" xref="S4.E6.m1.2.2.1.1.1.2.3">𝑛</ci></apply><apply id="S4.E6.m1.2.2.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1"><times id="S4.E6.m1.2.2.1.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.2"></times><apply id="S4.E6.m1.2.2.1.1.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.3"><log id="S4.E6.m1.2.2.1.1.1.1.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.3.1"></log><ci id="S4.E6.m1.2.2.1.1.1.1.3.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.3.2">𝑃</ci></apply><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.2">𝐱</ci><apply id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3"><lt id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E6.m1.2.2.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.2c">\mathcal{L}_{LM}(\mathbf{x})=\sum_{i=1}^{n}\log P(x_{i}|\mathbf{x}_{&lt;i}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p">Since most language tasks can be cast as the prediction problem based on the input, these decoder-only LLMs might be potentially advantageous to implicitly learn how to accomplish these tasks in a unified LM way.
Some studies have also revealed that decoder-only LLMs can be naturally transferred to certain tasks by autoregressively predicting the next tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, without fine-tuning.
An important variant of LM is the <em id="S4.SS2.SSS3.p3.1.1" class="ltx_emph ltx_font_italic">prefix language modeling</em> task, which is designed for pre-training models with the prefix decoder architecture.
The tokens within a randomly selected prefix would not be used in computing the loss of prefix language modeling.

With the same amount of tokens seen during pre-training, prefix language modeling performs slightly worse than language modeling, since fewer tokens in the sequence are involved for model pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS3.p4.2" class="ltx_p"><span id="S4.SS2.SSS3.p4.2.1" class="ltx_text ltx_font_bold">Denoising Autoencoding.</span>

In addition to conventional LM, the denoising autoencoding task (DAE) has also been widely used to pre-train language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.
The inputs <math id="S4.SS2.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{x}_{\backslash\tilde{\mathbf{x}}}" display="inline"><semantics id="S4.SS2.SSS3.p4.1.m1.1a"><msub id="S4.SS2.SSS3.p4.1.m1.1.1" xref="S4.SS2.SSS3.p4.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p4.1.m1.1.1.2" xref="S4.SS2.SSS3.p4.1.m1.1.1.2.cmml">𝐱</mi><mrow id="S4.SS2.SSS3.p4.1.m1.1.1.3" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.cmml"><mi id="S4.SS2.SSS3.p4.1.m1.1.1.3.2" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS3.p4.1.m1.1.1.3.1" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.1.cmml">\</mo><mover accent="true" id="S4.SS2.SSS3.p4.1.m1.1.1.3.3" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.3.cmml"><mi id="S4.SS2.SSS3.p4.1.m1.1.1.3.3.2" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.3.2.cmml">𝐱</mi><mo id="S4.SS2.SSS3.p4.1.m1.1.1.3.3.1" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.3.1.cmml">~</mo></mover></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p4.1.m1.1b"><apply id="S4.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p4.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p4.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.2">𝐱</ci><apply id="S4.SS2.SSS3.p4.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.3"><ci id="S4.SS2.SSS3.p4.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.1">\</ci><csymbol cd="latexml" id="S4.SS2.SSS3.p4.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.2">absent</csymbol><apply id="S4.SS2.SSS3.p4.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.3"><ci id="S4.SS2.SSS3.p4.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.3.1">~</ci><ci id="S4.SS2.SSS3.p4.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.SSS3.p4.1.m1.1.1.3.3.2">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p4.1.m1.1c">\mathbf{x}_{\backslash\tilde{\mathbf{x}}}</annotation></semantics></math> for DAE task are corrupted text with randomly replaced spans. Then, the language models are trained to recover the replaced tokens <math id="S4.SS2.SSS3.p4.2.m2.1" class="ltx_Math" alttext="\tilde{\mathbf{x}}" display="inline"><semantics id="S4.SS2.SSS3.p4.2.m2.1a"><mover accent="true" id="S4.SS2.SSS3.p4.2.m2.1.1" xref="S4.SS2.SSS3.p4.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p4.2.m2.1.1.2" xref="S4.SS2.SSS3.p4.2.m2.1.1.2.cmml">𝐱</mi><mo id="S4.SS2.SSS3.p4.2.m2.1.1.1" xref="S4.SS2.SSS3.p4.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p4.2.m2.1b"><apply id="S4.SS2.SSS3.p4.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1"><ci id="S4.SS2.SSS3.p4.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.1">~</ci><ci id="S4.SS2.SSS3.p4.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p4.2.m2.1.1.2">𝐱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p4.2.m2.1c">\tilde{\mathbf{x}}</annotation></semantics></math>. Formally, the training objective of DAE is denoted as follows:</p>
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.2" class="ltx_Math" alttext="\mathcal{L}_{DAE}(\mathbf{x})=\log P(\tilde{\mathbf{x}}|\mathbf{x}_{\backslash\tilde{\mathbf{x}}})." display="block"><semantics id="S4.E7.m1.2a"><mrow id="S4.E7.m1.2.2.1" xref="S4.E7.m1.2.2.1.1.cmml"><mrow id="S4.E7.m1.2.2.1.1" xref="S4.E7.m1.2.2.1.1.cmml"><mrow id="S4.E7.m1.2.2.1.1.3" xref="S4.E7.m1.2.2.1.1.3.cmml"><msub id="S4.E7.m1.2.2.1.1.3.2" xref="S4.E7.m1.2.2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.2.2.1.1.3.2.2" xref="S4.E7.m1.2.2.1.1.3.2.2.cmml">ℒ</mi><mrow id="S4.E7.m1.2.2.1.1.3.2.3" xref="S4.E7.m1.2.2.1.1.3.2.3.cmml"><mi id="S4.E7.m1.2.2.1.1.3.2.3.2" xref="S4.E7.m1.2.2.1.1.3.2.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.3.2.3.1" xref="S4.E7.m1.2.2.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.3.2.3.3" xref="S4.E7.m1.2.2.1.1.3.2.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.3.2.3.1a" xref="S4.E7.m1.2.2.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.3.2.3.4" xref="S4.E7.m1.2.2.1.1.3.2.3.4.cmml">E</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.3.1" xref="S4.E7.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S4.E7.m1.2.2.1.1.3.3.2" xref="S4.E7.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S4.E7.m1.2.2.1.1.3.3.2.1" xref="S4.E7.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.E7.m1.2.2.1.1.3.3.2.2" xref="S4.E7.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.2.2.1.1.2" xref="S4.E7.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E7.m1.2.2.1.1.1" xref="S4.E7.m1.2.2.1.1.1.cmml"><mrow id="S4.E7.m1.2.2.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.3.cmml"><mi id="S4.E7.m1.2.2.1.1.1.3.1" xref="S4.E7.m1.2.2.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E7.m1.2.2.1.1.1.3a" xref="S4.E7.m1.2.2.1.1.1.3.cmml">⁡</mo><mi id="S4.E7.m1.2.2.1.1.1.3.2" xref="S4.E7.m1.2.2.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.1.2" xref="S4.E7.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.E7.m1.2.2.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.2.2.1.1.1.1.1.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E7.m1.2.2.1.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E7.m1.2.2.1.1.1.1.1.1.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.2.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.2.2.cmml">𝐱</mi><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.2.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S4.E7.m1.2.2.1.1.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.cmml">|</mo><msub id="S4.E7.m1.2.2.1.1.1.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.3.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.2.cmml">𝐱</mi><mrow id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.1.cmml">\</mo><mover accent="true" id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.2.cmml">𝐱</mi><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.1.cmml">~</mo></mover></mrow></msub></mrow><mo stretchy="false" id="S4.E7.m1.2.2.1.1.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E7.m1.2.2.1.2" xref="S4.E7.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.2b"><apply id="S4.E7.m1.2.2.1.1.cmml" xref="S4.E7.m1.2.2.1"><eq id="S4.E7.m1.2.2.1.1.2.cmml" xref="S4.E7.m1.2.2.1.1.2"></eq><apply id="S4.E7.m1.2.2.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.3"><times id="S4.E7.m1.2.2.1.1.3.1.cmml" xref="S4.E7.m1.2.2.1.1.3.1"></times><apply id="S4.E7.m1.2.2.1.1.3.2.cmml" xref="S4.E7.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.3.2.1.cmml" xref="S4.E7.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S4.E7.m1.2.2.1.1.3.2.2.cmml" xref="S4.E7.m1.2.2.1.1.3.2.2">ℒ</ci><apply id="S4.E7.m1.2.2.1.1.3.2.3.cmml" xref="S4.E7.m1.2.2.1.1.3.2.3"><times id="S4.E7.m1.2.2.1.1.3.2.3.1.cmml" xref="S4.E7.m1.2.2.1.1.3.2.3.1"></times><ci id="S4.E7.m1.2.2.1.1.3.2.3.2.cmml" xref="S4.E7.m1.2.2.1.1.3.2.3.2">𝐷</ci><ci id="S4.E7.m1.2.2.1.1.3.2.3.3.cmml" xref="S4.E7.m1.2.2.1.1.3.2.3.3">𝐴</ci><ci id="S4.E7.m1.2.2.1.1.3.2.3.4.cmml" xref="S4.E7.m1.2.2.1.1.3.2.3.4">𝐸</ci></apply></apply><ci id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1">𝐱</ci></apply><apply id="S4.E7.m1.2.2.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1"><times id="S4.E7.m1.2.2.1.1.1.2.cmml" xref="S4.E7.m1.2.2.1.1.1.2"></times><apply id="S4.E7.m1.2.2.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.1.3"><log id="S4.E7.m1.2.2.1.1.1.3.1.cmml" xref="S4.E7.m1.2.2.1.1.1.3.1"></log><ci id="S4.E7.m1.2.2.1.1.1.3.2.cmml" xref="S4.E7.m1.2.2.1.1.1.3.2">𝑃</ci></apply><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S4.E7.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1">conditional</csymbol><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.2"><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.2.1">~</ci><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.2.2">𝐱</ci></apply><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.2">𝐱</ci><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3"><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.1">\</ci><csymbol cd="latexml" id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.2">absent</csymbol><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3"><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.1">~</ci><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.3.3.3.2">𝐱</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.2c">\mathcal{L}_{DAE}(\mathbf{x})=\log P(\tilde{\mathbf{x}}|\mathbf{x}_{\backslash\tilde{\mathbf{x}}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS3.p5" class="ltx_para">
<p id="S4.SS2.SSS3.p5.1" class="ltx_p">However, the DAE task seems to be more complicated in implementation than LM task. As a result, it has not been widely used to pre-train large language models.
Existing LLMs that take DAE as pre-training objectives include T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> and GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. These models are mainly trained to recover the replaced spans in an autoregressive way.</p>
</div>
<div id="S4.SS2.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS3.p6.1" class="ltx_p"><span id="S4.SS2.SSS3.p6.1.1" class="ltx_text ltx_font_bold">Mixture-of-Denoisers.</span> Mixture-of-Denoisers (MoD)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, also known as UL2 loss, was introduced as a unified objective for pre-training language models. MoD regards both LM and DAE objectives as different types of denoising tasks, namely S-denoiser (LM), R-denoiser (DAE, short span and low corruption), and X-denoiser (DAE, long span or high corruption).
Among the three denoising tasks, S-denoiser is similar to the conventional LM objective (Equation&nbsp;(<a href="#S4.E6" title="In 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>)), while R-denoiser and X-denoiser are similar to DAE objectives (Equation&nbsp;(<a href="#S4.E7" title="In 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>)) but differ from each other in the lengths of spans and ratio of corrupted text.
For input sentences started with different special tokens (<em id="S4.SS2.SSS3.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> {<span id="S4.SS2.SSS3.p6.1.3" class="ltx_text ltx_font_typewriter">[R]</span>, <span id="S4.SS2.SSS3.p6.1.4" class="ltx_text ltx_font_typewriter">[S]</span>, <span id="S4.SS2.SSS3.p6.1.5" class="ltx_text ltx_font_typewriter">[X]</span>}), the model will be optimized using the corresponding denoisers.
MoD has been
applied in the latest PaLM 2 model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>.</p>
</div>
<figure id="S4.F10" class="ltx_figure">
<table id="S4.F10.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.F10.1.1" class="ltx_tr">
<td id="S4.F10.1.1.1" class="ltx_td ltx_align_center" colspan="6">I am sleepy. I start a pot of _____</td>
</tr>
<tr id="S4.F10.1.2" class="ltx_tr">
<td id="S4.F10.1.2.1" class="ltx_td ltx_align_left ltx_border_t">coffee</td>
<td id="S4.F10.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.661</td>
<td id="S4.F10.1.2.3" class="ltx_td ltx_align_left ltx_border_t">strong</td>
<td id="S4.F10.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S4.F10.1.2.5" class="ltx_td ltx_align_left ltx_border_t">soup</td>
<td id="S4.F10.1.2.6" class="ltx_td ltx_align_center ltx_border_t">0.005</td>
</tr>
<tr id="S4.F10.1.3" class="ltx_tr">
<td id="S4.F10.1.3.1" class="ltx_td ltx_align_left">water</td>
<td id="S4.F10.1.3.2" class="ltx_td ltx_align_center ltx_border_r">0.119</td>
<td id="S4.F10.1.3.3" class="ltx_td ltx_align_left">black</td>
<td id="S4.F10.1.3.4" class="ltx_td ltx_align_center ltx_border_r">0.008</td>
<td id="S4.F10.1.3.5" class="ltx_td ltx_align_left">…</td>
<td id="S4.F10.1.3.6" class="ltx_td ltx_align_center">…</td>
</tr>
<tr id="S4.F10.1.4" class="ltx_tr">
<td id="S4.F10.1.4.1" class="ltx_td ltx_align_left">tea</td>
<td id="S4.F10.1.4.2" class="ltx_td ltx_align_center ltx_border_r">0.057</td>
<td id="S4.F10.1.4.3" class="ltx_td ltx_align_left">hot</td>
<td id="S4.F10.1.4.4" class="ltx_td ltx_align_center ltx_border_r">0.007</td>
<td id="S4.F10.1.4.5" class="ltx_td ltx_align_left">happy</td>
<td id="S4.F10.1.4.6" class="ltx_td ltx_align_center">4.3e-6</td>
</tr>
<tr id="S4.F10.1.5" class="ltx_tr">
<td id="S4.F10.1.5.1" class="ltx_td ltx_align_left">rice</td>
<td id="S4.F10.1.5.2" class="ltx_td ltx_align_center ltx_border_r">0.017</td>
<td id="S4.F10.1.5.3" class="ltx_td ltx_align_left">oat</td>
<td id="S4.F10.1.5.4" class="ltx_td ltx_align_center ltx_border_r">0.006</td>
<td id="S4.F10.1.5.5" class="ltx_td ltx_align_left">Boh</td>
<td id="S4.F10.1.5.6" class="ltx_td ltx_align_center">4.3e-6</td>
</tr>
<tr id="S4.F10.1.6" class="ltx_tr">
<td id="S4.F10.1.6.1" class="ltx_td ltx_align_left">chai</td>
<td id="S4.F10.1.6.2" class="ltx_td ltx_align_center ltx_border_r">0.012</td>
<td id="S4.F10.1.6.3" class="ltx_td ltx_align_left">beans</td>
<td id="S4.F10.1.6.4" class="ltx_td ltx_align_center ltx_border_r">0.006</td>
<td id="S4.F10.1.6.5" class="ltx_td ltx_align_left">…</td>
<td id="S4.F10.1.6.6" class="ltx_td ltx_align_center">…</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The probability distribution over the vocabulary in descending order for the next token of the context “<em id="S4.F10.3.1" class="ltx_emph ltx_font_italic">I am sleepy. I start a pot of</em>”. For ease of discussion, this example is given in word units instead of subword units. </figcaption>
</figure>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Long Context Modeling</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">In real applications, there is an increasing demand for long context modeling capacities of LLMs, such as PDF processing and story writing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib286" title="" class="ltx_ref">286</a>]</cite>. Many closed-source LLMs provide professional support for long text processing. For instance, OpenAI releases GPT-4 Turbo with a 128K context window, and Anthropic releases Claude 2.1 with a 200K context window. To enhance the long context modeling abilities, there are generally two feasible directions, namely scaling position embeddings and adapting context window. Next, we introduce the two parts in detail.</p>
</div>
<div id="S4.SS2.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS4.p2.1" class="ltx_p"><span id="S4.SS2.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Scaling Position Embeddings.</span>
Transformer-based LLMs can learn effective position embeddings within the maximum training length. Thus, when adapting LLMs to language tasks beyond the maximum training length, it is necessary to scale to larger position indices.
Some specific position embeddings have been shown to possess a certain degree of ability to generalize to text beyond the training length, which is formally termed <span id="S4.SS2.SSS4.p2.1.2" class="ltx_text ltx_font_italic">extrapolation capability</span>, including T5 bias&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, ALiBi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite>, xPos&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib277" title="" class="ltx_ref">277</a>]</cite> and even NoPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite>.
However, as one of the mainstream position embedding methods, RoPE exhibits limited extrapolation ability in empirical studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>]</cite>.
In the following, we discuss several methods that can scale RoPE to longer texts.</p>
</div>
<div id="S4.SS2.SSS4.p3" class="ltx_para">
<p id="S4.SS2.SSS4.p3.3" class="ltx_p"><math id="S4.SS2.SSS4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p3.1.m1.1a"><mo id="S4.SS2.SSS4.p3.1.m1.1.1" xref="S4.SS2.SSS4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p3.1.m1.1b"><ci id="S4.SS2.SSS4.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p3.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S4.SS2.SSS4.p3.3.1" class="ltx_emph ltx_font_italic">Direct model fine-tuning.</em>
To adapt LLMs to a long context window, a straightforward approach is to directly fine-tune the models on long texts with the desired length.
The context extension can be scheduled with increased lengths in a multi-stage approach (<em id="S4.SS2.SSS4.p3.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> 2K <math id="S4.SS2.SSS4.p3.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.SSS4.p3.2.m2.1a"><mo stretchy="false" id="S4.SS2.SSS4.p3.2.m2.1.1" xref="S4.SS2.SSS4.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p3.2.m2.1b"><ci id="S4.SS2.SSS4.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p3.2.m2.1c">\rightarrow</annotation></semantics></math> 8K <math id="S4.SS2.SSS4.p3.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.SSS4.p3.3.m3.1a"><mo stretchy="false" id="S4.SS2.SSS4.p3.3.m3.1.1" xref="S4.SS2.SSS4.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p3.3.m3.1b"><ci id="S4.SS2.SSS4.p3.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p3.3.m3.1c">\rightarrow</annotation></semantics></math> 32K).
To conduct effective extension, it needs specially prepared long texts for training.
Specially, some recent study has shown that the quality is more important than the lengths of training text in long context models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib288" title="" class="ltx_ref">288</a>]</cite>. However, a recent study has highlighted that
the fine-tuning approach tends to be inherently slow when adapting LLMs for long texts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS4.p4" class="ltx_para">
<p id="S4.SS2.SSS4.p4.5" class="ltx_p"><math id="S4.SS2.SSS4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p4.1.m1.1a"><mo id="S4.SS2.SSS4.p4.1.m1.1.1" xref="S4.SS2.SSS4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p4.1.m1.1b"><ci id="S4.SS2.SSS4.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p4.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S4.SS2.SSS4.p4.5.1" class="ltx_emph ltx_font_italic">Position interpolation.</em> This method downscales the position indices within the original context window, to avoid out-of-distribution rotation angles during pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib289" title="" class="ltx_ref">289</a>]</cite>. To be more specific, this approach multiplies all position indices by a coefficient <math id="S4.SS2.SSS4.p4.2.m2.1" class="ltx_Math" alttext="L/L^{\prime}" display="inline"><semantics id="S4.SS2.SSS4.p4.2.m2.1a"><mrow id="S4.SS2.SSS4.p4.2.m2.1.1" xref="S4.SS2.SSS4.p4.2.m2.1.1.cmml"><mi id="S4.SS2.SSS4.p4.2.m2.1.1.2" xref="S4.SS2.SSS4.p4.2.m2.1.1.2.cmml">L</mi><mo id="S4.SS2.SSS4.p4.2.m2.1.1.1" xref="S4.SS2.SSS4.p4.2.m2.1.1.1.cmml">/</mo><msup id="S4.SS2.SSS4.p4.2.m2.1.1.3" xref="S4.SS2.SSS4.p4.2.m2.1.1.3.cmml"><mi id="S4.SS2.SSS4.p4.2.m2.1.1.3.2" xref="S4.SS2.SSS4.p4.2.m2.1.1.3.2.cmml">L</mi><mo id="S4.SS2.SSS4.p4.2.m2.1.1.3.3" xref="S4.SS2.SSS4.p4.2.m2.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p4.2.m2.1b"><apply id="S4.SS2.SSS4.p4.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1"><divide id="S4.SS2.SSS4.p4.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1.1"></divide><ci id="S4.SS2.SSS4.p4.2.m2.1.1.2.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1.2">𝐿</ci><apply id="S4.SS2.SSS4.p4.2.m2.1.1.3.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p4.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS2.SSS4.p4.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1.3.2">𝐿</ci><ci id="S4.SS2.SSS4.p4.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS4.p4.2.m2.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p4.2.m2.1c">L/L^{\prime}</annotation></semantics></math> (<math id="S4.SS2.SSS4.p4.3.m3.1" class="ltx_Math" alttext="L<L^{\prime}" display="inline"><semantics id="S4.SS2.SSS4.p4.3.m3.1a"><mrow id="S4.SS2.SSS4.p4.3.m3.1.1" xref="S4.SS2.SSS4.p4.3.m3.1.1.cmml"><mi id="S4.SS2.SSS4.p4.3.m3.1.1.2" xref="S4.SS2.SSS4.p4.3.m3.1.1.2.cmml">L</mi><mo id="S4.SS2.SSS4.p4.3.m3.1.1.1" xref="S4.SS2.SSS4.p4.3.m3.1.1.1.cmml">&lt;</mo><msup id="S4.SS2.SSS4.p4.3.m3.1.1.3" xref="S4.SS2.SSS4.p4.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS4.p4.3.m3.1.1.3.2" xref="S4.SS2.SSS4.p4.3.m3.1.1.3.2.cmml">L</mi><mo id="S4.SS2.SSS4.p4.3.m3.1.1.3.3" xref="S4.SS2.SSS4.p4.3.m3.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p4.3.m3.1b"><apply id="S4.SS2.SSS4.p4.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1"><lt id="S4.SS2.SSS4.p4.3.m3.1.1.1.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1.1"></lt><ci id="S4.SS2.SSS4.p4.3.m3.1.1.2.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1.2">𝐿</ci><apply id="S4.SS2.SSS4.p4.3.m3.1.1.3.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p4.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS2.SSS4.p4.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1.3.2">𝐿</ci><ci id="S4.SS2.SSS4.p4.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS4.p4.3.m3.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p4.3.m3.1c">L&lt;L^{\prime}</annotation></semantics></math>), where <math id="S4.SS2.SSS4.p4.4.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS2.SSS4.p4.4.m4.1a"><mi id="S4.SS2.SSS4.p4.4.m4.1.1" xref="S4.SS2.SSS4.p4.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p4.4.m4.1b"><ci id="S4.SS2.SSS4.p4.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p4.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p4.4.m4.1c">L</annotation></semantics></math> and <math id="S4.SS2.SSS4.p4.5.m5.1" class="ltx_Math" alttext="L^{\prime}" display="inline"><semantics id="S4.SS2.SSS4.p4.5.m5.1a"><msup id="S4.SS2.SSS4.p4.5.m5.1.1" xref="S4.SS2.SSS4.p4.5.m5.1.1.cmml"><mi id="S4.SS2.SSS4.p4.5.m5.1.1.2" xref="S4.SS2.SSS4.p4.5.m5.1.1.2.cmml">L</mi><mo id="S4.SS2.SSS4.p4.5.m5.1.1.3" xref="S4.SS2.SSS4.p4.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p4.5.m5.1b"><apply id="S4.SS2.SSS4.p4.5.m5.1.1.cmml" xref="S4.SS2.SSS4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p4.5.m5.1.1.1.cmml" xref="S4.SS2.SSS4.p4.5.m5.1.1">superscript</csymbol><ci id="S4.SS2.SSS4.p4.5.m5.1.1.2.cmml" xref="S4.SS2.SSS4.p4.5.m5.1.1.2">𝐿</ci><ci id="S4.SS2.SSS4.p4.5.m5.1.1.3.cmml" xref="S4.SS2.SSS4.p4.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p4.5.m5.1c">L^{\prime}</annotation></semantics></math> represent the original and target context window length, respectively. 
Experimental results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>]</cite> have shown that this method can extend the context window effectively and efficiently, compared to the above approach of direct model fine-tuning. However, it is worth noting that this technique may have an adverse impact on the model’s performance when handling shorter texts<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib290" title="" class="ltx_ref">290</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS4.p5" class="ltx_para">
<p id="S4.SS2.SSS4.p5.1" class="ltx_p"><math id="S4.SS2.SSS4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p5.1.m1.1a"><mo id="S4.SS2.SSS4.p5.1.m1.1.1" xref="S4.SS2.SSS4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p5.1.m1.1b"><ci id="S4.SS2.SSS4.p5.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p5.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S4.SS2.SSS4.p5.1.1" class="ltx_emph ltx_font_italic">Position truncation.</em> To mitigate the challenges posed by out-of-distribution rotation angles, another practical approach is to truncate longer relative positions to satisfy the requirement of the maximum training length. Specifically, ReRoPE and LeakyReRoPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib291" title="" class="ltx_ref">291</a>]</cite> introduce a pre-defined window length, which is smaller than the maximum training length.
Position indices within this pre-defined window are retained, while those indices beyond the window are either truncated to the pre-defined window length or interpolated to align with the maximum training length. This strategy can reserve local position relationships and enhance the extrapolation capacity. However, this approach needs to compute the attention matrices twice, accommodating additional computational budget.</p>
</div>
<div id="S4.SS2.SSS4.p6" class="ltx_para">
<p id="S4.SS2.SSS4.p6.5" class="ltx_p"><math id="S4.SS2.SSS4.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p6.1.m1.1a"><mo id="S4.SS2.SSS4.p6.1.m1.1.1" xref="S4.SS2.SSS4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p6.1.m1.1b"><ci id="S4.SS2.SSS4.p6.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS4.p6.5.1" class="ltx_emph ltx_font_italic">Base modification.</em>
LLMs are usually trained with a pre-set maximum training length, <em id="S4.SS2.SSS4.p6.5.2" class="ltx_emph ltx_font_italic">e.g.,</em> 4096 in Llama 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. However, wavelengths in certain dimensions of RoPE may exceed the training length for longer text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite>, so that language models have not undergone sufficient training (<em id="S4.SS2.SSS4.p6.5.3" class="ltx_emph ltx_font_italic">i.e.,</em> a complete rotation cycle) on these dimensions. Thus, when we adapt LLMs to longer texts, the rotation angles for certain dimensions would be never seen in the training phase&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>]</cite>.
Given a fixed rotation angle <math id="S4.SS2.SSS4.p6.2.m2.1" class="ltx_Math" alttext="t\cdot\theta_{i}" display="inline"><semantics id="S4.SS2.SSS4.p6.2.m2.1a"><mrow id="S4.SS2.SSS4.p6.2.m2.1.1" xref="S4.SS2.SSS4.p6.2.m2.1.1.cmml"><mi id="S4.SS2.SSS4.p6.2.m2.1.1.2" xref="S4.SS2.SSS4.p6.2.m2.1.1.2.cmml">t</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS4.p6.2.m2.1.1.1" xref="S4.SS2.SSS4.p6.2.m2.1.1.1.cmml">⋅</mo><msub id="S4.SS2.SSS4.p6.2.m2.1.1.3" xref="S4.SS2.SSS4.p6.2.m2.1.1.3.cmml"><mi id="S4.SS2.SSS4.p6.2.m2.1.1.3.2" xref="S4.SS2.SSS4.p6.2.m2.1.1.3.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p6.2.m2.1.1.3.3" xref="S4.SS2.SSS4.p6.2.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p6.2.m2.1b"><apply id="S4.SS2.SSS4.p6.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1"><ci id="S4.SS2.SSS4.p6.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1.1">⋅</ci><ci id="S4.SS2.SSS4.p6.2.m2.1.1.2.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1.2">𝑡</ci><apply id="S4.SS2.SSS4.p6.2.m2.1.1.3.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p6.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS4.p6.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1.3.2">𝜃</ci><ci id="S4.SS2.SSS4.p6.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS4.p6.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p6.2.m2.1c">t\cdot\theta_{i}</annotation></semantics></math>, a smaller basis <math id="S4.SS2.SSS4.p6.3.m3.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S4.SS2.SSS4.p6.3.m3.1a"><msub id="S4.SS2.SSS4.p6.3.m3.1.1" xref="S4.SS2.SSS4.p6.3.m3.1.1.cmml"><mi id="S4.SS2.SSS4.p6.3.m3.1.1.2" xref="S4.SS2.SSS4.p6.3.m3.1.1.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p6.3.m3.1.1.3" xref="S4.SS2.SSS4.p6.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p6.3.m3.1b"><apply id="S4.SS2.SSS4.p6.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p6.3.m3.1.1.1.cmml" xref="S4.SS2.SSS4.p6.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p6.3.m3.1.1.2.cmml" xref="S4.SS2.SSS4.p6.3.m3.1.1.2">𝜃</ci><ci id="S4.SS2.SSS4.p6.3.m3.1.1.3.cmml" xref="S4.SS2.SSS4.p6.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p6.3.m3.1c">\theta_{i}</annotation></semantics></math> allows for a greater distance <math id="S4.SS2.SSS4.p6.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS4.p6.4.m4.1a"><mi id="S4.SS2.SSS4.p6.4.m4.1.1" xref="S4.SS2.SSS4.p6.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p6.4.m4.1b"><ci id="S4.SS2.SSS4.p6.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p6.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p6.4.m4.1c">t</annotation></semantics></math>, <em id="S4.SS2.SSS4.p6.5.4" class="ltx_emph ltx_font_italic">i.e.,</em> enabling the modeling of longer texts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>, <a href="#bib.bib235" title="" class="ltx_ref">235</a>, <a href="#bib.bib288" title="" class="ltx_ref">288</a>]</cite>.
According to the formula <math id="S4.SS2.SSS4.p6.5.m5.1" class="ltx_Math" alttext="\theta_{i}=b^{-2(i-1)/d}" display="inline"><semantics id="S4.SS2.SSS4.p6.5.m5.1a"><mrow id="S4.SS2.SSS4.p6.5.m5.1.2" xref="S4.SS2.SSS4.p6.5.m5.1.2.cmml"><msub id="S4.SS2.SSS4.p6.5.m5.1.2.2" xref="S4.SS2.SSS4.p6.5.m5.1.2.2.cmml"><mi id="S4.SS2.SSS4.p6.5.m5.1.2.2.2" xref="S4.SS2.SSS4.p6.5.m5.1.2.2.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p6.5.m5.1.2.2.3" xref="S4.SS2.SSS4.p6.5.m5.1.2.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS4.p6.5.m5.1.2.1" xref="S4.SS2.SSS4.p6.5.m5.1.2.1.cmml">=</mo><msup id="S4.SS2.SSS4.p6.5.m5.1.2.3" xref="S4.SS2.SSS4.p6.5.m5.1.2.3.cmml"><mi id="S4.SS2.SSS4.p6.5.m5.1.2.3.2" xref="S4.SS2.SSS4.p6.5.m5.1.2.3.2.cmml">b</mi><mrow id="S4.SS2.SSS4.p6.5.m5.1.1.1" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.cmml"><mo id="S4.SS2.SSS4.p6.5.m5.1.1.1a" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.cmml">−</mo><mrow id="S4.SS2.SSS4.p6.5.m5.1.1.1.1" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.cmml"><mrow id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.cmml"><mn id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.3" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.2" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.2" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.2" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.3" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.3" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.2" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.2.cmml">/</mo><mi id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.3" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.3.cmml">d</mi></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p6.5.m5.1b"><apply id="S4.SS2.SSS4.p6.5.m5.1.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2"><eq id="S4.SS2.SSS4.p6.5.m5.1.2.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.1"></eq><apply id="S4.SS2.SSS4.p6.5.m5.1.2.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p6.5.m5.1.2.2.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.2">subscript</csymbol><ci id="S4.SS2.SSS4.p6.5.m5.1.2.2.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.2.2">𝜃</ci><ci id="S4.SS2.SSS4.p6.5.m5.1.2.2.3.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.2.3">𝑖</ci></apply><apply id="S4.SS2.SSS4.p6.5.m5.1.2.3.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p6.5.m5.1.2.3.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.3">superscript</csymbol><ci id="S4.SS2.SSS4.p6.5.m5.1.2.3.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.2.3.2">𝑏</ci><apply id="S4.SS2.SSS4.p6.5.m5.1.1.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1"><minus id="S4.SS2.SSS4.p6.5.m5.1.1.1.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1"></minus><apply id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1"><divide id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.2"></divide><apply id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1"><times id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.2"></times><cn type="integer" id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.3">2</cn><apply id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1"><minus id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S4.SS2.SSS4.p6.5.m5.1.1.1.1.3.cmml" xref="S4.SS2.SSS4.p6.5.m5.1.1.1.1.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p6.5.m5.1c">\theta_{i}=b^{-2(i-1)/d}</annotation></semantics></math> in Equation&nbsp;<a href="#S4.E4" title="In 4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, decreasing the basis can be achieved by increasing the value of the base. In addition,
decreasing the base can also help re-scale the wavelengths of all dimensions below the training length, while it often needs continual pre-training to adapt the LLMs to long context windows&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>]</cite>.
A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>]</cite> has empirically compared these two base modification methods, and shown that decreasing the base demonstrates a better extrapolation capacity beyond the training length, while increasing the base performs better within the training length.</p>
</div>
<div id="S4.SS2.SSS4.p7" class="ltx_para">
<p id="S4.SS2.SSS4.p7.8" class="ltx_p"><math id="S4.SS2.SSS4.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p7.1.m1.1a"><mo id="S4.SS2.SSS4.p7.1.m1.1.1" xref="S4.SS2.SSS4.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.1.m1.1b"><ci id="S4.SS2.SSS4.p7.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS4.p7.8.1" class="ltx_emph ltx_font_italic">Basis truncation.</em> Similar to the base modification, the truncation of the basis also concentrates on dealing with the singular dimensions with wavelengths exceeding the training length&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite>. According to the definition <math id="S4.SS2.SSS4.p7.2.m2.1" class="ltx_Math" alttext="\lambda_{i}=2\pi/\theta_{i}" display="inline"><semantics id="S4.SS2.SSS4.p7.2.m2.1a"><mrow id="S4.SS2.SSS4.p7.2.m2.1.1" xref="S4.SS2.SSS4.p7.2.m2.1.1.cmml"><msub id="S4.SS2.SSS4.p7.2.m2.1.1.2" xref="S4.SS2.SSS4.p7.2.m2.1.1.2.cmml"><mi id="S4.SS2.SSS4.p7.2.m2.1.1.2.2" xref="S4.SS2.SSS4.p7.2.m2.1.1.2.2.cmml">λ</mi><mi id="S4.SS2.SSS4.p7.2.m2.1.1.2.3" xref="S4.SS2.SSS4.p7.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS4.p7.2.m2.1.1.1" xref="S4.SS2.SSS4.p7.2.m2.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS4.p7.2.m2.1.1.3" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.cmml"><mrow id="S4.SS2.SSS4.p7.2.m2.1.1.3.2" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.cmml"><mn id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.2" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.1" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.3" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.3.cmml">π</mi></mrow><mo id="S4.SS2.SSS4.p7.2.m2.1.1.3.1" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.1.cmml">/</mo><msub id="S4.SS2.SSS4.p7.2.m2.1.1.3.3" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3.cmml"><mi id="S4.SS2.SSS4.p7.2.m2.1.1.3.3.2" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p7.2.m2.1.1.3.3.3" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.2.m2.1b"><apply id="S4.SS2.SSS4.p7.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1"><eq id="S4.SS2.SSS4.p7.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.1"></eq><apply id="S4.SS2.SSS4.p7.2.m2.1.1.2.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.2.m2.1.1.2.1.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS4.p7.2.m2.1.1.2.2.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.2.2">𝜆</ci><ci id="S4.SS2.SSS4.p7.2.m2.1.1.2.3.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.SSS4.p7.2.m2.1.1.3.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3"><divide id="S4.SS2.SSS4.p7.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.1"></divide><apply id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2"><times id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.1.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.1"></times><cn type="integer" id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.2.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.2">2</cn><ci id="S4.SS2.SSS4.p7.2.m2.1.1.3.2.3.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.2.3">𝜋</ci></apply><apply id="S4.SS2.SSS4.p7.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3">subscript</csymbol><ci id="S4.SS2.SSS4.p7.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3.2">𝜃</ci><ci id="S4.SS2.SSS4.p7.2.m2.1.1.3.3.3.cmml" xref="S4.SS2.SSS4.p7.2.m2.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.2.m2.1c">\lambda_{i}=2\pi/\theta_{i}</annotation></semantics></math> in Equation&nbsp;<a href="#S4.E5" title="In 4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the dimension with a large wavelength <math id="S4.SS2.SSS4.p7.3.m3.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.SS2.SSS4.p7.3.m3.1a"><msub id="S4.SS2.SSS4.p7.3.m3.1.1" xref="S4.SS2.SSS4.p7.3.m3.1.1.cmml"><mi id="S4.SS2.SSS4.p7.3.m3.1.1.2" xref="S4.SS2.SSS4.p7.3.m3.1.1.2.cmml">λ</mi><mi id="S4.SS2.SSS4.p7.3.m3.1.1.3" xref="S4.SS2.SSS4.p7.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.3.m3.1b"><apply id="S4.SS2.SSS4.p7.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.3.m3.1.1.1.cmml" xref="S4.SS2.SSS4.p7.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p7.3.m3.1.1.2.cmml" xref="S4.SS2.SSS4.p7.3.m3.1.1.2">𝜆</ci><ci id="S4.SS2.SSS4.p7.3.m3.1.1.3.cmml" xref="S4.SS2.SSS4.p7.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.3.m3.1c">\lambda_{i}</annotation></semantics></math> has a small basis <math id="S4.SS2.SSS4.p7.4.m4.1" class="ltx_Math" alttext="\theta_{i}" display="inline"><semantics id="S4.SS2.SSS4.p7.4.m4.1a"><msub id="S4.SS2.SSS4.p7.4.m4.1.1" xref="S4.SS2.SSS4.p7.4.m4.1.1.cmml"><mi id="S4.SS2.SSS4.p7.4.m4.1.1.2" xref="S4.SS2.SSS4.p7.4.m4.1.1.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p7.4.m4.1.1.3" xref="S4.SS2.SSS4.p7.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.4.m4.1b"><apply id="S4.SS2.SSS4.p7.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.4.m4.1.1.1.cmml" xref="S4.SS2.SSS4.p7.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.SSS4.p7.4.m4.1.1.2.cmml" xref="S4.SS2.SSS4.p7.4.m4.1.1.2">𝜃</ci><ci id="S4.SS2.SSS4.p7.4.m4.1.1.3.cmml" xref="S4.SS2.SSS4.p7.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.4.m4.1c">\theta_{i}</annotation></semantics></math> accordingly. Based on this observation, this approach first defines a basis range <math id="S4.SS2.SSS4.p7.5.m5.2" class="ltx_Math" alttext="[a,c]" display="inline"><semantics id="S4.SS2.SSS4.p7.5.m5.2a"><mrow id="S4.SS2.SSS4.p7.5.m5.2.3.2" xref="S4.SS2.SSS4.p7.5.m5.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.SSS4.p7.5.m5.2.3.2.1" xref="S4.SS2.SSS4.p7.5.m5.2.3.1.cmml">[</mo><mi id="S4.SS2.SSS4.p7.5.m5.1.1" xref="S4.SS2.SSS4.p7.5.m5.1.1.cmml">a</mi><mo id="S4.SS2.SSS4.p7.5.m5.2.3.2.2" xref="S4.SS2.SSS4.p7.5.m5.2.3.1.cmml">,</mo><mi id="S4.SS2.SSS4.p7.5.m5.2.2" xref="S4.SS2.SSS4.p7.5.m5.2.2.cmml">c</mi><mo stretchy="false" id="S4.SS2.SSS4.p7.5.m5.2.3.2.3" xref="S4.SS2.SSS4.p7.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.5.m5.2b"><interval closure="closed" id="S4.SS2.SSS4.p7.5.m5.2.3.1.cmml" xref="S4.SS2.SSS4.p7.5.m5.2.3.2"><ci id="S4.SS2.SSS4.p7.5.m5.1.1.cmml" xref="S4.SS2.SSS4.p7.5.m5.1.1">𝑎</ci><ci id="S4.SS2.SSS4.p7.5.m5.2.2.cmml" xref="S4.SS2.SSS4.p7.5.m5.2.2">𝑐</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.5.m5.2c">[a,c]</annotation></semantics></math>.

Given the basis range, the value of basis is modified according to the following ways: (1) when <math id="S4.SS2.SSS4.p7.6.m6.1" class="ltx_Math" alttext="\theta_{i}\geq c" display="inline"><semantics id="S4.SS2.SSS4.p7.6.m6.1a"><mrow id="S4.SS2.SSS4.p7.6.m6.1.1" xref="S4.SS2.SSS4.p7.6.m6.1.1.cmml"><msub id="S4.SS2.SSS4.p7.6.m6.1.1.2" xref="S4.SS2.SSS4.p7.6.m6.1.1.2.cmml"><mi id="S4.SS2.SSS4.p7.6.m6.1.1.2.2" xref="S4.SS2.SSS4.p7.6.m6.1.1.2.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p7.6.m6.1.1.2.3" xref="S4.SS2.SSS4.p7.6.m6.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS4.p7.6.m6.1.1.1" xref="S4.SS2.SSS4.p7.6.m6.1.1.1.cmml">≥</mo><mi id="S4.SS2.SSS4.p7.6.m6.1.1.3" xref="S4.SS2.SSS4.p7.6.m6.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.6.m6.1b"><apply id="S4.SS2.SSS4.p7.6.m6.1.1.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1"><geq id="S4.SS2.SSS4.p7.6.m6.1.1.1.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1.1"></geq><apply id="S4.SS2.SSS4.p7.6.m6.1.1.2.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.6.m6.1.1.2.1.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS4.p7.6.m6.1.1.2.2.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1.2.2">𝜃</ci><ci id="S4.SS2.SSS4.p7.6.m6.1.1.2.3.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1.2.3">𝑖</ci></apply><ci id="S4.SS2.SSS4.p7.6.m6.1.1.3.cmml" xref="S4.SS2.SSS4.p7.6.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.6.m6.1c">\theta_{i}\geq c</annotation></semantics></math>, the value is retained, (2) when <math id="S4.SS2.SSS4.p7.7.m7.1" class="ltx_Math" alttext="\theta_{i}\leq a" display="inline"><semantics id="S4.SS2.SSS4.p7.7.m7.1a"><mrow id="S4.SS2.SSS4.p7.7.m7.1.1" xref="S4.SS2.SSS4.p7.7.m7.1.1.cmml"><msub id="S4.SS2.SSS4.p7.7.m7.1.1.2" xref="S4.SS2.SSS4.p7.7.m7.1.1.2.cmml"><mi id="S4.SS2.SSS4.p7.7.m7.1.1.2.2" xref="S4.SS2.SSS4.p7.7.m7.1.1.2.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p7.7.m7.1.1.2.3" xref="S4.SS2.SSS4.p7.7.m7.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS4.p7.7.m7.1.1.1" xref="S4.SS2.SSS4.p7.7.m7.1.1.1.cmml">≤</mo><mi id="S4.SS2.SSS4.p7.7.m7.1.1.3" xref="S4.SS2.SSS4.p7.7.m7.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.7.m7.1b"><apply id="S4.SS2.SSS4.p7.7.m7.1.1.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1"><leq id="S4.SS2.SSS4.p7.7.m7.1.1.1.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1.1"></leq><apply id="S4.SS2.SSS4.p7.7.m7.1.1.2.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.7.m7.1.1.2.1.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS4.p7.7.m7.1.1.2.2.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1.2.2">𝜃</ci><ci id="S4.SS2.SSS4.p7.7.m7.1.1.2.3.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1.2.3">𝑖</ci></apply><ci id="S4.SS2.SSS4.p7.7.m7.1.1.3.cmml" xref="S4.SS2.SSS4.p7.7.m7.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.7.m7.1c">\theta_{i}\leq a</annotation></semantics></math>, the value is set to zero, and (3) when <math id="S4.SS2.SSS4.p7.8.m8.1" class="ltx_Math" alttext="a<\theta_{i}<c" display="inline"><semantics id="S4.SS2.SSS4.p7.8.m8.1a"><mrow id="S4.SS2.SSS4.p7.8.m8.1.1" xref="S4.SS2.SSS4.p7.8.m8.1.1.cmml"><mi id="S4.SS2.SSS4.p7.8.m8.1.1.2" xref="S4.SS2.SSS4.p7.8.m8.1.1.2.cmml">a</mi><mo id="S4.SS2.SSS4.p7.8.m8.1.1.3" xref="S4.SS2.SSS4.p7.8.m8.1.1.3.cmml">&lt;</mo><msub id="S4.SS2.SSS4.p7.8.m8.1.1.4" xref="S4.SS2.SSS4.p7.8.m8.1.1.4.cmml"><mi id="S4.SS2.SSS4.p7.8.m8.1.1.4.2" xref="S4.SS2.SSS4.p7.8.m8.1.1.4.2.cmml">θ</mi><mi id="S4.SS2.SSS4.p7.8.m8.1.1.4.3" xref="S4.SS2.SSS4.p7.8.m8.1.1.4.3.cmml">i</mi></msub><mo id="S4.SS2.SSS4.p7.8.m8.1.1.5" xref="S4.SS2.SSS4.p7.8.m8.1.1.5.cmml">&lt;</mo><mi id="S4.SS2.SSS4.p7.8.m8.1.1.6" xref="S4.SS2.SSS4.p7.8.m8.1.1.6.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p7.8.m8.1b"><apply id="S4.SS2.SSS4.p7.8.m8.1.1.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1"><and id="S4.SS2.SSS4.p7.8.m8.1.1a.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1"></and><apply id="S4.SS2.SSS4.p7.8.m8.1.1b.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1"><lt id="S4.SS2.SSS4.p7.8.m8.1.1.3.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.3"></lt><ci id="S4.SS2.SSS4.p7.8.m8.1.1.2.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.2">𝑎</ci><apply id="S4.SS2.SSS4.p7.8.m8.1.1.4.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.SSS4.p7.8.m8.1.1.4.1.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.4">subscript</csymbol><ci id="S4.SS2.SSS4.p7.8.m8.1.1.4.2.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.4.2">𝜃</ci><ci id="S4.SS2.SSS4.p7.8.m8.1.1.4.3.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.4.3">𝑖</ci></apply></apply><apply id="S4.SS2.SSS4.p7.8.m8.1.1c.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1"><lt id="S4.SS2.SSS4.p7.8.m8.1.1.5.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.5"></lt><share href="#S4.SS2.SSS4.p7.8.m8.1.1.4.cmml" id="S4.SS2.SSS4.p7.8.m8.1.1d.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1"></share><ci id="S4.SS2.SSS4.p7.8.m8.1.1.6.cmml" xref="S4.SS2.SSS4.p7.8.m8.1.1.6">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p7.8.m8.1c">a&lt;\theta_{i}&lt;c</annotation></semantics></math>, the value is truncated to a fixed small value.
Via basis truncation, the out-of-distribution rotation angles can be avoided at larger position indices. However, this approach does not perform very well at long context tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS4.p8" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS4.p8.1" class="ltx_p"><span id="S4.SS2.SSS4.p8.1.1" class="ltx_text ltx_font_bold">Adapting Context Window.</span>
Since Transformer-based LLMs have limited context windows, they can not directly integrate or utilize the entire information of the long sequences exceeding the context window. To alleviate the limitation, several methods adapting LLMs to long context have been proposed, as discussed below.</p>
</div>
<div id="S4.SS2.SSS4.p9" class="ltx_para">
<p id="S4.SS2.SSS4.p9.1" class="ltx_p"><math id="S4.SS2.SSS4.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p9.1.m1.1a"><mo id="S4.SS2.SSS4.p9.1.m1.1.1" xref="S4.SS2.SSS4.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p9.1.m1.1b"><ci id="S4.SS2.SSS4.p9.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS4.p9.1.1" class="ltx_emph ltx_font_italic">Parallel context window.</em> Inspired by fusion-in-decoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref">294</a>]</cite>, parallel context window methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib295" title="" class="ltx_ref">295</a>, <a href="#bib.bib296" title="" class="ltx_ref">296</a>]</cite> adopt a divide-and-conquer strategy to process input text. Specially, it divides the input text into multiple segments, each independently encoded with shared position embeddings. In the generation stage, the attention masks are modified to make that subsequent tokens can access to previous tokens in each segment.
Nevertheless, this method cannot distinguish the order of different segments, constraining the model capacity on certain tasks.</p>
</div>
<div id="S4.SS2.SSS4.p10" class="ltx_para">
<p id="S4.SS2.SSS4.p10.3" class="ltx_p"><math id="S4.SS2.SSS4.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p10.1.m1.1a"><mo id="S4.SS2.SSS4.p10.1.m1.1.1" xref="S4.SS2.SSS4.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p10.1.m1.1b"><ci id="S4.SS2.SSS4.p10.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS4.p10.2.1" class="ltx_emph ltx_font_italic"><math id="S4.SS2.SSS4.p10.2.1.m1.1" class="ltx_Math" alttext="\Lambda" display="inline"><semantics id="S4.SS2.SSS4.p10.2.1.m1.1a"><mi mathvariant="normal" id="S4.SS2.SSS4.p10.2.1.m1.1.1" xref="S4.SS2.SSS4.p10.2.1.m1.1.1.cmml">Λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p10.2.1.m1.1b"><ci id="S4.SS2.SSS4.p10.2.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p10.2.1.m1.1.1">Λ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p10.2.1.m1.1c">\Lambda</annotation></semantics></math>-shaped context window.</em>
Some prior work has revealed that LLMs tend to allocate greater attention weights to the starting and nearest tokens among all previous tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib297" title="" class="ltx_ref">297</a>, <a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite>, so called the “<em id="S4.SS2.SSS4.p10.3.2" class="ltx_emph ltx_font_italic">lost in the middle</em>” phenomenon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib299" title="" class="ltx_ref">299</a>]</cite>.
Based on this observation,
LM-Infinite&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib300" title="" class="ltx_ref">300</a>]</cite> and StreamingLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite> propose to employ a “<math id="S4.SS2.SSS4.p10.3.m2.1" class="ltx_Math" alttext="\Lambda" display="inline"><semantics id="S4.SS2.SSS4.p10.3.m2.1a"><mi mathvariant="normal" id="S4.SS2.SSS4.p10.3.m2.1.1" xref="S4.SS2.SSS4.p10.3.m2.1.1.cmml">Λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p10.3.m2.1b"><ci id="S4.SS2.SSS4.p10.3.m2.1.1.cmml" xref="S4.SS2.SSS4.p10.3.m2.1.1">Λ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p10.3.m2.1c">\Lambda</annotation></semantics></math>-shaped” attention mask, which selectively preserves the initial tokens and the nearest tokens that each query can attend to and then discards any tokens beyond this scope. Experiments demonstrate that this method can facilitate extra-long text generation with a fixed memory&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite>. However, it may struggle to model the long-range dependency in prompts, since it cannot effectively utilize the information from the discarded tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS4.p11" class="ltx_para">
<p id="S4.SS2.SSS4.p11.5" class="ltx_p"><math id="S4.SS2.SSS4.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS4.p11.1.m1.1a"><mo id="S4.SS2.SSS4.p11.1.m1.1.1" xref="S4.SS2.SSS4.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p11.1.m1.1b"><ci id="S4.SS2.SSS4.p11.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p11.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS4.p11.5.1" class="ltx_emph ltx_font_italic">External memory.</em>
It has been shown that a relatively small subset of tokens can effectively capture the majority of attention patterns in a Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib301" title="" class="ltx_ref">301</a>]</cite>, <em id="S4.SS2.SSS4.p11.5.2" class="ltx_emph ltx_font_italic">i.e.,</em> the top-<math id="S4.SS2.SSS4.p11.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS4.p11.2.m2.1a"><mi id="S4.SS2.SSS4.p11.2.m2.1.1" xref="S4.SS2.SSS4.p11.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p11.2.m2.1b"><ci id="S4.SS2.SSS4.p11.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p11.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p11.2.m2.1c">k</annotation></semantics></math> attention keys can well approximate the original full attention. Therefore, a number of studies propose to store the past keys in external memory and utilize a <math id="S4.SS2.SSS4.p11.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS4.p11.3.m3.1a"><mi id="S4.SS2.SSS4.p11.3.m3.1.1" xref="S4.SS2.SSS4.p11.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p11.3.m3.1b"><ci id="S4.SS2.SSS4.p11.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p11.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p11.3.m3.1c">k</annotation></semantics></math>-NN search method to retrieve the <math id="S4.SS2.SSS4.p11.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS4.p11.4.m4.1a"><mi id="S4.SS2.SSS4.p11.4.m4.1.1" xref="S4.SS2.SSS4.p11.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p11.4.m4.1b"><ci id="S4.SS2.SSS4.p11.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p11.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p11.4.m4.1c">k</annotation></semantics></math> most relevant tokens for generation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref">302</a>, <a href="#bib.bib301" title="" class="ltx_ref">301</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite>. For a decoder model,
it typically employs one certain layer to access these top-<math id="S4.SS2.SSS4.p11.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS4.p11.5.m5.1a"><mi id="S4.SS2.SSS4.p11.5.m5.1.1" xref="S4.SS2.SSS4.p11.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p11.5.m5.1b"><ci id="S4.SS2.SSS4.p11.5.m5.1.1.cmml" xref="S4.SS2.SSS4.p11.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p11.5.m5.1c">k</annotation></semantics></math> external tokens, while still adopts the normal context window in the rest layers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref">302</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS4.p12" class="ltx_para">
<p id="S4.SS2.SSS4.p12.1" class="ltx_p">In addition to the studies based on vanilla Transformer, there are a surge of Transformer variants with efficient attentions and other efficient architectures, aiming to alleviate high computational cost for modeling long texts. These studies have been extensively discussed in Section&nbsp;<a href="#S4.SS2.SSS1" title="4.2.1 Typical Architectures ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a> and Section&nbsp;<a href="#S4.SS2.SSS2" title="4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>.
Furthermore, context compression and prompting techniques (<em id="S4.SS2.SSS4.p12.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> iterative reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib303" title="" class="ltx_ref">303</a>]</cite>) have also been proven to be a viable strategy for handling long text tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib304" title="" class="ltx_ref">304</a>, <a href="#bib.bib305" title="" class="ltx_ref">305</a>, <a href="#bib.bib303" title="" class="ltx_ref">303</a>, <a href="#bib.bib306" title="" class="ltx_ref">306</a>]</cite>, without the need of model adaption.</p>
</div>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Decoding Strategy</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">After the LLMs have been pre-trained, it is essential to employ a specific decoding strategy to generate the appropriate output from the LLMs.</p>
</div>
<div id="S4.SS2.SSS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS5.p2.4" class="ltx_p"><span id="S4.SS2.SSS5.p2.4.1" class="ltx_text ltx_font_bold">Background.</span> We start the discussion with the prevalent decoder-only architecture, and introduce the auto-regressive decoding mechanism. Since such LLMs are pre-trained based on the language modeling task (Equation&nbsp;<a href="#S4.E6" title="In 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), a basic decoding method is <em id="S4.SS2.SSS5.p2.4.2" class="ltx_emph ltx_font_italic">greedy search</em> that predicts the most likely token at each step based on the previously generated tokens, formally modeled as:</p>
<table id="S4.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E8.m1.1" class="ltx_Math" alttext="{x_{i}=\underset{x}{\arg\max}P(x|\mathbf{x}_{<i}),}" display="block"><semantics id="S4.E8.m1.1a"><mrow id="S4.E8.m1.1.1.1" xref="S4.E8.m1.1.1.1.1.cmml"><mrow id="S4.E8.m1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.cmml"><msub id="S4.E8.m1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.3.cmml"><mi id="S4.E8.m1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.3.2.cmml">x</mi><mi id="S4.E8.m1.1.1.1.1.3.3" xref="S4.E8.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S4.E8.m1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E8.m1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.cmml"><munder accentunder="true" id="S4.E8.m1.1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.1.3.cmml"><mrow id="S4.E8.m1.1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.1.3.2.cmml"><mi id="S4.E8.m1.1.1.1.1.1.3.2.1" xref="S4.E8.m1.1.1.1.1.1.3.2.1.cmml">arg</mi><mo lspace="0.167em" id="S4.E8.m1.1.1.1.1.1.3.2a" xref="S4.E8.m1.1.1.1.1.1.3.2.cmml">⁡</mo><mi id="S4.E8.m1.1.1.1.1.1.3.2.2" xref="S4.E8.m1.1.1.1.1.1.3.2.2.cmml">max</mi></mrow><mo id="S4.E8.m1.1.1.1.1.1.3.1" xref="S4.E8.m1.1.1.1.1.1.3.1.cmml">𝑥</mo></munder><mo lspace="0.167em" rspace="0em" id="S4.E8.m1.1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S4.E8.m1.1.1.1.1.1.4" xref="S4.E8.m1.1.1.1.1.1.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.1.1.1.1.1.2a" xref="S4.E8.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E8.m1.1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E8.m1.1.1.1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E8.m1.1.1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S4.E8.m1.1.1.1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S4.E8.m1.1.1.1.1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.2.cmml">𝐱</mi><mrow id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E8.m1.1.1.1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E8.m1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.1b"><apply id="S4.E8.m1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1"><eq id="S4.E8.m1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.2"></eq><apply id="S4.E8.m1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.3.2">𝑥</ci><ci id="S4.E8.m1.1.1.1.1.3.3.cmml" xref="S4.E8.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S4.E8.m1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1"><times id="S4.E8.m1.1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.1.2"></times><apply id="S4.E8.m1.1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.1.3"><ci id="S4.E8.m1.1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.3.1">𝑥</ci><apply id="S4.E8.m1.1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.3.2"><arg id="S4.E8.m1.1.1.1.1.1.3.2.1.cmml" xref="S4.E8.m1.1.1.1.1.1.3.2.1"></arg><max id="S4.E8.m1.1.1.1.1.1.3.2.2.cmml" xref="S4.E8.m1.1.1.1.1.1.3.2.2"></max></apply></apply><ci id="S4.E8.m1.1.1.1.1.1.4.cmml" xref="S4.E8.m1.1.1.1.1.1.4">𝑃</ci><apply id="S4.E8.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E8.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S4.E8.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.2">𝐱</ci><apply id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3"><lt id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.1c">{x_{i}=\underset{x}{\arg\max}P(x|\mathbf{x}_{&lt;i}),}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS5.p2.3" class="ltx_p">where <math id="S4.SS2.SSS5.p2.1.m1.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS2.SSS5.p2.1.m1.1a"><msub id="S4.SS2.SSS5.p2.1.m1.1.1" xref="S4.SS2.SSS5.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS5.p2.1.m1.1.1.2" xref="S4.SS2.SSS5.p2.1.m1.1.1.2.cmml">x</mi><mi id="S4.SS2.SSS5.p2.1.m1.1.1.3" xref="S4.SS2.SSS5.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.1.m1.1b"><apply id="S4.SS2.SSS5.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS5.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS5.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS5.p2.1.m1.1.1.2">𝑥</ci><ci id="S4.SS2.SSS5.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS5.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.1.m1.1c">x_{i}</annotation></semantics></math> is the token with the highest probability at <math id="S4.SS2.SSS5.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS5.p2.2.m2.1a"><mi id="S4.SS2.SSS5.p2.2.m2.1.1" xref="S4.SS2.SSS5.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.2.m2.1b"><ci id="S4.SS2.SSS5.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.2.m2.1c">i</annotation></semantics></math>-th step of generation conditioned on the context <math id="S4.SS2.SSS5.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{x}_{<i}" display="inline"><semantics id="S4.SS2.SSS5.p2.3.m3.1a"><msub id="S4.SS2.SSS5.p2.3.m3.1.1" xref="S4.SS2.SSS5.p2.3.m3.1.1.cmml"><mi id="S4.SS2.SSS5.p2.3.m3.1.1.2" xref="S4.SS2.SSS5.p2.3.m3.1.1.2.cmml">𝐱</mi><mrow id="S4.SS2.SSS5.p2.3.m3.1.1.3" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS5.p2.3.m3.1.1.3.2" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.2.cmml"></mi><mo id="S4.SS2.SSS5.p2.3.m3.1.1.3.1" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.1.cmml">&lt;</mo><mi id="S4.SS2.SSS5.p2.3.m3.1.1.3.3" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p2.3.m3.1b"><apply id="S4.SS2.SSS5.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS5.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1.2">𝐱</ci><apply id="S4.SS2.SSS5.p2.3.m3.1.1.3.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1.3"><lt id="S4.SS2.SSS5.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.1"></lt><csymbol cd="latexml" id="S4.SS2.SSS5.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.2">absent</csymbol><ci id="S4.SS2.SSS5.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS5.p2.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p2.3.m3.1c">\mathbf{x}_{&lt;i}</annotation></semantics></math>. For instance in Figure&nbsp;<a href="#S4.F10" title="Figure 10 ‣ 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, when predicting the next token of the sentence <em id="S4.SS2.SSS5.p2.3.1" class="ltx_emph ltx_font_italic">“I am sleepy. I start a pot of”</em>, greedy search selects the token “coffee” which has the highest probability at the current step. Greedy search can achieve satisfactory results in text generation tasks (<em id="S4.SS2.SSS5.p2.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> machine translation and text summarization), in which the output is highly dependent on the input&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib307" title="" class="ltx_ref">307</a>]</cite>. However, in terms of open-ended generation tasks (<em id="S4.SS2.SSS5.p2.3.3" class="ltx_emph ltx_font_italic">e.g.,</em> story generation and dialog), greedy search sometimes tends to generate awkward and repetitive sentences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib308" title="" class="ltx_ref">308</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS5.p3" class="ltx_para">
<p id="S4.SS2.SSS5.p3.1" class="ltx_p">As another alternative decoding strategy, sampling-based methods are proposed to randomly select the next token based on the probability distribution to enhance the randomness and diversity during generation:</p>
<table id="S4.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E9.m1.1" class="ltx_Math" alttext="x_{i}\sim P(x|\mathbf{x}_{<i})." display="block"><semantics id="S4.E9.m1.1a"><mrow id="S4.E9.m1.1.1.1" xref="S4.E9.m1.1.1.1.1.cmml"><mrow id="S4.E9.m1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.cmml"><msub id="S4.E9.m1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.3.cmml"><mi id="S4.E9.m1.1.1.1.1.3.2" xref="S4.E9.m1.1.1.1.1.3.2.cmml">x</mi><mi id="S4.E9.m1.1.1.1.1.3.3" xref="S4.E9.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S4.E9.m1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.2.cmml">∼</mo><mrow id="S4.E9.m1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.cmml"><mi id="S4.E9.m1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E9.m1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S4.E9.m1.1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S4.E9.m1.1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.2.cmml">𝐱</mi><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E9.m1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E9.m1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.1b"><apply id="S4.E9.m1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1"><csymbol cd="latexml" id="S4.E9.m1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.2">similar-to</csymbol><apply id="S4.E9.m1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E9.m1.1.1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.1.1.3.2">𝑥</ci><ci id="S4.E9.m1.1.1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S4.E9.m1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1"><times id="S4.E9.m1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.2"></times><ci id="S4.E9.m1.1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.1.3">𝑃</ci><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E9.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.2">𝐱</ci><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3"><lt id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.1c">x_{i}\sim P(x|\mathbf{x}_{&lt;i}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS5.p3.2" class="ltx_p">For the example in Figure&nbsp;<a href="#S4.F10" title="Figure 10 ‣ 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, sampling-based methods will sample the word “coffee” with higher probability while also retaining the possibilities of selecting the rest words, “water”, “tea”, “rice”, <em id="S4.SS2.SSS5.p3.2.1" class="ltx_emph ltx_font_italic">etc</em>.</p>
</div>
<div id="S4.SS2.SSS5.p4" class="ltx_para">
<p id="S4.SS2.SSS5.p4.1" class="ltx_p">Not limited to the decoder-only architecture, these two decoding methods can be generally applied to encoder-decoder models and prefix decoder models in a similar way.</p>
</div>
<div id="S4.SS2.SSS5.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS5.p5.1" class="ltx_p"><span id="S4.SS2.SSS5.p5.1.1" class="ltx_text ltx_font_bold">Improvement for Greedy Search.</span>
Selecting the token with the highest probability at each step may result in overlooking a sentence with a higher overall probability but a lower local estimation. Next, we introduce several improvement strategies to alleviate this issue.</p>
</div>
<div id="S4.SS2.SSS5.p6" class="ltx_para">
<p id="S4.SS2.SSS5.p6.2" class="ltx_p"><math id="S4.SS2.SSS5.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p6.1.m1.1a"><mo id="S4.SS2.SSS5.p6.1.m1.1.1" xref="S4.SS2.SSS5.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p6.1.m1.1b"><ci id="S4.SS2.SSS5.p6.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p6.2.1" class="ltx_emph ltx_font_italic">Beam search.</em>
Beam search&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib309" title="" class="ltx_ref">309</a>]</cite> retains the sentences with the <math id="S4.SS2.SSS5.p6.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.SSS5.p6.2.m2.1a"><mi id="S4.SS2.SSS5.p6.2.m2.1.1" xref="S4.SS2.SSS5.p6.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p6.2.m2.1b"><ci id="S4.SS2.SSS5.p6.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p6.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p6.2.m2.1c">n</annotation></semantics></math> (beam size) highest probabilities at each step during the decoding process, and finally selects the generated response with the top probability. Typically, the beam size is configured within the range of 3 to 6. However, opting for a larger beam size might result in a decline in performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib310" title="" class="ltx_ref">310</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS5.p7" class="ltx_para">
<p id="S4.SS2.SSS5.p7.2" class="ltx_p"><math id="S4.SS2.SSS5.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p7.1.m1.1a"><mo id="S4.SS2.SSS5.p7.1.m1.1.1" xref="S4.SS2.SSS5.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p7.1.m1.1b"><ci id="S4.SS2.SSS5.p7.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p7.2.1" class="ltx_emph ltx_font_italic">Length penalty.</em>
Since beam search favours shorter sentences, imposing length penalty (<em id="S4.SS2.SSS5.p7.2.2" class="ltx_emph ltx_font_italic">a.k.a.,</em> length normalization) is a commonly used technique&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib311" title="" class="ltx_ref">311</a>]</cite> to overcome this issue, which normalizes the sentence probability according to the sentence length (divided by an exponential power <math id="S4.SS2.SSS5.p7.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.SSS5.p7.2.m2.1a"><mi id="S4.SS2.SSS5.p7.2.m2.1.1" xref="S4.SS2.SSS5.p7.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p7.2.m2.1b"><ci id="S4.SS2.SSS5.p7.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p7.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p7.2.m2.1c">\alpha</annotation></semantics></math> of the length).</p>
</div>
<div id="S4.SS2.SSS5.p8" class="ltx_para">
<p id="S4.SS2.SSS5.p8.1" class="ltx_p">Besides, some researchers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib312" title="" class="ltx_ref">312</a>]</cite> propose to penalize the generation of previously generated tokens or <math id="S4.SS2.SSS5.p8.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.SSS5.p8.1.m1.1a"><mi id="S4.SS2.SSS5.p8.1.m1.1.1" xref="S4.SS2.SSS5.p8.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p8.1.m1.1b"><ci id="S4.SS2.SSS5.p8.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p8.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p8.1.m1.1c">n</annotation></semantics></math>-grams to alleviate the issue of repetitive generation. In addition, diverse beam search&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib313" title="" class="ltx_ref">313</a>]</cite> can be leveraged to produce a set of diverse outputs based on the same input.</p>
</div>
<div id="S4.SS2.SSS5.p9" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS5.p9.1" class="ltx_p"><span id="S4.SS2.SSS5.p9.1.1" class="ltx_text ltx_font_bold">Improvement for Random Sampling.</span>
Sampling-based methods sample the token over the whole vocabulary, which may select wrong or irrelevant tokens (<em id="S4.SS2.SSS5.p9.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> “happy” and “Boh” in Figure&nbsp;<a href="#S4.F10" title="Figure 10 ‣ 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>) based on the context. To improve the generation quality, several strategies have been proposed for mitigating or preventing the selection of words with exceedingly low probabilities.</p>
</div>
<div id="S4.SS2.SSS5.p10" class="ltx_para">
<p id="S4.SS2.SSS5.p10.2" class="ltx_p"><math id="S4.SS2.SSS5.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p10.1.m1.1a"><mo id="S4.SS2.SSS5.p10.1.m1.1.1" xref="S4.SS2.SSS5.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.1.m1.1b"><ci id="S4.SS2.SSS5.p10.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p10.2.1" class="ltx_emph ltx_font_italic">Temperature sampling.</em>
To modulate the randomness of sampling, a practical method is to adjust the temperature coefficient of the softmax function for computing the probability of the <math id="S4.SS2.SSS5.p10.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS2.SSS5.p10.2.m2.1a"><mi id="S4.SS2.SSS5.p10.2.m2.1.1" xref="S4.SS2.SSS5.p10.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.2.m2.1b"><ci id="S4.SS2.SSS5.p10.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p10.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.2.m2.1c">j</annotation></semantics></math>-th token over the vocabulary:</p>
<table id="S4.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E10.m1.5" class="ltx_Math" alttext="P(x_{j}|\mathbf{x}_{<i})=\frac{\exp{(l_{j}/t)}}{\sum_{j^{\prime}}\exp{(l_{j^{\prime}}/t)}}," display="block"><semantics id="S4.E10.m1.5a"><mrow id="S4.E10.m1.5.5.1" xref="S4.E10.m1.5.5.1.1.cmml"><mrow id="S4.E10.m1.5.5.1.1" xref="S4.E10.m1.5.5.1.1.cmml"><mrow id="S4.E10.m1.5.5.1.1.1" xref="S4.E10.m1.5.5.1.1.1.cmml"><mi id="S4.E10.m1.5.5.1.1.1.3" xref="S4.E10.m1.5.5.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.5.5.1.1.1.2" xref="S4.E10.m1.5.5.1.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.5.5.1.1.1.1.1" xref="S4.E10.m1.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E10.m1.5.5.1.1.1.1.1.2" xref="S4.E10.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E10.m1.5.5.1.1.1.1.1.1" xref="S4.E10.m1.5.5.1.1.1.1.1.1.cmml"><msub id="S4.E10.m1.5.5.1.1.1.1.1.1.2" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2.cmml"><mi id="S4.E10.m1.5.5.1.1.1.1.1.1.2.2" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S4.E10.m1.5.5.1.1.1.1.1.1.2.3" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo fence="false" id="S4.E10.m1.5.5.1.1.1.1.1.1.1" xref="S4.E10.m1.5.5.1.1.1.1.1.1.1.cmml">|</mo><msub id="S4.E10.m1.5.5.1.1.1.1.1.1.3" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.cmml"><mi id="S4.E10.m1.5.5.1.1.1.1.1.1.3.2" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.2.cmml">𝐱</mi><mrow id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.2" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.1" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.3" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E10.m1.5.5.1.1.1.1.1.3" xref="S4.E10.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E10.m1.5.5.1.1.2" xref="S4.E10.m1.5.5.1.1.2.cmml">=</mo><mfrac id="S4.E10.m1.4.4" xref="S4.E10.m1.4.4.cmml"><mrow id="S4.E10.m1.2.2.2.2" xref="S4.E10.m1.2.2.2.3.cmml"><mi id="S4.E10.m1.1.1.1.1" xref="S4.E10.m1.1.1.1.1.cmml">exp</mi><mo id="S4.E10.m1.2.2.2.2a" xref="S4.E10.m1.2.2.2.3.cmml">⁡</mo><mrow id="S4.E10.m1.2.2.2.2.1" xref="S4.E10.m1.2.2.2.3.cmml"><mo stretchy="false" id="S4.E10.m1.2.2.2.2.1.2" xref="S4.E10.m1.2.2.2.3.cmml">(</mo><mrow id="S4.E10.m1.2.2.2.2.1.1" xref="S4.E10.m1.2.2.2.2.1.1.cmml"><msub id="S4.E10.m1.2.2.2.2.1.1.2" xref="S4.E10.m1.2.2.2.2.1.1.2.cmml"><mi id="S4.E10.m1.2.2.2.2.1.1.2.2" xref="S4.E10.m1.2.2.2.2.1.1.2.2.cmml">l</mi><mi id="S4.E10.m1.2.2.2.2.1.1.2.3" xref="S4.E10.m1.2.2.2.2.1.1.2.3.cmml">j</mi></msub><mo id="S4.E10.m1.2.2.2.2.1.1.1" xref="S4.E10.m1.2.2.2.2.1.1.1.cmml">/</mo><mi id="S4.E10.m1.2.2.2.2.1.1.3" xref="S4.E10.m1.2.2.2.2.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S4.E10.m1.2.2.2.2.1.3" xref="S4.E10.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S4.E10.m1.4.4.4" xref="S4.E10.m1.4.4.4.cmml"><msub id="S4.E10.m1.4.4.4.3" xref="S4.E10.m1.4.4.4.3.cmml"><mo id="S4.E10.m1.4.4.4.3.2" xref="S4.E10.m1.4.4.4.3.2.cmml">∑</mo><msup id="S4.E10.m1.4.4.4.3.3" xref="S4.E10.m1.4.4.4.3.3.cmml"><mi id="S4.E10.m1.4.4.4.3.3.2" xref="S4.E10.m1.4.4.4.3.3.2.cmml">j</mi><mo id="S4.E10.m1.4.4.4.3.3.3" xref="S4.E10.m1.4.4.4.3.3.3.cmml">′</mo></msup></msub><mrow id="S4.E10.m1.4.4.4.2.1" xref="S4.E10.m1.4.4.4.2.2.cmml"><mi id="S4.E10.m1.3.3.3.1" xref="S4.E10.m1.3.3.3.1.cmml">exp</mi><mo id="S4.E10.m1.4.4.4.2.1a" xref="S4.E10.m1.4.4.4.2.2.cmml">⁡</mo><mrow id="S4.E10.m1.4.4.4.2.1.1" xref="S4.E10.m1.4.4.4.2.2.cmml"><mo stretchy="false" id="S4.E10.m1.4.4.4.2.1.1.2" xref="S4.E10.m1.4.4.4.2.2.cmml">(</mo><mrow id="S4.E10.m1.4.4.4.2.1.1.1" xref="S4.E10.m1.4.4.4.2.1.1.1.cmml"><msub id="S4.E10.m1.4.4.4.2.1.1.1.2" xref="S4.E10.m1.4.4.4.2.1.1.1.2.cmml"><mi id="S4.E10.m1.4.4.4.2.1.1.1.2.2" xref="S4.E10.m1.4.4.4.2.1.1.1.2.2.cmml">l</mi><msup id="S4.E10.m1.4.4.4.2.1.1.1.2.3" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3.cmml"><mi id="S4.E10.m1.4.4.4.2.1.1.1.2.3.2" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3.2.cmml">j</mi><mo id="S4.E10.m1.4.4.4.2.1.1.1.2.3.3" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3.3.cmml">′</mo></msup></msub><mo id="S4.E10.m1.4.4.4.2.1.1.1.1" xref="S4.E10.m1.4.4.4.2.1.1.1.1.cmml">/</mo><mi id="S4.E10.m1.4.4.4.2.1.1.1.3" xref="S4.E10.m1.4.4.4.2.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S4.E10.m1.4.4.4.2.1.1.3" xref="S4.E10.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id="S4.E10.m1.5.5.1.2" xref="S4.E10.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E10.m1.5b"><apply id="S4.E10.m1.5.5.1.1.cmml" xref="S4.E10.m1.5.5.1"><eq id="S4.E10.m1.5.5.1.1.2.cmml" xref="S4.E10.m1.5.5.1.1.2"></eq><apply id="S4.E10.m1.5.5.1.1.1.cmml" xref="S4.E10.m1.5.5.1.1.1"><times id="S4.E10.m1.5.5.1.1.1.2.cmml" xref="S4.E10.m1.5.5.1.1.1.2"></times><ci id="S4.E10.m1.5.5.1.1.1.3.cmml" xref="S4.E10.m1.5.5.1.1.1.3">𝑃</ci><apply id="S4.E10.m1.5.5.1.1.1.1.1.1.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S4.E10.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.1">conditional</csymbol><apply id="S4.E10.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E10.m1.5.5.1.1.1.1.1.1.2.1.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E10.m1.5.5.1.1.1.1.1.1.2.2.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S4.E10.m1.5.5.1.1.1.1.1.1.2.3.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S4.E10.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E10.m1.5.5.1.1.1.1.1.1.3.1.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E10.m1.5.5.1.1.1.1.1.1.3.2.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.2">𝐱</ci><apply id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3"><lt id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E10.m1.5.5.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S4.E10.m1.4.4.cmml" xref="S4.E10.m1.4.4"><divide id="S4.E10.m1.4.4.5.cmml" xref="S4.E10.m1.4.4"></divide><apply id="S4.E10.m1.2.2.2.3.cmml" xref="S4.E10.m1.2.2.2.2"><exp id="S4.E10.m1.1.1.1.1.cmml" xref="S4.E10.m1.1.1.1.1"></exp><apply id="S4.E10.m1.2.2.2.2.1.1.cmml" xref="S4.E10.m1.2.2.2.2.1.1"><divide id="S4.E10.m1.2.2.2.2.1.1.1.cmml" xref="S4.E10.m1.2.2.2.2.1.1.1"></divide><apply id="S4.E10.m1.2.2.2.2.1.1.2.cmml" xref="S4.E10.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E10.m1.2.2.2.2.1.1.2.1.cmml" xref="S4.E10.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S4.E10.m1.2.2.2.2.1.1.2.2.cmml" xref="S4.E10.m1.2.2.2.2.1.1.2.2">𝑙</ci><ci id="S4.E10.m1.2.2.2.2.1.1.2.3.cmml" xref="S4.E10.m1.2.2.2.2.1.1.2.3">𝑗</ci></apply><ci id="S4.E10.m1.2.2.2.2.1.1.3.cmml" xref="S4.E10.m1.2.2.2.2.1.1.3">𝑡</ci></apply></apply><apply id="S4.E10.m1.4.4.4.cmml" xref="S4.E10.m1.4.4.4"><apply id="S4.E10.m1.4.4.4.3.cmml" xref="S4.E10.m1.4.4.4.3"><csymbol cd="ambiguous" id="S4.E10.m1.4.4.4.3.1.cmml" xref="S4.E10.m1.4.4.4.3">subscript</csymbol><sum id="S4.E10.m1.4.4.4.3.2.cmml" xref="S4.E10.m1.4.4.4.3.2"></sum><apply id="S4.E10.m1.4.4.4.3.3.cmml" xref="S4.E10.m1.4.4.4.3.3"><csymbol cd="ambiguous" id="S4.E10.m1.4.4.4.3.3.1.cmml" xref="S4.E10.m1.4.4.4.3.3">superscript</csymbol><ci id="S4.E10.m1.4.4.4.3.3.2.cmml" xref="S4.E10.m1.4.4.4.3.3.2">𝑗</ci><ci id="S4.E10.m1.4.4.4.3.3.3.cmml" xref="S4.E10.m1.4.4.4.3.3.3">′</ci></apply></apply><apply id="S4.E10.m1.4.4.4.2.2.cmml" xref="S4.E10.m1.4.4.4.2.1"><exp id="S4.E10.m1.3.3.3.1.cmml" xref="S4.E10.m1.3.3.3.1"></exp><apply id="S4.E10.m1.4.4.4.2.1.1.1.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1"><divide id="S4.E10.m1.4.4.4.2.1.1.1.1.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.1"></divide><apply id="S4.E10.m1.4.4.4.2.1.1.1.2.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E10.m1.4.4.4.2.1.1.1.2.1.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2">subscript</csymbol><ci id="S4.E10.m1.4.4.4.2.1.1.1.2.2.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2.2">𝑙</ci><apply id="S4.E10.m1.4.4.4.2.1.1.1.2.3.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E10.m1.4.4.4.2.1.1.1.2.3.1.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3">superscript</csymbol><ci id="S4.E10.m1.4.4.4.2.1.1.1.2.3.2.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3.2">𝑗</ci><ci id="S4.E10.m1.4.4.4.2.1.1.1.2.3.3.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.2.3.3">′</ci></apply></apply><ci id="S4.E10.m1.4.4.4.2.1.1.1.3.cmml" xref="S4.E10.m1.4.4.4.2.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E10.m1.5c">P(x_{j}|\mathbf{x}_{&lt;i})=\frac{\exp{(l_{j}/t)}}{\sum_{j^{\prime}}\exp{(l_{j^{\prime}}/t)}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS5.p10.8" class="ltx_p">where <math id="S4.SS2.SSS5.p10.3.m1.1" class="ltx_Math" alttext="l_{j^{\prime}}" display="inline"><semantics id="S4.SS2.SSS5.p10.3.m1.1a"><msub id="S4.SS2.SSS5.p10.3.m1.1.1" xref="S4.SS2.SSS5.p10.3.m1.1.1.cmml"><mi id="S4.SS2.SSS5.p10.3.m1.1.1.2" xref="S4.SS2.SSS5.p10.3.m1.1.1.2.cmml">l</mi><msup id="S4.SS2.SSS5.p10.3.m1.1.1.3" xref="S4.SS2.SSS5.p10.3.m1.1.1.3.cmml"><mi id="S4.SS2.SSS5.p10.3.m1.1.1.3.2" xref="S4.SS2.SSS5.p10.3.m1.1.1.3.2.cmml">j</mi><mo id="S4.SS2.SSS5.p10.3.m1.1.1.3.3" xref="S4.SS2.SSS5.p10.3.m1.1.1.3.3.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.3.m1.1b"><apply id="S4.SS2.SSS5.p10.3.m1.1.1.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p10.3.m1.1.1.1.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS5.p10.3.m1.1.1.2.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1.2">𝑙</ci><apply id="S4.SS2.SSS5.p10.3.m1.1.1.3.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS5.p10.3.m1.1.1.3.1.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.SSS5.p10.3.m1.1.1.3.2.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1.3.2">𝑗</ci><ci id="S4.SS2.SSS5.p10.3.m1.1.1.3.3.cmml" xref="S4.SS2.SSS5.p10.3.m1.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.3.m1.1c">l_{j^{\prime}}</annotation></semantics></math> is the logits of each word and <math id="S4.SS2.SSS5.p10.4.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS5.p10.4.m2.1a"><mi id="S4.SS2.SSS5.p10.4.m2.1.1" xref="S4.SS2.SSS5.p10.4.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.4.m2.1b"><ci id="S4.SS2.SSS5.p10.4.m2.1.1.cmml" xref="S4.SS2.SSS5.p10.4.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.4.m2.1c">t</annotation></semantics></math> is the temperature coefficient. Reducing the temperature <math id="S4.SS2.SSS5.p10.5.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS5.p10.5.m3.1a"><mi id="S4.SS2.SSS5.p10.5.m3.1.1" xref="S4.SS2.SSS5.p10.5.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.5.m3.1b"><ci id="S4.SS2.SSS5.p10.5.m3.1.1.cmml" xref="S4.SS2.SSS5.p10.5.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.5.m3.1c">t</annotation></semantics></math> increases the chance of selecting words with high probabilities while decreases the chances of selecting words with low probabilities.
When <math id="S4.SS2.SSS5.p10.6.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS5.p10.6.m4.1a"><mi id="S4.SS2.SSS5.p10.6.m4.1.1" xref="S4.SS2.SSS5.p10.6.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.6.m4.1b"><ci id="S4.SS2.SSS5.p10.6.m4.1.1.cmml" xref="S4.SS2.SSS5.p10.6.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.6.m4.1c">t</annotation></semantics></math> is set to 1, it becomes the default random sampling; when <math id="S4.SS2.SSS5.p10.7.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS5.p10.7.m5.1a"><mi id="S4.SS2.SSS5.p10.7.m5.1.1" xref="S4.SS2.SSS5.p10.7.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.7.m5.1b"><ci id="S4.SS2.SSS5.p10.7.m5.1.1.cmml" xref="S4.SS2.SSS5.p10.7.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.7.m5.1c">t</annotation></semantics></math> is approaching 0, it is equivalent to greedy search.
In addition, when <math id="S4.SS2.SSS5.p10.8.m6.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.SSS5.p10.8.m6.1a"><mi id="S4.SS2.SSS5.p10.8.m6.1.1" xref="S4.SS2.SSS5.p10.8.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p10.8.m6.1b"><ci id="S4.SS2.SSS5.p10.8.m6.1.1.cmml" xref="S4.SS2.SSS5.p10.8.m6.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p10.8.m6.1c">t</annotation></semantics></math> goes to infinity, it degenerates to uniform sampling.</p>
</div>
<div id="S4.SS2.SSS5.p11" class="ltx_para">
<p id="S4.SS2.SSS5.p11.5" class="ltx_p"><math id="S4.SS2.SSS5.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p11.1.m1.1a"><mo id="S4.SS2.SSS5.p11.1.m1.1.1" xref="S4.SS2.SSS5.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p11.1.m1.1b"><ci id="S4.SS2.SSS5.p11.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p11.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p11.2.1" class="ltx_emph ltx_font_italic">Top-<math id="S4.SS2.SSS5.p11.2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS5.p11.2.1.m1.1a"><mi id="S4.SS2.SSS5.p11.2.1.m1.1.1" xref="S4.SS2.SSS5.p11.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p11.2.1.m1.1b"><ci id="S4.SS2.SSS5.p11.2.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p11.2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p11.2.1.m1.1c">k</annotation></semantics></math> sampling.</em>
Different from temperature sampling, top-<math id="S4.SS2.SSS5.p11.3.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS5.p11.3.m2.1a"><mi id="S4.SS2.SSS5.p11.3.m2.1.1" xref="S4.SS2.SSS5.p11.3.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p11.3.m2.1b"><ci id="S4.SS2.SSS5.p11.3.m2.1.1.cmml" xref="S4.SS2.SSS5.p11.3.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p11.3.m2.1c">k</annotation></semantics></math> sampling directly truncates the tokens with lower probability and only samples from the tokens with the top <math id="S4.SS2.SSS5.p11.4.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS5.p11.4.m3.1a"><mi id="S4.SS2.SSS5.p11.4.m3.1.1" xref="S4.SS2.SSS5.p11.4.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p11.4.m3.1b"><ci id="S4.SS2.SSS5.p11.4.m3.1.1.cmml" xref="S4.SS2.SSS5.p11.4.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p11.4.m3.1c">k</annotation></semantics></math> highest probabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib314" title="" class="ltx_ref">314</a>]</cite>. For example in Figure&nbsp;<a href="#S4.F10" title="Figure 10 ‣ 4.2.3 Pre-training Tasks ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, top-<math id="S4.SS2.SSS5.p11.5.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.SSS5.p11.5.m4.1a"><mn id="S4.SS2.SSS5.p11.5.m4.1.1" xref="S4.SS2.SSS5.p11.5.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p11.5.m4.1b"><cn type="integer" id="S4.SS2.SSS5.p11.5.m4.1.1.cmml" xref="S4.SS2.SSS5.p11.5.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p11.5.m4.1c">5</annotation></semantics></math> sampling will sample from the words “coffee”, “water”, “tea”, “rice”, and “chai” from their re-scaled probabilities.</p>
</div>
<div id="S4.SS2.SSS5.p12" class="ltx_para">
<p id="S4.SS2.SSS5.p12.7" class="ltx_p"><math id="S4.SS2.SSS5.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p12.1.m1.1a"><mo id="S4.SS2.SSS5.p12.1.m1.1.1" xref="S4.SS2.SSS5.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.1.m1.1b"><ci id="S4.SS2.SSS5.p12.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p12.2.1" class="ltx_emph ltx_font_italic">Top-<math id="S4.SS2.SSS5.p12.2.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS5.p12.2.1.m1.1a"><mi id="S4.SS2.SSS5.p12.2.1.m1.1.1" xref="S4.SS2.SSS5.p12.2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.2.1.m1.1b"><ci id="S4.SS2.SSS5.p12.2.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p12.2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.2.1.m1.1c">p</annotation></semantics></math> sampling.</em>
Since top-<math id="S4.SS2.SSS5.p12.3.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS5.p12.3.m2.1a"><mi id="S4.SS2.SSS5.p12.3.m2.1.1" xref="S4.SS2.SSS5.p12.3.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.3.m2.1b"><ci id="S4.SS2.SSS5.p12.3.m2.1.1.cmml" xref="S4.SS2.SSS5.p12.3.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.3.m2.1c">k</annotation></semantics></math> sampling does not consider the overall possibility distribution, a constant value of <math id="S4.SS2.SSS5.p12.4.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS5.p12.4.m3.1a"><mi id="S4.SS2.SSS5.p12.4.m3.1.1" xref="S4.SS2.SSS5.p12.4.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.4.m3.1b"><ci id="S4.SS2.SSS5.p12.4.m3.1.1.cmml" xref="S4.SS2.SSS5.p12.4.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.4.m3.1c">k</annotation></semantics></math> may be not be suitable for different contexts. Therefore, top-<math id="S4.SS2.SSS5.p12.5.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS5.p12.5.m4.1a"><mi id="S4.SS2.SSS5.p12.5.m4.1.1" xref="S4.SS2.SSS5.p12.5.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.5.m4.1b"><ci id="S4.SS2.SSS5.p12.5.m4.1.1.cmml" xref="S4.SS2.SSS5.p12.5.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.5.m4.1c">p</annotation></semantics></math> sampling (<em id="S4.SS2.SSS5.p12.7.2" class="ltx_emph ltx_font_italic">a.k.a.,</em> nucleus sampling) is proposed by sampling from the smallest set having a cumulative probability above (or equal to) <math id="S4.SS2.SSS5.p12.6.m5.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS5.p12.6.m5.1a"><mi id="S4.SS2.SSS5.p12.6.m5.1.1" xref="S4.SS2.SSS5.p12.6.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.6.m5.1b"><ci id="S4.SS2.SSS5.p12.6.m5.1.1.cmml" xref="S4.SS2.SSS5.p12.6.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.6.m5.1c">p</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib308" title="" class="ltx_ref">308</a>]</cite>. In practice, the smallest set can be constructed by gradually adding tokens from the vocabulary sorted in descending order of generative probability, until their cumulative value exceeds <math id="S4.SS2.SSS5.p12.7.m6.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS5.p12.7.m6.1a"><mi id="S4.SS2.SSS5.p12.7.m6.1.1" xref="S4.SS2.SSS5.p12.7.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p12.7.m6.1b"><ci id="S4.SS2.SSS5.p12.7.m6.1.1.cmml" xref="S4.SS2.SSS5.p12.7.m6.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p12.7.m6.1c">p</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.SSS5.p13" class="ltx_para">
<p id="S4.SS2.SSS5.p13.2" class="ltx_p">Recently, researchers have also explored other sampling strategies for LLMs. For instance, <em id="S4.SS2.SSS5.p13.1.1" class="ltx_emph ltx_font_italic"><math id="S4.SS2.SSS5.p13.1.1.m1.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S4.SS2.SSS5.p13.1.1.m1.1a"><mi id="S4.SS2.SSS5.p13.1.1.m1.1.1" xref="S4.SS2.SSS5.p13.1.1.m1.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p13.1.1.m1.1b"><ci id="S4.SS2.SSS5.p13.1.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p13.1.1.m1.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p13.1.1.m1.1c">\eta</annotation></semantics></math>-sampling</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib315" title="" class="ltx_ref">315</a>]</cite> further improves top-<math id="S4.SS2.SSS5.p13.2.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS5.p13.2.m1.1a"><mi id="S4.SS2.SSS5.p13.2.m1.1.1" xref="S4.SS2.SSS5.p13.2.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p13.2.m1.1b"><ci id="S4.SS2.SSS5.p13.2.m1.1.1.cmml" xref="S4.SS2.SSS5.p13.2.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p13.2.m1.1c">p</annotation></semantics></math> sampling by introducing a dynamic threshold based on the probability distribution. Furthermore, <em id="S4.SS2.SSS5.p13.2.2" class="ltx_emph ltx_font_italic">contrastive search</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib316" title="" class="ltx_ref">316</a>]</cite> and <em id="S4.SS2.SSS5.p13.2.3" class="ltx_emph ltx_font_italic">typical sampling</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib317" title="" class="ltx_ref">317</a>]</cite> can be utilized to improve the generation coherence during decoding.
Since it has been found that large models tend to assign higher probability to important tokens compared to small models, <em id="S4.SS2.SSS5.p13.2.4" class="ltx_emph ltx_font_italic">contrastive decoding</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib318" title="" class="ltx_ref">318</a>]</cite> utilizes a larger LM (<em id="S4.SS2.SSS5.p13.2.5" class="ltx_emph ltx_font_italic">e.g.,</em> OPT-13B) and a smaller LM (<em id="S4.SS2.SSS5.p13.2.6" class="ltx_emph ltx_font_italic">e.g.,</em> OPT-125M) to measure their log-likelihood differences. Subsequently, tokens are sampled based on the delta value of the probability distribution, thereby amplifying the impact of important tokens.
Based on this contrastive idea, DoLa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib319" title="" class="ltx_ref">319</a>]</cite> further extends this approach to contrasting the logits across different layers of a single LLM, as higher layers tend to assign more weight to important tokens.</p>
</div>
<div id="S4.SS2.SSS5.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S4.SS2.SSS5.1.p1.pic1" class="ltx_picture" height="315.68" overflow="visible" version="1.1" width="288"><g transform="translate(0,315.68) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 309.77 C 0 313.03 2.64 315.68 5.91 315.68 L 282.09 315.68 C 285.35 315.68 288 313.03 288 309.77 L 288 5.91 C 288 2.64 285.35 0 282.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 291.57 L 286.03 291.57 L 286.03 5.91 C 286.03 3.73 284.27 1.97 282.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 297.47)"><foreignObject width="244.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S4.SS2.SSS5.1.p1.pic1.15.15.15.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:176.8pt;">
<span id="S4.SS2.SSS5.1.p1.pic1.15.15.15.1.1.1" class="ltx_p">Memory Wall</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="244.69" height="265.98" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:176.8pt;">
<span id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.15" class="ltx_p">When generating a new token, the most time-consuming steps revolve around data transfer and weight computation. A main issue is the significant amount of time overwhelmed by data transfer, often referred to as the <em id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.15.1" class="ltx_emph ltx_font_italic">memory wall</em> issue.</span>
<span id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8" class="ltx_p">To address this issue, researchers formally quantify data transfer from GPU memory to GPU caches using the number of bytes in I/O, and they assess weight computation by measuring the number of FLOPs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib320" title="" class="ltx_ref">320</a>]</cite>. Specifically, let <math id="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">b</annotation></semantics></math>, <math id="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mi id="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">s</annotation></semantics></math>, <math id="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1a"><mi id="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1" xref="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1b"><ci id="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1c">n</annotation></semantics></math>, <math id="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1a"><mi id="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1" xref="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1b"><ci id="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1c">d</annotation></semantics></math>, and <math id="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1a"><mi id="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1" xref="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1b"><ci id="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1c">h</annotation></semantics></math> denote the batch size, sequence length, number of attention heads, hidden size of each head, and overall hidden size (<math id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1" class="ltx_Math" alttext="h=n\cdot d" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1a"><mrow id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.cmml"><mi id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.2" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.2.cmml">h</mi><mo id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.cmml"><mi id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.2" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.1" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.1.cmml">⋅</mo><mi id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.3" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.3.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1b"><apply id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1"><eq id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.1"></eq><ci id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.2">ℎ</ci><apply id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3"><ci id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.1">⋅</ci><ci id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.2">𝑛</ci><ci id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1c">h=n\cdot d</annotation></semantics></math>), respectively. During the layer-wise multi-head self-attention calculation in causal decoder, the I/O bytes and FLOPs at each decoding step can be expressed as <math id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1" class="ltx_Math" alttext="8bsn+4bsnd+4bnd" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1a"><mrow id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.cmml"><mrow id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.cmml"><mn id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.2" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.3" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1a" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.4" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1b" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.5" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.5.cmml">n</mi></mrow><mo id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.1.cmml">+</mo><mrow id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.cmml"><mn id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.2" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.3" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1a" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.4" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1b" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.5" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1c" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.6" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.6.cmml">d</mi></mrow><mo id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.1a" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.1.cmml">+</mo><mrow id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.cmml"><mn id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.2" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.3" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1a" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.4" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1b" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.5" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.5.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1b"><apply id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1"><plus id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.1"></plus><apply id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2"><times id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.1"></times><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.2">8</cn><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.3">𝑏</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.4.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.4">𝑠</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.5.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.2.5">𝑛</ci></apply><apply id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3"><times id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.1"></times><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.2">4</cn><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.3">𝑏</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.4.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.4">𝑠</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.5.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.5">𝑛</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.6.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.3.6">𝑑</ci></apply><apply id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4"><times id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.1"></times><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.2">4</cn><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.3">𝑏</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.4.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.4">𝑛</ci><ci id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.5.cmml" xref="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.4.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1c">8bsn+4bsnd+4bnd</annotation></semantics></math> and <math id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1" class="ltx_Math" alttext="8bsnd" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1a"><mrow id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.cmml"><mn id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.2" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.3" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1a" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.4" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1b" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.5" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1c" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.6" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.6.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1b"><apply id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1"><times id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.1"></times><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.2">8</cn><ci id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.3">𝑏</ci><ci id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.4.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.4">𝑠</ci><ci id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.5.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.5">𝑛</ci><ci id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.6.cmml" xref="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.6">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1c">8bsnd</annotation></semantics></math>, respectively&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib320" title="" class="ltx_ref">320</a>]</cite>.</span>
<span id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.16" class="ltx_p"><em id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.16.1" class="ltx_emph ltx_font_italic">Arithmetic intensity</em> is further defined as the ratio of FLOPs to I/O bytes:</span>
<span id="S4.E11" class="ltx_equation ltx_eqn_table">

<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math id="S4.E11.m1.1" class="ltx_Math" alttext="\text{intensity}=\frac{\text{FLOPs}}{\text{I/O bytes}}=\frac{2}{1+\frac{2}{d}+\frac{1}{s}}" display="block"><semantics id="S4.E11.m1.1a"><mrow id="S4.E11.m1.1.1" xref="S4.E11.m1.1.1.cmml"><mtext id="S4.E11.m1.1.1.2" xref="S4.E11.m1.1.1.2a.cmml">intensity</mtext><mo id="S4.E11.m1.1.1.3" xref="S4.E11.m1.1.1.3.cmml">=</mo><mfrac id="S4.E11.m1.1.1.4" xref="S4.E11.m1.1.1.4.cmml"><mtext id="S4.E11.m1.1.1.4.2" xref="S4.E11.m1.1.1.4.2a.cmml">FLOPs</mtext><mtext id="S4.E11.m1.1.1.4.3" xref="S4.E11.m1.1.1.4.3a.cmml">I/O bytes</mtext></mfrac><mo id="S4.E11.m1.1.1.5" xref="S4.E11.m1.1.1.5.cmml">=</mo><mfrac id="S4.E11.m1.1.1.6" xref="S4.E11.m1.1.1.6.cmml"><mn id="S4.E11.m1.1.1.6.2" xref="S4.E11.m1.1.1.6.2.cmml">2</mn><mrow id="S4.E11.m1.1.1.6.3" xref="S4.E11.m1.1.1.6.3.cmml"><mn id="S4.E11.m1.1.1.6.3.2" xref="S4.E11.m1.1.1.6.3.2.cmml">1</mn><mo id="S4.E11.m1.1.1.6.3.1" xref="S4.E11.m1.1.1.6.3.1.cmml">+</mo><mfrac id="S4.E11.m1.1.1.6.3.3" xref="S4.E11.m1.1.1.6.3.3.cmml"><mn id="S4.E11.m1.1.1.6.3.3.2" xref="S4.E11.m1.1.1.6.3.3.2.cmml">2</mn><mi id="S4.E11.m1.1.1.6.3.3.3" xref="S4.E11.m1.1.1.6.3.3.3.cmml">d</mi></mfrac><mo id="S4.E11.m1.1.1.6.3.1a" xref="S4.E11.m1.1.1.6.3.1.cmml">+</mo><mfrac id="S4.E11.m1.1.1.6.3.4" xref="S4.E11.m1.1.1.6.3.4.cmml"><mn id="S4.E11.m1.1.1.6.3.4.2" xref="S4.E11.m1.1.1.6.3.4.2.cmml">1</mn><mi id="S4.E11.m1.1.1.6.3.4.3" xref="S4.E11.m1.1.1.6.3.4.3.cmml">s</mi></mfrac></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E11.m1.1b"><apply id="S4.E11.m1.1.1.cmml" xref="S4.E11.m1.1.1"><and id="S4.E11.m1.1.1a.cmml" xref="S4.E11.m1.1.1"></and><apply id="S4.E11.m1.1.1b.cmml" xref="S4.E11.m1.1.1"><eq id="S4.E11.m1.1.1.3.cmml" xref="S4.E11.m1.1.1.3"></eq><ci id="S4.E11.m1.1.1.2a.cmml" xref="S4.E11.m1.1.1.2"><mtext id="S4.E11.m1.1.1.2.cmml" xref="S4.E11.m1.1.1.2">intensity</mtext></ci><apply id="S4.E11.m1.1.1.4.cmml" xref="S4.E11.m1.1.1.4"><divide id="S4.E11.m1.1.1.4.1.cmml" xref="S4.E11.m1.1.1.4"></divide><ci id="S4.E11.m1.1.1.4.2a.cmml" xref="S4.E11.m1.1.1.4.2"><mtext id="S4.E11.m1.1.1.4.2.cmml" xref="S4.E11.m1.1.1.4.2">FLOPs</mtext></ci><ci id="S4.E11.m1.1.1.4.3a.cmml" xref="S4.E11.m1.1.1.4.3"><mtext id="S4.E11.m1.1.1.4.3.cmml" xref="S4.E11.m1.1.1.4.3">I/O bytes</mtext></ci></apply></apply><apply id="S4.E11.m1.1.1c.cmml" xref="S4.E11.m1.1.1"><eq id="S4.E11.m1.1.1.5.cmml" xref="S4.E11.m1.1.1.5"></eq><share href="#S4.E11.m1.1.1.4.cmml" id="S4.E11.m1.1.1d.cmml" xref="S4.E11.m1.1.1"></share><apply id="S4.E11.m1.1.1.6.cmml" xref="S4.E11.m1.1.1.6"><divide id="S4.E11.m1.1.1.6.1.cmml" xref="S4.E11.m1.1.1.6"></divide><cn type="integer" id="S4.E11.m1.1.1.6.2.cmml" xref="S4.E11.m1.1.1.6.2">2</cn><apply id="S4.E11.m1.1.1.6.3.cmml" xref="S4.E11.m1.1.1.6.3"><plus id="S4.E11.m1.1.1.6.3.1.cmml" xref="S4.E11.m1.1.1.6.3.1"></plus><cn type="integer" id="S4.E11.m1.1.1.6.3.2.cmml" xref="S4.E11.m1.1.1.6.3.2">1</cn><apply id="S4.E11.m1.1.1.6.3.3.cmml" xref="S4.E11.m1.1.1.6.3.3"><divide id="S4.E11.m1.1.1.6.3.3.1.cmml" xref="S4.E11.m1.1.1.6.3.3"></divide><cn type="integer" id="S4.E11.m1.1.1.6.3.3.2.cmml" xref="S4.E11.m1.1.1.6.3.3.2">2</cn><ci id="S4.E11.m1.1.1.6.3.3.3.cmml" xref="S4.E11.m1.1.1.6.3.3.3">𝑑</ci></apply><apply id="S4.E11.m1.1.1.6.3.4.cmml" xref="S4.E11.m1.1.1.6.3.4"><divide id="S4.E11.m1.1.1.6.3.4.1.cmml" xref="S4.E11.m1.1.1.6.3.4"></divide><cn type="integer" id="S4.E11.m1.1.1.6.3.4.2.cmml" xref="S4.E11.m1.1.1.6.3.4.2">1</cn><ci id="S4.E11.m1.1.1.6.3.4.3.cmml" xref="S4.E11.m1.1.1.6.3.4.3">𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E11.m1.1c">\text{intensity}=\frac{\text{FLOPs}}{\text{I/O bytes}}=\frac{2}{1+\frac{2}{d}+\frac{1}{s}}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></span></span></span>
</span>
<span id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14" class="ltx_p">Let’s consider LLaMA 13B (<math id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1" class="ltx_Math" alttext="d=128" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1a"><mrow id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml"><mi id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2.cmml">d</mi><mo id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1b"><apply id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1"><eq id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.1"></eq><ci id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.2">𝑑</ci><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1c">d=128</annotation></semantics></math>) with a sequence length of 1024 (<math id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1" class="ltx_Math" alttext="s=1024" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1a"><mrow id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.cmml"><mi id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.2" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.2.cmml">s</mi><mo id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.1" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.3" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1b"><apply id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1"><eq id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.1"></eq><ci id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.2.cmml" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.2">𝑠</ci><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.3.cmml" xref="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m2.1c">s=1024</annotation></semantics></math>) as an example. The calculated arithmetic intensity is <math id="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1" class="ltx_Math" alttext="1.97" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1a"><mn id="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1.1" xref="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1.1.cmml">1.97</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1b"><cn type="float" id="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1.1">1.97</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m3.1c">1.97</annotation></semantics></math>. However, the A100 80G GPU can perform <math id="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1" class="ltx_Math" alttext="312" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1a"><mn id="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1.1" xref="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1.1.cmml">312</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1b"><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1.1">312</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m4.1c">312</annotation></semantics></math> TFLOPs and transfer <math id="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1a"><mn id="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1.1" xref="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1b"><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m5.1c">2</annotation></semantics></math> TB of data in one second, <em id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1" class="ltx_emph ltx_font_italic">i.e.,</em> its ideal arithmetic intensity is <math id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1" class="ltx_Math" alttext="156" display="inline"><semantics id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1a"><mn id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1.1" xref="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1.1.cmml">156</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1b"><cn type="integer" id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1.1.cmml" xref="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1.1">156</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m6.1c">156</annotation></semantics></math>. This indicates that the bottleneck in attention calculation lies in the process of data transfer (<em id="S4.SS2.SSS5.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2" class="ltx_emph ltx_font_italic">i.e.,</em> excessive I/O loading).</span>
</span></foreignObject></g></g></svg>
</div>
<div id="S4.SS2.SSS5.p14" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS5.p14.2" class="ltx_p"><span id="S4.SS2.SSS5.p14.2.1" class="ltx_text ltx_font_bold">Decoding Efficiency Issues.</span>
In this part, we briefly analyze the decoding efficiency issues of LLMs.
Overall, the decoding process of LLMs can be divided into two stages for overhead analysis: (1) the <em id="S4.SS2.SSS5.p14.2.2" class="ltx_emph ltx_font_italic">prefill</em> stage, which computes the hidden states of the input sequence, and (2) the <em id="S4.SS2.SSS5.p14.2.3" class="ltx_emph ltx_font_italic">incremental decoding</em> stage, which generates a token and updates hidden states in an auto-regressive manner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib321" title="" class="ltx_ref">321</a>]</cite>. As shown in the above <em id="S4.SS2.SSS5.p14.2.4" class="ltx_emph ltx_font_italic">memory wall</em> box, the arithmetic intensity of the incremental decoding stage is only <math id="S4.SS2.SSS5.p14.1.m1.1" class="ltx_Math" alttext="1.97" display="inline"><semantics id="S4.SS2.SSS5.p14.1.m1.1a"><mn id="S4.SS2.SSS5.p14.1.m1.1.1" xref="S4.SS2.SSS5.p14.1.m1.1.1.cmml">1.97</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p14.1.m1.1b"><cn type="float" id="S4.SS2.SSS5.p14.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p14.1.m1.1.1">1.97</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p14.1.m1.1c">1.97</annotation></semantics></math>, which is far from the expected value of 156 (calculated according to the standard configuration of A100 80GB GPU). In contrast, the arithmetic intensity of the prefill stage achieves <math id="S4.SS2.SSS5.p14.2.m2.1" class="ltx_Math" alttext="113.78" display="inline"><semantics id="S4.SS2.SSS5.p14.2.m2.1a"><mn id="S4.SS2.SSS5.p14.2.m2.1.1" xref="S4.SS2.SSS5.p14.2.m2.1.1.cmml">113.78</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p14.2.m2.1b"><cn type="float" id="S4.SS2.SSS5.p14.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p14.2.m2.1.1">113.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p14.2.m2.1c">113.78</annotation></semantics></math> for LLaMA-13B. Consequently, existing work mainly investigates how to enhance the efficiency of the incremental decoding algorithm, which can be categorized into two main approaches:</p>
</div>
<div id="S4.SS2.SSS5.p15" class="ltx_para">
<p id="S4.SS2.SSS5.p15.1" class="ltx_p"><math id="S4.SS2.SSS5.p15.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p15.1.m1.1a"><mo id="S4.SS2.SSS5.p15.1.m1.1.1" xref="S4.SS2.SSS5.p15.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p15.1.m1.1b"><ci id="S4.SS2.SSS5.p15.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p15.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p15.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p15.1.1" class="ltx_emph ltx_font_italic">Reducing data transfer</em> mainly focuses on optimizing GPU memory access, thereby increasing the arithmetic intensity. As introduced in Section&nbsp;<a href="#S4.SS2.SSS2" title="4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, KV cache can avoid redundant computation of previous tokens and PagedAttention allocates KV caches into continuous blocks to reduce memory fragmentation. Furthermore, Flash-Decoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib322" title="" class="ltx_ref">322</a>]</cite> speeds up attention computation by loading the keys and values in parallel, especially effective for long text generation. As another alternative approach, multi-query and grouped-query attention can reduce the GPU memory bandwidth overhead by sharing KV parameters (loading fewer weights).</p>
</div>
<div id="S4.SS2.SSS5.p16" class="ltx_para">
<p id="S4.SS2.SSS5.p16.4" class="ltx_p"><math id="S4.SS2.SSS5.p16.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p16.1.m1.1a"><mo id="S4.SS2.SSS5.p16.1.m1.1.1" xref="S4.SS2.SSS5.p16.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p16.1.m1.1b"><ci id="S4.SS2.SSS5.p16.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p16.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p16.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p16.4.1" class="ltx_emph ltx_font_italic">Decoding strategy optimization</em> aims to improve the sequential nature of the auto-regressive generation manner in different ways. As a representative study, <em id="S4.SS2.SSS5.p16.4.2" class="ltx_emph ltx_font_italic">speculative decoding</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib323" title="" class="ltx_ref">323</a>, <a href="#bib.bib324" title="" class="ltx_ref">324</a>]</cite> first leverages a compact but efficient model (<em id="S4.SS2.SSS5.p16.4.3" class="ltx_emph ltx_font_italic">e.g.,</em> a <math id="S4.SS2.SSS5.p16.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.SSS5.p16.2.m2.1a"><mi id="S4.SS2.SSS5.p16.2.m2.1.1" xref="S4.SS2.SSS5.p16.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p16.2.m2.1b"><ci id="S4.SS2.SSS5.p16.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p16.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p16.2.m2.1c">n</annotation></semantics></math>-gram model or a small PLM) to generate short segments and then utilizes the LLM to verify and correct these drafts. It can lead to a notable 2<math id="S4.SS2.SSS5.p16.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS5.p16.3.m3.1a"><mo id="S4.SS2.SSS5.p16.3.m3.1.1" xref="S4.SS2.SSS5.p16.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p16.3.m3.1b"><times id="S4.SS2.SSS5.p16.3.m3.1.1.cmml" xref="S4.SS2.SSS5.p16.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p16.3.m3.1c">\times</annotation></semantics></math> to 3<math id="S4.SS2.SSS5.p16.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS5.p16.4.m4.1a"><mo id="S4.SS2.SSS5.p16.4.m4.1.1" xref="S4.SS2.SSS5.p16.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p16.4.m4.1b"><times id="S4.SS2.SSS5.p16.4.m4.1.1.cmml" xref="S4.SS2.SSS5.p16.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p16.4.m4.1c">\times</annotation></semantics></math> speedup without compromising the generation quality.
Researchers further suggest several variants to improve the efficiency of this approach, such as a learning-based method to combine several small models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib325" title="" class="ltx_ref">325</a>]</cite> and a stage-wise acceleration which employs a more smaller LM to accelerate the small LM first&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib326" title="" class="ltx_ref">326</a>]</cite>.
In addition, token-level early-exit techniques have been proposed enabling the generation of a token at lower Transformer layers, rather than passing through all the layers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib327" title="" class="ltx_ref">327</a>]</cite>. It can attain greater speedup, but at the cost of sacrificing generation quality.</p>
</div>
<div id="S4.SS2.SSS5.p17" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS5.p17.1" class="ltx_p"><span id="S4.SS2.SSS5.p17.1.1" class="ltx_text ltx_font_bold">Practical Settings.</span>
In practice, existing libraries (<em id="S4.SS2.SSS5.p17.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Transformers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>) and public APIs of LLMs (<em id="S4.SS2.SSS5.p17.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> OpenAI) have supported various decoding strategies to serve different scenarios of text generation. Next, we present the decoding settings of several representative LLMs:</p>
</div>
<div id="S4.SS2.SSS5.p18" class="ltx_para">
<p id="S4.SS2.SSS5.p18.1" class="ltx_p"><math id="S4.SS2.SSS5.p18.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p18.1.m1.1a"><mo id="S4.SS2.SSS5.p18.1.m1.1.1" xref="S4.SS2.SSS5.p18.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p18.1.m1.1b"><ci id="S4.SS2.SSS5.p18.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p18.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p18.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p18.1.1" class="ltx_emph ltx_font_italic">T5</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> utilizes greedy search as the default setting and applies beam search (beam size of 4) with a length penalty of 0.6 for translation and summarization tasks.</p>
</div>
<div id="S4.SS2.SSS5.p19" class="ltx_para">
<p id="S4.SS2.SSS5.p19.1" class="ltx_p"><math id="S4.SS2.SSS5.p19.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p19.1.m1.1a"><mo id="S4.SS2.SSS5.p19.1.m1.1.1" xref="S4.SS2.SSS5.p19.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p19.1.m1.1b"><ci id="S4.SS2.SSS5.p19.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p19.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p19.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p19.1.1" class="ltx_emph ltx_font_italic">GPT-3</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> employs beam search with a beam size of 4 and a length penalty of 0.6 for all generation tasks.</p>
</div>
<div id="S4.SS2.SSS5.p20" class="ltx_para">
<p id="S4.SS2.SSS5.p20.5" class="ltx_p"><math id="S4.SS2.SSS5.p20.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p20.1.m1.1a"><mo id="S4.SS2.SSS5.p20.1.m1.1.1" xref="S4.SS2.SSS5.p20.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p20.1.m1.1b"><ci id="S4.SS2.SSS5.p20.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p20.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p20.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p20.5.1" class="ltx_emph ltx_font_italic">Alpaca</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> utilizes sampling-based strategies with top-<math id="S4.SS2.SSS5.p20.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.SSS5.p20.2.m2.1a"><mi id="S4.SS2.SSS5.p20.2.m2.1.1" xref="S4.SS2.SSS5.p20.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p20.2.m2.1b"><ci id="S4.SS2.SSS5.p20.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p20.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p20.2.m2.1c">k</annotation></semantics></math> (<math id="S4.SS2.SSS5.p20.3.m3.1" class="ltx_Math" alttext="k=50" display="inline"><semantics id="S4.SS2.SSS5.p20.3.m3.1a"><mrow id="S4.SS2.SSS5.p20.3.m3.1.1" xref="S4.SS2.SSS5.p20.3.m3.1.1.cmml"><mi id="S4.SS2.SSS5.p20.3.m3.1.1.2" xref="S4.SS2.SSS5.p20.3.m3.1.1.2.cmml">k</mi><mo id="S4.SS2.SSS5.p20.3.m3.1.1.1" xref="S4.SS2.SSS5.p20.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS5.p20.3.m3.1.1.3" xref="S4.SS2.SSS5.p20.3.m3.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p20.3.m3.1b"><apply id="S4.SS2.SSS5.p20.3.m3.1.1.cmml" xref="S4.SS2.SSS5.p20.3.m3.1.1"><eq id="S4.SS2.SSS5.p20.3.m3.1.1.1.cmml" xref="S4.SS2.SSS5.p20.3.m3.1.1.1"></eq><ci id="S4.SS2.SSS5.p20.3.m3.1.1.2.cmml" xref="S4.SS2.SSS5.p20.3.m3.1.1.2">𝑘</ci><cn type="integer" id="S4.SS2.SSS5.p20.3.m3.1.1.3.cmml" xref="S4.SS2.SSS5.p20.3.m3.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p20.3.m3.1c">k=50</annotation></semantics></math>), top-<math id="S4.SS2.SSS5.p20.4.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS5.p20.4.m4.1a"><mi id="S4.SS2.SSS5.p20.4.m4.1.1" xref="S4.SS2.SSS5.p20.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p20.4.m4.1b"><ci id="S4.SS2.SSS5.p20.4.m4.1.1.cmml" xref="S4.SS2.SSS5.p20.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p20.4.m4.1c">p</annotation></semantics></math> (<math id="S4.SS2.SSS5.p20.5.m5.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S4.SS2.SSS5.p20.5.m5.1a"><mrow id="S4.SS2.SSS5.p20.5.m5.1.1" xref="S4.SS2.SSS5.p20.5.m5.1.1.cmml"><mi id="S4.SS2.SSS5.p20.5.m5.1.1.2" xref="S4.SS2.SSS5.p20.5.m5.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS5.p20.5.m5.1.1.1" xref="S4.SS2.SSS5.p20.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS5.p20.5.m5.1.1.3" xref="S4.SS2.SSS5.p20.5.m5.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p20.5.m5.1b"><apply id="S4.SS2.SSS5.p20.5.m5.1.1.cmml" xref="S4.SS2.SSS5.p20.5.m5.1.1"><eq id="S4.SS2.SSS5.p20.5.m5.1.1.1.cmml" xref="S4.SS2.SSS5.p20.5.m5.1.1.1"></eq><ci id="S4.SS2.SSS5.p20.5.m5.1.1.2.cmml" xref="S4.SS2.SSS5.p20.5.m5.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.SSS5.p20.5.m5.1.1.3.cmml" xref="S4.SS2.SSS5.p20.5.m5.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p20.5.m5.1c">p=0.9</annotation></semantics></math>), and temperature of 0.7 for open-ended generation.</p>
</div>
<div id="S4.SS2.SSS5.p21" class="ltx_para">
<p id="S4.SS2.SSS5.p21.1" class="ltx_p"><math id="S4.SS2.SSS5.p21.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p21.1.m1.1a"><mo id="S4.SS2.SSS5.p21.1.m1.1.1" xref="S4.SS2.SSS5.p21.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p21.1.m1.1b"><ci id="S4.SS2.SSS5.p21.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p21.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p21.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p21.1.1" class="ltx_emph ltx_font_italic">LLaMA</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> applies diverse decoding strategies tailored to specific tasks. For instance, it employs the greedy search for question answering tasks while utilizes a sampling strategy with the temperature settings of 0.1 (pass@1) and 0.8 (pass@100) for code generation.</p>
</div>
<div id="S4.SS2.SSS5.p22" class="ltx_para">
<p id="S4.SS2.SSS5.p22.1" class="ltx_p"><math id="S4.SS2.SSS5.p22.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS5.p22.1.m1.1a"><mo id="S4.SS2.SSS5.p22.1.m1.1.1" xref="S4.SS2.SSS5.p22.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p22.1.m1.1b"><ci id="S4.SS2.SSS5.p22.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p22.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p22.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS2.SSS5.p22.1.1" class="ltx_emph ltx_font_italic">OpenAI API</em> supports several basic decoding strategies, including greedy search (by setting <span id="S4.SS2.SSS5.p22.1.2" class="ltx_text ltx_font_typewriter">temperature</span> to 0), beam search (with the setting <span id="S4.SS2.SSS5.p22.1.3" class="ltx_text ltx_font_typewriter">best_of</span>), temperature sampling (with the setting <span id="S4.SS2.SSS5.p22.1.4" class="ltx_text ltx_font_typewriter">temperature</span>), nucleus sampling (with the setting <span id="S4.SS2.SSS5.p22.1.5" class="ltx_text ltx_font_typewriter">top_p</span>). It also introduce parameters <span id="S4.SS2.SSS5.p22.1.6" class="ltx_text ltx_font_typewriter">presence_penalty</span> and <span id="S4.SS2.SSS5.p22.1.7" class="ltx_text ltx_font_typewriter">frequency_penalty</span> to control the repetition degree of generation.
According to the OpenAI’s document, their APIs would produce different outputs even if the input and the hyper-parameters are the same. Setting temperature to 0 can yield more deterministic outputs, albeit with a slight chance of variability.</p>
</div>
</section>
<section id="S4.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.6 </span>Summary and Discussion</h4>

<div id="S4.SS2.SSS6.p1" class="ltx_para">
<p id="S4.SS2.SSS6.p1.1" class="ltx_p">The choice of architecture and pre-training tasks may incur different inductive biases for LLMs, which would lead to different model capacities. In this part, we discuss one open issue about the architecture choice for LLMs.</p>
</div>
<div id="S4.SS2.SSS6.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S4.SS2.SSS6.1.p1.pic1" class="ltx_picture" height="326.36" overflow="visible" version="1.1" width="288"><g transform="translate(0,326.36) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 320.46 C 0 323.72 2.64 326.36 5.91 326.36 L 282.09 326.36 C 285.35 326.36 288 323.72 288 320.46 L 288 5.91 C 288 2.64 285.35 0 282.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 288.34 L 286.03 288.34 L 286.03 5.91 C 286.03 3.73 284.27 1.97 282.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 294.25)"><foreignObject width="244.69" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S4.SS2.SSS6.1.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:176.8pt;">
<span id="S4.SS2.SSS6.1.p1.pic1.1.1.1.1.1.1" class="ltx_p">Why does Predicting the Next Word Works?</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="244.69" height="262.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S4.SS2.SSS6.1.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:176.8pt;">
<span id="S4.SS2.SSS6.1.p1.pic1.2.2.2.1.1.1" class="ltx_p">The essence of decoder-only architecture is to <em id="S4.SS2.SSS6.1.p1.pic1.2.2.2.1.1.1.1" class="ltx_emph ltx_font_italic">accurately predict the next word</em> for reconstructing the pre-training data. Till now, there has been no formal study that theoretically demonstrates its advantage over other architectures. An interesting explanation was from Ilya Sutskever during the interview held by Jensen Huang<span id="footnote24" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span>https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S52092/</span></span></span>.
The original transcript from the interview was copied below<span id="footnote25" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span>https://lifearchitect.ai/ilya/</span></span></span>:</span>
<span id="S4.SS2.SSS6.1.p1.pic1.2.2.2.1.1.2" class="ltx_p"><span id="S4.SS2.SSS6.1.p1.pic1.2.2.2.1.1.2.1" class="ltx_text ltx_font_typewriter">Say you read a detective novel. It’s like complicated plot, a storyline, different characters, lots of events, mysteries like clues, it’s unclear. Then, let’s say that at the last page of the book, the detective has gathered all the clues, gathered all the people and saying, "okay, I’m going to reveal the identity of whoever committed the crime and that person’s name is". Predict that word. ...
<br class="ltx_break">Now, there are many different words. But predicting those words better and better, the understanding of the text keeps on increasing. GPT-4 predicts the next word better.
</span></span>
</span></foreignObject></g></g></svg>
</div>
<div id="S4.SS2.SSS6.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS6.p2.1" class="ltx_p"><span id="S4.SS2.SSS6.p2.1.1" class="ltx_text ltx_font_bold">Architecture Choice</span>. In earlier literature of pre-trained language models, there are lots of discussions on the effects of different architectures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>.
However, most LLMs are developed based on the causal decoder architecture, and there still lacks a theoretical analysis on its advantage over the other alternatives. Next, we briefly summarize existing discussions on this issue.</p>
</div>
<div id="S4.SS2.SSS6.p3" class="ltx_para">
<p id="S4.SS2.SSS6.p3.1" class="ltx_p"><math id="S4.SS2.SSS6.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS6.p3.1.m1.1a"><mo id="S4.SS2.SSS6.p3.1.m1.1.1" xref="S4.SS2.SSS6.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS6.p3.1.m1.1b"><ci id="S4.SS2.SSS6.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS6.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS6.p3.1.m1.1c">\bullet</annotation></semantics></math> By pre-training with the LM objective, it seems that causal decoder architecture can achieve a superior zero-shot and few-shot generalization capacity. Existing research has shown that without multi-task fine-tuning, the causal decoder has better zero-shot performance than other architectures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The success of GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> has demonstrates that the large causal decoder model can be a good few-shot learner. In addition, instruction tuning and alignment tuning discussed in Section&nbsp;<a href="#S5" title="5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> have been proven to further enhance the capability of large causal decoder models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS6.p4" class="ltx_para">
<p id="S4.SS2.SSS6.p4.1" class="ltx_p"><math id="S4.SS2.SSS6.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS6.p4.1.m1.1a"><mo id="S4.SS2.SSS6.p4.1.m1.1.1" xref="S4.SS2.SSS6.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS6.p4.1.m1.1b"><ci id="S4.SS2.SSS6.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS6.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS6.p4.1.m1.1c">\bullet</annotation></semantics></math> Scaling law has been widely observed in causal decoders. By scaling the model size, the dataset size, and the total computation, the performance of causal decoders can be substantially improved&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.
Thus, it has become an important strategy to increase the model capacity of the causal decoder via scaling. However, more detailed investigation on encoder-decoder models is still lacking, and more efforts are needed to investigate the performance of encoder-decoder models at a large scale.</p>
</div>
<div id="S4.SS2.SSS6.p5" class="ltx_para">
<p id="S4.SS2.SSS6.p5.1" class="ltx_p">More research efforts about the discussions on architectures and pre-training objectives are in need to analyze how the choices of the architecture and pre-training tasks affect the capacity of LLMs, especially for encoder-decoder architectures.
Despite the effectiveness of decoder-only architecture, it is also suggested to make more diverse exploration on architecture design.
Besides the major architecture, the detailed configuration of LLM is also worth attention, which has been discussed in Section&nbsp;<a href="#S4.SS2.SSS2" title="4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Detailed optimization settings of several existing LLMs.
</figcaption>
<div id="S4.T7.20" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:888.9pt;height:499.8pt;vertical-align:-1.4pt;"><span class="ltx_transformed_inner" style="transform:translate(118.6pt,-66.5pt) scale(1.36393496387527,1.36393496387527) ;">
<table id="S4.T7.20.20" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T7.20.20.21" class="ltx_tr">
<td id="S4.T7.20.20.21.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T7.20.20.21.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T7.20.20.21.2" class="ltx_td ltx_align_right ltx_border_tt">
<table id="S4.T7.20.20.21.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T7.20.20.21.2.1.1" class="ltx_tr">
<td id="S4.T7.20.20.21.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S4.T7.20.20.21.2.1.1.1.1" class="ltx_text ltx_font_bold">Batch Size</span></td>
</tr>
<tr id="S4.T7.20.20.21.2.1.2" class="ltx_tr">
<td id="S4.T7.20.20.21.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S4.T7.20.20.21.2.1.2.1.1" class="ltx_text ltx_font_bold">(#tokens)</span></td>
</tr>
</tbody></table>
</td>
<td id="S4.T7.20.20.21.3" class="ltx_td ltx_align_right ltx_border_tt">
<table id="S4.T7.20.20.21.3.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T7.20.20.21.3.1.1" class="ltx_tr">
<td id="S4.T7.20.20.21.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S4.T7.20.20.21.3.1.1.1.1" class="ltx_text ltx_font_bold">Learning</span></td>
</tr>
<tr id="S4.T7.20.20.21.3.1.2" class="ltx_tr">
<td id="S4.T7.20.20.21.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S4.T7.20.20.21.3.1.2.1.1" class="ltx_text ltx_font_bold">Rate</span></td>
</tr>
</tbody></table>
</td>
<td id="S4.T7.20.20.21.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.20.20.21.4.1" class="ltx_text ltx_font_bold">Warmup</span></td>
<td id="S4.T7.20.20.21.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.20.20.21.5.1" class="ltx_text ltx_font_bold">Decay Method</span></td>
<td id="S4.T7.20.20.21.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.20.20.21.6.1" class="ltx_text ltx_font_bold">Optimizer</span></td>
<td id="S4.T7.20.20.21.7" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T7.20.20.21.7.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T7.20.20.21.7.1.1" class="ltx_tr">
<td id="S4.T7.20.20.21.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T7.20.20.21.7.1.1.1.1" class="ltx_text ltx_font_bold">Precision</span></td>
</tr>
<tr id="S4.T7.20.20.21.7.1.2" class="ltx_tr">
<td id="S4.T7.20.20.21.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T7.20.20.21.7.1.2.1.1" class="ltx_text ltx_font_bold">Type</span></td>
</tr>
</tbody></table>
</td>
<td id="S4.T7.20.20.21.8" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T7.20.20.21.8.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T7.20.20.21.8.1.1" class="ltx_tr">
<td id="S4.T7.20.20.21.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T7.20.20.21.8.1.1.1.1" class="ltx_text ltx_font_bold">Weight</span></td>
</tr>
<tr id="S4.T7.20.20.21.8.1.2" class="ltx_tr">
<td id="S4.T7.20.20.21.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T7.20.20.21.8.1.2.1.1" class="ltx_text ltx_font_bold">Decay</span></td>
</tr>
</tbody></table>
</td>
<td id="S4.T7.20.20.21.9" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T7.20.20.21.9.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T7.20.20.21.9.1.1" class="ltx_tr">
<td id="S4.T7.20.20.21.9.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T7.20.20.21.9.1.1.1.1" class="ltx_text ltx_font_bold">Grad</span></td>
</tr>
<tr id="S4.T7.20.20.21.9.1.2" class="ltx_tr">
<td id="S4.T7.20.20.21.9.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T7.20.20.21.9.1.2.1.1" class="ltx_text ltx_font_bold">Clip</span></td>
</tr>
</tbody></table>
</td>
<td id="S4.T7.20.20.21.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.20.20.21.10.1" class="ltx_text ltx_font_bold">Dropout</span></td>
</tr>
<tr id="S4.T7.1.1.1" class="ltx_tr">
<td id="S4.T7.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">GPT3&nbsp;(175B)</td>
<td id="S4.T7.1.1.1.3" class="ltx_td ltx_align_right ltx_border_t">32K→3.2M</td>
<td id="S4.T7.1.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="6\times 10^{-5}" display="inline"><semantics id="S4.T7.1.1.1.1.m1.1a"><mrow id="S4.T7.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mn id="S4.T7.1.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.1.m1.1.1.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.1.1.1.1.m1.1.1.1" xref="S4.T7.1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.1.1.1.1.m1.1.1.3" xref="S4.T7.1.1.1.1.m1.1.1.3.cmml"><mn id="S4.T7.1.1.1.1.m1.1.1.3.2" xref="S4.T7.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.1.1.1.1.m1.1.1.3.3" xref="S4.T7.1.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T7.1.1.1.1.m1.1.1.3.3a" xref="S4.T7.1.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.1.1.1.1.m1.1.1.3.3.2" xref="S4.T7.1.1.1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1"><times id="S4.T7.1.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.1.1.1.1.m1.1.1.2.cmml" xref="S4.T7.1.1.1.1.m1.1.1.2">6</cn><apply id="S4.T7.1.1.1.1.m1.1.1.3.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3.2">10</cn><apply id="S4.T7.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3.3"><minus id="S4.T7.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T7.1.1.1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.m1.1c">6\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S4.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">cosine decay to 10%</td>
<td id="S4.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Adam</td>
<td id="S4.T7.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">FP16</td>
<td id="S4.T7.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="S4.T7.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">1.0</td>
<td id="S4.T7.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T7.3.3.3" class="ltx_tr">
<td id="S4.T7.2.2.2.1" class="ltx_td ltx_align_left">PanGu-<math id="S4.T7.2.2.2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T7.2.2.2.1.m1.1a"><mi id="S4.T7.2.2.2.1.m1.1.1" xref="S4.T7.2.2.2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.1.m1.1b"><ci id="S4.T7.2.2.2.1.m1.1.1.cmml" xref="S4.T7.2.2.2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.1.m1.1c">\alpha</annotation></semantics></math>&nbsp;(200B)</td>
<td id="S4.T7.3.3.3.3" class="ltx_td ltx_align_right">-</td>
<td id="S4.T7.3.3.3.2" class="ltx_td ltx_align_right"><math id="S4.T7.3.3.3.2.m1.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S4.T7.3.3.3.2.m1.1a"><mrow id="S4.T7.3.3.3.2.m1.1.1" xref="S4.T7.3.3.3.2.m1.1.1.cmml"><mn id="S4.T7.3.3.3.2.m1.1.1.2" xref="S4.T7.3.3.3.2.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.3.3.3.2.m1.1.1.1" xref="S4.T7.3.3.3.2.m1.1.1.1.cmml">×</mo><msup id="S4.T7.3.3.3.2.m1.1.1.3" xref="S4.T7.3.3.3.2.m1.1.1.3.cmml"><mn id="S4.T7.3.3.3.2.m1.1.1.3.2" xref="S4.T7.3.3.3.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.3.3.3.2.m1.1.1.3.3" xref="S4.T7.3.3.3.2.m1.1.1.3.3.cmml"><mo id="S4.T7.3.3.3.2.m1.1.1.3.3a" xref="S4.T7.3.3.3.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.3.3.3.2.m1.1.1.3.3.2" xref="S4.T7.3.3.3.2.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.2.m1.1b"><apply id="S4.T7.3.3.3.2.m1.1.1.cmml" xref="S4.T7.3.3.3.2.m1.1.1"><times id="S4.T7.3.3.3.2.m1.1.1.1.cmml" xref="S4.T7.3.3.3.2.m1.1.1.1"></times><cn type="integer" id="S4.T7.3.3.3.2.m1.1.1.2.cmml" xref="S4.T7.3.3.3.2.m1.1.1.2">2</cn><apply id="S4.T7.3.3.3.2.m1.1.1.3.cmml" xref="S4.T7.3.3.3.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.3.3.3.2.m1.1.1.3.1.cmml" xref="S4.T7.3.3.3.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.3.3.3.2.m1.1.1.3.2.cmml" xref="S4.T7.3.3.3.2.m1.1.1.3.2">10</cn><apply id="S4.T7.3.3.3.2.m1.1.1.3.3.cmml" xref="S4.T7.3.3.3.2.m1.1.1.3.3"><minus id="S4.T7.3.3.3.2.m1.1.1.3.3.1.cmml" xref="S4.T7.3.3.3.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.3.3.3.2.m1.1.1.3.3.2.cmml" xref="S4.T7.3.3.3.2.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.2.m1.1c">2\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.3.3.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.3.3.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.3.3.3.6" class="ltx_td ltx_align_center">Adam</td>
<td id="S4.T7.3.3.3.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.3.3.3.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.3.3.3.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.3.3.3.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.4.4.4" class="ltx_tr">
<td id="S4.T7.4.4.4.2" class="ltx_td ltx_align_left">OPT&nbsp;(175B)</td>
<td id="S4.T7.4.4.4.3" class="ltx_td ltx_align_right">2M</td>
<td id="S4.T7.4.4.4.1" class="ltx_td ltx_align_right"><math id="S4.T7.4.4.4.1.m1.1" class="ltx_Math" alttext="1.2\times 10^{-4}" display="inline"><semantics id="S4.T7.4.4.4.1.m1.1a"><mrow id="S4.T7.4.4.4.1.m1.1.1" xref="S4.T7.4.4.4.1.m1.1.1.cmml"><mn id="S4.T7.4.4.4.1.m1.1.1.2" xref="S4.T7.4.4.4.1.m1.1.1.2.cmml">1.2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.4.4.4.1.m1.1.1.1" xref="S4.T7.4.4.4.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.4.4.4.1.m1.1.1.3" xref="S4.T7.4.4.4.1.m1.1.1.3.cmml"><mn id="S4.T7.4.4.4.1.m1.1.1.3.2" xref="S4.T7.4.4.4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.4.4.4.1.m1.1.1.3.3" xref="S4.T7.4.4.4.1.m1.1.1.3.3.cmml"><mo id="S4.T7.4.4.4.1.m1.1.1.3.3a" xref="S4.T7.4.4.4.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.4.4.4.1.m1.1.1.3.3.2" xref="S4.T7.4.4.4.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.1.m1.1b"><apply id="S4.T7.4.4.4.1.m1.1.1.cmml" xref="S4.T7.4.4.4.1.m1.1.1"><times id="S4.T7.4.4.4.1.m1.1.1.1.cmml" xref="S4.T7.4.4.4.1.m1.1.1.1"></times><cn type="float" id="S4.T7.4.4.4.1.m1.1.1.2.cmml" xref="S4.T7.4.4.4.1.m1.1.1.2">1.2</cn><apply id="S4.T7.4.4.4.1.m1.1.1.3.cmml" xref="S4.T7.4.4.4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.4.4.4.1.m1.1.1.3.1.cmml" xref="S4.T7.4.4.4.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.4.4.4.1.m1.1.1.3.2.cmml" xref="S4.T7.4.4.4.1.m1.1.1.3.2">10</cn><apply id="S4.T7.4.4.4.1.m1.1.1.3.3.cmml" xref="S4.T7.4.4.4.1.m1.1.1.3.3"><minus id="S4.T7.4.4.4.1.m1.1.1.3.3.1.cmml" xref="S4.T7.4.4.4.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.4.4.4.1.m1.1.1.3.3.2.cmml" xref="S4.T7.4.4.4.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.1.m1.1c">1.2\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T7.4.4.4.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.4.4.4.5" class="ltx_td ltx_align_center">manual decay</td>
<td id="S4.T7.4.4.4.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.4.4.4.7" class="ltx_td ltx_align_center">FP16</td>
<td id="S4.T7.4.4.4.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.4.4.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.4.4.4.10" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="S4.T7.6.6.6" class="ltx_tr">
<td id="S4.T7.6.6.6.3" class="ltx_td ltx_align_left">PaLM&nbsp;(540B)</td>
<td id="S4.T7.6.6.6.4" class="ltx_td ltx_align_right">1M→4M</td>
<td id="S4.T7.5.5.5.1" class="ltx_td ltx_align_right"><math id="S4.T7.5.5.5.1.m1.1" class="ltx_Math" alttext="1\times 10^{-2}" display="inline"><semantics id="S4.T7.5.5.5.1.m1.1a"><mrow id="S4.T7.5.5.5.1.m1.1.1" xref="S4.T7.5.5.5.1.m1.1.1.cmml"><mn id="S4.T7.5.5.5.1.m1.1.1.2" xref="S4.T7.5.5.5.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.5.5.5.1.m1.1.1.1" xref="S4.T7.5.5.5.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.5.5.5.1.m1.1.1.3" xref="S4.T7.5.5.5.1.m1.1.1.3.cmml"><mn id="S4.T7.5.5.5.1.m1.1.1.3.2" xref="S4.T7.5.5.5.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.5.5.5.1.m1.1.1.3.3" xref="S4.T7.5.5.5.1.m1.1.1.3.3.cmml"><mo id="S4.T7.5.5.5.1.m1.1.1.3.3a" xref="S4.T7.5.5.5.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.5.5.5.1.m1.1.1.3.3.2" xref="S4.T7.5.5.5.1.m1.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.5.1.m1.1b"><apply id="S4.T7.5.5.5.1.m1.1.1.cmml" xref="S4.T7.5.5.5.1.m1.1.1"><times id="S4.T7.5.5.5.1.m1.1.1.1.cmml" xref="S4.T7.5.5.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.5.5.5.1.m1.1.1.2.cmml" xref="S4.T7.5.5.5.1.m1.1.1.2">1</cn><apply id="S4.T7.5.5.5.1.m1.1.1.3.cmml" xref="S4.T7.5.5.5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.5.5.5.1.m1.1.1.3.1.cmml" xref="S4.T7.5.5.5.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.5.5.5.1.m1.1.1.3.2.cmml" xref="S4.T7.5.5.5.1.m1.1.1.3.2">10</cn><apply id="S4.T7.5.5.5.1.m1.1.1.3.3.cmml" xref="S4.T7.5.5.5.1.m1.1.1.3.3"><minus id="S4.T7.5.5.5.1.m1.1.1.3.3.1.cmml" xref="S4.T7.5.5.5.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.5.5.5.1.m1.1.1.3.3.2.cmml" xref="S4.T7.5.5.5.1.m1.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.5.1.m1.1c">1\times 10^{-2}</annotation></semantics></math></td>
<td id="S4.T7.6.6.6.5" class="ltx_td ltx_align_center">no</td>
<td id="S4.T7.6.6.6.6" class="ltx_td ltx_align_center">inverse square root</td>
<td id="S4.T7.6.6.6.7" class="ltx_td ltx_align_center">Adafactor</td>
<td id="S4.T7.6.6.6.8" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.6.6.6.2" class="ltx_td ltx_align_center"><math id="S4.T7.6.6.6.2.m1.1" class="ltx_Math" alttext="lr^{2}" display="inline"><semantics id="S4.T7.6.6.6.2.m1.1a"><mrow id="S4.T7.6.6.6.2.m1.1.1" xref="S4.T7.6.6.6.2.m1.1.1.cmml"><mi id="S4.T7.6.6.6.2.m1.1.1.2" xref="S4.T7.6.6.6.2.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T7.6.6.6.2.m1.1.1.1" xref="S4.T7.6.6.6.2.m1.1.1.1.cmml">​</mo><msup id="S4.T7.6.6.6.2.m1.1.1.3" xref="S4.T7.6.6.6.2.m1.1.1.3.cmml"><mi id="S4.T7.6.6.6.2.m1.1.1.3.2" xref="S4.T7.6.6.6.2.m1.1.1.3.2.cmml">r</mi><mn id="S4.T7.6.6.6.2.m1.1.1.3.3" xref="S4.T7.6.6.6.2.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.6.6.6.2.m1.1b"><apply id="S4.T7.6.6.6.2.m1.1.1.cmml" xref="S4.T7.6.6.6.2.m1.1.1"><times id="S4.T7.6.6.6.2.m1.1.1.1.cmml" xref="S4.T7.6.6.6.2.m1.1.1.1"></times><ci id="S4.T7.6.6.6.2.m1.1.1.2.cmml" xref="S4.T7.6.6.6.2.m1.1.1.2">𝑙</ci><apply id="S4.T7.6.6.6.2.m1.1.1.3.cmml" xref="S4.T7.6.6.6.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.6.6.6.2.m1.1.1.3.1.cmml" xref="S4.T7.6.6.6.2.m1.1.1.3">superscript</csymbol><ci id="S4.T7.6.6.6.2.m1.1.1.3.2.cmml" xref="S4.T7.6.6.6.2.m1.1.1.3.2">𝑟</ci><cn type="integer" id="S4.T7.6.6.6.2.m1.1.1.3.3.cmml" xref="S4.T7.6.6.6.2.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.6.6.6.2.m1.1c">lr^{2}</annotation></semantics></math></td>
<td id="S4.T7.6.6.6.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.6.6.6.10" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="S4.T7.7.7.7" class="ltx_tr">
<td id="S4.T7.7.7.7.2" class="ltx_td ltx_align_left">BLOOM&nbsp;(176B)</td>
<td id="S4.T7.7.7.7.3" class="ltx_td ltx_align_right">4M</td>
<td id="S4.T7.7.7.7.1" class="ltx_td ltx_align_right"><math id="S4.T7.7.7.7.1.m1.1" class="ltx_Math" alttext="6\times 10^{-5}" display="inline"><semantics id="S4.T7.7.7.7.1.m1.1a"><mrow id="S4.T7.7.7.7.1.m1.1.1" xref="S4.T7.7.7.7.1.m1.1.1.cmml"><mn id="S4.T7.7.7.7.1.m1.1.1.2" xref="S4.T7.7.7.7.1.m1.1.1.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.7.7.7.1.m1.1.1.1" xref="S4.T7.7.7.7.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.7.7.7.1.m1.1.1.3" xref="S4.T7.7.7.7.1.m1.1.1.3.cmml"><mn id="S4.T7.7.7.7.1.m1.1.1.3.2" xref="S4.T7.7.7.7.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.7.7.7.1.m1.1.1.3.3" xref="S4.T7.7.7.7.1.m1.1.1.3.3.cmml"><mo id="S4.T7.7.7.7.1.m1.1.1.3.3a" xref="S4.T7.7.7.7.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.7.7.7.1.m1.1.1.3.3.2" xref="S4.T7.7.7.7.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.7.7.7.1.m1.1b"><apply id="S4.T7.7.7.7.1.m1.1.1.cmml" xref="S4.T7.7.7.7.1.m1.1.1"><times id="S4.T7.7.7.7.1.m1.1.1.1.cmml" xref="S4.T7.7.7.7.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.7.7.7.1.m1.1.1.2.cmml" xref="S4.T7.7.7.7.1.m1.1.1.2">6</cn><apply id="S4.T7.7.7.7.1.m1.1.1.3.cmml" xref="S4.T7.7.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.7.7.7.1.m1.1.1.3.1.cmml" xref="S4.T7.7.7.7.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.7.7.7.1.m1.1.1.3.2.cmml" xref="S4.T7.7.7.7.1.m1.1.1.3.2">10</cn><apply id="S4.T7.7.7.7.1.m1.1.1.3.3.cmml" xref="S4.T7.7.7.7.1.m1.1.1.3.3"><minus id="S4.T7.7.7.7.1.m1.1.1.3.3.1.cmml" xref="S4.T7.7.7.7.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.7.7.7.1.m1.1.1.3.3.2.cmml" xref="S4.T7.7.7.7.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.7.7.7.1.m1.1c">6\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.7.7.7.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.7.7.7.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.7.7.7.6" class="ltx_td ltx_align_center">Adam</td>
<td id="S4.T7.7.7.7.7" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.7.7.7.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.7.7.7.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.7.7.7.10" class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr id="S4.T7.8.8.8" class="ltx_tr">
<td id="S4.T7.8.8.8.2" class="ltx_td ltx_align_left">MT-NLG&nbsp;(530B)</td>
<td id="S4.T7.8.8.8.3" class="ltx_td ltx_align_right">64 K→3.75M</td>
<td id="S4.T7.8.8.8.1" class="ltx_td ltx_align_right"><math id="S4.T7.8.8.8.1.m1.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S4.T7.8.8.8.1.m1.1a"><mrow id="S4.T7.8.8.8.1.m1.1.1" xref="S4.T7.8.8.8.1.m1.1.1.cmml"><mn id="S4.T7.8.8.8.1.m1.1.1.2" xref="S4.T7.8.8.8.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.8.8.8.1.m1.1.1.1" xref="S4.T7.8.8.8.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.8.8.8.1.m1.1.1.3" xref="S4.T7.8.8.8.1.m1.1.1.3.cmml"><mn id="S4.T7.8.8.8.1.m1.1.1.3.2" xref="S4.T7.8.8.8.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.8.8.8.1.m1.1.1.3.3" xref="S4.T7.8.8.8.1.m1.1.1.3.3.cmml"><mo id="S4.T7.8.8.8.1.m1.1.1.3.3a" xref="S4.T7.8.8.8.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.8.8.8.1.m1.1.1.3.3.2" xref="S4.T7.8.8.8.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.8.8.8.1.m1.1b"><apply id="S4.T7.8.8.8.1.m1.1.1.cmml" xref="S4.T7.8.8.8.1.m1.1.1"><times id="S4.T7.8.8.8.1.m1.1.1.1.cmml" xref="S4.T7.8.8.8.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.8.8.8.1.m1.1.1.2.cmml" xref="S4.T7.8.8.8.1.m1.1.1.2">5</cn><apply id="S4.T7.8.8.8.1.m1.1.1.3.cmml" xref="S4.T7.8.8.8.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.8.8.8.1.m1.1.1.3.1.cmml" xref="S4.T7.8.8.8.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.8.8.8.1.m1.1.1.3.2.cmml" xref="S4.T7.8.8.8.1.m1.1.1.3.2">10</cn><apply id="S4.T7.8.8.8.1.m1.1.1.3.3.cmml" xref="S4.T7.8.8.8.1.m1.1.1.3.3"><minus id="S4.T7.8.8.8.1.m1.1.1.3.3.1.cmml" xref="S4.T7.8.8.8.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.8.8.8.1.m1.1.1.3.3.2.cmml" xref="S4.T7.8.8.8.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.8.8.8.1.m1.1c">5\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.8.8.8.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.8.8.8.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.8.8.8.6" class="ltx_td ltx_align_center">Adam</td>
<td id="S4.T7.8.8.8.7" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.8.8.8.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.8.8.8.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.8.8.8.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.9.9.9" class="ltx_tr">
<td id="S4.T7.9.9.9.2" class="ltx_td ltx_align_left">Gopher&nbsp;(280B)</td>
<td id="S4.T7.9.9.9.3" class="ltx_td ltx_align_right">3M→6M</td>
<td id="S4.T7.9.9.9.1" class="ltx_td ltx_align_right"><math id="S4.T7.9.9.9.1.m1.1" class="ltx_Math" alttext="4\times 10^{-5}" display="inline"><semantics id="S4.T7.9.9.9.1.m1.1a"><mrow id="S4.T7.9.9.9.1.m1.1.1" xref="S4.T7.9.9.9.1.m1.1.1.cmml"><mn id="S4.T7.9.9.9.1.m1.1.1.2" xref="S4.T7.9.9.9.1.m1.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.9.9.9.1.m1.1.1.1" xref="S4.T7.9.9.9.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.9.9.9.1.m1.1.1.3" xref="S4.T7.9.9.9.1.m1.1.1.3.cmml"><mn id="S4.T7.9.9.9.1.m1.1.1.3.2" xref="S4.T7.9.9.9.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.9.9.9.1.m1.1.1.3.3" xref="S4.T7.9.9.9.1.m1.1.1.3.3.cmml"><mo id="S4.T7.9.9.9.1.m1.1.1.3.3a" xref="S4.T7.9.9.9.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.9.9.9.1.m1.1.1.3.3.2" xref="S4.T7.9.9.9.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.9.9.9.1.m1.1b"><apply id="S4.T7.9.9.9.1.m1.1.1.cmml" xref="S4.T7.9.9.9.1.m1.1.1"><times id="S4.T7.9.9.9.1.m1.1.1.1.cmml" xref="S4.T7.9.9.9.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.9.9.9.1.m1.1.1.2.cmml" xref="S4.T7.9.9.9.1.m1.1.1.2">4</cn><apply id="S4.T7.9.9.9.1.m1.1.1.3.cmml" xref="S4.T7.9.9.9.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.9.9.9.1.m1.1.1.3.1.cmml" xref="S4.T7.9.9.9.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.9.9.9.1.m1.1.1.3.2.cmml" xref="S4.T7.9.9.9.1.m1.1.1.3.2">10</cn><apply id="S4.T7.9.9.9.1.m1.1.1.3.3.cmml" xref="S4.T7.9.9.9.1.m1.1.1.3.3"><minus id="S4.T7.9.9.9.1.m1.1.1.3.3.1.cmml" xref="S4.T7.9.9.9.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.9.9.9.1.m1.1.1.3.3.2.cmml" xref="S4.T7.9.9.9.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.9.9.9.1.m1.1c">4\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.9.9.9.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.9.9.9.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.9.9.9.6" class="ltx_td ltx_align_center">Adam</td>
<td id="S4.T7.9.9.9.7" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.9.9.9.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.9.9.9.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.9.9.9.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.10.10.10" class="ltx_tr">
<td id="S4.T7.10.10.10.2" class="ltx_td ltx_align_left">Chinchilla&nbsp;(70B)</td>
<td id="S4.T7.10.10.10.3" class="ltx_td ltx_align_right">1.5M→3M</td>
<td id="S4.T7.10.10.10.1" class="ltx_td ltx_align_right"><math id="S4.T7.10.10.10.1.m1.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.T7.10.10.10.1.m1.1a"><mrow id="S4.T7.10.10.10.1.m1.1.1" xref="S4.T7.10.10.10.1.m1.1.1.cmml"><mn id="S4.T7.10.10.10.1.m1.1.1.2" xref="S4.T7.10.10.10.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.10.10.10.1.m1.1.1.1" xref="S4.T7.10.10.10.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.10.10.10.1.m1.1.1.3" xref="S4.T7.10.10.10.1.m1.1.1.3.cmml"><mn id="S4.T7.10.10.10.1.m1.1.1.3.2" xref="S4.T7.10.10.10.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.10.10.10.1.m1.1.1.3.3" xref="S4.T7.10.10.10.1.m1.1.1.3.3.cmml"><mo id="S4.T7.10.10.10.1.m1.1.1.3.3a" xref="S4.T7.10.10.10.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.10.10.10.1.m1.1.1.3.3.2" xref="S4.T7.10.10.10.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.10.10.10.1.m1.1b"><apply id="S4.T7.10.10.10.1.m1.1.1.cmml" xref="S4.T7.10.10.10.1.m1.1.1"><times id="S4.T7.10.10.10.1.m1.1.1.1.cmml" xref="S4.T7.10.10.10.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.10.10.10.1.m1.1.1.2.cmml" xref="S4.T7.10.10.10.1.m1.1.1.2">1</cn><apply id="S4.T7.10.10.10.1.m1.1.1.3.cmml" xref="S4.T7.10.10.10.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.10.10.10.1.m1.1.1.3.1.cmml" xref="S4.T7.10.10.10.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.10.10.10.1.m1.1.1.3.2.cmml" xref="S4.T7.10.10.10.1.m1.1.1.3.2">10</cn><apply id="S4.T7.10.10.10.1.m1.1.1.3.3.cmml" xref="S4.T7.10.10.10.1.m1.1.1.3.3"><minus id="S4.T7.10.10.10.1.m1.1.1.3.3.1.cmml" xref="S4.T7.10.10.10.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.10.10.10.1.m1.1.1.3.3.2.cmml" xref="S4.T7.10.10.10.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.10.10.10.1.m1.1c">1\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T7.10.10.10.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.10.10.10.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.10.10.10.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.10.10.10.7" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.10.10.10.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.10.10.10.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.10.10.10.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.11.11.11" class="ltx_tr">
<td id="S4.T7.11.11.11.2" class="ltx_td ltx_align_left">Galactica&nbsp;(120B)</td>
<td id="S4.T7.11.11.11.3" class="ltx_td ltx_align_right">2M</td>
<td id="S4.T7.11.11.11.1" class="ltx_td ltx_align_right"><math id="S4.T7.11.11.11.1.m1.1" class="ltx_Math" alttext="7\times 10^{-6}" display="inline"><semantics id="S4.T7.11.11.11.1.m1.1a"><mrow id="S4.T7.11.11.11.1.m1.1.1" xref="S4.T7.11.11.11.1.m1.1.1.cmml"><mn id="S4.T7.11.11.11.1.m1.1.1.2" xref="S4.T7.11.11.11.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.11.11.11.1.m1.1.1.1" xref="S4.T7.11.11.11.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.11.11.11.1.m1.1.1.3" xref="S4.T7.11.11.11.1.m1.1.1.3.cmml"><mn id="S4.T7.11.11.11.1.m1.1.1.3.2" xref="S4.T7.11.11.11.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.11.11.11.1.m1.1.1.3.3" xref="S4.T7.11.11.11.1.m1.1.1.3.3.cmml"><mo id="S4.T7.11.11.11.1.m1.1.1.3.3a" xref="S4.T7.11.11.11.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.11.11.11.1.m1.1.1.3.3.2" xref="S4.T7.11.11.11.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.11.11.11.1.m1.1b"><apply id="S4.T7.11.11.11.1.m1.1.1.cmml" xref="S4.T7.11.11.11.1.m1.1.1"><times id="S4.T7.11.11.11.1.m1.1.1.1.cmml" xref="S4.T7.11.11.11.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.11.11.11.1.m1.1.1.2.cmml" xref="S4.T7.11.11.11.1.m1.1.1.2">7</cn><apply id="S4.T7.11.11.11.1.m1.1.1.3.cmml" xref="S4.T7.11.11.11.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.11.11.11.1.m1.1.1.3.1.cmml" xref="S4.T7.11.11.11.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.11.11.11.1.m1.1.1.3.2.cmml" xref="S4.T7.11.11.11.1.m1.1.1.3.2">10</cn><apply id="S4.T7.11.11.11.1.m1.1.1.3.3.cmml" xref="S4.T7.11.11.11.1.m1.1.1.3.3"><minus id="S4.T7.11.11.11.1.m1.1.1.3.3.1.cmml" xref="S4.T7.11.11.11.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.11.11.11.1.m1.1.1.3.3.2.cmml" xref="S4.T7.11.11.11.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.11.11.11.1.m1.1c">7\times 10^{-6}</annotation></semantics></math></td>
<td id="S4.T7.11.11.11.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.11.11.11.5" class="ltx_td ltx_align_center">linear decay to 10%</td>
<td id="S4.T7.11.11.11.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.11.11.11.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.11.11.11.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.11.11.11.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.11.11.11.10" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="S4.T7.20.20.22" class="ltx_tr">
<td id="S4.T7.20.20.22.1" class="ltx_td ltx_align_left">LaMDA&nbsp;(137B)</td>
<td id="S4.T7.20.20.22.2" class="ltx_td ltx_align_right">256K</td>
<td id="S4.T7.20.20.22.3" class="ltx_td ltx_align_right">-</td>
<td id="S4.T7.20.20.22.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.20.20.22.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.20.20.22.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.20.20.22.7" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.20.20.22.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.20.20.22.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.20.20.22.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.12.12.12" class="ltx_tr">
<td id="S4.T7.12.12.12.2" class="ltx_td ltx_align_left">Jurassic-1&nbsp;(178B)</td>
<td id="S4.T7.12.12.12.3" class="ltx_td ltx_align_right">32 K→3.2M</td>
<td id="S4.T7.12.12.12.1" class="ltx_td ltx_align_right"><math id="S4.T7.12.12.12.1.m1.1" class="ltx_Math" alttext="6\times 10^{-5}" display="inline"><semantics id="S4.T7.12.12.12.1.m1.1a"><mrow id="S4.T7.12.12.12.1.m1.1.1" xref="S4.T7.12.12.12.1.m1.1.1.cmml"><mn id="S4.T7.12.12.12.1.m1.1.1.2" xref="S4.T7.12.12.12.1.m1.1.1.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.12.12.12.1.m1.1.1.1" xref="S4.T7.12.12.12.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.12.12.12.1.m1.1.1.3" xref="S4.T7.12.12.12.1.m1.1.1.3.cmml"><mn id="S4.T7.12.12.12.1.m1.1.1.3.2" xref="S4.T7.12.12.12.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.12.12.12.1.m1.1.1.3.3" xref="S4.T7.12.12.12.1.m1.1.1.3.3.cmml"><mo id="S4.T7.12.12.12.1.m1.1.1.3.3a" xref="S4.T7.12.12.12.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.12.12.12.1.m1.1.1.3.3.2" xref="S4.T7.12.12.12.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.12.12.12.1.m1.1b"><apply id="S4.T7.12.12.12.1.m1.1.1.cmml" xref="S4.T7.12.12.12.1.m1.1.1"><times id="S4.T7.12.12.12.1.m1.1.1.1.cmml" xref="S4.T7.12.12.12.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.12.12.12.1.m1.1.1.2.cmml" xref="S4.T7.12.12.12.1.m1.1.1.2">6</cn><apply id="S4.T7.12.12.12.1.m1.1.1.3.cmml" xref="S4.T7.12.12.12.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.12.12.12.1.m1.1.1.3.1.cmml" xref="S4.T7.12.12.12.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.12.12.12.1.m1.1.1.3.2.cmml" xref="S4.T7.12.12.12.1.m1.1.1.3.2">10</cn><apply id="S4.T7.12.12.12.1.m1.1.1.3.3.cmml" xref="S4.T7.12.12.12.1.m1.1.1.3.3"><minus id="S4.T7.12.12.12.1.m1.1.1.3.3.1.cmml" xref="S4.T7.12.12.12.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.12.12.12.1.m1.1.1.3.3.2.cmml" xref="S4.T7.12.12.12.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.12.12.12.1.m1.1c">6\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.12.12.12.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.12.12.12.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.12.12.12.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.12.12.12.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.12.12.12.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.12.12.12.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.12.12.12.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.13.13.13" class="ltx_tr">
<td id="S4.T7.13.13.13.2" class="ltx_td ltx_align_left">LLaMA&nbsp;(65B)</td>
<td id="S4.T7.13.13.13.3" class="ltx_td ltx_align_right">4M</td>
<td id="S4.T7.13.13.13.1" class="ltx_td ltx_align_right"><math id="S4.T7.13.13.13.1.m1.1" class="ltx_Math" alttext="1.5\times 10^{-4}" display="inline"><semantics id="S4.T7.13.13.13.1.m1.1a"><mrow id="S4.T7.13.13.13.1.m1.1.1" xref="S4.T7.13.13.13.1.m1.1.1.cmml"><mn id="S4.T7.13.13.13.1.m1.1.1.2" xref="S4.T7.13.13.13.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.13.13.13.1.m1.1.1.1" xref="S4.T7.13.13.13.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.13.13.13.1.m1.1.1.3" xref="S4.T7.13.13.13.1.m1.1.1.3.cmml"><mn id="S4.T7.13.13.13.1.m1.1.1.3.2" xref="S4.T7.13.13.13.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.13.13.13.1.m1.1.1.3.3" xref="S4.T7.13.13.13.1.m1.1.1.3.3.cmml"><mo id="S4.T7.13.13.13.1.m1.1.1.3.3a" xref="S4.T7.13.13.13.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.13.13.13.1.m1.1.1.3.3.2" xref="S4.T7.13.13.13.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.13.13.13.1.m1.1b"><apply id="S4.T7.13.13.13.1.m1.1.1.cmml" xref="S4.T7.13.13.13.1.m1.1.1"><times id="S4.T7.13.13.13.1.m1.1.1.1.cmml" xref="S4.T7.13.13.13.1.m1.1.1.1"></times><cn type="float" id="S4.T7.13.13.13.1.m1.1.1.2.cmml" xref="S4.T7.13.13.13.1.m1.1.1.2">1.5</cn><apply id="S4.T7.13.13.13.1.m1.1.1.3.cmml" xref="S4.T7.13.13.13.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.13.13.13.1.m1.1.1.3.1.cmml" xref="S4.T7.13.13.13.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.13.13.13.1.m1.1.1.3.2.cmml" xref="S4.T7.13.13.13.1.m1.1.1.3.2">10</cn><apply id="S4.T7.13.13.13.1.m1.1.1.3.3.cmml" xref="S4.T7.13.13.13.1.m1.1.1.3.3"><minus id="S4.T7.13.13.13.1.m1.1.1.3.3.1.cmml" xref="S4.T7.13.13.13.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.13.13.13.1.m1.1.1.3.3.2.cmml" xref="S4.T7.13.13.13.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.13.13.13.1.m1.1c">1.5\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T7.13.13.13.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.13.13.13.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.13.13.13.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.13.13.13.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.13.13.13.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.13.13.13.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.13.13.13.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.14.14.14" class="ltx_tr">
<td id="S4.T7.14.14.14.2" class="ltx_td ltx_align_left">LLaMA 2&nbsp;(70B)</td>
<td id="S4.T7.14.14.14.3" class="ltx_td ltx_align_right">4M</td>
<td id="S4.T7.14.14.14.1" class="ltx_td ltx_align_right"><math id="S4.T7.14.14.14.1.m1.1" class="ltx_Math" alttext="1.5\times 10^{-4}" display="inline"><semantics id="S4.T7.14.14.14.1.m1.1a"><mrow id="S4.T7.14.14.14.1.m1.1.1" xref="S4.T7.14.14.14.1.m1.1.1.cmml"><mn id="S4.T7.14.14.14.1.m1.1.1.2" xref="S4.T7.14.14.14.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.14.14.14.1.m1.1.1.1" xref="S4.T7.14.14.14.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.14.14.14.1.m1.1.1.3" xref="S4.T7.14.14.14.1.m1.1.1.3.cmml"><mn id="S4.T7.14.14.14.1.m1.1.1.3.2" xref="S4.T7.14.14.14.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.14.14.14.1.m1.1.1.3.3" xref="S4.T7.14.14.14.1.m1.1.1.3.3.cmml"><mo id="S4.T7.14.14.14.1.m1.1.1.3.3a" xref="S4.T7.14.14.14.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.14.14.14.1.m1.1.1.3.3.2" xref="S4.T7.14.14.14.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.14.14.14.1.m1.1b"><apply id="S4.T7.14.14.14.1.m1.1.1.cmml" xref="S4.T7.14.14.14.1.m1.1.1"><times id="S4.T7.14.14.14.1.m1.1.1.1.cmml" xref="S4.T7.14.14.14.1.m1.1.1.1"></times><cn type="float" id="S4.T7.14.14.14.1.m1.1.1.2.cmml" xref="S4.T7.14.14.14.1.m1.1.1.2">1.5</cn><apply id="S4.T7.14.14.14.1.m1.1.1.3.cmml" xref="S4.T7.14.14.14.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.14.14.14.1.m1.1.1.3.1.cmml" xref="S4.T7.14.14.14.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.14.14.14.1.m1.1.1.3.2.cmml" xref="S4.T7.14.14.14.1.m1.1.1.3.2">10</cn><apply id="S4.T7.14.14.14.1.m1.1.1.3.3.cmml" xref="S4.T7.14.14.14.1.m1.1.1.3.3"><minus id="S4.T7.14.14.14.1.m1.1.1.3.3.1.cmml" xref="S4.T7.14.14.14.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.14.14.14.1.m1.1.1.3.3.2.cmml" xref="S4.T7.14.14.14.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.14.14.14.1.m1.1c">1.5\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T7.14.14.14.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.14.14.14.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.14.14.14.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.14.14.14.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.14.14.14.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.14.14.14.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.14.14.14.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.15.15.15" class="ltx_tr">
<td id="S4.T7.15.15.15.2" class="ltx_td ltx_align_left">Falcon&nbsp;(40B)</td>
<td id="S4.T7.15.15.15.3" class="ltx_td ltx_align_right">2M</td>
<td id="S4.T7.15.15.15.1" class="ltx_td ltx_align_right"><math id="S4.T7.15.15.15.1.m1.1" class="ltx_Math" alttext="1.85\times 10^{-4}" display="inline"><semantics id="S4.T7.15.15.15.1.m1.1a"><mrow id="S4.T7.15.15.15.1.m1.1.1" xref="S4.T7.15.15.15.1.m1.1.1.cmml"><mn id="S4.T7.15.15.15.1.m1.1.1.2" xref="S4.T7.15.15.15.1.m1.1.1.2.cmml">1.85</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.15.15.15.1.m1.1.1.1" xref="S4.T7.15.15.15.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.15.15.15.1.m1.1.1.3" xref="S4.T7.15.15.15.1.m1.1.1.3.cmml"><mn id="S4.T7.15.15.15.1.m1.1.1.3.2" xref="S4.T7.15.15.15.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.15.15.15.1.m1.1.1.3.3" xref="S4.T7.15.15.15.1.m1.1.1.3.3.cmml"><mo id="S4.T7.15.15.15.1.m1.1.1.3.3a" xref="S4.T7.15.15.15.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.15.15.15.1.m1.1.1.3.3.2" xref="S4.T7.15.15.15.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.15.15.15.1.m1.1b"><apply id="S4.T7.15.15.15.1.m1.1.1.cmml" xref="S4.T7.15.15.15.1.m1.1.1"><times id="S4.T7.15.15.15.1.m1.1.1.1.cmml" xref="S4.T7.15.15.15.1.m1.1.1.1"></times><cn type="float" id="S4.T7.15.15.15.1.m1.1.1.2.cmml" xref="S4.T7.15.15.15.1.m1.1.1.2">1.85</cn><apply id="S4.T7.15.15.15.1.m1.1.1.3.cmml" xref="S4.T7.15.15.15.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.15.15.15.1.m1.1.1.3.1.cmml" xref="S4.T7.15.15.15.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.15.15.15.1.m1.1.1.3.2.cmml" xref="S4.T7.15.15.15.1.m1.1.1.3.2">10</cn><apply id="S4.T7.15.15.15.1.m1.1.1.3.3.cmml" xref="S4.T7.15.15.15.1.m1.1.1.3.3"><minus id="S4.T7.15.15.15.1.m1.1.1.3.3.1.cmml" xref="S4.T7.15.15.15.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.15.15.15.1.m1.1.1.3.3.2.cmml" xref="S4.T7.15.15.15.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.15.15.15.1.m1.1c">1.85\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T7.15.15.15.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.15.15.15.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.15.15.15.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.15.15.15.7" class="ltx_td ltx_align_center">BF16</td>
<td id="S4.T7.15.15.15.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.15.15.15.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.15.15.15.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.16.16.16" class="ltx_tr">
<td id="S4.T7.16.16.16.2" class="ltx_td ltx_align_left">GLM&nbsp;(130B)</td>
<td id="S4.T7.16.16.16.3" class="ltx_td ltx_align_right">0.4M→8.25M</td>
<td id="S4.T7.16.16.16.1" class="ltx_td ltx_align_right"><math id="S4.T7.16.16.16.1.m1.1" class="ltx_Math" alttext="8\times 10^{-5}" display="inline"><semantics id="S4.T7.16.16.16.1.m1.1a"><mrow id="S4.T7.16.16.16.1.m1.1.1" xref="S4.T7.16.16.16.1.m1.1.1.cmml"><mn id="S4.T7.16.16.16.1.m1.1.1.2" xref="S4.T7.16.16.16.1.m1.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.16.16.16.1.m1.1.1.1" xref="S4.T7.16.16.16.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.16.16.16.1.m1.1.1.3" xref="S4.T7.16.16.16.1.m1.1.1.3.cmml"><mn id="S4.T7.16.16.16.1.m1.1.1.3.2" xref="S4.T7.16.16.16.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.16.16.16.1.m1.1.1.3.3" xref="S4.T7.16.16.16.1.m1.1.1.3.3.cmml"><mo id="S4.T7.16.16.16.1.m1.1.1.3.3a" xref="S4.T7.16.16.16.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.16.16.16.1.m1.1.1.3.3.2" xref="S4.T7.16.16.16.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.16.16.16.1.m1.1b"><apply id="S4.T7.16.16.16.1.m1.1.1.cmml" xref="S4.T7.16.16.16.1.m1.1.1"><times id="S4.T7.16.16.16.1.m1.1.1.1.cmml" xref="S4.T7.16.16.16.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.16.16.16.1.m1.1.1.2.cmml" xref="S4.T7.16.16.16.1.m1.1.1.2">8</cn><apply id="S4.T7.16.16.16.1.m1.1.1.3.cmml" xref="S4.T7.16.16.16.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.16.16.16.1.m1.1.1.3.1.cmml" xref="S4.T7.16.16.16.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.16.16.16.1.m1.1.1.3.2.cmml" xref="S4.T7.16.16.16.1.m1.1.1.3.2">10</cn><apply id="S4.T7.16.16.16.1.m1.1.1.3.3.cmml" xref="S4.T7.16.16.16.1.m1.1.1.3.3"><minus id="S4.T7.16.16.16.1.m1.1.1.3.3.1.cmml" xref="S4.T7.16.16.16.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.16.16.16.1.m1.1.1.3.3.2.cmml" xref="S4.T7.16.16.16.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.16.16.16.1.m1.1c">8\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.16.16.16.4" class="ltx_td ltx_align_center">yes</td>
<td id="S4.T7.16.16.16.5" class="ltx_td ltx_align_center">cosine decay to 10%</td>
<td id="S4.T7.16.16.16.6" class="ltx_td ltx_align_center">AdamW</td>
<td id="S4.T7.16.16.16.7" class="ltx_td ltx_align_center">FP16</td>
<td id="S4.T7.16.16.16.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.16.16.16.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.16.16.16.10" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="S4.T7.17.17.17" class="ltx_tr">
<td id="S4.T7.17.17.17.2" class="ltx_td ltx_align_left">T5&nbsp;(11B)</td>
<td id="S4.T7.17.17.17.3" class="ltx_td ltx_align_right">64K</td>
<td id="S4.T7.17.17.17.1" class="ltx_td ltx_align_right"><math id="S4.T7.17.17.17.1.m1.1" class="ltx_Math" alttext="1\times 10^{-2}" display="inline"><semantics id="S4.T7.17.17.17.1.m1.1a"><mrow id="S4.T7.17.17.17.1.m1.1.1" xref="S4.T7.17.17.17.1.m1.1.1.cmml"><mn id="S4.T7.17.17.17.1.m1.1.1.2" xref="S4.T7.17.17.17.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.17.17.17.1.m1.1.1.1" xref="S4.T7.17.17.17.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.17.17.17.1.m1.1.1.3" xref="S4.T7.17.17.17.1.m1.1.1.3.cmml"><mn id="S4.T7.17.17.17.1.m1.1.1.3.2" xref="S4.T7.17.17.17.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.17.17.17.1.m1.1.1.3.3" xref="S4.T7.17.17.17.1.m1.1.1.3.3.cmml"><mo id="S4.T7.17.17.17.1.m1.1.1.3.3a" xref="S4.T7.17.17.17.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.17.17.17.1.m1.1.1.3.3.2" xref="S4.T7.17.17.17.1.m1.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.17.17.17.1.m1.1b"><apply id="S4.T7.17.17.17.1.m1.1.1.cmml" xref="S4.T7.17.17.17.1.m1.1.1"><times id="S4.T7.17.17.17.1.m1.1.1.1.cmml" xref="S4.T7.17.17.17.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.17.17.17.1.m1.1.1.2.cmml" xref="S4.T7.17.17.17.1.m1.1.1.2">1</cn><apply id="S4.T7.17.17.17.1.m1.1.1.3.cmml" xref="S4.T7.17.17.17.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.17.17.17.1.m1.1.1.3.1.cmml" xref="S4.T7.17.17.17.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.17.17.17.1.m1.1.1.3.2.cmml" xref="S4.T7.17.17.17.1.m1.1.1.3.2">10</cn><apply id="S4.T7.17.17.17.1.m1.1.1.3.3.cmml" xref="S4.T7.17.17.17.1.m1.1.1.3.3"><minus id="S4.T7.17.17.17.1.m1.1.1.3.3.1.cmml" xref="S4.T7.17.17.17.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.17.17.17.1.m1.1.1.3.3.2.cmml" xref="S4.T7.17.17.17.1.m1.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.17.17.17.1.m1.1c">1\times 10^{-2}</annotation></semantics></math></td>
<td id="S4.T7.17.17.17.4" class="ltx_td ltx_align_center">no</td>
<td id="S4.T7.17.17.17.5" class="ltx_td ltx_align_center">inverse square root</td>
<td id="S4.T7.17.17.17.6" class="ltx_td ltx_align_center">AdaFactor</td>
<td id="S4.T7.17.17.17.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.17.17.17.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.17.17.17.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.17.17.17.10" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="S4.T7.18.18.18" class="ltx_tr">
<td id="S4.T7.18.18.18.2" class="ltx_td ltx_align_left">ERNIE 3.0 Titan&nbsp;(260B)</td>
<td id="S4.T7.18.18.18.3" class="ltx_td ltx_align_right">-</td>
<td id="S4.T7.18.18.18.1" class="ltx_td ltx_align_right"><math id="S4.T7.18.18.18.1.m1.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.T7.18.18.18.1.m1.1a"><mrow id="S4.T7.18.18.18.1.m1.1.1" xref="S4.T7.18.18.18.1.m1.1.1.cmml"><mn id="S4.T7.18.18.18.1.m1.1.1.2" xref="S4.T7.18.18.18.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.18.18.18.1.m1.1.1.1" xref="S4.T7.18.18.18.1.m1.1.1.1.cmml">×</mo><msup id="S4.T7.18.18.18.1.m1.1.1.3" xref="S4.T7.18.18.18.1.m1.1.1.3.cmml"><mn id="S4.T7.18.18.18.1.m1.1.1.3.2" xref="S4.T7.18.18.18.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.18.18.18.1.m1.1.1.3.3" xref="S4.T7.18.18.18.1.m1.1.1.3.3.cmml"><mo id="S4.T7.18.18.18.1.m1.1.1.3.3a" xref="S4.T7.18.18.18.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.18.18.18.1.m1.1.1.3.3.2" xref="S4.T7.18.18.18.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.18.18.18.1.m1.1b"><apply id="S4.T7.18.18.18.1.m1.1.1.cmml" xref="S4.T7.18.18.18.1.m1.1.1"><times id="S4.T7.18.18.18.1.m1.1.1.1.cmml" xref="S4.T7.18.18.18.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.18.18.18.1.m1.1.1.2.cmml" xref="S4.T7.18.18.18.1.m1.1.1.2">1</cn><apply id="S4.T7.18.18.18.1.m1.1.1.3.cmml" xref="S4.T7.18.18.18.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.18.18.18.1.m1.1.1.3.1.cmml" xref="S4.T7.18.18.18.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.18.18.18.1.m1.1.1.3.2.cmml" xref="S4.T7.18.18.18.1.m1.1.1.3.2">10</cn><apply id="S4.T7.18.18.18.1.m1.1.1.3.3.cmml" xref="S4.T7.18.18.18.1.m1.1.1.3.3"><minus id="S4.T7.18.18.18.1.m1.1.1.3.3.1.cmml" xref="S4.T7.18.18.18.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.18.18.18.1.m1.1.1.3.3.2.cmml" xref="S4.T7.18.18.18.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.18.18.18.1.m1.1c">1\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T7.18.18.18.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.18.18.18.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T7.18.18.18.6" class="ltx_td ltx_align_center">Adam</td>
<td id="S4.T7.18.18.18.7" class="ltx_td ltx_align_center">FP16</td>
<td id="S4.T7.18.18.18.8" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T7.18.18.18.9" class="ltx_td ltx_align_center">1.0</td>
<td id="S4.T7.18.18.18.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T7.20.20.20" class="ltx_tr">
<td id="S4.T7.19.19.19.1" class="ltx_td ltx_align_left ltx_border_bb">PanGu-<math id="S4.T7.19.19.19.1.m1.1" class="ltx_Math" alttext="\Sigma" display="inline"><semantics id="S4.T7.19.19.19.1.m1.1a"><mi mathvariant="normal" id="S4.T7.19.19.19.1.m1.1.1" xref="S4.T7.19.19.19.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S4.T7.19.19.19.1.m1.1b"><ci id="S4.T7.19.19.19.1.m1.1.1.cmml" xref="S4.T7.19.19.19.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.19.19.19.1.m1.1c">\Sigma</annotation></semantics></math>&nbsp;(1.085T)</td>
<td id="S4.T7.20.20.20.3" class="ltx_td ltx_align_right ltx_border_bb">0.5M</td>
<td id="S4.T7.20.20.20.2" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T7.20.20.20.2.m1.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S4.T7.20.20.20.2.m1.1a"><mrow id="S4.T7.20.20.20.2.m1.1.1" xref="S4.T7.20.20.20.2.m1.1.1.cmml"><mn id="S4.T7.20.20.20.2.m1.1.1.2" xref="S4.T7.20.20.20.2.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.20.20.20.2.m1.1.1.1" xref="S4.T7.20.20.20.2.m1.1.1.1.cmml">×</mo><msup id="S4.T7.20.20.20.2.m1.1.1.3" xref="S4.T7.20.20.20.2.m1.1.1.3.cmml"><mn id="S4.T7.20.20.20.2.m1.1.1.3.2" xref="S4.T7.20.20.20.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T7.20.20.20.2.m1.1.1.3.3" xref="S4.T7.20.20.20.2.m1.1.1.3.3.cmml"><mo id="S4.T7.20.20.20.2.m1.1.1.3.3a" xref="S4.T7.20.20.20.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T7.20.20.20.2.m1.1.1.3.3.2" xref="S4.T7.20.20.20.2.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.20.20.20.2.m1.1b"><apply id="S4.T7.20.20.20.2.m1.1.1.cmml" xref="S4.T7.20.20.20.2.m1.1.1"><times id="S4.T7.20.20.20.2.m1.1.1.1.cmml" xref="S4.T7.20.20.20.2.m1.1.1.1"></times><cn type="integer" id="S4.T7.20.20.20.2.m1.1.1.2.cmml" xref="S4.T7.20.20.20.2.m1.1.1.2">2</cn><apply id="S4.T7.20.20.20.2.m1.1.1.3.cmml" xref="S4.T7.20.20.20.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T7.20.20.20.2.m1.1.1.3.1.cmml" xref="S4.T7.20.20.20.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T7.20.20.20.2.m1.1.1.3.2.cmml" xref="S4.T7.20.20.20.2.m1.1.1.3.2">10</cn><apply id="S4.T7.20.20.20.2.m1.1.1.3.3.cmml" xref="S4.T7.20.20.20.2.m1.1.1.3.3"><minus id="S4.T7.20.20.20.2.m1.1.1.3.3.1.cmml" xref="S4.T7.20.20.20.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T7.20.20.20.2.m1.1.1.3.3.2.cmml" xref="S4.T7.20.20.20.2.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.20.20.20.2.m1.1c">2\times 10^{-5}</annotation></semantics></math></td>
<td id="S4.T7.20.20.20.4" class="ltx_td ltx_align_center ltx_border_bb">yes</td>
<td id="S4.T7.20.20.20.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T7.20.20.20.6" class="ltx_td ltx_align_center ltx_border_bb">Adam</td>
<td id="S4.T7.20.20.20.7" class="ltx_td ltx_align_center ltx_border_bb">FP16</td>
<td id="S4.T7.20.20.20.8" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T7.20.20.20.9" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T7.20.20.20.10" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</tbody></table>
</span></div>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_italic">Model Training</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this part, we review the important settings, techniques, or tricks for training LLMs.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Optimization Setting</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">For parameter optimization of LLMs, we present the commonly used settings for batch training, learning rate, optimizer, and training stability.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p"><span id="S4.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Batch Training.</span>
For language model pre-training, existing work generally sets the batch size to a large number (<em id="S4.SS3.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> 2,048 examples or 4M tokens) to improve the training stability and throughput.
For LLMs such as GPT-3 and PaLM, they have introduced a new strategy that dynamically increases the batch size during training, ultimately reaching a million scale.
Specifically, the batch size of GPT-3 is gradually increasing from 32K to 3.2M tokens.
Empirical results have demonstrated that the dynamic schedule of batch size can effectively stabilize the training process of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p3.3" class="ltx_p"><span id="S4.SS3.SSS1.p3.3.1" class="ltx_text ltx_font_bold">Learning Rate.</span>
Existing LLMs usually adopt a similar learning rate schedule with the warm-up and decay strategies during pre-training.
Specifically, in the initial 0.1% to 0.5% of the training steps, a linear warm-up schedule is employed for gradually increasing the learning rate to the maximum value that ranges from approximately <math id="S4.SS3.SSS1.p3.1.m1.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S4.SS3.SSS1.p3.1.m1.1a"><mrow id="S4.SS3.SSS1.p3.1.m1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p3.1.m1.1.1.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.SSS1.p3.1.m1.1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS3.SSS1.p3.1.m1.1.1.3" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.cmml"><mn id="S4.SS3.SSS1.p3.1.m1.1.1.3.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.SSS1.p3.1.m1.1.1.3.3" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.SSS1.p3.1.m1.1.1.3.3a" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS3.SSS1.p3.1.m1.1.1.3.3.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.1.m1.1b"><apply id="S4.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1"><times id="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2">5</cn><apply id="S4.SS3.SSS1.p3.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.2">10</cn><apply id="S4.SS3.SSS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3"><minus id="S4.SS3.SSS1.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS3.SSS1.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.1.m1.1c">5\times 10^{-5}</annotation></semantics></math> to <math id="S4.SS3.SSS1.p3.2.m2.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.SS3.SSS1.p3.2.m2.1a"><mrow id="S4.SS3.SSS1.p3.2.m2.1.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.cmml"><mn id="S4.SS3.SSS1.p3.2.m2.1.1.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.SSS1.p3.2.m2.1.1.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS3.SSS1.p3.2.m2.1.1.3" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.cmml"><mn id="S4.SS3.SSS1.p3.2.m2.1.1.3.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.SSS1.p3.2.m2.1.1.3.3" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.3.cmml"><mo id="S4.SS3.SSS1.p3.2.m2.1.1.3.3a" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS3.SSS1.p3.2.m2.1.1.3.3.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.2.m2.1b"><apply id="S4.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1"><times id="S4.SS3.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.2">1</cn><apply id="S4.SS3.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.SSS1.p3.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.2">10</cn><apply id="S4.SS3.SSS1.p3.2.m2.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.3"><minus id="S4.SS3.SSS1.p3.2.m2.1.1.3.3.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS3.SSS1.p3.2.m2.1.1.3.3.2.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.2.m2.1c">1\times 10^{-4}</annotation></semantics></math> (<em id="S4.SS3.SSS1.p3.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> <math id="S4.SS3.SSS1.p3.3.m3.1" class="ltx_Math" alttext="6\times 10^{-5}" display="inline"><semantics id="S4.SS3.SSS1.p3.3.m3.1a"><mrow id="S4.SS3.SSS1.p3.3.m3.1.1" xref="S4.SS3.SSS1.p3.3.m3.1.1.cmml"><mn id="S4.SS3.SSS1.p3.3.m3.1.1.2" xref="S4.SS3.SSS1.p3.3.m3.1.1.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.SSS1.p3.3.m3.1.1.1" xref="S4.SS3.SSS1.p3.3.m3.1.1.1.cmml">×</mo><msup id="S4.SS3.SSS1.p3.3.m3.1.1.3" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.cmml"><mn id="S4.SS3.SSS1.p3.3.m3.1.1.3.2" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.SSS1.p3.3.m3.1.1.3.3" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.3.cmml"><mo id="S4.SS3.SSS1.p3.3.m3.1.1.3.3a" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS3.SSS1.p3.3.m3.1.1.3.3.2" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.3.m3.1b"><apply id="S4.SS3.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1"><times id="S4.SS3.SSS1.p3.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.1"></times><cn type="integer" id="S4.SS3.SSS1.p3.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.2">6</cn><apply id="S4.SS3.SSS1.p3.3.m3.1.1.3.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.3.m3.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.SSS1.p3.3.m3.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.2">10</cn><apply id="S4.SS3.SSS1.p3.3.m3.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.3"><minus id="S4.SS3.SSS1.p3.3.m3.1.1.3.3.1.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS3.SSS1.p3.3.m3.1.1.3.3.2.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.3.m3.1c">6\times 10^{-5}</annotation></semantics></math> for GPT-3).
Then, a cosine decay strategy is adopted in the subsequent steps, gradually reducing the learning rate to approximately 10% of its maximum value, until the convergence of the training loss.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p4.6" class="ltx_p"><span id="S4.SS3.SSS1.p4.6.1" class="ltx_text ltx_font_bold">Optimizer.</span>
The Adam optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib328" title="" class="ltx_ref">328</a>]</cite> and AdamW optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib329" title="" class="ltx_ref">329</a>]</cite> are widely utilized for training LLMs (<em id="S4.SS3.SSS1.p4.6.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-3), which are based on adaptive estimates of lower-order moments for first-order gradient-based optimization.
Commonly, its hyper-parameters are set as follows: <math id="S4.SS3.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS3.SSS1.p4.1.m1.1a"><mrow id="S4.SS3.SSS1.p4.1.m1.1.1" xref="S4.SS3.SSS1.p4.1.m1.1.1.cmml"><msub id="S4.SS3.SSS1.p4.1.m1.1.1.2" xref="S4.SS3.SSS1.p4.1.m1.1.1.2.cmml"><mi id="S4.SS3.SSS1.p4.1.m1.1.1.2.2" xref="S4.SS3.SSS1.p4.1.m1.1.1.2.2.cmml">β</mi><mn id="S4.SS3.SSS1.p4.1.m1.1.1.2.3" xref="S4.SS3.SSS1.p4.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS3.SSS1.p4.1.m1.1.1.1" xref="S4.SS3.SSS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p4.1.m1.1.1.3" xref="S4.SS3.SSS1.p4.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.1.m1.1b"><apply id="S4.SS3.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1"><eq id="S4.SS3.SSS1.p4.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1.1"></eq><apply id="S4.SS3.SSS1.p4.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p4.1.m1.1.1.2.1.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p4.1.m1.1.1.2.2.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS3.SSS1.p4.1.m1.1.1.2.3.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS3.SSS1.p4.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S4.SS3.SSS1.p4.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.95" display="inline"><semantics id="S4.SS3.SSS1.p4.2.m2.1a"><mrow id="S4.SS3.SSS1.p4.2.m2.1.1" xref="S4.SS3.SSS1.p4.2.m2.1.1.cmml"><msub id="S4.SS3.SSS1.p4.2.m2.1.1.2" xref="S4.SS3.SSS1.p4.2.m2.1.1.2.cmml"><mi id="S4.SS3.SSS1.p4.2.m2.1.1.2.2" xref="S4.SS3.SSS1.p4.2.m2.1.1.2.2.cmml">β</mi><mn id="S4.SS3.SSS1.p4.2.m2.1.1.2.3" xref="S4.SS3.SSS1.p4.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS3.SSS1.p4.2.m2.1.1.1" xref="S4.SS3.SSS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p4.2.m2.1.1.3" xref="S4.SS3.SSS1.p4.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.2.m2.1b"><apply id="S4.SS3.SSS1.p4.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1"><eq id="S4.SS3.SSS1.p4.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1.1"></eq><apply id="S4.SS3.SSS1.p4.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p4.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p4.2.m2.1.1.2.2.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS3.SSS1.p4.2.m2.1.1.2.3.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS3.SSS1.p4.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p4.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.2.m2.1c">\beta_{2}=0.95</annotation></semantics></math> and <math id="S4.SS3.SSS1.p4.3.m3.1" class="ltx_Math" alttext="\epsilon=10^{-8}" display="inline"><semantics id="S4.SS3.SSS1.p4.3.m3.1a"><mrow id="S4.SS3.SSS1.p4.3.m3.1.1" xref="S4.SS3.SSS1.p4.3.m3.1.1.cmml"><mi id="S4.SS3.SSS1.p4.3.m3.1.1.2" xref="S4.SS3.SSS1.p4.3.m3.1.1.2.cmml">ϵ</mi><mo id="S4.SS3.SSS1.p4.3.m3.1.1.1" xref="S4.SS3.SSS1.p4.3.m3.1.1.1.cmml">=</mo><msup id="S4.SS3.SSS1.p4.3.m3.1.1.3" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.cmml"><mn id="S4.SS3.SSS1.p4.3.m3.1.1.3.2" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.SSS1.p4.3.m3.1.1.3.3" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.3.cmml"><mo id="S4.SS3.SSS1.p4.3.m3.1.1.3.3a" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS3.SSS1.p4.3.m3.1.1.3.3.2" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.3.m3.1b"><apply id="S4.SS3.SSS1.p4.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1"><eq id="S4.SS3.SSS1.p4.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.1"></eq><ci id="S4.SS3.SSS1.p4.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.2">italic-ϵ</ci><apply id="S4.SS3.SSS1.p4.3.m3.1.1.3.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p4.3.m3.1.1.3.1.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.SSS1.p4.3.m3.1.1.3.2.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.2">10</cn><apply id="S4.SS3.SSS1.p4.3.m3.1.1.3.3.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.3"><minus id="S4.SS3.SSS1.p4.3.m3.1.1.3.3.1.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS3.SSS1.p4.3.m3.1.1.3.3.2.cmml" xref="S4.SS3.SSS1.p4.3.m3.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.3.m3.1c">\epsilon=10^{-8}</annotation></semantics></math>.
Meanwhile, the Adafactor optimizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib330" title="" class="ltx_ref">330</a>]</cite> has also been utilized in training LLMs (<em id="S4.SS3.SSS1.p4.6.3" class="ltx_emph ltx_font_italic">e.g.,</em> PaLM and T5), which is a variant of the Adam optimizer specially designed for conserving GPU memory during training.
The hyper-parameters of the Adafactor optimizer are set as: <math id="S4.SS3.SSS1.p4.4.m4.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS3.SSS1.p4.4.m4.1a"><mrow id="S4.SS3.SSS1.p4.4.m4.1.1" xref="S4.SS3.SSS1.p4.4.m4.1.1.cmml"><msub id="S4.SS3.SSS1.p4.4.m4.1.1.2" xref="S4.SS3.SSS1.p4.4.m4.1.1.2.cmml"><mi id="S4.SS3.SSS1.p4.4.m4.1.1.2.2" xref="S4.SS3.SSS1.p4.4.m4.1.1.2.2.cmml">β</mi><mn id="S4.SS3.SSS1.p4.4.m4.1.1.2.3" xref="S4.SS3.SSS1.p4.4.m4.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS3.SSS1.p4.4.m4.1.1.1" xref="S4.SS3.SSS1.p4.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p4.4.m4.1.1.3" xref="S4.SS3.SSS1.p4.4.m4.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.4.m4.1b"><apply id="S4.SS3.SSS1.p4.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1"><eq id="S4.SS3.SSS1.p4.4.m4.1.1.1.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1.1"></eq><apply id="S4.SS3.SSS1.p4.4.m4.1.1.2.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p4.4.m4.1.1.2.1.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p4.4.m4.1.1.2.2.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS3.SSS1.p4.4.m4.1.1.2.3.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS3.SSS1.p4.4.m4.1.1.3.cmml" xref="S4.SS3.SSS1.p4.4.m4.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.4.m4.1c">\beta_{1}=0.9</annotation></semantics></math> and <math id="S4.SS3.SSS1.p4.5.m5.1" class="ltx_Math" alttext="\beta_{2}=1.0-k^{-0.8}" display="inline"><semantics id="S4.SS3.SSS1.p4.5.m5.1a"><mrow id="S4.SS3.SSS1.p4.5.m5.1.1" xref="S4.SS3.SSS1.p4.5.m5.1.1.cmml"><msub id="S4.SS3.SSS1.p4.5.m5.1.1.2" xref="S4.SS3.SSS1.p4.5.m5.1.1.2.cmml"><mi id="S4.SS3.SSS1.p4.5.m5.1.1.2.2" xref="S4.SS3.SSS1.p4.5.m5.1.1.2.2.cmml">β</mi><mn id="S4.SS3.SSS1.p4.5.m5.1.1.2.3" xref="S4.SS3.SSS1.p4.5.m5.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS3.SSS1.p4.5.m5.1.1.1" xref="S4.SS3.SSS1.p4.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS3.SSS1.p4.5.m5.1.1.3" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.cmml"><mn id="S4.SS3.SSS1.p4.5.m5.1.1.3.2" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.2.cmml">1.0</mn><mo id="S4.SS3.SSS1.p4.5.m5.1.1.3.1" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.1.cmml">−</mo><msup id="S4.SS3.SSS1.p4.5.m5.1.1.3.3" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.cmml"><mi id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.2" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.2.cmml">k</mi><mrow id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.cmml"><mo id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3a" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.cmml">−</mo><mn id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.2" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.2.cmml">0.8</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.5.m5.1b"><apply id="S4.SS3.SSS1.p4.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1"><eq id="S4.SS3.SSS1.p4.5.m5.1.1.1.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.1"></eq><apply id="S4.SS3.SSS1.p4.5.m5.1.1.2.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p4.5.m5.1.1.2.1.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p4.5.m5.1.1.2.2.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS3.SSS1.p4.5.m5.1.1.2.3.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.2.3">2</cn></apply><apply id="S4.SS3.SSS1.p4.5.m5.1.1.3.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3"><minus id="S4.SS3.SSS1.p4.5.m5.1.1.3.1.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.1"></minus><cn type="float" id="S4.SS3.SSS1.p4.5.m5.1.1.3.2.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.2">1.0</cn><apply id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.1.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3">superscript</csymbol><ci id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.2.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.2">𝑘</ci><apply id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3"><minus id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.1.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3"></minus><cn type="float" id="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.2.cmml" xref="S4.SS3.SSS1.p4.5.m5.1.1.3.3.3.2">0.8</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.5.m5.1c">\beta_{2}=1.0-k^{-0.8}</annotation></semantics></math>, where <math id="S4.SS3.SSS1.p4.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.SSS1.p4.6.m6.1a"><mi id="S4.SS3.SSS1.p4.6.m6.1.1" xref="S4.SS3.SSS1.p4.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.6.m6.1b"><ci id="S4.SS3.SSS1.p4.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p4.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.6.m6.1c">k</annotation></semantics></math> denotes the number of training steps.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p5.1" class="ltx_p"><span id="S4.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Stabilizing the Training.</span>
During the pre-training of LLMs, it often suffers from the training instability issue, which may cause the model collapse.
To address this issue, weight decay and gradient clipping have been widely utilized, where existing studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> commonly set the threshold of gradient clipping to 1.0 and weight decay rate to 0.1.
However, with the scaling of LLMs, the training loss spike is also more likely to occur, leading to unstable training.
To mitigate this problem, PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> use a simple strategy that restarts the training process from an earlier checkpoint before the occurrence of the spike and skips over the data that may have caused the problem.
Further, GLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> finds that the abnormal gradients of the embedding layer usually lead to spikes, and proposes to shrink the embedding layer gradients to alleviate it.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Scalable Training Techniques</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">As the model and data sizes increase, it has become challenging to efficiently train LLMs under a limited computational resource.
Especially, two primary technical issues are required to be resolved, <em id="S4.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> increasing training throughput and loading larger models into GPU memory.
In this part, we review several widely used approaches in existing work to address the above two challenges, namely
3D parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib331" title="" class="ltx_ref">331</a>, <a href="#bib.bib332" title="" class="ltx_ref">332</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, ZeRO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib333" title="" class="ltx_ref">333</a>]</cite>, and mixed precision training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib334" title="" class="ltx_ref">334</a>]</cite>, and also give general suggestions about how to utilize them for training.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p"><span id="S4.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">3D Parallelism.</span> 3D parallelism is actually a combination of three commonly used parallel training techniques, namely data parallelism, pipeline parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib331" title="" class="ltx_ref">331</a>, <a href="#bib.bib332" title="" class="ltx_ref">332</a>]</cite>, and tensor parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite><span id="footnote26" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span>Model parallelism is a more broader term that includes tensor parallelism and pipeline parallelism in some work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>.</span></span></span>. We next introduce the three parallel training techniques.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p"><math id="S4.SS3.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS3.SSS2.p3.1.m1.1a"><mo id="S4.SS3.SSS2.p3.1.m1.1.1" xref="S4.SS3.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.1.m1.1b"><ci id="S4.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS3.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">Data parallelism.</em>
Data parallelism is one of the most fundamental approaches to improving the training throughput.
It replicates the model parameters and optimizer states across multiple GPUs and then distributes the whole training corpus into these GPUs.
In this way, each GPU only needs to process the assigned data for it, and performs the forward and backward propagation to obtain the gradients.
The computed gradients on different GPUs will be further aggregated to obtain the gradients of the entire batch for updating the models in all GPUs.
In this way, as the calculations of gradients are independently performed on different GPUs, the data parallelism mechanism is highly scalable, enabling the way that increases the number of GPUs to improve training throughput.
Furthermore, this technique is simple in implementation, and most of existing popular deep learning libraries have already implemented data parallelism, such as TensorFlow and PyTorch.</p>
</div>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p"><math id="S4.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS3.SSS2.p4.1.m1.1a"><mo id="S4.SS3.SSS2.p4.1.m1.1.1" xref="S4.SS3.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p4.1.m1.1b"><ci id="S4.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS3.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">Pipeline parallelism.</em>
Pipeline parallelism aims to distribute the different layers of a LLM into multiple GPUs.
Especially, in the case of a Transformer model, pipeline parallelism loads consecutive layers onto the same GPU, to reduce the cost of transmitting the computed hidden states or gradients between GPUs.
However, a naive implementation of pipeline parallelism may result in a lower GPU utilization rate as each GPU has to wait for the previous one to complete the computation, leading to the unnecessary cost of <em id="S4.SS3.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">bubbles overhead</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib331" title="" class="ltx_ref">331</a>]</cite>.
To reduce these bubbles in pipeline parallelism, GPipe&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib331" title="" class="ltx_ref">331</a>]</cite> and PipeDream&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib332" title="" class="ltx_ref">332</a>]</cite> propose the techniques of padding multiple batches of data and asynchronous gradient update to improve the pipeline efficiency.</p>
</div>
<div id="S4.SS3.SSS2.p5" class="ltx_para">
<p id="S4.SS3.SSS2.p5.8" class="ltx_p"><math id="S4.SS3.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS3.SSS2.p5.1.m1.1a"><mo id="S4.SS3.SSS2.p5.1.m1.1.1" xref="S4.SS3.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.1.m1.1b"><ci id="S4.SS3.SSS2.p5.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S4.SS3.SSS2.p5.8.1" class="ltx_emph ltx_font_italic">Tensor parallelism.</em>
Tensor parallelism is also a commonly used technique that aims to decompose the LLM for multi-GPU loading.
Unlike pipeline parallelism, tensor parallelism focuses on decomposing the tensors (the parameter matrices) of LLMs.
For a matrix multiplication operation <math id="S4.SS3.SSS2.p5.2.m2.1" class="ltx_Math" alttext="Y=XA" display="inline"><semantics id="S4.SS3.SSS2.p5.2.m2.1a"><mrow id="S4.SS3.SSS2.p5.2.m2.1.1" xref="S4.SS3.SSS2.p5.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p5.2.m2.1.1.2" xref="S4.SS3.SSS2.p5.2.m2.1.1.2.cmml">Y</mi><mo id="S4.SS3.SSS2.p5.2.m2.1.1.1" xref="S4.SS3.SSS2.p5.2.m2.1.1.1.cmml">=</mo><mrow id="S4.SS3.SSS2.p5.2.m2.1.1.3" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.cmml"><mi id="S4.SS3.SSS2.p5.2.m2.1.1.3.2" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p5.2.m2.1.1.3.1" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.SSS2.p5.2.m2.1.1.3.3" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.3.cmml">A</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.2.m2.1b"><apply id="S4.SS3.SSS2.p5.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1"><eq id="S4.SS3.SSS2.p5.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.1"></eq><ci id="S4.SS3.SSS2.p5.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.2">𝑌</ci><apply id="S4.SS3.SSS2.p5.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.3"><times id="S4.SS3.SSS2.p5.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.1"></times><ci id="S4.SS3.SSS2.p5.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.2">𝑋</ci><ci id="S4.SS3.SSS2.p5.2.m2.1.1.3.3.cmml" xref="S4.SS3.SSS2.p5.2.m2.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.2.m2.1c">Y=XA</annotation></semantics></math> in the LLM, the parameter matrix <math id="S4.SS3.SSS2.p5.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS3.SSS2.p5.3.m3.1a"><mi id="S4.SS3.SSS2.p5.3.m3.1.1" xref="S4.SS3.SSS2.p5.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.3.m3.1b"><ci id="S4.SS3.SSS2.p5.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p5.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.3.m3.1c">A</annotation></semantics></math> can be split into two submatrices, <math id="S4.SS3.SSS2.p5.4.m4.1" class="ltx_Math" alttext="A_{1}" display="inline"><semantics id="S4.SS3.SSS2.p5.4.m4.1a"><msub id="S4.SS3.SSS2.p5.4.m4.1.1" xref="S4.SS3.SSS2.p5.4.m4.1.1.cmml"><mi id="S4.SS3.SSS2.p5.4.m4.1.1.2" xref="S4.SS3.SSS2.p5.4.m4.1.1.2.cmml">A</mi><mn id="S4.SS3.SSS2.p5.4.m4.1.1.3" xref="S4.SS3.SSS2.p5.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.4.m4.1b"><apply id="S4.SS3.SSS2.p5.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.4.m4.1.1.1.cmml" xref="S4.SS3.SSS2.p5.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.4.m4.1.1.2.cmml" xref="S4.SS3.SSS2.p5.4.m4.1.1.2">𝐴</ci><cn type="integer" id="S4.SS3.SSS2.p5.4.m4.1.1.3.cmml" xref="S4.SS3.SSS2.p5.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.4.m4.1c">A_{1}</annotation></semantics></math> and <math id="S4.SS3.SSS2.p5.5.m5.1" class="ltx_Math" alttext="A_{2}" display="inline"><semantics id="S4.SS3.SSS2.p5.5.m5.1a"><msub id="S4.SS3.SSS2.p5.5.m5.1.1" xref="S4.SS3.SSS2.p5.5.m5.1.1.cmml"><mi id="S4.SS3.SSS2.p5.5.m5.1.1.2" xref="S4.SS3.SSS2.p5.5.m5.1.1.2.cmml">A</mi><mn id="S4.SS3.SSS2.p5.5.m5.1.1.3" xref="S4.SS3.SSS2.p5.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.5.m5.1b"><apply id="S4.SS3.SSS2.p5.5.m5.1.1.cmml" xref="S4.SS3.SSS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.5.m5.1.1.1.cmml" xref="S4.SS3.SSS2.p5.5.m5.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.5.m5.1.1.2.cmml" xref="S4.SS3.SSS2.p5.5.m5.1.1.2">𝐴</ci><cn type="integer" id="S4.SS3.SSS2.p5.5.m5.1.1.3.cmml" xref="S4.SS3.SSS2.p5.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.5.m5.1c">A_{2}</annotation></semantics></math>, by column, which can be expressed as <math id="S4.SS3.SSS2.p5.6.m6.2" class="ltx_Math" alttext="Y=[XA_{1},XA_{2}]" display="inline"><semantics id="S4.SS3.SSS2.p5.6.m6.2a"><mrow id="S4.SS3.SSS2.p5.6.m6.2.2" xref="S4.SS3.SSS2.p5.6.m6.2.2.cmml"><mi id="S4.SS3.SSS2.p5.6.m6.2.2.4" xref="S4.SS3.SSS2.p5.6.m6.2.2.4.cmml">Y</mi><mo id="S4.SS3.SSS2.p5.6.m6.2.2.3" xref="S4.SS3.SSS2.p5.6.m6.2.2.3.cmml">=</mo><mrow id="S4.SS3.SSS2.p5.6.m6.2.2.2.2" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.3" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.3.cmml">[</mo><mrow id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.2" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.1" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.1.cmml">​</mo><msub id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.cmml"><mi id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.2" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.2.cmml">A</mi><mn id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.3" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.4" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.3.cmml">,</mo><mrow id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.cmml"><mi id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.2" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.1" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.1.cmml">​</mo><msub id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.cmml"><mi id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.2" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.2.cmml">A</mi><mn id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.3" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.5" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.6.m6.2b"><apply id="S4.SS3.SSS2.p5.6.m6.2.2.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2"><eq id="S4.SS3.SSS2.p5.6.m6.2.2.3.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.3"></eq><ci id="S4.SS3.SSS2.p5.6.m6.2.2.4.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.4">𝑌</ci><interval closure="closed" id="S4.SS3.SSS2.p5.6.m6.2.2.2.3.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2"><apply id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1"><times id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.1"></times><ci id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.2">𝑋</ci><apply id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.1.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.2.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.2">𝐴</ci><cn type="integer" id="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.3.cmml" xref="S4.SS3.SSS2.p5.6.m6.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2"><times id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.1.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.1"></times><ci id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.2.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.2">𝑋</ci><apply id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.1.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3">subscript</csymbol><ci id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.2.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.2">𝐴</ci><cn type="integer" id="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.3.cmml" xref="S4.SS3.SSS2.p5.6.m6.2.2.2.2.2.3.3">2</cn></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.6.m6.2c">Y=[XA_{1},XA_{2}]</annotation></semantics></math>.
By placing matrices <math id="S4.SS3.SSS2.p5.7.m7.1" class="ltx_Math" alttext="A_{1}" display="inline"><semantics id="S4.SS3.SSS2.p5.7.m7.1a"><msub id="S4.SS3.SSS2.p5.7.m7.1.1" xref="S4.SS3.SSS2.p5.7.m7.1.1.cmml"><mi id="S4.SS3.SSS2.p5.7.m7.1.1.2" xref="S4.SS3.SSS2.p5.7.m7.1.1.2.cmml">A</mi><mn id="S4.SS3.SSS2.p5.7.m7.1.1.3" xref="S4.SS3.SSS2.p5.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.7.m7.1b"><apply id="S4.SS3.SSS2.p5.7.m7.1.1.cmml" xref="S4.SS3.SSS2.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.7.m7.1.1.1.cmml" xref="S4.SS3.SSS2.p5.7.m7.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.7.m7.1.1.2.cmml" xref="S4.SS3.SSS2.p5.7.m7.1.1.2">𝐴</ci><cn type="integer" id="S4.SS3.SSS2.p5.7.m7.1.1.3.cmml" xref="S4.SS3.SSS2.p5.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.7.m7.1c">A_{1}</annotation></semantics></math> and <math id="S4.SS3.SSS2.p5.8.m8.1" class="ltx_Math" alttext="A_{2}" display="inline"><semantics id="S4.SS3.SSS2.p5.8.m8.1a"><msub id="S4.SS3.SSS2.p5.8.m8.1.1" xref="S4.SS3.SSS2.p5.8.m8.1.1.cmml"><mi id="S4.SS3.SSS2.p5.8.m8.1.1.2" xref="S4.SS3.SSS2.p5.8.m8.1.1.2.cmml">A</mi><mn id="S4.SS3.SSS2.p5.8.m8.1.1.3" xref="S4.SS3.SSS2.p5.8.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p5.8.m8.1b"><apply id="S4.SS3.SSS2.p5.8.m8.1.1.cmml" xref="S4.SS3.SSS2.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p5.8.m8.1.1.1.cmml" xref="S4.SS3.SSS2.p5.8.m8.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p5.8.m8.1.1.2.cmml" xref="S4.SS3.SSS2.p5.8.m8.1.1.2">𝐴</ci><cn type="integer" id="S4.SS3.SSS2.p5.8.m8.1.1.3.cmml" xref="S4.SS3.SSS2.p5.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p5.8.m8.1c">A_{2}</annotation></semantics></math> on different GPUs, the matrix multiplication operation would be invoked at two GPUs in parallel, and the final result can be obtained by combining the outputs from the two GPUs through across-GPU communication.
Currently, tensor parallelism has been supported in several open-source libraries, <em id="S4.SS3.SSS2.p5.8.2" class="ltx_emph ltx_font_italic">e.g.,</em> Megatron-LM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, and can be extended to higher-dimensional tensors.
Also, Colossal-AI has implemented tensor parallelism for higher-dimensional tensors&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib335" title="" class="ltx_ref">335</a>, <a href="#bib.bib336" title="" class="ltx_ref">336</a>, <a href="#bib.bib337" title="" class="ltx_ref">337</a>]</cite> and proposed sequence parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib338" title="" class="ltx_ref">338</a>]</cite> especially for sequence data, which can further decompose the attention operation of the Transformer model.</p>
</div>
<div id="S4.SS3.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS2.p6.1" class="ltx_p"><span id="S4.SS3.SSS2.p6.1.1" class="ltx_text ltx_font_bold">ZeRO.</span>
ZeRO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib333" title="" class="ltx_ref">333</a>]</cite> technique, proposed by the DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
library, focuses on the issue of memory redundancy in data parallelism.
As mentioned before, data parallelism requires each GPU to store the same copy of a LLM, including model parameters, model gradients, and optimizer parameters.
Whereas, not all of the above data is necessary to be retained on each GPU, which would cause a memory redundancy problem.
To resolve it, the ZeRO technique aims to retain only a fraction of data on each GPU, while the rest data can be retrieved from other GPUs when required.
Specifically, ZeRO provides three solutions, depending on how the three parts of the data are stored,

namely optimizer state partitioning, gradient partitioning, and parameter partitioning.

Empirical results indicate that the first two solutions do not increase the communication overhead, and the third solution increases about 50% communication overhead but saves memory proportional to the number of GPUs.
PyTorch has implemented a similar technique as ZeRO, called FSDP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib339" title="" class="ltx_ref">339</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS2.p7.1" class="ltx_p"><span id="S4.SS3.SSS2.p7.1.1" class="ltx_text ltx_font_bold">Mixed Precision Training.</span>
In previous PLMs (<em id="S4.SS3.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> BERT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>), 32-bit floating-point numbers, also known as FP32, have been predominantly used for pre-training.
In recent years, to pre-train extremely large language models, some studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib334" title="" class="ltx_ref">334</a>]</cite> have started to utilize 16-bit floating-point numbers (FP16), which reduces memory usage and communication overhead.
Additionally, as popular NVIDIA GPUs (<em id="S4.SS3.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> A100) have twice the amount of FP16 computation units as FP32, the computational efficiency of FP16 can be further improved.
However, existing work has found that FP16 may lead to the loss of computational accuracy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, which affects the final model performance.
To alleviate it, an alternative called <em id="S4.SS3.SSS2.p7.1.4" class="ltx_emph ltx_font_italic">Brain Floating Point (BF16)</em> has been used for training, which
allocates more exponent bits and fewer significant bits than FP16.
For pre-training,
BF16 generally performs better than FP16 on representation accuracy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS2.p8" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS2.p8.1" class="ltx_p"><span id="S4.SS3.SSS2.p8.1.1" class="ltx_text ltx_font_bold">Overall Training Suggestion.</span>
In practice, the above training techniques, especially 3D parallelism, are often jointly used to improve the training throughput and large model loading.
For instance, researchers have incorporated 8-way data parallelism, 4-way tensor parallelism, and 12-way pipeline parallelism, enabling the training of BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> on 384 A100 GPUs. Currently, open-source libraries like DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, Colossal-AI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>,
and Alpa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib340" title="" class="ltx_ref">340</a>]</cite>
can well support the three parallel training methods.
To reduce the memory redundancy, ZeRO, FSDP, and activation recomputation techniques&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib341" title="" class="ltx_ref">341</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> can be also employed for training LLMs, which have already been integrated into DeepSpeed, PyTorch, and Megatron-LM. 
In addition, the mixed precision training technique such as BF16 can be also leveraged to improve the training efficiency and reduce GPU memory usage, while it requires necessary support on hardware (<em id="S4.SS3.SSS2.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> A100 GPU).
Because training large models is a time-intensive process,
it would be useful to forecast the model performance and detect abnormal issues at an early stage.
For this purpose, GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> has recently introduced a new mechanism called <em id="S4.SS3.SSS2.p8.1.3" class="ltx_emph ltx_font_italic">predictable scaling</em> built on a deep learning stack, enabling the performance prediction of large models with a much smaller model, which might be quite useful for developing LLMs.
In practice, one can further leverage the supporting training techniques of
mainstream deep learning frameworks.
For instance, PyTorch supports the data parallel training algorithm FSDP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib339" title="" class="ltx_ref">339</a>]</cite> (<em id="S4.SS3.SSS2.p8.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> fully sharded data parallel), which allows for partial offloading of training computations to CPUs if desired.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Adaptation of LLMs</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that LLM’s abilities can be further adapted according to specific goals.
In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences.
Further, we will also discuss efficient tuning and quantization for model adaptation in resource-limited settings.
In what follows, we will introduce the four parts in detail.</p>
</div>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2303.18223/assets/x11.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>An illustration of instance formatting and three different methods for constructing the instruction-formatted instances. </figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">Instruction Tuning</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In essence, instruction tuning is the approach to fine-tuning pre-trained LLMs on a collection of formatted instances in the form of natural language&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, which is highly related to supervised fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and multi-task prompted training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In order to perform instruction tuning, we first need to collect or construct instruction-formatted instances.
Then, we employ these formatted instances to fine-tune LLMs in a supervised learning way (<em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> training with the sequence-to-sequence loss).
After instruction tuning, LLMs can demonstrate superior abilities to generalize to unseen tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, even in a multilingual setting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">A recent survey&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib342" title="" class="ltx_ref">342</a>]</cite> presents a systematic overview of the research on instruction tuning. In comparison to that, we mainly focus on the effect of instruction tuning on LLMs and provide detailed guidelines or strategies for instance collection and tuning. In addition, we also discuss the use of instruction tuning for satisfying the real needs of users, which has been widely applied in existing LLMs, <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Formatted Instance Construction</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">Generally, an instruction-formatted instance consists of a task description (called an <em id="S5.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">instruction</em>), an optional input, the corresponding output, and a small number of demonstrations (optional).
As important public resources, existing studies have released a large number of labeled data formatted in natural language (see the list of available resources in Table&nbsp;<a href="#S3.T3" title="TABLE III ‣ 3.2 Commonly Used Corpora for Pre-training ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) as introduced in Section&nbsp;<a href="#S3.SS3.SSS1" title="3.3.1 Instruction Tuning Datasets ‣ 3.3 Commonly Used Datasets for Fine-tuning ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>.
Next, we introduce three major methods for constructing formatted instances (see an illustration in Figure&nbsp;<a href="#S5.F11" title="Figure 11 ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) and then discuss several key factors for instance construction.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p"><span id="S5.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Formatting NLP Task Datasets.</span>
Before instruction tuning was proposed, several early studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib343" title="" class="ltx_ref">343</a>, <a href="#bib.bib344" title="" class="ltx_ref">344</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> collected the instances from a diverse range of traditional NLP tasks (<em id="S5.SS1.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> text summarization, text classification, and translation) to create supervised multi-task training datasets.
As a major source of instruction tuning instances, it is convenient to format these multi-task training datasets with natural language task descriptions.
Specifically, recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> augments the labeled datasets with human-written task descriptions, which instructs LLMs to understand the tasks by explaining the task goal. For example, in Figure&nbsp;<a href="#S5.F11" title="Figure 11 ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>(a), a task description “<em id="S5.SS1.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">Please answer this question</em>” is added for each example in the question-answering task.
After instruction tuning, LLMs can generalize well to other unseen tasks by following their task descriptions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
In particular, it has been shown that instructions are the crucial factor in task generalization ability for LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>:
by fine-tuning the model on labeled datasets with the task descriptions removed, it results in a dramatic drop in model performance.
To better generate labeled instances for instruction tuning, a crowd-sourcing platform, PromptSource&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite> has been proposed to effectively create, share, and verify the task descriptions for different datasets.
To enrich the training instances, several studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>, <a href="#bib.bib345" title="" class="ltx_ref">345</a>]</cite> also try to invert the input-output pairs of existing instances with specially designed task descriptions for instruction tuning. For instance, given a question-answer pair, we can create a new instance by predicting the answer-conditioned question (<em id="S5.SS1.SSS1.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> <em id="S5.SS1.SSS1.p2.1.5" class="ltx_emph ltx_font_italic">“Please generate a question based on the answer:”</em>).</p>
</div>
<div id="S5.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p3.1" class="ltx_p"><span id="S5.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Formatting Daily Chat Data.</span>
Despite that a large number of training instances have been formatted with instructions, they mainly come from public NLP datasets, either lacking instruction diversity or mismatching with real human needs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
To overcome this issue, InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> proposes to take the queries that real users have submitted to the OpenAI API as the task descriptions. Additionally, to enrich the task diversity, human labelers are also asked to compose the instructions for real-life tasks, including open-ended generation, open question answering, brainstorming, and chatting.
Then, they let another group of labelers directly answer these instructions as the output.
Finally, they pair one instruction (<em id="S5.SS1.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> the collected user query) and the expected output (<em id="S5.SS1.SSS1.p3.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> the human-written answer) as a training instance.
Note that InstructGPT also employs these real-world tasks formatted in natural language for alignment tuning (discussed in Section&nbsp;<a href="#S5.SS2" title="5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).
Further, GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> has designed potentially high-risk instructions and guided the model to reject these instructions through supervised fine-tuning for safety concerns.
Considering the absence of high-quality public chat data, several studies have also collected users’ chat requests as input data, and then utilized ChatGPT or GPT-4 to generate responses as output data. A notable example of such a dataset is the conversational data from ShareGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>. Additionally, Dolly&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite> and OpenAssistant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> have further released their conversation data, which has been carefully labeled by human annotators to attain a high level of quality.</p>
</div>
<div id="S5.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p4.1" class="ltx_p"><span id="S5.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Formatting Synthetic Data.</span>
To reduce the burden of human annotation or manual collection, several semi-automated approaches&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> have been proposed for constructing instances by feeding existing instances into LLMs to synthesize diverse task descriptions and instances. As illustrated in Figure&nbsp;<a href="#S5.F11" title="Figure 11 ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>(c), the Self-Instruct method only needs 175 instances as the initial task pool. Then, they randomly select a few instances from the pool as demonstrations and prompt a LLM to generate new instructions and corresponding input-output pairs. After the quality and diversity filtering, newly generated instances would be added into the task pool. Hence, the synthetic method is an effective and economical way to generate large-scale instruction data for LLMs.
However, the instances generated by the Self-Instruct method might be simplistic or lack the diversity. To improve the quality of synthetic int ructions, WizardLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite> introduces Evol-Instruct by proposing in-depth and in-breadth evolving to enrich the complexity and diversity of the instances. Furthermore, Self-Align&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib347" title="" class="ltx_ref">347</a>]</cite> establishes multiple human-aligned principles to filter the synthesized instances. It then employs these instances to train a LLM in order to yield more aligned instances. To enhance the quality of the instance output, researchers directly adopt human-written texts as the output and synthesize corresponding instructions using ICL examples&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib348" title="" class="ltx_ref">348</a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p5.1" class="ltx_p"><span id="S5.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Key Factors for Instance Construction.</span>
The quality of instruction instances has an important impact on the performance of the model.
Here, we discuss some essential factors for instance construction.</p>
</div>
<div id="S5.SS1.SSS1.p6" class="ltx_para">
<p id="S5.SS1.SSS1.p6.1" class="ltx_p"><math id="S5.SS1.SSS1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.SSS1.p6.1.m1.1a"><mo id="S5.SS1.SSS1.p6.1.m1.1.1" xref="S5.SS1.SSS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.1.m1.1b"><ci id="S5.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS1.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">Scaling the instructions.</em>
It has been widely shown that scaling the number of tasks can largely enhance the generalization ability of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
With the increasing of the task number, the model performance initially shows a continuous growth pattern, while the gain becomes negligible when it reaches a certain level&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
A plausible speculation is that a certain number of representative tasks can provide relatively sufficient knowledge and adding more tasks may not bring additional gains&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Also, it is beneficial to enhance the diversity of the task descriptions in several aspects, such as length, structure, and creativity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
As for the number of instances per task, it has been found that a small number of instances can usually saturate the generalization performance of the model to perform a specific task&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Specially, several recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib349" title="" class="ltx_ref">349</a>, <a href="#bib.bib350" title="" class="ltx_ref">350</a>]</cite> has explored the effect of fine-tuning with a small amount of high-quality instruction data (<em id="S5.SS1.SSS1.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> one or a few thousand instances), showing very promising results on the evaluation tasks.
In contrast, another line of studies continue to explore the scaling effect of instruction data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib351" title="" class="ltx_ref">351</a>, <a href="#bib.bib352" title="" class="ltx_ref">352</a>]</cite>. For example, Orca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib351" title="" class="ltx_ref">351</a>]</cite> scales up the synthesized instances to 5 million with step-by-step explanations, and it achieves superior performance across a wide range of tasks compared to the methods tuned with instruction data.</p>
</div>
<div id="S5.SS1.SSS1.p7" class="ltx_para">
<p id="S5.SS1.SSS1.p7.1" class="ltx_p"><math id="S5.SS1.SSS1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.SSS1.p7.1.m1.1a"><mo id="S5.SS1.SSS1.p7.1.m1.1.1" xref="S5.SS1.SSS1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.1.m1.1b"><ci id="S5.SS1.SSS1.p7.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS1.SSS1.p7.1.1" class="ltx_emph ltx_font_italic">Formatting design.</em>
As an important factor, the design of natural language format also highly impacts the generalization performance of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
Typically, we can add task descriptions and optional demonstrations to the input-output pairs of existing datasets, where the task description is the most key part for LLMs to understand the task&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
Further, it can lead to substantial improvements by using an appropriate number of exemplars as demonstrations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, which also alleviates the model sensitivity to instruction engineering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
However, incorporating other components (<em id="S5.SS1.SSS1.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> things to avoid, reasons, and suggestions) into instructions may have a negligible or even adverse effect on the performance of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
Recently, to elicit the step-by-step reasoning ability of LLMs, some work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> proposes to include chain-of-thought (CoT) examples for some reasoning datasets, such as arithmetic reasoning.
It has been shown that fine-tuning LLMs with both CoT and non-CoT examples can lead to a good performance across various reasoning tasks, including those that require multi-hop reasoning ability (<em id="S5.SS1.SSS1.p7.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> commonsense question answering and arithmetic reasoning) as well as those without the need for such a reasoning way (<em id="S5.SS1.SSS1.p7.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> sentiment analysis and extractive question answering)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS1.p8" class="ltx_para">
<p id="S5.SS1.SSS1.p8.1" class="ltx_p">To summarize, diversity and quality of instructions seem to be more important than the number of instances&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib349" title="" class="ltx_ref">349</a>]</cite> since the well-performing InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and LLaMA-2-Chat&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> utilize fewer but more diverse instructions (or instances) than the Flan-series LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. However, a large amount of training data may compensate for the absence of high-quality data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib351" title="" class="ltx_ref">351</a>]</cite>.
Further, it is more useful to invite labelers to compose human-need tasks than using dataset-specific tasks. However, it still lacks general guidelines to annotate human-need instances, making the task composition somehow heuristic.
To reduce human efforts, we can either reuse existing formatted datasets (Table&nbsp;<a href="#S3.T3" title="TABLE III ‣ 3.2 Commonly Used Corpora for Pre-training ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) or automatically construct the instructions using existing LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>. We conduct a preliminary experiment to show the effectiveness of different construction methods in Section&nbsp;<a href="#S5.SS1.SSS4" title="5.1.4 Empirical Analysis for Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.4</span></a>.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning and LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based on two Linux servers having 8 A800-80G SXM4 GPUs with 6 NVSwitch and 8 3090-24G GPUs, respectively. The major difference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and inference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same.

For full tuning experiments, we use data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally, the LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. All the experiments are conducted with Alpaca-52K dataset by training LLaMA models three epochs. The max sequence length for both training settings is set to 512. The inference experiments are performed with the batch size set to 1.
</figcaption>
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T8.1.1" class="ltx_tr">
<td id="S5.T8.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S5.T8.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S5.T8.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3"><span id="S5.T8.1.1.2.1" class="ltx_text ltx_font_bold">A800 Full Training</span></td>
<td id="S5.T8.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="3"><span id="S5.T8.1.1.3.1" class="ltx_text ltx_font_bold">A800 LoRA Training</span></td>
<td id="S5.T8.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S5.T8.1.1.4.1" class="ltx_text ltx_font_bold">A800 Inference (16-bit)</span></td>
<td id="S5.T8.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S5.T8.1.1.5.1" class="ltx_text ltx_font_bold">3090 Inference (16-bit)</span></td>
<td id="S5.T8.1.1.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S5.T8.1.1.6.1" class="ltx_text ltx_font_bold">3090 Inference (8-bit)</span></td>
</tr>
<tr id="S5.T8.1.2" class="ltx_tr">
<td id="S5.T8.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">#GPU</td>
<td id="S5.T8.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">BS</td>
<td id="S5.T8.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Time</td>
<td id="S5.T8.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">#GPU</td>
<td id="S5.T8.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">BS</td>
<td id="S5.T8.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">Time</td>
<td id="S5.T8.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">#GPU</td>
<td id="S5.T8.1.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">#Token/s</td>
<td id="S5.T8.1.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">#GPU</td>
<td id="S5.T8.1.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">#Token/s</td>
<td id="S5.T8.1.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">#GPU</td>
<td id="S5.T8.1.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">#Token/s</td>
</tr>
<tr id="S5.T8.1.3" class="ltx_tr">
<td id="S5.T8.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">LLaMA (7B)</td>
<td id="S5.T8.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">2</td>
<td id="S5.T8.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">8</td>
<td id="S5.T8.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">3.0h</td>
<td id="S5.T8.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">80</td>
<td id="S5.T8.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">3.5h</td>
<td id="S5.T8.1.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">36.6</td>
<td id="S5.T8.1.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.3.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">24.3</td>
<td id="S5.T8.1.3.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.3.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">7.5</td>
</tr>
<tr id="S5.T8.1.4" class="ltx_tr">
<td id="S5.T8.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">LLaMA (13B)</td>
<td id="S5.T8.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">4</td>
<td id="S5.T8.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">8</td>
<td id="S5.T8.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">3.1h</td>
<td id="S5.T8.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">48</td>
<td id="S5.T8.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">5.1h</td>
<td id="S5.T8.1.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">26.8</td>
<td id="S5.T8.1.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">2</td>
<td id="S5.T8.1.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">9.9</td>
<td id="S5.T8.1.4.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.4.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">4.5</td>
</tr>
<tr id="S5.T8.1.5" class="ltx_tr">
<td id="S5.T8.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">LLaMA (30B)</td>
<td id="S5.T8.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">8</td>
<td id="S5.T8.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">4</td>
<td id="S5.T8.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">6.1h</td>
<td id="S5.T8.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">24</td>
<td id="S5.T8.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">14.3h</td>
<td id="S5.T8.1.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">17.7</td>
<td id="S5.T8.1.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">4</td>
<td id="S5.T8.1.5.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">3.8</td>
<td id="S5.T8.1.5.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">2</td>
<td id="S5.T8.1.5.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">2.6</td>
</tr>
<tr id="S5.T8.1.6" class="ltx_tr">
<td id="S5.T8.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">LLaMA (65B)</td>
<td id="S5.T8.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">16</td>
<td id="S5.T8.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">2</td>
<td id="S5.T8.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">11.2h</td>
<td id="S5.T8.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">1</td>
<td id="S5.T8.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">4</td>
<td id="S5.T8.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">60.6h</td>
<td id="S5.T8.1.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">2</td>
<td id="S5.T8.1.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">8.8</td>
<td id="S5.T8.1.6.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">8</td>
<td id="S5.T8.1.6.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:0.0pt;padding-right:0.0pt;">2.0</td>
<td id="S5.T8.1.6.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">4</td>
<td id="S5.T8.1.6.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">1.5</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Instruction Tuning Strategies</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">Unlike pre-training, instruction tuning is often more efficient since only a moderate number of instances are used for training.
Since instruction tuning can be considered as a supervised training process, its optimization is different from pre-training in several aspects&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>,
such as the training objective (<em id="S5.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> sequence-to-sequence loss) and optimization configuration (<em id="S5.SS1.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> smaller batch size and learning rate), which require special attention in practice.
In addition to these optimization configurations, there are also four important aspects to consider for instruction tuning:</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p"><span id="S5.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Balancing the Data Distribution.</span>
Since instruction tuning involves a mixture of different tasks, it is important to balance the proportion of different tasks during fine-tuning. A widely used method is the <em id="S5.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">examples-proportional mixing</em> strategy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, <em id="S5.SS1.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> combining all the datasets and sampling each instance equally from the mixed datasets. Furthermore, increasing the sampling ratio of high-quality collections (<em id="S5.SS1.SSS2.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> and P3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>) can generally lead to performance improvement according to recent findings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. Further, it is common to set a <em id="S5.SS1.SSS2.p2.1.5" class="ltx_emph ltx_font_italic">maximum cap</em> to control the maximum number of examples that a dataset can contain during instruction tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, which is set to prevent larger datasets from overwhelming the entire distribution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. In practice, the maximum cap is typically set to several thousands or tens of thousands according to different datasets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Recently, it has been
empirically found that existing instruction datasets (Table&nbsp;<a href="#S3.T3" title="TABLE III ‣ 3.2 Commonly Used Corpora for Pre-training ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) mainly focus on enhancing LLMs’ capabilities in certain aspects,
and a single dataset alone cannot lead to a comprehensive enhancement in model capacity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib353" title="" class="ltx_ref">353</a>]</cite>. Therefore, it is often suggested to use a mixture of existing instruction datasets to achieve a balanced improvement in different capacities, including NLP task data (<em id="S5.SS1.SSS2.p2.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> FLAN v2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>]</cite>), chat data (<em id="S5.SS1.SSS2.p2.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> ShareGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>), and synthetic data (<em id="S5.SS1.SSS2.p2.1.8" class="ltx_emph ltx_font_italic">e.g.,</em> GPT4-Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib354" title="" class="ltx_ref">354</a>]</cite>).</p>
</div>
<div id="S5.SS1.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p3.1" class="ltx_p"><span id="S5.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Combining Instruction Tuning and Pre-Training.</span>
To make the tuning process more effective and stable, OPT-IML&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> incorporates pre-training data during instruction tuning, which can be regarded as regularization for model tuning.
Further, instead of using a separate two-stage process (<em id="S5.SS1.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">pre-training</em> then <em id="S5.SS1.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">instruction tuning</em>), some studies attempt to train a model from scratch with a mixture of pre-training data (<em id="S5.SS1.SSS2.p3.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> plain texts) and instruction tuning data (<em id="S5.SS1.SSS2.p3.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> formatted datasets) using multi-task learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. Specifically, GLM-130B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> and Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> integrate instruction-formatted datasets as a small proportion of the pre-training corpora to pre-train LLMs, which potentially achieves the advantages of pre-training and instruction tuning at the same time.</p>
</div>
<div id="S5.SS1.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p4.1" class="ltx_p"><span id="S5.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Multi-stage Instruction Tuning.</span>
For instruction tuning, there are two kinds of important instruction data, namely task-formatted instructions and daily chat instructions.
Generally, the former has a significantly larger volume than the latter.
It is important to balance the training with the two kinds of instruction data. In addition to carefully mixing different instruction data, we can also adopt a multi-stage instruction tuning strategy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib352" title="" class="ltx_ref">352</a>]</cite>, where LLMs are first fine-tuned with large-scale task-formatted instructions and subsequently fine-tuned on daily chat ones.
To avoid the capacity forgetting issue, it is also useful to add an amount of task-formatted instructions at the second stage.
Actually, such a multi-stage tuning strategy can be also applied to other settings for instruction tuning. For example, we can schedule different fine-tuning stages with progressively increased levels on difficulty and complexity, and gradually improve the capacities of LLMs to follow complex instructions.</p>
</div>
<div id="S5.SS1.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p5.1" class="ltx_p"><span id="S5.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Other Practical Tricks.</span>
In practice, there are also several useful strategies and tricks that are helpful to improve the fine-tuning performance of LLMs. We list several representative ones as follows:</p>
</div>
<div id="S5.SS1.SSS2.p6" class="ltx_para">
<p id="S5.SS1.SSS2.p6.1" class="ltx_p"><math id="S5.SS1.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.SSS2.p6.1.m1.1a"><mo id="S5.SS1.SSS2.p6.1.m1.1.1" xref="S5.SS1.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p6.1.m1.1b"><ci id="S5.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS1.SSS2.p6.1.1" class="ltx_emph ltx_font_italic">Efficient training for multi-turn chat data.</em>
Given a multi-turn chat example (the conversation between a user and chatbot), a straightforward fine-tuning way is to split it into multiple context-response pairs for training: a LLM is fine-tuned to generate the response based on the corresponding context for all splits (<em id="S5.SS1.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> at each utterance from the user). In such a fine-tuning way, it is apparent that there exist overlapping utterances in the split examples from a conversation.
To save the training cost, Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite> has adopted an efficient way that feeds the whole conversation into the LLM, but relies on a loss mask that only computes the loss on the responses of the chatbot for training.
It can significantly reduce the compute costs derived from the overlapped utterances.</p>
</div>
<div id="S5.SS1.SSS2.p7" class="ltx_para">
<p id="S5.SS1.SSS2.p7.1" class="ltx_p"><math id="S5.SS1.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.SSS2.p7.1.m1.1a"><mo id="S5.SS1.SSS2.p7.1.m1.1.1" xref="S5.SS1.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p7.1.m1.1b"><ci id="S5.SS1.SSS2.p7.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS1.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Establishing self-identification for LLM.</em>
To deploy LLMs for real-world applications, it is necessary to establish its identity and make LLMs aware of these identity information, such as name, developer and affiliation.
A practical way is to create identity-related instructions for fine-tuning the LLM. It is also feasible to prefix the input with the self-identification prompt, <em id="S5.SS1.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> “<em id="S5.SS1.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">The following is a conversation between a human and an AI assistant called <span id="S5.SS1.SSS2.p7.1.3.1" class="ltx_text ltx_font_smallcaps">ChatbotName</span>, developed by <span id="S5.SS1.SSS2.p7.1.3.2" class="ltx_text ltx_font_smallcaps">Developer</span>.</em>”, where <span id="S5.SS1.SSS2.p7.1.4" class="ltx_text ltx_font_smallcaps">ChatbotName</span> and <span id="S5.SS1.SSS2.p7.1.5" class="ltx_text ltx_font_smallcaps">Developer</span> refer to the name and developer of the chatbot, respectively.</p>
</div>
<div id="S5.SS1.SSS2.p8" class="ltx_para">
<p id="S5.SS1.SSS2.p8.1" class="ltx_p">In addition to the above practical strategies and tricks, existing work has also used other tricks, <em id="S5.SS1.SSS2.p8.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> concatenating multiple examples into a single sequence to approach the max length&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib355" title="" class="ltx_ref">355</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>The Effect of Instruction Tuning</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">In this part, we discuss the effect of instruction tuning on LLMs in three major aspects.</p>
</div>
<div id="S5.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p"><span id="S5.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Performance Improvement.</span>
Despite being tuned on a moderate number of instances, instruction tuning has become an important way to improve or unlock the abilities of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. Recent studies have experimented with language models in multiple scales (ranging from 77M to 540B), showing that the models of different scales can all benefit from instruction tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib345" title="" class="ltx_ref">345</a>]</cite>, yielding improved performance as the parameter scale increases&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.
Further, smaller models with instruction tuning can even perform better than larger models without fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Besides the model scale, instruction tuning demonstrates consistent improvements in various model architectures, pre-training objectives, and model adaptation methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
In practice, instruction tuning offers a general approach to enhancing the abilities of existing language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> (including small-sized PLMs). Also, it is much less costly than pre-training, since the amount of instruction data required by LLMs is significantly smaller than pre-training data.</p>
</div>
<div id="S5.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS3.p3.1" class="ltx_p"><span id="S5.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Task Generalization.</span>
Instruction tuning encourages the model to understand natural language instructions for task completion.
It endows LLMs with the ability (often considered as an emergent ability) to follow human instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to perform specific tasks without demonstrations, even on unseen tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
A large number of studies have confirmed the effectiveness of instruction tuning to achieve superior performance on both seen and unseen tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib345" title="" class="ltx_ref">345</a>]</cite>.
Also, instruction tuning has been shown to be useful in alleviating several weaknesses of LLMs (<em id="S5.SS1.SSS3.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> repetitive generation or complementing the input without accomplishing a certain task)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, leading to a superior capacity to solve real-world tasks for LLMs. Furthermore, LLMs trained with instruction tuning can generalize to related tasks across languages. For example, BLOOMZ-P3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> is fine-tuned based on BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> using English-only task collection P3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>. Interestingly, BLOOMZ-P3 can achieve a more than 50% improvement in multilingual sentence completion tasks compared to BLOOM, which shows that instruction tuning can help LLMs acquire general task skills from English-only datasets and transfer such skills into other languages&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.
In addition, it has been found that using English-only instructions can produce satisfactory results on multilingual tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, which helps reduce the effort of instruction engineering for a specific language.</p>
</div>
<div id="S5.SS1.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS3.p4.1" class="ltx_p"><span id="S5.SS1.SSS3.p4.1.1" class="ltx_text ltx_font_bold">Domain Specialization.</span>
Existing LLMs have showcased superior capabilities in traditional NLP tasks (<em id="S5.SS1.SSS3.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> generation and reasoning) and daily questions. However, they may still lack domain knowledge to accomplish specific tasks, such as medicine, law, and finance (See Section&nbsp;<a href="#S8" title="8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> for a detailed discussion of LLMs in different applications). Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts.
For instance, researchers propose to fine-tune Flan-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> using medical datasets to create Med-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>]</cite>, a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians.
Furthermore, a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref">357</a>]</cite> fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks.
There are also several open-sourced medical models instruction-tuned based on LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, such as BenTsao&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib358" title="" class="ltx_ref">358</a>]</cite>.
Also, researchers explore instruction tuning on law&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib359" title="" class="ltx_ref">359</a>]</cite>, finance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>, and arithmetic computation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib361" title="" class="ltx_ref">361</a>]</cite>.</p>
</div>
<figure id="S5.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLaMA (7B) and LLaMA (13B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K dataset, <em id="S5.T9.13.1" class="ltx_emph ltx_font_italic">i.e.,</em> enhancing the complexity (<em id="S5.T9.14.2" class="ltx_emph ltx_font_italic">w/ complexity</em>), increasing the diversity (<em id="S5.T9.15.3" class="ltx_emph ltx_font_italic">w/ diversity</em>), balancing the difficulty (<em id="S5.T9.16.4" class="ltx_emph ltx_font_italic">w/ difficulty</em>), and scaling the instruction number (<em id="S5.T9.17.5" class="ltx_emph ltx_font_italic">w/ scaling</em>). <sup id="S5.T9.18.6" class="ltx_sup">∗</sup>Since we select the LLaMA (7B)/(13B) model fine-tuned on Self-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.</figcaption>
<div id="S5.T9.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:693.8pt;height:675.1pt;vertical-align:-1.5pt;"><span class="ltx_transformed_inner" style="transform:translate(120.7pt,-117.2pt) scale(1.53364483040195,1.53364483040195) ;">
<table id="S5.T9.6.4" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S5.T9.6.4.5" class="ltx_tr">
<td id="S5.T9.6.4.5.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S5.T9.6.4.5.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S5.T9.6.4.5.2" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S5.T9.6.4.5.2.1" class="ltx_text">
<span id="S5.T9.6.4.5.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.5.2.1.1.1" class="ltx_tr">
<span id="S5.T9.6.4.5.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.6.4.5.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span></span>
<span id="S5.T9.6.4.5.2.1.1.2" class="ltx_tr">
<span id="S5.T9.6.4.5.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.6.4.5.2.1.1.2.1.1" class="ltx_text ltx_font_bold">Mixtures</span></span></span>
</span></span></td>
<td id="S5.T9.6.4.5.3" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S5.T9.6.4.5.3.1" class="ltx_text">
<span id="S5.T9.6.4.5.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.5.3.1.1.1" class="ltx_tr">
<span id="S5.T9.6.4.5.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.6.4.5.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Instruction</span></span></span>
<span id="S5.T9.6.4.5.3.1.1.2" class="ltx_tr">
<span id="S5.T9.6.4.5.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.6.4.5.3.1.1.2.1.1" class="ltx_text ltx_font_bold">Numbers</span></span></span>
</span></span></td>
<td id="S5.T9.6.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T9.6.4.5.4.1" class="ltx_text">
<span id="S5.T9.6.4.5.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.5.4.1.1.1" class="ltx_tr">
<span id="S5.T9.6.4.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.6.4.5.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Lexical</span></span></span>
<span id="S5.T9.6.4.5.4.1.1.2" class="ltx_tr">
<span id="S5.T9.6.4.5.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T9.6.4.5.4.1.1.2.1.1" class="ltx_text ltx_font_bold">Diversity</span></span></span>
</span></span></td>
<td id="S5.T9.6.4.5.5" class="ltx_td ltx_nopad_r ltx_border_r ltx_border_tt" rowspan="2"></td>
<td id="S5.T9.6.4.5.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T9.6.4.5.6.1" class="ltx_text ltx_font_bold">Chat</span></td>
<td id="S5.T9.6.4.5.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T9.6.4.5.7.1" class="ltx_text ltx_font_bold">QA</span></td>
</tr>
<tr id="S5.T9.6.4.6" class="ltx_tr">
<td id="S5.T9.6.4.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t"></td>
<td id="S5.T9.6.4.6.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">AlpacaFarm</td>
<td id="S5.T9.6.4.6.3" class="ltx_td ltx_align_center ltx_border_t">MMLU</td>
<td id="S5.T9.6.4.6.4" class="ltx_td ltx_align_center ltx_border_t">BBH3k</td>
</tr>
<tr id="S5.T9.6.4.7" class="ltx_tr">
<td id="S5.T9.6.4.7.1" class="ltx_td ltx_align_left ltx_border_t">LLaMA&nbsp;(7B)</td>
<td id="S5.T9.6.4.7.2" class="ltx_td ltx_align_left ltx_border_t">①&nbsp;FLAN-T5</td>
<td id="S5.T9.6.4.7.3" class="ltx_td ltx_align_right ltx_border_t">80,000</td>
<td id="S5.T9.6.4.7.4" class="ltx_td ltx_align_center ltx_border_t">48.48</td>
<td id="S5.T9.6.4.7.5" class="ltx_td ltx_nopad_r ltx_border_r ltx_border_t"></td>
<td id="S5.T9.6.4.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t"></td>
<td id="S5.T9.6.4.7.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">23.77</td>
<td id="S5.T9.6.4.7.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;"><span id="S5.T9.6.4.7.8.1" class="ltx_text" style="background-color:#C4DDEC;">38.58</span></td>
<td id="S5.T9.6.4.7.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;"><span id="S5.T9.6.4.7.9.1" class="ltx_text" style="background-color:#A7CBE2;">32.79</span></td>
</tr>
<tr id="S5.T9.6.4.8" class="ltx_tr">
<td id="S5.T9.6.4.8.1" class="ltx_td"></td>
<td id="S5.T9.6.4.8.2" class="ltx_td ltx_align_left">②&nbsp;ShareGPT</td>
<td id="S5.T9.6.4.8.3" class="ltx_td ltx_align_right">63,184</td>
<td id="S5.T9.6.4.8.4" class="ltx_td ltx_align_center">77.31</td>
<td id="S5.T9.6.4.8.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.8.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#A7CBE2;"><span id="S5.T9.6.4.8.7.1" class="ltx_text" style="background-color:#A7CBE2;">81.30</span></td>
<td id="S5.T9.6.4.8.8" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;"><span id="S5.T9.6.4.8.8.1" class="ltx_text" style="background-color:#E5F0F7;">38.11</span></td>
<td id="S5.T9.6.4.8.9" class="ltx_td ltx_align_center">27.71</td>
</tr>
<tr id="S5.T9.3.1.1" class="ltx_tr">
<td id="S5.T9.3.1.1.2" class="ltx_td"></td>
<td id="S5.T9.3.1.1.3" class="ltx_td ltx_align_left">③&nbsp;Self-Instruct-52K</td>
<td id="S5.T9.3.1.1.4" class="ltx_td ltx_align_right">82,439</td>
<td id="S5.T9.3.1.1.5" class="ltx_td ltx_align_center">25.92</td>
<td id="S5.T9.3.1.1.6" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.3.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.3.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center">/<sup id="S5.T9.3.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T9.3.1.1.8" class="ltx_td ltx_align_center">37.52</td>
<td id="S5.T9.3.1.1.9" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;"><span id="S5.T9.3.1.1.9.1" class="ltx_text" style="background-color:#E5F0F7;">29.81</span></td>
</tr>
<tr id="S5.T9.6.4.9" class="ltx_tr">
<td id="S5.T9.6.4.9.1" class="ltx_td"></td>
<td id="S5.T9.6.4.9.2" class="ltx_td ltx_align_left">② + ③</td>
<td id="S5.T9.6.4.9.3" class="ltx_td ltx_align_right">145,623</td>
<td id="S5.T9.6.4.9.4" class="ltx_td ltx_align_center">48.22</td>
<td id="S5.T9.6.4.9.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.9.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E5F0F7;"><span id="S5.T9.6.4.9.7.1" class="ltx_text" style="background-color:#E5F0F7;">71.36</span></td>
<td id="S5.T9.6.4.9.8" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;"><span id="S5.T9.6.4.9.8.1" class="ltx_text" style="background-color:#A7CBE2;">41.26</span></td>
<td id="S5.T9.6.4.9.9" class="ltx_td ltx_align_center">28.36</td>
</tr>
<tr id="S5.T9.6.4.10" class="ltx_tr">
<td id="S5.T9.6.4.10.1" class="ltx_td"></td>
<td id="S5.T9.6.4.10.2" class="ltx_td ltx_align_left">① + ② + ③</td>
<td id="S5.T9.6.4.10.3" class="ltx_td ltx_align_right">225,623</td>
<td id="S5.T9.6.4.10.4" class="ltx_td ltx_align_center">48.28</td>
<td id="S5.T9.6.4.10.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.10.7" class="ltx_td ltx_nopad_l ltx_align_center">70.00</td>
<td id="S5.T9.6.4.10.8" class="ltx_td ltx_align_center" style="background-color:#92BFDB;"><span id="S5.T9.6.4.10.8.1" class="ltx_text" style="background-color:#92BFDB;">43.69</span></td>
<td id="S5.T9.6.4.10.9" class="ltx_td ltx_align_center">29.69</td>
</tr>
<tr id="S5.T9.4.2.2" class="ltx_tr">
<td id="S5.T9.4.2.2.2" class="ltx_td"></td>
<td id="S5.T9.4.2.2.3" class="ltx_td ltx_align_left ltx_border_t">③&nbsp;Self-Instruct-52K</td>
<td id="S5.T9.4.2.2.4" class="ltx_td ltx_align_right ltx_border_t">82,439</td>
<td id="S5.T9.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t">25.92</td>
<td id="S5.T9.4.2.2.6" class="ltx_td ltx_nopad_r ltx_border_r ltx_border_t"></td>
<td id="S5.T9.4.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t"></td>
<td id="S5.T9.4.2.2.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">/<sup id="S5.T9.4.2.2.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T9.4.2.2.8" class="ltx_td ltx_align_center ltx_border_t">37.52</td>
<td id="S5.T9.4.2.2.9" class="ltx_td ltx_align_center ltx_border_t">29.81</td>
</tr>
<tr id="S5.T9.6.4.11" class="ltx_tr">
<td id="S5.T9.6.4.11.1" class="ltx_td"></td>
<td id="S5.T9.6.4.11.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.11.2.1" class="ltx_text"></span><span id="S5.T9.6.4.11.2.2" class="ltx_text">
<span id="S5.T9.6.4.11.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.11.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.11.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ complexity</span></span>
</span></span><span id="S5.T9.6.4.11.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.11.3" class="ltx_td ltx_align_right">70,000</td>
<td id="S5.T9.6.4.11.4" class="ltx_td ltx_align_center">70.43</td>
<td id="S5.T9.6.4.11.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.11.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#C4DDEC;"><span id="S5.T9.6.4.11.7.1" class="ltx_text" style="background-color:#C4DDEC;">76.96</span></td>
<td id="S5.T9.6.4.11.8" class="ltx_td ltx_align_center" style="background-color:#C6DEED;"><span id="S5.T9.6.4.11.8.1" class="ltx_text" style="background-color:#C6DEED;">39.73</span></td>
<td id="S5.T9.6.4.11.9" class="ltx_td ltx_align_center" style="background-color:#92BFDB;"><span id="S5.T9.6.4.11.9.1" class="ltx_text" style="background-color:#92BFDB;">33.25</span></td>
</tr>
<tr id="S5.T9.6.4.12" class="ltx_tr">
<td id="S5.T9.6.4.12.1" class="ltx_td"></td>
<td id="S5.T9.6.4.12.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.12.2.1" class="ltx_text"></span><span id="S5.T9.6.4.12.2.2" class="ltx_text">
<span id="S5.T9.6.4.12.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.12.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.12.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ diversity</span></span>
</span></span><span id="S5.T9.6.4.12.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.12.3" class="ltx_td ltx_align_right">70,000</td>
<td id="S5.T9.6.4.12.4" class="ltx_td ltx_align_center">75.59</td>
<td id="S5.T9.6.4.12.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.12.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#92BFDB;"><span id="S5.T9.6.4.12.7.1" class="ltx_text" style="background-color:#92BFDB;">81.55</span></td>
<td id="S5.T9.6.4.12.8" class="ltx_td ltx_align_center">38.01</td>
<td id="S5.T9.6.4.12.9" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;"><span id="S5.T9.6.4.12.9.1" class="ltx_text" style="background-color:#C4DDEC;">30.03</span></td>
</tr>
<tr id="S5.T9.6.4.13" class="ltx_tr">
<td id="S5.T9.6.4.13.1" class="ltx_td"></td>
<td id="S5.T9.6.4.13.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.13.2.1" class="ltx_text"></span><span id="S5.T9.6.4.13.2.2" class="ltx_text">
<span id="S5.T9.6.4.13.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.13.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.13.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ difficulty</span></span>
</span></span><span id="S5.T9.6.4.13.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.13.3" class="ltx_td ltx_align_right">70,000</td>
<td id="S5.T9.6.4.13.4" class="ltx_td ltx_align_center">73.48</td>
<td id="S5.T9.6.4.13.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.13.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#C6DEED;"><span id="S5.T9.6.4.13.7.1" class="ltx_text" style="background-color:#C6DEED;">79.15</span></td>
<td id="S5.T9.6.4.13.8" class="ltx_td ltx_align_center">32.55</td>
<td id="S5.T9.6.4.13.9" class="ltx_td ltx_align_center" style="background-color:#C6DEED;"><span id="S5.T9.6.4.13.9.1" class="ltx_text" style="background-color:#C6DEED;">31.25</span></td>
</tr>
<tr id="S5.T9.6.4.14" class="ltx_tr">
<td id="S5.T9.6.4.14.1" class="ltx_td"></td>
<td id="S5.T9.6.4.14.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.14.2.1" class="ltx_text"></span><span id="S5.T9.6.4.14.2.2" class="ltx_text">
<span id="S5.T9.6.4.14.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.14.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.14.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ scaling</span></span>
</span></span><span id="S5.T9.6.4.14.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.14.3" class="ltx_td ltx_align_right">220,000</td>
<td id="S5.T9.6.4.14.4" class="ltx_td ltx_align_center">57.78</td>
<td id="S5.T9.6.4.14.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.14.7" class="ltx_td ltx_nopad_l ltx_align_center">51.13</td>
<td id="S5.T9.6.4.14.8" class="ltx_td ltx_align_center">33.81</td>
<td id="S5.T9.6.4.14.9" class="ltx_td ltx_align_center">26.63</td>
</tr>
<tr id="S5.T9.6.4.15" class="ltx_tr">
<td id="S5.T9.6.4.15.1" class="ltx_td ltx_align_left ltx_border_t">LLaMA&nbsp;(13B)</td>
<td id="S5.T9.6.4.15.2" class="ltx_td ltx_align_left ltx_border_t">①&nbsp;FLAN-T5</td>
<td id="S5.T9.6.4.15.3" class="ltx_td ltx_align_right ltx_border_t">80,000</td>
<td id="S5.T9.6.4.15.4" class="ltx_td ltx_align_center ltx_border_t">48.48</td>
<td id="S5.T9.6.4.15.5" class="ltx_td ltx_nopad_r ltx_border_r ltx_border_t"></td>
<td id="S5.T9.6.4.15.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t"></td>
<td id="S5.T9.6.4.15.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">22.12</td>
<td id="S5.T9.6.4.15.8" class="ltx_td ltx_align_center ltx_border_t">34.12</td>
<td id="S5.T9.6.4.15.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;"><span id="S5.T9.6.4.15.9.1" class="ltx_text" style="background-color:#C4DDEC;">34.05</span></td>
</tr>
<tr id="S5.T9.6.4.16" class="ltx_tr">
<td id="S5.T9.6.4.16.1" class="ltx_td"></td>
<td id="S5.T9.6.4.16.2" class="ltx_td ltx_align_left">②&nbsp;ShareGPT</td>
<td id="S5.T9.6.4.16.3" class="ltx_td ltx_align_right">63,184</td>
<td id="S5.T9.6.4.16.4" class="ltx_td ltx_align_center">77.31</td>
<td id="S5.T9.6.4.16.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.16.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#C4DDEC;"><span id="S5.T9.6.4.16.7.1" class="ltx_text" style="background-color:#C4DDEC;">77.13</span></td>
<td id="S5.T9.6.4.16.8" class="ltx_td ltx_align_center" style="background-color:#92BFDB;"><span id="S5.T9.6.4.16.8.1" class="ltx_text" style="background-color:#92BFDB;">47.49</span></td>
<td id="S5.T9.6.4.16.9" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;"><span id="S5.T9.6.4.16.9.1" class="ltx_text" style="background-color:#E5F0F7;">33.82</span></td>
</tr>
<tr id="S5.T9.5.3.3" class="ltx_tr">
<td id="S5.T9.5.3.3.2" class="ltx_td"></td>
<td id="S5.T9.5.3.3.3" class="ltx_td ltx_align_left">③&nbsp;Self-Instruct-52K</td>
<td id="S5.T9.5.3.3.4" class="ltx_td ltx_align_right">82,439</td>
<td id="S5.T9.5.3.3.5" class="ltx_td ltx_align_center">25.92</td>
<td id="S5.T9.5.3.3.6" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.5.3.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.5.3.3.1" class="ltx_td ltx_nopad_l ltx_align_center">/<sup id="S5.T9.5.3.3.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T9.5.3.3.8" class="ltx_td ltx_align_center">36.73</td>
<td id="S5.T9.5.3.3.9" class="ltx_td ltx_align_center">25.43</td>
</tr>
<tr id="S5.T9.6.4.17" class="ltx_tr">
<td id="S5.T9.6.4.17.1" class="ltx_td"></td>
<td id="S5.T9.6.4.17.2" class="ltx_td ltx_align_left">② + ③</td>
<td id="S5.T9.6.4.17.3" class="ltx_td ltx_align_right">145,623</td>
<td id="S5.T9.6.4.17.4" class="ltx_td ltx_align_center">48.22</td>
<td id="S5.T9.6.4.17.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.17.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.17.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#E5F0F7;"><span id="S5.T9.6.4.17.7.1" class="ltx_text" style="background-color:#E5F0F7;">72.85</span></td>
<td id="S5.T9.6.4.17.8" class="ltx_td ltx_align_center">41.16</td>
<td id="S5.T9.6.4.17.9" class="ltx_td ltx_align_center">29.49</td>
</tr>
<tr id="S5.T9.6.4.18" class="ltx_tr">
<td id="S5.T9.6.4.18.1" class="ltx_td"></td>
<td id="S5.T9.6.4.18.2" class="ltx_td ltx_align_left">① + ② + ③</td>
<td id="S5.T9.6.4.18.3" class="ltx_td ltx_align_right">225,623</td>
<td id="S5.T9.6.4.18.4" class="ltx_td ltx_align_center">48.28</td>
<td id="S5.T9.6.4.18.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.18.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.18.7" class="ltx_td ltx_nopad_l ltx_align_center">69.49</td>
<td id="S5.T9.6.4.18.8" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;"><span id="S5.T9.6.4.18.8.1" class="ltx_text" style="background-color:#C4DDEC;">43.50</span></td>
<td id="S5.T9.6.4.18.9" class="ltx_td ltx_align_center">31.16</td>
</tr>
<tr id="S5.T9.6.4.4" class="ltx_tr">
<td id="S5.T9.6.4.4.2" class="ltx_td"></td>
<td id="S5.T9.6.4.4.3" class="ltx_td ltx_align_left ltx_border_t">③&nbsp;Self-Instruct-52K</td>
<td id="S5.T9.6.4.4.4" class="ltx_td ltx_align_right ltx_border_t">82,439</td>
<td id="S5.T9.6.4.4.5" class="ltx_td ltx_align_center ltx_border_t">25.92</td>
<td id="S5.T9.6.4.4.6" class="ltx_td ltx_nopad_r ltx_border_r ltx_border_t"></td>
<td id="S5.T9.6.4.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t"></td>
<td id="S5.T9.6.4.4.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">/<sup id="S5.T9.6.4.4.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T9.6.4.4.8" class="ltx_td ltx_align_center ltx_border_t">36.73</td>
<td id="S5.T9.6.4.4.9" class="ltx_td ltx_align_center ltx_border_t">25.43</td>
</tr>
<tr id="S5.T9.6.4.19" class="ltx_tr">
<td id="S5.T9.6.4.19.1" class="ltx_td"></td>
<td id="S5.T9.6.4.19.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.19.2.1" class="ltx_text"></span><span id="S5.T9.6.4.19.2.2" class="ltx_text">
<span id="S5.T9.6.4.19.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.19.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.19.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ complexity</span></span>
</span></span><span id="S5.T9.6.4.19.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.19.3" class="ltx_td ltx_align_right">70,000</td>
<td id="S5.T9.6.4.19.4" class="ltx_td ltx_align_center">70.43</td>
<td id="S5.T9.6.4.19.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.19.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.19.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#C6DEED;"><span id="S5.T9.6.4.19.7.1" class="ltx_text" style="background-color:#C6DEED;">77.94</span></td>
<td id="S5.T9.6.4.19.8" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;"><span id="S5.T9.6.4.19.8.1" class="ltx_text" style="background-color:#A7CBE2;">46.89</span></td>
<td id="S5.T9.6.4.19.9" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;"><span id="S5.T9.6.4.19.9.1" class="ltx_text" style="background-color:#A7CBE2;">35.75</span></td>
</tr>
<tr id="S5.T9.6.4.20" class="ltx_tr">
<td id="S5.T9.6.4.20.1" class="ltx_td"></td>
<td id="S5.T9.6.4.20.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.20.2.1" class="ltx_text"></span><span id="S5.T9.6.4.20.2.2" class="ltx_text">
<span id="S5.T9.6.4.20.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.20.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.20.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ diversity</span></span>
</span></span><span id="S5.T9.6.4.20.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.20.3" class="ltx_td ltx_align_right">70,000</td>
<td id="S5.T9.6.4.20.4" class="ltx_td ltx_align_center">75.59</td>
<td id="S5.T9.6.4.20.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.20.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.20.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#A7CBE2;"><span id="S5.T9.6.4.20.7.1" class="ltx_text" style="background-color:#A7CBE2;">78.92</span></td>
<td id="S5.T9.6.4.20.8" class="ltx_td ltx_align_center" style="background-color:#C6DEED;"><span id="S5.T9.6.4.20.8.1" class="ltx_text" style="background-color:#C6DEED;">44.97</span></td>
<td id="S5.T9.6.4.20.9" class="ltx_td ltx_align_center" style="background-color:#92BFDB;"><span id="S5.T9.6.4.20.9.1" class="ltx_text" style="background-color:#92BFDB;">36.40</span></td>
</tr>
<tr id="S5.T9.6.4.21" class="ltx_tr">
<td id="S5.T9.6.4.21.1" class="ltx_td"></td>
<td id="S5.T9.6.4.21.2" class="ltx_td ltx_align_left">
<span id="S5.T9.6.4.21.2.1" class="ltx_text"></span><span id="S5.T9.6.4.21.2.2" class="ltx_text">
<span id="S5.T9.6.4.21.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.21.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.21.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ difficulty</span></span>
</span></span><span id="S5.T9.6.4.21.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.21.3" class="ltx_td ltx_align_right">70,000</td>
<td id="S5.T9.6.4.21.4" class="ltx_td ltx_align_center">73.48</td>
<td id="S5.T9.6.4.21.5" class="ltx_td ltx_nopad_r ltx_border_r"></td>
<td id="S5.T9.6.4.21.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
<td id="S5.T9.6.4.21.7" class="ltx_td ltx_nopad_l ltx_align_center" style="background-color:#92BFDB;"><span id="S5.T9.6.4.21.7.1" class="ltx_text" style="background-color:#92BFDB;">80.45</span></td>
<td id="S5.T9.6.4.21.8" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;"><span id="S5.T9.6.4.21.8.1" class="ltx_text" style="background-color:#E5F0F7;">43.15</span></td>
<td id="S5.T9.6.4.21.9" class="ltx_td ltx_align_center" style="background-color:#C6DEED;"><span id="S5.T9.6.4.21.9.1" class="ltx_text" style="background-color:#C6DEED;">34.59</span></td>
</tr>
<tr id="S5.T9.6.4.22" class="ltx_tr">
<td id="S5.T9.6.4.22.1" class="ltx_td ltx_border_bb"></td>
<td id="S5.T9.6.4.22.2" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S5.T9.6.4.22.2.1" class="ltx_text"></span><span id="S5.T9.6.4.22.2.2" class="ltx_text">
<span id="S5.T9.6.4.22.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T9.6.4.22.2.2.1.1" class="ltx_tr">
<span id="S5.T9.6.4.22.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">w/ scaling</span></span>
</span></span><span id="S5.T9.6.4.22.2.3" class="ltx_text"></span></td>
<td id="S5.T9.6.4.22.3" class="ltx_td ltx_align_right ltx_border_bb">220,000</td>
<td id="S5.T9.6.4.22.4" class="ltx_td ltx_align_center ltx_border_bb">57.78</td>
<td id="S5.T9.6.4.22.5" class="ltx_td ltx_nopad_r ltx_border_bb ltx_border_r"></td>
<td id="S5.T9.6.4.22.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb"></td>
<td id="S5.T9.6.4.22.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">58.12</td>
<td id="S5.T9.6.4.22.8" class="ltx_td ltx_align_center ltx_border_bb">38.07</td>
<td id="S5.T9.6.4.22.9" class="ltx_td ltx_align_center ltx_border_bb">27.28</td>
</tr>
</tbody></table>
</span></div>
</figure>
</section>
<section id="S5.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.4 </span>Empirical Analysis for Instruction Tuning</h4>

<div id="S5.SS1.SSS4.p1" class="ltx_para">
<p id="S5.SS1.SSS4.p1.1" class="ltx_p">Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks. In this section, we will explore the effect of different types of instructions in fine-tuning LLMs (<em id="S5.SS1.SSS4.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> LLaMA (7B) and LLaMA (13B)<span id="footnote27" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span>Due to the limit of computational resources, we cannot conduct large-scale experiments on larger LLaMA variants right now, which would be scheduled in a future version.</span></span></span>), as well as examine the usefulness of several instruction improvement strategies.</p>
</div>
<div id="S5.SS1.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS4.p2.1" class="ltx_p"><span id="S5.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Instruction Datasets.</span> According to the discussion in Section&nbsp;<a href="#S5.SS1.SSS1" title="5.1.1 Formatted Instance Construction ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>, we mainly consider three common kinds of instructions as follows:</p>
</div>
<div id="S5.SS1.SSS4.p3" class="ltx_para">
<p id="S5.SS1.SSS4.p3.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p3.1.1" class="ltx_emph ltx_font_italic">Task-specific instructions.</em> For the first type of instructions, we adopt the most commonly-used multi-task instruction dataset, <em id="S5.SS1.SSS4.p3.1.2" class="ltx_emph ltx_font_italic">FLAN-T5</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, which contains 1,836 tasks and over 15M instructions by combining four data mixtures from prior work.</p>
</div>
<div id="S5.SS1.SSS4.p4" class="ltx_para">
<p id="S5.SS1.SSS4.p4.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p4.1.1" class="ltx_emph ltx_font_italic">Daily chat instructions.</em> This type of instructions are conversations posed by users about daily life, which are more closely related to real-life scenarios. We adopt the ShareGPT instruciton set, consisting of 63K real-user instructions. It has been used as the core instructions for Vicuna.</p>
</div>
<div id="S5.SS1.SSS4.p5" class="ltx_para">
<p id="S5.SS1.SSS4.p5.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p5.1.1" class="ltx_emph ltx_font_italic">Synthetic instructions.</em> In addition to reusing existing instructions, we can also automatically synthesize massive instructions using LLMs. We adopt the popular synthetic instruction dataset Self-Instruct-52K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>, consisting of 52K instructions paired with about 82K instance inputs and outputs. These generated instructions have a similar data distribution as the human-written seed tasks (<em id="S5.SS1.SSS4.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> grammar checking, brainstorming).</p>
</div>
<div id="S5.SS1.SSS4.p6" class="ltx_para">
<p id="S5.SS1.SSS4.p6.1" class="ltx_p">As the original FLAN-T5 dataset is very large (<em id="S5.SS1.SSS4.p6.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> over 15M), we randomly sample 80,000 instructions from it for conducting a fair comparison with other instruction datasets (<em id="S5.SS1.SSS4.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> ShareGPT and Self-Instruct-52K) at a similar scale.
In our experiments, we test on each individual instruction set to explore their own effects and also examine their combinatorial effects on model performance.</p>
</div>
<div id="S5.SS1.SSS4.p7" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS4.p7.1" class="ltx_p"><span id="S5.SS1.SSS4.p7.1.1" class="ltx_text ltx_font_bold">Improvement Strategies.</span> Although real-world instructions from human users are more suitable for fine-tuning LLMs, it is difficult to collect them at a large scale.
As alternatives to human-generated instructions, most existing research mainly adopts synthetic instructions generated by LLMs.
However, there are some potential problems with synthetic instructions, such as poor topic diversity and uneven instruction difficulty (either too simple or too difficult).
Thus, it is necessary to improve the quality of the synthetic instructions.
Next, we summarize four major improvement strategies widely used in existing work as follows:</p>
</div>
<div id="S5.SS1.SSS4.p8" class="ltx_para">
<p id="S5.SS1.SSS4.p8.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p8.1.1" class="ltx_emph ltx_font_italic">Enhancing the instruction complexity.</em>
As discussed in existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite>, enhancing the complexity of instructions can improve the model capacity of LLMs in following complex instructions, <em id="S5.SS1.SSS4.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> including more task demands or requiring more reasoning steps.
To validate this strategy, we follow WizardLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite> by gradually increasing the complexity levels, <em id="S5.SS1.SSS4.p8.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> adding constraints, increasing reasoning steps, and complicating the input.
We leverage the publicly released WizardLM-70K instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite> as the complexity-enhanced instruction dataset, which has been generated via the above enhancement approach based on the Self-Instruct-52K dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS4.p9" class="ltx_para">
<p id="S5.SS1.SSS4.p9.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p9.1.1" class="ltx_emph ltx_font_italic">Increasing the topic diversity.</em>
In addition to the complexity,
improving the topic diversity of the instruction dataset can help elicit different abilities of LLMs on diverse tasks in real world&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib347" title="" class="ltx_ref">347</a>]</cite>.
However, it is difficult to directly control the self-instruct process for generating diverse instructions. Following YuLan-Chat&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib352" title="" class="ltx_ref">352</a>]</cite>, we employ ChatGPT to rewrite the instructions from Self-Instruct-52K dataset for adapting them into 293 topics via specific prompts.
Finally, we obtain 70K instructions as the diversity-increased dataset.</p>
</div>
<div id="S5.SS1.SSS4.p10" class="ltx_para">
<p id="S5.SS1.SSS4.p10.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p10.1.1" class="ltx_emph ltx_font_italic">Scaling the instruction number.</em>
In addition to the above aspects, the number of instructions is also an important factor that may affect the model performance.
Specially, using more instructions can extend the task knowledge and improve the ability of instruction following for LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
To examine this strategy, we sample new instructions from the synthesized instruction set released from the MOSS project&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib362" title="" class="ltx_ref">362</a>]</cite>, as they are also synthesized using the same self-instruct method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>.
We mix them with the Self-Instruct-52K dataset to compose a larger one containing 220K instructions.</p>
</div>
<div id="S5.SS1.SSS4.p11" class="ltx_para">
<p id="S5.SS1.SSS4.p11.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p11.1.1" class="ltx_emph ltx_font_italic">Balancing the instruction difficulty.</em>
As the synthetic instructions tend to contain too easy or too hard ones, it is likely to result in training instability or even overfitting for LLMs. To explore the potential effects, we leverage the perplexity score of LLMs to estimate the difficulty of instructions and remove too easy or too hard instructions.
To generate the same scale of instructions for fair comparison, we adopt a LLaMA (7B) model to compute the perplexity for the 220K instructions from the large instruction dataset, and then keep 70K instructions of moderate perplexity scores as the difficulty-balanced dataset.</p>
</div>
<div id="S5.SS1.SSS4.p12" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS4.p12.1" class="ltx_p"><span id="S5.SS1.SSS4.p12.1.1" class="ltx_text ltx_font_bold">Experimental Setup.</span>
To conduct the experiments on the effect of instruction data, we leverage these new instruction datasets for tuning LLaMA, a popular LLM backbone that has been widely used for instruction-tuning.
We use the code from YuLan-Chat&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib352" title="" class="ltx_ref">352</a>]</cite> for our experiments, and train LLaMA 7B and 13B on a server of 8 A800-80G GPUs.
All the hyper-parameters settings remain the same as Stanford Alpaca.
To better evaluate the instruction following ability of fine-tuned models, we consider two settings, namely
<em id="S5.SS1.SSS4.p12.1.2" class="ltx_emph ltx_font_italic">Chat setting</em> and <em id="S5.SS1.SSS4.p12.1.3" class="ltx_emph ltx_font_italic">QA setting</em>.
The chat setting mainly utilizes user instructions and queries from daily chat, whereas the QA setting mainly employs question answering examples from existing NLP datasets. 
The evaluation on the chat setting is conducted based on the AlpacaFarm evaluation set&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib363" title="" class="ltx_ref">363</a>]</cite>.
Instead of using a full pairwise comparison, we select the LLaMA 7B and 13B models fine-tuned on Self-Instruct-52K as the reference baselines, and then compare them with other fine-tuned LLaMA 7B and 13B models using different instructions, respectively. Since our focus is to examine the usefulness of different strategies to generate the instructions, the model fine-tuned on Self-Instruct-52K can serve as a good reference.
Following AlpacaFarm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib363" title="" class="ltx_ref">363</a>]</cite>, for each comparison, we employ ChatGPT to automatically annotate which response from two compared models each time is the best for the user query, and report the win rate (%) as the evaluation metric.
For the QA setting, we select two benchmarks, MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> and BBH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib365" title="" class="ltx_ref">365</a>]</cite>, and evaluate the accuracy based on their default settings by using heuristic rules to parse the answers from these LLMs.</p>
</div>
<div id="S5.SS1.SSS4.p13" class="ltx_para">
<p id="S5.SS1.SSS4.p13.2" class="ltx_p">For both instruction tuning and evaluation, we adopt the following prompt: “<em id="S5.SS1.SSS4.p13.2.2" class="ltx_emph ltx_font_italic">The following is a conversation between a human and an AI assistant. The AI assistant gives helpful, detailed, and polite answers to the user’s questions.<math id="S5.SS1.SSS4.p13.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S5.SS1.SSS4.p13.1.1.m1.1a"><mo id="S5.SS1.SSS4.p13.1.1.m1.1.1" xref="S5.SS1.SSS4.p13.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS4.p13.1.1.m1.1b"><ci id="S5.SS1.SSS4.p13.1.1.m1.1.1.cmml" xref="S5.SS1.SSS4.p13.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS4.p13.1.1.m1.1c">\backslash</annotation></semantics></math>n [|Human|]:<span id="S5.SS1.SSS4.p13.2.2.2" class="ltx_text ltx_font_upright">{</span>input<span id="S5.SS1.SSS4.p13.2.2.1" class="ltx_text ltx_font_upright">}<math id="S5.SS1.SSS4.p13.2.2.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S5.SS1.SSS4.p13.2.2.1.m1.1a"><mo id="S5.SS1.SSS4.p13.2.2.1.m1.1.1" xref="S5.SS1.SSS4.p13.2.2.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS4.p13.2.2.1.m1.1b"><ci id="S5.SS1.SSS4.p13.2.2.1.m1.1.1.cmml" xref="S5.SS1.SSS4.p13.2.2.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS4.p13.2.2.1.m1.1c">\backslash</annotation></semantics></math></span>n[|AI|]:</em>”.
To reproduce our results, we release the code and data at the link: <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments</a>.</p>
</div>
<div id="S5.SS1.SSS4.p14" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS4.p14.1" class="ltx_p"><span id="S5.SS1.SSS4.p14.1.1" class="ltx_text ltx_font_bold">Results and Analysis.</span>
The results using different instruction datasets based on 7B and 13B LLaMA are in Table&nbsp;<a href="#S5.T9" title="TABLE IX ‣ 5.1.3 The Effect of Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>. Next, we summarize and analyze our findings in detail.</p>
</div>
<div id="S5.SS1.SSS4.p15" class="ltx_para">
<p id="S5.SS1.SSS4.p15.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p15.1.1" class="ltx_emph ltx_font_italic">Task-formatted instructions are more proper for the QA setting, but may not be useful for the chat setting.</em>
By comparing the performance of instruction tuning using FLAN-T5 with that of ShareGPT and Self-Instruct-52K, we can observe that FLAN-T5 mostly achieves a better performance on QA benchmarks while underperforms ShareGPT on the chat setting. The reason is that FLAN-T5 is composed of a mixture of instructions and examples from existing NLP tasks, <em id="S5.SS1.SSS4.p15.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> translation and reading comprehension.
As a result, LLaMA fine-tuned with FLAN-T5 performs better on QA tasks, but poorly on user queries.
In contrast, ShareGPT consists of real-world human-ChatGPT conversations, which is able to better elicit LLaMA to follow user instructions in daily life, while may not be suitable for accomplishing the QA tasks.</p>
</div>
<div id="S5.SS1.SSS4.p16" class="ltx_para">
<p id="S5.SS1.SSS4.p16.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p16.1.1" class="ltx_emph ltx_font_italic">A mixture of different kinds of instructions are helpful to improve the comprehensive abilities of LLMs.</em>
After mixing the three kinds of instructions for fine-tuning, we can see that the derived LLaMA variant (with FLAN-T5, ShareGPT and Self-Instruct-52K) performs well in both task settings.
In MMLU, the performance of LLaMA (7B) can surpass the ones using individual instruction set by a large margin, <em id="S5.SS1.SSS4.p16.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> 43.69 vs. 38.58 (FLAN-T5).
It shows that mixing multiple sources of instruction datasets is helpful to improve the performance of instruction-tuned LLMs, which scales the instruction number as well as increases the diversity.</p>
</div>
<div id="S5.SS1.SSS4.p17" class="ltx_para">
<p id="S5.SS1.SSS4.p17.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p17.1.1" class="ltx_emph ltx_font_italic">Enhancing the complexity and diversity of instructions leads to an improved model performance.</em>
By increasing the complexity and diversity of the Self-Instruct-52K dataset respectively, the chat and QA performance of LLaMA can be consistently improved, <em id="S5.SS1.SSS4.p17.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> from 37.52 to 39.73 in MMLU for LLaMA (7B).
It demonstrates that both strategies are useful to improve the instruction following ability of LLMs.
Further, we can see that improving the complexity yields a larger performance improvement on QA tasks.
The reason is that the QA tasks mostly consist of difficult questions for evaluating LLMs, which can be better solved by LLMs that have learned complex instructions at the fine-tuning stage.</p>
</div>
<div id="S5.SS1.SSS4.p18" class="ltx_para">
<p id="S5.SS1.SSS4.p18.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p18.1.1" class="ltx_emph ltx_font_italic">Simply increasing the number of instructions may not be that useful, and balancing the difficulty is not always helpful.</em>
As the results shown in Table&nbsp;<a href="#S5.T9" title="TABLE IX ‣ 5.1.3 The Effect of Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>, balancing the difficulty and increasing the number of fine-tuning instructions are not very helpful in our experiments.
Especially for scaling the instruction number, it even hurts the performance, <em id="S5.SS1.SSS4.p18.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> a decrease from 29.81 to 26.63 in BBH for LLaMA (7B).
It shows that simply scaling the number of synthesized instructions without quality control may not be effective to improve the performance.
Furthermore, fine-tuning with the instructions of moderate difficulty also performs well in the chat setting, while slightly decreasing the performance in the QA setting.
A possible reason is that we filter complex and hard instructions with large perplexity scores, hurting the model performance in answering complex questions.</p>
</div>
<div id="S5.SS1.SSS4.p19" class="ltx_para">
<p id="S5.SS1.SSS4.p19.1" class="ltx_p">•&nbsp;<em id="S5.SS1.SSS4.p19.1.1" class="ltx_emph ltx_font_italic">A larger model scale leads to a better instruction following performance.</em>
By comparing the performance of LLaMA (7B) and LLaMA (13B) models fine-tuned with the same set of instruction data, we can see that LLaMA (13B) mostly achieves a better performance.
It indicates that scaling the model size is helpful for improving the instruction following capability.
Besides, we can see that the QA performance has been improved a lot, <em id="S5.SS1.SSS4.p19.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> from 38.11 to 47.49 in MMLU.
It is likely because that the larger models generally have better knowledge utilization and reasoning capability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which can accurately answer more complex questions.</p>
</div>
<div id="S5.SS1.SSS4.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S5.SS1.SSS4.1.p1.pic1" class="ltx_picture" height="345.66" overflow="visible" version="1.1" width="288"><g transform="translate(0,345.66) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 339.75 C 0 343.02 2.64 345.66 5.91 345.66 L 282.09 345.66 C 285.35 345.66 288 343.02 288 339.75 L 288 5.91 C 288 2.64 285.35 0 282.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 321.7 L 286.03 321.7 L 286.03 5.91 C 286.03 3.73 284.27 1.97 282.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 327.61)"><foreignObject width="244.69" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S5.SS1.SSS4.1.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:176.8pt;">
<span id="S5.SS1.SSS4.1.p1.pic1.1.1.1.1.1.1" class="ltx_p">Instruction Tuning Suggestions</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="244.69" height="296.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S5.SS1.SSS4.1.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:176.8pt;">
<span id="S5.SS1.SSS4.1.p1.pic1.2.2.2.1.1.1" class="ltx_p">To conduct instruction tuning on LLMs, one can prepare the computational resources according to the basic statistics about the required number of GPUs and tuning time in Table&nbsp;<a href="#S5.T8" title="TABLE VIII ‣ 5.1.1 Formatted Instance Construction ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>.
After setting up the development environment, we recommend beginners to follow the code of Alpaca repository&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> for instruction tuning. Subsequently, one should select the base model and construct the instruction datasets as we discuss in this section.
When computational resources for training are constrained, users can utilize LoRA for parameter-efficient tuning (see Section&nbsp;<a href="#S5.SS3" title="5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>). As for inference, users can further use quantization methods to deploy LLMs on fewer or smaller GPUs (see Section&nbsp;<a href="#S5.SS4" title="5.4 Memory-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>).</span>
</span></foreignObject></g></g></svg>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">Alignment Tuning</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">This part first presents the background of alignment with its definition and criteria, then focuses on the collection of human feedback data for aligning LLMs, and finally discusses the key technique of reinforcement learning from human feedback (RLHF) for alignment tuning.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Background and Criteria for Alignment</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p"><span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Background.</span> LLMs have shown remarkable capabilities in a wide range of NLP tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. However, these models may sometimes exhibit unintended behaviors, <em id="S5.SS2.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> fabricating false information, pursuing inaccurate objectives, and producing harmful, misleading, and biased expressions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib366" title="" class="ltx_ref">366</a>]</cite>. For LLMs, the language modeling objective pre-trains the model parameters by word prediction while lacking the consideration of human values or preferences.
To avert these unexpected behaviors, human alignment has been proposed to make LLMs act in line with human expectations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib367" title="" class="ltx_ref">367</a>]</cite>.
However, unlike the original pre-training and adaptation tuning (<em id="S5.SS2.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> instruction tuning), such an alignment requires considering very different criteria (<em id="S5.SS2.SSS1.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> helpfulness, honesty, and harmlessness). It has been shown that alignment might harm the general abilities of LLMs to some extent, which is called <em id="S5.SS2.SSS1.p1.1.5" class="ltx_emph ltx_font_italic">alignment tax</em> in related literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p"><span id="S5.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Alignment Criteria.</span>
Recently, there is increasing attention on developing multifarious criteria to regulate the behaviors of LLMs.
Here, we take three representative alignment criteria (<em id="S5.SS2.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> helpful, honest, and harmless) as examples for discussion, which have been widely adopted in existing literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.

In addition, there are other alignment criteria for LLMs from different perspectives including behavior, intent, incentive, and inner aspects&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib366" title="" class="ltx_ref">366</a>]</cite>, which are essentially similar (or at least with similar alignment techniques) to the above three criteria.
It is also feasible to modify the three criteria according to specific needs, <em id="S5.SS2.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> substituting honesty with correctness&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>.
Next, we give brief explanations about the three representative alignment criteria:</p>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para">
<p id="S5.SS2.SSS1.p3.1" class="ltx_p"><math id="S5.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS1.p3.1.m1.1a"><mo id="S5.SS2.SSS1.p3.1.m1.1.1" xref="S5.SS2.SSS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p3.1.m1.1b"><ci id="S5.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS2.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">Helpfulness.</em> To be helpful, the LLM should demonstrate a clear attempt to assist users in solving their tasks or answering questions in a concise and efficient manner as possible. At a higher level, when further clarification is needed, the LLM should demonstrate the capability of eliciting additional relevant information through pertinent inquiries and exhibit suitable levels of sensitivity, perceptiveness, and prudence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>]</cite>. Realizing the alignment of helpful behavior is challenging for LLMs since it is difficult to precisely define and measure the intention of users&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib366" title="" class="ltx_ref">366</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS1.p4" class="ltx_para">
<p id="S5.SS2.SSS1.p4.1" class="ltx_p"><math id="S5.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS1.p4.1.m1.1a"><mo id="S5.SS2.SSS1.p4.1.m1.1.1" xref="S5.SS2.SSS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p4.1.m1.1b"><ci id="S5.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS2.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Honesty.</em> At a basic level, a LLM aligned to be honest should present accurate content to users instead of fabricating information. Additionally, it is crucial for the LLM to convey appropriate degrees of uncertainty in its output, in order to avoid any form of deception or misrepresentation of information. This requires the model to know about its capabilities and levels of knowledge (<em id="S5.SS2.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> “know unknowns”). According to the discussion in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>]</cite>, honesty is a more objective criterion compared to helpfulness and harmlessness, hence honesty alignment could potentially be developed with less reliance on human efforts.</p>
</div>
<div id="S5.SS2.SSS1.p5" class="ltx_para">
<p id="S5.SS2.SSS1.p5.1" class="ltx_p"><math id="S5.SS2.SSS1.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS1.p5.1.m1.1a"><mo id="S5.SS2.SSS1.p5.1.m1.1.1" xref="S5.SS2.SSS1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.1.m1.1b"><ci id="S5.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS2.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">Harmlessness.</em> To be harmless, it requires that the language produced by the model should not be offensive or discriminatory. To the best of its abilities, the model should be capable of detecting covert endeavors aimed at soliciting requests for malicious purposes. Ideally, when the model was induced to conduct a dangerous action (<em id="S5.SS2.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> committing a crime), the LLM should politely refuse. Nonetheless,
<em id="S5.SS2.SSS1.p5.1.3" class="ltx_emph ltx_font_italic">what behaviors</em> are deemed harmful and <em id="S5.SS2.SSS1.p5.1.4" class="ltx_emph ltx_font_italic">to what extent</em> vary amongst individuals or societies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>]</cite> highly depend on who is using the LLM, the type of the posed question, and the context (<em id="S5.SS2.SSS1.p5.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> time) at which the LLM is being used.</p>
</div>
<div id="S5.SS2.SSS1.p6" class="ltx_para">
<p id="S5.SS2.SSS1.p6.1" class="ltx_p">As we can see, these criteria are quite subjective, and are developed based on human cognition.
Thus, it is difficult to directly formulate them as optimization objectives for LLMs.
In existing work, there are many ways to fulfill these criteria when aligning LLMs.
A promising technique is <em id="S5.SS2.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">red teaming</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib369" title="" class="ltx_ref">369</a>]</cite>, which involves using manual or automated means to probe LLMs in an adversarial way to generate harmful outputs and then updates LLMs to prevent such outputs.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Collecting Human Feedback</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">During the pre-training stage, LLMs are trained using the language modeling objective on a large-scale corpus. However, it cannot take into account the subjective and qualitative evaluations of LLM outputs by humans (called <em id="S5.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">human feedback</em> in this survey).
High-quality human feedback is extremely important for aligning LLMs with human preferences and values.
In this part, we discuss how to select a team of human labelers for feedback data collection.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p"><span id="S5.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Human Labeler Selection.</span>
In existing work, the dominant method for generating human feedback data is human annotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib367" title="" class="ltx_ref">367</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. This highlights the critical role of selecting appropriate human labelers.
To provide high-quality feedback, human labelers are supposed to have a qualified level of education and excellent proficiency in English. For example, Sparrow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> requires human labelers to be UK-based native English speakers who have obtained at least an undergraduate-level educational qualification.
Even then, several studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib367" title="" class="ltx_ref">367</a>]</cite> have found that there still exists a mismatch between the intentions of researchers and human labelers, which may lead to low-quality human feedback and cause LLMs to produce unexpected output.
To address this issue, InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> further conducts a screening process to filter labelers by assessing the agreement between human labelers and researchers. Specifically, researchers first label a small amount of data and then measure the agreement between themselves and human labelers. The labelers with the highest agreement will be selected to proceed with the subsequent annotation work.
In some other work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib370" title="" class="ltx_ref">370</a>]</cite>, “super raters” are used to ensure the high quality of human feedback. Researchers evaluate the performance of human labelers and select a group of well-performing human labelers (<em id="S5.SS2.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> high agreement) as super raters. The super raters will be given priority to collaborate with the researchers in the subsequent study.
When human labelers annotate the output of LLMs, it is helpful to specify detailed instructions and provide instant guidance for human labelers, which can further regulate the annotation of labelers.</p>
</div>
<div id="S5.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p3.1" class="ltx_p"><span id="S5.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Human Feedback Collection.</span> In existing work, there are mainly three kinds of approaches to collecting feedback and preference data from human labelers.</p>
</div>
<div id="S5.SS2.SSS2.p4" class="ltx_para">
<p id="S5.SS2.SSS2.p4.1" class="ltx_p"><math id="S5.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS2.p4.1.m1.1a"><mo id="S5.SS2.SSS2.p4.1.m1.1.1" xref="S5.SS2.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p4.1.m1.1b"><ci id="S5.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS2.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">Ranking-based approach.</em> In early work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib367" title="" class="ltx_ref">367</a>]</cite>, human labelers often evaluate model-generated outputs in a coarse-grained manner (<em id="S5.SS2.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> only selecting the best) without taking into account more fine-grained alignment criteria. Nonetheless, different labelers may hold diverse opinions on the selection of the best candidate output, and this method disregards the unselected samples, which may lead to inaccurate or incomplete human feedback.
To address this issue, subsequent studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> introduce the Elo rating system to derive the preference ranking by comparing candidate outputs.
The ranking of outputs serves as the training signal that guides the model to prefer certain outputs over others, thus inducing outputs that are more reliable and safer.</p>
</div>
<div id="S5.SS2.SSS2.p5" class="ltx_para">
<p id="S5.SS2.SSS2.p5.1" class="ltx_p"><math id="S5.SS2.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS2.p5.1.m1.1a"><mo id="S5.SS2.SSS2.p5.1.m1.1.1" xref="S5.SS2.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p5.1.m1.1b"><ci id="S5.SS2.SSS2.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS2.SSS2.p5.1.1" class="ltx_emph ltx_font_italic">Question-based approach.</em>
Further, human labelers can provide more detailed feedback by answering certain questions designed by researchers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, covering the alignment criteria as well as additional constraints for LLMs. Specially, in WebGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, to assist the model in filtering and utilizing relevant information from retrieved documents, human labelers are required to answer questions with multiple options about whether the retrieved documents are useful for answering the given input.</p>
</div>
<div id="S5.SS2.SSS2.p6" class="ltx_para">
<p id="S5.SS2.SSS2.p6.1" class="ltx_p"><math id="S5.SS2.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS2.p6.1.m1.1a"><mo id="S5.SS2.SSS2.p6.1.m1.1.1" xref="S5.SS2.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p6.1.m1.1b"><ci id="S5.SS2.SSS2.p6.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS2.SSS2.p6.1.1" class="ltx_emph ltx_font_italic">Rule-based approach.</em>
Many studies also develop rule-based methods to provide more detailed human feedback.
As a typical case, Sparrow&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> not only selects the response that labelers consider the best but also uses a series of rules to test whether model-generated responses meet the alignment criteria of being helpful, correct, and harmless.
In this way, two kinds of human feedback data can be obtained: (1) the response preference feedback is obtained by comparing the quality of model-generated output in pairs, and (2) the rule violation feedback is obtained by collecting the assessment from human labelers (<em id="S5.SS2.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> a score indicating to what extent the generated output has violated the rules).
Furthermore, GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> utilizes a set of zero-shot classifiers (based on GPT-4 itself) as rule-based reward models, which can automatically determine whether the model-generated outputs violate a set of human-written rules.</p>
</div>
<div id="S5.SS2.SSS2.p7" class="ltx_para">
<p id="S5.SS2.SSS2.p7.1" class="ltx_p">In the following, we focus on a well-known technique, reinforcement learning from human feedback (RLHF), which has been widely used in the recent powerful LLMs such as ChatGPT. As discussed below, the alignment criteria introduced in Section&nbsp;<a href="#S5.SS2.SSS1" title="5.2.1 Background and Criteria for Alignment ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.1</span></a> can be fulfilled by learning from human feedback on the responses of LLMs to users’ queries.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Reinforcement Learning from Human Feedback</h4>

<figure id="S5.F12" class="ltx_figure"><img src="/html/2303.18223/assets/x12.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="230" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>The workflow of the RLHF algorithm.</figcaption>
</figure>
<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">To align LLMs with human values, reinforcement learning from human feedback (RLHF)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib367" title="" class="ltx_ref">367</a>]</cite> has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria (<em id="S5.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> helpfulness, honesty, and harmlessness).
RLHF employs reinforcement learning&nbsp;(RL) algorithms&nbsp;(<em id="S5.SS2.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Proximal Policy Optimization&nbsp;(PPO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>) to adapt LLMs to human feedback by learning a reward model. Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2303.18223/assets/x13.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>An illustration of four different parameter-efficient fine-tuning methods. MHA and FFN denote the multi-head attention and feed-forward networks in the Transformer layer, respectively.</figcaption>
</figure>
<div id="S5.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p"><span id="S5.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">RLHF System.</span>
The RLHF system mainly comprises three key components: a pre-trained LM to be aligned, a reward model learning from human feedback, and a RL algorithm training the LM.
Specifically, the <span id="S5.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">pre-trained LM</span> is typically a generative model that is initialized with existing pre-trained LM parameters. For example, OpenAI uses 175B GPT-3 for its first popular RLHF model, InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, and DeepMind uses the 280 billion parameter model Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> for its GopherCite model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib370" title="" class="ltx_ref">370</a>]</cite>.  Further, the <span id="S5.SS2.SSS3.p2.1.3" class="ltx_text ltx_font_italic">reward model&nbsp;(RM)</span> provides (learned) guidance signals that reflect human preferences for the text generated by the LM, usually in the form of a scalar value. The reward model can take on two forms: a fine-tuned LM or a LM trained de novo using human preference data.
Existing work typically employs reward models having a parameter scale different from that of the aligned LM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib370" title="" class="ltx_ref">370</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. For example, OpenAI uses 6B GPT-3 and DeepMind uses 7B Gopher as the reward model, respectively.
Finally, to optimize the pre-trained LM using the signal from the reward model, a specific <span id="S5.SS2.SSS3.p2.1.4" class="ltx_text ltx_font_italic">RL algorithm</span> is designed for large-scale model tuning. Specifically, Proximal Policy Optimization (PPO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> is a widely used RL algorithm for alignment in existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib370" title="" class="ltx_ref">370</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p3.1" class="ltx_p"><span id="S5.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Key Steps for RLHF.</span> Figure&nbsp;<a href="#S5.F12" title="Figure 12 ‣ 5.2.3 Reinforcement Learning from Human Feedback ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> illustrates the overall three-step process of RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> as introduced below.</p>
</div>
<div id="S5.SS2.SSS3.p4" class="ltx_para">
<p id="S5.SS2.SSS3.p4.1" class="ltx_p"><math id="S5.SS2.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS3.p4.1.m1.1a"><mo id="S5.SS2.SSS3.p4.1.m1.1.1" xref="S5.SS2.SSS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p4.1.m1.1b"><ci id="S5.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS3.p4.1.1" class="ltx_text ltx_font_italic">Supervised fine-tuning.</span> To make the LM initially perform desired behaviors, it usually needs to collect a supervised dataset containing input prompts (instruction) and desired outputs for fine-tuning the LM. These prompts and outputs can be written by human labelers for some specific tasks while ensuring the diversity of tasks.
For example, InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> asks human labelers to compose prompts (<em id="S5.SS2.SSS3.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> “<em id="S5.SS2.SSS3.p4.1.3" class="ltx_emph ltx_font_italic">List five ideas for how to regain enthusiasm for my career</em>”) and desired outputs for several generative tasks such as open QA, brainstorming, chatting, and rewriting. 
Note that the first step is optional in specific settings or scenarios.</p>
</div>
<div id="S5.SS2.SSS3.p5" class="ltx_para">
<p id="S5.SS2.SSS3.p5.1" class="ltx_p"><math id="S5.SS2.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS3.p5.1.m1.1a"><mo id="S5.SS2.SSS3.p5.1.m1.1.1" xref="S5.SS2.SSS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p5.1.m1.1b"><ci id="S5.SS2.SSS3.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS3.p5.1.1" class="ltx_text ltx_font_italic">Reward model training.</span> The second step is to train the RM using human feedback data. Specifically, we employ the LM to generate a certain number of output texts using sampled prompts (from either the supervised dataset or the human-generated prompt) as input. We then invite human labelers to annotate the preference for these pairs. The annotation process can be conducted in multiple forms, and a common approach is to annotate by ranking the generated candidate texts, which can reduce the inconsistency among annotators. Then, the RM is trained to predict the human-preferred output. In InstructGPT, labelers rank model-generated outputs from best to worst, and the RM (<em id="S5.SS2.SSS3.p5.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> 6B GPT-3) is trained to predict the ranking. Note that, in recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>]</cite>, the annotation of preference on response pairs has been conducted by an AI agent (usually an aligned LLM) instead of humans, which is called “<em id="S5.SS2.SSS3.p5.1.3" class="ltx_emph ltx_font_italic">reinforcement learning from AI feedback (RLAIF)</em>”.
LLMs trained with typical RLHF algorithms tend to generate harmless responses with less helpfulness, which is called <em id="S5.SS2.SSS3.p5.1.4" class="ltx_emph ltx_font_italic">evasion problem</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>]</cite>. To guarantee both the harmlessness and helpfulness, RLAIF generates the AI feedback based on pre-set alignment principles in instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>, <a href="#bib.bib372" title="" class="ltx_ref">372</a>]</cite>, which can also reduce the efforts of human annotation.</p>
</div>
<div id="S5.SS2.SSS3.p6" class="ltx_para">
<p id="S5.SS2.SSS3.p6.1" class="ltx_p"><math id="S5.SS2.SSS3.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS3.p6.1.m1.1a"><mo id="S5.SS2.SSS3.p6.1.m1.1.1" xref="S5.SS2.SSS3.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p6.1.m1.1b"><ci id="S5.SS2.SSS3.p6.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS3.p6.1.1" class="ltx_text ltx_font_italic">RL fine-tuning.</span> At this step, aligning (<em id="S5.SS2.SSS3.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> fine-tuning) the LM is formalized as an RL problem. In this setting, the pre-trained LM acts as the policy that takes as input a prompt and returns an output text, the action space of it is the vocabulary, the state is the currently generated token sequence, and the reward is provided by the RM. To avoid eviating significantly from the initial (before tuning) LM, a penalty term is commonly incorporated into the reward function.
For example, InstructGPT optimizes the LM against the RM using the PPO algorithm.
For each input prompt, InstructGPT calculates the KL divergence between the generated results from the current LM and the initial LM as the penalty.
It is noted that the second and final steps can be iterated in multiple turns for better aligning LLMs.
Due to the instability of the RL algorithm, recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib373" title="" class="ltx_ref">373</a>]</cite> replaces the RL tuning with another supervised fine-tuning by reusing the best ranked samples with higher rewards.</p>
</div>
<div id="S5.SS2.SSS3.p7" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p7.1" class="ltx_p"><span id="S5.SS2.SSS3.p7.1.1" class="ltx_text ltx_font_bold">Practical Strategies for RLHF.</span>
Although RLHF is promising to effectively improve the alignment of LLMs with humans, it is practically challenging for researchers to successfully implement it.
In this part, we focus on discussing several useful strategies and tricks for improving the effectiveness and efficiency of RLHF.
Concretely, we focus on the effective training of reward models, efficient and effective RL training, respectively.</p>
</div>
<div id="S5.SS2.SSS3.p8" class="ltx_para">
<p id="S5.SS2.SSS3.p8.1" class="ltx_p"><math id="S5.SS2.SSS3.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS3.p8.1.m1.1a"><mo id="S5.SS2.SSS3.p8.1.m1.1.1" xref="S5.SS2.SSS3.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p8.1.m1.1b"><ci id="S5.SS2.SSS3.p8.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS3.p8.1.1" class="ltx_text ltx_font_italic">Effective reward model training.</span>
Despite that InstructGPT used a small reward model (6B GPT model), increasing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> has shown it is often more effective to use a large reward model (<em id="S5.SS2.SSS3.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs.
In LLaMa 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, pretrained chat model checkpoints are used
to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge.
Whereas, it is common to encounter the overfitting problem when training large-scale reward models. As a simple yet effective solution, existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib374" title="" class="ltx_ref">374</a>, <a href="#bib.bib375" title="" class="ltx_ref">375</a>]</cite> has introduced the LM loss on the preferred response of the input prompt from the human-annotated alignment dataset as a regularizer, which alleviates the overfitting of the reward model on the binary classification task.
In addition, as there are multiple criteria for alignment (<em id="S5.SS2.SSS3.p8.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> helpfulness and honesty), it is often difficult to train a single reward model that can satisfy all the alignment criteria.
Therefore, it is useful to train multiple reward models that focus on different alignment criteria&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, and compute the final reward based on the produced ones from them via special combination strategies (<em id="S5.SS2.SSS3.p8.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> mean pooling and weighted sum).
Such a way enables more flexible rules or standards on multiple criteria, <em id="S5.SS2.SSS3.p8.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> relaxing the requirement on helpfulness while posing more strict limits on harmfulness.</p>
</div>
<div id="S5.SS2.SSS3.p9" class="ltx_para">
<p id="S5.SS2.SSS3.p9.3" class="ltx_p"><math id="S5.SS2.SSS3.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS3.p9.1.m1.1a"><mo id="S5.SS2.SSS3.p9.1.m1.1.1" xref="S5.SS2.SSS3.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p9.1.m1.1b"><ci id="S5.SS2.SSS3.p9.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p9.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS3.p9.3.2" class="ltx_text ltx_font_italic">Effective RL training.</span>
As the RL training process tends to be unstable and hyper-parameter sensitive,
it is suggested that the language model should be well supervised fine-tuned
before RL training, so as to reaching a good model capacity.
A commonly-used way is to fine-tune the LLM on its best outputs of the prompts (referred to as <em id="S5.SS2.SSS3.p9.3.3" class="ltx_emph ltx_font_italic">rejection sampling</em> or <em id="S5.SS2.SSS3.p9.2.1" class="ltx_emph ltx_font_italic">best-of-<math id="S5.SS2.SSS3.p9.2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.SSS3.p9.2.1.m1.1a"><mi id="S5.SS2.SSS3.p9.2.1.m1.1.1" xref="S5.SS2.SSS3.p9.2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p9.2.1.m1.1b"><ci id="S5.SS2.SSS3.p9.2.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p9.2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p9.2.1.m1.1c">N</annotation></semantics></math></em>) from the alignment dataset until convergence before RL.
Given a prompt, the LLM would first produce <math id="S5.SS2.SSS3.p9.3.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.SSS3.p9.3.m2.1a"><mi id="S5.SS2.SSS3.p9.3.m2.1.1" xref="S5.SS2.SSS3.p9.3.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p9.3.m2.1b"><ci id="S5.SS2.SSS3.p9.3.m2.1.1.cmml" xref="S5.SS2.SSS3.p9.3.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p9.3.m2.1c">N</annotation></semantics></math> outputs via the sampling algorithm, and then the best candidate from the model will be selected by the reward model for learning.
After fine-tuning the LLM on the best samples until convergence, the RL process will be performed to further improve the performance.
LLaMA 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> has successively trained five versions of RLHF models, where the LLM has been progressively improved with the improvement of the reward models.
In this way, the collected prompts and annotations of human preference data can better reflect the issues of the current model checkpoint, thus making special tuning to address these issues.
In addition, LLaMA 2 also adds samples from prior iterations into the subsequent ones, to alleviate the possible capacity regression issue during iterative optimization.</p>
</div>
<div id="S5.SS2.SSS3.p10" class="ltx_para">
<p id="S5.SS2.SSS3.p10.1" class="ltx_p"><math id="S5.SS2.SSS3.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS3.p10.1.m1.1a"><mo id="S5.SS2.SSS3.p10.1.m1.1.1" xref="S5.SS2.SSS3.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p10.1.m1.1b"><ci id="S5.SS2.SSS3.p10.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p10.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS3.p10.1.1" class="ltx_text ltx_font_italic">Efficient RL training.</span>
As the RL training requires to iterate the inference process of both the LLM and reward models, it would greatly increase the total memory and computation cost, especially for larger reward models and LLMs.
As a practical trick, we can deploy the reward model on a separate server, and invoke the corresponding API to work with the LLM on its own server.
In addition, as RLHF requires the LLM to generate multiple candidate outputs, instead of calling the sample decoding procedure for multiple times, it is more efficient to utilize the beam search decoding algorithm<span id="footnote28" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><a target="_blank" href="https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search</a></span></span></span>.
It only needs to perform one-pass decoding for response generation, meanwhile such a strategy can also enhance the diversity of the generated candidate responses.</p>
</div>
<div id="S5.SS2.SSS3.p11" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p11.1" class="ltx_p"><span id="S5.SS2.SSS3.p11.1.1" class="ltx_text ltx_font_bold">Process-Supervised RLHF.</span>

In existing literature of RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib376" title="" class="ltx_ref">376</a>]</cite>, the supervision signals for RL training can be generally classified into two distinct categories: outcome-supervision signals and process-supervision signals.
The outcome-supervised RLHF employs a quantitative score to assess the quality of the whole text generated by LLMs. In contrast, process-supervised RLHF offers an evaluation of each individual component (<em id="S5.SS2.SSS3.p11.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> sentence, word, or reasoning step) within the generated content, which can provide fine-grained supervision signals to guide the training, helping LLMs refine the undesired generation contents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib376" title="" class="ltx_ref">376</a>, <a href="#bib.bib377" title="" class="ltx_ref">377</a>]</cite>.
OpenAI has proposed a fine-grained annotation dataset named PRM800k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib377" title="" class="ltx_ref">377</a>]</cite> consisting of 12K process-annotated mathematical problems&nbsp;(<em id="S5.SS2.SSS3.p11.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> MATH dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib378" title="" class="ltx_ref">378</a>]</cite>) and 75K solutions generated by LLMs of these problems, where each reasoning step of mathematical problems is labeled as <em id="S5.SS2.SSS3.p11.1.4" class="ltx_emph ltx_font_italic">positive</em>, <em id="S5.SS2.SSS3.p11.1.5" class="ltx_emph ltx_font_italic">negative</em> or <em id="S5.SS2.SSS3.p11.1.6" class="ltx_emph ltx_font_italic">neutral</em> in PRM800k.
This fine-grained dataset has been utilized in existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib379" title="" class="ltx_ref">379</a>, <a href="#bib.bib377" title="" class="ltx_ref">377</a>]</cite> to train the process-supervised reward models&nbsp;(PRM),
and the probability from the prediction of each label can be considered as the supervision signals during RLHF procedure.
To effectively leverage process-supervision signals from PRMs, existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib376" title="" class="ltx_ref">376</a>]</cite> has utilized expert iteration&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib380" title="" class="ltx_ref">380</a>, <a href="#bib.bib381" title="" class="ltx_ref">381</a>]</cite>, an effective RL algorithm to improve the base policy via learning from expert policy.
Typically, expert iteration contains two main stages: policy improvement and distillation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib376" title="" class="ltx_ref">376</a>]</cite>.
In the policy improvement stage, expert policy processes the systematic search procedure to produce the samples.
PRMs provide process-supervision signals to guide expert policy in the search procedure and enhance the quality of samples.
Subsequently, during the distillation stage, the samples generated by expert policy in the first stage are utilized to improve the base policy through supervised fine-tuning.
In addition to expert iteration, PRMs can also be utilized to re-rank the candidates of the final answers generated by LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib377" title="" class="ltx_ref">377</a>]</cite> or to select better intermediate reasoning steps during step by step reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib379" title="" class="ltx_ref">379</a>, <a href="#bib.bib382" title="" class="ltx_ref">382</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4 </span>Alignment without RLHF</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">Although RLHF has achieved great success in aligning the behaviors of LLMs with human values and preferences, it also suffers from notable limitations. First, RLHF needs to train multiple LMs including the model being aligned, the reward model, and the reference model at the same time, which is tedious in algorithmic procedure and memory-consuming in practice. Besides, the commonly-used PPO algorithm in RLHF is rather complex and often sensitive to hyper-parameters. As an alternative, increasing studies explore to directly optimize LLMs to adhere to human preferences, using supervised fine-tuning without reinforcement learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib349" title="" class="ltx_ref">349</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p2.1" class="ltx_p"><span id="S5.SS2.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Overview.</span>
The basic idea of non-RL alignment approaches is to directly fine-tune LLMs with <em id="S5.SS2.SSS4.p2.1.2" class="ltx_emph ltx_font_italic">supervised learning</em> on high-quality <em id="S5.SS2.SSS4.p2.1.3" class="ltx_emph ltx_font_italic">alignment dataset</em>.
It basically assumes that response feedback or golden rules to avert unsafe behaviors have been injected or included in the specially curated alignment dataset, so that LLMs can directly learn aligned behaviors from these demonstration data via suitable fine-tuning strategies.
Thus, to implement this approach, two key issues are the construction of alignment dataset and the design of fine-tuning loss. For the first issue, the alignment dataset can be automatically constructed by an aligned LLMs according to human-written safety principles&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib347" title="" class="ltx_ref">347</a>]</cite> or refining existing examples using edits operations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib383" title="" class="ltx_ref">383</a>]</cite>. In addition, we can also reuse existing reward models to select high-rated responses from existing human feedback data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib373" title="" class="ltx_ref">373</a>]</cite>. For the second issue, non-RL alignment approaches mainly fine-tune LLMs in a supervised learning way (the same as the original instruction tuning loss) on a high-quality alignment dataset, meanwhile auxiliary learning objectives can be used to enhance the alignment performance, <em id="S5.SS2.SSS4.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> ranking responses or contrasting instruction-response pairs.</p>
</div>
<div id="S5.SS2.SSS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p3.1" class="ltx_p"><span id="S5.SS2.SSS4.p3.1.1" class="ltx_text ltx_font_bold">Alignment Data Collection.</span> The construction of alignment data is important to effectively align the behaviors of LLMs with human preferences. To collect high-quality alignment data,
some work tries to reuse existing reward models to select high-rated responses, and others explore to leverage powerful LLMs (<em id="S5.SS2.SSS4.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> ChatGPT) or build a simulated environment to generate synthetic alignment examples. Next, we will discuss these three lines of research.</p>
</div>
<div id="S5.SS2.SSS4.p4" class="ltx_para">
<p id="S5.SS2.SSS4.p4.1" class="ltx_p"><math id="S5.SS2.SSS4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS4.p4.1.m1.1a"><mo id="S5.SS2.SSS4.p4.1.m1.1.1" xref="S5.SS2.SSS4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS4.p4.1.m1.1b"><ci id="S5.SS2.SSS4.p4.1.m1.1.1.cmml" xref="S5.SS2.SSS4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS4.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS4.p4.1.1" class="ltx_text ltx_font_italic">Reward model based approaches.</span> The reward model in RLHF has been trained to measure the alignment degree on the responses of LLMs. It is straightforward to leverage existing reward models to select high-quality responses as alignment data for subsequent fine-tuning. Based on this idea, RAFT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib373" title="" class="ltx_ref">373</a>]</cite> adopts reward models trained on human preference data to rank the responses of LLMs and collect those with higher rewards for supervised fine-tuning.
In addition, the reward model can be also used to score model responses and assign them to different quality groups. Quark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib384" title="" class="ltx_ref">384</a>]</cite> sorts the responses of LLMs into different quantiles based on the reward scores. Each quantile is attached with a special reward token to represent the reward level of the quantile. Conditioned on the highest-reward tokens, LLMs are subsequently prompted to generate high-quality responses. Given an initial answer and the corresponding human feedback, ILF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib385" title="" class="ltx_ref">385</a>]</cite> first adopts LLMs to generate refined answers, then utilizes the reward model to select the answer that best matches the feedback for further training.
As valuable resources for aligning LLMs, several reward models have been released, including DeBERTa-base/large/xxlarge from OpenAssistant<span id="footnote29" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span>https://huggingface.co/OpenAssistant</span></span></span>, Moss-7B from Fudan<span id="footnote30" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><span class="ltx_tag ltx_tag_note">30</span>https://github.com/OpenLMLab/MOSS-RLHF</span></span></span>, and Flan-T5-xl from Stanford<span id="footnote31" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">31</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">31</sup><span class="ltx_tag ltx_tag_note">31</span>https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl</span></span></span>.</p>
</div>
<div id="S5.SS2.SSS4.p5" class="ltx_para">
<p id="S5.SS2.SSS4.p5.1" class="ltx_p"><math id="S5.SS2.SSS4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS4.p5.1.m1.1a"><mo id="S5.SS2.SSS4.p5.1.m1.1.1" xref="S5.SS2.SSS4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS4.p5.1.m1.1b"><ci id="S5.SS2.SSS4.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS4.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS4.p5.1.1" class="ltx_text ltx_font_italic">LLM based generative approaches.</span> Reward models help to select aligned data from model responses. However, training reward models itself necessitates substantial high-quality human-labeled data, which is typically expensive and in short supply.
In addition, although existing reward models can be reused, they might not be able to accurately capture the nonalignment behaviors in another separately trained LLM.
Therefore, some work explores leveraging powerful LLMs to automatically generate human-aligned data. As a representative work, constitutional AI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>]</cite> proposes that human supervision comes from a set of principles (<em id="S5.SS2.SSS4.p5.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> natural language instructions) governing AI behaviors. Based on these principles, LLMs will critique their own harmful responses and revise them repeatedly into finally aligned responses. Similarly, Self-Align&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib347" title="" class="ltx_ref">347</a>]</cite> first adopts self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> to generate instructions focusing on covering diverse topics. Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several in-context exemplars), to generate helpful, ethical, and reliable responses as alignment data.
To mitigate the limit that the original SFT method can only learn from positive responses,
FIGA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib386" title="" class="ltx_ref">386</a>]</cite> develops an improved supervised alignment approach, where both negative (the original output of low quality) and positive (the refined output by LLMs) responses are leveraged in a contrastive way, to enable LLMs to deeply understand what fine-grained revisions actually lead to good response.</p>
</div>
<div id="S5.SS2.SSS4.p6" class="ltx_para">
<p id="S5.SS2.SSS4.p6.1" class="ltx_p"><math id="S5.SS2.SSS4.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS4.p6.1.m1.1a"><mo id="S5.SS2.SSS4.p6.1.m1.1.1" xref="S5.SS2.SSS4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS4.p6.1.m1.1b"><ci id="S5.SS2.SSS4.p6.1.m1.1.1.cmml" xref="S5.SS2.SSS4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS4.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS4.p6.1.1" class="ltx_text ltx_font_italic">LLM based interactive approaches.</span> Most existing approaches train LLMs in isolation, where LLMs are not present in actual environments to improve themselves through external feedback signals. As a comparison, humans learn social norms and values from interactions with others in social environments&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib387" title="" class="ltx_ref">387</a>]</cite>. To mimic such a learning approach, Stable Alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> builds a simulated interaction environment consisting of a number of LLM agents, where AI agents keep interacting with and each other, receiving feedback on improvement.
Once a central agent receives an instruction, it produces a response and shares it with nearby agents. These critic agents generate feedback comprising ratings about the response and revision suggestions. Then the central agent would revise the original response following these suggestions. 
Such an alignment approach can be also extended to real-world environment with humans.</p>
</div>
<div id="S5.SS2.SSS4.p7" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p7.1" class="ltx_p"><span id="S5.SS2.SSS4.p7.1.1" class="ltx_text ltx_font_bold">Supervised Alignment Tuning.</span> After obtaining alignment data, it is also key to design suitable fine-tuning strategies for direct alignment. A straightforward approach is to optimize LLMs using the conventional sequence-to-sequence objective based on the alignment data.
In addition to the conventional optimization objective, several studies further explore auxiliary losses that enhance the learning from the alignment data.</p>
</div>
<div id="S5.SS2.SSS4.p8" class="ltx_para">
<p id="S5.SS2.SSS4.p8.1" class="ltx_p"><math id="S5.SS2.SSS4.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS4.p8.1.m1.1a"><mo id="S5.SS2.SSS4.p8.1.m1.1.1" xref="S5.SS2.SSS4.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS4.p8.1.m1.1b"><ci id="S5.SS2.SSS4.p8.1.m1.1.1.cmml" xref="S5.SS2.SSS4.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS4.p8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS4.p8.1.1" class="ltx_text ltx_font_italic">Primary training objective.</span> Since the alignment data typically consists of an input instruction and an output response, the primary training loss is still the traditional cross-entropy loss for sequence-to-sequence learning. Based on this loss, many studies propose a number of improvement variants for enhancing the supervised alignment tuning. For example, CoH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib388" title="" class="ltx_ref">388</a>]</cite> constructs the training data by prepending “<em id="S5.SS2.SSS4.p8.1.2" class="ltx_emph ltx_font_italic">A helpful answer:</em>” and “<em id="S5.SS2.SSS4.p8.1.3" class="ltx_emph ltx_font_italic">An unhelpful answer:</em>” to the annotated good and bad responses, respectively, and only compute losses for those response tokens with special masking. Quark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib384" title="" class="ltx_ref">384</a>]</cite> sorts model responses into different quantiles with varying alignment quality, it prepends a special reward token to each model response to represent the reward level of the response. Further, to enable the preference modeling via the maximum likelihood objective, DPO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib389" title="" class="ltx_ref">389</a>]</cite> first reparameterizes the response rewards using the policy model (<em id="S5.SS2.SSS4.p8.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> the language model being optimized), and then the original reward modelling objective can be reformulated only based on the policy model. In this way, DPO removes the explicit reward modeling step, and optimizing the new learning objective only involving the policy model is equivalent to optimizing the rewards.
Furthermore, FIGA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib386" title="" class="ltx_ref">386</a>]</cite> designs a fine-grained contrastive loss that aims to encourage desirable tokens, penalize undesirable ones, and disregard trivial tokens.</p>
</div>
<div id="S5.SS2.SSS4.p9" class="ltx_para">
<p id="S5.SS2.SSS4.p9.1" class="ltx_p"><math id="S5.SS2.SSS4.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.SSS4.p9.1.m1.1a"><mo id="S5.SS2.SSS4.p9.1.m1.1.1" xref="S5.SS2.SSS4.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS4.p9.1.m1.1b"><ci id="S5.SS2.SSS4.p9.1.m1.1.1.cmml" xref="S5.SS2.SSS4.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS4.p9.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.SSS4.p9.1.1" class="ltx_text ltx_font_italic">Auxiliary optimization objectives.</span> Besides the primary cross-entropy loss, several studies propose auxiliary training loss to enhance the learning from the alignment data. First, since the responses of each instruction can be scored by the reward model, the ranking loss can be used to train the model to preserve the ranking order of these responses. For example, RRHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib390" title="" class="ltx_ref">390</a>]</cite> samples responses from multiple sources, including model-generated responses, such as those derived from the model itself, ChatGPT, and GPT-4, as well as human-written responses, spanning both high-quality and low-quality instances. To align with the scores from reward models, it further optimizes the ranking loss by encouraging the model to have a higher conditional log probability for the response with a higher ranking. SLiC-HF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib391" title="" class="ltx_ref">391</a>]</cite> proposes to assess the similarity between model outputs and human preference via the distance in the latent space, and introduces specific calibration and regularization loss to calibrate the candidate sequences based on human-preference data. Second, to enhance the relatedness between the response and the instruction, some work adopts contrastive learning to push up the probability of correct instruction-response pairs while pushing down incorrect instruction-response pairs. Specifically, for an output response, the proposed approach in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib392" title="" class="ltx_ref">392</a>]</cite> contrasts the target instruction to the other irrelevant instructions. By doing so, it can enable the model to learn the right correlation between instructions and responses.</p>
</div>
</section>
<section id="S5.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.5 </span>Remarks on SFT and RLHF</h4>

<div id="S5.SS2.SSS5.p1" class="ltx_para">
<p id="S5.SS2.SSS5.p1.1" class="ltx_p">As discussed in Section&nbsp;<a href="#S5.SS1" title="5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, instruction tuning is the process of training pre-trained language models with
formatted demonstration data (instructions paired with desired outputs). At early exploration, instruction data was mainly collected from NLP tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, while it has been now extended to more diverse supervision data that pairs input and output texts (<em id="S5.SS2.SSS5.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> the utterances of open-ended dialogues). Training with such paired texts is also called <em id="S5.SS2.SSS5.p1.1.2" class="ltx_emph ltx_font_italic">supervised fine-tuning&nbsp;(SFT)</em> in the context of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
In this part, we mainly use the abbreviation <em id="S5.SS2.SSS5.p1.1.3" class="ltx_emph ltx_font_italic">SFT</em> for discussion but not instruction tuning, due to the simplicity and popularity.</p>
</div>
<div id="S5.SS2.SSS5.p2" class="ltx_para">
<p id="S5.SS2.SSS5.p2.1" class="ltx_p">Since SFT and RLHF are two major adaptation tuning methods for LLMs, it is important to understand the connections and difference between them.
Next, we make some discussions on this issue<span id="footnote32" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">32</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">32</sup><span class="ltx_tag ltx_tag_note">32</span>This part would be somehow subjective, mainly based on the authors’ opinions and experiences. Comments or corrections are welcome to enhance this part. </span></span></span>.</p>
</div>
<div id="S5.SS2.SSS5.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p3.1" class="ltx_p"><span id="S5.SS2.SSS5.p3.1.1" class="ltx_text ltx_font_bold">Overall Comparison with RL Formulation</span>. Following the discussion in Section&nbsp;<a href="#S5.SS2.SSS3" title="5.2.3 Reinforcement Learning from Human Feedback ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.3</span></a> (the part related to RL training), the text generation problem can be formulated as a decision-making process based on RL.
Taking a prompt as input, the task of a LLM is to generate a text completion that appropriately responds to the prompt. This task would be completed step by step. At each step, an agent (<em id="S5.SS2.SSS5.p3.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> LLM) will perform an action (<em id="S5.SS2.SSS5.p3.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> generating a token) according to the policy (<em id="S5.SS2.SSS5.p3.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> the generative probability distribution of LLM) conditioned on the current state (currently generated token sequence and other available context information).
It is expected that a high-quality output text would be produced by the LLM, which can earn a large reward score based on the entire response.
Overall, RLHF and SFT can be considered as two different training approaches to optimizing the above decision making process for LLMs.
Specially, RLHF
firstly learns the reward model, and then employs it to improve the LLM with RL training (<em id="S5.SS2.SSS5.p3.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> PPO). As a comparison, SFT adopts a teacher-forcing approach, which directly optimizes the likelihood of a demonstration output.
Such a token-level training way essentially does <em id="S5.SS2.SSS5.p3.1.6" class="ltx_emph ltx_font_italic">behavior cloning</em> (a special algorithm of imitation learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib393" title="" class="ltx_ref">393</a>]</cite>): it utilizes the expert’s action (<em id="S5.SS2.SSS5.p3.1.7" class="ltx_emph ltx_font_italic">i.e.,</em> the target token at each step) as the supervision label and directly learns to imitate the demonstrations from experts without specifying a reward model as in typical RL algorithms.
To learn the desired policies,
SFT adopts a “local” optimization way (<em id="S5.SS2.SSS5.p3.1.8" class="ltx_emph ltx_font_italic">i.e.,</em> token-level loss) based on demonstration data, while RLHF takes a “global” optimization way (<em id="S5.SS2.SSS5.p3.1.9" class="ltx_emph ltx_font_italic">i.e.,</em> text-level loss) by involving human preference. More theoretical analysis about imitation learning and reinforcement learning can be referred to the related RL literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib393" title="" class="ltx_ref">393</a>, <a href="#bib.bib394" title="" class="ltx_ref">394</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS5.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p4.1" class="ltx_p"><span id="S5.SS2.SSS5.p4.1.1" class="ltx_text ltx_font_bold">Pros and Cons of SFT</span>.
SFT has been shown to be an effective approach to boosting the performance of LLMs on various benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>, which can largely enhance the task generalization ability and flexibly endow specific functions (<em id="S5.SS2.SSS5.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> establishing the chatbot’s identity).
More discussions about the usefulness of SFT can be found in Section&nbsp;<a href="#S5.SS1.SSS3" title="5.1.3 The Effect of Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>.
It has been widely recognized that SFT mainly <em id="S5.SS2.SSS5.p4.1.3" class="ltx_emph ltx_font_italic">unlocks</em> the abilities but not <em id="S5.SS2.SSS5.p4.1.4" class="ltx_emph ltx_font_italic">inject</em> new abilities into LLMs.
Thus, it might become problematic when one tries to stimulate the non-endogenous abilities of LLMs via SFT. As a concrete scenario, it would potentially advocate the hallucination behaviors when demonstration data is beyond the knowledge or ability scope of LLMs, <em id="S5.SS2.SSS5.p4.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> training a LLM to answer questions about its unknown facts.
An interesting viewpoint from John Schulman’s talk on RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib395" title="" class="ltx_ref">395</a>]</cite> is that distilling superior models to train less capable models (<em id="S5.SS2.SSS5.p4.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> prompting GPT-4 to generate the response as fine-tuning data) might increase the possibilities of generating the hallucinated texts, thus likely affecting the factual accuracy of LLMs.
Furthermore, as a behavior cloning method, SFT aims to imitate the behaviors (without explorations) of the experts who construct the demonstration data.
However, there often exist variations among different annotators on the writing styles, quality, and preferences of demonstration data, which tends to affect the learning performance of SFT.
Thus, high-quality instruction data (but not the quantity) is the primary factor for effective training of LLMs during the SFT stage&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>.</p>
</div>
<div id="S5.SS2.SSS5.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p5.1" class="ltx_p"><span id="S5.SS2.SSS5.p5.1.1" class="ltx_text ltx_font_bold">Pros and Cons of RLHF</span>. RLHF was early explored in the literature of deep RL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, then borrowed to improve the capacity of language models (<em id="S5.SS2.SSS5.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> summarization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>), and subsequently adopted as the fundamental technique to develop InstructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. Recently, increasing evidence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib371" title="" class="ltx_ref">371</a>]</cite> has demonstrated the effectiveness of RLHF in mitigating the harmful responses and enhancing the model capacity.
Specially, LLaMA 2 has demonstrated that RLHF can improve both the helpfulness and harmlessness scores&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, and attributed this to a better human-LLM synergy for data annotation.
They explain this reason in two major aspects as follows.
First, since human annotators mainly provide preference annotations for RLHF,
it can largely alleviate the discrepancies of annotators as that in SFT. Secondly,
preference annotation is much easier than writing the demonstration data, and annotators can even judge the quality of more superior generations than those they create, making it possible to explore a broader state space beyond what can be demonstrated by human annotators.
Another key point is that RLHF essentially encourages LLMs to learn correct policies by contrasting the self-generated responses (discriminating between good and bad responses). It no longer forces the model to imitate external demonstration data, and thus can mitigate the hallucination issues with SFT as discussed above<span id="footnote33" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">33</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">33</sup><span class="ltx_tag ltx_tag_note">33</span>In RLHF, it seems to be also important that reward models should be aware of the knowledge or ability of a LLM to be aligned. For example, LLaMA 2 adopts pre-trained chat model checkpoints to initialize reward models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. </span></span></span>. Actually, RLHF has been demonstrated to be an important approach to reduce the hallucination behaviors in GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
However, RLHF inherits the drawbacks of classic RL algorithms, <em id="S5.SS2.SSS5.p5.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> sample inefficiency and training instability.
When adapted to LLMs, RLHF further relies on a strong SFT model as initial model checkpoint for efficiently achieving good performance.
In addition, human annotators are involved in a complex iterative optimization process, in which a number of important details (<em id="S5.SS2.SSS5.p5.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> the prompt selection, the schedule of reward model training and PPO training, and the settings of hyper-parameters) have important impact on the whole model performance.</p>
</div>
<div id="S5.SS2.SSS5.p6" class="ltx_para">
<p id="S5.SS2.SSS5.p6.1" class="ltx_p">Overall, SFT is particularly useful to increase the model capacity of pre-trained model checkpoints right after pre-training, while RLHF is promising to further improve the model capacity of SFT models. However, RLHF has been difficult to implement, and far from well explored (according to public literature), and more improvements (<em id="S5.SS2.SSS5.p6.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> efficient and reliable annotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>]</cite> and simplified optimization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib389" title="" class="ltx_ref">389</a>]</cite>) are still needed for further research.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">Parameter-Efficient Model Adaptation</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In the above, we have discussed the approaches of instruction tuning and alignment tuning to adapt LLMs according to specific goals. Since LLMs consist of a huge amount of model parameters, it would be costly to perform the full-parameter tuning. In this section, we will discuss how to conduct efficient tuning on LLMs. We first review several representative parameter-efficient fine-tuning methods for Transformer language models, and then summarize existing work on parameter-efficient fine-tuned LLMs.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Parameter-Efficient Fine-Tuning Methods</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">In existing literature, parameter-efficient fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>, <a href="#bib.bib397" title="" class="ltx_ref">397</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> has been an important topic that aims to reduce the number of trainable parameters while retaining a good performance as possible. In what follows, we briefly review four parameter-efficient fine-tuning methods for Transformer language models, including adapter tuning, prefix tuning, prompt tuning and
LoRA. The illustration of these four methods are shown in Figure&nbsp;<a href="#S5.F13" title="Figure 13 ‣ 5.2.3 Reinforcement Learning from Human Feedback ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p"><span id="S5.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Adapter Tuning</span>. Adapter tuning incorporates small neural network modules (called <em id="S5.SS3.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">adapter</em>) into the Transformer models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib398" title="" class="ltx_ref">398</a>]</cite>. To implement the adapter module, a bottleneck architecture has been proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib398" title="" class="ltx_ref">398</a>, <a href="#bib.bib399" title="" class="ltx_ref">399</a>]</cite>, which first compresses the original feature vector into a smaller dimension (followed by a nonlinear transformation) and then recovers it to the original dimension.
The adapter modules would be integrated into each Transformer layer, typically using a serial insertion after each of the two core parts (<em id="S5.SS3.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> attention layer and feed-forward layer) of a Transformer layer.
Alternatively, parallel adapters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib400" title="" class="ltx_ref">400</a>]</cite> can be also used in Transformer layers, where it places two adapter modules in parallel with the attention layer and feed-forward layer accordingly.
During fine-tuning, the adapter modules would be optimized according to the specific task goals, while the parameters of the original language model are frozen in this process.
In this way, we can effectively reduce the number of trainable parameters during fine-tuning.</p>
</div>
<div id="S5.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS1.p3.1" class="ltx_p"><span id="S5.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Prefix Tuning</span>.
Prefix tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>]</cite> prepends a sequence of prefixes, which are a set of trainable continuous vectors, to each Transformer layer in language models. These prefix vectors are task-specific, which can be considered as virtual token embeddings. To optimize the prefix vectors, a reparameterization trick&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>]</cite> has been proposed by learning a MLP function that maps a smaller matrix to the parameter matrix of prefixes, instead of directly optimizing the prefixes.
It has been shown that this trick is useful for stable training.
After optimization, the mapping function would be discarded, and only the derived prefix vectors are kept to enhance task-specific performance.
Since only the prefix parameters would be trained, it can lead to a parameter-efficient model optimization.
Similar to prefix tuning, p-tuning v2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib401" title="" class="ltx_ref">401</a>]</cite> incorporates layer-wise prompt vectors into the Transformer architecture specially for natural language understanding, which also utilizes multi-task learning for jointly optimizing shared prompts.
It has been shown to be useful in improving the model performance of different parameter scales on natural language understanding tasks.</p>
</div>
<div id="S5.SS3.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS1.p4.1" class="ltx_p"><span id="S5.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Prompt Tuning</span>. Different from prefix tuning, prompt tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib402" title="" class="ltx_ref">402</a>, <a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite> mainly focuses on incorporating trainable prompt vectors at the input layer<span id="footnote34" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">34</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">34</sup><span class="ltx_tag ltx_tag_note">34</span>Here, prompt tuning denotes a category of related efficient tuning methods exemplified by the work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib402" title="" class="ltx_ref">402</a>, <a href="#bib.bib397" title="" class="ltx_ref">397</a>, <a href="#bib.bib403" title="" class="ltx_ref">403</a>]</cite>, instead of a specific method as used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite>. Indeed, the prefix based tuning methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>, <a href="#bib.bib401" title="" class="ltx_ref">401</a>]</cite> can be also considered as prompting methods, which are called <em id="footnote34.1" class="ltx_emph ltx_font_italic">deep prompting tuning</em> in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib401" title="" class="ltx_ref">401</a>]</cite>. In this survey, prompt tuning specially refer to the methods that only include the prompt tokens at the input layer, in the context of LLMs.
We assign p-tuning v2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib401" title="" class="ltx_ref">401</a>]</cite> to the category of prefix tuning, because it incorporates layerwise prompts in langauge models. </span></span></span>.
Based on the discrete prompting methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib404" title="" class="ltx_ref">404</a>, <a href="#bib.bib405" title="" class="ltx_ref">405</a>]</cite>, it augments the input text by including a group of soft prompt tokens (either in a free form&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib402" title="" class="ltx_ref">402</a>]</cite> or a prefix form&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite>), and then takes the prompt-augmented input to solve specific downstream tasks.
In implementation, task-specific prompt embeddings are combined with the input text embeddings, which are subsequently fed into language models. P-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib402" title="" class="ltx_ref">402</a>]</cite> has proposed a free form to combine the context, prompt and target tokens, which can be applied to the architectures for both natural language understanding and generation. They further learn the representations of soft prompt tokens by a bidirectional LSTM.
Another representative approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite> named <em id="S5.SS3.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">prompt tuning</em> directly prepends prefix prompts to the input.
During training, only the prompt embeddings would be learned according to task-specific supervisions.
Since this method only includes a small number of trainable parameters at the input layer, it has been found that the performance highly relies on the model capacity of the underlying language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite>.</p>
</div>
<div id="S5.SS3.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS1.p5.8" class="ltx_p"><span id="S5.SS3.SSS1.p5.8.1" class="ltx_text ltx_font_bold">Low-Rank Adaptation&nbsp;(LoRA)</span>. LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> imposes the low-rank constraint for approximating the update matrix at each dense layer, so as to reduce the trainable parameters for adapting to downstream tasks.
Consider the case of optimizing a parameter matrix <math id="S5.SS3.SSS1.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S5.SS3.SSS1.p5.1.m1.1a"><mi id="S5.SS3.SSS1.p5.1.m1.1.1" xref="S5.SS3.SSS1.p5.1.m1.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.1.m1.1b"><ci id="S5.SS3.SSS1.p5.1.m1.1.1.cmml" xref="S5.SS3.SSS1.p5.1.m1.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.1.m1.1c">\mathbf{W}</annotation></semantics></math>. The update process can be written in a general form as: <math id="S5.SS3.SSS1.p5.2.m2.1" class="ltx_Math" alttext="\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}" display="inline"><semantics id="S5.SS3.SSS1.p5.2.m2.1a"><mrow id="S5.SS3.SSS1.p5.2.m2.1.1" xref="S5.SS3.SSS1.p5.2.m2.1.1.cmml"><mi id="S5.SS3.SSS1.p5.2.m2.1.1.2" xref="S5.SS3.SSS1.p5.2.m2.1.1.2.cmml">𝐖</mi><mo stretchy="false" id="S5.SS3.SSS1.p5.2.m2.1.1.1" xref="S5.SS3.SSS1.p5.2.m2.1.1.1.cmml">←</mo><mrow id="S5.SS3.SSS1.p5.2.m2.1.1.3" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.cmml"><mi id="S5.SS3.SSS1.p5.2.m2.1.1.3.2" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.2.cmml">𝐖</mi><mo id="S5.SS3.SSS1.p5.2.m2.1.1.3.1" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.1.cmml">+</mo><mrow id="S5.SS3.SSS1.p5.2.m2.1.1.3.3" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.cmml"><mi mathvariant="normal" id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.2" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.1" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.3" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.3.cmml">𝐖</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.2.m2.1b"><apply id="S5.SS3.SSS1.p5.2.m2.1.1.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1"><ci id="S5.SS3.SSS1.p5.2.m2.1.1.1.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.1">←</ci><ci id="S5.SS3.SSS1.p5.2.m2.1.1.2.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.2">𝐖</ci><apply id="S5.SS3.SSS1.p5.2.m2.1.1.3.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3"><plus id="S5.SS3.SSS1.p5.2.m2.1.1.3.1.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.1"></plus><ci id="S5.SS3.SSS1.p5.2.m2.1.1.3.2.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.2">𝐖</ci><apply id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3"><times id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.1.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.1"></times><ci id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.2.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.2">Δ</ci><ci id="S5.SS3.SSS1.p5.2.m2.1.1.3.3.3.cmml" xref="S5.SS3.SSS1.p5.2.m2.1.1.3.3.3">𝐖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.2.m2.1c">\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}</annotation></semantics></math>.
The basic idea of LoRA is to freeze the original matrix <math id="S5.SS3.SSS1.p5.3.m3.1" class="ltx_Math" alttext="\mathbf{W}\in\mathbb{R}^{m\times n}" display="inline"><semantics id="S5.SS3.SSS1.p5.3.m3.1a"><mrow id="S5.SS3.SSS1.p5.3.m3.1.1" xref="S5.SS3.SSS1.p5.3.m3.1.1.cmml"><mi id="S5.SS3.SSS1.p5.3.m3.1.1.2" xref="S5.SS3.SSS1.p5.3.m3.1.1.2.cmml">𝐖</mi><mo id="S5.SS3.SSS1.p5.3.m3.1.1.1" xref="S5.SS3.SSS1.p5.3.m3.1.1.1.cmml">∈</mo><msup id="S5.SS3.SSS1.p5.3.m3.1.1.3" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.cmml"><mi id="S5.SS3.SSS1.p5.3.m3.1.1.3.2" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS3.SSS1.p5.3.m3.1.1.3.3" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.cmml"><mi id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.2" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.1" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.3" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.3.m3.1b"><apply id="S5.SS3.SSS1.p5.3.m3.1.1.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1"><in id="S5.SS3.SSS1.p5.3.m3.1.1.1.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.1"></in><ci id="S5.SS3.SSS1.p5.3.m3.1.1.2.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.2">𝐖</ci><apply id="S5.SS3.SSS1.p5.3.m3.1.1.3.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.SSS1.p5.3.m3.1.1.3.1.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3">superscript</csymbol><ci id="S5.SS3.SSS1.p5.3.m3.1.1.3.2.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.2">ℝ</ci><apply id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3"><times id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.1.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.1"></times><ci id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.2.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.2">𝑚</ci><ci id="S5.SS3.SSS1.p5.3.m3.1.1.3.3.3.cmml" xref="S5.SS3.SSS1.p5.3.m3.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.3.m3.1c">\mathbf{W}\in\mathbb{R}^{m\times n}</annotation></semantics></math> while approximating the parameter update <math id="S5.SS3.SSS1.p5.4.m4.1" class="ltx_Math" alttext="\Delta\mathbf{W}" display="inline"><semantics id="S5.SS3.SSS1.p5.4.m4.1a"><mrow id="S5.SS3.SSS1.p5.4.m4.1.1" xref="S5.SS3.SSS1.p5.4.m4.1.1.cmml"><mi mathvariant="normal" id="S5.SS3.SSS1.p5.4.m4.1.1.2" xref="S5.SS3.SSS1.p5.4.m4.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.SS3.SSS1.p5.4.m4.1.1.1" xref="S5.SS3.SSS1.p5.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS3.SSS1.p5.4.m4.1.1.3" xref="S5.SS3.SSS1.p5.4.m4.1.1.3.cmml">𝐖</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.4.m4.1b"><apply id="S5.SS3.SSS1.p5.4.m4.1.1.cmml" xref="S5.SS3.SSS1.p5.4.m4.1.1"><times id="S5.SS3.SSS1.p5.4.m4.1.1.1.cmml" xref="S5.SS3.SSS1.p5.4.m4.1.1.1"></times><ci id="S5.SS3.SSS1.p5.4.m4.1.1.2.cmml" xref="S5.SS3.SSS1.p5.4.m4.1.1.2">Δ</ci><ci id="S5.SS3.SSS1.p5.4.m4.1.1.3.cmml" xref="S5.SS3.SSS1.p5.4.m4.1.1.3">𝐖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.4.m4.1c">\Delta\mathbf{W}</annotation></semantics></math> by low-rank decomposition matrices, <em id="S5.SS3.SSS1.p5.8.2" class="ltx_emph ltx_font_italic">i.e.,</em> <math id="S5.SS3.SSS1.p5.5.m5.1" class="ltx_Math" alttext="\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}" display="inline"><semantics id="S5.SS3.SSS1.p5.5.m5.1a"><mrow id="S5.SS3.SSS1.p5.5.m5.1.1" xref="S5.SS3.SSS1.p5.5.m5.1.1.cmml"><mrow id="S5.SS3.SSS1.p5.5.m5.1.1.2" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="S5.SS3.SSS1.p5.5.m5.1.1.2.2" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S5.SS3.SSS1.p5.5.m5.1.1.2.1" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.1.cmml">​</mo><mi id="S5.SS3.SSS1.p5.5.m5.1.1.2.3" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.3.cmml">𝐖</mi></mrow><mo id="S5.SS3.SSS1.p5.5.m5.1.1.1" xref="S5.SS3.SSS1.p5.5.m5.1.1.1.cmml">=</mo><mrow id="S5.SS3.SSS1.p5.5.m5.1.1.3" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.cmml"><mi id="S5.SS3.SSS1.p5.5.m5.1.1.3.2" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.2.cmml">𝐀</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.SSS1.p5.5.m5.1.1.3.1" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.1.cmml">⋅</mo><msup id="S5.SS3.SSS1.p5.5.m5.1.1.3.3" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3.cmml"><mi id="S5.SS3.SSS1.p5.5.m5.1.1.3.3.2" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3.2.cmml">𝐁</mi><mo id="S5.SS3.SSS1.p5.5.m5.1.1.3.3.3" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3.3.cmml">⊤</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.5.m5.1b"><apply id="S5.SS3.SSS1.p5.5.m5.1.1.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1"><eq id="S5.SS3.SSS1.p5.5.m5.1.1.1.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.1"></eq><apply id="S5.SS3.SSS1.p5.5.m5.1.1.2.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.2"><times id="S5.SS3.SSS1.p5.5.m5.1.1.2.1.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.1"></times><ci id="S5.SS3.SSS1.p5.5.m5.1.1.2.2.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.2">Δ</ci><ci id="S5.SS3.SSS1.p5.5.m5.1.1.2.3.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.2.3">𝐖</ci></apply><apply id="S5.SS3.SSS1.p5.5.m5.1.1.3.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3"><ci id="S5.SS3.SSS1.p5.5.m5.1.1.3.1.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.1">⋅</ci><ci id="S5.SS3.SSS1.p5.5.m5.1.1.3.2.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.2">𝐀</ci><apply id="S5.SS3.SSS1.p5.5.m5.1.1.3.3.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S5.SS3.SSS1.p5.5.m5.1.1.3.3.1.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3">superscript</csymbol><ci id="S5.SS3.SSS1.p5.5.m5.1.1.3.3.2.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3.2">𝐁</ci><csymbol cd="latexml" id="S5.SS3.SSS1.p5.5.m5.1.1.3.3.3.cmml" xref="S5.SS3.SSS1.p5.5.m5.1.1.3.3.3">top</csymbol></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.5.m5.1c">\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}</annotation></semantics></math>, where <math id="S5.SS3.SSS1.p5.6.m6.1" class="ltx_Math" alttext="\mathbf{A}\in\mathbb{R}^{m\times k}" display="inline"><semantics id="S5.SS3.SSS1.p5.6.m6.1a"><mrow id="S5.SS3.SSS1.p5.6.m6.1.1" xref="S5.SS3.SSS1.p5.6.m6.1.1.cmml"><mi id="S5.SS3.SSS1.p5.6.m6.1.1.2" xref="S5.SS3.SSS1.p5.6.m6.1.1.2.cmml">𝐀</mi><mo id="S5.SS3.SSS1.p5.6.m6.1.1.1" xref="S5.SS3.SSS1.p5.6.m6.1.1.1.cmml">∈</mo><msup id="S5.SS3.SSS1.p5.6.m6.1.1.3" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.cmml"><mi id="S5.SS3.SSS1.p5.6.m6.1.1.3.2" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS3.SSS1.p5.6.m6.1.1.3.3" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.cmml"><mi id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.2" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.1" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.1.cmml">×</mo><mi id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.3" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.6.m6.1b"><apply id="S5.SS3.SSS1.p5.6.m6.1.1.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1"><in id="S5.SS3.SSS1.p5.6.m6.1.1.1.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.1"></in><ci id="S5.SS3.SSS1.p5.6.m6.1.1.2.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.2">𝐀</ci><apply id="S5.SS3.SSS1.p5.6.m6.1.1.3.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.SSS1.p5.6.m6.1.1.3.1.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3">superscript</csymbol><ci id="S5.SS3.SSS1.p5.6.m6.1.1.3.2.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.2">ℝ</ci><apply id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3"><times id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.1.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.1"></times><ci id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.2.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.2">𝑚</ci><ci id="S5.SS3.SSS1.p5.6.m6.1.1.3.3.3.cmml" xref="S5.SS3.SSS1.p5.6.m6.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.6.m6.1c">\mathbf{A}\in\mathbb{R}^{m\times k}</annotation></semantics></math> and <math id="S5.SS3.SSS1.p5.7.m7.1" class="ltx_Math" alttext="\mathbf{B}\in\mathbb{R}^{n\times k}" display="inline"><semantics id="S5.SS3.SSS1.p5.7.m7.1a"><mrow id="S5.SS3.SSS1.p5.7.m7.1.1" xref="S5.SS3.SSS1.p5.7.m7.1.1.cmml"><mi id="S5.SS3.SSS1.p5.7.m7.1.1.2" xref="S5.SS3.SSS1.p5.7.m7.1.1.2.cmml">𝐁</mi><mo id="S5.SS3.SSS1.p5.7.m7.1.1.1" xref="S5.SS3.SSS1.p5.7.m7.1.1.1.cmml">∈</mo><msup id="S5.SS3.SSS1.p5.7.m7.1.1.3" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.cmml"><mi id="S5.SS3.SSS1.p5.7.m7.1.1.3.2" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS3.SSS1.p5.7.m7.1.1.3.3" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.cmml"><mi id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.2" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.1" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.1.cmml">×</mo><mi id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.3" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.7.m7.1b"><apply id="S5.SS3.SSS1.p5.7.m7.1.1.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1"><in id="S5.SS3.SSS1.p5.7.m7.1.1.1.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.1"></in><ci id="S5.SS3.SSS1.p5.7.m7.1.1.2.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.2">𝐁</ci><apply id="S5.SS3.SSS1.p5.7.m7.1.1.3.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.SSS1.p5.7.m7.1.1.3.1.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3">superscript</csymbol><ci id="S5.SS3.SSS1.p5.7.m7.1.1.3.2.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.2">ℝ</ci><apply id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3"><times id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.1.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.1"></times><ci id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.2.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.2">𝑛</ci><ci id="S5.SS3.SSS1.p5.7.m7.1.1.3.3.3.cmml" xref="S5.SS3.SSS1.p5.7.m7.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.7.m7.1c">\mathbf{B}\in\mathbb{R}^{n\times k}</annotation></semantics></math> are the trainable parameters for task adaptation and <math id="S5.SS3.SSS1.p5.8.m8.3" class="ltx_Math" alttext="k\ll\min(m,n)" display="inline"><semantics id="S5.SS3.SSS1.p5.8.m8.3a"><mrow id="S5.SS3.SSS1.p5.8.m8.3.4" xref="S5.SS3.SSS1.p5.8.m8.3.4.cmml"><mi id="S5.SS3.SSS1.p5.8.m8.3.4.2" xref="S5.SS3.SSS1.p5.8.m8.3.4.2.cmml">k</mi><mo id="S5.SS3.SSS1.p5.8.m8.3.4.1" xref="S5.SS3.SSS1.p5.8.m8.3.4.1.cmml">≪</mo><mrow id="S5.SS3.SSS1.p5.8.m8.3.4.3.2" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml"><mi id="S5.SS3.SSS1.p5.8.m8.1.1" xref="S5.SS3.SSS1.p5.8.m8.1.1.cmml">min</mi><mo id="S5.SS3.SSS1.p5.8.m8.3.4.3.2a" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml">⁡</mo><mrow id="S5.SS3.SSS1.p5.8.m8.3.4.3.2.1" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml"><mo stretchy="false" id="S5.SS3.SSS1.p5.8.m8.3.4.3.2.1.1" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml">(</mo><mi id="S5.SS3.SSS1.p5.8.m8.2.2" xref="S5.SS3.SSS1.p5.8.m8.2.2.cmml">m</mi><mo id="S5.SS3.SSS1.p5.8.m8.3.4.3.2.1.2" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml">,</mo><mi id="S5.SS3.SSS1.p5.8.m8.3.3" xref="S5.SS3.SSS1.p5.8.m8.3.3.cmml">n</mi><mo stretchy="false" id="S5.SS3.SSS1.p5.8.m8.3.4.3.2.1.3" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p5.8.m8.3b"><apply id="S5.SS3.SSS1.p5.8.m8.3.4.cmml" xref="S5.SS3.SSS1.p5.8.m8.3.4"><csymbol cd="latexml" id="S5.SS3.SSS1.p5.8.m8.3.4.1.cmml" xref="S5.SS3.SSS1.p5.8.m8.3.4.1">much-less-than</csymbol><ci id="S5.SS3.SSS1.p5.8.m8.3.4.2.cmml" xref="S5.SS3.SSS1.p5.8.m8.3.4.2">𝑘</ci><apply id="S5.SS3.SSS1.p5.8.m8.3.4.3.1.cmml" xref="S5.SS3.SSS1.p5.8.m8.3.4.3.2"><min id="S5.SS3.SSS1.p5.8.m8.1.1.cmml" xref="S5.SS3.SSS1.p5.8.m8.1.1"></min><ci id="S5.SS3.SSS1.p5.8.m8.2.2.cmml" xref="S5.SS3.SSS1.p5.8.m8.2.2">𝑚</ci><ci id="S5.SS3.SSS1.p5.8.m8.3.3.cmml" xref="S5.SS3.SSS1.p5.8.m8.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p5.8.m8.3c">k\ll\min(m,n)</annotation></semantics></math> is the reduced rank. The major merit of LoRA is that it can largely save the memory and storage usage (<em id="S5.SS3.SSS1.p5.8.3" class="ltx_emph ltx_font_italic">e.g.,</em> VRAM). Further, one can only keep a single large model copy, while maintaining a number of task-specific low-rank decomposition matrices for adapting to different downstream tasks.
Further, several studies have also discussed how to set the rank in a more principled approach, <em id="S5.SS3.SSS1.p5.8.4" class="ltx_emph ltx_font_italic">e.g.,</em> importance score based allocation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib406" title="" class="ltx_ref">406</a>]</cite> and search-free optimal rank selection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib407" title="" class="ltx_ref">407</a>]</cite>.</p>
</div>
<div id="S5.SS3.SSS1.p6" class="ltx_para">
<p id="S5.SS3.SSS1.p6.1" class="ltx_p">Besides the above methods, there is extensive research on efficient tuning of Transformer language models.
However, a more comprehensive discussion of efficient tuning is beyond the scope of this article, which can be found in the related papers on this topic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib400" title="" class="ltx_ref">400</a>, <a href="#bib.bib408" title="" class="ltx_ref">408</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Parameter-Efficient Fine-Tuning on LLMs</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">With the rising of LLMs, efficient tuning has attracted increasing research attention for developing a more lightweight adaptation approach in downstream tasks.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">In particular, LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> has been widely applied to open-source LLMs (<em id="S5.SS3.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> LLaMA and BLOOM) for parameter-efficient fine-tuning.
Among these research attempts, LLaMA and its variants have gained much attention for parameter-efficient tuning.
For example, Alpaca-LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite> has been trained using LoRA as a lightweight tuned version of Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> (a fine-tuned 7B LLaMA model with 52K human demonstrations of instruction following).
There are extensive explorations of Alpaca-LoRA ranging in different languages or model sizes, which can be found in the collection page<span id="footnote35" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">35</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">35</sup><span class="ltx_tag ltx_tag_note">35</span>https://github.com/tloen/alpaca-lora</span></span></span>.
A recent study LLaMA-Adapter&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib409" title="" class="ltx_ref">409</a>]</cite> inserts learnable prompt vectors into each Transformer layer, in which zero-initialized attention has been proposed to improve the training by mitigating the influence of under-fitted prompt vectors. They also extend this approach to a multi-modal setting, <em id="S5.SS3.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> visual question answering.</p>
</div>
<div id="S5.SS3.SSS2.p3" class="ltx_para">
<p id="S5.SS3.SSS2.p3.1" class="ltx_p">Further, an empirical study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib399" title="" class="ltx_ref">399</a>]</cite> has been conducted to examine the effect of different tuning methods on language models.
They compare four efficient tuning methods including serial adapter tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib398" title="" class="ltx_ref">398</a>]</cite>, parallel adapter tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib400" title="" class="ltx_ref">400</a>, <a href="#bib.bib410" title="" class="ltx_ref">410</a>]</cite>, and LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>, on three open-source LLMs, namely GPT-J (6B), BLOOM (7.1B) and LLaMA (7B), for evaluation. Based on the experimental results on six math reasoning datasets, they show that these efficient-tuning methods under-perform the reference baseline GPT-3.5 on difficult tasks, while achieving a comparable performance on simple tasks.
Overall, LoRA performs relatively well among these comparison methods, using significantly fewer trainable parameters.</p>
</div>
<div id="S5.SS3.SSS2.p4" class="ltx_para">
<p id="S5.SS3.SSS2.p4.1" class="ltx_p">As an important resource, the library <em id="S5.SS3.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">PEFT</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib411" title="" class="ltx_ref">411</a>]</cite> (standing for parameter-efficient fine-tuning) has been released on GitHub<span id="footnote36" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">36</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">36</sup><span class="ltx_tag ltx_tag_note">36</span>https://github.com/huggingface/peft</span></span></span>. It has included several widely used efficient tuning methods, including LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>/AdaLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib406" title="" class="ltx_ref">406</a>]</cite>, prefix-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>, <a href="#bib.bib401" title="" class="ltx_ref">401</a>]</cite>, P-Tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib402" title="" class="ltx_ref">402</a>]</cite>, and prompt-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite>. Further, it supports a number of language models such as GPT-2 and LLaMA, and also covers several representative vision Transformer models (<em id="S5.SS3.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> ViT and Swin Transformer).</p>
</div>
<div id="S5.SS3.SSS2.p5" class="ltx_para">
<p id="S5.SS3.SSS2.p5.1" class="ltx_p">As discussed in Section&nbsp;<a href="#S5.SS3.SSS1" title="5.3.1 Parameter-Efficient Fine-Tuning Methods ‣ 5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.1</span></a>, there have been a large number of efficient tuning methods proposed in the existing literature. However, most of these approaches are tested on small-sized pre-trained language models, instead of the LLMs.
So far, there still lacks a thorough investigation on the effect of different efficient tuning methods on large-sized language models at different settings or tasks.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span id="S5.SS4.1.1" class="ltx_text ltx_font_italic">Memory-Efficient Model Adaptation</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Due to the huge number of model parameters, LLMs take a significant memory footprint for inference, making it very costly to be deployed in real-world applications. In this section, we discuss how to reduce the memory footprint of LLMs via a popular model compression approach (<em id="S5.SS4.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> model quantization), so that large-sized LLMs can be used in resource-limited settings, which also likely reduces the inference latency.</p>
</div>
<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1 </span>Background for Quantization</h4>

<div id="S5.SS4.SSS1.p1" class="ltx_para">
<p id="S5.SS4.SSS1.p1.1" class="ltx_p">In this part, we
present a general introduction of quantization techniques for neural networks.</p>
</div>
<div id="S5.SS4.SSS1.p2" class="ltx_para">
<p id="S5.SS4.SSS1.p2.8" class="ltx_p">In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib412" title="" class="ltx_ref">412</a>]</cite>, especially the 8-bit integer quantization (<em id="S5.SS4.SSS1.p2.8.1" class="ltx_emph ltx_font_italic">i.e.,</em> <em id="S5.SS4.SSS1.p2.8.2" class="ltx_emph ltx_font_italic">INT8 quantization</em>).
For neural network models, there are typically two kinds of data to be quantized, namely <em id="S5.SS4.SSS1.p2.8.3" class="ltx_emph ltx_font_italic">weights</em> (model parameters) and <em id="S5.SS4.SSS1.p2.8.4" class="ltx_emph ltx_font_italic">activations</em> (hidden activations), which are originally represented in floating-point numbers. To illustrate the essential idea of model quantization, we introduce a simple yet popular quantization function:
<math id="S5.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="x_{q}=R(x/S)-Z" display="inline"><semantics id="S5.SS4.SSS1.p2.1.m1.1a"><mrow id="S5.SS4.SSS1.p2.1.m1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.cmml"><msub id="S5.SS4.SSS1.p2.1.m1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S5.SS4.SSS1.p2.1.m1.1.1.3.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.2.cmml">x</mi><mi id="S5.SS4.SSS1.p2.1.m1.1.1.3.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.3.cmml">q</mi></msub><mo id="S5.SS4.SSS1.p2.1.m1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.2.cmml">=</mo><mrow id="S5.SS4.SSS1.p2.1.m1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.cmml"><mrow id="S5.SS4.SSS1.p2.1.m1.1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.cmml"><mi id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.SS4.SSS1.p2.1.m1.1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.2.cmml">−</mo><mi id="S5.SS4.SSS1.p2.1.m1.1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.3.cmml">Z</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.1.m1.1b"><apply id="S5.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1"><eq id="S5.SS4.SSS1.p2.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.2"></eq><apply id="S5.SS4.SSS1.p2.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S5.SS4.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.2">𝑥</ci><ci id="S5.SS4.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.3">𝑞</ci></apply><apply id="S5.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1"><minus id="S5.SS4.SSS1.p2.1.m1.1.1.1.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.2"></minus><apply id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1"><times id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.2"></times><ci id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.3">𝑅</ci><apply id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1"><divide id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.1"></divide><ci id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.3">𝑆</ci></apply></apply><ci id="S5.SS4.SSS1.p2.1.m1.1.1.1.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1.3">𝑍</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.1.m1.1c">x_{q}=R(x/S)-Z</annotation></semantics></math>, which transforms a floating number <math id="S5.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS4.SSS1.p2.2.m2.1a"><mi id="S5.SS4.SSS1.p2.2.m2.1.1" xref="S5.SS4.SSS1.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.2.m2.1b"><ci id="S5.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.2.m2.1c">x</annotation></semantics></math> into a quantized value <math id="S5.SS4.SSS1.p2.3.m3.1" class="ltx_Math" alttext="x_{q}" display="inline"><semantics id="S5.SS4.SSS1.p2.3.m3.1a"><msub id="S5.SS4.SSS1.p2.3.m3.1.1" xref="S5.SS4.SSS1.p2.3.m3.1.1.cmml"><mi id="S5.SS4.SSS1.p2.3.m3.1.1.2" xref="S5.SS4.SSS1.p2.3.m3.1.1.2.cmml">x</mi><mi id="S5.SS4.SSS1.p2.3.m3.1.1.3" xref="S5.SS4.SSS1.p2.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.3.m3.1b"><apply id="S5.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.3.m3.1.1.1.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p2.3.m3.1.1.2.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1.2">𝑥</ci><ci id="S5.SS4.SSS1.p2.3.m3.1.1.3.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.3.m3.1c">x_{q}</annotation></semantics></math>. In this function,
<math id="S5.SS4.SSS1.p2.4.m4.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S5.SS4.SSS1.p2.4.m4.1a"><mi id="S5.SS4.SSS1.p2.4.m4.1.1" xref="S5.SS4.SSS1.p2.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.4.m4.1b"><ci id="S5.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.4.m4.1c">S</annotation></semantics></math> and <math id="S5.SS4.SSS1.p2.5.m5.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S5.SS4.SSS1.p2.5.m5.1a"><mi id="S5.SS4.SSS1.p2.5.m5.1.1" xref="S5.SS4.SSS1.p2.5.m5.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.5.m5.1b"><ci id="S5.SS4.SSS1.p2.5.m5.1.1.cmml" xref="S5.SS4.SSS1.p2.5.m5.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.5.m5.1c">Z</annotation></semantics></math> denote the scaling factor (involving two parameters <math id="S5.SS4.SSS1.p2.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS4.SSS1.p2.6.m6.1a"><mi id="S5.SS4.SSS1.p2.6.m6.1.1" xref="S5.SS4.SSS1.p2.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.6.m6.1b"><ci id="S5.SS4.SSS1.p2.6.m6.1.1.cmml" xref="S5.SS4.SSS1.p2.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.6.m6.1c">\alpha</annotation></semantics></math> and <math id="S5.SS4.SSS1.p2.7.m7.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS4.SSS1.p2.7.m7.1a"><mi id="S5.SS4.SSS1.p2.7.m7.1.1" xref="S5.SS4.SSS1.p2.7.m7.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.7.m7.1b"><ci id="S5.SS4.SSS1.p2.7.m7.1.1.cmml" xref="S5.SS4.SSS1.p2.7.m7.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.7.m7.1c">\beta</annotation></semantics></math> that determine the clipping range) and zero-point factor (determining symmetric or asymmetric quantization), respectively, and <math id="S5.SS4.SSS1.p2.8.m8.1" class="ltx_Math" alttext="R(\cdot)" display="inline"><semantics id="S5.SS4.SSS1.p2.8.m8.1a"><mrow id="S5.SS4.SSS1.p2.8.m8.1.2" xref="S5.SS4.SSS1.p2.8.m8.1.2.cmml"><mi id="S5.SS4.SSS1.p2.8.m8.1.2.2" xref="S5.SS4.SSS1.p2.8.m8.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.SS4.SSS1.p2.8.m8.1.2.1" xref="S5.SS4.SSS1.p2.8.m8.1.2.1.cmml">​</mo><mrow id="S5.SS4.SSS1.p2.8.m8.1.2.3.2" xref="S5.SS4.SSS1.p2.8.m8.1.2.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p2.8.m8.1.2.3.2.1" xref="S5.SS4.SSS1.p2.8.m8.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S5.SS4.SSS1.p2.8.m8.1.1" xref="S5.SS4.SSS1.p2.8.m8.1.1.cmml">⋅</mo><mo stretchy="false" id="S5.SS4.SSS1.p2.8.m8.1.2.3.2.2" xref="S5.SS4.SSS1.p2.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.8.m8.1b"><apply id="S5.SS4.SSS1.p2.8.m8.1.2.cmml" xref="S5.SS4.SSS1.p2.8.m8.1.2"><times id="S5.SS4.SSS1.p2.8.m8.1.2.1.cmml" xref="S5.SS4.SSS1.p2.8.m8.1.2.1"></times><ci id="S5.SS4.SSS1.p2.8.m8.1.2.2.cmml" xref="S5.SS4.SSS1.p2.8.m8.1.2.2">𝑅</ci><ci id="S5.SS4.SSS1.p2.8.m8.1.1.cmml" xref="S5.SS4.SSS1.p2.8.m8.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.8.m8.1c">R(\cdot)</annotation></semantics></math> denotes the rounding operation that maps a scaled floating value to an approximate integer.</p>
</div>
<div id="S5.SS4.SSS1.p3" class="ltx_para">
<p id="S5.SS4.SSS1.p3.5" class="ltx_p">As the reverse process, <em id="S5.SS4.SSS1.p3.5.1" class="ltx_emph ltx_font_italic">dequantization</em> recovers the original value from the quantized value accordingly: <math id="S5.SS4.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\tilde{x}=S\cdot(x_{q}+Z)" display="inline"><semantics id="S5.SS4.SSS1.p3.1.m1.1a"><mrow id="S5.SS4.SSS1.p3.1.m1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.cmml"><mover accent="true" id="S5.SS4.SSS1.p3.1.m1.1.1.3" xref="S5.SS4.SSS1.p3.1.m1.1.1.3.cmml"><mi id="S5.SS4.SSS1.p3.1.m1.1.1.3.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.3.2.cmml">x</mi><mo id="S5.SS4.SSS1.p3.1.m1.1.1.3.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.3.1.cmml">~</mo></mover><mo id="S5.SS4.SSS1.p3.1.m1.1.1.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.2.cmml">=</mo><mrow id="S5.SS4.SSS1.p3.1.m1.1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.cmml"><mi id="S5.SS4.SSS1.p3.1.m1.1.1.1.3" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.3.cmml">S</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS4.SSS1.p3.1.m1.1.1.1.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.2.cmml">⋅</mo><mrow id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.cmml"><msub id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.3" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.3.cmml">q</mi></msub><mo id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.3" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.3.cmml">Z</mi></mrow><mo stretchy="false" id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.3" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.1.m1.1b"><apply id="S5.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1"><eq id="S5.SS4.SSS1.p3.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.2"></eq><apply id="S5.SS4.SSS1.p3.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.3"><ci id="S5.SS4.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.3.1">~</ci><ci id="S5.SS4.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.3.2">𝑥</ci></apply><apply id="S5.SS4.SSS1.p3.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1"><ci id="S5.SS4.SSS1.p3.1.m1.1.1.1.2.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.2">⋅</ci><ci id="S5.SS4.SSS1.p3.1.m1.1.1.1.3.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.3">𝑆</ci><apply id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1"><plus id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.1"></plus><apply id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.2.3">𝑞</ci></apply><ci id="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1.1.1.1.3">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.1.m1.1c">\tilde{x}=S\cdot(x_{q}+Z)</annotation></semantics></math>. The quantization error is calculated as the numerical difference between the original value <math id="S5.SS4.SSS1.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS4.SSS1.p3.2.m2.1a"><mi id="S5.SS4.SSS1.p3.2.m2.1.1" xref="S5.SS4.SSS1.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.2.m2.1b"><ci id="S5.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.2.m2.1c">x</annotation></semantics></math> and the recovered value <math id="S5.SS4.SSS1.p3.3.m3.1" class="ltx_Math" alttext="\tilde{x}" display="inline"><semantics id="S5.SS4.SSS1.p3.3.m3.1a"><mover accent="true" id="S5.SS4.SSS1.p3.3.m3.1.1" xref="S5.SS4.SSS1.p3.3.m3.1.1.cmml"><mi id="S5.SS4.SSS1.p3.3.m3.1.1.2" xref="S5.SS4.SSS1.p3.3.m3.1.1.2.cmml">x</mi><mo id="S5.SS4.SSS1.p3.3.m3.1.1.1" xref="S5.SS4.SSS1.p3.3.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.3.m3.1b"><apply id="S5.SS4.SSS1.p3.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p3.3.m3.1.1"><ci id="S5.SS4.SSS1.p3.3.m3.1.1.1.cmml" xref="S5.SS4.SSS1.p3.3.m3.1.1.1">~</ci><ci id="S5.SS4.SSS1.p3.3.m3.1.1.2.cmml" xref="S5.SS4.SSS1.p3.3.m3.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.3.m3.1c">\tilde{x}</annotation></semantics></math>.
The range parameters <math id="S5.SS4.SSS1.p3.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS4.SSS1.p3.4.m4.1a"><mi id="S5.SS4.SSS1.p3.4.m4.1.1" xref="S5.SS4.SSS1.p3.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.4.m4.1b"><ci id="S5.SS4.SSS1.p3.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p3.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.4.m4.1c">\alpha</annotation></semantics></math> and <math id="S5.SS4.SSS1.p3.5.m5.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS4.SSS1.p3.5.m5.1a"><mi id="S5.SS4.SSS1.p3.5.m5.1.1" xref="S5.SS4.SSS1.p3.5.m5.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.5.m5.1b"><ci id="S5.SS4.SSS1.p3.5.m5.1.1.cmml" xref="S5.SS4.SSS1.p3.5.m5.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.5.m5.1c">\beta</annotation></semantics></math> have a large impact on the quantization performance, which often need to be <em id="S5.SS4.SSS1.p3.5.2" class="ltx_emph ltx_font_italic">calibrated</em> according to real data distributions, in either a <em id="S5.SS4.SSS1.p3.5.3" class="ltx_emph ltx_font_italic">static</em> (offline) or <em id="S5.SS4.SSS1.p3.5.4" class="ltx_emph ltx_font_italic">dynamic</em> way (runtime).</p>
</div>
<div id="S5.SS4.SSS1.p4" class="ltx_para">
<p id="S5.SS4.SSS1.p4.1" class="ltx_p">For more details, we refer to the readers to the excellent survey&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib412" title="" class="ltx_ref">412</a>]</cite> about quantization methods on neural networks.</p>
</div>
</section>
<section id="S5.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2 </span>Quantization Methods for LLMs</h4>

<div id="S5.SS4.SSS2.p1" class="ltx_para">
<p id="S5.SS4.SSS2.p1.1" class="ltx_p">There are generally two major model quantization approaches, namely <em id="S5.SS4.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">quantization-aware training&nbsp;(QAT)</em>&nbsp;(requiring additional full model retraining) and <em id="S5.SS4.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">post-training quantization&nbsp;(PTQ)</em> (requires no model retraining).
Compared with small-sized language models, two major differences need to be considered when designing or selecting quantization methods for LLMs. Firstly, LLMs consist of a huge number of parameters, and thus PTQ methods are more preferred due to a much lower computational cost than QAT methods. Secondly, LLMs exhibit very different activation patterns (<em id="S5.SS4.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> large outlier features), and it becomes more difficult to quantize LLMs, especially hidden activations. Next, we will briefly review several representative PTQ methods<span id="footnote37" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">37</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">37</sup><span class="ltx_tag ltx_tag_note">37</span>Since we mainly focus on discussing quantization methods in the context of LLMs, the line of quantization work on small-sized language models (<em id="footnote37.1" class="ltx_emph ltx_font_italic">e.g.,</em> BERT) has not been included in this survey.
</span></span></span> for LLMs.</p>
</div>
<div id="S5.SS4.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS2.p2.1" class="ltx_p"><span id="S5.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Post-Training Quantization&nbsp;(PTQ)</span>. We first introduce the PTQ methods for LLMs.</p>
</div>
<div id="S5.SS4.SSS2.p3" class="ltx_para">
<p id="S5.SS4.SSS2.p3.1" class="ltx_p"><math id="S5.SS4.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS2.p3.1.m1.1a"><mo id="S5.SS4.SSS2.p3.1.m1.1.1" xref="S5.SS4.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.1.m1.1b"><ci id="S5.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">Mixed-precision decomposition</em>.
As observed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref">413</a>]</cite>, extreme large values occur in hidden activations (called <em id="S5.SS4.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">the emergence of outliers</em>) when the model size reaches 6.7B parameters or above. Interestingly, these outliers are mainly distributed in some specific feature dimensions at Transformer layers.
Based on this finding, a vector-wise quantization approach, called <em id="S5.SS4.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">LLM.int8()</em>, has been proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref">413</a>]</cite>, which separates the feature dimensions with outliers and the rest dimensions in matrix multiplication.
Then, the calculations for the two parts are performed with <em id="S5.SS4.SSS2.p3.1.4" class="ltx_emph ltx_font_italic">16-bit floating numbers</em> and <em id="S5.SS4.SSS2.p3.1.5" class="ltx_emph ltx_font_italic">8-bit integers</em>, respectively, so as to recover these outliers in a high precision.</p>
</div>
<div id="S5.SS4.SSS2.p4" class="ltx_para">
<p id="S5.SS4.SSS2.p4.1" class="ltx_p"><math id="S5.SS4.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS2.p4.1.m1.1a"><mo id="S5.SS4.SSS2.p4.1.m1.1.1" xref="S5.SS4.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p4.1.m1.1b"><ci id="S5.SS4.SSS2.p4.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">Fine-grained quantization</em>.
For Transformer models, weights and activations are usually represented in the form of tensors. A straightforward approach is to use coarse-grained quantization parameters for the whole tensor (<em id="S5.SS4.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> per-tensor quantization)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib414" title="" class="ltx_ref">414</a>]</cite>. However, it usually leads to inaccurate reconstruction results.
Thus, fine-grained methods are proposed to reduce the quantization error. ZeroQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib415" title="" class="ltx_ref">415</a>]</cite> adopts a token-wise quantization approach with dynamic calibration for compressing activations. Whereas for weights (easier to be quantized), it uses a group-wise quantization. In practice, a group size of 128&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib415" title="" class="ltx_ref">415</a>, <a href="#bib.bib416" title="" class="ltx_ref">416</a>]</cite> is commonly used for model quantization.</p>
</div>
<div id="S5.SS4.SSS2.p5" class="ltx_para">
<p id="S5.SS4.SSS2.p5.6" class="ltx_p"><math id="S5.SS4.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS2.p5.1.m1.1a"><mo id="S5.SS4.SSS2.p5.1.m1.1.1" xref="S5.SS4.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p5.1.m1.1b"><ci id="S5.SS4.SSS2.p5.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS2.p5.6.1" class="ltx_emph ltx_font_italic">Balancing the quantization difficulty</em>.
Considering that weights are easier to be quantized than activations, SmoothQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib414" title="" class="ltx_ref">414</a>]</cite> proposes to migrate the difficulty from activations to weights. Specially, they incorporate a scaling transformation to balance the difficulty between weights and activations in a linear layer: <math id="S5.SS4.SSS2.p5.2.m2.4" class="ltx_Math" alttext="\mathbf{Y}=(\mathbf{X}\text{diag}(\mathbf{s})^{-1})\cdot(\text{diag}(\mathbf{s})\mathbf{W})" display="inline"><semantics id="S5.SS4.SSS2.p5.2.m2.4a"><mrow id="S5.SS4.SSS2.p5.2.m2.4.4" xref="S5.SS4.SSS2.p5.2.m2.4.4.cmml"><mi id="S5.SS4.SSS2.p5.2.m2.4.4.4" xref="S5.SS4.SSS2.p5.2.m2.4.4.4.cmml">𝐘</mi><mo id="S5.SS4.SSS2.p5.2.m2.4.4.3" xref="S5.SS4.SSS2.p5.2.m2.4.4.3.cmml">=</mo><mrow id="S5.SS4.SSS2.p5.2.m2.4.4.2" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.cmml"><mrow id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.2" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.cmml"><mi id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.2" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.1" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.1.cmml">​</mo><mtext id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.3" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.3a.cmml">diag</mtext><mo lspace="0em" rspace="0em" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.1a" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.1.cmml">​</mo><msup id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.cmml"><mrow id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.2.2" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.cmml"><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.2.2.1" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.cmml">(</mo><mi id="S5.SS4.SSS2.p5.2.m2.1.1" xref="S5.SS4.SSS2.p5.2.m2.1.1.cmml">𝐬</mi><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.2.2.2" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.cmml">)</mo></mrow><mrow id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.cmml"><mo id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3a" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.cmml">−</mo><mn id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.2" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.2.cmml">1</mn></mrow></msup></mrow><mo rspace="0.055em" stretchy="false" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.3" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S5.SS4.SSS2.p5.2.m2.4.4.2.3" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.3.cmml">⋅</mo><mrow id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml"><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.2" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml">(</mo><mrow id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml"><mtext id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.2" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.2a.cmml">diag</mtext><mo lspace="0em" rspace="0em" id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.1" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.1.cmml">​</mo><mrow id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.3.2" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml"><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.3.2.1" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml">(</mo><mi id="S5.SS4.SSS2.p5.2.m2.2.2" xref="S5.SS4.SSS2.p5.2.m2.2.2.cmml">𝐬</mi><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.3.2.2" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.1a" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.1.cmml">​</mo><mi id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.4" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.4.cmml">𝐖</mi></mrow><mo stretchy="false" id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.3" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p5.2.m2.4b"><apply id="S5.SS4.SSS2.p5.2.m2.4.4.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4"><eq id="S5.SS4.SSS2.p5.2.m2.4.4.3.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.3"></eq><ci id="S5.SS4.SSS2.p5.2.m2.4.4.4.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.4">𝐘</ci><apply id="S5.SS4.SSS2.p5.2.m2.4.4.2.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2"><ci id="S5.SS4.SSS2.p5.2.m2.4.4.2.3.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.3">⋅</ci><apply id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1"><times id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.1"></times><ci id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.2.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.2">𝐗</ci><ci id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.3a.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.3"><mtext id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.3.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.3">diag</mtext></ci><apply id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4">superscript</csymbol><ci id="S5.SS4.SSS2.p5.2.m2.1.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.1.1">𝐬</ci><apply id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3"><minus id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3"></minus><cn type="integer" id="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.2.cmml" xref="S5.SS4.SSS2.p5.2.m2.3.3.1.1.1.1.4.3.2">1</cn></apply></apply></apply><apply id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1"><times id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.1.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.1"></times><ci id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.2a.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.2"><mtext id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.2.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.2">diag</mtext></ci><ci id="S5.SS4.SSS2.p5.2.m2.2.2.cmml" xref="S5.SS4.SSS2.p5.2.m2.2.2">𝐬</ci><ci id="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.4.cmml" xref="S5.SS4.SSS2.p5.2.m2.4.4.2.2.1.1.4">𝐖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.2.m2.4c">\mathbf{Y}=(\mathbf{X}\text{diag}(\mathbf{s})^{-1})\cdot(\text{diag}(\mathbf{s})\mathbf{W})</annotation></semantics></math>. By introducing an mathematically equivalent transformation, this formula controls the quantization difficulty through the scaling factor <math id="S5.SS4.SSS2.p5.3.m3.1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><semantics id="S5.SS4.SSS2.p5.3.m3.1a"><mi id="S5.SS4.SSS2.p5.3.m3.1.1" xref="S5.SS4.SSS2.p5.3.m3.1.1.cmml">𝐬</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p5.3.m3.1b"><ci id="S5.SS4.SSS2.p5.3.m3.1.1.cmml" xref="S5.SS4.SSS2.p5.3.m3.1.1">𝐬</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.3.m3.1c">\mathbf{s}</annotation></semantics></math>.
To set <math id="S5.SS4.SSS2.p5.4.m4.1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><semantics id="S5.SS4.SSS2.p5.4.m4.1a"><mi id="S5.SS4.SSS2.p5.4.m4.1.1" xref="S5.SS4.SSS2.p5.4.m4.1.1.cmml">𝐬</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p5.4.m4.1b"><ci id="S5.SS4.SSS2.p5.4.m4.1.1.cmml" xref="S5.SS4.SSS2.p5.4.m4.1.1">𝐬</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.4.m4.1c">\mathbf{s}</annotation></semantics></math>, it incorporates a migration strength parameter <math id="S5.SS4.SSS2.p5.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS4.SSS2.p5.5.m5.1a"><mi id="S5.SS4.SSS2.p5.5.m5.1.1" xref="S5.SS4.SSS2.p5.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p5.5.m5.1b"><ci id="S5.SS4.SSS2.p5.5.m5.1.1.cmml" xref="S5.SS4.SSS2.p5.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.5.m5.1c">\alpha</annotation></semantics></math> to balance the difficulties, where each entry <math id="S5.SS4.SSS2.p5.6.m6.2" class="ltx_math_unparsed" alttext="s_{j}=\max(\mathbf{x}_{j})^{\alpha}/\max(\mathbf{w}_{j})^{(1-\alpha)}" display="inline"><semantics id="S5.SS4.SSS2.p5.6.m6.2a"><mrow id="S5.SS4.SSS2.p5.6.m6.2b"><msub id="S5.SS4.SSS2.p5.6.m6.2.3"><mi id="S5.SS4.SSS2.p5.6.m6.2.3.2">s</mi><mi id="S5.SS4.SSS2.p5.6.m6.2.3.3">j</mi></msub><mo id="S5.SS4.SSS2.p5.6.m6.2.4">=</mo><mi id="S5.SS4.SSS2.p5.6.m6.2.2">max</mi><msup id="S5.SS4.SSS2.p5.6.m6.2.5"><mrow id="S5.SS4.SSS2.p5.6.m6.2.5.2"><mo stretchy="false" id="S5.SS4.SSS2.p5.6.m6.2.5.2.1">(</mo><msub id="S5.SS4.SSS2.p5.6.m6.2.5.2.2"><mi id="S5.SS4.SSS2.p5.6.m6.2.5.2.2.2">𝐱</mi><mi id="S5.SS4.SSS2.p5.6.m6.2.5.2.2.3">j</mi></msub><mo stretchy="false" id="S5.SS4.SSS2.p5.6.m6.2.5.2.3">)</mo></mrow><mi id="S5.SS4.SSS2.p5.6.m6.2.5.3">α</mi></msup><mo id="S5.SS4.SSS2.p5.6.m6.2.6">/</mo><mi id="S5.SS4.SSS2.p5.6.m6.2.7">max</mi><msup id="S5.SS4.SSS2.p5.6.m6.2.8"><mrow id="S5.SS4.SSS2.p5.6.m6.2.8.2"><mo stretchy="false" id="S5.SS4.SSS2.p5.6.m6.2.8.2.1">(</mo><msub id="S5.SS4.SSS2.p5.6.m6.2.8.2.2"><mi id="S5.SS4.SSS2.p5.6.m6.2.8.2.2.2">𝐰</mi><mi id="S5.SS4.SSS2.p5.6.m6.2.8.2.2.3">j</mi></msub><mo stretchy="false" id="S5.SS4.SSS2.p5.6.m6.2.8.2.3">)</mo></mrow><mrow id="S5.SS4.SSS2.p5.6.m6.1.1.1.1"><mo stretchy="false" id="S5.SS4.SSS2.p5.6.m6.1.1.1.1.2">(</mo><mrow id="S5.SS4.SSS2.p5.6.m6.1.1.1.1.1"><mn id="S5.SS4.SSS2.p5.6.m6.1.1.1.1.1.2">1</mn><mo id="S5.SS4.SSS2.p5.6.m6.1.1.1.1.1.1">−</mo><mi id="S5.SS4.SSS2.p5.6.m6.1.1.1.1.1.3">α</mi></mrow><mo stretchy="false" id="S5.SS4.SSS2.p5.6.m6.1.1.1.1.3">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p5.6.m6.2c">s_{j}=\max(\mathbf{x}_{j})^{\alpha}/\max(\mathbf{w}_{j})^{(1-\alpha)}</annotation></semantics></math> is determined by the migration strength.</p>
</div>
<div id="S5.SS4.SSS2.p6" class="ltx_para">
<p id="S5.SS4.SSS2.p6.2" class="ltx_p"><math id="S5.SS4.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS2.p6.1.m1.1a"><mo id="S5.SS4.SSS2.p6.1.m1.1.1" xref="S5.SS4.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p6.1.m1.1b"><ci id="S5.SS4.SSS2.p6.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS2.p6.2.1" class="ltx_emph ltx_font_italic">Layerwise quantization</em>. This approach finds optimal quantized weights that minimize a layerwise reconstruction loss: <math id="S5.SS4.SSS2.p6.2.m2.1" class="ltx_Math" alttext="\arg\min_{\widehat{\mathbf{W}}}\parallel\mathbf{W}\mathbf{X}-\widehat{\mathbf{W}}\mathbf{X}\parallel_{2}^{2}" display="inline"><semantics id="S5.SS4.SSS2.p6.2.m2.1a"><mrow id="S5.SS4.SSS2.p6.2.m2.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.cmml"><mi id="S5.SS4.SSS2.p6.2.m2.1.1.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.2.cmml">arg</mi><mo lspace="0.167em" id="S5.SS4.SSS2.p6.2.m2.1.1a" xref="S5.SS4.SSS2.p6.2.m2.1.1.cmml">⁡</mo><mrow id="S5.SS4.SSS2.p6.2.m2.1.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.cmml"><msub id="S5.SS4.SSS2.p6.2.m2.1.1.1.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.cmml"><mi id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.2.cmml">min</mi><mover accent="true" id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.cmml"><mi id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.2.cmml">𝐖</mi><mo id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.1.cmml">^</mo></mover></msub><mo id="S5.SS4.SSS2.p6.2.m2.1.1.1a" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.cmml">⁡</mo><msubsup id="S5.SS4.SSS2.p6.2.m2.1.1.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.cmml"><mrow id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.2.cmml">𝐖𝐗</mi><mo id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.2" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.2.cmml">𝐖</mi><mo id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.1" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.3" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.3.cmml">𝐗</mi></mrow></mrow><mo stretchy="false" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.3" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.3" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.3.cmml">2</mn><mn id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.3" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p6.2.m2.1b"><apply id="S5.SS4.SSS2.p6.2.m2.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1"><arg id="S5.SS4.SSS2.p6.2.m2.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.2"></arg><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1"><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2">subscript</csymbol><min id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.2"></min><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3"><ci id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.1">^</ci><ci id="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.2.3.2">𝐖</ci></apply></apply><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1">superscript</csymbol><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1">subscript</csymbol><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1"><minus id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.1"></minus><ci id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.2">𝐖𝐗</ci><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3"><times id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.1"></times><apply id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2"><ci id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.2.2">𝐖</ci></apply><ci id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.1.1.1.3.3">𝐗</ci></apply></apply></apply><cn type="integer" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.3.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S5.SS4.SSS2.p6.2.m2.1.1.1.1.3.cmml" xref="S5.SS4.SSS2.p6.2.m2.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p6.2.m2.1c">\arg\min_{\widehat{\mathbf{W}}}\parallel\mathbf{W}\mathbf{X}-\widehat{\mathbf{W}}\mathbf{X}\parallel_{2}^{2}</annotation></semantics></math>. To efficiently optimize this objective, GPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref">417</a>]</cite> improves the original optimal brain quantization&nbsp;(OBQ)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib418" title="" class="ltx_ref">418</a>]</cite> method by fixing the quantization order of weights for all rows.
Further, with specially designed methods (<em id="S5.SS4.SSS2.p6.2.2" class="ltx_emph ltx_font_italic">i.e.,</em> lazy batch-updates and Cholesky reformulation), GPTQ is feasible to quantize very large models (<em id="S5.SS4.SSS2.p6.2.3" class="ltx_emph ltx_font_italic">e.g.,</em> 175B OPT) in 3 or 4 bit precision.
More recently, AWQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib416" title="" class="ltx_ref">416</a>]</cite> further simplifies the optimization form by incorporating activation-aware scaling for weights, which resembles the idea of SmoothQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib414" title="" class="ltx_ref">414</a>]</cite>: weights corresponding to outlier activations are more important to be precisely quantized. It does not directly optimize the reconstruction loss, but instead performs simple hyper-parameter search to achieve the minimal loss on calibration data.</p>
</div>
<div id="S5.SS4.SSS2.p7" class="ltx_para">
<p id="S5.SS4.SSS2.p7.1" class="ltx_p">These strategies in the above methods can be jointly used to improve the quantization performance.
In order to achieve high-efficiency implementation, quantization methods also rely on hardware- or system-level support (<em id="S5.SS4.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> efficient GPU kernels or hardware-friendly group partition).</p>
</div>
<div id="S5.SS4.SSS2.p8" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS2.p8.1" class="ltx_p"><span id="S5.SS4.SSS2.p8.1.1" class="ltx_text ltx_font_bold">Other Quantization Methods</span>. In the above, we mainly focus on PTQ methods, and next introduce two recent studies that explore efficient fine-tuning methods or QAT methods for quanitizing LLMs.</p>
</div>
<div id="S5.SS4.SSS2.p9" class="ltx_para">
<p id="S5.SS4.SSS2.p9.1" class="ltx_p"><math id="S5.SS4.SSS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS2.p9.1.m1.1a"><mo id="S5.SS4.SSS2.p9.1.m1.1.1" xref="S5.SS4.SSS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p9.1.m1.1b"><ci id="S5.SS4.SSS2.p9.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS2.p9.1.1" class="ltx_emph ltx_font_italic">Efficient fine-tuning enhanced quantization.</em>
For post-training quantization, direct low-bit quantization (<em id="S5.SS4.SSS2.p9.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> INT4 quantization) often results in large performance degradation. To overcome this challenge, QLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib419" title="" class="ltx_ref">419</a>]</cite>
incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning.
It combines the merits of LoRA&nbsp;(See Section&nbsp;<a href="#S5.SS3.SSS1" title="5.3.1 Parameter-Efficient Fine-Tuning Methods ‣ 5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.1</span></a>) and quantization methods. The experiment results show that 4-bit quantized models can
achieve the full 16-bit fine-tuning performance by QLoRA.</p>
</div>
<div id="S5.SS4.SSS2.p10" class="ltx_para">
<p id="S5.SS4.SSS2.p10.1" class="ltx_p"><math id="S5.SS4.SSS2.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS2.p10.1.m1.1a"><mo id="S5.SS4.SSS2.p10.1.m1.1.1" xref="S5.SS4.SSS2.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p10.1.m1.1b"><ci id="S5.SS4.SSS2.p10.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS2.p10.1.1" class="ltx_emph ltx_font_italic">Quantization-aware training&nbsp;(QAT) for LLMs</em>. A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib420" title="" class="ltx_ref">420</a>]</cite> explores the effect of QAT methods by applying a data-free distillation method to compress the weights, activations as well as key-value cache. By conducting extensive experiments based on LLaMA, they show promising results with 4-bit quantization on both weights and key-value cache, but not on 4-bit activation quantization, which still needs more exploration.</p>
</div>
</section>
<section id="S5.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.3 </span>Empirical Analysis and Findings</h4>

<div id="S5.SS4.SSS3.p1" class="ltx_para">
<p id="S5.SS4.SSS3.p1.1" class="ltx_p">Quantization has currently become a common technique to reduce the memory footprint and latency of LLMs in deployment.
In particular, it is important to understand what level of precision (<em id="S5.SS4.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> INT8 or INT4) can be applied to quantize different parts of LLMs (<em id="S5.SS4.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> weights or activations), while retaining a high accuracy.
In this part, we first summarize the major findings about the quantization of LLMs in existing literature, and then present some empirical analysis with quantization experiments.</p>
</div>
<div id="S5.SS4.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS3.p2.1" class="ltx_p"><span id="S5.SS4.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Important Findings from Existing Work</span>. Recently, a very comprehensive evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib421" title="" class="ltx_ref">421</a>]</cite> has been conducted about the impact of multiple factors (<em id="S5.SS4.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> model size and sensitivity) on the post-training quantization methods. Another study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib422" title="" class="ltx_ref">422</a>]</cite> examines the scaling law of <math id="S5.SS4.SSS3.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS3.p2.1.m1.1a"><mi id="S5.SS4.SSS3.p2.1.m1.1.1" xref="S5.SS4.SSS3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS3.p2.1.m1.1b"><ci id="S5.SS4.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS3.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS3.p2.1.m1.1c">k</annotation></semantics></math>-bit quantization in inference performance.
In addition to the overall performance, the study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib423" title="" class="ltx_ref">423</a>]</cite> specifically focuses on the potential impact of quantification on emergent capabilities, as well as the levels of performance that can be achieved across various levels of bit precision.
Also, prior work (<em id="S5.SS4.SSS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> LLM.int8()&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib424" title="" class="ltx_ref">424</a>]</cite>, GPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref">417</a>]</cite>, QLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib419" title="" class="ltx_ref">419</a>]</cite>, and GLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>) has also extensively examined the performance of quantization methods in various settings. Next, we summarize several important findings from these studies, which will be useful for those who may not want to delve into the technical details of quantization methods.</p>
</div>
<div id="S5.SS4.SSS3.p3" class="ltx_para">
<p id="S5.SS4.SSS3.p3.1" class="ltx_p"><math id="S5.SS4.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS3.p3.1.m1.1a"><mo id="S5.SS4.SSS3.p3.1.m1.1.1" xref="S5.SS4.SSS3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS3.p3.1.m1.1b"><ci id="S5.SS4.SSS3.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS3.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS3.p3.1.1" class="ltx_emph ltx_font_italic">INT8 weight quantization can often yield very good results on LLMs, while the performance of lower precision weight quantization depends on specific methods</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib421" title="" class="ltx_ref">421</a>, <a href="#bib.bib414" title="" class="ltx_ref">414</a>, <a href="#bib.bib417" title="" class="ltx_ref">417</a>, <a href="#bib.bib416" title="" class="ltx_ref">416</a>]</cite>. In most cases, INT8 weight quantization can be effectively applied to reduce the memory footprint without performance degradation.
While for INT4 (or INT3) weight quantization, existing methods rely on specific strategies to reduce the performance degradation, <em id="S5.SS4.SSS3.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> layerwise method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref">417</a>, <a href="#bib.bib415" title="" class="ltx_ref">415</a>]</cite>, activation-aware scaling&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib416" title="" class="ltx_ref">416</a>]</cite> and low-rank adapter tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib419" title="" class="ltx_ref">419</a>]</cite>. Interestingly, LLMs seem to be less sensitive to low-bit weight quantization than small-sized language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib421" title="" class="ltx_ref">421</a>]</cite>.
In practice, with the same memory cost, it is suggested to use a larger language model with a lower quantization precision rather than a smaller language model with a higher quantization precision. For example, a 4-bit 60GB LLM is demonstrated to have better performance than a 8-bit 30GB LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib422" title="" class="ltx_ref">422</a>]</cite>.
Moreover, focusing on emergent capabilities, the study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib423" title="" class="ltx_ref">423</a>]</cite> finds that in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization. This result suggests that INT4 quantization exhibits a favorable trade-off in terms of both total bits and performance of emergent abilities.</p>
</div>
<div id="S5.SS4.SSS3.p4" class="ltx_para">
<p id="S5.SS4.SSS3.p4.1" class="ltx_p"><math id="S5.SS4.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS3.p4.1.m1.1a"><mo id="S5.SS4.SSS3.p4.1.m1.1.1" xref="S5.SS4.SSS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS3.p4.1.m1.1b"><ci id="S5.SS4.SSS3.p4.1.m1.1.1.cmml" xref="S5.SS4.SSS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS3.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS3.p4.1.1" class="ltx_emph ltx_font_italic">Activations are more difficult to be quantized than weights</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib421" title="" class="ltx_ref">421</a>, <a href="#bib.bib413" title="" class="ltx_ref">413</a>, <a href="#bib.bib414" title="" class="ltx_ref">414</a>]</cite>. It has been found that large outliers would occur for Transformer language models having a size of 6.7B or above&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref">413</a>]</cite>. This issue has been one of the most fundamental difficulties to quantize LLMs.
To overcome this issue, various methods, <em id="S5.SS4.SSS3.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> mixed-precision decomposition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref">413</a>]</cite>, fine-grained quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib425" title="" class="ltx_ref">425</a>, <a href="#bib.bib413" title="" class="ltx_ref">413</a>]</cite> and difficulty migration&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib414" title="" class="ltx_ref">414</a>]</cite>, can be applied to alleviate the influence of outlier values.
Since large outliers mainly exist in the activations of LLMs, small language models are more resistant to activation quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib421" title="" class="ltx_ref">421</a>, <a href="#bib.bib423" title="" class="ltx_ref">423</a>]</cite>. In practice, high-quality INT8 activation quantization is still a difficult task, though several methods can attain satisfying results. 
Further, lower precision activation quantization has still not been successfully explored, even for QAT methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib420" title="" class="ltx_ref">420</a>]</cite>.</p>
</div>
<div id="S5.SS4.SSS3.p5" class="ltx_para">
<p id="S5.SS4.SSS3.p5.1" class="ltx_p"><math id="S5.SS4.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS3.p5.1.m1.1a"><mo id="S5.SS4.SSS3.p5.1.m1.1.1" xref="S5.SS4.SSS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS3.p5.1.m1.1b"><ci id="S5.SS4.SSS3.p5.1.m1.1.1.cmml" xref="S5.SS4.SSS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS3.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS3.p5.1.1" class="ltx_emph ltx_font_italic">Efficient fine-tuning enhanced quantization is a good option to enhance the performance of quantized LLMs</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib419" title="" class="ltx_ref">419</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>.
The benefits of efficient fune-tuning methods in quantization can be twofold. Firstly, it can directly compensate the performance degradation suffered from low-bit quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib421" title="" class="ltx_ref">421</a>, <a href="#bib.bib423" title="" class="ltx_ref">423</a>]</cite>, by
increasing the fitting capacity by updating high precision adapters. Secondly, it is flexible to support task-specific or goal-specific fine-tuning of LLMs in a lightweight way&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib419" title="" class="ltx_ref">419</a>]</cite>, <em id="S5.SS4.SSS3.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> instruction tuning or chat-oriented tuning, by only tuning the small adapters. Overall, it makes a good trade-off between the effectiveness and training cost, which provides a promising approach to enhancing the performance of quantized LLMs.</p>
</div>
<div id="S5.SS4.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS3.p6.1" class="ltx_p"><span id="S5.SS4.SSS3.p6.1.1" class="ltx_text ltx_font_bold">Empirical Analysis on Quantization Experiments</span>.
To further help readers understand the impact of quantization on LLMs, we also conduct a group of experiments to investigate the inference performance of quantized models here.
Specifically, we focus on the fine-tuned LLaMA models&nbsp;(<em id="S5.SS4.SSS3.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> 7B and 13B) using popular SFT datasets, including FLAN-v2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, Alpaca-52K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> and ShareGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>.
For evaluation, we utilize the same tasks in Table&nbsp;<a href="#S5.T9" title="TABLE IX ‣ 5.1.3 The Effect of Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>, and follow the quantization settings in the study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib423" title="" class="ltx_ref">423</a>]</cite> examining the performance of quantized language models at three precision levels: 4-bit, 8-bit and 16-bit. The results are summarized in Table&nbsp;<a href="#S5.T10" title="TABLE X ‣ 5.4.3 Empirical Analysis and Findings ‣ 5.4 Memory-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">X</span></a>.
As can be observed from Table&nbsp;<a href="#S5.T10" title="TABLE X ‣ 5.4.3 Empirical Analysis and Findings ‣ 5.4 Memory-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">X</span></a>, the results obtained with 8-bit and 4-bit weight quantization are close to the performance of 16-bit models while significantly reducing memory consumption.
In practice, it is recommended to first examine the performance of 4-bit weight quantization for LLMs if reducing memory usage is a critical consideration for deployment.</p>
</div>
<figure id="S5.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE X: </span>Evaluation results for quantized LLaMA models&nbsp;(7B and 13B). We employ existing model checkpoints provided by&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib353" title="" class="ltx_ref">353</a>]</cite> for quantization experiments, which have been fine-tuned on FLAN-v2, Alpaca-52K, and ShareGPT, respectively. Specifically, we report the performance with AlpacaFarm, MMLU, and BBH, as well as the memory usage of the loaded model&nbsp;(Mem.).
For quantization, we employ <em id="S5.T10.8.1" class="ltx_emph ltx_font_italic">bitesandbytes</em> to quantize the 16-bit models to 8/4 bits by specifying the commands <span id="S5.T10.9.2" class="ltx_text ltx_font_typewriter">load_in_8bit</span> and <span id="S5.T10.10.3" class="ltx_text ltx_font_typewriter">load_in_4bit</span> when loading the weights.
It is worth noting that we select&nbsp;<em id="S5.T10.11.4" class="ltx_emph ltx_font_italic">text-davinci-003</em> as the baseline model for the AlpacaFarm dataset.</figcaption>
<div id="S5.T10.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:867.2pt;height:170.6pt;vertical-align:-1.2pt;"><span class="ltx_transformed_inner" style="transform:translate(64.9pt,-12.7pt) scale(1.17612576102518,1.17612576102518) ;">
<table id="S5.T10.3.3" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S5.T10.3.3.4" class="ltx_tr">
<td id="S5.T10.3.3.4.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S5.T10.3.3.4.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S5.T10.3.3.4.2" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S5.T10.3.3.4.2.1" class="ltx_text">
<span id="S5.T10.3.3.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T10.3.3.4.2.1.1.1" class="ltx_tr">
<span id="S5.T10.3.3.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T10.3.3.4.2.1.1.1.1.1" class="ltx_text ltx_font_bold">SFT Dataset</span></span></span>
</span></span></td>
<td id="S5.T10.3.3.4.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S5.T10.3.3.4.3.1" class="ltx_text ltx_font_bold">16-bit</span></td>
<td id="S5.T10.3.3.4.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S5.T10.3.3.4.4.1" class="ltx_text ltx_font_bold">8-bit</span></td>
<td id="S5.T10.3.3.4.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S5.T10.3.3.4.5.1" class="ltx_text ltx_font_bold">4-bit</span></td>
</tr>
<tr id="S5.T10.3.3.3" class="ltx_tr">
<td id="S5.T10.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">AlpacaFarm</td>
<td id="S5.T10.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">MMLU</td>
<td id="S5.T10.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t">BBH</td>
<td id="S5.T10.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Mem.<sub id="S5.T10.1.1.1.1.1" class="ltx_sub">(GiB)</sub>
</td>
<td id="S5.T10.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">AlpacaFarm</td>
<td id="S5.T10.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t">MMLU</td>
<td id="S5.T10.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t">BBH</td>
<td id="S5.T10.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Mem.<sub id="S5.T10.2.2.2.2.1" class="ltx_sub">(GiB)</sub>
</td>
<td id="S5.T10.3.3.3.10" class="ltx_td ltx_align_center ltx_border_t">AlpacaFarm</td>
<td id="S5.T10.3.3.3.11" class="ltx_td ltx_align_center ltx_border_t">MMLU</td>
<td id="S5.T10.3.3.3.12" class="ltx_td ltx_align_center ltx_border_t">BBH</td>
<td id="S5.T10.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">Mem.<sub id="S5.T10.3.3.3.3.1" class="ltx_sub">(GiB)</sub>
</td>
</tr>
<tr id="S5.T10.3.3.5" class="ltx_tr">
<td id="S5.T10.3.3.5.1" class="ltx_td ltx_align_left ltx_border_t">LLaMA&nbsp;(7B)</td>
<td id="S5.T10.3.3.5.2" class="ltx_td ltx_align_left ltx_border_t">FLAN-v2</td>
<td id="S5.T10.3.3.5.3" class="ltx_td ltx_align_center ltx_border_t">6.65</td>
<td id="S5.T10.3.3.5.4" class="ltx_td ltx_align_center ltx_border_t">47.34</td>
<td id="S5.T10.3.3.5.5" class="ltx_td ltx_align_center ltx_border_t">35.05</td>
<td id="S5.T10.3.3.5.6" class="ltx_td ltx_align_center ltx_border_t">12.58</td>
<td id="S5.T10.3.3.5.7" class="ltx_td ltx_align_center ltx_border_t">6.15</td>
<td id="S5.T10.3.3.5.8" class="ltx_td ltx_align_center ltx_border_t">47.02</td>
<td id="S5.T10.3.3.5.9" class="ltx_td ltx_align_center ltx_border_t">35.17</td>
<td id="S5.T10.3.3.5.10" class="ltx_td ltx_align_center ltx_border_t">6.65</td>
<td id="S5.T10.3.3.5.11" class="ltx_td ltx_align_center ltx_border_t">7.83</td>
<td id="S5.T10.3.3.5.12" class="ltx_td ltx_align_center ltx_border_t">46.23</td>
<td id="S5.T10.3.3.5.13" class="ltx_td ltx_align_center ltx_border_t">34.77</td>
<td id="S5.T10.3.3.5.14" class="ltx_td ltx_align_center ltx_border_t">3.94</td>
</tr>
<tr id="S5.T10.3.3.6" class="ltx_tr">
<td id="S5.T10.3.3.6.1" class="ltx_td"></td>
<td id="S5.T10.3.3.6.2" class="ltx_td ltx_align_left">Alpaca-52K</td>
<td id="S5.T10.3.3.6.3" class="ltx_td ltx_align_center">32.55</td>
<td id="S5.T10.3.3.6.4" class="ltx_td ltx_align_center">40.87</td>
<td id="S5.T10.3.3.6.5" class="ltx_td ltx_align_center">33.66</td>
<td id="S5.T10.3.3.6.6" class="ltx_td ltx_align_center">12.58</td>
<td id="S5.T10.3.3.6.7" class="ltx_td ltx_align_center">33.60</td>
<td id="S5.T10.3.3.6.8" class="ltx_td ltx_align_center">39.98</td>
<td id="S5.T10.3.3.6.9" class="ltx_td ltx_align_center">34.38</td>
<td id="S5.T10.3.3.6.10" class="ltx_td ltx_align_center">6.65</td>
<td id="S5.T10.3.3.6.11" class="ltx_td ltx_align_center">29.57</td>
<td id="S5.T10.3.3.6.12" class="ltx_td ltx_align_center">39.24</td>
<td id="S5.T10.3.3.6.13" class="ltx_td ltx_align_center">32.80</td>
<td id="S5.T10.3.3.6.14" class="ltx_td ltx_align_center">3.94</td>
</tr>
<tr id="S5.T10.3.3.7" class="ltx_tr">
<td id="S5.T10.3.3.7.1" class="ltx_td"></td>
<td id="S5.T10.3.3.7.2" class="ltx_td ltx_align_left">ShareGPT</td>
<td id="S5.T10.3.3.7.3" class="ltx_td ltx_align_center">72.05</td>
<td id="S5.T10.3.3.7.4" class="ltx_td ltx_align_center">41.30</td>
<td id="S5.T10.3.3.7.5" class="ltx_td ltx_align_center">32.90</td>
<td id="S5.T10.3.3.7.6" class="ltx_td ltx_align_center">12.58</td>
<td id="S5.T10.3.3.7.7" class="ltx_td ltx_align_center">72.86</td>
<td id="S5.T10.3.3.7.8" class="ltx_td ltx_align_center">39.34</td>
<td id="S5.T10.3.3.7.9" class="ltx_td ltx_align_center">32.71</td>
<td id="S5.T10.3.3.7.10" class="ltx_td ltx_align_center">6.65</td>
<td id="S5.T10.3.3.7.11" class="ltx_td ltx_align_center">70.31</td>
<td id="S5.T10.3.3.7.12" class="ltx_td ltx_align_center">40.08</td>
<td id="S5.T10.3.3.7.13" class="ltx_td ltx_align_center">32.11</td>
<td id="S5.T10.3.3.7.14" class="ltx_td ltx_align_center">3.94</td>
</tr>
<tr id="S5.T10.3.3.8" class="ltx_tr">
<td id="S5.T10.3.3.8.1" class="ltx_td ltx_align_left ltx_border_t">LLaMA&nbsp;(13B)</td>
<td id="S5.T10.3.3.8.2" class="ltx_td ltx_align_left ltx_border_t">FLAN-v2</td>
<td id="S5.T10.3.3.8.3" class="ltx_td ltx_align_center ltx_border_t">8.14</td>
<td id="S5.T10.3.3.8.4" class="ltx_td ltx_align_center ltx_border_t">51.67</td>
<td id="S5.T10.3.3.8.5" class="ltx_td ltx_align_center ltx_border_t">41.46</td>
<td id="S5.T10.3.3.8.6" class="ltx_td ltx_align_center ltx_border_t">24.40</td>
<td id="S5.T10.3.3.8.7" class="ltx_td ltx_align_center ltx_border_t">7.64</td>
<td id="S5.T10.3.3.8.8" class="ltx_td ltx_align_center ltx_border_t">51.02</td>
<td id="S5.T10.3.3.8.9" class="ltx_td ltx_align_center ltx_border_t">41.25</td>
<td id="S5.T10.3.3.8.10" class="ltx_td ltx_align_center ltx_border_t">12.53</td>
<td id="S5.T10.3.3.8.11" class="ltx_td ltx_align_center ltx_border_t">7.52</td>
<td id="S5.T10.3.3.8.12" class="ltx_td ltx_align_center ltx_border_t">50.48</td>
<td id="S5.T10.3.3.8.13" class="ltx_td ltx_align_center ltx_border_t">40.68</td>
<td id="S5.T10.3.3.8.14" class="ltx_td ltx_align_center ltx_border_t">7.34</td>
</tr>
<tr id="S5.T10.3.3.9" class="ltx_tr">
<td id="S5.T10.3.3.9.1" class="ltx_td"></td>
<td id="S5.T10.3.3.9.2" class="ltx_td ltx_align_left">Alpaca-52K</td>
<td id="S5.T10.3.3.9.3" class="ltx_td ltx_align_center">33.60</td>
<td id="S5.T10.3.3.9.4" class="ltx_td ltx_align_center">47.63</td>
<td id="S5.T10.3.3.9.5" class="ltx_td ltx_align_center">36.10</td>
<td id="S5.T10.3.3.9.6" class="ltx_td ltx_align_center">24.40</td>
<td id="S5.T10.3.3.9.7" class="ltx_td ltx_align_center">31.43</td>
<td id="S5.T10.3.3.9.8" class="ltx_td ltx_align_center">47.04</td>
<td id="S5.T10.3.3.9.9" class="ltx_td ltx_align_center">35.98</td>
<td id="S5.T10.3.3.9.10" class="ltx_td ltx_align_center">12.53</td>
<td id="S5.T10.3.3.9.11" class="ltx_td ltx_align_center">30.87</td>
<td id="S5.T10.3.3.9.12" class="ltx_td ltx_align_center">46.20</td>
<td id="S5.T10.3.3.9.13" class="ltx_td ltx_align_center">36.16</td>
<td id="S5.T10.3.3.9.14" class="ltx_td ltx_align_center">7.34</td>
</tr>
<tr id="S5.T10.3.3.10" class="ltx_tr">
<td id="S5.T10.3.3.10.1" class="ltx_td ltx_border_bb"></td>
<td id="S5.T10.3.3.10.2" class="ltx_td ltx_align_left ltx_border_bb">ShareGPT</td>
<td id="S5.T10.3.3.10.3" class="ltx_td ltx_align_center ltx_border_bb">75.59</td>
<td id="S5.T10.3.3.10.4" class="ltx_td ltx_align_center ltx_border_bb">47.58</td>
<td id="S5.T10.3.3.10.5" class="ltx_td ltx_align_center ltx_border_bb">38.00</td>
<td id="S5.T10.3.3.10.6" class="ltx_td ltx_align_center ltx_border_bb">24.40</td>
<td id="S5.T10.3.3.10.7" class="ltx_td ltx_align_center ltx_border_bb">73.79</td>
<td id="S5.T10.3.3.10.8" class="ltx_td ltx_align_center ltx_border_bb">47.71</td>
<td id="S5.T10.3.3.10.9" class="ltx_td ltx_align_center ltx_border_bb">38.31</td>
<td id="S5.T10.3.3.10.10" class="ltx_td ltx_align_center ltx_border_bb">12.53</td>
<td id="S5.T10.3.3.10.11" class="ltx_td ltx_align_center ltx_border_bb">71.99</td>
<td id="S5.T10.3.3.10.12" class="ltx_td ltx_align_center ltx_border_bb">45.77</td>
<td id="S5.T10.3.3.10.13" class="ltx_td ltx_align_center ltx_border_bb">36.97</td>
<td id="S5.T10.3.3.10.14" class="ltx_td ltx_align_center ltx_border_bb">7.34</td>
</tr>
</tbody></table>
</span></div>
</figure>
</section>
<section id="S5.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.4 </span>Open-source Libraries and Quantized LLMs</h4>

<div id="S5.SS4.SSS4.p1" class="ltx_para">
<p id="S5.SS4.SSS4.p1.1" class="ltx_p">In this part, we briefly introduce the available open-source quantization libraries and quantized LLMs.</p>
</div>
<div id="S5.SS4.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS4.p2.1" class="ltx_p"><span id="S5.SS4.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Quantization Libraries</span>. Next, we introduce three major quantization libraries for LLMs, including:</p>
</div>
<div id="S5.SS4.SSS4.p3" class="ltx_para">
<p id="S5.SS4.SSS4.p3.1" class="ltx_p"><math id="S5.SS4.SSS4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS4.p3.1.m1.1a"><mo id="S5.SS4.SSS4.p3.1.m1.1.1" xref="S5.SS4.SSS4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS4.p3.1.m1.1b"><ci id="S5.SS4.SSS4.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS4.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS4.p3.1.1" class="ltx_emph ltx_font_italic">Bitsandbytes</em><span id="footnote38" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">38</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">38</sup><span class="ltx_tag ltx_tag_note">38</span>https://github.com/TimDettmers/bitsandbytes</span></span></span> is developed based on the methods introduced in the papers of LLM.int8()&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref">413</a>]</cite> and 8-bit optimizers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib426" title="" class="ltx_ref">426</a>]</cite>.
It focuses on the quantization of both activations and weights for LLMs, including the support on 8-bit and 4-bit&nbsp;(NF4,FP4) matrix multiplication for efficient inference, as well as an 8-bit optimizer for efficient training.</p>
</div>
<div id="S5.SS4.SSS4.p4" class="ltx_para">
<p id="S5.SS4.SSS4.p4.1" class="ltx_p"><math id="S5.SS4.SSS4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS4.p4.1.m1.1a"><mo id="S5.SS4.SSS4.p4.1.m1.1.1" xref="S5.SS4.SSS4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS4.p4.1.m1.1b"><ci id="S5.SS4.SSS4.p4.1.m1.1.1.cmml" xref="S5.SS4.SSS4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS4.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS4.p4.1.1" class="ltx_emph ltx_font_italic">GPTQ-for-LLaMA</em><span id="footnote39" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">39</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">39</sup><span class="ltx_tag ltx_tag_note">39</span>https://github.com/qwopqwop200/GPTQ-for-LLaMa</span></span></span> is developed specially for quantizing LLaMA models. It enables 4-bit quantization of LLaMA models of varied sizes based on the GPTQ algorithm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref">417</a>]</cite>. Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website.</p>
</div>
<div id="S5.SS4.SSS4.p5" class="ltx_para">
<p id="S5.SS4.SSS4.p5.1" class="ltx_p"><math id="S5.SS4.SSS4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS4.p5.1.m1.1a"><mo id="S5.SS4.SSS4.p5.1.m1.1.1" xref="S5.SS4.SSS4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS4.p5.1.m1.1b"><ci id="S5.SS4.SSS4.p5.1.m1.1.1.cmml" xref="S5.SS4.SSS4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS4.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS4.p5.1.1" class="ltx_emph ltx_font_italic">AutoGPTQ</em><span id="footnote40" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">40</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">40</sup><span class="ltx_tag ltx_tag_note">40</span>https://github.com/PanQiWei/AutoGPTQ</span></span></span> is a quantization package developed based on the GPTQ algorithm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref">417</a>]</cite>, which supports INT4 quantization for LLMs.
It includes a number of quantized models in the library, and supports LoRA by integrating with HuggingFace PEFT library.</p>
</div>
<div id="S5.SS4.SSS4.p6" class="ltx_para">
<p id="S5.SS4.SSS4.p6.1" class="ltx_p"><math id="S5.SS4.SSS4.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS4.SSS4.p6.1.m1.1a"><mo id="S5.SS4.SSS4.p6.1.m1.1.1" xref="S5.SS4.SSS4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS4.p6.1.m1.1b"><ci id="S5.SS4.SSS4.p6.1.m1.1.1.cmml" xref="S5.SS4.SSS4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS4.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S5.SS4.SSS4.p6.1.1" class="ltx_emph ltx_font_italic">llama.cpp</em><span id="footnote41" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">41</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">41</sup><span class="ltx_tag ltx_tag_note">41</span>https://github.com/ggerganov/llama.cpp</span></span></span> makes it feasible to run quantized LLaMA models on a MacBook device.
It supports INT4, INT5 and INT8 quantization, which is developed in efficient C/C++ implementation. It also supports a number of LLaMA based models, such as Alpaca and Vicuna.</p>
</div>
<div id="S5.SS4.SSS4.p7" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS4.p7.1" class="ltx_p"><span id="S5.SS4.SSS4.p7.1.1" class="ltx_text ltx_font_bold">Quantized LLMs</span>.
Compared with original models, quantized language models take a smaller memory footprint, and likely have a faster inference speed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref">413</a>, <a href="#bib.bib427" title="" class="ltx_ref">427</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.
Recently, a nubmer of quantized model copies of several publicly available language models have been released on HuggingFace, including BLOOM, GPT-J, and ChatGLM.
In particular, GPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref">417</a>]</cite> has been widely used to quantize generative language models, leading to various quantized variants for LLaMA and OPT. Further, it has been also applied to quantize instruction-tuned models, such as Vicuna and WizardLM. Due to the large number of quantized LLMs, we do not directly incorporate the corresponding links of these models. The readers can easily find them by searching on HuggingFace.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Utilization</span>
</h2>

<figure id="S6.T11" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XI: </span>Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note that the key points only highlight the most important technical contribution.</figcaption>
<div id="S6.T11.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:240.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-205.0pt,113.7pt) scale(0.513957070579328,0.513957070579328) ;">
<table id="S6.T11.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S6.T11.1.1.1" class="ltx_tr">
<td id="S6.T11.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S6.T11.1.1.1.1.1" class="ltx_text ltx_font_bold">Approach</span></td>
<td id="S6.T11.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S6.T11.1.1.1.2.1" class="ltx_text ltx_font_bold">Representative Work</span></td>
<td id="S6.T11.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T11.1.1.1.3.1" class="ltx_text ltx_font_bold">Key Point</span></td>
</tr>
<tr id="S6.T11.1.1.2" class="ltx_tr">
<td id="S6.T11.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="6"><span id="S6.T11.1.1.2.1.1" class="ltx_text">
<span id="S6.T11.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T11.1.1.2.1.1.1.1" class="ltx_tr">
<span id="S6.T11.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">In-context</span></span>
<span id="S6.T11.1.1.2.1.1.1.2" class="ltx_tr">
<span id="S6.T11.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Learning&nbsp;(ICL)</span></span>
</span></span></td>
<td id="S6.T11.1.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">KATE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib428" title="" class="ltx_ref">428</a>]</cite>
</td>
<td id="S6.T11.1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Demonstration selection (similar; k-NN)</td>
</tr>
<tr id="S6.T11.1.1.3" class="ltx_tr">
<td id="S6.T11.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">EPR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib429" title="" class="ltx_ref">429</a>]</cite>
</td>
<td id="S6.T11.1.1.3.2" class="ltx_td ltx_align_left">Demonstration selection (dense retrieval; constrative learning)</td>
</tr>
<tr id="S6.T11.1.1.4" class="ltx_tr">
<td id="S6.T11.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">SG-ICL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib430" title="" class="ltx_ref">430</a>]</cite>
</td>
<td id="S6.T11.1.1.4.2" class="ltx_td ltx_align_left">Demonstration selection (LLM as the demonstration generator)</td>
</tr>
<tr id="S6.T11.1.1.5" class="ltx_tr">
<td id="S6.T11.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">APE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib431" title="" class="ltx_ref">431</a>]</cite>
</td>
<td id="S6.T11.1.1.5.2" class="ltx_td ltx_align_left">Demonstration format (automatic generation &amp; selection)</td>
</tr>
<tr id="S6.T11.1.1.6" class="ltx_tr">
<td id="S6.T11.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">Structured Prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib296" title="" class="ltx_ref">296</a>]</cite>
</td>
<td id="S6.T11.1.1.6.2" class="ltx_td ltx_align_left">Demonstration format (grouped context encoding; rescaled attention)</td>
</tr>
<tr id="S6.T11.1.1.7" class="ltx_tr">
<td id="S6.T11.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r">GlobalE &amp; LocalE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib432" title="" class="ltx_ref">432</a>]</cite>
</td>
<td id="S6.T11.1.1.7.2" class="ltx_td ltx_align_left">Demonstration order (entropy-based metric; probing set generation with LLM)</td>
</tr>
<tr id="S6.T11.1.1.8" class="ltx_tr">
<td id="S6.T11.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="6"><span id="S6.T11.1.1.8.1.1" class="ltx_text">
<span id="S6.T11.1.1.8.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T11.1.1.8.1.1.1.1" class="ltx_tr">
<span id="S6.T11.1.1.8.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Chain-of-thought</span></span>
<span id="S6.T11.1.1.8.1.1.1.2" class="ltx_tr">
<span id="S6.T11.1.1.8.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Prompting&nbsp;(CoT)</span></span>
</span></span></td>
<td id="S6.T11.1.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Complex CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib433" title="" class="ltx_ref">433</a>]</cite>
</td>
<td id="S6.T11.1.1.8.3" class="ltx_td ltx_align_left ltx_border_t">Demonstration (complexity-based selection)</td>
</tr>
<tr id="S6.T11.1.1.9" class="ltx_tr">
<td id="S6.T11.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r">Auto-CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib434" title="" class="ltx_ref">434</a>]</cite>
</td>
<td id="S6.T11.1.1.9.2" class="ltx_td ltx_align_left">Demonstration (automatic generation)</td>
</tr>
<tr id="S6.T11.1.1.10" class="ltx_tr">
<td id="S6.T11.1.1.10.1" class="ltx_td ltx_align_left ltx_border_r">Selection-Inference&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib435" title="" class="ltx_ref">435</a>]</cite>
</td>
<td id="S6.T11.1.1.10.2" class="ltx_td ltx_align_left">Generation (alternate between selection and inference)</td>
</tr>
<tr id="S6.T11.1.1.11" class="ltx_tr">
<td id="S6.T11.1.1.11.1" class="ltx_td ltx_align_left ltx_border_r">Self-consistency&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib436" title="" class="ltx_ref">436</a>]</cite>
</td>
<td id="S6.T11.1.1.11.2" class="ltx_td ltx_align_left">Generation (diverse paths; self-ensemble)</td>
</tr>
<tr id="S6.T11.1.1.12" class="ltx_tr">
<td id="S6.T11.1.1.12.1" class="ltx_td ltx_align_left ltx_border_r">DIVERSE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib437" title="" class="ltx_ref">437</a>]</cite>
</td>
<td id="S6.T11.1.1.12.2" class="ltx_td ltx_align_left">Generation (diverse paths); Verification (step-wise voting)</td>
</tr>
<tr id="S6.T11.1.1.13" class="ltx_tr">
<td id="S6.T11.1.1.13.1" class="ltx_td ltx_align_left ltx_border_r">Rationale-augmented ensembles&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib438" title="" class="ltx_ref">438</a>]</cite>
</td>
<td id="S6.T11.1.1.13.2" class="ltx_td ltx_align_left">Generation (rationale sampling)</td>
</tr>
<tr id="S6.T11.1.1.14" class="ltx_tr">
<td id="S6.T11.1.1.14.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="13"><span id="S6.T11.1.1.14.1.1" class="ltx_text">Planning</span></td>
<td id="S6.T11.1.1.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Least-to-most prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib439" title="" class="ltx_ref">439</a>]</cite>
</td>
<td id="S6.T11.1.1.14.3" class="ltx_td ltx_align_left ltx_border_t">Plan generation (text-based; problem decomposition)</td>
</tr>
<tr id="S6.T11.1.1.15" class="ltx_tr">
<td id="S6.T11.1.1.15.1" class="ltx_td ltx_align_left ltx_border_r">DECOMP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib440" title="" class="ltx_ref">440</a>]</cite>
</td>
<td id="S6.T11.1.1.15.2" class="ltx_td ltx_align_left">Plan generation (text-based; problem decomposition)</td>
</tr>
<tr id="S6.T11.1.1.16" class="ltx_tr">
<td id="S6.T11.1.1.16.1" class="ltx_td ltx_align_left ltx_border_r">PS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib441" title="" class="ltx_ref">441</a>]</cite>
</td>
<td id="S6.T11.1.1.16.2" class="ltx_td ltx_align_left">Plan generation (text-based)</td>
</tr>
<tr id="S6.T11.1.1.17" class="ltx_tr">
<td id="S6.T11.1.1.17.1" class="ltx_td ltx_align_left ltx_border_r">Faithful CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib442" title="" class="ltx_ref">442</a>]</cite>
</td>
<td id="S6.T11.1.1.17.2" class="ltx_td ltx_align_left">Plan generation (code-based)</td>
</tr>
<tr id="S6.T11.1.1.18" class="ltx_tr">
<td id="S6.T11.1.1.18.1" class="ltx_td ltx_align_left ltx_border_r">PAL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>
</td>
<td id="S6.T11.1.1.18.2" class="ltx_td ltx_align_left">Plan generation (code-based; Python)</td>
</tr>
<tr id="S6.T11.1.1.19" class="ltx_tr">
<td id="S6.T11.1.1.19.1" class="ltx_td ltx_align_left ltx_border_r">HuggingGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib444" title="" class="ltx_ref">444</a>]</cite>
</td>
<td id="S6.T11.1.1.19.2" class="ltx_td ltx_align_left">Plan generation (code-based; models from HuggingFace)</td>
</tr>
<tr id="S6.T11.1.1.20" class="ltx_tr">
<td id="S6.T11.1.1.20.1" class="ltx_td ltx_align_left ltx_border_r">AdaPlanner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib445" title="" class="ltx_ref">445</a>]</cite>
</td>
<td id="S6.T11.1.1.20.2" class="ltx_td ltx_align_left">Plan refinement (skill memory)</td>
</tr>
<tr id="S6.T11.1.1.21" class="ltx_tr">
<td id="S6.T11.1.1.21.1" class="ltx_td ltx_align_left ltx_border_r">TIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib446" title="" class="ltx_ref">446</a>]</cite>
</td>
<td id="S6.T11.1.1.21.2" class="ltx_td ltx_align_left">Feedback acquisition (visual perception)</td>
</tr>
<tr id="S6.T11.1.1.22" class="ltx_tr">
<td id="S6.T11.1.1.22.1" class="ltx_td ltx_align_left ltx_border_r">RAP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib447" title="" class="ltx_ref">447</a>]</cite>
</td>
<td id="S6.T11.1.1.22.2" class="ltx_td ltx_align_left">Feedback acquisition (LLM as the world model); Plan refinement (Monte Carlo Tree Search)</td>
</tr>
<tr id="S6.T11.1.1.23" class="ltx_tr">
<td id="S6.T11.1.1.23.1" class="ltx_td ltx_align_left ltx_border_r">ChatCoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib448" title="" class="ltx_ref">448</a>]</cite>
</td>
<td id="S6.T11.1.1.23.2" class="ltx_td ltx_align_left">Feedback acquisition (tool); Plan refinement (conversation between LLM and tools)</td>
</tr>
<tr id="S6.T11.1.1.24" class="ltx_tr">
<td id="S6.T11.1.1.24.1" class="ltx_td ltx_align_left ltx_border_r">ReAct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref">449</a>]</cite>
</td>
<td id="S6.T11.1.1.24.2" class="ltx_td ltx_align_left">Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting)</td>
</tr>
<tr id="S6.T11.1.1.25" class="ltx_tr">
<td id="S6.T11.1.1.25.1" class="ltx_td ltx_align_left ltx_border_r">Reflexion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref">450</a>]</cite>
</td>
<td id="S6.T11.1.1.25.2" class="ltx_td ltx_align_left">Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory)</td>
</tr>
<tr id="S6.T11.1.1.26" class="ltx_tr">
<td id="S6.T11.1.1.26.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Tree of Thoughts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib451" title="" class="ltx_ref">451</a>]</cite>
</td>
<td id="S6.T11.1.1.26.2" class="ltx_td ltx_align_left ltx_border_bb">Feedback acquisition (vote comparison); Plan refinement (tree-based search)</td>
</tr>
</tbody></table>
</span></div>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">After pre-training or adaptation tuning,
a major approach to using LLMs is to design suitable <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">prompting</span> strategies for solving various tasks. In existing literature, task-specific prompts can be effectively learned through manual creation and automatic optimization.
A representative prompting method is <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">in-context learning</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, which formulates the task description and/or demonstrations in the form of natural language text.
In addition, <span id="S6.p1.1.3" class="ltx_text ltx_font_italic">chain-of-thought prompting</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> can be employed to enhance in-context learning by involving a series of intermediate reasoning steps in prompts.
Furthermore, <span id="S6.p1.1.4" class="ltx_text ltx_font_italic">planning</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib439" title="" class="ltx_ref">439</a>]</cite> is proposed for solving complex tasks, which first breaks them down into smaller sub-tasks and then generates a plan of action to solve these sub-tasks one by one.
We summarize representative work for these prompting approaches in Table&nbsp;<a href="#S6.T11" title="TABLE XI ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XI</span></a>.
Next, we will elaborate on the details of the four techniques.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span id="S6.SS1.1.1" class="ltx_text ltx_font_italic">Prompting</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">As discussed in previous work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, prompting is the major approach to utilizing LLMs for solving various tasks.
Since the quality of prompts will largely influence the performance of LLMs in specific tasks, there have been a series of studies proposed to generate suitable task prompts through manual creation or automatic optimization, which will be introduced in this section.</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Prompt Creation</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">The process of manually creating a suitable prompt is also called <em id="S6.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">prompt engineering</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib452" title="" class="ltx_ref">452</a>, <a href="#bib.bib453" title="" class="ltx_ref">453</a>]</cite>.
A well-designed prompt is very helpful to elicit the abilities of LLMs for accomplishing specific tasks.
In this part, we will first introduce the key components of prompts and discuss several principles for prompt design. Then, we evaluate ChatGPT with different prompts to show the results on several representative tasks.
We are aware that there have been several existing papers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib454" title="" class="ltx_ref">454</a>, <a href="#bib.bib453" title="" class="ltx_ref">453</a>]</cite> and websites&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib455" title="" class="ltx_ref">455</a>, <a href="#bib.bib456" title="" class="ltx_ref">456</a>, <a href="#bib.bib457" title="" class="ltx_ref">457</a>]</cite> that present the suggestions and guidelines to design good prompts.
As a comparison, we mainly aim to discuss the key factors (ingredients and principles) that are useful for prompt creation, and provide experimental results and analysis on popular tasks as the reference to the beginners.</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p2.1" class="ltx_p"><span id="S6.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Key Ingredients.</span>
Typically, there are four key ingredients that depict the functionality of a prompt for eliciting the abilities of LLMs to complete the tasks, including task description, input data, contextual information, and prompt style. To have an intuitive understanding of our discussion, we also present three prompt examples for question answering, meta-review generation, and text-to-SQL in Table&nbsp;<a href="#S6.T13" title="TABLE XIII ‣ 6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XIII</span></a>.</p>
</div>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p id="S6.SS1.SSS1.p3.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">Task description.</em>
A task description is typically a specific instruction that LLMs are expected to follow.
In general, one should clearly describe the task goal in natural language.
For the tasks with special input or output format, detailed clarifications are often needed, and one can further utilize keywords to highlight the special settings for better guiding LLMs in task completion.</p>
</div>
<div id="S6.SS1.SSS1.p4" class="ltx_para">
<p id="S6.SS1.SSS1.p4.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Input data.</em>
In common cases, it is straightforward to describe input data (<em id="S6.SS1.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> an instance to be responded by LLMs) in natural language.
For special input data, such as knowledge graph and table, it is necessary to apply an appropriate and convenient way to make them readable for LLMs. For structured data, linearization is commonly used to transform the original records (<em id="S6.SS1.SSS1.p4.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> knowledge triples) into sequences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref">458</a>]</cite> due to the simplicity.
Further, the programming language (<em id="S6.SS1.SSS1.p4.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> executable code) has also been utilized to formulate the structured data, which can also support using external tools (<em id="S6.SS1.SSS1.p4.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> program executor) to produce the precise results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib459" title="" class="ltx_ref">459</a>, <a href="#bib.bib460" title="" class="ltx_ref">460</a>]</cite>.</p>
</div>
<div id="S6.SS1.SSS1.p5" class="ltx_para">
<p id="S6.SS1.SSS1.p5.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">Contextual information.</em>
In addition to the task description and input data, contextual or background information is also essential for specific tasks. For example, retrieved documents are highly useful for open-domain question answering as supporting evidence. Both the quality of the retrieved documents and their relevance to the question have an impact on the generated answers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib461" title="" class="ltx_ref">461</a>]</cite>.
Thus, it needs to include such information in a proper prompt pattern or expression format.
Furthermore, in-context task exemplars are also helpful for eliciting LLMs to accomplish a complex task, which can better depict the task goal, the special output formats, and the mapping relation between input and output.</p>
</div>
<div id="S6.SS1.SSS1.p6" class="ltx_para">
<p id="S6.SS1.SSS1.p6.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">Prompt style.</em>
For different LLMs, it is important to design a suitable prompt style for eliciting their abilities to solve specific tasks.
Overall, one should express the prompt as a clear question or detailed instruction that can be well understood and answered.
In some cases, it is also useful to add the prefix or suffix to better guide LLMs.
For example, using the prefix “<em id="S6.SS1.SSS1.p6.1.2" class="ltx_emph ltx_font_italic">Let us think step by step</em>” can help elicit LLMs perform step-by-step reasoning, and using the prefix “<em id="S6.SS1.SSS1.p6.1.3" class="ltx_emph ltx_font_italic">You are an expert on this task (or in this domain)</em>” can boost the performance of LLMs in some specific tasks.
Further,
for chat-based LLMs (<em id="S6.SS1.SSS1.p6.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> ChatGPT), instead of directly feeding a long or complex task prompt, it is suggested to decompose it into multiple prompts for the sub-tasks and then feed them into LLMs via a multi-turn conversation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib448" title="" class="ltx_ref">448</a>]</cite>.</p>
</div>
<div id="S6.SS1.SSS1.p7" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p7.1" class="ltx_p"><span id="S6.SS1.SSS1.p7.1.1" class="ltx_text ltx_font_bold">Design Principles.</span> Based on the key ingredients of prompts, we summarize several critical design principles that can help create more effective prompts for solving various tasks.</p>
</div>
<div id="S6.SS1.SSS1.p8" class="ltx_para">
<p id="S6.SS1.SSS1.p8.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p8.1.1" class="ltx_emph ltx_font_italic">Expressing the task goal clearly.</em> Task descriptions should not be ambiguous or unclear, which likely lead to inaccurate or inappropriate responses. This highlights the need for clear and unambiguous directives when utilizing these models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. A clear and detailed description should contain various elements to explain a task, including task objective, input/output data (<em id="S6.SS1.SSS1.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> “<em id="S6.SS1.SSS1.p8.1.3" class="ltx_emph ltx_font_italic">Given a long document, I want you to generate a concise summary.</em>”), and the response constraints (<em id="S6.SS1.SSS1.p8.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> “<em id="S6.SS1.SSS1.p8.1.5" class="ltx_emph ltx_font_italic">the length of the summary cannot exceed 50.</em>”). By providing a well-clarified task description, LLMs can more effectively understand the target task and generate the desired output.</p>
</div>
<div id="S6.SS1.SSS1.p9" class="ltx_para">
<p id="S6.SS1.SSS1.p9.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p9.1.1" class="ltx_emph ltx_font_italic">Decomposing into easy, detailed sub-tasks.</em> To solve complex tasks, it is important to decompose the difficult task into several more easier, detailed sub-tasks for helping LLMs accomplish the goal step by step, which is closely related to the planning technique in Section&nbsp;<a href="#S6.SS4" title="6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>.

For example, following the suggestion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib454" title="" class="ltx_ref">454</a>]</cite>, we can explicitly list the sub-tasks in the form of multiple numbered items (<em id="S6.SS1.SSS1.p9.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> “<em id="S6.SS1.SSS1.p9.1.3" class="ltx_emph ltx_font_italic">Braid a coherent narrative by performing the following tasks: 1. …; 2. …; 3. …</em>”). By decomposing a target task into sub-tasks, LLMs can focus on solving easier sub-tasks and finally achieve more accurate results for complex tasks.</p>
</div>
<div id="S6.SS1.SSS1.p10" class="ltx_para">
<p id="S6.SS1.SSS1.p10.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p10.1.1" class="ltx_emph ltx_font_italic">Providing few-shot demonstrations.</em> As discussed in Section&nbsp;<a href="#S6.SS2" title="6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>, LLMs can benefit from in-context learning for solving complex tasks, where the prompts contain a small number of task examples of the desired input-output pairs, <em id="S6.SS1.SSS1.p10.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> few-shot demonstrations. Few-shot demonstrations can help LLMs learn the semantic mapping between input and output without parameter tuning.
In practice, it is suggested that one should generate a few high-quality demonstrations for the target task, which would highly benefit the final task performance.</p>
</div>
<div id="S6.SS1.SSS1.p11" class="ltx_para">
<p id="S6.SS1.SSS1.p11.1" class="ltx_p">•&nbsp;<em id="S6.SS1.SSS1.p11.1.1" class="ltx_emph ltx_font_italic">Utilizing model-friendly format.</em>
Since LLMs are pretrained on specially constructed datasets, there are some prompt formats that can make LLMs better understand the instruction. For example, as the OpenAI documentation suggests, we can use <span id="S6.SS1.SSS1.p11.1.2" class="ltx_text ltx_font_typewriter">###</span> or <span id="S6.SS1.SSS1.p11.1.3" class="ltx_text ltx_font_typewriter">"""</span> as a stop symbol to separate the instruction and context, which can be better understood by LLMs. As a general guideline, most existing LLMs perform a task better in English, thus it is useful to employ English instructions to solve difficult tasks based on machine translation.</p>
</div>
<div id="S6.SS1.SSS1.p12" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p12.1" class="ltx_p"><span id="S6.SS1.SSS1.p12.1.1" class="ltx_text ltx_font_bold">Useful Tips.</span>
In addition to the design principles, we also present a collection of useful prompt tips based on existing work or our empirical experiences in Table&nbsp;<a href="#S6.T12" title="TABLE XII ‣ 6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XII</span></a>.
Note that these tips are suggested in a general manner, it does not indicate that they are the best prompts for the corresponding tasks.
This part will be continuously updated with more guidelines or tips. We welcome readers to contribute to this collection of prompt tips.
We present the detailed procedure to contribute to the prompt tips, at the link: <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts</a>.</p>
</div>
<figure id="S6.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XII: </span>A collection of useful tips for designing prompts that are collected from online notes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib454" title="" class="ltx_ref">454</a>, <a href="#bib.bib453" title="" class="ltx_ref">453</a>, <a href="#bib.bib455" title="" class="ltx_ref">455</a>, <a href="#bib.bib456" title="" class="ltx_ref">456</a>]</cite> and experiences from our authors, where we also show the related ingredients and principles (introduced in Section&nbsp;<a href="#S6.SS1.SSS1" title="6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>). We abbreviate principles as Prin. and list the IDs of the related principles for each prompt. ①: expressing the task goal clearly; ②: decomposing into easy, detailed sub-tasks; ③: providing few-shot demonstrations; ④: utilizing model-friendly format.</figcaption>
<table id="S6.T12.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T12.8.9" class="ltx_tr">
<td id="S6.T12.8.9.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T12.8.9.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Ingredient</span></td>
<td id="S6.T12.8.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S6.T12.8.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.9.2.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.9.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Collected Prompts</span></span>
</span>
</td>
<td id="S6.T12.8.9.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T12.8.9.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">
Prin.</span></td>
</tr>
<tr id="S6.T12.8.10" class="ltx_tr">
<td id="S6.T12.8.10.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S6.T12.8.10.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Task Description</span></td>
<td id="S6.T12.8.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T12.8.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.10.2.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.10.2.1.1.1" class="ltx_text" style="font-size:70%;">T1. Make your prompt </span><span id="S6.T12.8.10.2.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">as detailed as possible</span><span id="S6.T12.8.10.2.1.1.3" class="ltx_text" style="font-size:70%;">, </span><em id="S6.T12.8.10.2.1.1.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.10.2.1.1.5" class="ltx_text" style="font-size:70%;"> “</span><em id="S6.T12.8.10.2.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">Summarize the article into a short paragraph within 50 words. The major storyline and conclusion should be included, and the unimportant details can be omitted.</em><span id="S6.T12.8.10.2.1.1.7" class="ltx_text" style="font-size:70%;">”</span></span>
</span>
</td>
<td id="S6.T12.8.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T12.8.10.3.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.11" class="ltx_tr">
<td id="S6.T12.8.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.11.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.11.1.1.1.1" class="ltx_text" style="font-size:70%;">T2. It is helpful to let the LLM know that it is </span><span id="S6.T12.8.11.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">an expert with a prefixed prompt</span><span id="S6.T12.8.11.1.1.1.3" class="ltx_text" style="font-size:70%;">, </span><em id="S6.T12.8.11.1.1.1.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.11.1.1.1.5" class="ltx_text" style="font-size:70%;"> “</span><em id="S6.T12.8.11.1.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">You are a sophisticated expert in the domain of compute science.</em><span id="S6.T12.8.11.1.1.1.7" class="ltx_text" style="font-size:70%;">”</span></span>
</span>
</td>
<td id="S6.T12.8.11.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.11.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.12" class="ltx_tr">
<td id="S6.T12.8.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.12.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.12.1.1.1.1" class="ltx_text" style="font-size:70%;">T3. Tell the model </span><span id="S6.T12.8.12.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">more what it should do</span><span id="S6.T12.8.12.1.1.1.3" class="ltx_text" style="font-size:70%;">, but not what it should not do.</span></span>
</span>
</td>
<td id="S6.T12.8.12.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.12.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.13" class="ltx_tr">
<td id="S6.T12.8.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.13.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.13.1.1.1.1" class="ltx_text" style="font-size:70%;">T4. To avoid the LLM to generate too long output, you can just use the prompt: “</span><em id="S6.T12.8.13.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">Question:  Short Answer: </em><span id="S6.T12.8.13.1.1.1.3" class="ltx_text" style="font-size:70%;">”. Besides, you can also use the following suffixes, “</span><em id="S6.T12.8.13.1.1.1.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">in a or a few words</em><span id="S6.T12.8.13.1.1.1.5" class="ltx_text" style="font-size:70%;">”, “</span><em id="S6.T12.8.13.1.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">in one of two sentences</em><span id="S6.T12.8.13.1.1.1.7" class="ltx_text" style="font-size:70%;">”.</span></span>
</span>
</td>
<td id="S6.T12.8.13.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.13.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.14" class="ltx_tr">
<td id="S6.T12.8.14.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S6.T12.8.14.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Input Data</span></td>
<td id="S6.T12.8.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T12.8.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.14.2.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.14.2.1.1.1" class="ltx_text" style="font-size:70%;">I1. For the question required factual knowledge, it is useful to first </span><span id="S6.T12.8.14.2.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">retrieve relevant documents</span><span id="S6.T12.8.14.2.1.1.3" class="ltx_text" style="font-size:70%;"> via the search engine, and then </span><span id="S6.T12.8.14.2.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">concatenate them into the prompt</span><span id="S6.T12.8.14.2.1.1.5" class="ltx_text" style="font-size:70%;"> as reference.</span></span>
</span>
</td>
<td id="S6.T12.8.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T12.8.14.3.1" class="ltx_text" style="font-size:70%;">④</span></td>
</tr>
<tr id="S6.T12.2.2" class="ltx_tr">
<td id="S6.T12.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.2.2.2.2.2" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">I2. To highlight some important parts in your prompt, please </span><span id="S6.T12.2.2.2.2.2.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">use special marks</span><span id="S6.T12.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">, </span><em id="S6.T12.2.2.2.2.2.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.2.2.2.2.2.5" class="ltx_text" style="font-size:70%;"> </span><em id="S6.T12.2.2.2.2.2.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">quotation</em><span id="S6.T12.2.2.2.2.2.7" class="ltx_text" style="font-size:70%;"> (</span><math id="S6.T12.1.1.1.1.1.m1.1" class="ltx_Math" alttext="&quot;&quot;" display="inline"><semantics id="S6.T12.1.1.1.1.1.m1.1a"><mrow id="S6.T12.1.1.1.1.1.m1.1.1" xref="S6.T12.1.1.1.1.1.m1.1.1.cmml"><mi mathsize="70%" mathvariant="normal" id="S6.T12.1.1.1.1.1.m1.1.1.2" xref="S6.T12.1.1.1.1.1.m1.1.1.2.cmml">"</mi><mo lspace="0em" rspace="0em" id="S6.T12.1.1.1.1.1.m1.1.1.1" xref="S6.T12.1.1.1.1.1.m1.1.1.1.cmml">​</mo><mi mathsize="70%" mathvariant="normal" id="S6.T12.1.1.1.1.1.m1.1.1.3" xref="S6.T12.1.1.1.1.1.m1.1.1.3.cmml">"</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T12.1.1.1.1.1.m1.1b"><apply id="S6.T12.1.1.1.1.1.m1.1.1.cmml" xref="S6.T12.1.1.1.1.1.m1.1.1"><times id="S6.T12.1.1.1.1.1.m1.1.1.1.cmml" xref="S6.T12.1.1.1.1.1.m1.1.1.1"></times><ci id="S6.T12.1.1.1.1.1.m1.1.1.2.cmml" xref="S6.T12.1.1.1.1.1.m1.1.1.2">"</ci><ci id="S6.T12.1.1.1.1.1.m1.1.1.3.cmml" xref="S6.T12.1.1.1.1.1.m1.1.1.3">"</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.1.1.1.1.1.m1.1c">""</annotation></semantics></math><span id="S6.T12.2.2.2.2.2.8" class="ltx_text" style="font-size:70%;">) and </span><em id="S6.T12.2.2.2.2.2.9" class="ltx_emph ltx_font_italic" style="font-size:70%;">line break</em><span id="S6.T12.2.2.2.2.2.10" class="ltx_text" style="font-size:70%;"> (</span><math id="S6.T12.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S6.T12.2.2.2.2.2.m2.1a"><mo mathsize="70%" id="S6.T12.2.2.2.2.2.m2.1.1" xref="S6.T12.2.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S6.T12.2.2.2.2.2.m2.1b"><ci id="S6.T12.2.2.2.2.2.m2.1.1.cmml" xref="S6.T12.2.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.2.2.2.2.2.m2.1c">\backslash</annotation></semantics></math><span id="S6.T12.2.2.2.2.2.11" class="ltx_text" style="font-size:70%;">n). You can also use both of them for emphasizing.</span></span>
</span>
</td>
<td id="S6.T12.2.2.3" class="ltx_td ltx_align_center"><span id="S6.T12.2.2.3.1" class="ltx_text" style="font-size:70%;">④</span></td>
</tr>
<tr id="S6.T12.3.3" class="ltx_tr">
<td id="S6.T12.3.3.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S6.T12.3.3.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Contextual Information</span></td>
<td id="S6.T12.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T12.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.3.3.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.3.3.1.1.1.2" class="ltx_text" style="font-size:70%;">C1. For complex tasks, you can </span><span id="S6.T12.3.3.1.1.1.3" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">clearly describe the required intermediate steps</span><span id="S6.T12.3.3.1.1.1.4" class="ltx_text" style="font-size:70%;"> to accomplish it, </span><em id="S6.T12.3.3.1.1.1.5" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.3.3.1.1.1.6" class="ltx_text" style="font-size:70%;"> “</span><em id="S6.T12.3.3.1.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Please answer the question step by step as: Step 1 - Decompose the question into several sub-questions, <math id="S6.T12.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S6.T12.3.3.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S6.T12.3.3.1.1.1.1.m1.1.1" xref="S6.T12.3.3.1.1.1.1.m1.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="S6.T12.3.3.1.1.1.1.m1.1b"><ci id="S6.T12.3.3.1.1.1.1.m1.1.1.cmml" xref="S6.T12.3.3.1.1.1.1.m1.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.3.3.1.1.1.1.m1.1c">\cdots</annotation></semantics></math></em><span id="S6.T12.3.3.1.1.1.7" class="ltx_text" style="font-size:70%;">”</span></span>
</span>
</td>
<td id="S6.T12.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T12.3.3.3.1" class="ltx_text" style="font-size:70%;">②</span></td>
</tr>
<tr id="S6.T12.8.15" class="ltx_tr">
<td id="S6.T12.8.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.15.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.15.1.1.1.1" class="ltx_text" style="font-size:70%;">C2. If you want LLMs to provide the score for a text, it is necessary to provide a </span><span id="S6.T12.8.15.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">detailed description about the</span><span id="S6.T12.8.15.1.1.1.3" class="ltx_text" style="font-size:70%;"> </span><span id="S6.T12.8.15.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">scoring standard</span><span id="S6.T12.8.15.1.1.1.5" class="ltx_text" style="font-size:70%;"> with examples as reference.</span></span>
</span>
</td>
<td id="S6.T12.8.15.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.15.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.16" class="ltx_tr">
<td id="S6.T12.8.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.16.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.16.1.1.1.1" class="ltx_text" style="font-size:70%;">C3. When LLMs generate text according to some context (</span><em id="S6.T12.8.16.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.16.1.1.1.3" class="ltx_text" style="font-size:70%;"> making recommendations according to purchase history), instructing them with </span><span id="S6.T12.8.16.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">the explanation about the generated result</span><span id="S6.T12.8.16.1.1.1.5" class="ltx_text" style="font-size:70%;"> conditioned on context is helpful to improve the quality of the generated text.</span></span>
</span>
</td>
<td id="S6.T12.8.16.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.16.2.1" class="ltx_text" style="font-size:70%;">②</span></td>
</tr>
<tr id="S6.T12.8.17" class="ltx_tr">
<td id="S6.T12.8.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.17.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.17.1.1.1.1" class="ltx_text" style="font-size:70%;">C4. An approach similar to </span><span id="S6.T12.8.17.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">tree-of-thoughts</span><span id="S6.T12.8.17.1.1.1.3" class="ltx_text" style="font-size:70%;"> but can be </span><span id="S6.T12.8.17.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">done in one prompt</span><span id="S6.T12.8.17.1.1.1.5" class="ltx_text" style="font-size:70%;">: </span><em id="S6.T12.8.17.1.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.17.1.1.1.7" class="ltx_text" style="font-size:70%;"> </span><em id="S6.T12.8.17.1.1.1.8" class="ltx_emph ltx_font_italic" style="font-size:70%;">Imagine three different experts are answering this question. All experts will write down one step of their thinking, then share it with the group of experts. Then all experts will go on to the next step, etc. If any expert realizes they’re wrong at any point then they leave. The question is</em></span>
</span>
</td>
<td id="S6.T12.8.17.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.17.2.1" class="ltx_text" style="font-size:70%;">②</span></td>
</tr>
<tr id="S6.T12.8.18" class="ltx_tr">
<td id="S6.T12.8.18.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="9"><span id="S6.T12.8.18.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Demonstration</span></td>
<td id="S6.T12.8.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T12.8.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.18.2.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.18.2.1.1.1" class="ltx_text" style="font-size:70%;">D1. </span><span id="S6.T12.8.18.2.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">Well-formatted in-context exemplars</span><span id="S6.T12.8.18.2.1.1.3" class="ltx_text" style="font-size:70%;"> are very useful, especially for producing the outputs with complex formats.</span></span>
</span>
</td>
<td id="S6.T12.8.18.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T12.8.18.3.1" class="ltx_text" style="font-size:70%;">③</span></td>
</tr>
<tr id="S6.T12.4.4" class="ltx_tr">
<td id="S6.T12.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.4.4.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.4.4.1.1.1.2" class="ltx_text" style="font-size:70%;">D2. For few-shot chain-of-thought prompting, you can also use the prompt “</span><em id="S6.T12.4.4.1.1.1.3" class="ltx_emph ltx_font_italic" style="font-size:70%;">Let’s think step-by-step</em><span id="S6.T12.4.4.1.1.1.4" class="ltx_text" style="font-size:70%;">”, and the few-shot examples should be </span><span id="S6.T12.4.4.1.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">separated by “<math id="S6.T12.4.4.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S6.T12.4.4.1.1.1.1.m1.1a"><mo id="S6.T12.4.4.1.1.1.1.m1.1.1" xref="S6.T12.4.4.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S6.T12.4.4.1.1.1.1.m1.1b"><ci id="S6.T12.4.4.1.1.1.1.m1.1.1.cmml" xref="S6.T12.4.4.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.4.4.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n”</span><span id="S6.T12.4.4.1.1.1.5" class="ltx_text" style="font-size:70%;"> instead of full stop.</span></span>
</span>
</td>
<td id="S6.T12.4.4.2" class="ltx_td ltx_align_center"><span id="S6.T12.4.4.2.1" class="ltx_text" style="font-size:70%;">①③</span></td>
</tr>
<tr id="S6.T12.8.19" class="ltx_tr">
<td id="S6.T12.8.19.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.19.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.19.1.1.1.1" class="ltx_text" style="font-size:70%;">D3. You can also </span><span id="S6.T12.8.19.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">retrieve similar examples</span><span id="S6.T12.8.19.1.1.1.3" class="ltx_text" style="font-size:70%;"> in context to supply the useful task-specific knowledge for LLMs. To retrieve more relevant examples, it is useful to </span><span id="S6.T12.8.19.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">first obtain the answer</span><span id="S6.T12.8.19.1.1.1.5" class="ltx_text" style="font-size:70%;"> of the question, and then concatenate it with the question for retrieval.</span></span>
</span>
</td>
<td id="S6.T12.8.19.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.19.2.1" class="ltx_text" style="font-size:70%;">③④</span></td>
</tr>
<tr id="S6.T12.8.20" class="ltx_tr">
<td id="S6.T12.8.20.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.20.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.20.1.1.1.1" class="ltx_text" style="font-size:70%;">D4. The </span><span id="S6.T12.8.20.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">diversity of the in-context exemplars</span><span id="S6.T12.8.20.1.1.1.3" class="ltx_text" style="font-size:70%;"> within the prompt is also useful. If it is not easy to obtain diverse questions, you can also seek to keep the </span><span id="S6.T12.8.20.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">diversity of the solutions</span><span id="S6.T12.8.20.1.1.1.5" class="ltx_text" style="font-size:70%;"> for the questions.</span></span>
</span>
</td>
<td id="S6.T12.8.20.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.20.2.1" class="ltx_text" style="font-size:70%;">③</span></td>
</tr>
<tr id="S6.T12.8.21" class="ltx_tr">
<td id="S6.T12.8.21.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.21.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.21.1.1.1.1" class="ltx_text" style="font-size:70%;">D5. When using chat-based LLMs, you can </span><span id="S6.T12.8.21.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">decompose in-context exemplars into multi-turn messages</span><span id="S6.T12.8.21.1.1.1.3" class="ltx_text" style="font-size:70%;">, to better match the human-chatbot conversation format. Similarly, you can also decompose the reasoning process of an exemplars into multi-turn conversation.</span></span>
</span>
</td>
<td id="S6.T12.8.21.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.21.2.1" class="ltx_text" style="font-size:70%;">③</span></td>
</tr>
<tr id="S6.T12.8.22" class="ltx_tr">
<td id="S6.T12.8.22.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.22.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.22.1.1.1.1" class="ltx_text" style="font-size:70%;">D6. </span><span id="S6.T12.8.22.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">Complex and informative</span><span id="S6.T12.8.22.1.1.1.3" class="ltx_text" style="font-size:70%;"> in-context exemplars can help LLMs answer complex questions.</span></span>
</span>
</td>
<td id="S6.T12.8.22.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.22.2.1" class="ltx_text" style="font-size:70%;">③</span></td>
</tr>
<tr id="S6.T12.8.8" class="ltx_tr">
<td id="S6.T12.8.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.8.4.4" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.8.4.4.4" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.8.4.4.4.1" class="ltx_text" style="font-size:70%;">D7. As a symbol sequence can typically be divided into multiple segments (</span><em id="S6.T12.8.8.4.4.4.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.8.4.4.4.3" class="ltx_text" style="font-size:70%;"> </span><math id="S6.T12.5.5.1.1.1.m1.3" class="ltx_Math" alttext="i_{1},i_{2},i_{3}" display="inline"><semantics id="S6.T12.5.5.1.1.1.m1.3a"><mrow id="S6.T12.5.5.1.1.1.m1.3.3.3" xref="S6.T12.5.5.1.1.1.m1.3.3.4.cmml"><msub id="S6.T12.5.5.1.1.1.m1.1.1.1.1" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1.cmml"><mi mathsize="70%" id="S6.T12.5.5.1.1.1.m1.1.1.1.1.2" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.5.5.1.1.1.m1.1.1.1.1.3" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo mathsize="70%" id="S6.T12.5.5.1.1.1.m1.3.3.3.4" xref="S6.T12.5.5.1.1.1.m1.3.3.4.cmml">,</mo><msub id="S6.T12.5.5.1.1.1.m1.2.2.2.2" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2.cmml"><mi mathsize="70%" id="S6.T12.5.5.1.1.1.m1.2.2.2.2.2" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.5.5.1.1.1.m1.2.2.2.2.3" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo mathsize="70%" id="S6.T12.5.5.1.1.1.m1.3.3.3.5" xref="S6.T12.5.5.1.1.1.m1.3.3.4.cmml">,</mo><msub id="S6.T12.5.5.1.1.1.m1.3.3.3.3" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3.cmml"><mi mathsize="70%" id="S6.T12.5.5.1.1.1.m1.3.3.3.3.2" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.5.5.1.1.1.m1.3.3.3.3.3" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T12.5.5.1.1.1.m1.3b"><list id="S6.T12.5.5.1.1.1.m1.3.3.4.cmml" xref="S6.T12.5.5.1.1.1.m1.3.3.3"><apply id="S6.T12.5.5.1.1.1.m1.1.1.1.1.cmml" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S6.T12.5.5.1.1.1.m1.1.1.1.1.1.cmml" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1">subscript</csymbol><ci id="S6.T12.5.5.1.1.1.m1.1.1.1.1.2.cmml" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S6.T12.5.5.1.1.1.m1.1.1.1.1.3.cmml" xref="S6.T12.5.5.1.1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S6.T12.5.5.1.1.1.m1.2.2.2.2.cmml" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S6.T12.5.5.1.1.1.m1.2.2.2.2.1.cmml" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2">subscript</csymbol><ci id="S6.T12.5.5.1.1.1.m1.2.2.2.2.2.cmml" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2.2">𝑖</ci><cn type="integer" id="S6.T12.5.5.1.1.1.m1.2.2.2.2.3.cmml" xref="S6.T12.5.5.1.1.1.m1.2.2.2.2.3">2</cn></apply><apply id="S6.T12.5.5.1.1.1.m1.3.3.3.3.cmml" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S6.T12.5.5.1.1.1.m1.3.3.3.3.1.cmml" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3">subscript</csymbol><ci id="S6.T12.5.5.1.1.1.m1.3.3.3.3.2.cmml" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3.2">𝑖</ci><cn type="integer" id="S6.T12.5.5.1.1.1.m1.3.3.3.3.3.cmml" xref="S6.T12.5.5.1.1.1.m1.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.5.5.1.1.1.m1.3c">i_{1},i_{2},i_{3}</annotation></semantics></math><span id="S6.T12.8.8.4.4.4.4" class="ltx_text" style="font-size:70%;"> </span><math id="S6.T12.6.6.2.2.2.m2.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S6.T12.6.6.2.2.2.m2.1a"><mo mathsize="70%" stretchy="false" id="S6.T12.6.6.2.2.2.m2.1.1" xref="S6.T12.6.6.2.2.2.m2.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S6.T12.6.6.2.2.2.m2.1b"><ci id="S6.T12.6.6.2.2.2.m2.1.1.cmml" xref="S6.T12.6.6.2.2.2.m2.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.6.6.2.2.2.m2.1c">\longrightarrow</annotation></semantics></math><span id="S6.T12.8.8.4.4.4.5" class="ltx_text" style="font-size:70%;"> </span><math id="S6.T12.7.7.3.3.3.m3.2" class="ltx_Math" alttext="i_{1},i_{2}" display="inline"><semantics id="S6.T12.7.7.3.3.3.m3.2a"><mrow id="S6.T12.7.7.3.3.3.m3.2.2.2" xref="S6.T12.7.7.3.3.3.m3.2.2.3.cmml"><msub id="S6.T12.7.7.3.3.3.m3.1.1.1.1" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1.cmml"><mi mathsize="70%" id="S6.T12.7.7.3.3.3.m3.1.1.1.1.2" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.7.7.3.3.3.m3.1.1.1.1.3" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1.3.cmml">1</mn></msub><mo mathsize="70%" id="S6.T12.7.7.3.3.3.m3.2.2.2.3" xref="S6.T12.7.7.3.3.3.m3.2.2.3.cmml">,</mo><msub id="S6.T12.7.7.3.3.3.m3.2.2.2.2" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2.cmml"><mi mathsize="70%" id="S6.T12.7.7.3.3.3.m3.2.2.2.2.2" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.7.7.3.3.3.m3.2.2.2.2.3" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T12.7.7.3.3.3.m3.2b"><list id="S6.T12.7.7.3.3.3.m3.2.2.3.cmml" xref="S6.T12.7.7.3.3.3.m3.2.2.2"><apply id="S6.T12.7.7.3.3.3.m3.1.1.1.1.cmml" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S6.T12.7.7.3.3.3.m3.1.1.1.1.1.cmml" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1">subscript</csymbol><ci id="S6.T12.7.7.3.3.3.m3.1.1.1.1.2.cmml" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1.2">𝑖</ci><cn type="integer" id="S6.T12.7.7.3.3.3.m3.1.1.1.1.3.cmml" xref="S6.T12.7.7.3.3.3.m3.1.1.1.1.3">1</cn></apply><apply id="S6.T12.7.7.3.3.3.m3.2.2.2.2.cmml" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S6.T12.7.7.3.3.3.m3.2.2.2.2.1.cmml" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2">subscript</csymbol><ci id="S6.T12.7.7.3.3.3.m3.2.2.2.2.2.cmml" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2.2">𝑖</ci><cn type="integer" id="S6.T12.7.7.3.3.3.m3.2.2.2.2.3.cmml" xref="S6.T12.7.7.3.3.3.m3.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.7.7.3.3.3.m3.2c">i_{1},i_{2}</annotation></semantics></math><span id="S6.T12.8.8.4.4.4.6" class="ltx_text" style="font-size:70%;"> and </span><math id="S6.T12.8.8.4.4.4.m4.2" class="ltx_Math" alttext="i_{2},i_{3}" display="inline"><semantics id="S6.T12.8.8.4.4.4.m4.2a"><mrow id="S6.T12.8.8.4.4.4.m4.2.2.2" xref="S6.T12.8.8.4.4.4.m4.2.2.3.cmml"><msub id="S6.T12.8.8.4.4.4.m4.1.1.1.1" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1.cmml"><mi mathsize="70%" id="S6.T12.8.8.4.4.4.m4.1.1.1.1.2" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.8.8.4.4.4.m4.1.1.1.1.3" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1.3.cmml">2</mn></msub><mo mathsize="70%" id="S6.T12.8.8.4.4.4.m4.2.2.2.3" xref="S6.T12.8.8.4.4.4.m4.2.2.3.cmml">,</mo><msub id="S6.T12.8.8.4.4.4.m4.2.2.2.2" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2.cmml"><mi mathsize="70%" id="S6.T12.8.8.4.4.4.m4.2.2.2.2.2" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2.2.cmml">i</mi><mn mathsize="70%" id="S6.T12.8.8.4.4.4.m4.2.2.2.2.3" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T12.8.8.4.4.4.m4.2b"><list id="S6.T12.8.8.4.4.4.m4.2.2.3.cmml" xref="S6.T12.8.8.4.4.4.m4.2.2.2"><apply id="S6.T12.8.8.4.4.4.m4.1.1.1.1.cmml" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S6.T12.8.8.4.4.4.m4.1.1.1.1.1.cmml" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1">subscript</csymbol><ci id="S6.T12.8.8.4.4.4.m4.1.1.1.1.2.cmml" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1.2">𝑖</ci><cn type="integer" id="S6.T12.8.8.4.4.4.m4.1.1.1.1.3.cmml" xref="S6.T12.8.8.4.4.4.m4.1.1.1.1.3">2</cn></apply><apply id="S6.T12.8.8.4.4.4.m4.2.2.2.2.cmml" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S6.T12.8.8.4.4.4.m4.2.2.2.2.1.cmml" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2">subscript</csymbol><ci id="S6.T12.8.8.4.4.4.m4.2.2.2.2.2.cmml" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2.2">𝑖</ci><cn type="integer" id="S6.T12.8.8.4.4.4.m4.2.2.2.2.3.cmml" xref="S6.T12.8.8.4.4.4.m4.2.2.2.2.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.T12.8.8.4.4.4.m4.2c">i_{2},i_{3}</annotation></semantics></math><span id="S6.T12.8.8.4.4.4.7" class="ltx_text" style="font-size:70%;">), the preceding ones can be used </span><span id="S6.T12.8.8.4.4.4.8" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">as in-context exemplars</span><span id="S6.T12.8.8.4.4.4.9" class="ltx_text" style="font-size:70%;"> to guide LLMs to predict the subsequent ones, meanwhile providing historical information.</span></span>
</span>
</td>
<td id="S6.T12.8.8.5" class="ltx_td ltx_align_center"><span id="S6.T12.8.8.5.1" class="ltx_text" style="font-size:70%;">②③</span></td>
</tr>
<tr id="S6.T12.8.23" class="ltx_tr">
<td id="S6.T12.8.23.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.23.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.23.1.1.1.1" class="ltx_text" style="font-size:70%;">D8. </span><span id="S6.T12.8.23.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">Order matters</span><span id="S6.T12.8.23.1.1.1.3" class="ltx_text" style="font-size:70%;"> for in-context exemplars and prompts components. For very long input data, the position of the question (first or last) may also affect the performance.</span></span>
</span>
</td>
<td id="S6.T12.8.23.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.23.2.1" class="ltx_text" style="font-size:70%;">③</span></td>
</tr>
<tr id="S6.T12.8.24" class="ltx_tr">
<td id="S6.T12.8.24.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.24.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.24.1.1.1.1" class="ltx_text" style="font-size:70%;">D9. If you can not obtain the in-context exemplars from existing datasets, an alternative way is to use the </span><span id="S6.T12.8.24.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">zero-shot</span><span id="S6.T12.8.24.1.1.1.3" class="ltx_text" style="font-size:70%;"> </span><span id="S6.T12.8.24.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">generated ones</span><span id="S6.T12.8.24.1.1.1.5" class="ltx_text" style="font-size:70%;"> from the LLM itself.</span></span>
</span>
</td>
<td id="S6.T12.8.24.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.24.2.1" class="ltx_text" style="font-size:70%;">③</span></td>
</tr>
<tr id="S6.T12.8.25" class="ltx_tr">
<td id="S6.T12.8.25.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="8"><span id="S6.T12.8.25.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Other Designs</span></td>
<td id="S6.T12.8.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T12.8.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.25.2.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.25.2.1.1.1" class="ltx_text" style="font-size:70%;">O1. Let the </span><span id="S6.T12.8.25.2.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">LLM check its outputs</span><span id="S6.T12.8.25.2.1.1.3" class="ltx_text" style="font-size:70%;"> before draw the conclusion, </span><em id="S6.T12.8.25.2.1.1.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.25.2.1.1.5" class="ltx_text" style="font-size:70%;"> “</span><em id="S6.T12.8.25.2.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">Check whether the above solution is correct or not.</em><span id="S6.T12.8.25.2.1.1.7" class="ltx_text" style="font-size:70%;">”</span></span>
</span>
</td>
<td id="S6.T12.8.25.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T12.8.25.3.1" class="ltx_text" style="font-size:70%;">②</span></td>
</tr>
<tr id="S6.T12.8.26" class="ltx_tr">
<td id="S6.T12.8.26.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.26.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.26.1.1.1.1" class="ltx_text" style="font-size:70%;">O2. If the LLM can not well solve the task, you can </span><span id="S6.T12.8.26.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">seek help from external tools</span><span id="S6.T12.8.26.1.1.1.3" class="ltx_text" style="font-size:70%;"> by prompting the LLM to manipulate them. In this way, the tools should be encapsulated into callable APIs with detailed description about their functions, to better guide the LLM to utilize the tools.</span></span>
</span>
</td>
<td id="S6.T12.8.26.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.26.2.1" class="ltx_text" style="font-size:70%;">④</span></td>
</tr>
<tr id="S6.T12.8.27" class="ltx_tr">
<td id="S6.T12.8.27.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.27.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.27.1.1.1.1" class="ltx_text" style="font-size:70%;">O3. The prompt should be </span><span id="S6.T12.8.27.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">self-contained</span><span id="S6.T12.8.27.1.1.1.3" class="ltx_text" style="font-size:70%;">, and better not include pronouns (</span><em id="S6.T12.8.27.1.1.1.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.27.1.1.1.5" class="ltx_text" style="font-size:70%;"> it and they) in the context.</span></span>
</span>
</td>
<td id="S6.T12.8.27.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.27.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.28" class="ltx_tr">
<td id="S6.T12.8.28.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.28.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.28.1.1.1.1" class="ltx_text" style="font-size:70%;">O4. When using LLMs for </span><span id="S6.T12.8.28.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">comparing</span><span id="S6.T12.8.28.1.1.1.3" class="ltx_text" style="font-size:70%;"> two or more examples, the order affects the performance a lot.</span></span>
</span>
</td>
<td id="S6.T12.8.28.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.28.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.29" class="ltx_tr">
<td id="S6.T12.8.29.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.29.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.29.1.1.1.1" class="ltx_text" style="font-size:70%;">O5. Before the prompt, </span><span id="S6.T12.8.29.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">assigning a role for the LLM</span><span id="S6.T12.8.29.1.1.1.3" class="ltx_text" style="font-size:70%;"> is useful to help it better fulfill the following task instruction, </span><em id="S6.T12.8.29.1.1.1.4" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.29.1.1.1.5" class="ltx_text" style="font-size:70%;"> </span><em id="S6.T12.8.29.1.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">“I want you to act as a lawyer”</em><span id="S6.T12.8.29.1.1.1.7" class="ltx_text" style="font-size:70%;">.</span></span>
</span>
</td>
<td id="S6.T12.8.29.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.29.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.30" class="ltx_tr">
<td id="S6.T12.8.30.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.30.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.30.1.1.1.1" class="ltx_text" style="font-size:70%;">O6. OpenAI models can perform a task better in English than other languages. Thus, it is useful to first </span><span id="S6.T12.8.30.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">translate the input into English</span><span id="S6.T12.8.30.1.1.1.3" class="ltx_text" style="font-size:70%;"> and then feed it to LLMs.</span></span>
</span>
</td>
<td id="S6.T12.8.30.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.30.2.1" class="ltx_text" style="font-size:70%;">④</span></td>
</tr>
<tr id="S6.T12.8.31" class="ltx_tr">
<td id="S6.T12.8.31.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T12.8.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.31.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.31.1.1.1.1" class="ltx_text" style="font-size:70%;">O7. For multi-choice questions, it is useful to </span><span id="S6.T12.8.31.1.1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">constrain the output space</span><span id="S6.T12.8.31.1.1.1.3" class="ltx_text" style="font-size:70%;"> of the LLM. You can use a more detailed explanation or just imposing constraints on the logits.</span></span>
</span>
</td>
<td id="S6.T12.8.31.2" class="ltx_td ltx_align_center"><span id="S6.T12.8.31.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
<tr id="S6.T12.8.32" class="ltx_tr">
<td id="S6.T12.8.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S6.T12.8.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T12.8.32.1.1.1" class="ltx_p" style="width:325.2pt;"><span id="S6.T12.8.32.1.1.1.1" class="ltx_text" style="font-size:70%;">O8. For sorting based tasks (</span><em id="S6.T12.8.32.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.32.1.1.1.3" class="ltx_text" style="font-size:70%;"> recommendation), instead of directly outputting the complete text of each item after sorting, one can </span><span id="S6.T12.8.32.1.1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:70%;">assign indicators</span><span id="S6.T12.8.32.1.1.1.5" class="ltx_text" style="font-size:70%;"> (</span><em id="S6.T12.8.32.1.1.1.6" class="ltx_emph ltx_font_italic" style="font-size:70%;">e.g.,</em><span id="S6.T12.8.32.1.1.1.7" class="ltx_text" style="font-size:70%;"> </span><em id="S6.T12.8.32.1.1.1.8" class="ltx_emph ltx_font_italic" style="font-size:70%;">ABCD</em><span id="S6.T12.8.32.1.1.1.9" class="ltx_text" style="font-size:70%;">) to the unsorted items and instruct the LLMs to directly output the sorted indicators.</span></span>
</span>
</td>
<td id="S6.T12.8.32.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T12.8.32.2.1" class="ltx_text" style="font-size:70%;">①</span></td>
</tr>
</tbody></table>
</figure>
<div id="S6.SS1.SSS1.p13" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p13.1" class="ltx_p"><span id="S6.SS1.SSS1.p13.1.1" class="ltx_text ltx_font_bold">Empirical Analysis.</span>
We further conduct empirical studies to present the impact of prompts on task performance.
To conduct the experiments, we select a variety of tasks that span language generation, knowledge utilization, complex reasoning, structure data generation, and information retrieval.
For each task, we manually write a prompt that follows general guidelines introduced above. Note that the tested prompts may not be the optimal for these tasks, since they mainly aim to help readers understand how to write an effective prompt for solving different tasks.
Also, we add a simplified prompt as the comparison for most tasks.
Following the experimental settings in Section&nbsp;<a href="#S7.SS4" title="7.4 Empirical Evaluation ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a>, we examine the 3-shot performance of ChatGPT on complex reasoning tasks (Colored Objects and GSM8k), and zero-shot performance on other tasks.
We report the experimental results in Table&nbsp;<a href="#S7.T17" title="TABLE XVII ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVII</span></a>, where we also include the supervised performance in existing papers as reference.</p>
</div>
<div id="S6.SS1.SSS1.p14" class="ltx_para">
<p id="S6.SS1.SSS1.p14.1" class="ltx_p"><math id="S6.SS1.SSS1.p14.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS1.p14.1.m1.1a"><mo id="S6.SS1.SSS1.p14.1.m1.1.1" xref="S6.SS1.SSS1.p14.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p14.1.m1.1b"><ci id="S6.SS1.SSS1.p14.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p14.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p14.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS1.SSS1.p14.1.1" class="ltx_emph ltx_font_italic">Carefully designed prompts can boost the zero-shot or few-shot performance of ChatGPT.</em>
By comparing the results of using different prompts on the same task, we can see that using the carefully designed prompts can achieve better performance than the simpler ones.
In the carefully designed prompts, we provide a more clearly expressed task description (<em id="S6.SS1.SSS1.p14.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> WMT and WikiFact), or use a model-friendly format (<em id="S6.SS1.SSS1.p14.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> GSM8k and OBQA).
For example, for WikiFact task, the prompt with a more detailed task description leads to a performance increase from 29.25 to 31.21.</p>
</div>
<div id="S6.SS1.SSS1.p15" class="ltx_para">
<p id="S6.SS1.SSS1.p15.1" class="ltx_p"><math id="S6.SS1.SSS1.p15.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS1.p15.1.m1.1a"><mo id="S6.SS1.SSS1.p15.1.m1.1.1" xref="S6.SS1.SSS1.p15.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p15.1.m1.1b"><ci id="S6.SS1.SSS1.p15.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p15.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p15.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS1.SSS1.p15.1.1" class="ltx_emph ltx_font_italic">More complex tasks can benefit more from careful prompt engineering on ChatGPT.</em>
In the WikiFact and Colored Objects tasks, the designed prompts have greatly improved the performance of ChatGPT, <em id="S6.SS1.SSS1.p15.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> from 23.61 to 28.47 on WikiFact and from 53.20 to 66.75 on Colored Objects.
It indicates the necessity of prompt engineering for LLMs to perform well on complex tasks, since these tasks typically have specific output formats or require background knowledge.
Our example prompts provide more detailed task description (<em id="S6.SS1.SSS1.p15.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> output format and task goal), which can help ChatGPT better understand the complex task requirement for fulfilling it.</p>
</div>
<div id="S6.SS1.SSS1.p16" class="ltx_para">
<p id="S6.SS1.SSS1.p16.1" class="ltx_p"><math id="S6.SS1.SSS1.p16.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS1.p16.1.m1.1a"><mo id="S6.SS1.SSS1.p16.1.m1.1.1" xref="S6.SS1.SSS1.p16.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p16.1.m1.1b"><ci id="S6.SS1.SSS1.p16.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p16.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p16.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS1.SSS1.p16.1.1" class="ltx_emph ltx_font_italic">For mathematical reasoning tasks, it is more effective to design specific prompts based on the format of programming language.</em>
For GSM8k, the designed prompt employs code-formatted few-shot demonstrations to convert this mathematical reasoning task into code generation task, which can leverage the strong code synthesis ability of ChatGPT for solving mathematical problems.
Further, with the help of an external program executor, we are able to obtain more precise results instead of using LLMs for arithmetic operation.
As we can see, the performance is boosted from 78.47 to 79.30 on GSM8k, indicating the usefulness of programming language in mathematical reasoning tasks.</p>
</div>
<div id="S6.SS1.SSS1.p17" class="ltx_para">
<p id="S6.SS1.SSS1.p17.1" class="ltx_p"><math id="S6.SS1.SSS1.p17.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS1.p17.1.m1.1a"><mo id="S6.SS1.SSS1.p17.1.m1.1.1" xref="S6.SS1.SSS1.p17.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p17.1.m1.1b"><ci id="S6.SS1.SSS1.p17.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p17.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p17.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS1.SSS1.p17.1.1" class="ltx_emph ltx_font_italic">In knowledge utilization and complex reasoning tasks, ChatGPT with proper prompts achieves comparable performance or even outperforms the supervised baselines methods.</em>
In knowledge utilization and complex reasoning tasks, ChatGPT with proper zero-shot or few-shot prompts can achieve comparable performance or even outperform the supervised methods, <em id="S6.SS1.SSS1.p17.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> 31.21 (ChatGPT) <em id="S6.SS1.SSS1.p17.1.3" class="ltx_emph ltx_font_italic">v.s.</em> 34.20 (supervised baseline) on WikiFact.
Despite that, ChatGPT still performs worse than supervised baseline models on some specific tasks (<em id="S6.SS1.SSS1.p17.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> ARC and WikiFact), since these supervised models have been specially optimized with task-specific data.</p>
</div>
<div id="S6.SS1.SSS1.p18" class="ltx_para">
<p id="S6.SS1.SSS1.p18.1" class="ltx_p"><math id="S6.SS1.SSS1.p18.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS1.p18.1.m1.1a"><mo id="S6.SS1.SSS1.p18.1.m1.1.1" xref="S6.SS1.SSS1.p18.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p18.1.m1.1b"><ci id="S6.SS1.SSS1.p18.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p18.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p18.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS1.SSS1.p18.1.1" class="ltx_emph ltx_font_italic">Through suitable prompt engineering, LLMs can handle some non-traditional NLP tasks.</em>
With the help of specific prompts, ChatGPT can also accomplish non-traditional NLP tasks, <em id="S6.SS1.SSS1.p18.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> the general recommendation and conversational recommendation.
A key point is that these tasks can be well expressed or described in natural language. 
However, the performance of ChatGPT is still far from the referenced performance in these tasks, as LLMs cannot directly fit these tasks, which require specific domain knowledge and task adaptation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref">357</a>, <a href="#bib.bib462" title="" class="ltx_ref">462</a>]</cite>.</p>
</div>
<figure id="S6.T13" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE XIII: </span>Example instructions collected from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib454" title="" class="ltx_ref">454</a>, <a href="#bib.bib463" title="" class="ltx_ref">463</a>]</cite>. The <span id="S6.T13.38.1" class="ltx_text" style="background-color:#B0E2FF;">blue</span> text denotes the task description, the <span id="S6.T13.39.2" class="ltx_text" style="background-color:#FFCCCC;">red</span> text denotes the contextual information, the <span id="S6.T13.40.3" class="ltx_text" style="background-color:#CDE6C7;">green</span> text denotes the demonstrations, and the <span id="S6.T13.41.4" class="ltx_text" style="background-color:#FFF68F;">gold</span> text denotes the prompt style.</figcaption>
<table id="S6.T13.24" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T13.24.25" class="ltx_tr" style="background-color:#B0E2FF;">
<td id="S6.T13.24.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S6.T13.24.25.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#B0E2FF;">
<span id="S6.T13.24.25.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.25.1.1.1.1" class="ltx_text" style="font-size:80%;">Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write “I could not find an answer.”</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.26" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.26.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T13.24.26.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.26.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.26.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Articles:</span><span id="S6.T13.24.26.1.1.1.2" class="ltx_text" style="font-size:80%;"> “““Joao Moutinho is a Portuguese footballer who last played as a central midfielder for Premier League club Wolverhampton Wanderers and the Portugal national team.”””</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.27" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T13.24.27.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.27.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.27.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S6.T13.24.27.1.1.1.2" class="ltx_text" style="font-size:80%;"> Is the following sentence plausible?
’Joao Moutinho was out at third.’</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.28" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.28.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.28.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.28.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.28.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Answer:</span><span id="S6.T13.24.28.1.1.1.2" class="ltx_text" style="font-size:80%;"> </span><span id="S6.T13.24.28.1.1.1.3" class="ltx_text" style="font-size:80%;background-color:#EEE685;">Let’s think step by step. Joao Moutinho is a soccer player. Being out at third is part of baseball, not soccer.</span><span id="S6.T13.24.28.1.1.1.4" class="ltx_text" style="font-size:80%;"> So the answer is No.</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.29" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.29.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.29.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.29.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.29.1.1.1.1" class="ltx_text" style="font-size:80%;">…</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.2.2" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.2.2.2.2" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.2.2.2.2.2" class="ltx_p" style="width:433.6pt;"><math id="S6.T13.1.1.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.1.1.1.1.1.m1.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.1.1.1.1.1.m1.1.1" xref="S6.T13.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.1.1.1.1.1.m1.1b"><lt id="S6.T13.1.1.1.1.1.m1.1.1.cmml" xref="S6.T13.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.1.1.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.2.2.2.2.2.1" class="ltx_text" style="font-size:80%;">Demonstrations</span><math id="S6.T13.2.2.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.2.2.2.2.2.m2.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.2.2.2.2.2.m2.1.1" xref="S6.T13.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.2.2.2.2.2.m2.1b"><gt id="S6.T13.2.2.2.2.2.m2.1.1.cmml" xref="S6.T13.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.2.2.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.4.4" class="ltx_tr">
<td id="S6.T13.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.4.4.2.2" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.4.4.2.2.2" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.4.4.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Articles:</span><span id="S6.T13.4.4.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S6.T13.3.3.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.3.3.1.1.1.m1.1a"><mo mathsize="80%" id="S6.T13.3.3.1.1.1.m1.1.1" xref="S6.T13.3.3.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.3.3.1.1.1.m1.1b"><lt id="S6.T13.3.3.1.1.1.m1.1.1.cmml" xref="S6.T13.3.3.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.3.3.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.4.4.2.2.2.3" class="ltx_text" style="font-size:80%;">insert articles, each delimited by triple quotes</span><math id="S6.T13.4.4.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.4.4.2.2.2.m2.1a"><mo mathsize="80%" id="S6.T13.4.4.2.2.2.m2.1.1" xref="S6.T13.4.4.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.4.4.2.2.2.m2.1b"><gt id="S6.T13.4.4.2.2.2.m2.1.1.cmml" xref="S6.T13.4.4.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.4.4.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.6.6" class="ltx_tr">
<td id="S6.T13.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.6.6.2.2" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.6.6.2.2.2" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.6.6.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S6.T13.6.6.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S6.T13.5.5.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.5.5.1.1.1.m1.1a"><mo mathsize="80%" id="S6.T13.5.5.1.1.1.m1.1.1" xref="S6.T13.5.5.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.5.5.1.1.1.m1.1b"><lt id="S6.T13.5.5.1.1.1.m1.1.1.cmml" xref="S6.T13.5.5.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.5.5.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.6.6.2.2.2.3" class="ltx_text" style="font-size:80%;">insert question</span><math id="S6.T13.6.6.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.6.6.2.2.2.m2.1a"><mo mathsize="80%" id="S6.T13.6.6.2.2.2.m2.1.1" xref="S6.T13.6.6.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.6.6.2.2.2.m2.1b"><gt id="S6.T13.6.6.2.2.2.m2.1.1.cmml" xref="S6.T13.6.6.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.6.6.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.30" class="ltx_tr">
<td id="S6.T13.24.30.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.24.30.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.30.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Answer:</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.31" class="ltx_tr" style="background-color:#B0E2FF;">
<td id="S6.T13.24.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S6.T13.24.31.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#B0E2FF;">
<span id="S6.T13.24.31.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.31.1.1.1.1" class="ltx_text" style="font-size:80%;">Prepare a meta-review by answering the following questions from the reviewer comments (provided after the questions).</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.32" class="ltx_tr" style="background-color:#FFF68F;">
<td id="S6.T13.24.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T13.24.32.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFF68F;">
<span id="S6.T13.24.32.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.32.1.1.1.1" class="ltx_text" style="font-size:80%;">1. Based on the reviewer’s comments, what are the core contributions made by this manuscript?</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.33" class="ltx_tr" style="background-color:#FFF68F;">
<td id="S6.T13.24.33.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.33.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFF68F;">
<span id="S6.T13.24.33.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.33.1.1.1.1" class="ltx_text" style="font-size:80%;">2. What are the common strengths of this work, as mentioned by multiple reviewers?</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.34" class="ltx_tr" style="background-color:#FFF68F;">
<td id="S6.T13.24.34.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.34.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFF68F;">
<span id="S6.T13.24.34.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.34.1.1.1.1" class="ltx_text" style="font-size:80%;">3. What are the common weaknesses of this work, as highlighted by multiple reviewers?</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.35" class="ltx_tr" style="background-color:#FFF68F;">
<td id="S6.T13.24.35.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.35.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFF68F;">
<span id="S6.T13.24.35.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.35.1.1.1.1" class="ltx_text" style="font-size:80%;">4. What suggestions would you provide for improving this paper?</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.36" class="ltx_tr" style="background-color:#FFF68F;">
<td id="S6.T13.24.36.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.36.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFF68F;">
<span id="S6.T13.24.36.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.36.1.1.1.1" class="ltx_text" style="font-size:80%;">5. What are the missing references mentioned by the individual reviews?</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.11.11" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T13.11.11.5.5" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.11.11.5.5.5" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.11.11.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">The review texts are below:</span><span id="S6.T13.11.11.5.5.5.2" class="ltx_text" style="font-size:80%;"> </span><math id="S6.T13.7.7.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.7.7.1.1.1.m1.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.7.7.1.1.1.m1.1.1" xref="S6.T13.7.7.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.7.7.1.1.1.m1.1b"><lt id="S6.T13.7.7.1.1.1.m1.1.1.cmml" xref="S6.T13.7.7.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.7.7.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.11.11.5.5.5.3" class="ltx_text" style="font-size:80%;">insert three comments </span><math id="S6.T13.8.8.2.2.2.m2.1" class="ltx_Math" alttext="R_{1}" display="inline"><semantics id="S6.T13.8.8.2.2.2.m2.1a"><msub id="S6.T13.8.8.2.2.2.m2.1.1" xref="S6.T13.8.8.2.2.2.m2.1.1.cmml"><mi mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.8.8.2.2.2.m2.1.1.2" xref="S6.T13.8.8.2.2.2.m2.1.1.2.cmml">R</mi><mn mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.8.8.2.2.2.m2.1.1.3" xref="S6.T13.8.8.2.2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T13.8.8.2.2.2.m2.1b"><apply id="S6.T13.8.8.2.2.2.m2.1.1.cmml" xref="S6.T13.8.8.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.T13.8.8.2.2.2.m2.1.1.1.cmml" xref="S6.T13.8.8.2.2.2.m2.1.1">subscript</csymbol><ci id="S6.T13.8.8.2.2.2.m2.1.1.2.cmml" xref="S6.T13.8.8.2.2.2.m2.1.1.2">𝑅</ci><cn type="integer" id="S6.T13.8.8.2.2.2.m2.1.1.3.cmml" xref="S6.T13.8.8.2.2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.8.8.2.2.2.m2.1c">R_{1}</annotation></semantics></math><span id="S6.T13.11.11.5.5.5.4" class="ltx_text" style="font-size:80%;">, </span><math id="S6.T13.9.9.3.3.3.m3.1" class="ltx_Math" alttext="R_{2}" display="inline"><semantics id="S6.T13.9.9.3.3.3.m3.1a"><msub id="S6.T13.9.9.3.3.3.m3.1.1" xref="S6.T13.9.9.3.3.3.m3.1.1.cmml"><mi mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.9.9.3.3.3.m3.1.1.2" xref="S6.T13.9.9.3.3.3.m3.1.1.2.cmml">R</mi><mn mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.9.9.3.3.3.m3.1.1.3" xref="S6.T13.9.9.3.3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T13.9.9.3.3.3.m3.1b"><apply id="S6.T13.9.9.3.3.3.m3.1.1.cmml" xref="S6.T13.9.9.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S6.T13.9.9.3.3.3.m3.1.1.1.cmml" xref="S6.T13.9.9.3.3.3.m3.1.1">subscript</csymbol><ci id="S6.T13.9.9.3.3.3.m3.1.1.2.cmml" xref="S6.T13.9.9.3.3.3.m3.1.1.2">𝑅</ci><cn type="integer" id="S6.T13.9.9.3.3.3.m3.1.1.3.cmml" xref="S6.T13.9.9.3.3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.9.9.3.3.3.m3.1c">R_{2}</annotation></semantics></math><span id="S6.T13.11.11.5.5.5.5" class="ltx_text" style="font-size:80%;">, </span><math id="S6.T13.10.10.4.4.4.m4.1" class="ltx_Math" alttext="R_{3}" display="inline"><semantics id="S6.T13.10.10.4.4.4.m4.1a"><msub id="S6.T13.10.10.4.4.4.m4.1.1" xref="S6.T13.10.10.4.4.4.m4.1.1.cmml"><mi mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.10.10.4.4.4.m4.1.1.2" xref="S6.T13.10.10.4.4.4.m4.1.1.2.cmml">R</mi><mn mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.10.10.4.4.4.m4.1.1.3" xref="S6.T13.10.10.4.4.4.m4.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T13.10.10.4.4.4.m4.1b"><apply id="S6.T13.10.10.4.4.4.m4.1.1.cmml" xref="S6.T13.10.10.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S6.T13.10.10.4.4.4.m4.1.1.1.cmml" xref="S6.T13.10.10.4.4.4.m4.1.1">subscript</csymbol><ci id="S6.T13.10.10.4.4.4.m4.1.1.2.cmml" xref="S6.T13.10.10.4.4.4.m4.1.1.2">𝑅</ci><cn type="integer" id="S6.T13.10.10.4.4.4.m4.1.1.3.cmml" xref="S6.T13.10.10.4.4.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.10.10.4.4.4.m4.1c">R_{3}</annotation></semantics></math><span id="S6.T13.11.11.5.5.5.6" class="ltx_text" style="font-size:80%;"> from the reviewers</span><math id="S6.T13.11.11.5.5.5.m5.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.11.11.5.5.5.m5.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.11.11.5.5.5.m5.1.1" xref="S6.T13.11.11.5.5.5.m5.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.11.11.5.5.5.m5.1b"><gt id="S6.T13.11.11.5.5.5.m5.1.1.cmml" xref="S6.T13.11.11.5.5.5.m5.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.11.11.5.5.5.m5.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.13.13" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.13.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.13.13.2.2" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.13.13.2.2.2" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.13.13.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Meta-review:</span><span id="S6.T13.13.13.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S6.T13.12.12.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.12.12.1.1.1.m1.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.12.12.1.1.1.m1.1.1" xref="S6.T13.12.12.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.12.12.1.1.1.m1.1b"><lt id="S6.T13.12.12.1.1.1.m1.1.1.cmml" xref="S6.T13.12.12.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.12.12.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.13.13.2.2.2.3" class="ltx_text" style="font-size:80%;">insert meta-review</span><math id="S6.T13.13.13.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.13.13.2.2.2.m2.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.13.13.2.2.2.m2.1.1" xref="S6.T13.13.13.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.13.13.2.2.2.m2.1b"><gt id="S6.T13.13.13.2.2.2.m2.1.1.cmml" xref="S6.T13.13.13.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.13.13.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.37" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.37.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.37.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.37.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.37.1.1.1.1" class="ltx_text" style="font-size:80%;">…</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.15.15" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.15.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.15.15.2.2" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.15.15.2.2.2" class="ltx_p" style="width:433.6pt;"><math id="S6.T13.14.14.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.14.14.1.1.1.m1.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.14.14.1.1.1.m1.1.1" xref="S6.T13.14.14.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.14.14.1.1.1.m1.1b"><lt id="S6.T13.14.14.1.1.1.m1.1.1.cmml" xref="S6.T13.14.14.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.14.14.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.15.15.2.2.2.1" class="ltx_text" style="font-size:80%;">Demonstrations</span><math id="S6.T13.15.15.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.15.15.2.2.2.m2.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.15.15.2.2.2.m2.1.1" xref="S6.T13.15.15.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.15.15.2.2.2.m2.1b"><gt id="S6.T13.15.15.2.2.2.m2.1.1.cmml" xref="S6.T13.15.15.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.15.15.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.38" class="ltx_tr">
<td id="S6.T13.24.38.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.38.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.24.38.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.38.1.1.1.1" class="ltx_text" style="font-size:80%;">Provide justification for your response in detail by explaining why you made the choices you actually made. A good output should be coherent, highlight major strengths/issues mentioned by multiple reviewers, be less than 400 words in length, and finally, the response should be in English only.</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.20.20" class="ltx_tr">
<td id="S6.T13.20.20.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.20.20.5.5" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.20.20.5.5.5" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.20.20.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">The review texts are below:</span><span id="S6.T13.20.20.5.5.5.2" class="ltx_text" style="font-size:80%;"> </span><math id="S6.T13.16.16.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.16.16.1.1.1.m1.1a"><mo mathsize="80%" id="S6.T13.16.16.1.1.1.m1.1.1" xref="S6.T13.16.16.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.16.16.1.1.1.m1.1b"><lt id="S6.T13.16.16.1.1.1.m1.1.1.cmml" xref="S6.T13.16.16.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.16.16.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.20.20.5.5.5.3" class="ltx_text" style="font-size:80%;">insert three comments </span><math id="S6.T13.17.17.2.2.2.m2.1" class="ltx_Math" alttext="R_{1}" display="inline"><semantics id="S6.T13.17.17.2.2.2.m2.1a"><msub id="S6.T13.17.17.2.2.2.m2.1.1" xref="S6.T13.17.17.2.2.2.m2.1.1.cmml"><mi mathsize="80%" id="S6.T13.17.17.2.2.2.m2.1.1.2" xref="S6.T13.17.17.2.2.2.m2.1.1.2.cmml">R</mi><mn mathsize="80%" id="S6.T13.17.17.2.2.2.m2.1.1.3" xref="S6.T13.17.17.2.2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T13.17.17.2.2.2.m2.1b"><apply id="S6.T13.17.17.2.2.2.m2.1.1.cmml" xref="S6.T13.17.17.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.T13.17.17.2.2.2.m2.1.1.1.cmml" xref="S6.T13.17.17.2.2.2.m2.1.1">subscript</csymbol><ci id="S6.T13.17.17.2.2.2.m2.1.1.2.cmml" xref="S6.T13.17.17.2.2.2.m2.1.1.2">𝑅</ci><cn type="integer" id="S6.T13.17.17.2.2.2.m2.1.1.3.cmml" xref="S6.T13.17.17.2.2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.17.17.2.2.2.m2.1c">R_{1}</annotation></semantics></math><span id="S6.T13.20.20.5.5.5.4" class="ltx_text" style="font-size:80%;">, </span><math id="S6.T13.18.18.3.3.3.m3.1" class="ltx_Math" alttext="R_{2}" display="inline"><semantics id="S6.T13.18.18.3.3.3.m3.1a"><msub id="S6.T13.18.18.3.3.3.m3.1.1" xref="S6.T13.18.18.3.3.3.m3.1.1.cmml"><mi mathsize="80%" id="S6.T13.18.18.3.3.3.m3.1.1.2" xref="S6.T13.18.18.3.3.3.m3.1.1.2.cmml">R</mi><mn mathsize="80%" id="S6.T13.18.18.3.3.3.m3.1.1.3" xref="S6.T13.18.18.3.3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T13.18.18.3.3.3.m3.1b"><apply id="S6.T13.18.18.3.3.3.m3.1.1.cmml" xref="S6.T13.18.18.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S6.T13.18.18.3.3.3.m3.1.1.1.cmml" xref="S6.T13.18.18.3.3.3.m3.1.1">subscript</csymbol><ci id="S6.T13.18.18.3.3.3.m3.1.1.2.cmml" xref="S6.T13.18.18.3.3.3.m3.1.1.2">𝑅</ci><cn type="integer" id="S6.T13.18.18.3.3.3.m3.1.1.3.cmml" xref="S6.T13.18.18.3.3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.18.18.3.3.3.m3.1c">R_{2}</annotation></semantics></math><span id="S6.T13.20.20.5.5.5.5" class="ltx_text" style="font-size:80%;">, </span><math id="S6.T13.19.19.4.4.4.m4.1" class="ltx_Math" alttext="R_{3}" display="inline"><semantics id="S6.T13.19.19.4.4.4.m4.1a"><msub id="S6.T13.19.19.4.4.4.m4.1.1" xref="S6.T13.19.19.4.4.4.m4.1.1.cmml"><mi mathsize="80%" id="S6.T13.19.19.4.4.4.m4.1.1.2" xref="S6.T13.19.19.4.4.4.m4.1.1.2.cmml">R</mi><mn mathsize="80%" id="S6.T13.19.19.4.4.4.m4.1.1.3" xref="S6.T13.19.19.4.4.4.m4.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T13.19.19.4.4.4.m4.1b"><apply id="S6.T13.19.19.4.4.4.m4.1.1.cmml" xref="S6.T13.19.19.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S6.T13.19.19.4.4.4.m4.1.1.1.cmml" xref="S6.T13.19.19.4.4.4.m4.1.1">subscript</csymbol><ci id="S6.T13.19.19.4.4.4.m4.1.1.2.cmml" xref="S6.T13.19.19.4.4.4.m4.1.1.2">𝑅</ci><cn type="integer" id="S6.T13.19.19.4.4.4.m4.1.1.3.cmml" xref="S6.T13.19.19.4.4.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.19.19.4.4.4.m4.1c">R_{3}</annotation></semantics></math><span id="S6.T13.20.20.5.5.5.6" class="ltx_text" style="font-size:80%;"> from the reviewers</span><math id="S6.T13.20.20.5.5.5.m5.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.20.20.5.5.5.m5.1a"><mo mathsize="80%" id="S6.T13.20.20.5.5.5.m5.1.1" xref="S6.T13.20.20.5.5.5.m5.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.20.20.5.5.5.m5.1b"><gt id="S6.T13.20.20.5.5.5.m5.1.1.cmml" xref="S6.T13.20.20.5.5.5.m5.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.20.20.5.5.5.m5.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.39" class="ltx_tr">
<td id="S6.T13.24.39.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.39.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.24.39.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.39.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Meta-review:</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.40" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.40.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S6.T13.24.40.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.40.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.40.1.1.1.1" class="ltx_text" style="font-size:80%;">CREATE TABLE Highschooler (</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.41" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.41.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.41.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.41.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.41.1.1.1.1" class="ltx_text" style="font-size:80%;">ID int primary key,</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.42" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.42.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.42.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.42.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.42.1.1.1.1" class="ltx_text" style="font-size:80%;">name text,</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.43" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.43.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.43.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.43.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.43.1.1.1.1" class="ltx_text" style="font-size:80%;">grade int</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.44" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.44.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.44.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.44.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.44.1.1.1.1" class="ltx_text" style="font-size:80%;">);</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.45" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.45.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.45.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.45.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.45.1.1.1.1" class="ltx_text" style="font-size:80%;">/*</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.46" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.46.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.46.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.46.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.46.1.1.1.1" class="ltx_text" style="font-size:80%;">3 example rows:</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.47" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.47.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.47.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.47.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.47.1.1.1.1" class="ltx_text" style="font-size:80%;">SELECT * FROM Highschooler LIMIT 3;</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.48" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.48.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.48.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.48.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.48.1.1.1.1" class="ltx_text" style="font-size:80%;">ID  name  grade</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.49" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.49.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.49.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.49.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.49.1.1.1.1" class="ltx_text" style="font-size:80%;">1234  Janie  8</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.50" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.50.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.50.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.50.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.50.1.1.1.1" class="ltx_text" style="font-size:80%;">5678  Mary  8</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.51" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.51.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.51.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.51.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.51.1.1.1.1" class="ltx_text" style="font-size:80%;">9012  Mike  9</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.52" class="ltx_tr" style="background-color:#FFCCCC;">
<td id="S6.T13.24.52.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.52.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#FFCCCC;">
<span id="S6.T13.24.52.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.52.1.1.1.1" class="ltx_text" style="font-size:80%;">*/</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.53" class="ltx_tr" style="background-color:#B0E2FF;">
<td id="S6.T13.24.53.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T13.24.53.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#B0E2FF;">
<span id="S6.T13.24.53.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.53.1.1.1.1" class="ltx_text" style="font-size:80%;">Using valid SQLite, answer the following questions for the tables provided above.</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.54" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.54.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T13.24.54.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.54.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.54.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S6.T13.24.54.1.1.1.2" class="ltx_text" style="font-size:80%;"> What is Kyle’s id?</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.55" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.55.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.55.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.55.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.55.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SQL:</span><span id="S6.T13.24.55.1.1.1.2" class="ltx_text" style="font-size:80%;"> SELECT ID FROM Highschooler WHERE name=“Kyle”;</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.56" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.24.56.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.56.1.1" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.24.56.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.56.1.1.1.1" class="ltx_text" style="font-size:80%;">…</span></span>
</span>
</td>
</tr>
<tr id="S6.T13.22.22" class="ltx_tr" style="background-color:#CDE6C7;">
<td id="S6.T13.22.22.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.22.22.2.2" class="ltx_inline-block ltx_align_top" style="background-color:#CDE6C7;">
<span id="S6.T13.22.22.2.2.2" class="ltx_p" style="width:433.6pt;"><math id="S6.T13.21.21.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.21.21.1.1.1.m1.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.21.21.1.1.1.m1.1.1" xref="S6.T13.21.21.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.21.21.1.1.1.m1.1b"><lt id="S6.T13.21.21.1.1.1.m1.1.1.cmml" xref="S6.T13.21.21.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.21.21.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.22.22.2.2.2.1" class="ltx_text" style="font-size:80%;">Demonstrations</span><math id="S6.T13.22.22.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.22.22.2.2.2.m2.1a"><mo mathbackground="#CDE6C7" mathsize="80%" id="S6.T13.22.22.2.2.2.m2.1.1" xref="S6.T13.22.22.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.22.22.2.2.2.m2.1b"><gt id="S6.T13.22.22.2.2.2.m2.1.1.cmml" xref="S6.T13.22.22.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.22.22.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.24" class="ltx_tr">
<td id="S6.T13.24.24.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T13.24.24.2.2" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.24.24.2.2.2" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.24.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S6.T13.24.24.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S6.T13.23.23.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S6.T13.23.23.1.1.1.m1.1a"><mo mathsize="80%" id="S6.T13.23.23.1.1.1.m1.1.1" xref="S6.T13.23.23.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.23.23.1.1.1.m1.1b"><lt id="S6.T13.23.23.1.1.1.m1.1.1.cmml" xref="S6.T13.23.23.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.23.23.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S6.T13.24.24.2.2.2.3" class="ltx_text" style="font-size:80%;">insert question</span><math id="S6.T13.24.24.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T13.24.24.2.2.2.m2.1a"><mo mathsize="80%" id="S6.T13.24.24.2.2.2.m2.1.1" xref="S6.T13.24.24.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T13.24.24.2.2.2.m2.1b"><gt id="S6.T13.24.24.2.2.2.m2.1.1.cmml" xref="S6.T13.24.24.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T13.24.24.2.2.2.m2.1c">&gt;</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S6.T13.24.57" class="ltx_tr">
<td id="S6.T13.24.57.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S6.T13.24.57.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T13.24.57.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="S6.T13.24.57.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SQL:</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Prompt Optimization</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">Although manually creating task prompts is more intuitive, it is time consuming and, more importantly, models are highly sensitive to the crafted prompts—improper prompts will lead to low task performance (as shown in Table&nbsp;<a href="#S7.T17" title="TABLE XVII ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVII</span></a>). Therefore, a large body of studies propose automatic optimization approaches for discrete prompts and continuous prompts to achieve the optimal performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib405" title="" class="ltx_ref">405</a>, <a href="#bib.bib396" title="" class="ltx_ref">396</a>]</cite>. In this part, we will detail these studies from two perspectives, <em id="S6.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> discrete prompts and continuous prompts.</p>
</div>
<div id="S6.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS2.p2.1" class="ltx_p"><span id="S6.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Discrete Prompt Optimization.</span> Discrete prompt is typically composed of a sequence of natural language tokens. Despite that the form is simple and flexible, optimizing prompts in discrete space is a challenging problem due to the combinatorial huge search space. To automatically search effective prompts for downstream tasks, existing studies propose a wide spectrum of discrete prompt approaches, which are detailed as follows.</p>
</div>
<div id="S6.SS1.SSS2.p3" class="ltx_para">
<p id="S6.SS1.SSS2.p3.1" class="ltx_p"><math id="S6.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS2.p3.1.m1.1a"><mo id="S6.SS1.SSS2.p3.1.m1.1.1" xref="S6.SS1.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p3.1.m1.1b"><ci id="S6.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_italic">Gradient-based approaches.</span> This kind of approaches aims to optimize the prompt search process by maximizing the output likelihood via gradient update&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib405" title="" class="ltx_ref">405</a>, <a href="#bib.bib464" title="" class="ltx_ref">464</a>, <a href="#bib.bib465" title="" class="ltx_ref">465</a>, <a href="#bib.bib466" title="" class="ltx_ref">466</a>]</cite>.
As a representative work, Auto-Prompt&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib405" title="" class="ltx_ref">405</a>]</cite> proposes a gradient-guided method to greedily searches the optimal token for each position of the prompt, leveraging the gradient approximated by the change in the log-likelihood when replacing a prompt token with another candidate token from vocabulary. However, such a search process can be extremely expensive since it needs to evaluate each candidate token for each position of the prompt, leading to a number of additional forward passes. Therefore, improved gradient method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib464" title="" class="ltx_ref">464</a>]</cite> has been proposed by transforming discrete tokens into continuous embeddings and computing the gradient on continuous space during optimization.</p>
</div>
<div id="S6.SS1.SSS2.p4" class="ltx_para">
<p id="S6.SS1.SSS2.p4.1" class="ltx_p"><math id="S6.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS2.p4.1.m1.1a"><mo id="S6.SS1.SSS2.p4.1.m1.1.1" xref="S6.SS1.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p4.1.m1.1b"><ci id="S6.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_italic">RL-based approaches.</span>
Since discrete prompts are difficult to be learned through gradient back-propagation, a number of studies propose to formulate the discrete prompt optimization as a reinforcement learning (RL) problem and leverage RL algorithms for optimization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib467" title="" class="ltx_ref">467</a>, <a href="#bib.bib468" title="" class="ltx_ref">468</a>]</cite>. For example, RLPrompt&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib467" title="" class="ltx_ref">467</a>]</cite> trains a policy network to generate desired prompts with multiple reward functions. In this approach, several effective reward stabilization strategies are also proposed to enhance the RL training efficiency. Compared to previous work that requires sufficient data for training, TEMPERA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib468" title="" class="ltx_ref">468</a>]</cite> proposes to directly generate prompts at test time by utilizing a pre-trained RL agent to sequentially edit different parts of an manually-written initial prompt.</p>
</div>
<div id="S6.SS1.SSS2.p5" class="ltx_para">
<p id="S6.SS1.SSS2.p5.1" class="ltx_p"><math id="S6.SS1.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS2.p5.1.m1.1a"><mo id="S6.SS1.SSS2.p5.1.m1.1.1" xref="S6.SS1.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p5.1.m1.1b"><ci id="S6.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_italic">Edit-based approaches.</span>
For the above methods, gradient-based and RL-based tuning can be extremely computationally demanding for ever larger models, and may not be feasible for API-based model calls (<em id="S6.SS1.SSS2.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> ChatGPT). Therefore, another line of work aims to directly edit existing prompts based on the task performance. Specifically, GPS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib469" title="" class="ltx_ref">469</a>]</cite> borrows an idea from the genetic algorithm and proposes a genetic prompt search method that utilizes a language model (<em id="S6.SS1.SSS2.p5.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> T5) to edit prompts by taking the cloze task form. In addition to model based edit methods, human-defined operations can be also employed for prompt editing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib470" title="" class="ltx_ref">470</a>]</cite>, including delete, swap, paraphrase, and addition. Based on these operations, they iteratively edit the prompts and greedily search for the best prompt guided by the model performance on a small pool of examples.</p>
</div>
<div id="S6.SS1.SSS2.p6" class="ltx_para">
<p id="S6.SS1.SSS2.p6.1" class="ltx_p"><math id="S6.SS1.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS2.p6.1.m1.1a"><mo id="S6.SS1.SSS2.p6.1.m1.1.1" xref="S6.SS1.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p6.1.m1.1b"><ci id="S6.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.SS1.SSS2.p6.1.1" class="ltx_text ltx_font_italic">LLM-based approaches.</span> Due to the exceptional capacities of LLMs, an increasing number of studies directly leverage LLMs as prompt generator&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib471" title="" class="ltx_ref">471</a>, <a href="#bib.bib472" title="" class="ltx_ref">472</a>, <a href="#bib.bib473" title="" class="ltx_ref">473</a>]</cite>. Specifically, APE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib471" title="" class="ltx_ref">471</a>]</cite> utilizes an LLM to generate initial prompts, then selects the best prompt with the highest accuracy, and finally improves the best candidate through an iterative Monte Carlo search method. Similarly, APO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib472" title="" class="ltx_ref">472</a>]</cite> instructs the LLM to generate text feedback on how to refine an old prompt into new improved prompts. However, their search in the prompt space might be inefficient without fully considering the whole refinement trace of previous prompts, thus potentially leading to sub-optimal results. Therefore, another study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib473" title="" class="ltx_ref">473</a>]</cite> incorporates the previous prompts with their scores to instruct LLMs for progressively generating better new prompts. However, these approaches still struggle in exploring the vast space of effective prompts. Inspired by human-like trial-and-error, prompt optimization is further formulated as a strategic planning problem&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib474" title="" class="ltx_ref">474</a>]</cite> and uses Monte Carlo tree search to navigate the vast prompt space.</p>
</div>
<div id="S6.SS1.SSS2.p7" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS2.p7.1" class="ltx_p"><span id="S6.SS1.SSS2.p7.1.1" class="ltx_text ltx_font_bold">Continuous Prompt Optimization.</span> Different from discrete prompts, continuous prompts consist of a set of continuous embeddings, which can be directly optimized through the gradient update based on the loss of downstream tasks. Note that continuous prompt optimization has been mainly studied in PLMs, but draws limited attention in era of LLMs due to their massive magnitudes of parameters. We include the discussion of this part for content completeness. In prior work, most studies typically rely on supervised learning to train continuous prompts based on task data. Furthermore, in data-scarce scenarios, transfer learning methods can be employed to alleviate the lack of labeled data on target tasks. These two approaches are detailed below.</p>
</div>
<div id="S6.SS1.SSS2.p8" class="ltx_para">
<p id="S6.SS1.SSS2.p8.1" class="ltx_p"><math id="S6.SS1.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS2.p8.1.m1.1a"><mo id="S6.SS1.SSS2.p8.1.m1.1.1" xref="S6.SS1.SSS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p8.1.m1.1b"><ci id="S6.SS1.SSS2.p8.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.SS1.SSS2.p8.1.1" class="ltx_text ltx_font_italic">Prompt learning with sufficient data.</span> In this approach, most existing methods regard continuous prompts as trainable model parameters and then leverage supervised learning to optimize the continuous prompts by minimizing the cross-entropy loss based on sufficient downstream task data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>, <a href="#bib.bib397" title="" class="ltx_ref">397</a>, <a href="#bib.bib475" title="" class="ltx_ref">475</a>, <a href="#bib.bib401" title="" class="ltx_ref">401</a>]</cite>. As discussed in Section&nbsp;<a href="#S5.SS3.SSS1" title="5.3.1 Parameter-Efficient Fine-Tuning Methods ‣ 5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.1</span></a>, prefix tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib396" title="" class="ltx_ref">396</a>]</cite> prepends a sequence of prefixes (<em id="S6.SS1.SSS2.p8.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> a set of trainable continuous vectors) to each Transformer layer in language models, while prompt tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib397" title="" class="ltx_ref">397</a>]</cite> only incorporates trainable prompt vectors at the input layer. By fixing the large-scale parameters of LLMs and only tuning continuous prompt vector, this kind of approaches can be extremely parameter-efficient (Section&nbsp;<a href="#S5.SS3" title="5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>). However, these approaches are typically independent of the inputs, lacking sufficient consideration of input semantics. Therefore, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib475" title="" class="ltx_ref">475</a>]</cite> propose context tuning, where the continuous prompts are derived based on the input text and learned through the downstream task losses.</p>
</div>
<div id="S6.SS1.SSS2.p9" class="ltx_para">
<p id="S6.SS1.SSS2.p9.1" class="ltx_p"><math id="S6.SS1.SSS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS1.SSS2.p9.1.m1.1a"><mo id="S6.SS1.SSS2.p9.1.m1.1.1" xref="S6.SS1.SSS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p9.1.m1.1b"><ci id="S6.SS1.SSS2.p9.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.SS1.SSS2.p9.1.1" class="ltx_text ltx_font_italic">Prompt transferring with scarce data.</span> Supervised learning approaches demand in sufficient training data to learn optimal continuous prompts, which may not work well in data-scarce domains and tasks. To address this problem, SPoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib476" title="" class="ltx_ref">476</a>]</cite> proposes a prompt-based transfer learning approach, which first learns a single continuous prompt for several representative source tasks and then uses this prompt to initialize the prompt for a target task. However, this approach leverages the same prompt for solving all instances of the target task. For a single task, even a well-learned prompt may not be suitable for all the data instances from a large population. To address this issue, an improved method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib477" title="" class="ltx_ref">477</a>]</cite> designs an adaptive attention mechanism during the prompt transfer process to derive the target prompts, considering both task- and instance-level information. The prompt transfer paradigm can leverage the knowledge of data-sufficient source tasks encoded in source prompts for solving data-scarce target tasks.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span id="S6.SS2.1.1" class="ltx_text ltx_font_italic">In-Context Learning</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">As a special prompting form, in-context learning&nbsp;(ICL) is first proposed along with GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which has become a typical approach to utilizing LLMs.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>ICL Formulation</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">As stated in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, ICL uses a formatted natural language prompt, consisting of the task description and/or a few task examples as demonstrations.
Figure&nbsp;<a href="#S6.F14" title="Figure 14 ‣ 6.2.1 ICL Formulation ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> presents an illustration of ICL.
First, starting with a task description, a few examples are selected from the task dataset as demonstrations.
Then, they are combined in a specific order to form natural language prompts with specially designed templates.
Finally, the test instance is appended to the demonstration as the input for LLMs to generate the output.
Based on task demonstrations, LLMs can recognize and perform a new task without explicit gradient update.</p>
</div>
<div id="S6.SS2.SSS1.p2" class="ltx_para">
<p id="S6.SS2.SSS1.p2.8" class="ltx_p">Formally, let <math id="S6.SS2.SSS1.p2.1.m1.3" class="ltx_Math" alttext="D_{k}=\{f(x_{1},y_{1}),\dots,f(x_{k},y_{k})\}" display="inline"><semantics id="S6.SS2.SSS1.p2.1.m1.3a"><mrow id="S6.SS2.SSS1.p2.1.m1.3.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.cmml"><msub id="S6.SS2.SSS1.p2.1.m1.3.3.4" xref="S6.SS2.SSS1.p2.1.m1.3.3.4.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.3.3.4.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.4.2.cmml">D</mi><mi id="S6.SS2.SSS1.p2.1.m1.3.3.4.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.4.3.cmml">k</mi></msub><mo id="S6.SS2.SSS1.p2.1.m1.3.3.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.3.cmml">=</mo><mrow id="S6.SS2.SSS1.p2.1.m1.3.3.2.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.3.cmml">{</mo><mrow id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.4" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.3" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.3.cmml">​</mo><mrow id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.3.cmml"><mo stretchy="false" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.3" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.3.cmml">(</mo><msub id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.2" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.2.cmml">x</mi><mn id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.3" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.4" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.3.cmml">,</mo><msub id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.2" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.2.cmml">y</mi><mn id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.3" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.5" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.4" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S6.SS2.SSS1.p2.1.m1.1.1" xref="S6.SS2.SSS1.p2.1.m1.1.1.cmml">…</mi><mo id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.5" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.3.cmml">,</mo><mrow id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.4" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.3.cmml">​</mo><mrow id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.3.cmml"><mo stretchy="false" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.3.cmml">(</mo><msub id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.2.cmml">x</mi><mi id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.3.cmml">k</mi></msub><mo id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.4" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.3.cmml">,</mo><msub id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.cmml"><mi id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.2" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.2.cmml">y</mi><mi id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.3" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.5" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.6" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.1.m1.3b"><apply id="S6.SS2.SSS1.p2.1.m1.3.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3"><eq id="S6.SS2.SSS1.p2.1.m1.3.3.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.3"></eq><apply id="S6.SS2.SSS1.p2.1.m1.3.3.4.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.4"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.1.m1.3.3.4.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.4">subscript</csymbol><ci id="S6.SS2.SSS1.p2.1.m1.3.3.4.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.4.2">𝐷</ci><ci id="S6.SS2.SSS1.p2.1.m1.3.3.4.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.4.3">𝑘</ci></apply><set id="S6.SS2.SSS1.p2.1.m1.3.3.2.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2"><apply id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1"><times id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.3"></times><ci id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.4.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.4">𝑓</ci><interval closure="open" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2"><apply id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.1.1.1.3">1</cn></apply><apply id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2">subscript</csymbol><ci id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.2">𝑦</ci><cn type="integer" id="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.2.2.1.1.1.2.2.2.3">1</cn></apply></interval></apply><ci id="S6.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.1.1">…</ci><apply id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2"><times id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.3"></times><ci id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.4.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.4">𝑓</ci><interval closure="open" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2"><apply id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.2">𝑥</ci><ci id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.1.1.1.3">𝑘</ci></apply><apply id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.1.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2">subscript</csymbol><ci id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.2.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.2">𝑦</ci><ci id="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.3.cmml" xref="S6.SS2.SSS1.p2.1.m1.3.3.2.2.2.2.2.2.3">𝑘</ci></apply></interval></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.1.m1.3c">D_{k}=\{f(x_{1},y_{1}),\dots,f(x_{k},y_{k})\}</annotation></semantics></math> represent a set of demonstrations with <math id="S6.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS2.SSS1.p2.2.m2.1a"><mi id="S6.SS2.SSS1.p2.2.m2.1.1" xref="S6.SS2.SSS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.2.m2.1b"><ci id="S6.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S6.SS2.SSS1.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.2.m2.1c">k</annotation></semantics></math> examples, where <math id="S6.SS2.SSS1.p2.3.m3.2" class="ltx_Math" alttext="f(x_{k},y_{k})" display="inline"><semantics id="S6.SS2.SSS1.p2.3.m3.2a"><mrow id="S6.SS2.SSS1.p2.3.m3.2.2" xref="S6.SS2.SSS1.p2.3.m3.2.2.cmml"><mi id="S6.SS2.SSS1.p2.3.m3.2.2.4" xref="S6.SS2.SSS1.p2.3.m3.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.SS2.SSS1.p2.3.m3.2.2.3" xref="S6.SS2.SSS1.p2.3.m3.2.2.3.cmml">​</mo><mrow id="S6.SS2.SSS1.p2.3.m3.2.2.2.2" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.3.cmml"><mo stretchy="false" id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.3" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.3.cmml">(</mo><msub id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.2" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.2.cmml">x</mi><mi id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.3" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.4" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.3.cmml">,</mo><msub id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.cmml"><mi id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.2" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.2.cmml">y</mi><mi id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.3" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.5" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.3.m3.2b"><apply id="S6.SS2.SSS1.p2.3.m3.2.2.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2"><times id="S6.SS2.SSS1.p2.3.m3.2.2.3.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.3"></times><ci id="S6.SS2.SSS1.p2.3.m3.2.2.4.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.4">𝑓</ci><interval closure="open" id="S6.SS2.SSS1.p2.3.m3.2.2.2.3.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2"><apply id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.cmml" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.2">𝑥</ci><ci id="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S6.SS2.SSS1.p2.3.m3.1.1.1.1.1.3">𝑘</ci></apply><apply id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.1.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.2.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.2">𝑦</ci><ci id="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.3.cmml" xref="S6.SS2.SSS1.p2.3.m3.2.2.2.2.2.3">𝑘</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.3.m3.2c">f(x_{k},y_{k})</annotation></semantics></math> is the prompt function that transforms the <math id="S6.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS2.SSS1.p2.4.m4.1a"><mi id="S6.SS2.SSS1.p2.4.m4.1.1" xref="S6.SS2.SSS1.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.4.m4.1b"><ci id="S6.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S6.SS2.SSS1.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.4.m4.1c">k</annotation></semantics></math>-th task example into natural language prompts.
Given the task description <math id="S6.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S6.SS2.SSS1.p2.5.m5.1a"><mi id="S6.SS2.SSS1.p2.5.m5.1.1" xref="S6.SS2.SSS1.p2.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.5.m5.1b"><ci id="S6.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S6.SS2.SSS1.p2.5.m5.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.5.m5.1c">I</annotation></semantics></math>, demonstration <math id="S6.SS2.SSS1.p2.6.m6.1" class="ltx_Math" alttext="D_{k}" display="inline"><semantics id="S6.SS2.SSS1.p2.6.m6.1a"><msub id="S6.SS2.SSS1.p2.6.m6.1.1" xref="S6.SS2.SSS1.p2.6.m6.1.1.cmml"><mi id="S6.SS2.SSS1.p2.6.m6.1.1.2" xref="S6.SS2.SSS1.p2.6.m6.1.1.2.cmml">D</mi><mi id="S6.SS2.SSS1.p2.6.m6.1.1.3" xref="S6.SS2.SSS1.p2.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.6.m6.1b"><apply id="S6.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S6.SS2.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.6.m6.1.1.1.cmml" xref="S6.SS2.SSS1.p2.6.m6.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p2.6.m6.1.1.2.cmml" xref="S6.SS2.SSS1.p2.6.m6.1.1.2">𝐷</ci><ci id="S6.SS2.SSS1.p2.6.m6.1.1.3.cmml" xref="S6.SS2.SSS1.p2.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.6.m6.1c">D_{k}</annotation></semantics></math>, and a new input query <math id="S6.SS2.SSS1.p2.7.m7.1" class="ltx_Math" alttext="x_{k+1}" display="inline"><semantics id="S6.SS2.SSS1.p2.7.m7.1a"><msub id="S6.SS2.SSS1.p2.7.m7.1.1" xref="S6.SS2.SSS1.p2.7.m7.1.1.cmml"><mi id="S6.SS2.SSS1.p2.7.m7.1.1.2" xref="S6.SS2.SSS1.p2.7.m7.1.1.2.cmml">x</mi><mrow id="S6.SS2.SSS1.p2.7.m7.1.1.3" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.cmml"><mi id="S6.SS2.SSS1.p2.7.m7.1.1.3.2" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.2.cmml">k</mi><mo id="S6.SS2.SSS1.p2.7.m7.1.1.3.1" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.1.cmml">+</mo><mn id="S6.SS2.SSS1.p2.7.m7.1.1.3.3" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.7.m7.1b"><apply id="S6.SS2.SSS1.p2.7.m7.1.1.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.7.m7.1.1.1.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p2.7.m7.1.1.2.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1.2">𝑥</ci><apply id="S6.SS2.SSS1.p2.7.m7.1.1.3.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1.3"><plus id="S6.SS2.SSS1.p2.7.m7.1.1.3.1.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.1"></plus><ci id="S6.SS2.SSS1.p2.7.m7.1.1.3.2.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.2">𝑘</ci><cn type="integer" id="S6.SS2.SSS1.p2.7.m7.1.1.3.3.cmml" xref="S6.SS2.SSS1.p2.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.7.m7.1c">x_{k+1}</annotation></semantics></math>, the prediction of the output <math id="S6.SS2.SSS1.p2.8.m8.1" class="ltx_Math" alttext="\hat{y}_{k+1}" display="inline"><semantics id="S6.SS2.SSS1.p2.8.m8.1a"><msub id="S6.SS2.SSS1.p2.8.m8.1.1" xref="S6.SS2.SSS1.p2.8.m8.1.1.cmml"><mover accent="true" id="S6.SS2.SSS1.p2.8.m8.1.1.2" xref="S6.SS2.SSS1.p2.8.m8.1.1.2.cmml"><mi id="S6.SS2.SSS1.p2.8.m8.1.1.2.2" xref="S6.SS2.SSS1.p2.8.m8.1.1.2.2.cmml">y</mi><mo id="S6.SS2.SSS1.p2.8.m8.1.1.2.1" xref="S6.SS2.SSS1.p2.8.m8.1.1.2.1.cmml">^</mo></mover><mrow id="S6.SS2.SSS1.p2.8.m8.1.1.3" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.cmml"><mi id="S6.SS2.SSS1.p2.8.m8.1.1.3.2" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.2.cmml">k</mi><mo id="S6.SS2.SSS1.p2.8.m8.1.1.3.1" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.1.cmml">+</mo><mn id="S6.SS2.SSS1.p2.8.m8.1.1.3.3" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.8.m8.1b"><apply id="S6.SS2.SSS1.p2.8.m8.1.1.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.8.m8.1.1.1.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1">subscript</csymbol><apply id="S6.SS2.SSS1.p2.8.m8.1.1.2.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.2"><ci id="S6.SS2.SSS1.p2.8.m8.1.1.2.1.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.2.1">^</ci><ci id="S6.SS2.SSS1.p2.8.m8.1.1.2.2.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.2.2">𝑦</ci></apply><apply id="S6.SS2.SSS1.p2.8.m8.1.1.3.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.3"><plus id="S6.SS2.SSS1.p2.8.m8.1.1.3.1.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.1"></plus><ci id="S6.SS2.SSS1.p2.8.m8.1.1.3.2.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.2">𝑘</ci><cn type="integer" id="S6.SS2.SSS1.p2.8.m8.1.1.3.3.cmml" xref="S6.SS2.SSS1.p2.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.8.m8.1c">\hat{y}_{k+1}</annotation></semantics></math> generated from LLMs can be formulated as follows<span id="footnote42" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">42</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">42</sup><span class="ltx_tag ltx_tag_note">42</span>
When ICL was introduced in the GPT-3’s paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, it was originally defined to be a combination of the task description and demonstration examples, wherein either component is dispensable. Following this definition, when a LLM is required to solve an unseen task by using only task descriptions, it can be also considered to perform ICL for task solving, whereas the ICL ability can be enhanced by instruction tuning.
</span></span></span>:</p>
<table id="S6.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E12.m1.5" class="ltx_Math" alttext="\text{LLM}\big{(}I,\underbrace{f(x_{1},y_{1}),\dots,f(x_{k},y_{k})}_{\text{demonstrations}},f(\underbrace{x_{k+1}}_{\text{input}},\underbrace{\vphantom{\hat{y}_{k+1}}\_\_\_}_{\text{answer}})\big{)}\rightarrow\hat{y}_{k+1}." display="block"><semantics id="S6.E12.m1.5a"><mrow id="S6.E12.m1.5.5.1" xref="S6.E12.m1.5.5.1.1.cmml"><mrow id="S6.E12.m1.5.5.1.1" xref="S6.E12.m1.5.5.1.1.cmml"><mrow id="S6.E12.m1.5.5.1.1.2" xref="S6.E12.m1.5.5.1.1.2.cmml"><mtext id="S6.E12.m1.5.5.1.1.2.4" xref="S6.E12.m1.5.5.1.1.2.4a.cmml">LLM</mtext><mo lspace="0em" rspace="0em" id="S6.E12.m1.5.5.1.1.2.3" xref="S6.E12.m1.5.5.1.1.2.3.cmml">​</mo><mrow id="S6.E12.m1.5.5.1.1.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S6.E12.m1.5.5.1.1.2.2.2.3" xref="S6.E12.m1.5.5.1.1.2.2.3.cmml">(</mo><mi id="S6.E12.m1.4.4" xref="S6.E12.m1.4.4.cmml">I</mi><mo id="S6.E12.m1.5.5.1.1.2.2.2.4" xref="S6.E12.m1.5.5.1.1.2.2.3.cmml">,</mo><munder id="S6.E12.m1.5.5.1.1.1.1.1.1" xref="S6.E12.m1.5.5.1.1.1.1.1.1.cmml"><munder accentunder="true" id="S6.E12.m1.3.3" xref="S6.E12.m1.3.3.cmml"><mrow id="S6.E12.m1.3.3.3.3" xref="S6.E12.m1.3.3.3.4.cmml"><mrow id="S6.E12.m1.2.2.2.2.1" xref="S6.E12.m1.2.2.2.2.1.cmml"><mi id="S6.E12.m1.2.2.2.2.1.4" xref="S6.E12.m1.2.2.2.2.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.E12.m1.2.2.2.2.1.3" xref="S6.E12.m1.2.2.2.2.1.3.cmml">​</mo><mrow id="S6.E12.m1.2.2.2.2.1.2.2" xref="S6.E12.m1.2.2.2.2.1.2.3.cmml"><mo stretchy="false" id="S6.E12.m1.2.2.2.2.1.2.2.3" xref="S6.E12.m1.2.2.2.2.1.2.3.cmml">(</mo><msub id="S6.E12.m1.2.2.2.2.1.1.1.1" xref="S6.E12.m1.2.2.2.2.1.1.1.1.cmml"><mi id="S6.E12.m1.2.2.2.2.1.1.1.1.2" xref="S6.E12.m1.2.2.2.2.1.1.1.1.2.cmml">x</mi><mn id="S6.E12.m1.2.2.2.2.1.1.1.1.3" xref="S6.E12.m1.2.2.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S6.E12.m1.2.2.2.2.1.2.2.4" xref="S6.E12.m1.2.2.2.2.1.2.3.cmml">,</mo><msub id="S6.E12.m1.2.2.2.2.1.2.2.2" xref="S6.E12.m1.2.2.2.2.1.2.2.2.cmml"><mi id="S6.E12.m1.2.2.2.2.1.2.2.2.2" xref="S6.E12.m1.2.2.2.2.1.2.2.2.2.cmml">y</mi><mn id="S6.E12.m1.2.2.2.2.1.2.2.2.3" xref="S6.E12.m1.2.2.2.2.1.2.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S6.E12.m1.2.2.2.2.1.2.2.5" xref="S6.E12.m1.2.2.2.2.1.2.3.cmml">)</mo></mrow></mrow><mo id="S6.E12.m1.3.3.3.3.3" xref="S6.E12.m1.3.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S6.E12.m1.1.1.1.1" xref="S6.E12.m1.1.1.1.1.cmml">…</mi><mo id="S6.E12.m1.3.3.3.3.4" xref="S6.E12.m1.3.3.3.4.cmml">,</mo><mrow id="S6.E12.m1.3.3.3.3.2" xref="S6.E12.m1.3.3.3.3.2.cmml"><mi id="S6.E12.m1.3.3.3.3.2.4" xref="S6.E12.m1.3.3.3.3.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.E12.m1.3.3.3.3.2.3" xref="S6.E12.m1.3.3.3.3.2.3.cmml">​</mo><mrow id="S6.E12.m1.3.3.3.3.2.2.2" xref="S6.E12.m1.3.3.3.3.2.2.3.cmml"><mo stretchy="false" id="S6.E12.m1.3.3.3.3.2.2.2.3" xref="S6.E12.m1.3.3.3.3.2.2.3.cmml">(</mo><msub id="S6.E12.m1.3.3.3.3.2.1.1.1" xref="S6.E12.m1.3.3.3.3.2.1.1.1.cmml"><mi id="S6.E12.m1.3.3.3.3.2.1.1.1.2" xref="S6.E12.m1.3.3.3.3.2.1.1.1.2.cmml">x</mi><mi id="S6.E12.m1.3.3.3.3.2.1.1.1.3" xref="S6.E12.m1.3.3.3.3.2.1.1.1.3.cmml">k</mi></msub><mo id="S6.E12.m1.3.3.3.3.2.2.2.4" xref="S6.E12.m1.3.3.3.3.2.2.3.cmml">,</mo><msub id="S6.E12.m1.3.3.3.3.2.2.2.2" xref="S6.E12.m1.3.3.3.3.2.2.2.2.cmml"><mi id="S6.E12.m1.3.3.3.3.2.2.2.2.2" xref="S6.E12.m1.3.3.3.3.2.2.2.2.2.cmml">y</mi><mi id="S6.E12.m1.3.3.3.3.2.2.2.2.3" xref="S6.E12.m1.3.3.3.3.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S6.E12.m1.3.3.3.3.2.2.2.5" xref="S6.E12.m1.3.3.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S6.E12.m1.3.3.4" xref="S6.E12.m1.3.3.4.cmml">⏟</mo></munder><mtext id="S6.E12.m1.5.5.1.1.1.1.1.1.2" xref="S6.E12.m1.5.5.1.1.1.1.1.1.2a.cmml">demonstrations</mtext></munder><mo id="S6.E12.m1.5.5.1.1.2.2.2.5" xref="S6.E12.m1.5.5.1.1.2.2.3.cmml">,</mo><mrow id="S6.E12.m1.5.5.1.1.2.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.cmml"><mi id="S6.E12.m1.5.5.1.1.2.2.2.2.4" xref="S6.E12.m1.5.5.1.1.2.2.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.E12.m1.5.5.1.1.2.2.2.2.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.3.cmml">​</mo><mrow id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.3.cmml">(</mo><munder id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.cmml"><munder accentunder="true" id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.cmml"><msub id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.cmml"><mi id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.2.cmml">x</mi><mrow id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.cmml"><mi id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.2.cmml">k</mi><mo id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.1" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.1.cmml">+</mo><mn id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.1" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.1.cmml">⏟</mo></munder><mtext id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.3a.cmml">input</mtext></munder><mo id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.4" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.3.cmml">,</mo><munder id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.cmml"><munder accentunder="true" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.cmml"><mrow id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.cmml"><mi mathvariant="normal" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.2" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.2.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.1" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.1a" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.4" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.4.cmml">_</mi></mrow><mo id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.1" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.1.cmml">⏟</mo></munder><mtext id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.3" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.3a.cmml">answer</mtext></munder><mo stretchy="false" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.5" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%" id="S6.E12.m1.5.5.1.1.2.2.2.6" xref="S6.E12.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S6.E12.m1.5.5.1.1.3" xref="S6.E12.m1.5.5.1.1.3.cmml">→</mo><msub id="S6.E12.m1.5.5.1.1.4" xref="S6.E12.m1.5.5.1.1.4.cmml"><mover accent="true" id="S6.E12.m1.5.5.1.1.4.2" xref="S6.E12.m1.5.5.1.1.4.2.cmml"><mi id="S6.E12.m1.5.5.1.1.4.2.2" xref="S6.E12.m1.5.5.1.1.4.2.2.cmml">y</mi><mo id="S6.E12.m1.5.5.1.1.4.2.1" xref="S6.E12.m1.5.5.1.1.4.2.1.cmml">^</mo></mover><mrow id="S6.E12.m1.5.5.1.1.4.3" xref="S6.E12.m1.5.5.1.1.4.3.cmml"><mi id="S6.E12.m1.5.5.1.1.4.3.2" xref="S6.E12.m1.5.5.1.1.4.3.2.cmml">k</mi><mo id="S6.E12.m1.5.5.1.1.4.3.1" xref="S6.E12.m1.5.5.1.1.4.3.1.cmml">+</mo><mn id="S6.E12.m1.5.5.1.1.4.3.3" xref="S6.E12.m1.5.5.1.1.4.3.3.cmml">1</mn></mrow></msub></mrow><mo lspace="0em" id="S6.E12.m1.5.5.1.2" xref="S6.E12.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E12.m1.5b"><apply id="S6.E12.m1.5.5.1.1.cmml" xref="S6.E12.m1.5.5.1"><ci id="S6.E12.m1.5.5.1.1.3.cmml" xref="S6.E12.m1.5.5.1.1.3">→</ci><apply id="S6.E12.m1.5.5.1.1.2.cmml" xref="S6.E12.m1.5.5.1.1.2"><times id="S6.E12.m1.5.5.1.1.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.3"></times><ci id="S6.E12.m1.5.5.1.1.2.4a.cmml" xref="S6.E12.m1.5.5.1.1.2.4"><mtext id="S6.E12.m1.5.5.1.1.2.4.cmml" xref="S6.E12.m1.5.5.1.1.2.4">LLM</mtext></ci><vector id="S6.E12.m1.5.5.1.1.2.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2"><ci id="S6.E12.m1.4.4.cmml" xref="S6.E12.m1.4.4">𝐼</ci><apply id="S6.E12.m1.5.5.1.1.1.1.1.1.cmml" xref="S6.E12.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E12.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S6.E12.m1.5.5.1.1.1.1.1.1">subscript</csymbol><apply id="S6.E12.m1.3.3.cmml" xref="S6.E12.m1.3.3"><ci id="S6.E12.m1.3.3.4.cmml" xref="S6.E12.m1.3.3.4">⏟</ci><list id="S6.E12.m1.3.3.3.4.cmml" xref="S6.E12.m1.3.3.3.3"><apply id="S6.E12.m1.2.2.2.2.1.cmml" xref="S6.E12.m1.2.2.2.2.1"><times id="S6.E12.m1.2.2.2.2.1.3.cmml" xref="S6.E12.m1.2.2.2.2.1.3"></times><ci id="S6.E12.m1.2.2.2.2.1.4.cmml" xref="S6.E12.m1.2.2.2.2.1.4">𝑓</ci><interval closure="open" id="S6.E12.m1.2.2.2.2.1.2.3.cmml" xref="S6.E12.m1.2.2.2.2.1.2.2"><apply id="S6.E12.m1.2.2.2.2.1.1.1.1.cmml" xref="S6.E12.m1.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S6.E12.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S6.E12.m1.2.2.2.2.1.1.1.1">subscript</csymbol><ci id="S6.E12.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S6.E12.m1.2.2.2.2.1.1.1.1.2">𝑥</ci><cn type="integer" id="S6.E12.m1.2.2.2.2.1.1.1.1.3.cmml" xref="S6.E12.m1.2.2.2.2.1.1.1.1.3">1</cn></apply><apply id="S6.E12.m1.2.2.2.2.1.2.2.2.cmml" xref="S6.E12.m1.2.2.2.2.1.2.2.2"><csymbol cd="ambiguous" id="S6.E12.m1.2.2.2.2.1.2.2.2.1.cmml" xref="S6.E12.m1.2.2.2.2.1.2.2.2">subscript</csymbol><ci id="S6.E12.m1.2.2.2.2.1.2.2.2.2.cmml" xref="S6.E12.m1.2.2.2.2.1.2.2.2.2">𝑦</ci><cn type="integer" id="S6.E12.m1.2.2.2.2.1.2.2.2.3.cmml" xref="S6.E12.m1.2.2.2.2.1.2.2.2.3">1</cn></apply></interval></apply><ci id="S6.E12.m1.1.1.1.1.cmml" xref="S6.E12.m1.1.1.1.1">…</ci><apply id="S6.E12.m1.3.3.3.3.2.cmml" xref="S6.E12.m1.3.3.3.3.2"><times id="S6.E12.m1.3.3.3.3.2.3.cmml" xref="S6.E12.m1.3.3.3.3.2.3"></times><ci id="S6.E12.m1.3.3.3.3.2.4.cmml" xref="S6.E12.m1.3.3.3.3.2.4">𝑓</ci><interval closure="open" id="S6.E12.m1.3.3.3.3.2.2.3.cmml" xref="S6.E12.m1.3.3.3.3.2.2.2"><apply id="S6.E12.m1.3.3.3.3.2.1.1.1.cmml" xref="S6.E12.m1.3.3.3.3.2.1.1.1"><csymbol cd="ambiguous" id="S6.E12.m1.3.3.3.3.2.1.1.1.1.cmml" xref="S6.E12.m1.3.3.3.3.2.1.1.1">subscript</csymbol><ci id="S6.E12.m1.3.3.3.3.2.1.1.1.2.cmml" xref="S6.E12.m1.3.3.3.3.2.1.1.1.2">𝑥</ci><ci id="S6.E12.m1.3.3.3.3.2.1.1.1.3.cmml" xref="S6.E12.m1.3.3.3.3.2.1.1.1.3">𝑘</ci></apply><apply id="S6.E12.m1.3.3.3.3.2.2.2.2.cmml" xref="S6.E12.m1.3.3.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S6.E12.m1.3.3.3.3.2.2.2.2.1.cmml" xref="S6.E12.m1.3.3.3.3.2.2.2.2">subscript</csymbol><ci id="S6.E12.m1.3.3.3.3.2.2.2.2.2.cmml" xref="S6.E12.m1.3.3.3.3.2.2.2.2.2">𝑦</ci><ci id="S6.E12.m1.3.3.3.3.2.2.2.2.3.cmml" xref="S6.E12.m1.3.3.3.3.2.2.2.2.3">𝑘</ci></apply></interval></apply></list></apply><ci id="S6.E12.m1.5.5.1.1.1.1.1.1.2a.cmml" xref="S6.E12.m1.5.5.1.1.1.1.1.1.2"><mtext mathsize="70%" id="S6.E12.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S6.E12.m1.5.5.1.1.1.1.1.1.2">demonstrations</mtext></ci></apply><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2"><times id="S6.E12.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.3"></times><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.4.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.4">𝑓</ci><interval closure="open" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2"><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1">subscript</csymbol><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2"><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.1">⏟</ci><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2">subscript</csymbol><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.2">𝑥</ci><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3"><plus id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.1"></plus><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.2.2.3.3">1</cn></apply></apply></apply><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.3a.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.3"><mtext mathsize="70%" id="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.1.1.1.3">input</mtext></ci></apply><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2">subscript</csymbol><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2"><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.1">⏟</ci><apply id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2"><times id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.1"></times><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.2">_</ci><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.3">_</ci><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.4.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.2.2.4">_</ci></apply></apply><ci id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.3a.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.3.cmml" xref="S6.E12.m1.5.5.1.1.2.2.2.2.2.2.2.3">answer</mtext></ci></apply></interval></apply></vector></apply><apply id="S6.E12.m1.5.5.1.1.4.cmml" xref="S6.E12.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S6.E12.m1.5.5.1.1.4.1.cmml" xref="S6.E12.m1.5.5.1.1.4">subscript</csymbol><apply id="S6.E12.m1.5.5.1.1.4.2.cmml" xref="S6.E12.m1.5.5.1.1.4.2"><ci id="S6.E12.m1.5.5.1.1.4.2.1.cmml" xref="S6.E12.m1.5.5.1.1.4.2.1">^</ci><ci id="S6.E12.m1.5.5.1.1.4.2.2.cmml" xref="S6.E12.m1.5.5.1.1.4.2.2">𝑦</ci></apply><apply id="S6.E12.m1.5.5.1.1.4.3.cmml" xref="S6.E12.m1.5.5.1.1.4.3"><plus id="S6.E12.m1.5.5.1.1.4.3.1.cmml" xref="S6.E12.m1.5.5.1.1.4.3.1"></plus><ci id="S6.E12.m1.5.5.1.1.4.3.2.cmml" xref="S6.E12.m1.5.5.1.1.4.3.2">𝑘</ci><cn type="integer" id="S6.E12.m1.5.5.1.1.4.3.3.cmml" xref="S6.E12.m1.5.5.1.1.4.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E12.m1.5c">\text{LLM}\big{(}I,\underbrace{f(x_{1},y_{1}),\dots,f(x_{k},y_{k})}_{\text{demonstrations}},f(\underbrace{x_{k+1}}_{\text{input}},\underbrace{\vphantom{\hat{y}_{k+1}}\_\_\_}_{\text{answer}})\big{)}\rightarrow\hat{y}_{k+1}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p id="S6.SS2.SSS1.p2.10" class="ltx_p">where the actual answer <math id="S6.SS2.SSS1.p2.9.m1.1" class="ltx_Math" alttext="y_{k+1}" display="inline"><semantics id="S6.SS2.SSS1.p2.9.m1.1a"><msub id="S6.SS2.SSS1.p2.9.m1.1.1" xref="S6.SS2.SSS1.p2.9.m1.1.1.cmml"><mi id="S6.SS2.SSS1.p2.9.m1.1.1.2" xref="S6.SS2.SSS1.p2.9.m1.1.1.2.cmml">y</mi><mrow id="S6.SS2.SSS1.p2.9.m1.1.1.3" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.cmml"><mi id="S6.SS2.SSS1.p2.9.m1.1.1.3.2" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.2.cmml">k</mi><mo id="S6.SS2.SSS1.p2.9.m1.1.1.3.1" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.1.cmml">+</mo><mn id="S6.SS2.SSS1.p2.9.m1.1.1.3.3" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.9.m1.1b"><apply id="S6.SS2.SSS1.p2.9.m1.1.1.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p2.9.m1.1.1.1.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p2.9.m1.1.1.2.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1.2">𝑦</ci><apply id="S6.SS2.SSS1.p2.9.m1.1.1.3.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1.3"><plus id="S6.SS2.SSS1.p2.9.m1.1.1.3.1.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.1"></plus><ci id="S6.SS2.SSS1.p2.9.m1.1.1.3.2.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.2">𝑘</ci><cn type="integer" id="S6.SS2.SSS1.p2.9.m1.1.1.3.3.cmml" xref="S6.SS2.SSS1.p2.9.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.9.m1.1c">y_{k+1}</annotation></semantics></math> is left as a blank to be predicted by the LLM. Since the performance of ICL heavily relies on demonstrations, it is important to properly design them in the prompts.
According to the construction process in Equation&nbsp;(<a href="#S6.E12" title="In 6.2.1 ICL Formulation ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>), we focus on three major aspects of formatting demonstrations in the prompts, including how to select examples that make up demonstrations, format each example into the prompt with the function <math id="S6.SS2.SSS1.p2.10.m2.1" class="ltx_Math" alttext="f(\cdot)" display="inline"><semantics id="S6.SS2.SSS1.p2.10.m2.1a"><mrow id="S6.SS2.SSS1.p2.10.m2.1.2" xref="S6.SS2.SSS1.p2.10.m2.1.2.cmml"><mi id="S6.SS2.SSS1.p2.10.m2.1.2.2" xref="S6.SS2.SSS1.p2.10.m2.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.SS2.SSS1.p2.10.m2.1.2.1" xref="S6.SS2.SSS1.p2.10.m2.1.2.1.cmml">​</mo><mrow id="S6.SS2.SSS1.p2.10.m2.1.2.3.2" xref="S6.SS2.SSS1.p2.10.m2.1.2.cmml"><mo stretchy="false" id="S6.SS2.SSS1.p2.10.m2.1.2.3.2.1" xref="S6.SS2.SSS1.p2.10.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S6.SS2.SSS1.p2.10.m2.1.1" xref="S6.SS2.SSS1.p2.10.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S6.SS2.SSS1.p2.10.m2.1.2.3.2.2" xref="S6.SS2.SSS1.p2.10.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p2.10.m2.1b"><apply id="S6.SS2.SSS1.p2.10.m2.1.2.cmml" xref="S6.SS2.SSS1.p2.10.m2.1.2"><times id="S6.SS2.SSS1.p2.10.m2.1.2.1.cmml" xref="S6.SS2.SSS1.p2.10.m2.1.2.1"></times><ci id="S6.SS2.SSS1.p2.10.m2.1.2.2.cmml" xref="S6.SS2.SSS1.p2.10.m2.1.2.2">𝑓</ci><ci id="S6.SS2.SSS1.p2.10.m2.1.1.cmml" xref="S6.SS2.SSS1.p2.10.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p2.10.m2.1c">f(\cdot)</annotation></semantics></math>, and arrange demonstrations in a reasonable order.</p>
</div>
<div id="S6.SS2.SSS1.p3" class="ltx_para">
<p id="S6.SS2.SSS1.p3.1" class="ltx_p">A comprehensive review of ICL has been presented in the survey paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and we suggest the readers referring to it for a more general, detailed discussion on this topic. Compared with this survey, we specially focus on the discussion of applying ICL to LLMs in two major aspects, <em id="S6.SS2.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> demonstration design and the underlying mechanism of ICL.

Also, ICL has a close connection with instruction tuning (discussed in Section&nbsp;<a href="#S5.SS1" title="5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>) in that
both utilize natural language to format the task or instances.
However, instruction tuning needs to fine-tune LLMs for adaptation, while ICL only prompts LLMs for utilization.
Furthermore, instruction tuning can enhance the ICL ability of LLMs to perform target tasks, especially in the zero-shot setting (only using task descriptions)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.</p>
</div>
<figure id="S6.F14" class="ltx_figure"><img src="/html/2303.18223/assets/x14.png" id="S6.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>
A comparative illustration of in-context learning&nbsp;(ICL) and chain-of-thought&nbsp;(CoT) prompting.
ICL prompts LLMs with a natural language description, several demonstrations, and a test query, while
CoT prompting involves a series of intermediate reasoning steps in prompts.
</figcaption>
</figure>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Demonstration Design</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">Several studies have shown that the effectiveness of ICL is highly affected by the design of demonstrations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib478" title="" class="ltx_ref">478</a>, <a href="#bib.bib432" title="" class="ltx_ref">432</a>, <a href="#bib.bib479" title="" class="ltx_ref">479</a>]</cite>
Following the discussion in Section&nbsp;<a href="#S6.SS2.SSS1" title="6.2.1 ICL Formulation ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.1</span></a>, we will introduce the demonstration design of ICL from three major aspects, <em id="S6.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> demonstration selection, format, and order.</p>
</div>
<div id="S6.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.SSS2.p2.1" class="ltx_p"><span id="S6.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Demonstration Selection.</span>

The performance of ICL tends to have a large variance with different demonstration examples&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib428" title="" class="ltx_ref">428</a>]</cite>, so it is important to select a subset of examples that can effectively leverage the ICL capability of LLMs.
There are two main demonstration selection approaches, namely heuristic and LLM-based approaches:</p>
</div>
<div id="S6.SS2.SSS2.p3" class="ltx_para">
<p id="S6.SS2.SSS2.p3.2" class="ltx_p"><math id="S6.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS2.SSS2.p3.1.m1.1a"><mo id="S6.SS2.SSS2.p3.1.m1.1.1" xref="S6.SS2.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.1.m1.1b"><ci id="S6.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S6.SS2.SSS2.p3.2.1" class="ltx_emph ltx_font_italic">Heuristic approaches.</em>
Due to their simplicity and low costs, existing work widely adopts heuristic methods to select demonstrations.
Several studies employ a <math id="S6.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS2.SSS2.p3.2.m2.1a"><mi id="S6.SS2.SSS2.p3.2.m2.1.1" xref="S6.SS2.SSS2.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.2.m2.1b"><ci id="S6.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S6.SS2.SSS2.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.2.m2.1c">k</annotation></semantics></math>-NN based retriever to select examples that are semantically relevant to the query&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib428" title="" class="ltx_ref">428</a>, <a href="#bib.bib480" title="" class="ltx_ref">480</a>]</cite>.
However, they perform the selection individually for each example, rather than evaluating the example set as a whole.
To resolve this issue, diversity-based selection strategies are proposed to choose the most representative set of examples for specific tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib481" title="" class="ltx_ref">481</a>, <a href="#bib.bib482" title="" class="ltx_ref">482</a>]</cite>.
Furthermore, in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib483" title="" class="ltx_ref">483</a>]</cite>, both relevance and diversity are taken into consideration when selecting demonstrations.</p>
</div>
<div id="S6.SS2.SSS2.p4" class="ltx_para">
<p id="S6.SS2.SSS2.p4.1" class="ltx_p"><math id="S6.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS2.SSS2.p4.1.m1.1a"><mo id="S6.SS2.SSS2.p4.1.m1.1.1" xref="S6.SS2.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p4.1.m1.1b"><ci id="S6.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S6.SS2.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">LLM-based approaches.</em>
Another line of work selects demonstrations by making use of LLMs.
For example, LLMs can be utilized to directly measure the informativeness of each example according to the performance gain after adding the example&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib484" title="" class="ltx_ref">484</a>]</cite>.
In addition, EPR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib429" title="" class="ltx_ref">429</a>]</cite> proposes a two-stage retrieval approach that first recalls similar examples with an unsupervised method (<em id="S6.SS2.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> BM25) and then ranks them using a dense retriever (trained with positive and negative examples labeled by LLMs).
As an alternative approach, the task of demonstration selection can be formulated into a RL problem, where LLMs serve as the reward function to provide feedback for training the policy model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib485" title="" class="ltx_ref">485</a>]</cite>. Since LLMs perform well for text annotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib486" title="" class="ltx_ref">486</a>]</cite>, some recent studies employ LLM itself as the demonstration generator without human intervention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib487" title="" class="ltx_ref">487</a>]</cite>.</p>
</div>
<div id="S6.SS2.SSS2.p5" class="ltx_para">
<p id="S6.SS2.SSS2.p5.1" class="ltx_p">To summarize, as discussed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib488" title="" class="ltx_ref">488</a>]</cite>, the selected demonstration examples in ICL should contain sufficient information about the task to solve as well as be relevant to the test query, for the above two selection approaches.</p>
</div>
<div id="S6.SS2.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S6.SS2.SSS2.p6.1" class="ltx_p"><span id="S6.SS2.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Demonstration Format.</span>
After selecting task examples, the next step is to integrate and format them into a natural language prompt for LLMs.
A straightforward method is to instantiate a pre-defined template with the corresponding input-output pairs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
To construct more informative templates, recent studies consider adding task descriptions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> or enhancing the reasoning capability of LLMs with chain-of-thought prompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
For instance, in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, the authors collect a large-scale dataset with task descriptions written by humans.
After tuning with this dataset, the performance on seen tasks can be boosted, and LLMs can also generalize to unseen tasks to some extent.
To reduce the annotation costs, a semi-automated approach has been proposed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> by employing a seed set consisting of human-written task descriptions to guide LLMs to generate task descriptions for new tasks.
Since it is costly to manually annotate demonstration formats for different tasks, some work also studies how to automatically generate high-quality ones.
As two representative methods, Auto-CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib434" title="" class="ltx_ref">434</a>]</cite> leverages LLMs with the zero-shot prompt “<em id="S6.SS2.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">Let’s think step by step</em>” for generating intermediate reasoning steps, while least-to-most prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib439" title="" class="ltx_ref">439</a>]</cite> first queries LLMs to perform problem decomposition and then utilizes LLMs to sequentially solve sub-problems based on the intermediate answers to previously solved ones.</p>
</div>
<div id="S6.SS2.SSS2.p7" class="ltx_para ltx_noindent">
<p id="S6.SS2.SSS2.p7.1" class="ltx_p"><span id="S6.SS2.SSS2.p7.1.1" class="ltx_text ltx_font_bold">Demonstration Order.</span>
LLMs are shown to sometimes suffer from the recency bias, <em id="S6.SS2.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> they are prone to repeat answers that are near the end of demonstrations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib479" title="" class="ltx_ref">479</a>]</cite>.
Thus, it is important to arrange demonstrations (<em id="S6.SS2.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> task examples) in a reasonable order.
Early work proposes several heuristic methods to quickly find a good order.
For example, demonstrations can be directly organized according to their similarity to the query in the embedding space&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib428" title="" class="ltx_ref">428</a>]</cite>: the more similar, the closer to the end.
In addition, global and local entropy metrics can be used to score different demonstration orders&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib432" title="" class="ltx_ref">432</a>]</cite>.
To integrate more task information, some recent studies propose to minimize the
code length required to compress and transmit task labels, which is inspired by information theory&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib489" title="" class="ltx_ref">489</a>]</cite>.
However, these methods need additional labeled data as the validation set to evaluate the performance of specific demonstration orders.
To eliminate this need, the authors in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib432" title="" class="ltx_ref">432</a>]</cite> propose to sample the validation data from the LLM itself.</p>
</div>
</section>
<section id="S6.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3 </span>Underlying Mechanism</h4>

<div id="S6.SS2.SSS3.p1" class="ltx_para">
<p id="S6.SS2.SSS3.p1.1" class="ltx_p">After pre-training, LLMs can exhibit intriguing ICL capability without being updated.
In what follows, we discuss two key questions about the ICL ability of LLMs, <em id="S6.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> “<em id="S6.SS2.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">how does pre-training affect the ICL ability</em>” and “<em id="S6.SS2.SSS3.p1.1.3" class="ltx_emph ltx_font_italic">how do LLMs perform ICL during inference</em>”.</p>
</div>
<div id="S6.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.SSS3.p2.1" class="ltx_p"><span id="S6.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">How Pre-Training Affects ICL?</span>
ICL is first proposed in GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and it has been shown that the ICL ability becomes more significant with a larger model size.
Further, some studies reveal that small-scale PLMs can also demonstrate a strong ICL ability by continual pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib490" title="" class="ltx_ref">490</a>]</cite> or fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib491" title="" class="ltx_ref">491</a>]</cite> on specially designed training tasks, which typically involve additional task examples in the input during the training process.
It suggests that the design of training tasks is an important influence factor on the ICL capability of LLMs.
Besides training tasks, recent studies have also investigated the relationship between ICL and pre-training corpora&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib488" title="" class="ltx_ref">488</a>, <a href="#bib.bib492" title="" class="ltx_ref">492</a>]</cite>.
For example, ICL can be theoretically explained as the product of pre-training on documents that exhibit long-range coherence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib488" title="" class="ltx_ref">488</a>]</cite>.

Further, another study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib492" title="" class="ltx_ref">492</a>]</cite> theoretically analyzes that when scaling parameters and data, LLMs based on next-word prediction can emerge the ability of ICL by learning from the compositional structure (<em id="S6.SS2.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> how words and phrases are combined to form larger linguistic units like sentences) present in language data.</p>
</div>
<div id="S6.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S6.SS2.SSS3.p3.1" class="ltx_p"><span id="S6.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">How LLMs Perform ICL?</span>
At the inference stage, researchers focus on analyzing how the ICL capability operates based on given demonstrations since no explicit learning or updating is involved.
According to the discussion in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib493" title="" class="ltx_ref">493</a>]</cite>, there are two main ways for LLMs to utilize demonstrations: task recognition and task learning.</p>
</div>
<div id="S6.SS2.SSS3.p4" class="ltx_para">
<p id="S6.SS2.SSS3.p4.1" class="ltx_p"><math id="S6.SS2.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS2.SSS3.p4.1.m1.1a"><mo id="S6.SS2.SSS3.p4.1.m1.1.1" xref="S6.SS2.SSS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p4.1.m1.1b"><ci id="S6.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S6.SS2.SSS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p4.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S6.SS2.SSS3.p4.1.1" class="ltx_emph ltx_font_italic">Task recognition.</em>
In the first way, LLMs recognize the task from demonstrations and utilize the prior knowledge obtained from pre-training to solve new test tasks.
A Probably Approximately Correct&nbsp;(PAC) framework&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib494" title="" class="ltx_ref">494</a>]</cite> has been proposed to assess the learnability of ICL.
It assumes that there exists a latent variable representing the task in the pre-training data, and LLMs have been shown to be capable of capturing this variable from demonstrations, enabling them to recognize the task in ICL.
Also, the interpretation of ICL as task recognition is supported by several empirical studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib478" title="" class="ltx_ref">478</a>, <a href="#bib.bib495" title="" class="ltx_ref">495</a>]</cite>.
For example, it has been observed that replacing the inputs or labels of demonstrations with random ones sampled from the input or label space does not seriously hurt the performance of LLMs, indicating that LLMs mainly recognize the target task from demonstrations instead of learning from them&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib478" title="" class="ltx_ref">478</a>, <a href="#bib.bib493" title="" class="ltx_ref">493</a>]</cite>.
Similarly, LLMs can exhibit decent performance even if the prompt template is irrelevant or misleading&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib495" title="" class="ltx_ref">495</a>]</cite>.</p>
</div>
<div id="S6.SS2.SSS3.p5" class="ltx_para">
<p id="S6.SS2.SSS3.p5.1" class="ltx_p"><math id="S6.SS2.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS2.SSS3.p5.1.m1.1a"><mo id="S6.SS2.SSS3.p5.1.m1.1.1" xref="S6.SS2.SSS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p5.1.m1.1b"><ci id="S6.SS2.SSS3.p5.1.m1.1.1.cmml" xref="S6.SS2.SSS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p5.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S6.SS2.SSS3.p5.1.1" class="ltx_emph ltx_font_italic">Task learning.</em>
In the second way, LLMs learn new tasks unseen in the pre-training stage only through demonstrations.
Specially, task learning is analyzed mainly from the perspective of gradient descent and considered as implicit fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib496" title="" class="ltx_ref">496</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>.
Then, ICL can be explained as follows: by means of forward computation, LLMs generate meta-gradients with respect to demonstrations and implicitly perform gradient descent via the attention mechanism.
Experiments also show that certain attention heads in LLMs are capable of performing task-agnostic atomic operations&nbsp;(<em id="S6.SS2.SSS3.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> copying and prefix matching), which are closely related to the ICL ability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib497" title="" class="ltx_ref">497</a>]</cite>.
Furthermore, some studies abstract ICL as an algorithm learning process&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib498" title="" class="ltx_ref">498</a>]</cite>.
For example, the authors in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib498" title="" class="ltx_ref">498</a>]</cite> find that LLMs essentially encode implicit models through their parameters during pre-training.
With the examples provided in ICL, LLMs can implement learning algorithms such as gradient descent or directly compute the closed-form solution to update these models during forward computation.
Under this explanation framework, it has been shown that LLMs can effectively learn simple linear functions and even some complex functions like decision trees with ICL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib498" title="" class="ltx_ref">498</a>]</cite>.</p>
</div>
<div id="S6.SS2.SSS3.p6" class="ltx_para">
<p id="S6.SS2.SSS3.p6.1" class="ltx_p">As discussed in a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib493" title="" class="ltx_ref">493</a>]</cite>, LLMs exhibit the abilities of both task recognition and task learning in ICL, but the two abilities seem to be possessed with different model scales.
As shown in the experiments&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib493" title="" class="ltx_ref">493</a>]</cite>, the ability of task recognition is easier to obtain, and even a small LM with only 350M parameters can exhibit this ability, while task learning can only emerge for LLMs with at least 66B parameters.
Another study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib499" title="" class="ltx_ref">499</a>]</cite> also supports this finding with specially designed experiments.
They set up the tasks with flipped and semantically unrelated labels in the experiment, which require task learning when performing ICL.
The results suggest that small LMs tend to disregard the labels and mainly depend on their prior knowledge to accomplish the task, while LLMs have the ability to surpass their prior knowledge and acquire new knowledge from demonstrations, resulting in better outcomes.
Furthermore, to improve the task learning ability, Meta-In-Context Learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib500" title="" class="ltx_ref">500</a>]</cite> proposes to include multiple related tasks instead of just a single one in the prompt.
In addition, Symbol Tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib501" title="" class="ltx_ref">501</a>]</cite> fine-tunes LLMs on demonstrations with semantically unrelated labels (<em id="S6.SS2.SSS3.p6.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> foo/bar instead of positive/negative for sentiment analysis), forcing LLMs to learn the task from demonstrations instead of relying on prior knowledge.</p>
</div>
<figure id="S6.F15" class="ltx_figure"><img src="/html/2303.18223/assets/x15.png" id="S6.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>
An illustration of the evolution of CoT prompting strategies. It begins with the basic CoT approach and progresses to enhanced CoT generation techniques, including sampling-based and verification-based methods. Finally, it extends to variations of the chain structure, such as trees and graphs. Here, “thought” refers to an intermediate reasoning step as stated in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib451" title="" class="ltx_ref">451</a>]</cite>.
</figcaption>
</figure>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span id="S6.SS3.1.1" class="ltx_text ltx_font_italic">Chain-of-Thought Prompting</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Chain-of-Thought&nbsp;(CoT) prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib502" title="" class="ltx_ref">502</a>]</cite> is an improved prompting strategy to boost the performance of LLMs on complex reasoning tasks, such as arithmetic reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib503" title="" class="ltx_ref">503</a>]</cite>, commonsense reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib504" title="" class="ltx_ref">504</a>]</cite>, and symbolic reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
Instead of simply constructing the prompts with input-output pairs like ICL, CoT prompting further incorporates intermediate reasoning steps, which serve as the bridge between inputs and outputs.

Figure&nbsp;<a href="#S6.F14" title="Figure 14 ‣ 6.2.1 ICL Formulation ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> presents an illustration of CoT.
In the following part, we will first elaborate on the basic CoT prompting approach and its improved strategies, then discuss when and why CoT prompting works.</p>
</div>
<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1 </span>Basic CoT Prompting Approach</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.6" class="ltx_p">CoT prompting is first proposed as an extension of ICL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which augments each demonstration <math id="S6.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S6.SS3.SSS1.p1.1.m1.1a"><mo stretchy="false" id="S6.SS3.SSS1.p1.1.m1.1.1" xref="S6.SS3.SSS1.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.1.m1.1b"><ci id="S6.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS1.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.1.m1.1c">\langle</annotation></semantics></math><em id="S6.SS3.SSS1.p1.6.1" class="ltx_emph ltx_font_italic">input, output</em><math id="S6.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S6.SS3.SSS1.p1.2.m2.1a"><mo stretchy="false" id="S6.SS3.SSS1.p1.2.m2.1.1" xref="S6.SS3.SSS1.p1.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.2.m2.1b"><ci id="S6.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS1.p1.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.2.m2.1c">\rangle</annotation></semantics></math> as <math id="S6.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S6.SS3.SSS1.p1.3.m3.1a"><mo stretchy="false" id="S6.SS3.SSS1.p1.3.m3.1.1" xref="S6.SS3.SSS1.p1.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.3.m3.1b"><ci id="S6.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S6.SS3.SSS1.p1.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.3.m3.1c">\langle</annotation></semantics></math><em id="S6.SS3.SSS1.p1.6.2" class="ltx_emph ltx_font_italic">input, CoT, output</em><math id="S6.SS3.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S6.SS3.SSS1.p1.4.m4.1a"><mo stretchy="false" id="S6.SS3.SSS1.p1.4.m4.1.1" xref="S6.SS3.SSS1.p1.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.4.m4.1b"><ci id="S6.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S6.SS3.SSS1.p1.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.4.m4.1c">\rangle</annotation></semantics></math>.
A <span id="S6.SS3.SSS1.p1.6.3" class="ltx_text ltx_font_italic">CoT</span> is a series of intermediate reasoning steps for connecting the <span id="S6.SS3.SSS1.p1.6.4" class="ltx_text ltx_font_italic">input</span> and <span id="S6.SS3.SSS1.p1.6.5" class="ltx_text ltx_font_italic">output</span>.
With these augmented demonstrations, LLMs can follow them to generate CoTs and the answer for a new input.
However, unlike <math id="S6.SS3.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S6.SS3.SSS1.p1.5.m5.1a"><mo stretchy="false" id="S6.SS3.SSS1.p1.5.m5.1.1" xref="S6.SS3.SSS1.p1.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.5.m5.1b"><ci id="S6.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S6.SS3.SSS1.p1.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.5.m5.1c">\langle</annotation></semantics></math><em id="S6.SS3.SSS1.p1.6.6" class="ltx_emph ltx_font_italic">input, output</em><math id="S6.SS3.SSS1.p1.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S6.SS3.SSS1.p1.6.m6.1a"><mo stretchy="false" id="S6.SS3.SSS1.p1.6.m6.1.1" xref="S6.SS3.SSS1.p1.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p1.6.m6.1b"><ci id="S6.SS3.SSS1.p1.6.m6.1.1.cmml" xref="S6.SS3.SSS1.p1.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p1.6.m6.1c">\rangle</annotation></semantics></math> pairs in ICL, CoTs are difficult to obtain and usually require human annotation.
Fortunately, it has been found that LLMs can be triggered to generate CoTs through simple instructions like “<em id="S6.SS3.SSS1.p1.6.7" class="ltx_emph ltx_font_italic">Let’s think step by step.</em>”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib505" title="" class="ltx_ref">505</a>]</cite>, making CoT prompting easy to use.
There are also alternative magic prompts that can elicit the ability of CoT reasoning and further improve the performance of LLMs, such as “<em id="S6.SS3.SSS1.p1.6.8" class="ltx_emph ltx_font_italic">Take a deep breath and work on this problem step-by-step.</em>”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib473" title="" class="ltx_ref">473</a>]</cite>.</p>
</div>
<div id="S6.SS3.SSS1.p2" class="ltx_para">
<p id="S6.SS3.SSS1.p2.1" class="ltx_p">As illustrated in Figure&nbsp;<a href="#S6.F15" title="Figure 15 ‣ 6.2.3 Underlying Mechanism ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, the generation process of CoT follows a chain structure in the basic CoT prompting approach, where LLMs generate CoTs step by step.
Typically, CoT takes the format of natural language text.
However, textual CoTs may not work well on complex tasks that require rigorous logic for reasoning.
Considering this, some work uses code&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib506" title="" class="ltx_ref">506</a>, <a href="#bib.bib507" title="" class="ltx_ref">507</a>]</cite> due to its structured and precise nature.
Furthermore, the authors in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib508" title="" class="ltx_ref">508</a>]</cite> propose to dynamically select text or code as the format of CoTs to combine their advantages.</p>
</div>
</section>
<section id="S6.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2 </span>Improved CoT Prompting Strategies</h4>

<div id="S6.SS3.SSS2.p1" class="ltx_para">
<p id="S6.SS3.SSS2.p1.1" class="ltx_p">Despite the performance improvement in complex reasoning tasks, CoT prompting still suffers from problems like incorrect reasoning and instability.
In this part, we first introduce how to design better CoT prompts and enhanced CoT generation strategies, and then introduce the extension of the basic chain structure of CoT.
Figure&nbsp;<a href="#S6.F15" title="Figure 15 ‣ 6.2.3 Underlying Mechanism ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> illustrates the evolution of representative CoT prompting strategies.</p>
</div>
<div id="S6.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS3.SSS2.p2.1" class="ltx_p"><span id="S6.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Better Prompt Design.</span>
Since CoT prompting relies on prompts to elicit the reasoning capabilities of LLMs, the design of prompts is critical to its performance.
As a direct approach, it is shown that using diverse CoTs (<em id="S6.SS3.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> multiple reasoning paths for each problem) can effectively enhance the performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib437" title="" class="ltx_ref">437</a>]</cite>.
Another intuitive idea is that prompts with more complex reasoning paths are more likely to elicit the reasoning ability of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib433" title="" class="ltx_ref">433</a>]</cite>, which can result in higher accuracy in generating correct answers.
However, all these approaches rely on annotated CoT datasets, which limits their use in practice.
To overcome this limitation, magic instructions such as “<em id="S6.SS3.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">Let’s think step by step</em>” can be used to automatically construct CoTs by prompting LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib434" title="" class="ltx_ref">434</a>]</cite>.</p>
</div>
<div id="S6.SS3.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S6.SS3.SSS2.p3.1" class="ltx_p"><span id="S6.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Enhanced CoT Generation.</span>

Since LLMs are prone to producing incorrect reasoning steps and exhibiting instability in the generation process, there are a number of studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib509" title="" class="ltx_ref">509</a>, <a href="#bib.bib436" title="" class="ltx_ref">436</a>]</cite> to improve the generation of CoT.
In this part, we will introduce two typical approaches to enhancing the generation of CoT: sampling- and verification-based methods.</p>
</div>
<div id="S6.SS3.SSS2.p4" class="ltx_para">
<p id="S6.SS3.SSS2.p4.2" class="ltx_p"><math id="S6.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS3.SSS2.p4.1.m1.1a"><mo id="S6.SS3.SSS2.p4.1.m1.1.1" xref="S6.SS3.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p4.1.m1.1b"><ci id="S6.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS3.SSS2.p4.2.1" class="ltx_emph ltx_font_italic">Sampling-based methods.</em>

LLMs are known to suffer from instability during inference, which can lead to unfaithfulness in the generated reasoning steps.
To address this issue, some work proposes to sample multiple reasoning paths instead of using greedy decoding.
As a representative solution, self-consistency&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib436" title="" class="ltx_ref">436</a>]</cite>
first generates several reasoning paths and then takes an ensemble over the corresponding answers, selecting the most consistent one through majority voting.
However, such a method can still lead to wrong answers when most of the reasoning paths are misled.
Considering this, the authors in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib433" title="" class="ltx_ref">433</a>]</cite> only vote on the <math id="S6.SS3.SSS2.p4.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.SSS2.p4.2.m2.1a"><mi id="S6.SS3.SSS2.p4.2.m2.1.1" xref="S6.SS3.SSS2.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p4.2.m2.1b"><ci id="S6.SS3.SSS2.p4.2.m2.1.1.cmml" xref="S6.SS3.SSS2.p4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p4.2.m2.1c">k</annotation></semantics></math> most complex reasoning paths based on their observation that reasoning paths with higher complexity (<em id="S6.SS3.SSS2.p4.2.2" class="ltx_emph ltx_font_italic">e.g.,</em> more reasoning steps) usually have better performance.
Furthermore, MCR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib510" title="" class="ltx_ref">510</a>]</cite> proposes referring to the steps from other reasoning paths when generating the next step, and performs reasoning across multiple reasoning paths to generate the final answer.</p>
</div>
<div id="S6.SS3.SSS2.p5" class="ltx_para">
<p id="S6.SS3.SSS2.p5.1" class="ltx_p"><math id="S6.SS3.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS3.SSS2.p5.1.m1.1a"><mo id="S6.SS3.SSS2.p5.1.m1.1.1" xref="S6.SS3.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p5.1.m1.1b"><ci id="S6.SS3.SSS2.p5.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS3.SSS2.p5.1.1" class="ltx_emph ltx_font_italic">Verification-based methods.</em> 
The sequential nature of reasoning steps in CoTs can lead to the accumulation of errors in the generated CoTs when certain steps are incorrect.
To mitigate this problem, recent studies propose to verify the correctness of generated reasoning steps with either trained verifiers or LLMs themselves.
For example, DIVERSE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib509" title="" class="ltx_ref">509</a>]</cite> trains solution-level and step-level verifiers respectively to examine the reasoning steps at different granularities.
Another approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib511" title="" class="ltx_ref">511</a>]</cite> utilizes LLMs to verify the correctness of reasoning steps through step-by-step self-verification with a specially designed reasoning format.
In addition, several studies propose backward reasoning for verification:
it first deduces the necessary question conditions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib512" title="" class="ltx_ref">512</a>, <a href="#bib.bib513" title="" class="ltx_ref">513</a>]</cite> or variables&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib514" title="" class="ltx_ref">514</a>]</cite> from the model’s predictions, and then compares them with the original ones.</p>
</div>
<div id="S6.SS3.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S6.SS3.SSS2.p6.1" class="ltx_p"><span id="S6.SS3.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Reasoning Structure Extension.</span>

Despite the generality, the chain reasoning structure of basic CoT prompting limits its effectiveness in solving complex tasks, which require exploration like foresight and backtracking during inference.
Therefore, many studies have been devoted to extending the reasoning structure by designing more intricate thought processes, <em id="S6.SS3.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> tree- and graph-structured reasoning.</p>
</div>
<div id="S6.SS3.SSS2.p7" class="ltx_para">
<p id="S6.SS3.SSS2.p7.1" class="ltx_p"><math id="S6.SS3.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS3.SSS2.p7.1.m1.1a"><mo id="S6.SS3.SSS2.p7.1.m1.1.1" xref="S6.SS3.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p7.1.m1.1b"><ci id="S6.SS3.SSS2.p7.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS3.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Tree-structured reasoning.</em>
This approach (exemplified by Tree of Thoughts&nbsp;(ToT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib451" title="" class="ltx_ref">451</a>, <a href="#bib.bib515" title="" class="ltx_ref">515</a>]</cite>) formulates the reasoning process in a hierarchical tree structure, where intermediate thoughts are nodes.
In this way, it enables LLMs to explore multiple reasoning paths in parallel and further supports the operation of lookahead and backtracking to facilitate more comprehensive decisions.
In addition, TouT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib516" title="" class="ltx_ref">516</a>]</cite> takes the uncertainty of intermediate thoughts into account for thought evaluation based on Monte Carlo Dropout.</p>
</div>
<div id="S6.SS3.SSS2.p8" class="ltx_para">
<p id="S6.SS3.SSS2.p8.1" class="ltx_p"><math id="S6.SS3.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS3.SSS2.p8.1.m1.1a"><mo id="S6.SS3.SSS2.p8.1.m1.1.1" xref="S6.SS3.SSS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p8.1.m1.1b"><ci id="S6.SS3.SSS2.p8.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS3.SSS2.p8.1.1" class="ltx_emph ltx_font_italic">Graph-structured reasoning.</em> 
Although the tree structure facilitates parallel reasoning, it also imposes restrictions on the reasoning process.
With more complex topological structures, graphs offer greater flexibility in reasoning, enabling the characterization of more intricate relationships and interactions.
For instance, Graph of Thoughts&nbsp;(GoT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib517" title="" class="ltx_ref">517</a>, <a href="#bib.bib518" title="" class="ltx_ref">518</a>]</cite> conceptualizes the reasoning process as an arbitrary graph, where vertices denote intermediate thoughts and edges denote the interdependence between these thoughts.
Compared with ToT, it can further utilize thoughts from other reasoning paths when generating new thoughts.
However, such an approach requires a large number of interactions with LLMs, making the thought exploration process highly inefficient.


To reduce potentially meaningless thought exploration, XoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib519" title="" class="ltx_ref">519</a>]</cite> further proposes to guide the search of thoughts with pre-trained policy and value networks.</p>
</div>
</section>
<section id="S6.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.3 </span>Further Discussion on CoT Prompting</h4>

<div id="S6.SS3.SSS3.p1" class="ltx_para">
<p id="S6.SS3.SSS3.p1.1" class="ltx_p">In this part, we present discussions regarding two fundamental questions related to CoT prompting, <em id="S6.SS3.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> “<span id="S6.SS3.SSS3.p1.1.2" class="ltx_text ltx_font_italic">when does CoT prompting work for LLMs</span>” and “<span id="S6.SS3.SSS3.p1.1.3" class="ltx_text ltx_font_italic">why can LLMs perform CoT reasoning</span>”.</p>
</div>
<div id="S6.SS3.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS3.SSS3.p2.1" class="ltx_p"><span id="S6.SS3.SSS3.p2.1.1" class="ltx_text ltx_font_bold">When CoT Prompting Works For LLMs?</span>
Since CoT reasoning is an emergent ability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, it only has a positive effect on sufficiently large models (typically containing 10B or more parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>) but not on small models.
Moreover, since CoT prompting augments the standard prompting with intermediate reasoning steps, it is mainly effective for the tasks that require step-by-step reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, <em id="S6.SS3.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> arithmetic reasoning, commonsense reasoning, and symbolic reasoning.
Whereas, for other tasks that do not rely on complex reasoning, CoT prompting might lead to worse performance than standard prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib438" title="" class="ltx_ref">438</a>]</cite>, <em id="S6.SS3.SSS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> MNLI-m/mm, SST-2, and QQP from GLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib260" title="" class="ltx_ref">260</a>]</cite>.
Interestingly, it seems that the performance gain brought by CoT prompting could be significant only when standard prompting yields poor results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S6.SS3.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S6.SS3.SSS3.p3.1" class="ltx_p"><span id="S6.SS3.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Why LLMs Can Perform CoT Reasoning?</span>
As the second question, we discuss the underlying mechanism of CoT prompting in the following two aspects.</p>
</div>
<div id="S6.SS3.SSS3.p4" class="ltx_para">
<p id="S6.SS3.SSS3.p4.1" class="ltx_p"><math id="S6.SS3.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS3.SSS3.p4.1.m1.1a"><mo id="S6.SS3.SSS3.p4.1.m1.1.1" xref="S6.SS3.SSS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p4.1.m1.1b"><ci id="S6.SS3.SSS3.p4.1.m1.1.1.cmml" xref="S6.SS3.SSS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS3.SSS3.p4.1.1" class="ltx_emph ltx_font_italic">The source of CoT reasoning ability</em>.
Regarding the source of CoT reasoning capability, it is widely hypothesized that it can be attributed to training on code since models trained on it show a strong reasoning ability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib520" title="" class="ltx_ref">520</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib521" title="" class="ltx_ref">521</a>]</cite>.
Intuitively, code data is well organized with algorithmic logic and programming flow, which may be useful to improve the reasoning performance of LLMs.
However, this hypothesis still lacks publicly reported evidence of ablation experiments (<em id="S6.SS3.SSS3.p4.1.2" class="ltx_emph ltx_font_italic">with</em> and <em id="S6.SS3.SSS3.p4.1.3" class="ltx_emph ltx_font_italic">without</em> training on code).
In addition, instruction tuning seems not to be the key reason for obtaining the CoT reasoning ability, since
it has been empirically shown that instruction tuning on non-CoT data does not improve the performance on held-out CoT reasoning benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.</p>
</div>
<div id="S6.SS3.SSS3.p5" class="ltx_para">
<p id="S6.SS3.SSS3.p5.1" class="ltx_p"><math id="S6.SS3.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.SS3.SSS3.p5.1.m1.1a"><mo id="S6.SS3.SSS3.p5.1.m1.1.1" xref="S6.SS3.SSS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p5.1.m1.1b"><ci id="S6.SS3.SSS3.p5.1.m1.1.1.cmml" xref="S6.SS3.SSS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S6.SS3.SSS3.p5.1.1" class="ltx_emph ltx_font_italic">The effect of CoT prompting components</em>.
The major distinction between CoT prompting and standard prompting is the incorporation of reasoning paths prior to the final answer.
Thus, some researchers investigate the effects of different components in the reasoning paths.
Specifically, a recent study identifies three key components in CoT prompting, namely <em id="S6.SS3.SSS3.p5.1.2" class="ltx_emph ltx_font_italic">symbols</em>&nbsp;(<em id="S6.SS3.SSS3.p5.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> numerical quantities in arithmetic reasoning), <em id="S6.SS3.SSS3.p5.1.4" class="ltx_emph ltx_font_italic">patterns</em>&nbsp;(<em id="S6.SS3.SSS3.p5.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> equations in arithmetic reasoning), and <em id="S6.SS3.SSS3.p5.1.6" class="ltx_emph ltx_font_italic">text</em>&nbsp;(<em id="S6.SS3.SSS3.p5.1.7" class="ltx_emph ltx_font_italic">i.e.,</em> the rest of tokens that are not symbols or patterns)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib522" title="" class="ltx_ref">522</a>]</cite>.
It is shown that the latter two parts (<em id="S6.SS3.SSS3.p5.1.8" class="ltx_emph ltx_font_italic">i.e.,</em> patterns and text) are essential to the model performance, and removing either one would lead to a significant performance drop.
However, the correctness of symbols and patterns does not seem critical.
Further, there exists a symbiotic relationship between text and patterns: the text helps LLMs to generate useful patterns, and patterns aid LLMs to understand tasks and generate texts that help solve them&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib522" title="" class="ltx_ref">522</a>]</cite>.</p>
</div>
<div id="S6.SS3.SSS3.p6" class="ltx_para">
<p id="S6.SS3.SSS3.p6.1" class="ltx_p">In summary, CoT prompting provides a general and flexible approach to eliciting the reasoning ability of LLMs.
There are also some preliminary attempts to extend this technique to solve multimodal&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib523" title="" class="ltx_ref">523</a>]</cite> and multilingual tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib524" title="" class="ltx_ref">524</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span><span id="S6.SS4.1.1" class="ltx_text ltx_font_italic">Planning for Complex Task Solving</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Prompting with ICL and CoT is a conceptually simple yet general approach to solving various tasks.
However, this approach struggles with complex tasks like mathematical reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib525" title="" class="ltx_ref">525</a>]</cite> and multi-hop question answering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib526" title="" class="ltx_ref">526</a>]</cite>.
As an enhanced approach, prompt-based planning has been proposed to break down complex tasks into smaller sub-tasks and generate a plan of actions to accomplish the task.</p>
</div>
<section id="S6.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.1 </span>The Overall Framework</h4>

<figure id="S6.F16" class="ltx_figure"><img src="/html/2303.18223/assets/x16.png" id="S6.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>An illustration of the formulation for prompt based planning by LLMs for solving complex tasks.</figcaption>
</figure>
<div id="S6.SS4.SSS1.p1" class="ltx_para">
<p id="S6.SS4.SSS1.p1.1" class="ltx_p">In this part, we first formulate the general planning paradigm of LLMs for solving complex tasks, which is illustrated in Figure&nbsp;<a href="#S6.F16" title="Figure 16 ‣ 6.4.1 The Overall Framework ‣ 6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>.</p>
</div>
<div id="S6.SS4.SSS1.p2" class="ltx_para">
<p id="S6.SS4.SSS1.p2.1" class="ltx_p">In this paradigm, there are typically three components: <em id="S6.SS4.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">task planner</em>, <em id="S6.SS4.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">plan executor</em>, and <em id="S6.SS4.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">environment</em><span id="footnote43" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">43</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">43</sup><span class="ltx_tag ltx_tag_note">43</span>
Despite the similarity with RL, our formulation decouples the planning and execution phases, whereas in RL, they are typically interleaved in the agent.
This paradigm is defined in a general yet slightly loose way, and it mainly aims to help readers understand the key idea underlying the planning approaches of LLMs.
</span></span></span>.
Specifically, task planner, which is played by LLMs, aims to generate the whole plan to solve a target task.
The plan can be presented in various forms, <em id="S6.SS4.SSS1.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> an action sequence in the form of natural language&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib439" title="" class="ltx_ref">439</a>]</cite> or an executable program written in programming language&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>.
The LLM-based task planner can be enhanced with the memory mechanism for plan storage and retrieval, which is helpful for long-horizon tasks.
Then, plan executor is responsible for executing the actions in the plan.
It can be implemented by models like LLMs for textual tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib441" title="" class="ltx_ref">441</a>]</cite> or by tools like code interpreters for coding tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref">450</a>]</cite>.
Furthermore, environment refers to where the plan executor carries out the actions, which can be set differently according to specific tasks, <em id="S6.SS4.SSS1.p2.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> the LLM itself&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib527" title="" class="ltx_ref">527</a>]</cite> or an external virtual world like Minecraft&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib528" title="" class="ltx_ref">528</a>]</cite>.
It provides <span id="S6.SS4.SSS1.p2.1.6" class="ltx_text ltx_font_italic">feedback</span> about the execution result of the action to the task planner, either in the form of natural language&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref">450</a>]</cite> or from other multimodal signals&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib446" title="" class="ltx_ref">446</a>]</cite>.</p>
</div>
<div id="S6.SS4.SSS1.p3" class="ltx_para">
<p id="S6.SS4.SSS1.p3.1" class="ltx_p">For solving a complex task, the task planner first needs to clearly understand the task goal and generate a reasonable plan based on the reasoning of LLMs (See Section&nbsp;<a href="#S6.SS4.SSS2" title="6.4.2 Plan Generation ‣ 6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4.2</span></a>).
Then, the plan executor acts according to the plan in the environment, and the environment will produce feedback for the task planner (See Section&nbsp;<a href="#S6.SS4.SSS3" title="6.4.3 Feedback Acquisition ‣ 6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4.3</span></a>).
The task planner can further incorporate the feedback obtained from the environment to refine its initial plan and iteratively perform the above process to get better results as the task solution (See Section&nbsp;<a href="#S6.SS4.SSS4" title="6.4.4 Plan Refinement ‣ 6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4.4</span></a>).</p>
</div>
</section>
<section id="S6.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.2 </span>Plan Generation</h4>

<div id="S6.SS4.SSS2.p1" class="ltx_para">
<p id="S6.SS4.SSS2.p1.1" class="ltx_p">Plan generation focuses on directly generating action sequences by prompting LLMs.
Based on the format of the generated plans, existing work can be divided into two groups: text-based and code-based approaches.</p>
</div>
<div id="S6.SS4.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS2.p2.1" class="ltx_p"><span id="S6.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Text-based Approaches.</span>
It is straightforward for LLMs to generate plans in the form of natural language.
In this approach, LLMs are prompted to generate a sequence of actions for the plan executor to perform and solve the complex task.
For example, Plan-and-Solve&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib441" title="" class="ltx_ref">441</a>]</cite> adds explicit instructions like “<span id="S6.SS4.SSS2.p2.1.2" class="ltx_text ltx_font_typewriter">devise a plan</span>” to directly prompt the LLM for planning in a zero-shot manner, while Self-planning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib529" title="" class="ltx_ref">529</a>]</cite> and DECOMP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib440" title="" class="ltx_ref">440</a>]</cite> add demonstrations in the prompt to guide the LLM to devise a plan through ICL.
Following this way, some work further considers incorporating extra tools or models when planning.
For example, ToolFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> first annotates a pre-training corpus with potential API calls using LLMs, and then fine-tunes LLMs on it, so that LLMs can learn when and how to call APIs and incorporate the results returned by APIs during generation.
HuggingGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib444" title="" class="ltx_ref">444</a>]</cite> introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution.</p>
</div>
<div id="S6.SS4.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS2.p3.1" class="ltx_p"><span id="S6.SS4.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Code-based Approaches.</span>
Although text-based approaches sound intuitive, they cannot guarantee faithful execution of the plan, which may lead to failure even when the plan is sound.
To address this issue, code-based approaches have been proposed to generate more verifiable plans in the form of executable code in programming languages, <em id="S6.SS4.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Python or PDDL.
In this way, LLMs are first prompted to generate the program and then utilize a deterministic solver to execute it.
For example, Faithful CoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib442" title="" class="ltx_ref">442</a>]</cite> and PAL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite> decompose a reasoning task into two stages: at the first stage, the LLM generates a plan conditioned on the query; at the second stage, a deterministic solver executes the plan to derive the final answer.
Furthermore, code-based approaches can be applied to embodied agents in a similar way.
For example, PROGPROMPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib530" title="" class="ltx_ref">530</a>]</cite> and LLM+P&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib531" title="" class="ltx_ref">531</a>]</cite> first utilize LLMs to generate plans in the form of python functions or PDDL files, and then leverage a virtual agent or classical planner to solve the problem according to the code-based plans.</p>
</div>
</section>
<section id="S6.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.3 </span>Feedback Acquisition</h4>

<div id="S6.SS4.SSS3.p1" class="ltx_para">
<p id="S6.SS4.SSS3.p1.1" class="ltx_p">After executing the generated plan, the environment would produce the feedback signal to the LLM-based task planner, which can be used to refine its initial plan for better results.
In existing work, there are typically two sources of feedback from the environment, depending on their relationship with the LLM-based task planner: internal (<em id="S6.SS4.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> the LLM itself) and external (<em id="S6.SS4.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> tools or virtual worlds) feedback.</p>
</div>
<div id="S6.SS4.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS3.p2.1" class="ltx_p"><span id="S6.SS4.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Internal Feedback.</span>
The LLM itself can be utilized as a feedback provider.
One straightforward way is to directly evaluate the quality of the generated plans through prompting.
For example, RAP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib447" title="" class="ltx_ref">447</a>]</cite> evaluate the likelihood that each candidate plan can lead to task success, while Tree of Thoughts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib527" title="" class="ltx_ref">527</a>]</cite> proposes to vote across plans by making comparisons between them.
Further, LLMs can provide feedback based on the intermediate results from the plan executor.
For example, Reflexion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref">450</a>]</cite> utilizes LLMs to transform sparse result signals (<em id="S6.SS4.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> success or failure) into concrete text-based feedback (<em id="S6.SS4.SSS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> “<em id="S6.SS4.SSS3.p2.1.4" class="ltx_emph ltx_font_italic">You should recommend comedies that the user mentions in the query instead of horror movies</em>”) and stores this feedback in long-term memory for future planning.</p>
</div>
<div id="S6.SS4.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS3.p3.1" class="ltx_p"><span id="S6.SS4.SSS3.p3.1.1" class="ltx_text ltx_font_bold">External Feedback.</span>
In addition to LLMs, external objects can also provide feedback signals.
For example, tools like code interpreters are widely used in programming tasks to provide real-time error messages&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref">450</a>]</cite>, models like stable diffusion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib532" title="" class="ltx_ref">532</a>]</cite> can be used in multimodal tasks to provide visual perception&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib446" title="" class="ltx_ref">446</a>]</cite>, and virtual worlds like Minecraft can provide immersive experiences&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib528" title="" class="ltx_ref">528</a>]</cite>.
Besides, some work (<em id="S6.SS4.SSS3.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Generative Agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib533" title="" class="ltx_ref">533</a>]</cite>) explores multi-agent collaboration in simulated environments, where each agent receives feedback not only from interaction with the environment but also from communication with other agents.</p>
</div>
</section>
<section id="S6.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.4 </span>Plan Refinement</h4>

<div id="S6.SS4.SSS4.p1" class="ltx_para">
<p id="S6.SS4.SSS4.p1.1" class="ltx_p">With access to feedback from the environment, the task planner can accordingly refine its current plan and iteratively go through the “<em id="S6.SS4.SSS4.p1.1.1" class="ltx_emph ltx_font_italic">planning – execution – refinement</em>” loop for better results.
In this part, we summarizes three major refinement approaches in existing work.</p>
</div>
<div id="S6.SS4.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS4.p2.1" class="ltx_p"><span id="S6.SS4.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Reasoning.</span>
The feedback data from the environment may not be directly suitable to be utilized by LLMs for plan refinement, <em id="S6.SS4.SSS4.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> containing irrelevant information or taking a non-language form.
To solve this, some work adds the explicit reasoning process to extract critical information from feedback&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref">449</a>, <a href="#bib.bib448" title="" class="ltx_ref">448</a>]</cite>.
For example, React&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref">449</a>]</cite> prompts LLMs with demonstrations to generate reasoning traces over feedback.
It has been widely used in autonomous agent projects, such as AutoGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib534" title="" class="ltx_ref">534</a>]</cite>, which can automatically reason over the observed feedback to revise the initial plan for solving various user requests.
However, these approaches typically fix the order of reasoning and planning.
To support flexible switching between the two processes for better performance, ChatCoT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib448" title="" class="ltx_ref">448</a>]</cite> further unifies the tool-augmented reasoning process into a multi-turn conversation between the LLM-based task planner and the tool-based environment.</p>
</div>
<div id="S6.SS4.SSS4.p3" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS4.p3.1" class="ltx_p"><span id="S6.SS4.SSS4.p3.1.1" class="ltx_text ltx_font_bold">Backtracking.</span>
Early methods mainly consider planning forward actions while maintaining the existing plan, thus likely leading to local optimal plans based on a short-term evaluation.
To solve this, Tree of Thoughts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib527" title="" class="ltx_ref">527</a>]</cite> allows backtracking with search algorithms like breadth-first and depth-first search to make global planning.
It refines the plan step by step by backtracking to the last state in the initial plan and choosing the next unexplored action.
Furthermore, some studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib535" title="" class="ltx_ref">535</a>, <a href="#bib.bib446" title="" class="ltx_ref">446</a>]</cite> utilize feedback signals to revise the entire plan.
For example, DEPS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib535" title="" class="ltx_ref">535</a>]</cite> selects a better plan according to feedback signals, while TIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib446" title="" class="ltx_ref">446</a>]</cite> adds feedback signals to prompts for the LLM-based planner to revise each step in the initial plan.</p>
</div>
<div id="S6.SS4.SSS4.p4" class="ltx_para ltx_noindent">
<p id="S6.SS4.SSS4.p4.1" class="ltx_p"><span id="S6.SS4.SSS4.p4.1.1" class="ltx_text ltx_font_bold">Memorization.</span>
In order to handle long-horizon tasks, it has become a key approach to aid plan refinement with <em id="S6.SS4.SSS4.p4.1.2" class="ltx_emph ltx_font_italic">long-term memory</em> in addition to utilizing the <em id="S6.SS4.SSS4.p4.1.3" class="ltx_emph ltx_font_italic">short-term memory</em> of LLMs through ICL.
For example, Reflexion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref">450</a>]</cite> stores the feedback from self-reflection into the memory, so previous feedback can be retrieved for plan refinement.
Generative Agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib533" title="" class="ltx_ref">533</a>]</cite> designs the memory stream mechanism as the core component of agents for action planning and reflection.
Further, the skill library mechanism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib528" title="" class="ltx_ref">528</a>, <a href="#bib.bib445" title="" class="ltx_ref">445</a>]</cite> is proposed to store successful plans in the library, which can be reused and synthesized as complex plans for novel tasks.
To implement the long-term memory mechanism, tools like vector databases (<em id="S6.SS4.SSS4.p4.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> milvus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib536" title="" class="ltx_ref">536</a>]</cite>) can be used to encode plans or feedbacks into high-dimensional vectors for efficient storage and retrieval at a large scale.
MemoryBank&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib537" title="" class="ltx_ref">537</a>]</cite> further proposes the memory updating mechanism to allow memory forgetting and strengthening following the Ebbinghaus Forgetting Curve theory.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Capacity and Evaluation</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">To examine the effectiveness and superiority of LLMs, a surge of tasks and benchmarks have been proposed for conducting empirical ability evaluation and analysis.
In this section, we first introduce three types of basic ability evaluation of LLMs for language generation and understanding, then present several advanced ability evaluations with more complicated settings or goals, and finally discuss existing benchmarks, evaluation approaches, and empirical analysis.</p>
</div>
<figure id="S7.T14" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XIV: </span>Representative basic and advanced abilities and corresponding representative datasets for evaluating.</figcaption>
<table id="S7.T14.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T14.1.1" class="ltx_tr">
<td id="S7.T14.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Level</span></td>
<td id="S7.T14.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Ability</span></td>
<td id="S7.T14.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></td>
<td id="S7.T14.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></td>
</tr>
<tr id="S7.T14.1.2" class="ltx_tr">
<td id="S7.T14.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="25"><span id="S7.T14.1.2.1.1" class="ltx_text" style="font-size:80%;">Basic</span></td>
<td id="S7.T14.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="6"><span id="S7.T14.1.2.2.1" class="ltx_text" style="font-size:80%;">Language Generation</span></td>
<td id="S7.T14.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.2.3.1" class="ltx_text" style="font-size:80%;">Language Modeling</span></td>
<td id="S7.T14.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.2.4.1" class="ltx_text" style="font-size:80%;">Penn Treebank&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.2.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib538" title="" class="ltx_ref">538</a><span id="S7.T14.1.2.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.2.4.4" class="ltx_text" style="font-size:80%;">, WikiText-103&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.2.4.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib539" title="" class="ltx_ref">539</a><span id="S7.T14.1.2.4.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.2.4.7" class="ltx_text" style="font-size:80%;">, the Pile&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.2.4.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib161" title="" class="ltx_ref">161</a><span id="S7.T14.1.2.4.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.2.4.10" class="ltx_text" style="font-size:80%;">, LAMBADA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.2.4.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib233" title="" class="ltx_ref">233</a><span id="S7.T14.1.2.4.12.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.3" class="ltx_tr">
<td id="S7.T14.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="S7.T14.1.3.1.1" class="ltx_text" style="font-size:80%;">Conditional Text Generation</span></td>
<td id="S7.T14.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.3.2.1" class="ltx_text" style="font-size:80%;">WMT’14,16,19,20,21,22&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib540" title="" class="ltx_ref">540</a>, <a href="#bib.bib541" title="" class="ltx_ref">541</a>, <a href="#bib.bib542" title="" class="ltx_ref">542</a>, <a href="#bib.bib543" title="" class="ltx_ref">543</a>, <a href="#bib.bib544" title="" class="ltx_ref">544</a>, <a href="#bib.bib545" title="" class="ltx_ref">545</a><span id="S7.T14.1.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.3.2.4" class="ltx_text" style="font-size:80%;">, Flores-101&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.3.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib546" title="" class="ltx_ref">546</a><span id="S7.T14.1.3.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.3.2.7" class="ltx_text" style="font-size:80%;">, DiaBLa&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.3.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib547" title="" class="ltx_ref">547</a><span id="S7.T14.1.3.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.3.2.10" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.4" class="ltx_tr">
<td id="S7.T14.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.4.1.1" class="ltx_text" style="font-size:80%;">CNN/DailyMail&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib548" title="" class="ltx_ref">548</a><span id="S7.T14.1.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.4.1.4" class="ltx_text" style="font-size:80%;">, XSum&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.4.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib549" title="" class="ltx_ref">549</a><span id="S7.T14.1.4.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.4.1.7" class="ltx_text" style="font-size:80%;">, WikiLingua&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.4.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib550" title="" class="ltx_ref">550</a><span id="S7.T14.1.4.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.5" class="ltx_tr">
<td id="S7.T14.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.5.1.1" class="ltx_text" style="font-size:80%;">OpenDialKG&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib551" title="" class="ltx_ref">551</a><span id="S7.T14.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.6" class="ltx_tr">
<td id="S7.T14.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S7.T14.1.6.1.1" class="ltx_text" style="font-size:80%;">Code Synthesis</span></td>
<td id="S7.T14.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.6.2.1" class="ltx_text" style="font-size:80%;">APPS&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib378" title="" class="ltx_ref">378</a><span id="S7.T14.1.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.6.2.4" class="ltx_text" style="font-size:80%;">, HumanEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.6.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib105" title="" class="ltx_ref">105</a><span id="S7.T14.1.6.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.6.2.7" class="ltx_text" style="font-size:80%;">, MBPP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.6.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib208" title="" class="ltx_ref">208</a><span id="S7.T14.1.6.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.6.2.10" class="ltx_text" style="font-size:80%;">, CodeContest&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.6.2.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib114" title="" class="ltx_ref">114</a><span id="S7.T14.1.6.2.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.6.2.13" class="ltx_text" style="font-size:80%;">,
MTPB&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.6.2.14.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="S7.T14.1.6.2.15.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.6.2.16" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.7" class="ltx_tr">
<td id="S7.T14.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.7.1.1" class="ltx_text" style="font-size:80%;">DS-1000&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib552" title="" class="ltx_ref">552</a><span id="S7.T14.1.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.7.1.4" class="ltx_text" style="font-size:80%;">, ODEX&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.7.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib553" title="" class="ltx_ref">553</a><span id="S7.T14.1.7.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.8" class="ltx_tr">
<td id="S7.T14.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="8"><span id="S7.T14.1.8.1.1" class="ltx_text" style="font-size:80%;">Knowledge Utilization</span></td>
<td id="S7.T14.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="S7.T14.1.8.2.1" class="ltx_text" style="font-size:80%;">Closed-Book QA</span></td>
<td id="S7.T14.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.8.3.1" class="ltx_text" style="font-size:80%;">Natural Questions&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.8.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib554" title="" class="ltx_ref">554</a><span id="S7.T14.1.8.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.8.3.4" class="ltx_text" style="font-size:80%;">,
ARC&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.8.3.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib555" title="" class="ltx_ref">555</a><span id="S7.T14.1.8.3.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.8.3.7" class="ltx_text" style="font-size:80%;">,
TruthfulQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.8.3.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib556" title="" class="ltx_ref">556</a><span id="S7.T14.1.8.3.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.8.3.10" class="ltx_text" style="font-size:80%;">,
Web Questions&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.8.3.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib557" title="" class="ltx_ref">557</a><span id="S7.T14.1.8.3.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.8.3.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.9" class="ltx_tr">
<td id="S7.T14.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.9.1.1" class="ltx_text" style="font-size:80%;">TriviaQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib558" title="" class="ltx_ref">558</a><span id="S7.T14.1.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.9.1.4" class="ltx_text" style="font-size:80%;">,
PIQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.9.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib559" title="" class="ltx_ref">559</a><span id="S7.T14.1.9.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.9.1.7" class="ltx_text" style="font-size:80%;">,
LC-quad2.0&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.9.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib560" title="" class="ltx_ref">560</a><span id="S7.T14.1.9.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.9.1.10" class="ltx_text" style="font-size:80%;">,
GrailQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.9.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib561" title="" class="ltx_ref">561</a><span id="S7.T14.1.9.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.9.1.13" class="ltx_text" style="font-size:80%;">,
KQApro&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.9.1.14.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib562" title="" class="ltx_ref">562</a><span id="S7.T14.1.9.1.15.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.9.1.16" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.10" class="ltx_tr">
<td id="S7.T14.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.10.1.1" class="ltx_text" style="font-size:80%;">CWQ&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib563" title="" class="ltx_ref">563</a><span id="S7.T14.1.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.10.1.4" class="ltx_text" style="font-size:80%;">,
MKQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.10.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib564" title="" class="ltx_ref">564</a><span id="S7.T14.1.10.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.10.1.7" class="ltx_text" style="font-size:80%;">,
ScienceQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.10.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib565" title="" class="ltx_ref">565</a><span id="S7.T14.1.10.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.11" class="ltx_tr">
<td id="S7.T14.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="S7.T14.1.11.1.1" class="ltx_text" style="font-size:80%;">Open-Book QA</span></td>
<td id="S7.T14.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.11.2.1" class="ltx_text" style="font-size:80%;">Natural Questions&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.11.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib554" title="" class="ltx_ref">554</a><span id="S7.T14.1.11.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.11.2.4" class="ltx_text" style="font-size:80%;">,
OpenBookQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.11.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib566" title="" class="ltx_ref">566</a><span id="S7.T14.1.11.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.11.2.7" class="ltx_text" style="font-size:80%;">,
ARC&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.11.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib555" title="" class="ltx_ref">555</a><span id="S7.T14.1.11.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.11.2.10" class="ltx_text" style="font-size:80%;">,
TriviaQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.11.2.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib558" title="" class="ltx_ref">558</a><span id="S7.T14.1.11.2.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.11.2.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.12" class="ltx_tr">
<td id="S7.T14.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.12.1.1" class="ltx_text" style="font-size:80%;">Web Questions&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib557" title="" class="ltx_ref">557</a><span id="S7.T14.1.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.12.1.4" class="ltx_text" style="font-size:80%;">,
MS MARCO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.12.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib567" title="" class="ltx_ref">567</a><span id="S7.T14.1.12.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.12.1.7" class="ltx_text" style="font-size:80%;">,
QASC&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.12.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib568" title="" class="ltx_ref">568</a><span id="S7.T14.1.12.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.12.1.10" class="ltx_text" style="font-size:80%;">,
SQuAD&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.12.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib569" title="" class="ltx_ref">569</a><span id="S7.T14.1.12.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.12.1.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.13" class="ltx_tr">
<td id="S7.T14.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.13.1.1" class="ltx_text" style="font-size:80%;">WikiMovies&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib570" title="" class="ltx_ref">570</a><span id="S7.T14.1.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.14" class="ltx_tr">
<td id="S7.T14.1.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S7.T14.1.14.1.1" class="ltx_text" style="font-size:80%;">Knowledge Completion</span></td>
<td id="S7.T14.1.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.14.2.1" class="ltx_text" style="font-size:80%;">WikiFact&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.14.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib571" title="" class="ltx_ref">571</a><span id="S7.T14.1.14.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.14.2.4" class="ltx_text" style="font-size:80%;">,
FB15k-237&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.14.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib572" title="" class="ltx_ref">572</a><span id="S7.T14.1.14.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.14.2.7" class="ltx_text" style="font-size:80%;">,
Freebase&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.14.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib573" title="" class="ltx_ref">573</a><span id="S7.T14.1.14.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.14.2.10" class="ltx_text" style="font-size:80%;">,
WN18RR&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.14.2.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib574" title="" class="ltx_ref">574</a><span id="S7.T14.1.14.2.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.14.2.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.15" class="ltx_tr">
<td id="S7.T14.1.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.15.1.1" class="ltx_text" style="font-size:80%;">WordNet&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.15.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib575" title="" class="ltx_ref">575</a><span id="S7.T14.1.15.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.15.1.4" class="ltx_text" style="font-size:80%;">,
LAMA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.15.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib576" title="" class="ltx_ref">576</a><span id="S7.T14.1.15.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.15.1.7" class="ltx_text" style="font-size:80%;">,
YAGO3-10&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.15.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib577" title="" class="ltx_ref">577</a><span id="S7.T14.1.15.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.15.1.10" class="ltx_text" style="font-size:80%;">,
YAGO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.15.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib578" title="" class="ltx_ref">578</a><span id="S7.T14.1.15.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.16" class="ltx_tr">
<td id="S7.T14.1.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="11"><span id="S7.T14.1.16.1.1" class="ltx_text" style="font-size:80%;">Complex Reasoning</span></td>
<td id="S7.T14.1.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="4"><span id="S7.T14.1.16.2.1" class="ltx_text" style="font-size:80%;">Knowledge Reasoning</span></td>
<td id="S7.T14.1.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.16.3.1" class="ltx_text" style="font-size:80%;">CSQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.16.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib504" title="" class="ltx_ref">504</a><span id="S7.T14.1.16.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.16.3.4" class="ltx_text" style="font-size:80%;">, StrategyQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.16.3.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib185" title="" class="ltx_ref">185</a><span id="S7.T14.1.16.3.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.16.3.7" class="ltx_text" style="font-size:80%;">, HotpotQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.16.3.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib579" title="" class="ltx_ref">579</a><span id="S7.T14.1.16.3.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.16.3.10" class="ltx_text" style="font-size:80%;">, ARC&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.16.3.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib555" title="" class="ltx_ref">555</a><span id="S7.T14.1.16.3.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.16.3.13" class="ltx_text" style="font-size:80%;">, BoolQ&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.16.3.14.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib580" title="" class="ltx_ref">580</a><span id="S7.T14.1.16.3.15.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.16.3.16" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.17" class="ltx_tr">
<td id="S7.T14.1.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.17.1.1" class="ltx_text" style="font-size:80%;">PIQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.17.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib559" title="" class="ltx_ref">559</a><span id="S7.T14.1.17.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.17.1.4" class="ltx_text" style="font-size:80%;">,
SIQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.17.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib581" title="" class="ltx_ref">581</a><span id="S7.T14.1.17.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.17.1.7" class="ltx_text" style="font-size:80%;">, HellaSwag&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.17.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib582" title="" class="ltx_ref">582</a><span id="S7.T14.1.17.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.17.1.10" class="ltx_text" style="font-size:80%;">, WinoGrande&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.17.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib583" title="" class="ltx_ref">583</a><span id="S7.T14.1.17.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.17.1.13" class="ltx_text" style="font-size:80%;">,
COPA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.17.1.14.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib584" title="" class="ltx_ref">584</a><span id="S7.T14.1.17.1.15.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.17.1.16" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.18" class="ltx_tr">
<td id="S7.T14.1.18.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.18.1.1" class="ltx_text" style="font-size:80%;">OpenBookQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.18.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib566" title="" class="ltx_ref">566</a><span id="S7.T14.1.18.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.18.1.4" class="ltx_text" style="font-size:80%;">,
ScienceQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.18.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib565" title="" class="ltx_ref">565</a><span id="S7.T14.1.18.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.18.1.7" class="ltx_text" style="font-size:80%;">, proScript&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.18.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib585" title="" class="ltx_ref">585</a><span id="S7.T14.1.18.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.18.1.10" class="ltx_text" style="font-size:80%;">, ProPara&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.18.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib586" title="" class="ltx_ref">586</a><span id="S7.T14.1.18.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.18.1.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.19" class="ltx_tr">
<td id="S7.T14.1.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.19.1.1" class="ltx_text" style="font-size:80%;">ExplaGraphs&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.19.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib587" title="" class="ltx_ref">587</a><span id="S7.T14.1.19.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.19.1.4" class="ltx_text" style="font-size:80%;">,
ProofWriter&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.19.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib588" title="" class="ltx_ref">588</a><span id="S7.T14.1.19.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.19.1.7" class="ltx_text" style="font-size:80%;">, EntailmentBank&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.19.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib589" title="" class="ltx_ref">589</a><span id="S7.T14.1.19.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.19.1.10" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.20" class="ltx_tr">
<td id="S7.T14.1.20.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td id="S7.T14.1.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.20.2.1" class="ltx_text" style="font-size:80%;">ProOntoQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.20.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib590" title="" class="ltx_ref">590</a><span id="S7.T14.1.20.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.21" class="ltx_tr">
<td id="S7.T14.1.21.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="S7.T14.1.21.1.1" class="ltx_text" style="font-size:80%;">Symbolic Reasoning</span></td>
<td id="S7.T14.1.21.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.21.2.1" class="ltx_text" style="font-size:80%;">CoinFlip&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.21.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S7.T14.1.21.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.21.2.4" class="ltx_text" style="font-size:80%;">, ReverseList&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.21.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S7.T14.1.21.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.21.2.7" class="ltx_text" style="font-size:80%;">, LastLetter&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.21.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S7.T14.1.21.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.21.2.10" class="ltx_text" style="font-size:80%;">, Boolean Assignment&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.21.2.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib591" title="" class="ltx_ref">591</a><span id="S7.T14.1.21.2.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.21.2.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.22" class="ltx_tr">
<td id="S7.T14.1.22.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.22.1.1" class="ltx_text" style="font-size:80%;">Parity&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.22.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib591" title="" class="ltx_ref">591</a><span id="S7.T14.1.22.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.22.1.4" class="ltx_text" style="font-size:80%;">, Colored Object&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.22.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S7.T14.1.22.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.22.1.7" class="ltx_text" style="font-size:80%;">, Penguins in a Table&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.22.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S7.T14.1.22.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.22.1.10" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.23" class="ltx_tr">
<td id="S7.T14.1.23.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.23.1.1" class="ltx_text" style="font-size:80%;">Repeat Copy&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.23.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib443" title="" class="ltx_ref">443</a><span id="S7.T14.1.23.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.23.1.4" class="ltx_text" style="font-size:80%;">, Object Counting&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.23.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib443" title="" class="ltx_ref">443</a><span id="S7.T14.1.23.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.24" class="ltx_tr">
<td id="S7.T14.1.24.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="S7.T14.1.24.1.1" class="ltx_text" style="font-size:80%;">Mathematical Reasoning</span></td>
<td id="S7.T14.1.24.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.24.2.1" class="ltx_text" style="font-size:80%;">MATH&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.24.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib364" title="" class="ltx_ref">364</a><span id="S7.T14.1.24.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.24.2.4" class="ltx_text" style="font-size:80%;">, GSM8k&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.24.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib184" title="" class="ltx_ref">184</a><span id="S7.T14.1.24.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.24.2.7" class="ltx_text" style="font-size:80%;">, SVAMP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.24.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib592" title="" class="ltx_ref">592</a><span id="S7.T14.1.24.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.24.2.10" class="ltx_text" style="font-size:80%;">, MultiArith&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.24.2.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib593" title="" class="ltx_ref">593</a><span id="S7.T14.1.24.2.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.24.2.13" class="ltx_text" style="font-size:80%;">, ASDiv&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.24.2.14.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib503" title="" class="ltx_ref">503</a><span id="S7.T14.1.24.2.15.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.24.2.16" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.25" class="ltx_tr">
<td id="S7.T14.1.25.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.25.1.1" class="ltx_text" style="font-size:80%;">MathQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.25.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib594" title="" class="ltx_ref">594</a><span id="S7.T14.1.25.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.25.1.4" class="ltx_text" style="font-size:80%;">,
AQUA-RAT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.25.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib595" title="" class="ltx_ref">595</a><span id="S7.T14.1.25.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.25.1.7" class="ltx_text" style="font-size:80%;">, MAWPS&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.25.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib596" title="" class="ltx_ref">596</a><span id="S7.T14.1.25.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.25.1.10" class="ltx_text" style="font-size:80%;">, DROP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.25.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib597" title="" class="ltx_ref">597</a><span id="S7.T14.1.25.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.25.1.13" class="ltx_text" style="font-size:80%;">,</span>
</td>
</tr>
<tr id="S7.T14.1.26" class="ltx_tr">
<td id="S7.T14.1.26.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.26.1.1" class="ltx_text" style="font-size:80%;">NaturalProofs&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.26.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib598" title="" class="ltx_ref">598</a><span id="S7.T14.1.26.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.26.1.4" class="ltx_text" style="font-size:80%;">,
PISA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.26.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib599" title="" class="ltx_ref">599</a><span id="S7.T14.1.26.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.26.1.7" class="ltx_text" style="font-size:80%;">,
miniF2F&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.26.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib600" title="" class="ltx_ref">600</a><span id="S7.T14.1.26.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.26.1.10" class="ltx_text" style="font-size:80%;">, ProofNet&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.26.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib601" title="" class="ltx_ref">601</a><span id="S7.T14.1.26.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.27" class="ltx_tr">
<td id="S7.T14.1.27.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="18"><span id="S7.T14.1.27.1.1" class="ltx_text" style="font-size:80%;">Advanced</span></td>
<td id="S7.T14.1.27.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="4"><span id="S7.T14.1.27.2.1" class="ltx_text" style="font-size:80%;">Human Alignment</span></td>
<td id="S7.T14.1.27.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.27.3.1" class="ltx_text" style="font-size:80%;">Honestness</span></td>
<td id="S7.T14.1.27.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.27.4.1" class="ltx_text" style="font-size:80%;">TruthfulQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.27.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib556" title="" class="ltx_ref">556</a><span id="S7.T14.1.27.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.27.4.4" class="ltx_text" style="font-size:80%;">, HaluEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.27.4.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib602" title="" class="ltx_ref">602</a><span id="S7.T14.1.27.4.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.28" class="ltx_tr">
<td id="S7.T14.1.28.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S7.T14.1.28.1.1" class="ltx_text" style="font-size:80%;">Helpfulness</span></td>
<td id="S7.T14.1.28.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.28.2.1" class="ltx_text" style="font-size:80%;">HH-RLHF&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.28.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib170" title="" class="ltx_ref">170</a><span id="S7.T14.1.28.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.29" class="ltx_tr">
<td id="S7.T14.1.29.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S7.T14.1.29.1.1" class="ltx_text" style="font-size:80%;">Harmlessness</span></td>
<td id="S7.T14.1.29.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.29.2.1" class="ltx_text" style="font-size:80%;">HH-RLHF&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.29.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib170" title="" class="ltx_ref">170</a><span id="S7.T14.1.29.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.29.2.4" class="ltx_text" style="font-size:80%;">, Crows-Pairs&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.29.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib603" title="" class="ltx_ref">603</a><span id="S7.T14.1.29.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.30" class="ltx_tr">
<td id="S7.T14.1.30.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.30.1.1" class="ltx_text" style="font-size:80%;">WinoGender&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.30.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib604" title="" class="ltx_ref">604</a><span id="S7.T14.1.30.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.30.1.4" class="ltx_text" style="font-size:80%;">, RealToxicityPrompts&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.30.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib605" title="" class="ltx_ref">605</a><span id="S7.T14.1.30.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.31" class="ltx_tr">
<td id="S7.T14.1.31.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="3"><span id="S7.T14.1.31.1.1" class="ltx_text" style="font-size:80%;"><span id="S7.T14.1.31.1.1.1" class="ltx_text"></span> <span id="S7.T14.1.31.1.1.2" class="ltx_text">
<span id="S7.T14.1.31.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S7.T14.1.31.1.1.2.1.1" class="ltx_tr">
<span id="S7.T14.1.31.1.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">Interaction with</span></span>
<span id="S7.T14.1.31.1.1.2.1.2" class="ltx_tr">
<span id="S7.T14.1.31.1.1.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">External Environment</span></span>
</span></span> <span id="S7.T14.1.31.1.1.3" class="ltx_text"></span></span></td>
<td id="S7.T14.1.31.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.31.2.1" class="ltx_text" style="font-size:80%;">H</span><span id="S7.T14.1.31.2.2" class="ltx_text" style="font-size:80%;">ousehold</span>
</td>
<td id="S7.T14.1.31.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.31.3.1" class="ltx_text" style="font-size:80%;">VirtualHome&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.31.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib606" title="" class="ltx_ref">606</a><span id="S7.T14.1.31.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.31.3.4" class="ltx_text" style="font-size:80%;">, BEHAVIOR&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.31.3.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib607" title="" class="ltx_ref">607</a><span id="S7.T14.1.31.3.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.31.3.7" class="ltx_text" style="font-size:80%;">, ALFRED&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.31.3.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib608" title="" class="ltx_ref">608</a><span id="S7.T14.1.31.3.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.31.3.10" class="ltx_text" style="font-size:80%;">,ALFWorld&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.31.3.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib609" title="" class="ltx_ref">609</a><span id="S7.T14.1.31.3.12.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.32" class="ltx_tr">
<td id="S7.T14.1.32.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.32.1.1" class="ltx_text" style="font-size:80%;">W</span><span id="S7.T14.1.32.1.2" class="ltx_text" style="font-size:80%;">ebsite Environment</span>
</td>
<td id="S7.T14.1.32.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.32.2.1" class="ltx_text" style="font-size:80%;">WebShop&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.32.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib610" title="" class="ltx_ref">610</a><span id="S7.T14.1.32.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.32.2.4" class="ltx_text" style="font-size:80%;">, Mind2Web&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.32.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib611" title="" class="ltx_ref">611</a><span id="S7.T14.1.32.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.33" class="ltx_tr">
<td id="S7.T14.1.33.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.33.1.1" class="ltx_text" style="font-size:80%;">O</span><span id="S7.T14.1.33.1.2" class="ltx_text" style="font-size:80%;">pen World</span>
</td>
<td id="S7.T14.1.33.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.33.2.1" class="ltx_text" style="font-size:80%;">MineRL&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.33.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib612" title="" class="ltx_ref">612</a><span id="S7.T14.1.33.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.33.2.4" class="ltx_text" style="font-size:80%;">, MineDojo&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.33.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib613" title="" class="ltx_ref">613</a><span id="S7.T14.1.33.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.34" class="ltx_tr">
<td id="S7.T14.1.34.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="8"><span id="S7.T14.1.34.1.1" class="ltx_text" style="font-size:80%;">Tool Manipulation</span></td>
<td id="S7.T14.1.34.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.34.2.1" class="ltx_text" style="font-size:80%;">S</span><span id="S7.T14.1.34.2.2" class="ltx_text" style="font-size:80%;">earch Engine</span>
</td>
<td id="S7.T14.1.34.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.34.3.1" class="ltx_text" style="font-size:80%;">HotpotQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.34.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib579" title="" class="ltx_ref">579</a><span id="S7.T14.1.34.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.34.3.4" class="ltx_text" style="font-size:80%;">, TriviaQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.34.3.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib558" title="" class="ltx_ref">558</a><span id="S7.T14.1.34.3.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.34.3.7" class="ltx_text" style="font-size:80%;">, Natural Questions&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.34.3.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib554" title="" class="ltx_ref">554</a><span id="S7.T14.1.34.3.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.35" class="ltx_tr">
<td id="S7.T14.1.35.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.35.1.1" class="ltx_text" style="font-size:80%;">C</span><span id="S7.T14.1.35.1.2" class="ltx_text" style="font-size:80%;">ode Executor</span>
</td>
<td id="S7.T14.1.35.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.35.2.1" class="ltx_text" style="font-size:80%;">GSM8k&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.35.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib184" title="" class="ltx_ref">184</a><span id="S7.T14.1.35.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.35.2.4" class="ltx_text" style="font-size:80%;">, TabMWP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.35.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib614" title="" class="ltx_ref">614</a><span id="S7.T14.1.35.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.35.2.7" class="ltx_text" style="font-size:80%;">, Date Understanding&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.35.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S7.T14.1.35.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.36" class="ltx_tr">
<td id="S7.T14.1.36.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.36.1.1" class="ltx_text" style="font-size:80%;">C</span><span id="S7.T14.1.36.1.2" class="ltx_text" style="font-size:80%;">alculator</span>
</td>
<td id="S7.T14.1.36.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.36.2.1" class="ltx_text" style="font-size:80%;">GSM8k&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.36.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib184" title="" class="ltx_ref">184</a><span id="S7.T14.1.36.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.36.2.4" class="ltx_text" style="font-size:80%;">, MATH&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.36.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib364" title="" class="ltx_ref">364</a><span id="S7.T14.1.36.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.36.2.7" class="ltx_text" style="font-size:80%;">, CARP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.36.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib615" title="" class="ltx_ref">615</a><span id="S7.T14.1.36.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.37" class="ltx_tr">
<td id="S7.T14.1.37.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.37.1.1" class="ltx_text" style="font-size:80%;">M</span><span id="S7.T14.1.37.1.2" class="ltx_text" style="font-size:80%;">odel Interface</span>
</td>
<td id="S7.T14.1.37.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.37.2.1" class="ltx_text" style="font-size:80%;">GPT4Tools&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.37.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib616" title="" class="ltx_ref">616</a><span id="S7.T14.1.37.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.37.2.4" class="ltx_text" style="font-size:80%;">, Gorilla&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.37.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib617" title="" class="ltx_ref">617</a><span id="S7.T14.1.37.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.38" class="ltx_tr">
<td id="S7.T14.1.38.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;" rowspan="2"><span id="S7.T14.1.38.1.1" class="ltx_text" style="font-size:80%;">Data Interface</span></td>
<td id="S7.T14.1.38.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.38.2.1" class="ltx_text" style="font-size:80%;">WebQSP&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.38.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib618" title="" class="ltx_ref">618</a><span id="S7.T14.1.38.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.38.2.4" class="ltx_text" style="font-size:80%;">, MetaQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.38.2.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib619" title="" class="ltx_ref">619</a><span id="S7.T14.1.38.2.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.38.2.7" class="ltx_text" style="font-size:80%;">, WTQ&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.38.2.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib620" title="" class="ltx_ref">620</a><span id="S7.T14.1.38.2.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
<tr id="S7.T14.1.39" class="ltx_tr">
<td id="S7.T14.1.39.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.0pt;padding-right:0.0pt;">
<span id="S7.T14.1.39.1.1" class="ltx_text" style="font-size:80%;">WikiSQL&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.39.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib621" title="" class="ltx_ref">621</a><span id="S7.T14.1.39.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.39.1.4" class="ltx_text" style="font-size:80%;">, TabFact&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.39.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib622" title="" class="ltx_ref">622</a><span id="S7.T14.1.39.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S7.T14.1.39.1.7" class="ltx_text" style="font-size:80%;">, Spider&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T14.1.39.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib623" title="" class="ltx_ref">623</a><span id="S7.T14.1.39.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
</tbody></table>
</figure>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span><span id="S7.SS1.1.1" class="ltx_text ltx_font_italic">Basic Ability</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">In this part, we mainly focus on three basic types of ability evaluation for LLMs, <em id="S7.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> language generation, knowledge utilization, and complex reasoning.
It is noted that we do not intend to have complete coverage of all the related tasks, but instead only focus on the most widely discussed or studied tasks for LLMs. Next, we introduce these tasks in detail.</p>
</div>
<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Language Generation</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">According to the task definition, existing tasks about language generation can be roughly categorized into language modeling, conditional text generation, and code synthesis tasks. Note that code synthesis is not a typical NLP task, we include it for discussion because it can be directly solved by a number of LLMs (trained on code data) in a similar generation approach as natural language text.</p>
</div>
<div id="S7.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS1.p2.1" class="ltx_p"><span id="S7.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Language Modeling.</span>
As the most fundamental ability of LLMs, <em id="S7.SS1.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">language modeling</em> aims to predict the next token based on the previous tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which mainly focuses on the capacity of basic language understanding and generation. For evaluating such an ability, typical language modeling datasets that existing work uses include Penn Treebank&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib538" title="" class="ltx_ref">538</a>]</cite>, WikiText-103&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib539" title="" class="ltx_ref">539</a>]</cite>, and the Pile&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>, where the metric of <em id="S7.SS1.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">perplexity</em> is commonly used for evaluating the model performance under the zero-shot setting.
Empirical studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> show that LLMs bring substantial performance gains over the previous state-of-the-art methods on these evaluation datasets.
To better test the modeling capacity of long-range dependencies in text, the LAMBADA dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> has been introduced, where LLMs are required to predict the last word of sentences based on a paragraph of context.
Then, the accuracy and perplexity of the predicted last words are employed to evaluate LLMs.
As shown in existing work, the performance on the language modeling tasks typically follows the scaling law&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, which means that scaling language models would improve the accuracy and reduce the perplexity.</p>
</div>
<div id="S7.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS1.p3.1" class="ltx_p"><span id="S7.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Conditional Text Generation.</span>
As an important topic in language generation, conditional text generation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> focuses on generating texts satisfying specific task demands based on the given conditions, typically including machine translation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib624" title="" class="ltx_ref">624</a>]</cite>, text summarization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib548" title="" class="ltx_ref">548</a>]</cite>, and question answering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib557" title="" class="ltx_ref">557</a>]</cite>.
To measure the quality of the generated text, automatic metrics (<em id="S7.SS1.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Accuracy, BLEU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib625" title="" class="ltx_ref">625</a>]</cite> and ROUGE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib626" title="" class="ltx_ref">626</a>]</cite>) and human ratings have been typically used for evaluating the performance. 
Due to the powerful language generation capabilities, LLMs have achieved remarkable performance on existing datasets and benchmarks.
For instance, GPT-4 exhibits comparable performance as commercial translation products, even for the translation task of languages that are with significant linguistic distance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib627" title="" class="ltx_ref">627</a>]</cite>.
On news summarization tasks&nbsp;(<em id="S7.SS1.SSS1.p3.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> CNN/DM and XSUM), LLMs also demonstrate comparable performance with human freelance writers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib628" title="" class="ltx_ref">628</a>]</cite>.
Despite the rapid progress on model capacity, there are increasing concerns on the feasibility of existing automatic metrics to faithfully assess the performance of LLMs in conditional text generation tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib628" title="" class="ltx_ref">628</a>, <a href="#bib.bib629" title="" class="ltx_ref">629</a>, <a href="#bib.bib630" title="" class="ltx_ref">630</a>]</cite>.
As the alternatives to automatic metrics, recent studies also propose to incorporate LLMs as generation evaluators to examine the quality of the generated content&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib631" title="" class="ltx_ref">631</a>, <a href="#bib.bib632" title="" class="ltx_ref">632</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>.
Moreover, researchers also explore more challenging language generation tasks for LLMs, such as structured data generation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref">458</a>]</cite> and long text generation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib633" title="" class="ltx_ref">633</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib634" title="" class="ltx_ref">634</a>]</cite>.</p>
</div>
<div id="S7.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS1.p4.1" class="ltx_p"><span id="S7.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Code Synthesis.</span>
In addition to generating high-quality natural language text, existing LLMs also show strong abilities to generate formal language, especially computer programs (<em id="S7.SS1.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> code) that satisfy specific conditions, called <em id="S7.SS1.SSS1.p4.1.3" class="ltx_emph ltx_font_italic">code synthesis</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib635" title="" class="ltx_ref">635</a>]</cite>.
Unlike natural language generation, as the generated code can be directly checked by execution with corresponding compilers or interpreters, existing work mostly evaluates the quality of the generated code from LLMs by calculating the pass rate against the test cases, <em id="S7.SS1.SSS1.p4.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> pass@<math id="S7.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S7.SS1.SSS1.p4.1.m1.1a"><mi id="S7.SS1.SSS1.p4.1.m1.1.1" xref="S7.SS1.SSS1.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS1.p4.1.m1.1b"><ci id="S7.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S7.SS1.SSS1.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS1.p4.1.m1.1c">k</annotation></semantics></math><span id="footnote44" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">44</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">44</sup><span class="ltx_tag ltx_tag_note">44</span>Given <math id="footnote44.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="footnote44.m1.1b"><mi id="footnote44.m1.1.1" xref="footnote44.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote44.m1.1c"><ci id="footnote44.m1.1.1.cmml" xref="footnote44.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote44.m1.1d">k</annotation></semantics></math> programs generated by the LLM, pass@<math id="footnote44.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="footnote44.m2.1b"><mi id="footnote44.m2.1.1" xref="footnote44.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote44.m2.1c"><ci id="footnote44.m2.1.1.cmml" xref="footnote44.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote44.m2.1d">k</annotation></semantics></math> is computed as 1 when at least one program passes all test cases, or else 0</span></span></span>. Recently, several code benchmarks focusing on functional correctness are proposed to assess the code synthesis abilities of LLMs, such as APPS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib378" title="" class="ltx_ref">378</a>]</cite>, HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, and MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>.
Typically, they consist of diverse programming problems, with text specification and test cases for correctness checking.
To improve such an ability, it is key to fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>.
In addition, existing work has proposed new strategies to generate code, <em id="S7.SS1.SSS1.p4.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> sampling multiple candidate solutions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite> and planning-guided decoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib636" title="" class="ltx_ref">636</a>]</cite>, which can be considered as the imitation of bug-fixing and code-planning processes by programmers.
Impressively, LLMs have recently shown competitive performance with humans by achieving a ranking of the top 28% among users on the programming contest platform Codeforces&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>.
Further, GitHub Copilot has been released to assist programming in coding IDEs (<em id="S7.SS1.SSS1.p4.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> Visual Studio and JetBrains IDEs), which can support a variety of languages including Python, JavaScript, and Java.
A viewpoint article entitled “<em id="S7.SS1.SSS1.p4.1.7" class="ltx_emph ltx_font_italic">The End of Programming</em>”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib637" title="" class="ltx_ref">637</a>]</cite> in Communications of the ACM has discussed the impact of AI programming in the field of computer science, emphasizing an important shift towards the highly adaptive LLM as a new atomic unit of computation.</p>
</div>
<div id="S7.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS1.p5.1" class="ltx_p"><span id="S7.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Major Issues.</span> Although LLMs have achieved splendid performance in generating human-like text, they are susceptible to suffering from two major issues in language generation as discussed below.</p>
</div>
<div id="S7.SS1.SSS1.p6" class="ltx_para">
<p id="S7.SS1.SSS1.p6.1" class="ltx_p"><math id="S7.SS1.SSS1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS1.SSS1.p6.1.m1.1a"><mo id="S7.SS1.SSS1.p6.1.m1.1.1" xref="S7.SS1.SSS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS1.p6.1.m1.1b"><ci id="S7.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S7.SS1.SSS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS1.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS1.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">Unreliable generation evaluation.</em>

With the advancement of language generation ability of LLMs, existing studies find that
the generated texts from LLMs have reached a comparable quality to the reference texts on a variety of text generation tasks. However, due to the intrinsic weakness of existing evaluation benchmarks, there exists pronounced inconsistency between human evaluation and automatic reference-based metrics&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib628" title="" class="ltx_ref">628</a>, <a href="#bib.bib629" title="" class="ltx_ref">629</a>, <a href="#bib.bib630" title="" class="ltx_ref">630</a>, <a href="#bib.bib638" title="" class="ltx_ref">638</a>]</cite>. For example,
in OpenDialKG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib551" title="" class="ltx_ref">551</a>]</cite>, ChatGPT underperforms a fine-tuned GPT-2 on BLEU and ROUGE-L metrics, while earning more favor from human judgment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib638" title="" class="ltx_ref">638</a>]</cite>. Furthermore, existing work argues that even human evaluation may not be robust enough&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib629" title="" class="ltx_ref">629</a>, <a href="#bib.bib628" title="" class="ltx_ref">628</a>, <a href="#bib.bib639" title="" class="ltx_ref">639</a>, <a href="#bib.bib640" title="" class="ltx_ref">640</a>]</cite>.
In some cases, it is difficult to achieve a high level of consensus among human annotators&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib629" title="" class="ltx_ref">629</a>]</cite>, and there is also a large gap between the annotation quality of crowdworkers and experts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib640" title="" class="ltx_ref">640</a>, <a href="#bib.bib639" title="" class="ltx_ref">639</a>]</cite>.
Thus, how to conduct reliable evaluation for language generation tasks in the era of LLMs has become a fundamental yet challenging research topic.
Recently, increasing research work proposes to leverage LLMs to improve the evaluation quality of the generated texts.
Specially, LLMs can be used to improve the evaluation quality of existing metrics.
For example, Para-Ref&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib641" title="" class="ltx_ref">641</a>]</cite> augments various automatic metrics by leveraging LLMs to paraphrase existing references into semantically equivalent references with diverse expressions.
Further, LLMs are widely employed as the evaluators of text generation in a reference-free manner, including evaluating a single prediction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib631" title="" class="ltx_ref">631</a>, <a href="#bib.bib632" title="" class="ltx_ref">632</a>, <a href="#bib.bib642" title="" class="ltx_ref">642</a>]</cite> or comparing several candidates&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib643" title="" class="ltx_ref">643</a>, <a href="#bib.bib644" title="" class="ltx_ref">644</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib645" title="" class="ltx_ref">645</a>]</cite>.
Nevertheless, LLMs may expose bias (<em id="S7.SS1.SSS1.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> order bias or preference for LLM-generated texts over human-written texts) as language generation evaluators, demonstrating disparities when compared to human evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib646" title="" class="ltx_ref">646</a>, <a href="#bib.bib632" title="" class="ltx_ref">632</a>, <a href="#bib.bib647" title="" class="ltx_ref">647</a>]</cite>.</p>
</div>
<div id="S7.SS1.SSS1.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S7.SS1.SSS1.1.p1.pic1" class="ltx_picture" height="239.27" overflow="visible" version="1.1" width="276"><g transform="translate(0,239.27) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 233.36 C 0 236.62 2.64 239.27 5.91 239.27 L 270.1 239.27 C 273.36 239.27 276 236.62 276 233.36 L 276 5.91 C 276 2.64 273.36 0 270.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 217.85 L 274.04 217.85 L 274.04 5.91 C 274.04 3.73 272.27 1.97 270.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 223.75)"><foreignObject width="232.7" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S7.SS1.SSS1.1.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS1.1.p1.pic1.1.1.1.1.1.1" class="ltx_p">Unreliable Generation Evaluation</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="232.7" height="192.26" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S7.SS1.SSS1.1.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS1.1.p1.pic1.2.2.2.1.1.1" class="ltx_p">LLMs have been capable of generating texts with a comparable quality to human-written texts, which however might be underestimated by automatic reference-based metrics. As an alternative evaluation approach,
LLMs can serve as language generation evaluators to evaluate a single text, compare multiple candidates, and improve existing metrics. However, this evaluation approach still needs more inspections and examinations in real-world tasks.</span>
</span></foreignObject></g></g></svg>
</div>
<div id="S7.SS1.SSS1.p7" class="ltx_para">
<p id="S7.SS1.SSS1.p7.1" class="ltx_p"><math id="S7.SS1.SSS1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS1.SSS1.p7.1.m1.1a"><mo id="S7.SS1.SSS1.p7.1.m1.1.1" xref="S7.SS1.SSS1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS1.p7.1.m1.1b"><ci id="S7.SS1.SSS1.p7.1.m1.1.1.cmml" xref="S7.SS1.SSS1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS1.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS1.SSS1.p7.1.1" class="ltx_emph ltx_font_italic">Underperforming specialized generation</em>. Although LLMs have learned general language patterns to generate coherent text,
their proficiency in generation might be constrained when dealing with a specialized domain or task. For instance, a language model that has been trained on general web articles may face challenges when generating a medical report which involves many medical jargon and methods.
Intuitively, domain knowledge should be critical for model specialization. However, it is not easy to inject such specialized knowledge into LLMs.
As discussed in recent analyses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib648" title="" class="ltx_ref">648</a>]</cite>, when LLMs are trained to exhibit some specific ability that allows them to excel in some areas, they might struggle in others. Such an issue is related to <em id="S7.SS1.SSS1.p7.1.2" class="ltx_emph ltx_font_italic">catastrophic forgetting</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib649" title="" class="ltx_ref">649</a>, <a href="#bib.bib650" title="" class="ltx_ref">650</a>]</cite> in training neural networks, which refers to the conflict phenomenon of integrating new and old knowledge.
Similar cases also occur in human alignment of LLMs, where “<em id="S7.SS1.SSS1.p7.1.3" class="ltx_emph ltx_font_italic">alignment tax</em>”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> (<em id="S7.SS1.SSS1.p7.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> a potential loss in the in-context learning ability) has to be paid for aligning to human values and needs.

Moreover, due to the limitations of sequence modeling architecture, LLMs still face challenges in the understanding and generation of structured data. Consequently, they often fall behind task-specific models on complex structured data tasks, such as knowledge-base question answering and semantic parsing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib651" title="" class="ltx_ref">651</a>, <a href="#bib.bib458" title="" class="ltx_ref">458</a>]</cite>.

Therefore, it is important to develop effective model specialization methods that can flexibly adapt LLMs to various task scenarios, meanwhile retaining the original abilities as possible.</p>
</div>
<div id="S7.SS1.SSS1.2.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S7.SS1.SSS1.2.p1.pic1" class="ltx_picture" height="165.55" overflow="visible" version="1.1" width="276"><g transform="translate(0,165.55) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 159.64 C 0 162.9 2.64 165.55 5.91 165.55 L 270.1 165.55 C 273.36 165.55 276 162.9 276 159.64 L 276 5.91 C 276 2.64 273.36 0 270.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 131.17 L 274.04 131.17 L 274.04 5.91 C 274.04 3.73 272.27 1.97 270.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 137.08)"><foreignObject width="232.7" height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S7.SS1.SSS1.2.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS1.2.p1.pic1.1.1.1.1.1.1" class="ltx_p">Underperforming Specialized Generation</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="232.7" height="105.58" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S7.SS1.SSS1.2.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS1.2.p1.pic1.2.2.2.1.1.1" class="ltx_p">LLMs may fall short in mastering generation tasks that require domain-specific knowledge or generating structured data. It is non-trivial to inject specialized knowledge into LLMs, meanwhile maintaining the original abilities of LLMs.</span>
</span></foreignObject></g></g></svg>
</div>
<figure id="S7.F17" class="ltx_figure"><img src="/html/2303.18223/assets/x17.png" id="S7.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM gives a conflicting judgment about the relationship between Cindy and Amy, which contradicts the input.
For extrinsic hallucination, in this example, the LLM seems to have an incorrect understanding of the meaning of RLHF (reinforcement learning from human feedback), though it can correctly understand the meaning of LLMs (in this context).  </figcaption>
</figure>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2 </span>Knowledge Utilization</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">Knowledge utilization is an important ability of intelligent systems to accomplish knowledge-intensive tasks (<em id="S7.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> commonsense question answering and fact completion) based on supporting factual evidence.
Concretely, it requires LLMs to properly utilize the rich factual knowledge from the pre-training corpus or retrieve external data when necessary.
In particular, question answering&nbsp;(QA) and knowledge completion have been two commonly used tasks for evaluating this ability.
According to the test tasks (question answering or knowledge completion) and evaluation settings (<em id="S7.SS1.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">with</em> or <em id="S7.SS1.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">without</em> external resources), we categorize existing knowledge utilization tasks into three types, namely closed-book QA, open-book QA<span id="footnote45" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">45</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">45</sup><span class="ltx_tag ltx_tag_note">45</span>In this part, open-book QA refers to the QA tasks that require to extract and utilize useful information from external knowledge resources, as the antithesis of closed-book QA (only using the encoded information from pre-training corpus). Note that there is a dataset also named OpenBookQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib566" title="" class="ltx_ref">566</a>]</cite>, which follows the settings of open-book QA tasks by extracting and utilizing external science facts.</span></span></span>, and knowledge completion.</p>
</div>
<div id="S7.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS2.p2.1" class="ltx_p"><span id="S7.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Closed-Book QA.</span>
Closed-book QA tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib652" title="" class="ltx_ref">652</a>]</cite> test the acquired factual knowledge of LLMs from the pre-training corpus, where LLMs should answer the question only based on the given context without using external resources.
For evaluating this ability, there are several datasets that can be leveraged, including Natural Questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib554" title="" class="ltx_ref">554</a>]</cite>, Web Questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib557" title="" class="ltx_ref">557</a>]</cite>, and TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib558" title="" class="ltx_ref">558</a>]</cite>, where the accuracy metric is widely adopted.
Empirical results have revealed that LLMs can perform well in this setting and even match the performance of state-of-the-art open-domain QA systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Also, the performance of LLMs on closed-book QA tasks shows a scaling law pattern in terms of both model size and data size:
scaling the parameters and training tokens can increase the capacity of LLMs and help them learn (or memorize) more knowledge from the pre-training data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Further, under a similar parameter scale, LLMs with more pre-training data relevant to the evaluated tasks would achieve better performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.
Also, the closed-book QA setting provides a testbed for probing the accuracy of the factual knowledge encoded by LLMs.
However, as shown in existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, LLMs might perform less well on QA tasks relying on fine-grained knowledge, even when it exists in the pre-training data.</p>
</div>
<div id="S7.SS1.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS2.p3.1" class="ltx_p"><span id="S7.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Open-Book QA.</span> Unlike closed-book QA, in open-book QA tasks, LLMs can extract useful evidence from the external knowledge base or document collections, and then answer the question based on the extracted evidence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib653" title="" class="ltx_ref">653</a>, <a href="#bib.bib654" title="" class="ltx_ref">654</a>, <a href="#bib.bib655" title="" class="ltx_ref">655</a>, <a href="#bib.bib656" title="" class="ltx_ref">656</a>]</cite>.
Typical open-book QA datasets (<em id="S7.SS1.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Natural Questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib554" title="" class="ltx_ref">554</a>]</cite>, OpenBookQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib566" title="" class="ltx_ref">566</a>]</cite>, and SQuAD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib569" title="" class="ltx_ref">569</a>]</cite>) have overlap with closed-book QA datasets, but they incorporate external data sources, <em id="S7.SS1.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> Wikipedia.
The metrics of accuracy and F1 score are widely used in open-book QA tasks for evaluation.
To select relevant knowledge from external resources, LLMs are often paired with a text retriever (or even a search engine), which is trained independently or jointly with LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib653" title="" class="ltx_ref">653</a>, <a href="#bib.bib657" title="" class="ltx_ref">657</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.

Also, previous work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib658" title="" class="ltx_ref">658</a>, <a href="#bib.bib659" title="" class="ltx_ref">659</a>, <a href="#bib.bib660" title="" class="ltx_ref">660</a>]</cite> has indicated that retrievers can assist LLMs in verifying and rectifying the reasoning path.

In evaluation, existing studies mainly focus on testing how LLMs utilize the extracted knowledge to answer the question and show that the retrieved evidence can largely improve the accuracy of the generated answers, even enabling a smaller LLM to outperform <math id="S7.SS1.SSS2.p3.1.m1.1" class="ltx_math_unparsed" alttext="10\times" display="inline"><semantics id="S7.SS1.SSS2.p3.1.m1.1a"><mrow id="S7.SS1.SSS2.p3.1.m1.1b"><mn id="S7.SS1.SSS2.p3.1.m1.1.1">10</mn><mo lspace="0.222em" id="S7.SS1.SSS2.p3.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS1.SSS2.p3.1.m1.1c">10\times</annotation></semantics></math> larger ones&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib653" title="" class="ltx_ref">653</a>, <a href="#bib.bib657" title="" class="ltx_ref">657</a>]</cite>.
Further, open-book QA tasks can be also employed to evaluate the recency of knowledge information.
Pre-training or retrieving from outdated knowledge resources may cause LLMs to generate incorrect answers for time-sensitive questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib653" title="" class="ltx_ref">653</a>]</cite>.</p>
</div>
<div id="S7.SS1.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS2.p4.1" class="ltx_p"><span id="S7.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Knowledge Completion.</span>
In knowledge completion tasks, LLMs might be (to some extent) considered as a knowledge base&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib576" title="" class="ltx_ref">576</a>]</cite>, which can be leveraged to complete or predict the missing parts of knowledge units (<em id="S7.SS1.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> knowledge triples).
Such tasks can probe and evaluate <em id="S7.SS1.SSS2.p4.1.3" class="ltx_emph ltx_font_italic">how much</em> and <em id="S7.SS1.SSS2.p4.1.4" class="ltx_emph ltx_font_italic">what kind of</em> knowledge LLMs have learned from the pre-training data.
Existing knowledge completion tasks can be roughly divided into knowledge graph completion tasks (<em id="S7.SS1.SSS2.p4.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> FB15k-237&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib572" title="" class="ltx_ref">572</a>]</cite> and WN18RR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib574" title="" class="ltx_ref">574</a>]</cite>) and fact completion tasks (<em id="S7.SS1.SSS2.p4.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> WikiFact&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib571" title="" class="ltx_ref">571</a>]</cite>), which aim to complete the triples from a knowledge graph and incomplete sentences about specific facts, respectively.
Empirical studies have revealed that it is difficult for existing LLMs to accomplish knowledge completion tasks related to specific relation types&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib520" title="" class="ltx_ref">520</a>]</cite>.
As shown in the evaluation results on WikiFact, LLMs perform well on several frequent relations that occur in the pre-training data (<em id="S7.SS1.SSS2.p4.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> <span id="S7.SS1.SSS2.p4.1.8" class="ltx_text ltx_font_typewriter">currency</span> and <span id="S7.SS1.SSS2.p4.1.9" class="ltx_text ltx_font_typewriter">author</span>), while not well on rare ones (<em id="S7.SS1.SSS2.p4.1.10" class="ltx_emph ltx_font_italic">e.g.,</em> <span id="S7.SS1.SSS2.p4.1.11" class="ltx_text ltx_font_typewriter">discoverer_or_inventor</span> and <span id="S7.SS1.SSS2.p4.1.12" class="ltx_text ltx_font_typewriter">place_of_birth</span>).
Interestingly, under the same evaluation settings (<em id="S7.SS1.SSS2.p4.1.13" class="ltx_emph ltx_font_italic">e.g.,</em> in-context learning), InstructGPT (<em id="S7.SS1.SSS2.p4.1.14" class="ltx_emph ltx_font_italic">i.e.,</em> <span id="S7.SS1.SSS2.p4.1.15" class="ltx_text ltx_font_typewriter">text-davinci-002)</span> outperforms GPT-3 in all subsets of WikiFact.</p>
</div>
<div id="S7.SS1.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS2.p5.1" class="ltx_p"><span id="S7.SS1.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Major Issues</span>. Although LLMs have achieved key progress in capturing and utilizing knowledge information, they suffer from two major issues as discussed below.</p>
</div>
<div id="S7.SS1.SSS2.p6" class="ltx_para">
<p id="S7.SS1.SSS2.p6.1" class="ltx_p"><math id="S7.SS1.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS1.SSS2.p6.1.m1.1a"><mo id="S7.SS1.SSS2.p6.1.m1.1.1" xref="S7.SS1.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS2.p6.1.m1.1b"><ci id="S7.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S7.SS1.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS1.SSS2.p6.1.1" class="ltx_emph ltx_font_italic">Hallucination</em>. In generating factual texts, a challenging issue is <em id="S7.SS1.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">hallucination generations</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib638" title="" class="ltx_ref">638</a>, <a href="#bib.bib661" title="" class="ltx_ref">661</a>]</cite>, where the generated information is either in conflict with the existing source (<em id="S7.SS1.SSS2.p6.1.3" class="ltx_emph ltx_font_italic">intrinsic hallucination</em>) or cannot be verified by the available source (<em id="S7.SS1.SSS2.p6.1.4" class="ltx_emph ltx_font_italic">extrinsic hallucination</em>), which are illustrated by two examples in Figure&nbsp;<a href="#S7.F17" title="Figure 17 ‣ 7.1.1 Language Generation ‣ 7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>.
Hallucination widely occurs in existing LLMs, even the most superior LLMs such as GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.

Furthermore, existing work shows that LLMs encounter difficulties in recognizing the hallucinated content in text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib602" title="" class="ltx_ref">602</a>]</cite>, even the powerful ChatGPT.
Additionally, beyond language tasks, a recent study has shown that large vision-language models (LVLM) also face challenges with hallucination, <em id="S7.SS1.SSS2.p6.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> generating objects that are not present in the accompanying images&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib662" title="" class="ltx_ref">662</a>]</cite>.

In essence, LLMs seem to “unconsciously” utilize the knowledge in task solving, which still lack an ability to accurately control the use of internal or external knowledge.
Hallucinations would mislead LLMs to generate undesired outputs and mostly degrade the performance, leading to potential risks when deploying LLMs in real-world applications.
To alleviate this problem, alignment tuning strategies (as discussed in Section&nbsp;<a href="#S5.SS2" title="5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>) have been widely utilized in existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, which rely on tuning LLMs on high-quality data or using human feedback.

Moreover, the integration of external tools for the provision of credible information sources can help alleviate the hallucination issue&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib602" title="" class="ltx_ref">602</a>, <a href="#bib.bib659" title="" class="ltx_ref">659</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.
Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib663" title="" class="ltx_ref">663</a>, <a href="#bib.bib664" title="" class="ltx_ref">664</a>]</cite>.
For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheckGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib664" title="" class="ltx_ref">664</a>]</cite> detects hallucination by measuring information inconsistency within sampled outputs.

For the evaluation of the hallucination problem, a set of hallucination detection tasks have been proposed, <em id="S7.SS1.SSS2.p6.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> TruthfulQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib556" title="" class="ltx_ref">556</a>]</cite> for detecting human falsehood mimicked by models. More recently, HaluEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib602" title="" class="ltx_ref">602</a>]</cite> creates a large-scale LLM-generated and human-annotated
hallucinated samples to evaluate the ability of language models to recognize hallucination in both task-specific and general scenarios.</p>
<div id="S7.SS1.SSS2.p6.2" class="ltx_logical-block">
<div id="S7.SS1.SSS2.p6.2.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S7.SS1.SSS2.p6.2.p1.pic1" class="ltx_picture" height="208.75" overflow="visible" version="1.1" width="276"><g transform="translate(0,208.75) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 202.84 C 0 206.11 2.64 208.75 5.91 208.75 L 270.1 208.75 C 273.36 208.75 276 206.11 276 202.84 L 276 5.91 C 276 2.64 273.36 0 270.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 187.33 L 274.04 187.33 L 274.04 5.91 C 274.04 3.73 272.27 1.97 270.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 193.24)"><foreignObject width="232.7" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S7.SS1.SSS2.p6.2.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS2.p6.2.p1.pic1.1.1.1.1.1.1" class="ltx_p">Hallucination</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="232.7" height="161.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S7.SS1.SSS2.p6.2.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS2.p6.2.p1.pic1.2.2.2.1.1.1" class="ltx_p">LLMs are prone to generate untruthful information that either conflicts with the existing source or cannot be verified by the available source. Even the most powerful LLMs such as ChatGPT face great challenges in migrating the hallucinations of the generated texts.
This issue can be partially alleviated by special approaches such as alignment tuning and tool utilization.</span>
</span></foreignObject></g></g></svg>
</div>
</div>
</div>
<div id="S7.SS1.SSS2.p7" class="ltx_para">
<p id="S7.SS1.SSS2.p7.1" class="ltx_p"><math id="S7.SS1.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS1.SSS2.p7.1.m1.1a"><mo id="S7.SS1.SSS2.p7.1.m1.1.1" xref="S7.SS1.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS2.p7.1.m1.1b"><ci id="S7.SS1.SSS2.p7.1.m1.1.1.cmml" xref="S7.SS1.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS1.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Knowledge recency</em>. As another major challenge, LLMs would encounter difficulties when solving tasks that require the latest knowledge beyond the training data. To tackle this issue, a straightforward approach is to regularly update LLMs with new data.
However, it is very costly to fine-tune LLMs, and also likely to cause the catastrophic forgetting issue when incrementally training LLMs.
Therefore, it is necessary to develop efficient and effective approaches that can integrate new knowledge into existing LLMs, making them up-to-date.
Existing studies have explored how to utilize the external knowledge source (<em id="S7.SS1.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> search engine) to complement LLMs, which can be either jointly optimized with LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib653" title="" class="ltx_ref">653</a>]</cite> or used as a plug-and-play module&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib659" title="" class="ltx_ref">659</a>]</cite>. For instance, ChatGPT utilizes a retrieval plugin to access up-to-date information sources&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib665" title="" class="ltx_ref">665</a>]</cite>.
By incorporating the extracted relevant information into the context&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib666" title="" class="ltx_ref">666</a>, <a href="#bib.bib667" title="" class="ltx_ref">667</a>, <a href="#bib.bib668" title="" class="ltx_ref">668</a>]</cite>, LLMs can acquire new factual knowledge and perform better on relevant tasks.
However, such an approach seems to be still at a superficial level. In addition,
existing studies also explore editing parameters of language models to update intrinsic knowledge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib669" title="" class="ltx_ref">669</a>, <a href="#bib.bib670" title="" class="ltx_ref">670</a>, <a href="#bib.bib671" title="" class="ltx_ref">671</a>]</cite>.
Nevertheless, previous work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib672" title="" class="ltx_ref">672</a>]</cite> has shown that several parameter editing methods perform not well on LLMs, though they can improve the performance of small language models.
Therefore, it is still difficult to directly amend intrinsic knowledge or inject specific knowledge into LLMs, which remains an open research problem&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib672" title="" class="ltx_ref">672</a>]</cite>.
Recently, a useful framework <em id="S7.SS1.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">EasyEdit</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib673" title="" class="ltx_ref">673</a>]</cite> has been released to facilitate the research of knowledge editing for LLMs.</p>
</div>
<div id="S7.SS1.SSS2.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S7.SS1.SSS2.1.p1.pic1" class="ltx_picture" height="165.93" overflow="visible" version="1.1" width="276"><g transform="translate(0,165.93) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 160.03 C 0 163.29 2.64 165.93 5.91 165.93 L 270.1 165.93 C 273.36 165.93 276 163.29 276 160.03 L 276 5.91 C 276 2.64 273.36 0 270.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 141.82 L 274.04 141.82 L 274.04 5.91 C 274.04 3.73 272.27 1.97 270.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 147.73)"><foreignObject width="232.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S7.SS1.SSS2.1.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS2.1.p1.pic1.1.1.1.1.1.1" class="ltx_p">Knowledge Recency</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="232.7" height="116.23" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S7.SS1.SSS2.1.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS2.1.p1.pic1.2.2.2.1.1.1" class="ltx_p">The parametric knowledge of LLMs is hard to be updated in a timely manner.
Augmenting LLMs with external knowledge sources is a practical approach to tackling the issue.
However, how to effectively update knowledge within LLMs remains an open research problem.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3 </span>Complex Reasoning</h4>

<div id="S7.SS1.SSS3.p1" class="ltx_para">
<p id="S7.SS1.SSS3.p1.1" class="ltx_p">Complex reasoning refers to the ability of understanding and utilizing supporting evidence or logic to derive conclusions or make decisions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
According to the type of involved logic and evidence in the reasoning process,
we consider dividing existing evaluation tasks into three major categories, namely
knowledge reasoning, symbolic reasoning, and mathematical reasoning.</p>
</div>
<div id="S7.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS3.p2.1" class="ltx_p"><span id="S7.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Knowledge Reasoning.</span>
The knowledge reasoning tasks rely on logical relations and evidence about factual knowledge to answer the given question.
Existing work mainly uses specific datasets to evaluate the reasoning capacity of the corresponding type of knowledge, <em id="S7.SS1.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> CSQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib504" title="" class="ltx_ref">504</a>]</cite>/StrategyQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> for commonsense knowledge reasoning and ScienceQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib565" title="" class="ltx_ref">565</a>]</cite> for science knowledge reasoning.
In addition to the accuracy of the predicted results, existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib565" title="" class="ltx_ref">565</a>]</cite> has also evaluated the quality of the generated reasoning process, via automatic metrics (<em id="S7.SS1.SSS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> BLEU) or human evaluation.
Typically, these tasks require LLMs to perform step-by-step reasoning based on factual knowledge, until reaching the answer to the given question.
To elicit the step-by-step reasoning ability, chain-of-thought&nbsp;(CoT) prompting strategy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> has been proposed for enhancing the complex reasoning capacity of LLMs.
As discussed in Section&nbsp;<a href="#S6.SS3" title="6.3 Chain-of-Thought Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>, CoT involves the intermediate reasoning steps, which can be manually created&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> or automatically generated&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib674" title="" class="ltx_ref">674</a>]</cite>, into the prompts to guide LLMs to perform multi-step reasoning.
Such a way largely improves the reasoning performance of LLMs, leading to new state-of-the-art results on several complex knowledge reasoning tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib526" title="" class="ltx_ref">526</a>]</cite>.
Further, after reformulating knowledge reasoning tasks into code generation tasks, researchers have found that the performance of LLMs can be further improved&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>, especially with the LLMs pre-trained on code.
However, due to the complexity of knowledge reasoning tasks, the performance of current LLMs still lags behind human results on tasks such as commonsense reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib675" title="" class="ltx_ref">675</a>]</cite>.
As a common type of mistakes, LLMs might generate inaccurate intermediate steps, leading to a wrong final result.
To address this issue, existing work has proposed special decoding or ensemble strategies to improve the accuracy of the whole reasoning chain&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib436" title="" class="ltx_ref">436</a>, <a href="#bib.bib437" title="" class="ltx_ref">437</a>]</cite>.</p>
</div>
<div id="S7.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS3.p3.1" class="ltx_p"><span id="S7.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Symbolic Reasoning<span id="footnote46" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">46</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">46</sup><span class="ltx_tag ltx_tag_note"><span id="footnote46.1.1.1" class="ltx_text ltx_font_medium">46</span></span><span id="footnote46.5" class="ltx_text ltx_font_medium">Following&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="footnote46.6.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="footnote46.7.2" class="ltx_text ltx_font_medium">]</span></cite><span id="footnote46.8" class="ltx_text ltx_font_medium">, we mainly discuss symbolic reasoning tasks specially designed for evaluating LLMs. We do not consider symbolic reasoning methods in traditional NLP tasks, such as deducing logical rules from the knowledge graphs in KBQA.</span></span></span></span>.</span>

The symbolic reasoning tasks mainly focus on manipulating the symbols in a formal rule setting to fulfill some specific goal&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, where the operations and rules may have never been seen by LLMs during pre-training. 

Existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib505" title="" class="ltx_ref">505</a>, <a href="#bib.bib439" title="" class="ltx_ref">439</a>]</cite> commonly evaluates LLMs on the task of last letter concatenation and coin flip, where the evaluation examples require the same reasoning steps as the in-context examples (called <em id="S7.SS1.SSS3.p3.1.2" class="ltx_emph ltx_font_italic">in-domain test</em>) or more steps (called <em id="S7.SS1.SSS3.p3.1.3" class="ltx_emph ltx_font_italic">out-of-domain test</em>).
For an example of the out-of-domain test, LLMs could only see the examples with two words in context, but it requires LLMs to concatenate the last letters of three or more words.
Typically, the accuracy of the generated symbols is adopted to evaluate the performance of LLMs on these tasks.
Thus, LLMs need to understand the semantic relations among the symbolic operations and their composition in complex scenarios.
However, under the out-of-domain setting, as LLMs have not seen the complex compositions of symbolic operations and rules (<em id="S7.SS1.SSS3.p3.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> twice the number of operations in context examples), it is hard for LLMs to capture their accurate meanings.
To solve this issue, existing studies incorporate scratchpad&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib591" title="" class="ltx_ref">591</a>, <a href="#bib.bib676" title="" class="ltx_ref">676</a>]</cite> and tutor&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib677" title="" class="ltx_ref">677</a>]</cite> strategies to help LLMs better manipulate symbolic operations, for generating longer and more complex reasoning processes.
Another line of research work utilizes the formal programming language to represent the symbolic operations and rules, which requires LLMs to generate code and perform the reasoning process by executing it with external interpreters.
Such a way can decompose the complex reasoning process into code synthesis and program execution for LLMs and interpreters, respectively, leading to a simplified reasoning process with yet more accurate results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>.</p>
</div>
<div id="S7.SS1.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS3.p4.1" class="ltx_p"><span id="S7.SS1.SSS3.p4.1.1" class="ltx_text ltx_font_bold">Mathematical Reasoning.</span>
The mathematical reasoning tasks need to comprehensively utilize mathematical knowledge, logic, and computation for solving problems or generating proof statements.
Existing mathematical reasoning tasks can be mainly categorized into math problem solving and automated theorem proving.

For math problem solving tasks, SVAMP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib592" title="" class="ltx_ref">592</a>]</cite>, GSM8k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> and MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> datasets are commonly used for evaluation, where LLMs need to generate accurate concrete numbers or equations to answer the mathematical problem.
As these tasks also require multi-step reasoning, the CoT prompting strategy has been widely adopted for LLMs to improve the reasoning performance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.

As another practical strategy, continually pre-training LLMs on large-scale mathematical corpora can largely boost their performance on mathematical reasoning tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib678" title="" class="ltx_ref">678</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>.
Further, since math problems in different languages share the same mathematical logic, researchers also propose a multilingual math word problem benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib524" title="" class="ltx_ref">524</a>]</cite> to evaluate the multilingual mathematical reasoning capacity of LLMs.
As another challenging task, automated theorem proving (ATP)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib600" title="" class="ltx_ref">600</a>, <a href="#bib.bib598" title="" class="ltx_ref">598</a>, <a href="#bib.bib679" title="" class="ltx_ref">679</a>]</cite> requires the reasoning model to strictly follow the reasoning logic and mathematical skills. To evaluate the performance on this task, PISA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib599" title="" class="ltx_ref">599</a>]</cite> and miniF2F&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib600" title="" class="ltx_ref">600</a>]</cite> are two typical ATP datasets with the <em id="S7.SS1.SSS3.p4.1.2" class="ltx_emph ltx_font_italic">proof success rate</em> as the evaluation metric.
As a typical approach, existing work on ATP utilizes LLMs to aid the search for proofs using an interactive theorem prover (ITP), such as Lean, Metamath, and Isabelle&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib680" title="" class="ltx_ref">680</a>, <a href="#bib.bib681" title="" class="ltx_ref">681</a>, <a href="#bib.bib682" title="" class="ltx_ref">682</a>]</cite>.
A major limitation of ATP research is the lack of related corpora in formal language.
To tackle it, several studies utilize LLMs to convert informal statements into formal proofs for augmenting new data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib683" title="" class="ltx_ref">683</a>]</cite> or generate drafts and proof sketches to reduce the search space of the proofs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib684" title="" class="ltx_ref">684</a>]</cite>.</p>
</div>
<div id="S7.SS1.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S7.SS1.SSS3.p5.1" class="ltx_p"><span id="S7.SS1.SSS3.p5.1.1" class="ltx_text ltx_font_bold">Major Issues.</span>
In spite of the advancements, LLMs still have several limitations in solving complex reasoning tasks.</p>
</div>
<div id="S7.SS1.SSS3.p6" class="ltx_para">
<p id="S7.SS1.SSS3.p6.1" class="ltx_p"><math id="S7.SS1.SSS3.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS1.SSS3.p6.1.m1.1a"><mo id="S7.SS1.SSS3.p6.1.m1.1.1" xref="S7.SS1.SSS3.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p6.1.m1.1b"><ci id="S7.SS1.SSS3.p6.1.m1.1.1.cmml" xref="S7.SS1.SSS3.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p6.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS1.SSS3.p6.1.1" class="ltx_emph ltx_font_italic">Reasoning inconsistency</em>.
With improved reasoning strategies (<em id="S7.SS1.SSS3.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> CoT prompting), LLMs can solve some complex reasoning tasks, by performing
step-by-step reasoning based on the supporting logic and evidence.
Despite the effectiveness,
the <em id="S7.SS1.SSS3.p6.1.3" class="ltx_emph ltx_font_italic">reasoning inconsistency</em> issue often occurs in the decomposed reasoning process.
Concretely, LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib442" title="" class="ltx_ref">442</a>]</cite>, leading to inconsistency between the derived answer and the reasoning process.

To alleviate this problem, existing work has proposed to guide the whole generation process of LLMs via external tools or models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib636" title="" class="ltx_ref">636</a>, <a href="#bib.bib437" title="" class="ltx_ref">437</a>, <a href="#bib.bib451" title="" class="ltx_ref">451</a>]</cite>, to re-check the reasoning process and final answer for correcting the potential errors&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib685" title="" class="ltx_ref">685</a>, <a href="#bib.bib686" title="" class="ltx_ref">686</a>, <a href="#bib.bib687" title="" class="ltx_ref">687</a>]</cite> or fine-tune LLMs with process-based feedback&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib688" title="" class="ltx_ref">688</a>, <a href="#bib.bib689" title="" class="ltx_ref">689</a>]</cite>.
For instance, <em id="S7.SS1.SSS3.p6.1.4" class="ltx_emph ltx_font_italic">Tree of Thoughts&nbsp;(ToT)</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib451" title="" class="ltx_ref">451</a>]</cite> empowers LLMs to engage in the decision-making process by concurrently exploring and self-evaluating various reasoning paths.
To refine the reasoning processes, Self-Refine&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib685" title="" class="ltx_ref">685</a>]</cite> elicits feedback from LLMs on self-generated solutions, enabling the iterative refinement of solutions based on the feedback.
Moreover, several studies improve the consistency in the reasoning chain of LLMs through the integration of process-based supervision during training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib688" title="" class="ltx_ref">688</a>, <a href="#bib.bib689" title="" class="ltx_ref">689</a>]</cite>.


As a promising solution,
recent approaches reformulate the complex reasoning tasks into code generation tasks, where the strict execution of the generated code ensures the consistency between the reasoning process and the outcome.

Also, it has been revealed that there might exist inconsistency between tasks with similar inputs, where small changes in the task description may cause the model to produce different results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib592" title="" class="ltx_ref">592</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.

To mitigate this problem, self-consistency&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib436" title="" class="ltx_ref">436</a>]</cite> adopts the ensemble of multiple reasoning paths to enhance the decoding process of LLMs.</p>
<div id="S7.SS1.SSS3.p6.2" class="ltx_logical-block">
<div id="S7.SS1.SSS3.p6.2.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S7.SS1.SSS3.p6.2.p1.pic1" class="ltx_picture" height="225.2" overflow="visible" version="1.1" width="276"><g transform="translate(0,225.2) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 219.29 C 0 222.56 2.64 225.2 5.91 225.2 L 270.1 225.2 C 273.36 225.2 276 222.56 276 219.29 L 276 5.91 C 276 2.64 273.36 0 270.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 201.24 L 274.04 201.24 L 274.04 5.91 C 274.04 3.73 272.27 1.97 270.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 207.15)"><foreignObject width="232.7" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S7.SS1.SSS3.p6.2.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS3.p6.2.p1.pic1.1.1.1.1.1.1" class="ltx_p">Reasoning Inconsistency</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="232.7" height="175.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S7.SS1.SSS3.p6.2.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS3.p6.2.p1.pic1.2.2.2.1.1.1" class="ltx_p">LLMs may generate the correct answer following an invalid reasoning path, or produce a wrong answer after a correct reasoning process, leading to inconsistency between the derived answer and the reasoning process.
The issue can be alleviated by fine-tuning LLMs with process-level feedback, using an ensemble of diverse reasoning paths, and refining the reasoning process with self-reflection or external feedback.</span>
</span></foreignObject></g></g></svg>
</div>
</div>
</div>
<div id="S7.SS1.SSS3.p7" class="ltx_para">
<p id="S7.SS1.SSS3.p7.3" class="ltx_p"><math id="S7.SS1.SSS3.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS1.SSS3.p7.1.m1.1a"><mo id="S7.SS1.SSS3.p7.1.m1.1.1" xref="S7.SS1.SSS3.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p7.1.m1.1b"><ci id="S7.SS1.SSS3.p7.1.m1.1.1.cmml" xref="S7.SS1.SSS3.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS1.SSS3.p7.3.1" class="ltx_emph ltx_font_italic">Numerical computation</em>.
For complex reasoning tasks, LLMs still face difficulties in the involved numerical computation, especially for the symbols that are seldom encountered during pre-training, such as arithmetic with large numbers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib677" title="" class="ltx_ref">677</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib690" title="" class="ltx_ref">690</a>]</cite>.
To tackle this issue, a direct way is to tune LLMs on synthesized arithmetic problems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib691" title="" class="ltx_ref">691</a>, <a href="#bib.bib361" title="" class="ltx_ref">361</a>]</cite>. Also, a surge of studies improve the numerical computation performance by tracing intermediate calculation steps in training and inference stages&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib676" title="" class="ltx_ref">676</a>, <a href="#bib.bib692" title="" class="ltx_ref">692</a>, <a href="#bib.bib361" title="" class="ltx_ref">361</a>]</cite>, <em id="S7.SS1.SSS3.p7.3.2" class="ltx_emph ltx_font_italic">e.g.,</em> scratchpad tracing.
In addition, existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> has also incorporated external tools (<em id="S7.SS1.SSS3.p7.3.3" class="ltx_emph ltx_font_italic">e.g.,</em> calculator), especially for handling arithmetic operations.
More recently, ChatGPT has provided a plugin mechanism to use external tools&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib665" title="" class="ltx_ref">665</a>]</cite>.
In this way, LLMs need to learn how to properly manipulate the tools. For this purpose, researchers have augmented the examples using tools (even the LLM itself) for tuning the LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib693" title="" class="ltx_ref">693</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, or devised instructions and exemplars for in-context learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>.

In addition to the aid of external tools, recent studies find that tokenizing digits into individual tokens (<em id="S7.SS1.SSS3.p7.3.4" class="ltx_emph ltx_font_italic">e.g.,</em> LLaMA and Galactica tokenizers) is a useful approach to enhancing the inherent arithmetic ability of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib690" title="" class="ltx_ref">690</a>, <a href="#bib.bib361" title="" class="ltx_ref">361</a>]</cite>. One possible explanation is that subword tokenization techniques can result in inconsistent sequences when tokenizing numbers. For instance, with a subword tokenizer the integer 7481 may be tokenized as <math id="S7.SS1.SSS3.p7.2.m2.1" class="ltx_Math" alttext="7\_481" display="inline"><semantics id="S7.SS1.SSS3.p7.2.m2.1a"><mrow id="S7.SS1.SSS3.p7.2.m2.1.1" xref="S7.SS1.SSS3.p7.2.m2.1.1.cmml"><mn id="S7.SS1.SSS3.p7.2.m2.1.1.2" xref="S7.SS1.SSS3.p7.2.m2.1.1.2.cmml">7</mn><mo lspace="0em" rspace="0em" id="S7.SS1.SSS3.p7.2.m2.1.1.1" xref="S7.SS1.SSS3.p7.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S7.SS1.SSS3.p7.2.m2.1.1.3" xref="S7.SS1.SSS3.p7.2.m2.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S7.SS1.SSS3.p7.2.m2.1.1.1a" xref="S7.SS1.SSS3.p7.2.m2.1.1.1.cmml">​</mo><mn id="S7.SS1.SSS3.p7.2.m2.1.1.4" xref="S7.SS1.SSS3.p7.2.m2.1.1.4.cmml">481</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p7.2.m2.1b"><apply id="S7.SS1.SSS3.p7.2.m2.1.1.cmml" xref="S7.SS1.SSS3.p7.2.m2.1.1"><times id="S7.SS1.SSS3.p7.2.m2.1.1.1.cmml" xref="S7.SS1.SSS3.p7.2.m2.1.1.1"></times><cn type="integer" id="S7.SS1.SSS3.p7.2.m2.1.1.2.cmml" xref="S7.SS1.SSS3.p7.2.m2.1.1.2">7</cn><ci id="S7.SS1.SSS3.p7.2.m2.1.1.3.cmml" xref="S7.SS1.SSS3.p7.2.m2.1.1.3">_</ci><cn type="integer" id="S7.SS1.SSS3.p7.2.m2.1.1.4.cmml" xref="S7.SS1.SSS3.p7.2.m2.1.1.4">481</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p7.2.m2.1c">7\_481</annotation></semantics></math>, while 74815 may be tokenized as <math id="S7.SS1.SSS3.p7.3.m3.1" class="ltx_Math" alttext="748\_15" display="inline"><semantics id="S7.SS1.SSS3.p7.3.m3.1a"><mrow id="S7.SS1.SSS3.p7.3.m3.1.1" xref="S7.SS1.SSS3.p7.3.m3.1.1.cmml"><mn id="S7.SS1.SSS3.p7.3.m3.1.1.2" xref="S7.SS1.SSS3.p7.3.m3.1.1.2.cmml">748</mn><mo lspace="0em" rspace="0em" id="S7.SS1.SSS3.p7.3.m3.1.1.1" xref="S7.SS1.SSS3.p7.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S7.SS1.SSS3.p7.3.m3.1.1.3" xref="S7.SS1.SSS3.p7.3.m3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S7.SS1.SSS3.p7.3.m3.1.1.1a" xref="S7.SS1.SSS3.p7.3.m3.1.1.1.cmml">​</mo><mn id="S7.SS1.SSS3.p7.3.m3.1.1.4" xref="S7.SS1.SSS3.p7.3.m3.1.1.4.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p7.3.m3.1b"><apply id="S7.SS1.SSS3.p7.3.m3.1.1.cmml" xref="S7.SS1.SSS3.p7.3.m3.1.1"><times id="S7.SS1.SSS3.p7.3.m3.1.1.1.cmml" xref="S7.SS1.SSS3.p7.3.m3.1.1.1"></times><cn type="integer" id="S7.SS1.SSS3.p7.3.m3.1.1.2.cmml" xref="S7.SS1.SSS3.p7.3.m3.1.1.2">748</cn><ci id="S7.SS1.SSS3.p7.3.m3.1.1.3.cmml" xref="S7.SS1.SSS3.p7.3.m3.1.1.3">_</ci><cn type="integer" id="S7.SS1.SSS3.p7.3.m3.1.1.4.cmml" xref="S7.SS1.SSS3.p7.3.m3.1.1.4">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p7.3.m3.1c">748\_15</annotation></semantics></math> (the same numerical substrings with different splits)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib361" title="" class="ltx_ref">361</a>]</cite>.
As a comparison, digit-based tokenization for numbers can avoid such an inconsistency, thus likely improving the numerical computation ability of LLMs.</p>
</div>
<div id="S7.SS1.SSS3.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="S7.SS1.SSS3.1.p1.pic1" class="ltx_picture" height="178.23" overflow="visible" version="1.1" width="276"><g transform="translate(0,178.23) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#00008C" fill-opacity="1.0"><path d="M 0 5.91 L 0 172.33 C 0 175.59 2.64 178.23 5.91 178.23 L 270.1 178.23 C 273.36 178.23 276 175.59 276 172.33 L 276 5.91 C 276 2.64 273.36 0 270.1 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 154.12 L 274.04 154.12 L 274.04 5.91 C 274.04 3.73 272.27 1.97 270.1 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 160.03)"><foreignObject width="232.7" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S7.SS1.SSS3.1.p1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS3.1.p1.pic1.1.1.1.1.1.1" class="ltx_p">Numerical Computation</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="232.7" height="128.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S7.SS1.SSS3.1.p1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:168.2pt;">
<span id="S7.SS1.SSS3.1.p1.pic1.2.2.2.1.1.1" class="ltx_p">LLMs face difficulties in numerical computation, especially for the symbols that are seldom encountered during pre-training.
In addition to using mathematical tools, tokenizing digits into individual tokens is also an effective design choice for improving the arithmetic ability of LLMs.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span><span id="S7.SS2.1.1" class="ltx_text ltx_font_italic">Advanced Ability</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">In addition to the above basic evaluation tasks, LLMs also exhibit some superior abilities that require special considerations for evaluation.
In this part, we discuss several representative advanced abilities and the corresponding evaluation approaches, including human alignment, interaction with the external environment, and tool manipulation. Next, we discuss these advanced abilities in detail.</p>
</div>
<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Human Alignment</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p id="S7.SS2.SSS1.p1.1" class="ltx_p">It is desired that LLMs could well conform to human values and needs, <em id="S7.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> human alignment, which is a key ability for the broad use of LLMs in real-world applications.</p>
</div>
<div id="S7.SS2.SSS1.p2" class="ltx_para">
<p id="S7.SS2.SSS1.p2.1" class="ltx_p">To evaluate this ability, existing studies consider multiple criteria for human alignment, such as helpfulness, honesty, and safety&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>.
For helpfulness and honesty, adversarial question answering tasks (<em id="S7.SS2.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> TruthfulQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib556" title="" class="ltx_ref">556</a>]</cite>) can be utilized to examine LLM’s ability in detecting possible falsehood in the text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
Furthermore, harmlessness can be also evaluated by several existing benchmarks, <em id="S7.SS2.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> CrowS-Pairs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib603" title="" class="ltx_ref">603</a>]</cite> and Winogender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib604" title="" class="ltx_ref">604</a>]</cite>.
Despite the automatic evaluation with the above datasets, human evaluation is still a more direct way to effectively test the human alignment ability of LLMs.
OpenAI invites many experts in domains related to AI risks to evaluate and improve the behaviors of GPT-4 when encountering risky contents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.

In addition, for other aspects of human alignment (<em id="S7.SS2.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> truthfulness), several studies propose to use specific instructions and devise annotation rules to guide the annotation process&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.
Empirical studies have revealed that these strategies can greatly improve the human alignment ability of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>. For instance, after alignment tuning on data collected through interactions with experts, the incorrect behavior rate of GPT-4 can be largely reduced when it deals with sensitive or disallowed prompts. 
In addition, high-quality pre-training data can reduce the effort required for alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
For instance, Galactica is potentially more harmless due to the less biased contents in the scientific corpus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Interaction with External Environment</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p id="S7.SS2.SSS2.p1.1" class="ltx_p">In addition to standard evaluation tasks, LLMs have the ability to receive feedback from the external environment and perform actions according to the behavior instruction, <em id="S7.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> generating action plans in natural language to manipulate agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib694" title="" class="ltx_ref">694</a>, <a href="#bib.bib695" title="" class="ltx_ref">695</a>]</cite>.
Such an ability is also emergent in LLMs that can generate detailed and highly realistic action plans, while smaller models (<em id="S7.SS2.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-2) tend to generate shorter or meaningless plans&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib694" title="" class="ltx_ref">694</a>]</cite>.</p>
</div>
<div id="S7.SS2.SSS2.p2" class="ltx_para">
<p id="S7.SS2.SSS2.p2.1" class="ltx_p">To test this ability, several embodied AI environments and benchmarks can be used for evaluation, described as follows. VirtualHome&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib606" title="" class="ltx_ref">606</a>]</cite> builds a 3D simulator for household tasks such as cleaning and cooking, in which the agent can execute natural language actions generated by LLMs. ALFRED&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib608" title="" class="ltx_ref">608</a>]</cite> includes more challenging tasks that require LLMs to accomplish compositional targets. BEHAVIOR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib607" title="" class="ltx_ref">607</a>]</cite> focuses on everyday chores in simulation environments and requires LLMs to generate complex solutions, <em id="S7.SS2.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> changing the internal status of objects.

Apart from restricted environments such as household tasks, a line of research work investigates the proficiency of LLM-based agents to explore open-world environments, such as Minecraft and the Internet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib696" title="" class="ltx_ref">696</a>, <a href="#bib.bib697" title="" class="ltx_ref">697</a>]</cite>.
Voyager&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib697" title="" class="ltx_ref">697</a>]</cite> introduces an automatic curriculum module that enables LLMs to continuously acquire new skills based on feedback from the environment.
GITM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib696" title="" class="ltx_ref">696</a>]</cite> focuses on solving various challenges in Minecraft based on LLM, through task decomposition, planning, and invocation of interfaces.

Based on the generated action plans or task completions, existing work either adopts the regular metrics (<em id="S7.SS2.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> executability and correctness of the generated action plans)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib694" title="" class="ltx_ref">694</a>]</cite> in the benchmark or directly conducts real-world experiments and measures the success rate&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib698" title="" class="ltx_ref">698</a>]</cite>, to evaluate such ability.
It has been shown that LLMs are capable in interacting with the external environment and generating accurate action plans&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib699" title="" class="ltx_ref">699</a>]</cite>.
Recently, several improvement methods have been proposed to enhance the interaction ability of LLMs, <em id="S7.SS2.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> designing code-like prompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib530" title="" class="ltx_ref">530</a>]</cite> and providing real-world grounding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib698" title="" class="ltx_ref">698</a>]</cite>.</p>
</div>
<div id="S7.SS2.SSS2.p3" class="ltx_para">
<p id="S7.SS2.SSS2.p3.1" class="ltx_p">In addition, recent work also explores multi-agent collaboration based on LLMs in simulated environments&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib533" title="" class="ltx_ref">533</a>, <a href="#bib.bib700" title="" class="ltx_ref">700</a>, <a href="#bib.bib701" title="" class="ltx_ref">701</a>]</cite>.
These studies simulate human social behaviors by instantiating multiple LLM-based agents with observations, planning, and memories in a sandbox environment.
In controlled evaluation, the abilities of generative agents to search, plan, and think are evaluated by humans in an interview-like manner.
Further, they also conduct descriptive measurements on multiple agents within a simulated environment to examine emergent social behaviors.</p>
</div>
</section>
<section id="S7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3 </span>Tool Manipulation</h4>

<div id="S7.SS2.SSS3.p1" class="ltx_para">
<p id="S7.SS2.SSS3.p1.1" class="ltx_p">When solving complex problems, LLMs can turn to external tools if they determine it is necessary. By encapsulating available tools with API calls, existing work has involved a variety of external tools, <em id="S7.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> search engine&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, calculator&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, and compiler&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>, to enhance the performance of LLMs on several specific tasks. Recently, OpenAI has supported the use of plugins in ChatGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib665" title="" class="ltx_ref">665</a>]</cite>, which can equip LLMs with broader capacities beyond language modeling. For example, the web browser plugin enables ChatGPT to access fresh information. Further, incorporating third-party plugins is particularly key for creating a prosperous ecosystem of applications based on LLMs.</p>
</div>
<div id="S7.SS2.SSS3.p2" class="ltx_para">
<p id="S7.SS2.SSS3.p2.1" class="ltx_p">To examine the ability of tool manipulation, existing work mostly adopts complex reasoning tasks for evaluation, such as mathematical problem solving (<em id="S7.SS2.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> GSM8k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> and SVAMP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib592" title="" class="ltx_ref">592</a>]</cite>) or knowledge question answering (<em id="S7.SS2.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> TruthfulQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib556" title="" class="ltx_ref">556</a>]</cite>), where the successful utilization of tools is very important for enhancing the required skills that LLMs are incapable in (<em id="S7.SS2.SSS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> numerical calculation).
In this way, the evaluated performance on these tasks can reflect the ability of LLMs in tool manipulation.
To teach LLMs to utilize tools, existing studies add exemplars using tools in context to elicit LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>, or fine-tune LLMs on simulated data about tool utilization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib693" title="" class="ltx_ref">693</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>.
It has been found that with the help of tools, LLMs become more capable of handling the issues that they are not good at, <em id="S7.SS2.SSS3.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> equation calculation and answering timely questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib448" title="" class="ltx_ref">448</a>]</cite>.

However, as the number of available tools increases, the limited context length of LLMs may pose challenges in describing and demonstrating extensive tool APIs.
To address this issue, existing work retrieves the usage of relevant tools, or encoding tool information as tokens within the embedding space&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib702" title="" class="ltx_ref">702</a>, <a href="#bib.bib703" title="" class="ltx_ref">703</a>, <a href="#bib.bib704" title="" class="ltx_ref">704</a>]</cite>.</p>
</div>
<div id="S7.SS2.SSS3.p3" class="ltx_para">
<p id="S7.SS2.SSS3.p3.1" class="ltx_p">In addition to existing tools developed by humans, LLMs possess the capability to make their own tools for specific tasks autonomously&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib705" title="" class="ltx_ref">705</a>]</cite>.
This enables the models to independently explore and manipulate these self-created tools, thereby expanding their potential for autonomous exploration in solving a wide range of real-world tasks.</p>
</div>
<div id="S7.SS2.SSS3.p4" class="ltx_para">
<p id="S7.SS2.SSS3.p4.1" class="ltx_p"><em id="S7.SS2.SSS3.p4.1.1" class="ltx_emph ltx_font_italic">Summary</em>. The above three abilities are of great value to the practical performance of LLMs: conforming to
human values and preferences (human alignment), acting properly in real-world scenarios (interaction with the external environment), and expanding the ability scope (tool manipulation).
In addition to the above three advanced abilities, LLMs might also show other abilities that are specially related to some tasks (<em id="S7.SS2.SSS3.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> data annotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib486" title="" class="ltx_ref">486</a>]</cite>) or learning mechanisms (<em id="S7.SS2.SSS3.p4.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> self-improvement&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib706" title="" class="ltx_ref">706</a>]</cite>).
It will be an open direction to discover, measure and evaluate these newly emerging abilities, so as to better utilize and improve LLMs.</p>
</div>
<figure id="S7.T15" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XV: </span>A category of existing evaluation work. “General” denotes that the evaluation focuses on an overall performance of multiple abilities. The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in Section&nbsp;<a href="#S7.SS1" title="7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a> and <a href="#S7.SS2" title="7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>.</figcaption>
<table id="S7.T15.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T15.1.1" class="ltx_tr">
<td id="S7.T15.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T15.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></td>
<td id="S7.T15.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T15.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Evaluation</span></td>
<td id="S7.T15.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T15.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model Types</span></td>
<td id="S7.T15.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T15.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Abilities/Domain</span></td>
<td id="S7.T15.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T15.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Data Source</span></td>
</tr>
<tr id="S7.T15.1.2" class="ltx_tr">
<td id="S7.T15.1.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="26"><span id="S7.T15.1.2.1.1" class="ltx_text" style="font-size:80%;">Benchmark</span></td>
<td id="S7.T15.1.2.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S7.T15.1.2.2.1" class="ltx_text" style="font-size:80%;">MMLU&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib364" title="" class="ltx_ref">364</a><span id="S7.T15.1.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.2.3.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.2.4.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.2.5.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.3" class="ltx_tr">
<td id="S7.T15.1.3.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.3.1.1" class="ltx_text" style="font-size:80%;">BIG-bench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib70" title="" class="ltx_ref">70</a><span id="S7.T15.1.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.3.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.3.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.3.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.3.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.3.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.3.4.1" class="ltx_text" style="font-size:80%;">Human annotation</span></td>
</tr>
<tr id="S7.T15.1.4" class="ltx_tr">
<td id="S7.T15.1.4.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.4.1.1" class="ltx_text" style="font-size:80%;">HELM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib520" title="" class="ltx_ref">520</a><span id="S7.T15.1.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.4.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.4.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.4.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.4.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.4.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.4.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.5" class="ltx_tr">
<td id="S7.T15.1.5.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.5.1.1" class="ltx_text" style="font-size:80%;">Open LLM Leaderboard&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib707" title="" class="ltx_ref">707</a><span id="S7.T15.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.5.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.5.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.5.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.5.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.5.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.5.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.6" class="ltx_tr">
<td id="S7.T15.1.6.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.6.1.1" class="ltx_text" style="font-size:80%;">AGIEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib708" title="" class="ltx_ref">708</a><span id="S7.T15.1.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.6.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.6.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.6.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.6.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.6.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.6.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.7" class="ltx_tr">
<td id="S7.T15.1.7.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.7.1.1" class="ltx_text" style="font-size:80%;">MMCU&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib709" title="" class="ltx_ref">709</a><span id="S7.T15.1.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.7.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.7.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.7.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.7.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.7.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.7.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.8" class="ltx_tr">
<td id="S7.T15.1.8.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.8.1.1" class="ltx_text" style="font-size:80%;">M3KE&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib710" title="" class="ltx_ref">710</a><span id="S7.T15.1.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.8.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.8.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.8.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.8.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.8.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.8.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.9" class="ltx_tr">
<td id="S7.T15.1.9.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.9.1.1" class="ltx_text" style="font-size:80%;">C-Eval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib711" title="" class="ltx_ref">711</a><span id="S7.T15.1.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.9.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.9.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.9.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.9.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.9.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.9.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.10" class="ltx_tr">
<td id="S7.T15.1.10.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.10.1.1" class="ltx_text" style="font-size:80%;">Xiezhi&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib712" title="" class="ltx_ref">712</a><span id="S7.T15.1.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.10.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.10.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.10.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.10.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.10.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.10.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.11" class="ltx_tr">
<td id="S7.T15.1.11.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.11.1.1" class="ltx_text" style="font-size:80%;">OpenCompass&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.11.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib713" title="" class="ltx_ref">713</a><span id="S7.T15.1.11.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.11.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.11.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.11.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.11.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.11.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.11.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.12" class="ltx_tr">
<td id="S7.T15.1.12.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.12.1.1" class="ltx_text" style="font-size:80%;">Chain-of-Thought Hub&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.12.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib714" title="" class="ltx_ref">714</a><span id="S7.T15.1.12.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.12.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.12.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.12.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.12.3.1" class="ltx_text" style="font-size:80%;">General</span></td>
<td id="S7.T15.1.12.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.12.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.13" class="ltx_tr">
<td id="S7.T15.1.13.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.13.1.1" class="ltx_text" style="font-size:80%;">KoLA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.13.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib715" title="" class="ltx_ref">715</a><span id="S7.T15.1.13.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.13.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.13.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.13.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.13.3.1" class="ltx_text" style="font-size:80%;">Knowledge utilization</span></td>
<td id="S7.T15.1.13.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.13.4.1" class="ltx_text" style="font-size:80%;">Web</span></td>
</tr>
<tr id="S7.T15.1.14" class="ltx_tr">
<td id="S7.T15.1.14.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.14.1.1" class="ltx_text" style="font-size:80%;">ARB&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.14.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib716" title="" class="ltx_ref">716</a><span id="S7.T15.1.14.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.14.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.14.2.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.14.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.14.3.1" class="ltx_text" style="font-size:80%;">Complex reasoning</span></td>
<td id="S7.T15.1.14.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.14.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.15" class="ltx_tr">
<td id="S7.T15.1.15.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.15.1.1" class="ltx_text" style="font-size:80%;">APIBench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.15.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib717" title="" class="ltx_ref">717</a><span id="S7.T15.1.15.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.15.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.15.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.15.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.15.3.1" class="ltx_text" style="font-size:80%;">Tool manipulation</span></td>
<td id="S7.T15.1.15.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.15.4.1" class="ltx_text" style="font-size:80%;">Web</span></td>
</tr>
<tr id="S7.T15.1.16" class="ltx_tr">
<td id="S7.T15.1.16.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.16.1.1" class="ltx_text" style="font-size:80%;">APIBank&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.16.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib718" title="" class="ltx_ref">718</a><span id="S7.T15.1.16.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.16.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.16.2.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.16.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.16.3.1" class="ltx_text" style="font-size:80%;">Tool manipulation</span></td>
<td id="S7.T15.1.16.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.16.4.1" class="ltx_text" style="font-size:80%;">Synthesis</span></td>
</tr>
<tr id="S7.T15.1.17" class="ltx_tr">
<td id="S7.T15.1.17.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.17.1.1" class="ltx_text" style="font-size:80%;">ToolAlpaca&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.17.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib719" title="" class="ltx_ref">719</a><span id="S7.T15.1.17.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.17.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.17.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.17.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.17.3.1" class="ltx_text" style="font-size:80%;">Tool manipulation</span></td>
<td id="S7.T15.1.17.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.17.4.1" class="ltx_text" style="font-size:80%;">Synthesis</span></td>
</tr>
<tr id="S7.T15.1.18" class="ltx_tr">
<td id="S7.T15.1.18.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.18.1.1" class="ltx_text" style="font-size:80%;">T-Bench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.18.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib720" title="" class="ltx_ref">720</a><span id="S7.T15.1.18.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.18.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.18.2.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.18.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.18.3.1" class="ltx_text" style="font-size:80%;">Tool manipulation</span></td>
<td id="S7.T15.1.18.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.18.4.1" class="ltx_text" style="font-size:80%;">Synthesis</span></td>
</tr>
<tr id="S7.T15.1.19" class="ltx_tr">
<td id="S7.T15.1.19.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.19.1.1" class="ltx_text" style="font-size:80%;">ToolBench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.19.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib721" title="" class="ltx_ref">721</a><span id="S7.T15.1.19.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.19.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.19.2.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.19.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.19.3.1" class="ltx_text" style="font-size:80%;">Tool manipulation</span></td>
<td id="S7.T15.1.19.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.19.4.1" class="ltx_text" style="font-size:80%;">Synthesis</span></td>
</tr>
<tr id="S7.T15.1.20" class="ltx_tr">
<td id="S7.T15.1.20.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.20.1.1" class="ltx_text" style="font-size:80%;">BOLAA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.20.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib722" title="" class="ltx_ref">722</a><span id="S7.T15.1.20.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.20.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.20.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.20.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.20.3.1" class="ltx_text" style="font-size:80%;">Environment interaction</span></td>
<td id="S7.T15.1.20.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.20.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.21" class="ltx_tr">
<td id="S7.T15.1.21.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.21.1.1" class="ltx_text" style="font-size:80%;">AgentBench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.21.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib723" title="" class="ltx_ref">723</a><span id="S7.T15.1.21.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.21.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.21.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.21.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.21.3.1" class="ltx_text" style="font-size:80%;">Environment interaction</span></td>
<td id="S7.T15.1.21.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.21.4.1" class="ltx_text" style="font-size:80%;">Human annotation/Synthesis</span></td>
</tr>
<tr id="S7.T15.1.22" class="ltx_tr">
<td id="S7.T15.1.22.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.22.1.1" class="ltx_text" style="font-size:80%;">HaluEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.22.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib602" title="" class="ltx_ref">602</a><span id="S7.T15.1.22.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.22.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.22.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.22.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.22.3.1" class="ltx_text" style="font-size:80%;">Human alignment</span></td>
<td id="S7.T15.1.22.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.22.4.1" class="ltx_text" style="font-size:80%;">Human annotation/Synthesis</span></td>
</tr>
<tr id="S7.T15.1.23" class="ltx_tr">
<td id="S7.T15.1.23.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.23.1.1" class="ltx_text" style="font-size:80%;">PromptBench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.23.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib724" title="" class="ltx_ref">724</a><span id="S7.T15.1.23.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.23.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.23.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.23.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.23.3.1" class="ltx_text" style="font-size:80%;">Robustness</span></td>
<td id="S7.T15.1.23.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.23.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.24" class="ltx_tr">
<td id="S7.T15.1.24.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.24.1.1" class="ltx_text" style="font-size:80%;">HumanEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.24.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib105" title="" class="ltx_ref">105</a><span id="S7.T15.1.24.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.24.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.24.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.24.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.24.3.1" class="ltx_text" style="font-size:80%;">Code synthesis</span></td>
<td id="S7.T15.1.24.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.24.4.1" class="ltx_text" style="font-size:80%;">Human annotation</span></td>
</tr>
<tr id="S7.T15.1.25" class="ltx_tr">
<td id="S7.T15.1.25.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.25.1.1" class="ltx_text" style="font-size:80%;">MultiMedQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.25.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib356" title="" class="ltx_ref">356</a><span id="S7.T15.1.25.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.25.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.25.2.1" class="ltx_text" style="font-size:80%;">Specialized</span></td>
<td id="S7.T15.1.25.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.25.3.1" class="ltx_text" style="font-size:80%;">Healthcare</span></td>
<td id="S7.T15.1.25.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.25.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.26" class="ltx_tr">
<td id="S7.T15.1.26.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.26.1.1" class="ltx_text" style="font-size:80%;">FLUE&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.26.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib725" title="" class="ltx_ref">725</a><span id="S7.T15.1.26.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.26.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.26.2.1" class="ltx_text" style="font-size:80%;">Specialized</span></td>
<td id="S7.T15.1.26.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.26.3.1" class="ltx_text" style="font-size:80%;">Finance</span></td>
<td id="S7.T15.1.26.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.26.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.27" class="ltx_tr">
<td id="S7.T15.1.27.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.27.1.1" class="ltx_text" style="font-size:80%;">LegalBench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.27.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib726" title="" class="ltx_ref">726</a><span id="S7.T15.1.27.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.27.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.27.2.1" class="ltx_text" style="font-size:80%;">Specialized</span></td>
<td id="S7.T15.1.27.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.27.3.1" class="ltx_text" style="font-size:80%;">Legal</span></td>
<td id="S7.T15.1.27.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.27.4.1" class="ltx_text" style="font-size:80%;">Human annotation</span></td>
</tr>
<tr id="S7.T15.1.28" class="ltx_tr">
<td id="S7.T15.1.28.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T15.1.28.1.1" class="ltx_text" style="font-size:80%;">Human</span></td>
<td id="S7.T15.1.28.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S7.T15.1.28.2.1" class="ltx_text" style="font-size:80%;">Chatbot Arena&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.28.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib727" title="" class="ltx_ref">727</a><span id="S7.T15.1.28.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.28.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.28.3.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned/Specialized</span></td>
<td id="S7.T15.1.28.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.28.4.1" class="ltx_text" style="font-size:80%;">Human Alignment</span></td>
<td id="S7.T15.1.28.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.28.5.1" class="ltx_text" style="font-size:80%;">Human annotation</span></td>
</tr>
<tr id="S7.T15.1.29" class="ltx_tr">
<td id="S7.T15.1.29.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.29.1.1" class="ltx_text" style="font-size:80%;">SciBench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.29.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib728" title="" class="ltx_ref">728</a><span id="S7.T15.1.29.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.29.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.29.2.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.29.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.29.3.1" class="ltx_text" style="font-size:80%;">Complex reasoning</span></td>
<td id="S7.T15.1.29.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.29.4.1" class="ltx_text" style="font-size:80%;">Human exam/practice</span></td>
</tr>
<tr id="S7.T15.1.30" class="ltx_tr">
<td id="S7.T15.1.30.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="5"><span id="S7.T15.1.30.1.1" class="ltx_text" style="font-size:80%;">Model</span></td>
<td id="S7.T15.1.30.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S7.T15.1.30.2.1" class="ltx_text" style="font-size:80%;">AlpacaEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.30.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib729" title="" class="ltx_ref">729</a><span id="S7.T15.1.30.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.30.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.30.3.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.30.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.30.4.1" class="ltx_text" style="font-size:80%;">Instruction following</span></td>
<td id="S7.T15.1.30.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T15.1.30.5.1" class="ltx_text" style="font-size:80%;">Synthesis</span></td>
</tr>
<tr id="S7.T15.1.31" class="ltx_tr">
<td id="S7.T15.1.31.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.31.1.1" class="ltx_text" style="font-size:80%;">MT-bench&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.31.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib727" title="" class="ltx_ref">727</a><span id="S7.T15.1.31.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.31.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.31.2.1" class="ltx_text" style="font-size:80%;">Fine-tuned</span></td>
<td id="S7.T15.1.31.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.31.3.1" class="ltx_text" style="font-size:80%;">Human alignment</span></td>
<td id="S7.T15.1.31.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.31.4.1" class="ltx_text" style="font-size:80%;">Human annotation</span></td>
</tr>
<tr id="S7.T15.1.32" class="ltx_tr">
<td id="S7.T15.1.32.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.32.1.1" class="ltx_text" style="font-size:80%;">TrustGPT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.32.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib730" title="" class="ltx_ref">730</a><span id="S7.T15.1.32.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.32.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.32.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.32.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.32.3.1" class="ltx_text" style="font-size:80%;">Human alignment</span></td>
<td id="S7.T15.1.32.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.32.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
<tr id="S7.T15.1.33" class="ltx_tr">
<td id="S7.T15.1.33.1" class="ltx_td ltx_align_center">
<span id="S7.T15.1.33.1.1" class="ltx_text" style="font-size:80%;">LMExamQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.33.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib731" title="" class="ltx_ref">731</a><span id="S7.T15.1.33.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.33.2" class="ltx_td ltx_align_center"><span id="S7.T15.1.33.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.33.3" class="ltx_td ltx_align_center"><span id="S7.T15.1.33.3.1" class="ltx_text" style="font-size:80%;">Knowledge utilization</span></td>
<td id="S7.T15.1.33.4" class="ltx_td ltx_align_center"><span id="S7.T15.1.33.4.1" class="ltx_text" style="font-size:80%;">Synthesis</span></td>
</tr>
<tr id="S7.T15.1.34" class="ltx_tr">
<td id="S7.T15.1.34.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S7.T15.1.34.1.1" class="ltx_text" style="font-size:80%;">ChatEval&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T15.1.34.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib732" title="" class="ltx_ref">732</a><span id="S7.T15.1.34.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S7.T15.1.34.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S7.T15.1.34.2.1" class="ltx_text" style="font-size:80%;">Base/Fine-tuned</span></td>
<td id="S7.T15.1.34.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S7.T15.1.34.3.1" class="ltx_text" style="font-size:80%;">Knowledge utilization</span></td>
<td id="S7.T15.1.34.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S7.T15.1.34.4.1" class="ltx_text" style="font-size:80%;">Benchmark collection</span></td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span><span id="S7.SS3.1.1" class="ltx_text ltx_font_italic">Benchmarks and Evaluation Approaches</span>
</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">In the above, we have discussed the basic and advanced abilities of LLMs.
Next, we will introduce existing evaluation benchmarks and approaches&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib733" title="" class="ltx_ref">733</a>, <a href="#bib.bib734" title="" class="ltx_ref">734</a>]</cite>.</p>
</div>
<section id="S7.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1 </span>Comprehensive Evaluation Benchmarks</h4>

<div id="S7.SS3.SSS1.p1" class="ltx_para">
<p id="S7.SS3.SSS1.p1.1" class="ltx_p">Recently, several comprehensive benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib520" title="" class="ltx_ref">520</a>]</cite> have been released for the evaluation of LLMs.
In this part, we introduce several widely used benchmarks, <em id="S7.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> MMLU, BIG-bench, HELM, and a series of human exam benchmarks.</p>
</div>
<div id="S7.SS3.SSS1.p2" class="ltx_para">
<p id="S7.SS3.SSS1.p2.1" class="ltx_p"><math id="S7.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS1.p2.1.m1.1a"><mo id="S7.SS3.SSS1.p2.1.m1.1.1" xref="S7.SS3.SSS1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS1.p2.1.m1.1b"><ci id="S7.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S7.SS3.SSS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS1.p2.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS3.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">MMLU</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> is a versatile benchmark for large-scale evaluation of multi-task knowledge understanding, covering a wide range of knowledge domains from mathematics and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced.
As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, which shows the scaling law in model size. More recently, GPT-4 achieves a remarkable record (86.4% in 5-shot setting) in MMLU, which is significantly better than the previous state-of-the-art models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S7.SS3.SSS1.p3" class="ltx_para">
<p id="S7.SS3.SSS1.p3.1" class="ltx_p"><math id="S7.SS3.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS1.p3.1.m1.1a"><mo id="S7.SS3.SSS1.p3.1.m1.1.1" xref="S7.SS3.SSS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS1.p3.1.m1.1b"><ci id="S7.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S7.SS3.SSS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS3.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">BIG-bench</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> is a collaborative benchmark intended to probe existing LLMs from various aspects.
It comprises 204 tasks that encompass a broad range of topics, including linguistics, childhood development, mathematics, commonsense reasoning, biology, physics, social bias, software development, and so on.
By scaling the model size, LLMs can even outperform the average human performance under the few-shot setting on 65% of tasks in BIG-bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Considering the high evaluation cost of the entire benchmark, a lightweight benchmark BIG-bench-Lite has been proposed, which contains 24 small yet diverse and challenging tasks from BIG-bench.
Additionally, the BIG-bench hard (BBH) benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib365" title="" class="ltx_ref">365</a>]</cite> has been proposed to concentrate on investigating the currently unsolvable tasks of LLMs by selecting the challenging tasks in which LLMs exhibit inferior performance compared to humans.
Since BBH becomes more difficult, small models mostly achieve performance close to random.
As a comparison, CoT prompting can elicit the abilities of LLMs to perform step-by-step reasoning for enhancing the performance, even exceeding the average human performance in BBH.</p>
</div>
<div id="S7.SS3.SSS1.p4" class="ltx_para">
<p id="S7.SS3.SSS1.p4.1" class="ltx_p"><math id="S7.SS3.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS1.p4.1.m1.1a"><mo id="S7.SS3.SSS1.p4.1.m1.1.1" xref="S7.SS3.SSS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS1.p4.1.m1.1b"><ci id="S7.SS3.SSS1.p4.1.m1.1.1.cmml" xref="S7.SS3.SSS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS1.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS3.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">HELM</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib520" title="" class="ltx_ref">520</a>]</cite> is a comprehensive benchmark that currently implements a core set of 16 scenarios and 7 categories of metrics. It is built on top of many prior studies, conducting a holistic evaluation of language models.
As shown in the experimental results of HELM, instruction tuning can consistently boost the performance of LLMs in terms of accuracy, robustness, and fairness. Further, for reasoning tasks, the LLMs that have been pre-trained on the code corpus show superior performance.</p>
</div>
<div id="S7.SS3.SSS1.p5" class="ltx_para">
<p id="S7.SS3.SSS1.p5.1" class="ltx_p"><math id="S7.SS3.SSS1.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS1.p5.1.m1.1a"><mo id="S7.SS3.SSS1.p5.1.m1.1.1" xref="S7.SS3.SSS1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS1.p5.1.m1.1b"><ci id="S7.SS3.SSS1.p5.1.m1.1.1.cmml" xref="S7.SS3.SSS1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS1.p5.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS3.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">Human-level test benchmarks</em> aim to evaluate the comprehensive ability of LLMs with questions designed for testing humans, such as AGIEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib708" title="" class="ltx_ref">708</a>]</cite>, MMCU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib709" title="" class="ltx_ref">709</a>]</cite>, M3KE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib710" title="" class="ltx_ref">710</a>]</cite>, C-Eval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib711" title="" class="ltx_ref">711</a>]</cite> and Xiezhi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib712" title="" class="ltx_ref">712</a>]</cite>.
These benchmarks encompass a wide range of domains, difficulty levels, and languages to provide a comprehensive evaluation of LLMs’ general capabilities.
Compared to publicly available models, models offering API services (<em id="S7.SS3.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-4, ChatGPT, Claude) demonstrate superior performance compared to publicly available models on these evaluation benchmarks.
As the best-performing model in evaluations, GPT-4 surpasses average human performance in AGIEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib708" title="" class="ltx_ref">708</a>]</cite>.
However, it still lags behind the top human performance on these challenging benchmarks.
Hence, there remains ample room for further enhancements in the overall abilities of LLMs, particularly for publicly accessible models.</p>
</div>
<div id="S7.SS3.SSS1.p6" class="ltx_para">
<p id="S7.SS3.SSS1.p6.1" class="ltx_p">The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs.
Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib735" title="" class="ltx_ref">735</a>]</cite> for multilingual knowledge utilization and MGSM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib524" title="" class="ltx_ref">524</a>]</cite> for multilingual mathematical reasoning. To conduct the evaluation, one can select suitable benchmarks according to specific goals.
In addition, there are also several open-source evaluation frameworks for researchers to evaluate LLMs on existing benchmarks or extend new tasks for customized evaluations, such as Language Model Evaluation Harness&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib736" title="" class="ltx_ref">736</a>]</cite> and OpenAI Evals&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
Further, some researchers
also construct continuously updated leaderboards by aggregating representative benchmarks, to compare the performance of existing LLMs, such as Open LLM Leaderboard&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib707" title="" class="ltx_ref">707</a>]</cite>.
The above benchmarks and leaderboards provide important references to demonstrate the basic and advanced abilities of LLMs. We will give more deep discussions on pros and cons on evaluation approaches in Section&nbsp;<a href="#S7.SS3.SSS2" title="7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3.2</span></a>.</p>
</div>
</section>
<section id="S7.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2 </span>Evaluation Approaches</h4>

<div id="S7.SS3.SSS2.p1" class="ltx_para">
<p id="S7.SS3.SSS2.p1.1" class="ltx_p">After introducing existing benchmarks, in this part, we will review existing evaluation approaches for assessing the performance of LLMs.
To organize our discussion, we categorize LLMs into three different types: <em id="S7.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">base LLMs</em> (pre-trained model checkpoints), <em id="S7.SS3.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">fine-tuned LLMs</em> (instruction or alignment fine-tuned model checkpoints), and <em id="S7.SS3.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">specialized LLMs</em> (adapted model checkpoints for some specific task or domain).
Here, we keep both fine-tuned LLMs and specialized LLMs, to distinguish the different purposes of LLMs: general or specific task solvers.
To evaluate the three types of LLMs, we can test the LLM’s performance related to different abilities (<em id="S7.SS3.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> basic or advanced abilities as discussed in Section&nbsp;<a href="#S7.SS1" title="7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a> and <a href="#S7.SS2" title="7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>).
In general, there are three main approaches to evaluating LLMs, namely benchmark-based approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite>, human-based approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite>, and model-based approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib729" title="" class="ltx_ref">729</a>]</cite>.
Table&nbsp;<a href="#S7.T15" title="TABLE XV ‣ 7.2.3 Tool Manipulation ‣ 7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XV</span></a> shows an illustration of the relationship among LLM type, evaluation approach, and tested abilities.
Next, we will discuss the evaluation approaches for different types of LLMs.</p>
</div>
<div id="S7.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S7.SS3.SSS2.p2.1" class="ltx_p"><span id="S7.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Evaluation of Base LLMs.</span>
Base LLMs refer to the model checkpoints obtained right after pre-training.
For base LLMs, we mainly focus on examining the basic abilities (Section&nbsp;<a href="#S7.SS1" title="7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a>), such as complex reasoning and knowledge utilization.
Since most of these basic abilities can be assessed with well-defined tasks, benchmark-based approaches have been widely used to evaluate base LLMs.
Next, we will introduce common evaluation benchmarks and evaluation procedures for base LLMs.</p>
</div>
<div id="S7.SS3.SSS2.p3" class="ltx_para">
<p id="S7.SS3.SSS2.p3.1" class="ltx_p"><math id="S7.SS3.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p3.1.m1.1a"><mo id="S7.SS3.SSS2.p3.1.m1.1.1" xref="S7.SS3.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.1.m1.1b"><ci id="S7.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">Common benchmarks.</em>
To evaluate base LLMs, typical benchmarks are designed in the form of close-ended problems like multiple-choice questions.
These commonly used benchmarks can be mainly divided into two categories: knowledge-oriented and reasoning-oriented benchmarks.
Knowledge-oriented benchmarks (<em id="S7.SS3.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> and C-Eval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib711" title="" class="ltx_ref">711</a>]</cite>) aim to evaluate the capacity of world knowledge, while reasoning-oriented benchmarks (<em id="S7.SS3.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib643" title="" class="ltx_ref">643</a>]</cite>, BBH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib365" title="" class="ltx_ref">365</a>]</cite>, and MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite>) focus on evaluating the capability of solving complex reasoning tasks.
Further, some recently proposed benchmarks (<em id="S7.SS3.SSS2.p3.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> OpenCompass&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib713" title="" class="ltx_ref">713</a>]</cite>) combine these two types for a comprehensive comparison.</p>
</div>
<div id="S7.SS3.SSS2.p4" class="ltx_para">
<p id="S7.SS3.SSS2.p4.1" class="ltx_p"><math id="S7.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p4.1.m1.1a"><mo id="S7.SS3.SSS2.p4.1.m1.1.1" xref="S7.SS3.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.1.m1.1b"><ci id="S7.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">Benchmark based evaluation procedure.</em>
To perform the benchmark evaluation, each problem will first be formatted into a prompt for LLMs to generate the result text.
Then, the generated result text will be parsed with human-written rules to get the predicted answer.
Finally, the performance of LLMs can be automatically calculated using standard metrics like accuracy by comparing the predicted answer with the ground-truth one.
The evaluation approach can be conducted in either the few-shot or zero-shot setting, which might lead to different evaluation results or rankings.
Since base LLMs have not been instruction fine-tuned (with relatively weak task generalization ability), the few-shot setting is often more suitable for evaluation.
For some complex reasoning tasks, CoT prompts also need to be used to fully exhibit the capacity during evaluation.
Another note is that this evaluation approach can also be applied to assess the abilities of fine-tuned LLMs.
Actually, several leaderboards (<em id="S7.SS3.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Open LLM Leaderboard&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib707" title="" class="ltx_ref">707</a>]</cite>) are built upon this approach, evaluating both base and fine-tuned LLMs.</p>
</div>
<div id="S7.SS3.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S7.SS3.SSS2.p5.1" class="ltx_p"><span id="S7.SS3.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Evaluation of Fine-tuned LLMs.</span>
Fine-tuned LLMs in this part refer to the model checkpoints obtained after instruction tuning or alignment tuning based on pre-trained model weights<span id="footnote47" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">47</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">47</sup><span class="ltx_tag ltx_tag_note">47</span>In some cases, it is also called <em id="footnote47.1" class="ltx_emph ltx_font_italic">chat models</em>.</span></span></span>.
Typically, fine-tuned LLMs will be tested on various abilities (<em id="S7.SS3.SSS2.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> knowledge utilization and human alignment), and thus it is common that they are assessed with multiple evaluation approaches.
In addition to benchmark-based evaluation, human-based and model-based approaches have also been widely used to evaluate the advanced abilities of fine-tuned LLMs.
Next, we will introduce the two evaluation methods.</p>
</div>
<div id="S7.SS3.SSS2.p6" class="ltx_para">
<p id="S7.SS3.SSS2.p6.1" class="ltx_p"><math id="S7.SS3.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p6.1.m1.1a"><mo id="S7.SS3.SSS2.p6.1.m1.1.1" xref="S7.SS3.SSS2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p6.1.m1.1b"><ci id="S7.SS3.SSS2.p6.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p6.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p6.1.1" class="ltx_emph ltx_font_italic">Human-based evaluation.</em>
Unlike automatic evaluation for basic abilities, human evaluation typically considers more factors or abilities in real-world use, such as human alignment and tool manipulation.
In this evaluation approach, test tasks are usually in the form of open-ended questions, and human evaluators are invited to make judgments on the quality of answers generated by LLMs.
Typically, there are two main types of scoring methods for human evaluators: pairwise comparison and single-answer grading.
In pairwise comparison, given the same question, humans are assigned two answers from different models to determine which one is better, while in single-answer grading, they only need to score a single answer at a time.
For example, HELM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib520" title="" class="ltx_ref">520</a>]</cite> employs humans to perform single-answer grading on summarization and disinformation tasks, while Chatbot Arena&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite> constructs a crowdsourcing platform that allows users to engage in conversations with two anonymous chat LLMs and report pairwise comparison results.</p>
</div>
<div id="S7.SS3.SSS2.p7" class="ltx_para">
<p id="S7.SS3.SSS2.p7.1" class="ltx_p"><math id="S7.SS3.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p7.1.m1.1a"><mo id="S7.SS3.SSS2.p7.1.m1.1.1" xref="S7.SS3.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p7.1.m1.1b"><ci id="S7.SS3.SSS2.p7.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Model-based evaluation.</em>
Since human-based evaluation is both expensive and time-consuming, some work has proposed leveraging powerful closed-source LLMs such as ChatGPT and GPT-4 as a surrogate for human evaluators&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib729" title="" class="ltx_ref">729</a>]</cite>.
For example, AlpacaEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib729" title="" class="ltx_ref">729</a>]</cite> collects a set of instructions and utilizes a capable LLM (<em id="S7.SS3.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-4) as the judge to perform pair-wise comparisons against the reference outputs.
Furthermore, MT-bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite> collects a set of multi-turn questions for evaluation and improves the reliability of LLM-based evaluators through methods like ICL and CoT.
Compared with human evaluators, LLMs such as ChatGPT and GPT-4 can achieve high agreement with humans, in both small-scale handcrafted and large-scale crowdsourced evaluation tasks.
Despite this, these closed-source LLMs are limited in access and have the potential risk of data leakage.
To address this, recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite> has explored fine-tuning open-source LLMs (<em id="S7.SS3.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>) as model evaluators using scoring data from human evaluators, which has narrowed the gap with powerful closed-source LLMs (<em id="S7.SS3.SSS2.p7.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-4).</p>
</div>
<div id="S7.SS3.SSS2.p8" class="ltx_para ltx_noindent">
<p id="S7.SS3.SSS2.p8.1" class="ltx_p"><span id="S7.SS3.SSS2.p8.1.1" class="ltx_text ltx_font_bold">Evaluation of Specialized LLMs.</span>
Specialized LLMs refer to the model checkpoints specially adapted to some domains or applications like healthcare&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>]</cite> and finance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib737" title="" class="ltx_ref">737</a>]</cite>.
As special task solvers, specialized LLMs will be tested not only on general abilities (<em id="S7.SS3.SSS2.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> basic ability like complex reasoning and advanced ability like human alignment), but also on specific abilities related to their designated domains or applications.
For this purpose, one often needs to construct specific benchmarks tailored for the target domains or applications.
Then, these domain-specific benchmarks can be combined with general benchmarks to conduct both comprehensive and targeted evaluation for specialized LLMs.
For example, MultiMedQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>]</cite> is a specific benchmark in healthcare, which includes medical examinations and healthcare questions.
In this work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>]</cite>, MultiMedQA has been combined with MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> to assess the performance of specialized LLMs for healthcare, such as Med-PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>]</cite>.
Similarly, FLUE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib737" title="" class="ltx_ref">737</a>]</cite> constructs a benchmark for finance, spanning from financial sentiment analysis to question answering.
It has been used collaboratively with BBH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib365" title="" class="ltx_ref">365</a>]</cite> to evaluate finical LLMs like BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>.</p>
</div>
<div id="S7.SS3.SSS2.p9" class="ltx_para ltx_noindent">
<p id="S7.SS3.SSS2.p9.1" class="ltx_p"><span id="S7.SS3.SSS2.p9.1.1" class="ltx_text ltx_font_bold">Pros and Cons of Different Evaluation Approaches</span>.
In the above, we have discussed different evaluation approaches to assess the abilities of LLMs.
Next, we simply analyze the pros and cons of each evaluation approach.</p>
</div>
<div id="S7.SS3.SSS2.p10" class="ltx_para">
<p id="S7.SS3.SSS2.p10.1" class="ltx_p"><math id="S7.SS3.SSS2.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p10.1.m1.1a"><mo id="S7.SS3.SSS2.p10.1.m1.1.1" xref="S7.SS3.SSS2.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p10.1.m1.1b"><ci id="S7.SS3.SSS2.p10.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p10.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p10.1.1" class="ltx_emph ltx_font_italic">Benchmark-based approach</em>.
This evaluation approach can leverage existing benchmarks for assessing the performance of LLMs.
The tasks involved in these benchmarks often contain sufficient test samples to measure the core abilities (<em id="S7.SS3.SSS2.p10.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> reasoning).
The whole evaluation procedure can be (almost) automatic, and it is convenient to carry out test experiments for various base LLMs, especially useful for monitoring the performance of model checkpoints during pre-training.
However, LLMs are often sensitive to the evaluation settings, including the question prompts, zero-shot or few-shot tests, and the answer parsing methods.
Thus, one should take possible influencing factors into consideration when conducting the evaluation experiments.
The evaluation results should be noted with the adopted evaluation settings.
Another issue is the data contamination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib738" title="" class="ltx_ref">738</a>]</cite>, <em id="S7.SS3.SSS2.p10.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> the test data itself or relevant content has been contained in the pre-training corpora.
This phenomenon has become increasingly severe since more and more open data has been collected for developing LLMs.</p>
</div>
<div id="S7.SS3.SSS2.p11" class="ltx_para">
<p id="S7.SS3.SSS2.p11.1" class="ltx_p"><math id="S7.SS3.SSS2.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p11.1.m1.1a"><mo id="S7.SS3.SSS2.p11.1.m1.1.1" xref="S7.SS3.SSS2.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p11.1.m1.1b"><ci id="S7.SS3.SSS2.p11.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p11.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p11.1.1" class="ltx_emph ltx_font_italic">Human-based approach</em>.
Human evaluation offers several advantages when assessing the capabilities of LLMs to solve real-world tasks.
One of the key benefits is its ability to directly reflect the actual abilities of LLMs.
Based on feedback and experiences from real users, human evaluation provides a more direct measure of LLMs’ performance in real-world scenarios.
Further, it can conduct more flexible and diverse evaluation tasks based on human evaluators.
For instance, users can submit various queries and test the abilities of LLMs according to their own task cognition.
It allows for a deep understanding of the strengths and weaknesses of LLMs across different types of tasks and contexts.
However, human evaluation also has inherent limitations that could potentially affect its accuracy and consistency.
Factors such as personalized tastes and varying education levels among evaluators can introduce biases or even inconsistencies in the evaluation process.
In some cases, users’ judgments are likely to be subjective, which may not reflect the true capabilities of the LLMs.
Moreover, conducting robust and reliable human evaluations often requires a large number of evaluators, which can be very expensive and time-consuming.
In addition, human evaluation is often not reproducible, making it infeasible to extend existing evaluation results or track the progress of LLMs.</p>
</div>
<div id="S7.SS3.SSS2.p12" class="ltx_para">
<p id="S7.SS3.SSS2.p12.1" class="ltx_p"><math id="S7.SS3.SSS2.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS3.SSS2.p12.1.m1.1a"><mo id="S7.SS3.SSS2.p12.1.m1.1.1" xref="S7.SS3.SSS2.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p12.1.m1.1b"><ci id="S7.SS3.SSS2.p12.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p12.1.m1.1c">\bullet</annotation></semantics></math>&nbsp;<em id="S7.SS3.SSS2.p12.1.1" class="ltx_emph ltx_font_italic">Model-based approach</em>.
As a surrogate for human-based approaches, model-based approaches serve to diminish the reliance on human involvement, and enable more efficient and scalable evaluation.
In addition, LLMs can provide meaningful explanations for the assigned rating scores, thereby enhancing the interpretability of evaluations.
Despite their scalability and explanability, model-based approaches have been found to suffer from several issues, including position, verbosity, and self-enhancement bias&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite>.
Specially, position bias (<em id="S7.SS3.SSS2.p12.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> the order to present the responses) refers to the fact that LLMs tend to assign high scores for the answers at specific positions over others,
verbosity bias means that LLMs favor verbose answers even if they are short in quality compared with shorter answers, and
self-enhancement bias indicates that LLMs often overrate in their own generations.
In addition, since LLMs have limited capacities in solving complex reasoning problems, they cannot serve as qualified evaluators for some difficult tasks (<em id="S7.SS3.SSS2.p12.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> mathematical reasoning).
These limitations can be mitigated to some extent by specific prompt engineering and fine-tuning strategies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite>.</p>
</div>
<div id="S7.SS3.SSS2.p13" class="ltx_para">
<p id="S7.SS3.SSS2.p13.1" class="ltx_p">To summarize, our categorization (Table&nbsp;<a href="#S7.T15" title="TABLE XV ‣ 7.2.3 Tool Manipulation ‣ 7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XV</span></a>) of existing work on LLM evaluation is mainly based on two major dimensions, namely evaluation methodology and model type, which are further extended with the test abilities.
There are some recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib733" title="" class="ltx_ref">733</a>, <a href="#bib.bib734" title="" class="ltx_ref">734</a>]</cite> that also has discussed the categorization or taxonomies of existing work for LLM evaluation.</p>
</div>
<figure id="S7.T16" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XVI: </span>Evaluation on the eight abilities of LLMs with specially selected tasks. The shade of the <span id="S7.T16.30.1" class="ltx_text" style="background-color:#FC8D59;">Orange</span> and <span id="S7.T16.31.2" class="ltx_text" style="background-color:#92BFDB;">Blue</span> fonts denote the performance orders of the results in closed-source and open-source models, respectively. This table will be continuously updated by incorporating the results of more models.
</figcaption>
<div id="S7.T16.27" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:802.2pt;height:1187.7pt;vertical-align:-1.3pt;"><span class="ltx_transformed_inner" style="transform:translate(90.7pt,-134.2pt) scale(1.29234016719243,1.29234016719243) ;">
<table id="S7.T16.27.27" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T16.27.27.28" class="ltx_tr">
<td id="S7.T16.27.27.28.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 2.5pt;" rowspan="2"><span id="S7.T16.27.27.28.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S7.T16.27.27.28.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.5pt;" colspan="4"><span id="S7.T16.27.27.28.2.1" class="ltx_text ltx_font_bold">Language Generation</span></td>
<td id="S7.T16.27.27.28.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.5pt;" colspan="5"><span id="S7.T16.27.27.28.3.1" class="ltx_text ltx_font_bold">Knowledge Utilization</span></td>
</tr>
<tr id="S7.T16.9.9.9" class="ltx_tr">
<td id="S7.T16.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">LBD<math id="S7.T16.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.1.1.1.1.m1.1a"><mo stretchy="false" id="S7.T16.1.1.1.1.m1.1.1" xref="S7.T16.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.1.1.1.1.m1.1b"><ci id="S7.T16.1.1.1.1.m1.1.1.cmml" xref="S7.T16.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">WMT<math id="S7.T16.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.2.2.2.2.m1.1a"><mo stretchy="false" id="S7.T16.2.2.2.2.m1.1.1" xref="S7.T16.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.2.2.2.2.m1.1b"><ci id="S7.T16.2.2.2.2.m1.1.1.cmml" xref="S7.T16.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">XSum<math id="S7.T16.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.3.3.3.3.m1.1a"><mo stretchy="false" id="S7.T16.3.3.3.3.m1.1.1" xref="S7.T16.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.3.3.3.3.m1.1b"><ci id="S7.T16.3.3.3.3.m1.1.1.cmml" xref="S7.T16.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">HumanEval<math id="S7.T16.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.4.4.4.4.m1.1a"><mo stretchy="false" id="S7.T16.4.4.4.4.m1.1.1" xref="S7.T16.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.4.4.4.4.m1.1b"><ci id="S7.T16.4.4.4.4.m1.1.1.cmml" xref="S7.T16.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">TriviaQA<math id="S7.T16.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.5.5.5.5.m1.1a"><mo stretchy="false" id="S7.T16.5.5.5.5.m1.1.1" xref="S7.T16.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.5.5.5.5.m1.1b"><ci id="S7.T16.5.5.5.5.m1.1.1.cmml" xref="S7.T16.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">NaturalQ<math id="S7.T16.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.6.6.6.6.m1.1a"><mo stretchy="false" id="S7.T16.6.6.6.6.m1.1.1" xref="S7.T16.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.6.6.6.6.m1.1b"><ci id="S7.T16.6.6.6.6.m1.1.1.cmml" xref="S7.T16.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">WebQ<math id="S7.T16.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.7.7.7.7.m1.1a"><mo stretchy="false" id="S7.T16.7.7.7.7.m1.1.1" xref="S7.T16.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.7.7.7.7.m1.1b"><ci id="S7.T16.7.7.7.7.m1.1.1.cmml" xref="S7.T16.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.8.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">ARC<math id="S7.T16.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.8.8.8.8.m1.1a"><mo stretchy="false" id="S7.T16.8.8.8.8.m1.1.1" xref="S7.T16.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.8.8.8.8.m1.1b"><ci id="S7.T16.8.8.8.8.m1.1.1.cmml" xref="S7.T16.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.9.9.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">WikiFact<math id="S7.T16.9.9.9.9.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.9.9.9.9.m1.1a"><mo stretchy="false" id="S7.T16.9.9.9.9.m1.1.1" xref="S7.T16.9.9.9.9.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.9.9.9.9.m1.1b"><ci id="S7.T16.9.9.9.9.m1.1.1.cmml" xref="S7.T16.9.9.9.9.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.9.9.9.9.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S7.T16.27.27.29" class="ltx_tr">
<td id="S7.T16.27.27.29.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">ChatGPT</td>
<td id="S7.T16.27.27.29.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.2.1" class="ltx_text" style="background-color:#FEE8DD;">55.81</span></td>
<td id="S7.T16.27.27.29.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.3.1" class="ltx_text" style="background-color:#FCA77F;">36.44</span></td>
<td id="S7.T16.27.27.29.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.4.1" class="ltx_text" style="background-color:#FC8D59;">21.71</span></td>
<td id="S7.T16.27.27.29.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.5.1" class="ltx_text" style="background-color:#FC8D59;">79.88</span></td>
<td id="S7.T16.27.27.29.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.6.1" class="ltx_text" style="background-color:#FC8D59;">54.54</span></td>
<td id="S7.T16.27.27.29.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.7.1" class="ltx_text" style="background-color:#FC8D59;">21.52</span></td>
<td id="S7.T16.27.27.29.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.8.1" class="ltx_text" style="background-color:#FEDCCC;">17.77</span></td>
<td id="S7.T16.27.27.29.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.9.1" class="ltx_text" style="background-color:#FC8D59;">93.69</span></td>
<td id="S7.T16.27.27.29.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.29.10.1" class="ltx_text" style="background-color:#FEDCCC;">29.25</span></td>
</tr>
<tr id="S7.T16.27.27.30" class="ltx_tr">
<td id="S7.T16.27.27.30.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Claude</td>
<td id="S7.T16.27.27.30.2" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.2.1" class="ltx_text" style="background-color:#FCA77F;">64.47</span></td>
<td id="S7.T16.27.27.30.3" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.3.1" class="ltx_text" style="background-color:#FEE8DD;">31.23</span></td>
<td id="S7.T16.27.27.30.4" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.4.1" class="ltx_text" style="background-color:#FEE8DD;">18.63</span></td>
<td id="S7.T16.27.27.30.5" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.5.1" class="ltx_text" style="background-color:#FEF7F3;">51.22</span></td>
<td id="S7.T16.27.27.30.6" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.6.1" class="ltx_text" style="background-color:#FEF7F3;">40.92</span></td>
<td id="S7.T16.27.27.30.7" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.7.1" class="ltx_text" style="background-color:#FEF7F3;">13.77</span></td>
<td id="S7.T16.27.27.30.8" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.8.1" class="ltx_text" style="background-color:#FEF7F3;">14.57</span></td>
<td id="S7.T16.27.27.30.9" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.9.1" class="ltx_text" style="background-color:#FEF7F3;">66.62</span></td>
<td id="S7.T16.27.27.30.10" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.30.10.1" class="ltx_text" style="background-color:#FCA77F;">34.34</span></td>
</tr>
<tr id="S7.T16.27.27.31" class="ltx_tr">
<td id="S7.T16.27.27.31.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Claude 2</td>
<td id="S7.T16.27.27.31.2" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.2.1" class="ltx_text" style="background-color:#FEF7F3;">45.20</span></td>
<td id="S7.T16.27.27.31.3" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.3.1" class="ltx_text" style="background-color:#FEF7F3;">12.93</span></td>
<td id="S7.T16.27.27.31.4" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.4.1" class="ltx_text" style="background-color:#FEDCCC;">19.13</span></td>
<td id="S7.T16.27.27.31.5" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.5.1" class="ltx_text" style="background-color:#FCA77F;">78.04</span></td>
<td id="S7.T16.27.27.31.6" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.6.1" class="ltx_text" style="background-color:#FCA77F;">54.30</span></td>
<td id="S7.T16.27.27.31.7" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.7.1" class="ltx_text" style="background-color:#FCA77F;">21.30</span></td>
<td id="S7.T16.27.27.31.8" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.8.1" class="ltx_text" style="background-color:#FC8D59;">21.06</span></td>
<td id="S7.T16.27.27.31.9" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.9.1" class="ltx_text" style="background-color:#FEE8DD;">79.97</span></td>
<td id="S7.T16.27.27.31.10" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.31.10.1" class="ltx_text" style="background-color:#FC8D59;">35.83</span></td>
</tr>
<tr id="S7.T16.27.27.32" class="ltx_tr">
<td id="S7.T16.27.27.32.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Davinci003</td>
<td id="S7.T16.27.27.32.2" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.2.1" class="ltx_text" style="background-color:#FC8D59;">69.98</span></td>
<td id="S7.T16.27.27.32.3" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.3.1" class="ltx_text" style="background-color:#FC8D59;">37.46</span></td>
<td id="S7.T16.27.27.32.4" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.4.1" class="ltx_text" style="background-color:#FEF7F3;">18.19</span></td>
<td id="S7.T16.27.27.32.5" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.5.1" class="ltx_text" style="background-color:#FEDCCC;">67.07</span></td>
<td id="S7.T16.27.27.32.6" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.6.1" class="ltx_text" style="background-color:#FEE8DD;">51.51</span></td>
<td id="S7.T16.27.27.32.7" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.7.1" class="ltx_text" style="background-color:#FEE8DD;">17.76</span></td>
<td id="S7.T16.27.27.32.8" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.8.1" class="ltx_text" style="background-color:#FEE8DD;">16.68</span></td>
<td id="S7.T16.27.27.32.9" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.9.1" class="ltx_text" style="background-color:#FEDCCC;">88.47</span></td>
<td id="S7.T16.27.27.32.10" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.32.10.1" class="ltx_text" style="background-color:#FEF7F3;">28.29</span></td>
</tr>
<tr id="S7.T16.27.27.33" class="ltx_tr">
<td id="S7.T16.27.27.33.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Davinci002</td>
<td id="S7.T16.27.27.33.2" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.2.1" class="ltx_text" style="background-color:#FEDCCC;">58.85</span></td>
<td id="S7.T16.27.27.33.3" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.3.1" class="ltx_text" style="background-color:#FEDCCC;">35.11</span></td>
<td id="S7.T16.27.27.33.4" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.4.1" class="ltx_text" style="background-color:#FCA77F;">19.15</span></td>
<td id="S7.T16.27.27.33.5" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.5.1" class="ltx_text" style="background-color:#FEE8DD;">56.70</span></td>
<td id="S7.T16.27.27.33.6" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.6.1" class="ltx_text" style="background-color:#FEDCCC;">52.11</span></td>
<td id="S7.T16.27.27.33.7" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.7.1" class="ltx_text" style="background-color:#FEDCCC;">20.47</span></td>
<td id="S7.T16.27.27.33.8" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.8.1" class="ltx_text" style="background-color:#FCA77F;">18.45</span></td>
<td id="S7.T16.27.27.33.9" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.9.1" class="ltx_text" style="background-color:#FCA77F;">89.23</span></td>
<td id="S7.T16.27.27.33.10" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.33.10.1" class="ltx_text" style="background-color:#FEE8DD;">29.15</span></td>
</tr>
<tr id="S7.T16.27.27.34" class="ltx_tr">
<td id="S7.T16.27.27.34.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">LLaMA 2-Chat&nbsp;(7B)</td>
<td id="S7.T16.27.27.34.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">56.12</td>
<td id="S7.T16.27.27.34.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">12.62</td>
<td id="S7.T16.27.27.34.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.34.4.1" class="ltx_text" style="background-color:#A7CBE2;">16.00</span></td>
<td id="S7.T16.27.27.34.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">11.59</td>
<td id="S7.T16.27.27.34.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.34.6.1" class="ltx_text" style="background-color:#92BFDB;">38.93</span></td>
<td id="S7.T16.27.27.34.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.34.7.1" class="ltx_text" style="background-color:#92BFDB;">12.96</span></td>
<td id="S7.T16.27.27.34.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.34.8.1" class="ltx_text" style="background-color:#A7CBE2;">11.32</span></td>
<td id="S7.T16.27.27.34.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.34.9.1" class="ltx_text" style="background-color:#92BFDB;">72.35</span></td>
<td id="S7.T16.27.27.34.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">23.37</td>
</tr>
<tr id="S7.T16.27.27.35" class="ltx_tr">
<td id="S7.T16.27.27.35.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Vicuna&nbsp;(13B)</td>
<td id="S7.T16.27.27.35.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">62.45</td>
<td id="S7.T16.27.27.35.3" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.3.1" class="ltx_text" style="background-color:#A7CBE2;">20.49</span></td>
<td id="S7.T16.27.27.35.4" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.4.1" class="ltx_text" style="background-color:#92BFDB;">17.87</span></td>
<td id="S7.T16.27.27.35.5" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.5.1" class="ltx_text" style="background-color:#92BFDB;">20.73</span></td>
<td id="S7.T16.27.27.35.6" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.6.1" class="ltx_text" style="background-color:#C4DDEC;">29.04</span></td>
<td id="S7.T16.27.27.35.7" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.7.1" class="ltx_text" style="background-color:#C6DEED;">10.75</span></td>
<td id="S7.T16.27.27.35.8" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.8.1" class="ltx_text" style="background-color:#92BFDB;">11.52</span></td>
<td id="S7.T16.27.27.35.9" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.9.1" class="ltx_text" style="background-color:#E5F0F7;">
20.69</span></td>
<td id="S7.T16.27.27.35.10" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.35.10.1" class="ltx_text" style="background-color:#92BFDB;">28.76</span></td>
</tr>
<tr id="S7.T16.27.27.36" class="ltx_tr">
<td id="S7.T16.27.27.36.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Vicuna&nbsp;(7B)</td>
<td id="S7.T16.27.27.36.2" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.36.2.1" class="ltx_text" style="background-color:#C4DDEC;">63.90</span></td>
<td id="S7.T16.27.27.36.3" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.36.3.1" class="ltx_text" style="background-color:#C6DEED;">19.95</span></td>
<td id="S7.T16.27.27.36.4" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.36.4.1" class="ltx_text" style="background-color:#C6DEED;">13.59</span></td>
<td id="S7.T16.27.27.36.5" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.36.5.1" class="ltx_text" style="background-color:#A7CBE2;">17.07</span></td>
<td id="S7.T16.27.27.36.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">28.58</td>
<td id="S7.T16.27.27.36.7" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.36.7.1" class="ltx_text" style="background-color:#C4DDEC;">9.17</span></td>
<td id="S7.T16.27.27.36.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">6.64</td>
<td id="S7.T16.27.27.36.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">16.96</td>
<td id="S7.T16.27.27.36.10" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.36.10.1" class="ltx_text" style="background-color:#C6DEED;">26.95</span></td>
</tr>
<tr id="S7.T16.27.27.37" class="ltx_tr">
<td id="S7.T16.27.27.37.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Alpaca&nbsp;(7B)</td>
<td id="S7.T16.27.27.37.2" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.37.2.1" class="ltx_text" style="background-color:#E5F0F7;">63.35</span></td>
<td id="S7.T16.27.27.37.3" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.37.3.1" class="ltx_text" style="background-color:#92BFDB;">21.52</span></td>
<td id="S7.T16.27.27.37.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">8.74</td>
<td id="S7.T16.27.27.37.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">13.41</td>
<td id="S7.T16.27.27.37.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">17.14</td>
<td id="S7.T16.27.27.37.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">3.24</td>
<td id="S7.T16.27.27.37.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">3.00</td>
<td id="S7.T16.27.27.37.9" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.37.9.1" class="ltx_text" style="background-color:#C6DEED;">49.75</span></td>
<td id="S7.T16.27.27.37.10" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.37.10.1" class="ltx_text" style="background-color:#C4DDEC;">26.05</span></td>
</tr>
<tr id="S7.T16.27.27.38" class="ltx_tr">
<td id="S7.T16.27.27.38.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">ChatGLM&nbsp;(6B)</td>
<td id="S7.T16.27.27.38.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">33.34</td>
<td id="S7.T16.27.27.38.3" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.38.3.1" class="ltx_text" style="background-color:#C4DDEC;">16.58</span></td>
<td id="S7.T16.27.27.38.4" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.38.4.1" class="ltx_text" style="background-color:#C4DDEC;">13.48</span></td>
<td id="S7.T16.27.27.38.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">13.42</td>
<td id="S7.T16.27.27.38.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">13.42</td>
<td id="S7.T16.27.27.38.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">4.40</td>
<td id="S7.T16.27.27.38.8" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.38.8.1" class="ltx_text" style="background-color:#C4DDEC;">9.20</span></td>
<td id="S7.T16.27.27.38.9" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.38.9.1" class="ltx_text" style="background-color:#A7CBE2;">55.39</span></td>
<td id="S7.T16.27.27.38.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">16.01</td>
</tr>
<tr id="S7.T16.27.27.39" class="ltx_tr">
<td id="S7.T16.27.27.39.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">LLaMA 2&nbsp;(7B)</td>
<td id="S7.T16.27.27.39.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.39.2.1" class="ltx_text" style="background-color:#C6DEED;">66.39</span></td>
<td id="S7.T16.27.27.39.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">11.57</td>
<td id="S7.T16.27.27.39.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.39.4.1" class="ltx_text" style="background-color:#E5F0F7;">11.57</span></td>
<td id="S7.T16.27.27.39.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.39.5.1" class="ltx_text" style="background-color:#A7CBE2;">17.07</span></td>
<td id="S7.T16.27.27.39.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.39.6.1" class="ltx_text" style="background-color:#C6DEED;">30.92</span></td>
<td id="S7.T16.27.27.39.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">5.15</td>
<td id="S7.T16.27.27.39.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">2.51</td>
<td id="S7.T16.27.27.39.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.39.9.1" class="ltx_text" style="background-color:#C4DDEC;">24.16</span></td>
<td id="S7.T16.27.27.39.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.39.10.1" class="ltx_text" style="background-color:#A7CBE2;">28.06</span></td>
</tr>
<tr id="S7.T16.27.27.40" class="ltx_tr">
<td id="S7.T16.27.27.40.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">LLaMA&nbsp;(7B)</td>
<td id="S7.T16.27.27.40.2" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.40.2.1" class="ltx_text" style="background-color:#92BFDB;">67.68</span></td>
<td id="S7.T16.27.27.40.3" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.40.3.1" class="ltx_text" style="background-color:#E5F0F7;">13.84</span></td>
<td id="S7.T16.27.27.40.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">8.77</td>
<td id="S7.T16.27.27.40.5" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.40.5.1" class="ltx_text" style="background-color:#C4DDEC;">15.24</span></td>
<td id="S7.T16.27.27.40.6" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.40.6.1" class="ltx_text" style="background-color:#A7CBE2;">34.62</span></td>
<td id="S7.T16.27.27.40.7" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.40.7.1" class="ltx_text" style="background-color:#E5F0F7;">7.92</span></td>
<td id="S7.T16.27.27.40.8" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.40.8.1" class="ltx_text" style="background-color:#C6DEED;">11.12</span></td>
<td id="S7.T16.27.27.40.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">4.88</td>
<td id="S7.T16.27.27.40.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">19.78</td>
</tr>
<tr id="S7.T16.27.27.41" class="ltx_tr">
<td id="S7.T16.27.27.41.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Falcon&nbsp;(7B)</td>
<td id="S7.T16.27.27.41.2" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.41.2.1" class="ltx_text" style="background-color:#A7CBE2;">66.89</span></td>
<td id="S7.T16.27.27.41.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">4.05</td>
<td id="S7.T16.27.27.41.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">10.00</td>
<td id="S7.T16.27.27.41.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">10.37</td>
<td id="S7.T16.27.27.41.6" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.41.6.1" class="ltx_text" style="background-color:#E5F0F7;">28.74</span></td>
<td id="S7.T16.27.27.41.7" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.41.7.1" class="ltx_text" style="background-color:#A7CBE2;">10.78</span></td>
<td id="S7.T16.27.27.41.8" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.41.8.1" class="ltx_text" style="background-color:#E5F0F7;">8.46</span></td>
<td id="S7.T16.27.27.41.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">4.08</td>
<td id="S7.T16.27.27.41.10" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.41.10.1" class="ltx_text" style="background-color:#E5F0F7;">23.91</span></td>
</tr>
<tr id="S7.T16.27.27.42" class="ltx_tr">
<td id="S7.T16.27.27.42.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Pythia&nbsp;(12B)</td>
<td id="S7.T16.27.27.42.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">61.19</td>
<td id="S7.T16.27.27.42.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">5.43</td>
<td id="S7.T16.27.27.42.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">8.87</td>
<td id="S7.T16.27.27.42.5" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.42.5.1" class="ltx_text" style="background-color:#E5F0F7;">14.63</span></td>
<td id="S7.T16.27.27.42.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">15.73</td>
<td id="S7.T16.27.27.42.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.99</td>
<td id="S7.T16.27.27.42.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">4.72</td>
<td id="S7.T16.27.27.42.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">11.66</td>
<td id="S7.T16.27.27.42.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">20.57</td>
</tr>
<tr id="S7.T16.27.27.43" class="ltx_tr">
<td id="S7.T16.27.27.43.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Pythia&nbsp;(7B)</td>
<td id="S7.T16.27.27.43.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">56.96</td>
<td id="S7.T16.27.27.43.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">3.68</td>
<td id="S7.T16.27.27.43.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">8.23</td>
<td id="S7.T16.27.27.43.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">9.15</td>
<td id="S7.T16.27.27.43.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">10.16</td>
<td id="S7.T16.27.27.43.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.77</td>
<td id="S7.T16.27.27.43.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">3.74</td>
<td id="S7.T16.27.27.43.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">11.03</td>
<td id="S7.T16.27.27.43.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">15.75</td>
</tr>
<tr id="S7.T16.27.27.44" class="ltx_tr">
<td id="S7.T16.27.27.44.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;" rowspan="2"><span id="S7.T16.27.27.44.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S7.T16.27.27.44.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;" colspan="3"><span id="S7.T16.27.27.44.2.1" class="ltx_text ltx_font_bold">Knowledge Reasoning</span></td>
<td id="S7.T16.27.27.44.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;" colspan="2"><span id="S7.T16.27.27.44.3.1" class="ltx_text ltx_font_bold">Symbolic Reasoning</span></td>
<td id="S7.T16.27.27.44.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;" colspan="2"><span id="S7.T16.27.27.44.4.1" class="ltx_text ltx_font_bold">Mathematical Reasoning</span></td>
<td id="S7.T16.27.27.44.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;" colspan="2"><span id="S7.T16.27.27.44.5.1" class="ltx_text ltx_font_bold">Interaction with Environment</span></td>
</tr>
<tr id="S7.T16.18.18.18" class="ltx_tr">
<td id="S7.T16.10.10.10.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">OBQA<math id="S7.T16.10.10.10.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.10.10.10.1.m1.1a"><mo stretchy="false" id="S7.T16.10.10.10.1.m1.1.1" xref="S7.T16.10.10.10.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.10.10.10.1.m1.1b"><ci id="S7.T16.10.10.10.1.m1.1.1.cmml" xref="S7.T16.10.10.10.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.10.10.10.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.11.11.11.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">HellaSwag<math id="S7.T16.11.11.11.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.11.11.11.2.m1.1a"><mo stretchy="false" id="S7.T16.11.11.11.2.m1.1.1" xref="S7.T16.11.11.11.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.11.11.11.2.m1.1b"><ci id="S7.T16.11.11.11.2.m1.1.1.cmml" xref="S7.T16.11.11.11.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.11.11.11.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.12.12.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">SocialIQA<math id="S7.T16.12.12.12.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.12.12.12.3.m1.1a"><mo stretchy="false" id="S7.T16.12.12.12.3.m1.1.1" xref="S7.T16.12.12.12.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.12.12.12.3.m1.1b"><ci id="S7.T16.12.12.12.3.m1.1.1.cmml" xref="S7.T16.12.12.12.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.12.12.12.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.13.13.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">C-Objects<math id="S7.T16.13.13.13.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.13.13.13.4.m1.1a"><mo stretchy="false" id="S7.T16.13.13.13.4.m1.1.1" xref="S7.T16.13.13.13.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.13.13.13.4.m1.1b"><ci id="S7.T16.13.13.13.4.m1.1.1.cmml" xref="S7.T16.13.13.13.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.13.13.13.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.14.14.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">Penguins<math id="S7.T16.14.14.14.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.14.14.14.5.m1.1a"><mo stretchy="false" id="S7.T16.14.14.14.5.m1.1.1" xref="S7.T16.14.14.14.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.14.14.14.5.m1.1b"><ci id="S7.T16.14.14.14.5.m1.1.1.cmml" xref="S7.T16.14.14.14.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.14.14.14.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.15.15.15.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">GSM8k<math id="S7.T16.15.15.15.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.15.15.15.6.m1.1a"><mo stretchy="false" id="S7.T16.15.15.15.6.m1.1.1" xref="S7.T16.15.15.15.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.15.15.15.6.m1.1b"><ci id="S7.T16.15.15.15.6.m1.1.1.cmml" xref="S7.T16.15.15.15.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.15.15.15.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.16.16.16.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">MATH<math id="S7.T16.16.16.16.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.16.16.16.7.m1.1a"><mo stretchy="false" id="S7.T16.16.16.16.7.m1.1.1" xref="S7.T16.16.16.16.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.16.16.16.7.m1.1b"><ci id="S7.T16.16.16.16.7.m1.1.1.cmml" xref="S7.T16.16.16.16.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.16.16.16.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.17.17.17.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">ALFW<math id="S7.T16.17.17.17.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.17.17.17.8.m1.1a"><mo stretchy="false" id="S7.T16.17.17.17.8.m1.1.1" xref="S7.T16.17.17.17.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.17.17.17.8.m1.1b"><ci id="S7.T16.17.17.17.8.m1.1.1.cmml" xref="S7.T16.17.17.17.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.17.17.17.8.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.18.18.18.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">WebShop<math id="S7.T16.18.18.18.9.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.18.18.18.9.m1.1a"><mo stretchy="false" id="S7.T16.18.18.18.9.m1.1.1" xref="S7.T16.18.18.18.9.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.18.18.18.9.m1.1b"><ci id="S7.T16.18.18.18.9.m1.1.1.cmml" xref="S7.T16.18.18.18.9.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.18.18.18.9.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S7.T16.27.27.45" class="ltx_tr">
<td id="S7.T16.27.27.45.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">ChatGPT</td>
<td id="S7.T16.27.27.45.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.2.1" class="ltx_text" style="background-color:#FCA77F;">81.20</span></td>
<td id="S7.T16.27.27.45.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.3.1" class="ltx_text" style="background-color:#FCA77F;">61.43</span></td>
<td id="S7.T16.27.27.45.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.4.1" class="ltx_text" style="background-color:#FC8D59;">73.23</span></td>
<td id="S7.T16.27.27.45.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.5.1" class="ltx_text" style="background-color:#FEF7F3;">53.20</span></td>
<td id="S7.T16.27.27.45.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.6.1" class="ltx_text" style="background-color:#FEF7F3;">40.27</span></td>
<td id="S7.T16.27.27.45.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.7.1" class="ltx_text" style="background-color:#FCA77F;">78.47</span></td>
<td id="S7.T16.27.27.45.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.8.1" class="ltx_text" style="background-color:#FC8D59;">33.78</span></td>
<td id="S7.T16.27.27.45.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.9.1" class="ltx_text" style="background-color:#FEF7F3;">58.96</span></td>
<td id="S7.T16.27.27.45.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.45.10.1" class="ltx_text" style="background-color:#FEDCCC;">45.12/15.60</span></td>
</tr>
<tr id="S7.T16.27.27.46" class="ltx_tr">
<td id="S7.T16.27.27.46.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Claude</td>
<td id="S7.T16.27.27.46.2" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.2.1" class="ltx_text" style="background-color:#FC8D59;">81.80</span></td>
<td id="S7.T16.27.27.46.3" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.3.1" class="ltx_text" style="background-color:#FEDCCC;">54.95</span></td>
<td id="S7.T16.27.27.46.4" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.4.1" class="ltx_text" style="background-color:#FC8D59;">73.23</span></td>
<td id="S7.T16.27.27.46.5" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.5.1" class="ltx_text" style="background-color:#FEE8DD;">59.95</span></td>
<td id="S7.T16.27.27.46.6" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.6.1" class="ltx_text" style="background-color:#FEE8DD;">47.65</span></td>
<td id="S7.T16.27.27.46.7" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.7.1" class="ltx_text" style="background-color:#FEDCCC;">70.81</span></td>
<td id="S7.T16.27.27.46.8" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.8.1" class="ltx_text" style="background-color:#FEDCCC;">20.18</span></td>
<td id="S7.T16.27.27.46.9" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.9.1" class="ltx_text" style="background-color:#FCA77F;">76.87</span></td>
<td id="S7.T16.27.27.46.10" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.46.10.1" class="ltx_text" style="background-color:#FCA77F;">47.72/23.00</span></td>
</tr>
<tr id="S7.T16.27.27.47" class="ltx_tr">
<td id="S7.T16.27.27.47.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Claude 2</td>
<td id="S7.T16.27.27.47.2" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.2.1" class="ltx_text" style="background-color:#FEE8DD;">71.60</span></td>
<td id="S7.T16.27.27.47.3" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.3.1" class="ltx_text" style="background-color:#FEE8DD;">50.75</span></td>
<td id="S7.T16.27.27.47.4" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.4.1" class="ltx_text" style="background-color:#FEE8DD;">58.34</span></td>
<td id="S7.T16.27.27.47.5" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.5.1" class="ltx_text" style="background-color:#FC8D59;">66.76</span></td>
<td id="S7.T16.27.27.47.6" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.6.1" class="ltx_text" style="background-color:#FC8D59;">74.50</span></td>
<td id="S7.T16.27.27.47.7" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.7.1" class="ltx_text" style="background-color:#FC8D59;">82.87</span></td>
<td id="S7.T16.27.27.47.8" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.8.1" class="ltx_text" style="background-color:#FCA77F;">32.24</span></td>
<td id="S7.T16.27.27.47.9" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.9.1" class="ltx_text" style="background-color:#FC8D59;">77.61</span></td>
<td id="S7.T16.27.27.47.10" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.47.10.1" class="ltx_text" style="background-color:#FEE8DD;">34.96/19.20</span></td>
</tr>
<tr id="S7.T16.27.27.48" class="ltx_tr">
<td id="S7.T16.27.27.48.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Davinci003</td>
<td id="S7.T16.27.27.48.2" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.2.1" class="ltx_text" style="background-color:#FEDCCC;">74.40</span></td>
<td id="S7.T16.27.27.48.3" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.3.1" class="ltx_text" style="background-color:#FC8D59;">62.65</span></td>
<td id="S7.T16.27.27.48.4" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.4.1" class="ltx_text" style="background-color:#FEDCCC;">69.70</span></td>
<td id="S7.T16.27.27.48.5" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.5.1" class="ltx_text" style="background-color:#FCA77F;">64.60</span></td>
<td id="S7.T16.27.27.48.6" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.6.1" class="ltx_text" style="background-color:#FEDCCC;">61.07</span></td>
<td id="S7.T16.27.27.48.7" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.7.1" class="ltx_text" style="background-color:#FEE8DD;">57.16</span></td>
<td id="S7.T16.27.27.48.8" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.8.1" class="ltx_text" style="background-color:#FEE8DD;">17.66</span></td>
<td id="S7.T16.27.27.48.9" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.9.1" class="ltx_text" style="background-color:#FEDCCC;">65.67</span></td>
<td id="S7.T16.27.27.48.10" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.48.10.1" class="ltx_text" style="background-color:#FC8D59;">64.08/32.40</span></td>
</tr>
<tr id="S7.T16.27.27.49" class="ltx_tr">
<td id="S7.T16.27.27.49.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Davinci002</td>
<td id="S7.T16.27.27.49.2" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.2.1" class="ltx_text" style="background-color:#FEF7F3;">69.80</span></td>
<td id="S7.T16.27.27.49.3" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.3.1" class="ltx_text" style="background-color:#FEF7F3;">47.81</span></td>
<td id="S7.T16.27.27.49.4" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.4.1" class="ltx_text" style="background-color:#FEF7F3;">57.01</span></td>
<td id="S7.T16.27.27.49.5" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.5.1" class="ltx_text" style="background-color:#FEDCCC;">62.55</span></td>
<td id="S7.T16.27.27.49.6" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.6.1" class="ltx_text" style="background-color:#FCA77F;">67.11</span></td>
<td id="S7.T16.27.27.49.7" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.7.1" class="ltx_text" style="background-color:#FEF7F3;">49.96</span></td>
<td id="S7.T16.27.27.49.8" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.8.1" class="ltx_text" style="background-color:#FEF7F3;">14.28</span></td>
<td id="S7.T16.27.27.49.9" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.9.1" class="ltx_text" style="background-color:#FCA77F;">76.87</span></td>
<td id="S7.T16.27.27.49.10" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.49.10.1" class="ltx_text" style="background-color:#FEF7F3;">29.66/15.20</span></td>
</tr>
<tr id="S7.T16.27.27.50" class="ltx_tr">
<td id="S7.T16.27.27.50.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">LLaMA 2-Chat&nbsp;(7B)</td>
<td id="S7.T16.27.27.50.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.2.1" class="ltx_text" style="background-color:#A7CBE2;">45.62</span></td>
<td id="S7.T16.27.27.50.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.3.1" class="ltx_text" style="background-color:#C6DEED;">
74.01</span></td>
<td id="S7.T16.27.27.50.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.4.1" class="ltx_text" style="background-color:#C4DDEC;">43.84</span></td>
<td id="S7.T16.27.27.50.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.5.1" class="ltx_text" style="background-color:#C4DDEC;">43.40</span></td>
<td id="S7.T16.27.27.50.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.6.1" class="ltx_text" style="background-color:#A7CBE2;">38.93</span></td>
<td id="S7.T16.27.27.50.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">9.63</td>
<td id="S7.T16.27.27.50.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">2.22</td>
<td id="S7.T16.27.27.50.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.9.1" class="ltx_text" style="background-color:#92BFDB;">11.19</span></td>
<td id="S7.T16.27.27.50.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.50.10.1" class="ltx_text" style="background-color:#92BFDB;">24.51/5.60</span></td>
</tr>
<tr id="S7.T16.27.27.51" class="ltx_tr">
<td id="S7.T16.27.27.51.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Vicuna&nbsp;(13B)</td>
<td id="S7.T16.27.27.51.2" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.2.1" class="ltx_text" style="background-color:#E5F0F7;">43.65</span></td>
<td id="S7.T16.27.27.51.3" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.3.1" class="ltx_text" style="background-color:#E5F0F7;">70.51</span></td>
<td id="S7.T16.27.27.51.4" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.4.1" class="ltx_text" style="background-color:#C6DEED;">45.97</span></td>
<td id="S7.T16.27.27.51.5" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.5.1" class="ltx_text" style="background-color:#92BFDB;">53.55</span></td>
<td id="S7.T16.27.27.51.6" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.6.1" class="ltx_text" style="background-color:#C6DEED;">36.91</span></td>
<td id="S7.T16.27.27.51.7" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.7.1" class="ltx_text" style="background-color:#92BFDB;">18.50</span></td>
<td id="S7.T16.27.27.51.8" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.8.1" class="ltx_text" style="background-color:#A7CBE2;">3.72</span></td>
<td id="S7.T16.27.27.51.9" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.9.1" class="ltx_text" style="background-color:#A7CBE2;">8.96</span></td>
<td id="S7.T16.27.27.51.10" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.51.10.1" class="ltx_text" style="background-color:#A7CBE2;">22.74/5.00</span></td>
</tr>
<tr id="S7.T16.27.27.52" class="ltx_tr">
<td id="S7.T16.27.27.52.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Vicuna&nbsp;(7B)</td>
<td id="S7.T16.27.27.52.2" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.2.1" class="ltx_text" style="background-color:#C4DDEC;">43.84</span></td>
<td id="S7.T16.27.27.52.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">69.25</td>
<td id="S7.T16.27.27.52.4" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.4.1" class="ltx_text" style="background-color:#A7CBE2;">
46.27</span></td>
<td id="S7.T16.27.27.52.5" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.5.1" class="ltx_text" style="background-color:#A7CBE2;">44.25</span></td>
<td id="S7.T16.27.27.52.6" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.6.1" class="ltx_text" style="background-color:#C4DDEC;">36.24</span></td>
<td id="S7.T16.27.27.52.7" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.7.1" class="ltx_text" style="background-color:#A7CBE2;">14.03</span></td>
<td id="S7.T16.27.27.52.8" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.8.1" class="ltx_text" style="background-color:#C6DEED;">3.54</span></td>
<td id="S7.T16.27.27.52.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.49</td>
<td id="S7.T16.27.27.52.10" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.52.10.1" class="ltx_text" style="background-color:#C4DDEC;">
6.90/1.40</span></td>
</tr>
<tr id="S7.T16.27.27.53" class="ltx_tr">
<td id="S7.T16.27.27.53.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Alpaca&nbsp;(7B)</td>
<td id="S7.T16.27.27.53.2" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.53.2.1" class="ltx_text" style="background-color:#92BFDB;">47.82</span></td>
<td id="S7.T16.27.27.53.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">69.81</td>
<td id="S7.T16.27.27.53.4" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.53.4.1" class="ltx_text" style="background-color:#92BFDB;">47.55</span></td>
<td id="S7.T16.27.27.53.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">39.35</td>
<td id="S7.T16.27.27.53.6" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.53.6.1" class="ltx_text" style="background-color:#92BFDB;">40.27</span></td>
<td id="S7.T16.27.27.53.7" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.53.7.1" class="ltx_text" style="background-color:#E5F0F7;">4.93</span></td>
<td id="S7.T16.27.27.53.8" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.53.8.1" class="ltx_text" style="background-color:#92BFDB;">4.16</span></td>
<td id="S7.T16.27.27.53.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">4.48</td>
<td id="S7.T16.27.27.53.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00/0.00</td>
</tr>
<tr id="S7.T16.27.27.54" class="ltx_tr">
<td id="S7.T16.27.27.54.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">ChatGLM&nbsp;(6B)</td>
<td id="S7.T16.27.27.54.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">30.42</td>
<td id="S7.T16.27.27.54.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">29.27</td>
<td id="S7.T16.27.27.54.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">33.18</td>
<td id="S7.T16.27.27.54.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">14.05</td>
<td id="S7.T16.27.27.54.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">14.09</td>
<td id="S7.T16.27.27.54.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">3.41</td>
<td id="S7.T16.27.27.54.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.10</td>
<td id="S7.T16.27.27.54.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.54.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00/0.00</td>
</tr>
<tr id="S7.T16.27.27.55" class="ltx_tr">
<td id="S7.T16.27.27.55.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">LLaMA 2&nbsp;(7B)</td>
<td id="S7.T16.27.27.55.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.2.1" class="ltx_text" style="background-color:#C6DEED;">44.81</span></td>
<td id="S7.T16.27.27.55.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.3.1" class="ltx_text" style="background-color:#A7CBE2;">74.25</span></td>
<td id="S7.T16.27.27.55.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">41.72</td>
<td id="S7.T16.27.27.55.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.5.1" class="ltx_text" style="background-color:#C6DEED;">43.95</span></td>
<td id="S7.T16.27.27.55.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.6.1" class="ltx_text" style="background-color:#E5F0F7;">35.75</span></td>
<td id="S7.T16.27.27.55.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.7.1" class="ltx_text" style="background-color:#C6DEED;">10.99</span></td>
<td id="S7.T16.27.27.55.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.8.1" class="ltx_text" style="background-color:#E5F0F7;">2.64</span></td>
<td id="S7.T16.27.27.55.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.55.9.1" class="ltx_text" style="background-color:#A7CBE2;">8.96</span></td>
<td id="S7.T16.27.27.55.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">0.00/0.00</td>
</tr>
<tr id="S7.T16.27.27.56" class="ltx_tr">
<td id="S7.T16.27.27.56.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">LLaMA&nbsp;(7B)</td>
<td id="S7.T16.27.27.56.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">42.42</td>
<td id="S7.T16.27.27.56.3" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.56.3.1" class="ltx_text" style="background-color:#C4DDEC;">73.91</span></td>
<td id="S7.T16.27.27.56.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">41.46</td>
<td id="S7.T16.27.27.56.5" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.56.5.1" class="ltx_text" style="background-color:#E5F0F7;">39.95</span></td>
<td id="S7.T16.27.27.56.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">34.90</td>
<td id="S7.T16.27.27.56.7" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.56.7.1" class="ltx_text" style="background-color:#C6DEED;">10.99</span></td>
<td id="S7.T16.27.27.56.8" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.56.8.1" class="ltx_text" style="background-color:#C4DDEC;">3.12</span></td>
<td id="S7.T16.27.27.56.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">2.24</td>
<td id="S7.T16.27.27.56.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00/0.00</td>
</tr>
<tr id="S7.T16.27.27.57" class="ltx_tr">
<td id="S7.T16.27.27.57.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Falcon&nbsp;(7B)</td>
<td id="S7.T16.27.27.57.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">39.46</td>
<td id="S7.T16.27.27.57.3" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.57.3.1" class="ltx_text" style="background-color:#92BFDB;">74.58</span></td>
<td id="S7.T16.27.27.57.4" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.57.4.1" class="ltx_text" style="background-color:#E5F0F7;">
42.53</span></td>
<td id="S7.T16.27.27.57.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">29.80</td>
<td id="S7.T16.27.27.57.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">24.16</td>
<td id="S7.T16.27.27.57.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.67</td>
<td id="S7.T16.27.27.57.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.94</td>
<td id="S7.T16.27.27.57.9" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.57.9.1" class="ltx_text" style="background-color:#C4DDEC;">
7.46</span></td>
<td id="S7.T16.27.27.57.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00/0.00</td>
</tr>
<tr id="S7.T16.27.27.58" class="ltx_tr">
<td id="S7.T16.27.27.58.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Pythia&nbsp;(12B)</td>
<td id="S7.T16.27.27.58.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">37.02</td>
<td id="S7.T16.27.27.58.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">65.45</td>
<td id="S7.T16.27.27.58.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">41.53</td>
<td id="S7.T16.27.27.58.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">32.40</td>
<td id="S7.T16.27.27.58.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">26.17</td>
<td id="S7.T16.27.27.58.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">2.88</td>
<td id="S7.T16.27.27.58.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.96</td>
<td id="S7.T16.27.27.58.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">5.22</td>
<td id="S7.T16.27.27.58.10" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.58.10.1" class="ltx_text" style="background-color:#E5F0F7;">3.68/0.60</span></td>
</tr>
<tr id="S7.T16.27.27.59" class="ltx_tr">
<td id="S7.T16.27.27.59.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Pythia&nbsp;(7B)</td>
<td id="S7.T16.27.27.59.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">34.88</td>
<td id="S7.T16.27.27.59.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">61.82</td>
<td id="S7.T16.27.27.59.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">41.01</td>
<td id="S7.T16.27.27.59.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">29.05</td>
<td id="S7.T16.27.27.59.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">27.52</td>
<td id="S7.T16.27.27.59.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.82</td>
<td id="S7.T16.27.27.59.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.46</td>
<td id="S7.T16.27.27.59.9" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.59.9.1" class="ltx_text" style="background-color:#C4DDEC;">7.46</span></td>
<td id="S7.T16.27.27.59.10" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.59.10.1" class="ltx_text" style="background-color:#C6DEED;">10.75/1.80</span></td>
</tr>
<tr id="S7.T16.27.27.60" class="ltx_tr">
<td id="S7.T16.27.27.60.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;" rowspan="2"><span id="S7.T16.27.27.60.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S7.T16.27.27.60.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;" colspan="5"><span id="S7.T16.27.27.60.2.1" class="ltx_text ltx_font_bold">Human Alignment</span></td>
<td id="S7.T16.27.27.60.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;" colspan="4"><span id="S7.T16.27.27.60.3.1" class="ltx_text ltx_font_bold">Tool Manipulation</span></td>
</tr>
<tr id="S7.T16.27.27.27" class="ltx_tr">
<td id="S7.T16.19.19.19.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">TfQA<math id="S7.T16.19.19.19.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.19.19.19.1.m1.1a"><mo stretchy="false" id="S7.T16.19.19.19.1.m1.1.1" xref="S7.T16.19.19.19.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.19.19.19.1.m1.1b"><ci id="S7.T16.19.19.19.1.m1.1.1.cmml" xref="S7.T16.19.19.19.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.19.19.19.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.20.20.20.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">C-Pairs<math id="S7.T16.20.20.20.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S7.T16.20.20.20.2.m1.1a"><mo stretchy="false" id="S7.T16.20.20.20.2.m1.1.1" xref="S7.T16.20.20.20.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S7.T16.20.20.20.2.m1.1b"><ci id="S7.T16.20.20.20.2.m1.1.1.cmml" xref="S7.T16.20.20.20.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.20.20.20.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S7.T16.21.21.21.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">WinoGender<math id="S7.T16.21.21.21.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.21.21.21.3.m1.1a"><mo stretchy="false" id="S7.T16.21.21.21.3.m1.1.1" xref="S7.T16.21.21.21.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.21.21.21.3.m1.1b"><ci id="S7.T16.21.21.21.3.m1.1.1.cmml" xref="S7.T16.21.21.21.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.21.21.21.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.22.22.22.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">RTP<math id="S7.T16.22.22.22.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S7.T16.22.22.22.4.m1.1a"><mo stretchy="false" id="S7.T16.22.22.22.4.m1.1.1" xref="S7.T16.22.22.22.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S7.T16.22.22.22.4.m1.1b"><ci id="S7.T16.22.22.22.4.m1.1.1.cmml" xref="S7.T16.22.22.22.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.22.22.22.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S7.T16.23.23.23.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">HaluEval<math id="S7.T16.23.23.23.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.23.23.23.5.m1.1a"><mo stretchy="false" id="S7.T16.23.23.23.5.m1.1.1" xref="S7.T16.23.23.23.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.23.23.23.5.m1.1b"><ci id="S7.T16.23.23.23.5.m1.1.1.cmml" xref="S7.T16.23.23.23.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.23.23.23.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.24.24.24.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">HotpotQA<math id="S7.T16.24.24.24.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.24.24.24.6.m1.1a"><mo stretchy="false" id="S7.T16.24.24.24.6.m1.1.1" xref="S7.T16.24.24.24.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.24.24.24.6.m1.1b"><ci id="S7.T16.24.24.24.6.m1.1.1.cmml" xref="S7.T16.24.24.24.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.24.24.24.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.25.25.25.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">Gorilla-TH<math id="S7.T16.25.25.25.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.25.25.25.7.m1.1a"><mo stretchy="false" id="S7.T16.25.25.25.7.m1.1.1" xref="S7.T16.25.25.25.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.25.25.25.7.m1.1b"><ci id="S7.T16.25.25.25.7.m1.1.1.cmml" xref="S7.T16.25.25.25.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.25.25.25.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.26.26.26.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">Gorilla-TF<math id="S7.T16.26.26.26.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.26.26.26.8.m1.1a"><mo stretchy="false" id="S7.T16.26.26.26.8.m1.1.1" xref="S7.T16.26.26.26.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.26.26.26.8.m1.1b"><ci id="S7.T16.26.26.26.8.m1.1.1.cmml" xref="S7.T16.26.26.26.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.26.26.26.8.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S7.T16.27.27.27.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">Gorilla-HF<math id="S7.T16.27.27.27.9.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T16.27.27.27.9.m1.1a"><mo stretchy="false" id="S7.T16.27.27.27.9.m1.1.1" xref="S7.T16.27.27.27.9.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T16.27.27.27.9.m1.1b"><ci id="S7.T16.27.27.27.9.m1.1.1.cmml" xref="S7.T16.27.27.27.9.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T16.27.27.27.9.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S7.T16.27.27.61" class="ltx_tr">
<td id="S7.T16.27.27.61.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">ChatGPT</td>
<td id="S7.T16.27.27.61.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.2.1" class="ltx_text" style="background-color:#FCA77F;">69.16</span></td>
<td id="S7.T16.27.27.61.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.3.1" class="ltx_text" style="background-color:#FEE8DD;">18.60</span></td>
<td id="S7.T16.27.27.61.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.4.1" class="ltx_text" style="background-color:#FCA77F;">62.50/<span id="S7.T16.27.27.61.4.1.1" class="ltx_text">72.50/<span id="S7.T16.27.27.61.4.1.1.1" class="ltx_text">79.17</span></span></span></td>
<td id="S7.T16.27.27.61.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.5.1" class="ltx_text" style="background-color:#FC8D59;">3.07</span></td>
<td id="S7.T16.27.27.61.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.6.1" class="ltx_text" style="background-color:#FC8D59;">66.64</span></td>
<td id="S7.T16.27.27.61.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.7.1" class="ltx_text" style="background-color:#FEF7F3;">23.80</span></td>
<td id="S7.T16.27.27.61.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.8.1" class="ltx_text" style="background-color:#FCA77F;">67.20</span></td>
<td id="S7.T16.27.27.61.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.9.1" class="ltx_text" style="background-color:#FC8D59;">
44.53</span></td>
<td id="S7.T16.27.27.61.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.61.10.1" class="ltx_text" style="background-color:#FCA77F;">19.36</span></td>
</tr>
<tr id="S7.T16.27.27.62" class="ltx_tr">
<td id="S7.T16.27.27.62.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Claude</td>
<td id="S7.T16.27.27.62.2" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.2.1" class="ltx_text" style="background-color:#FEDCCC;">67.93</span></td>
<td id="S7.T16.27.27.62.3" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.3.1" class="ltx_text" style="background-color:#FEF7F3;">32.73</span></td>
<td id="S7.T16.27.27.62.4" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.4.1" class="ltx_text" style="background-color:#FEE8DD;">71.67/<span id="S7.T16.27.27.62.4.1.1" class="ltx_text">55.00/<span id="S7.T16.27.27.62.4.1.1.1" class="ltx_text">52.50</span></span></span></td>
<td id="S7.T16.27.27.62.5" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.5.1" class="ltx_text" style="background-color:#FEDCCC;">3.75</span></td>
<td id="S7.T16.27.27.62.6" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.6.1" class="ltx_text" style="background-color:#FCA77F;">63.75</span></td>
<td id="S7.T16.27.27.62.7" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.7.1" class="ltx_text" style="background-color:#FEDCCC;">33.80</span></td>
<td id="S7.T16.27.27.62.8" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.8.1" class="ltx_text" style="background-color:#FEE8DD;">22.04</span></td>
<td id="S7.T16.27.27.62.9" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.9.1" class="ltx_text" style="background-color:#FEDCCC;">7.74</span></td>
<td id="S7.T16.27.27.62.10" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.62.10.1" class="ltx_text" style="background-color:#FEDCCC;">7.08</span></td>
</tr>
<tr id="S7.T16.27.27.63" class="ltx_tr">
<td id="S7.T16.27.27.63.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Claude 2</td>
<td id="S7.T16.27.27.63.2" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.2.1" class="ltx_text" style="background-color:#FC8D59;">71.11</span></td>
<td id="S7.T16.27.27.63.3" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.3.1" class="ltx_text" style="background-color:#FEDCCC;">10.67</span></td>
<td id="S7.T16.27.27.63.4" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.4.1" class="ltx_text" style="background-color:#FEF7F3;">60.00/60.00/55.83</span></td>
<td id="S7.T16.27.27.63.5" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.5.1" class="ltx_text" style="background-color:#FCA77F;">3.20</span></td>
<td id="S7.T16.27.27.63.6" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.6.1" class="ltx_text" style="background-color:#FEF7F3;">50.63</span></td>
<td id="S7.T16.27.27.63.7" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.7.1" class="ltx_text" style="background-color:#FC8D59;">36.4</span></td>
<td id="S7.T16.27.27.63.8" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.8.1" class="ltx_text" style="background-color:#FEDCCC;">61.29</span></td>
<td id="S7.T16.27.27.63.9" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.9.1" class="ltx_text" style="background-color:#FCA77F;">22.19</span></td>
<td id="S7.T16.27.27.63.10" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.63.10.1" class="ltx_text" style="background-color:#FC8D59;">23.67</span></td>
</tr>
<tr id="S7.T16.27.27.64" class="ltx_tr">
<td id="S7.T16.27.27.64.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Davinci003</td>
<td id="S7.T16.27.27.64.2" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.2.1" class="ltx_text" style="background-color:#FEE8DD;">60.83</span></td>
<td id="S7.T16.27.27.64.3" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.3.1" class="ltx_text" style="background-color:#FC8D59;">0.99</span></td>
<td id="S7.T16.27.27.64.4" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.4.1" class="ltx_text" style="background-color:#FC8D59;">67.50/68.33/79.17</span></td>
<td id="S7.T16.27.27.64.5" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.5.1" class="ltx_text" style="background-color:#FEE8DD;">8.81</span></td>
<td id="S7.T16.27.27.64.6" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.6.1" class="ltx_text" style="background-color:#FEE8DD;">58.94</span></td>
<td id="S7.T16.27.27.64.7" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.7.1" class="ltx_text" style="background-color:#FCA77F;">
34.40</span></td>
<td id="S7.T16.27.27.64.8" class="ltx_td ltx_align_center" style="background-color:#FC8D59;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.8.1" class="ltx_text" style="background-color:#FC8D59;">
72.58</span></td>
<td id="S7.T16.27.27.64.9" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.9.1" class="ltx_text" style="background-color:#FEE8DD;">3.80</span></td>
<td id="S7.T16.27.27.64.10" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.64.10.1" class="ltx_text" style="background-color:#FEE8DD;">6.42</span></td>
</tr>
<tr id="S7.T16.27.27.65" class="ltx_tr">
<td id="S7.T16.27.27.65.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Davinci002</td>
<td id="S7.T16.27.27.65.2" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.2.1" class="ltx_text" style="background-color:#FEF7F3;">53.73</span></td>
<td id="S7.T16.27.27.65.3" class="ltx_td ltx_align_center" style="background-color:#FCA77F;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.3.1" class="ltx_text" style="background-color:#FCA77F;">7.56</span></td>
<td id="S7.T16.27.27.65.4" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.4.1" class="ltx_text" style="background-color:#FEDCCC;">72.50/70.00/64.17</span></td>
<td id="S7.T16.27.27.65.5" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.5.1" class="ltx_text" style="background-color:#FEF7F3;">10.65</span></td>
<td id="S7.T16.27.27.65.6" class="ltx_td ltx_align_center" style="background-color:#FEDCCC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.6.1" class="ltx_text" style="background-color:#FEDCCC;">59.67</span></td>
<td id="S7.T16.27.27.65.7" class="ltx_td ltx_align_center" style="background-color:#FEE8DD;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.7.1" class="ltx_text" style="background-color:#FEE8DD;">26.00</span></td>
<td id="S7.T16.27.27.65.8" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.8.1" class="ltx_text" style="background-color:#FEF7F3;">2.69</span></td>
<td id="S7.T16.27.27.65.9" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.9.1" class="ltx_text" style="background-color:#FEF7F3;">1.02</span></td>
<td id="S7.T16.27.27.65.10" class="ltx_td ltx_align_center" style="background-color:#FEF7F3;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.65.10.1" class="ltx_text" style="background-color:#FEF7F3;">1.00</span></td>
</tr>
<tr id="S7.T16.27.27.66" class="ltx_tr">
<td id="S7.T16.27.27.66.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">LLaMA 2-Chat&nbsp;(7B)</td>
<td id="S7.T16.27.27.66.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.66.2.1" class="ltx_text" style="background-color:#92BFDB;">69.77</span></td>
<td id="S7.T16.27.27.66.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.66.3.1" class="ltx_text" style="background-color:#A7CBE2;">48.54</span></td>
<td id="S7.T16.27.27.66.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">47.50/46.67/46.67</td>
<td id="S7.T16.27.27.66.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.66.5.1" class="ltx_text" style="background-color:#A7CBE2;">4.61</span></td>
<td id="S7.T16.27.27.66.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.66.6.1" class="ltx_text" style="background-color:#C6DEED;">43.82</span></td>
<td id="S7.T16.27.27.66.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.66.7.1" class="ltx_text" style="background-color:#C4DDEC;">4.40</span></td>
<td id="S7.T16.27.27.66.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.66.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.66.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.66.10.1" class="ltx_text" style="background-color:#C6DEED;">0.22</span></td>
</tr>
<tr id="S7.T16.27.27.67" class="ltx_tr">
<td id="S7.T16.27.27.67.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Vicuna&nbsp;(13B)</td>
<td id="S7.T16.27.27.67.2" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.2.1" class="ltx_text" style="background-color:#C6DEED;">62.30</span></td>
<td id="S7.T16.27.27.67.3" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.3.1" class="ltx_text" style="background-color:#92BFDB;">45.95</span></td>
<td id="S7.T16.27.27.67.4" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.4.1" class="ltx_text" style="background-color:#C6DEED;">50.83/50.83/52.50</span></td>
<td id="S7.T16.27.27.67.5" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.5.1" class="ltx_text" style="background-color:#E5F0F7;">5.00</span></td>
<td id="S7.T16.27.27.67.6" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.6.1" class="ltx_text" style="background-color:#92BFDB;">49.01</span></td>
<td id="S7.T16.27.27.67.7" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.7.1" class="ltx_text" style="background-color:#A7CBE2;">11.20</span></td>
<td id="S7.T16.27.27.67.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.67.9" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.9.1" class="ltx_text" style="background-color:#92BFDB;">0.44</span></td>
<td id="S7.T16.27.27.67.10" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.67.10.1" class="ltx_text" style="background-color:#92BFDB;">0.89</span></td>
</tr>
<tr id="S7.T16.27.27.68" class="ltx_tr">
<td id="S7.T16.27.27.68.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Vicuna&nbsp;(7B)</td>
<td id="S7.T16.27.27.68.2" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.68.2.1" class="ltx_text" style="background-color:#C4DDEC;">57.77</span></td>
<td id="S7.T16.27.27.68.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">67.44</td>
<td id="S7.T16.27.27.68.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">49.17/49.17/49.17</td>
<td id="S7.T16.27.27.68.5" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.68.5.1" class="ltx_text" style="background-color:#C6DEED;">4.70</span></td>
<td id="S7.T16.27.27.68.6" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.68.6.1" class="ltx_text" style="background-color:#C4DDEC;">43.44</span></td>
<td id="S7.T16.27.27.68.7" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.68.7.1" class="ltx_text" style="background-color:#C6DEED;">6.20</span></td>
<td id="S7.T16.27.27.68.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.68.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.68.10" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.68.10.1" class="ltx_text" style="background-color:#A7CBE2;">0.33</span></td>
</tr>
<tr id="S7.T16.27.27.69" class="ltx_tr">
<td id="S7.T16.27.27.69.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Alpaca&nbsp;(7B)</td>
<td id="S7.T16.27.27.69.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">46.14</td>
<td id="S7.T16.27.27.69.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">65.45</td>
<td id="S7.T16.27.27.69.4" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.69.4.1" class="ltx_text" style="background-color:#A7CBE2;">53.33/51.67/53.33</span></td>
<td id="S7.T16.27.27.69.5" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.69.5.1" class="ltx_text" style="background-color:#C4DDEC;">4.78</span></td>
<td id="S7.T16.27.27.69.6" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.69.6.1" class="ltx_text" style="background-color:#A7CBE2;">44.16</span></td>
<td id="S7.T16.27.27.69.7" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.69.7.1" class="ltx_text" style="background-color:#92BFDB;">11.60</span></td>
<td id="S7.T16.27.27.69.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.69.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.69.10" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.69.10.1" class="ltx_text" style="background-color:#C4DDEC;">0.11</span></td>
</tr>
<tr id="S7.T16.27.27.70" class="ltx_tr">
<td id="S7.T16.27.27.70.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">ChatGLM&nbsp;(6B)</td>
<td id="S7.T16.27.27.70.2" class="ltx_td ltx_align_center" style="background-color:#A7CBE2;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.70.2.1" class="ltx_text" style="background-color:#A7CBE2;">63.53</span></td>
<td id="S7.T16.27.27.70.3" class="ltx_td ltx_align_center" style="background-color:#C6DEED;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.70.3.1" class="ltx_text" style="background-color:#C6DEED;">50.53</span></td>
<td id="S7.T16.27.27.70.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">47.50/47.50/46.67</td>
<td id="S7.T16.27.27.70.5" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.70.5.1" class="ltx_text" style="background-color:#92BFDB;">2.89</span></td>
<td id="S7.T16.27.27.70.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">41.82</td>
<td id="S7.T16.27.27.70.7" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.70.7.1" class="ltx_text" style="background-color:#E5F0F7;">4.00</span></td>
<td id="S7.T16.27.27.70.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.70.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.70.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
</tr>
<tr id="S7.T16.27.27.71" class="ltx_tr">
<td id="S7.T16.27.27.71.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.5pt;">LLaMA 2&nbsp;(7B)</td>
<td id="S7.T16.27.27.71.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">50.06</td>
<td id="S7.T16.27.27.71.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.71.3.1" class="ltx_text" style="background-color:#C4DDEC;">51.39</span></td>
<td id="S7.T16.27.27.71.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">48.83/48.83/50.83</td>
<td id="S7.T16.27.27.71.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">6.17</td>
<td id="S7.T16.27.27.71.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.71.6.1" class="ltx_text" style="background-color:#E5F0F7;">42.23</span></td>
<td id="S7.T16.27.27.71.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">3.80</td>
<td id="S7.T16.27.27.71.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.71.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.71.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.71.10.1" class="ltx_text" style="background-color:#C4DDEC;">0.11</span></td>
</tr>
<tr id="S7.T16.27.27.72" class="ltx_tr">
<td id="S7.T16.27.27.72.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">LLaMA&nbsp;(7B)</td>
<td id="S7.T16.27.27.72.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">47.86</td>
<td id="S7.T16.27.27.72.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">67.84</td>
<td id="S7.T16.27.27.72.4" class="ltx_td ltx_align_center" style="background-color:#92BFDB;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.72.4.1" class="ltx_text" style="background-color:#92BFDB;">54.17/52.50/51.67</span></td>
<td id="S7.T16.27.27.72.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">5.94</td>
<td id="S7.T16.27.27.72.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">14.18</td>
<td id="S7.T16.27.27.72.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.60</td>
<td id="S7.T16.27.27.72.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.72.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.72.10" class="ltx_td ltx_align_center" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.72.10.1" class="ltx_text" style="background-color:#C4DDEC;">0.11</span></td>
</tr>
<tr id="S7.T16.27.27.73" class="ltx_tr">
<td id="S7.T16.27.27.73.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Falcon&nbsp;(7B)</td>
<td id="S7.T16.27.27.73.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">53.24</td>
<td id="S7.T16.27.27.73.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">68.04</td>
<td id="S7.T16.27.27.73.4" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.73.4.1" class="ltx_text" style="background-color:#E5F0F7;">50.00/50.83/50.00</span></td>
<td id="S7.T16.27.27.73.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">6.71</td>
<td id="S7.T16.27.27.73.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">37.41</td>
<td id="S7.T16.27.27.73.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">1.00</td>
<td id="S7.T16.27.27.73.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.73.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.73.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
</tr>
<tr id="S7.T16.27.27.74" class="ltx_tr">
<td id="S7.T16.27.27.74.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.5pt;">Pythia&nbsp;(12B)</td>
<td id="S7.T16.27.27.74.2" class="ltx_td ltx_align_center" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.74.2.1" class="ltx_text" style="background-color:#E5F0F7;">54.47</span></td>
<td id="S7.T16.27.27.74.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">65.78</td>
<td id="S7.T16.27.27.74.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">49.17/48.33/49.17</td>
<td id="S7.T16.27.27.74.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">6.59</td>
<td id="S7.T16.27.27.74.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">27.09</td>
<td id="S7.T16.27.27.74.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.40</td>
<td id="S7.T16.27.27.74.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.74.9" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.74.10" class="ltx_td ltx_align_center" style="padding:0.5pt 2.5pt;">0.00</td>
</tr>
<tr id="S7.T16.27.27.75" class="ltx_tr">
<td id="S7.T16.27.27.75.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.5pt 2.5pt;">Pythia&nbsp;(7B)</td>
<td id="S7.T16.27.27.75.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">50.92</td>
<td id="S7.T16.27.27.75.3" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E5F0F7;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.75.3.1" class="ltx_text" style="background-color:#E5F0F7;">64.79</span></td>
<td id="S7.T16.27.27.75.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C4DDEC;padding:0.5pt 2.5pt;"><span id="S7.T16.27.27.75.4.1" class="ltx_text" style="background-color:#C4DDEC;">51.67/49.17/50.00</span></td>
<td id="S7.T16.27.27.75.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">13.02</td>
<td id="S7.T16.27.27.75.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">25.84</td>
<td id="S7.T16.27.27.75.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">0.20</td>
<td id="S7.T16.27.27.75.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.75.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">0.00</td>
<td id="S7.T16.27.27.75.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.5pt;">0.00</td>
</tr>
</tbody></table>
</span></div>
</figure>
<figure id="S7.T17" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE XVII: </span>Prompt examples and their performance of ChatGPT on representative tasks. For most tasks, we compare the performance for <em id="S7.T17.42.1" class="ltx_emph ltx_font_italic">simple</em> and <em id="S7.T17.43.2" class="ltx_emph ltx_font_italic">complex</em> prompts. We also present the reported performance of supervised methods. “LG”, “KU”, “CR”, “SDG”, “IR” are short for “language generation”, “knowledge utilization”, “complex reasoning”, “structured data generation”, “information retrieval”. “-” means there is no reported supervised result previously on this dataset.</figcaption>
<table id="S7.T17.39" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T17.39.40" class="ltx_tr">
<td id="S7.T17.39.40.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S7.T17.39.40.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Tasks</span></td>
<td id="S7.T17.39.40.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T17.39.40.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Datasets</span></td>
<td id="S7.T17.39.40.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S7.T17.39.40.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.40.3.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.40.3.1.1.1" class="ltx_text"></span><span id="S7.T17.39.40.3.1.1.2" class="ltx_text" style="font-size:70%;"> </span><span id="S7.T17.39.40.3.1.1.3" class="ltx_text" style="font-size:70%;">
<span id="S7.T17.39.40.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<span id="S7.T17.39.40.3.1.1.3.1.1" class="ltx_tr">
<span id="S7.T17.39.40.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T17.39.40.3.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Instructions</span></span></span>
</span></span><span id="S7.T17.39.40.3.1.1.4" class="ltx_text"></span><span id="S7.T17.39.40.3.1.1.5" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S7.T17.39.40.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S7.T17.39.40.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">ChatGPT</span></td>
<td id="S7.T17.39.40.5" class="ltx_td ltx_align_right ltx_border_tt"><span id="S7.T17.39.40.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Supervised</span></td>
</tr>
<tr id="S7.T17.39.41" class="ltx_tr">
<td id="S7.T17.39.41.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S7.T17.39.41.1.1" class="ltx_text" style="font-size:70%;">LG</span></td>
<td id="S7.T17.39.41.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S7.T17.39.41.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.41.2.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.41.2.1.1.1" class="ltx_text" style="font-size:70%;">Translation</span></span>
</span>
</td>
<td id="S7.T17.39.41.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.39.41.3.1" class="ltx_text" style="font-size:70%;">WMT</span></td>
<td id="S7.T17.39.41.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.41.4.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.41.4.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.41.4.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">I want you to act as a translator. Please translate the English sentence into Czech.</span></span>
</span>
</td>
<td id="S7.T17.39.41.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.41.5.1" class="ltx_text" style="font-size:70%;">20.66</span></td>
<td id="S7.T17.39.41.6" class="ltx_td ltx_align_right ltx_border_t" rowspan="2"><span id="S7.T17.39.41.6.1" class="ltx_text" style="font-size:70%;">41.40&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib739" title="" class="ltx_ref">739</a>]</cite></span></td>
</tr>
<tr id="S7.T17.2.2" class="ltx_tr">
<td id="S7.T17.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.2.2.2.2.2" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.2.2.2.2.2.2" class="ltx_text ltx_font_typewriter" style="font-size:70%;">I want you to act as a translator. Translate the given English sentence into Czech, and ensure that the translated sentence is semantically consistent with the given sentence. <math id="S7.T17.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.1.1.1.1.1.1.m1.1a"><mo id="S7.T17.1.1.1.1.1.1.m1.1.1" xref="S7.T17.1.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.1.1.1.1.1.1.m1.1b"><ci id="S7.T17.1.1.1.1.1.1.m1.1.1.cmml" xref="S7.T17.1.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.1.1.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n Sentence: {source sentence} <math id="S7.T17.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.2.2.2.2.2.2.m2.1a"><mo id="S7.T17.2.2.2.2.2.2.m2.1.1" xref="S7.T17.2.2.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.2.2.2.2.2.2.m2.1b"><ci id="S7.T17.2.2.2.2.2.2.m2.1.1.cmml" xref="S7.T17.2.2.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.2.2.2.2.2.2.m2.1c">\backslash</annotation></semantics></math>n Translation:</span></span>
</span>
</td>
<td id="S7.T17.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.2.2.3.1" class="ltx_text" style="font-size:70%;">21.12</span></td>
</tr>
<tr id="S7.T17.39.42" class="ltx_tr">
<td id="S7.T17.39.42.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S7.T17.39.42.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.42.1.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.42.1.1.1.1" class="ltx_text" style="font-size:70%;">Summarization</span></span>
</span>
</td>
<td id="S7.T17.39.42.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.39.42.2.1" class="ltx_text" style="font-size:70%;">XSum</span></td>
<td id="S7.T17.39.42.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.42.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.42.3.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.42.3.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Please generate a one-sentence summary for the given document.</span></span>
</span>
</td>
<td id="S7.T17.39.42.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.42.4.1" class="ltx_text" style="font-size:70%;">21.71</span></td>
<td id="S7.T17.39.42.5" class="ltx_td ltx_align_right ltx_border_t" rowspan="2"><span id="S7.T17.39.42.5.1" class="ltx_text" style="font-size:70%;">42.08&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib740" title="" class="ltx_ref">740</a>]</cite></span></td>
</tr>
<tr id="S7.T17.3.3" class="ltx_tr">
<td id="S7.T17.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.3.3.1.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.3.3.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">{document} Try your best to summarize the main content of the given document. And generate a short summary in 1 sentence for it.<math id="S7.T17.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.3.3.1.1.1.1.m1.1a"><mo id="S7.T17.3.3.1.1.1.1.m1.1.1" xref="S7.T17.3.3.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.3.3.1.1.1.1.m1.1b"><ci id="S7.T17.3.3.1.1.1.1.m1.1.1.cmml" xref="S7.T17.3.3.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.3.3.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n Summary:</span></span>
</span>
</td>
<td id="S7.T17.3.3.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.3.3.2.1" class="ltx_text" style="font-size:70%;">23.01</span></td>
</tr>
<tr id="S7.T17.39.43" class="ltx_tr">
<td id="S7.T17.39.43.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="6"><span id="S7.T17.39.43.1.1" class="ltx_text" style="font-size:70%;">KU</span></td>
<td id="S7.T17.39.43.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S7.T17.39.43.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.43.2.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.43.2.1.1.1" class="ltx_text" style="font-size:70%;">Closed-Book QA</span></span>
</span>
</td>
<td id="S7.T17.39.43.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.39.43.3.1" class="ltx_text" style="font-size:70%;">ARC</span></td>
<td id="S7.T17.39.43.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.43.4.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.43.4.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.43.4.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Choose your answer to the question. {query} {options}</span></span>
</span>
</td>
<td id="S7.T17.39.43.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.43.5.1" class="ltx_text" style="font-size:70%;">85.19</span></td>
<td id="S7.T17.39.43.6" class="ltx_td ltx_align_right ltx_border_t" rowspan="2"><span id="S7.T17.39.43.6.1" class="ltx_text" style="font-size:70%;">92.00&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib741" title="" class="ltx_ref">741</a>]</cite></span></td>
</tr>
<tr id="S7.T17.39.44" class="ltx_tr">
<td id="S7.T17.39.44.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.44.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.44.1.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.44.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Choose a correct answer according to the given question, and output the corresponding id, do not answer other content except the answer id.</span></span>
</span>
</td>
<td id="S7.T17.39.44.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.44.2.1" class="ltx_text" style="font-size:70%;">85.86</span></td>
</tr>
<tr id="S7.T17.39.45" class="ltx_tr">
<td id="S7.T17.39.45.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S7.T17.39.45.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.45.1.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.45.1.1.1.1" class="ltx_text" style="font-size:70%;">Open-Book QA</span></span>
</span>
</td>
<td id="S7.T17.39.45.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.39.45.2.1" class="ltx_text" style="font-size:70%;">OBQA</span></td>
<td id="S7.T17.39.45.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.45.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.45.3.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.45.3.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Choose your answer to the question: {question} {choices}. You must only output A, B, C, or D without any extra explanation. The answer is</span></span>
</span>
</td>
<td id="S7.T17.39.45.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.45.4.1" class="ltx_text" style="font-size:70%;">81.20</span></td>
<td id="S7.T17.39.45.5" class="ltx_td ltx_align_right ltx_border_t" rowspan="2"><span id="S7.T17.39.45.5.1" class="ltx_text" style="font-size:70%;">87.20&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib741" title="" class="ltx_ref">741</a>]</cite></span></td>
</tr>
<tr id="S7.T17.10.10" class="ltx_tr">
<td id="S7.T17.10.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.10.10.7.7" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.10.10.7.7.7" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.10.10.7.7.7.7" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Following is a question that requires multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension. Choose your answer to the question: <math id="S7.T17.4.4.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.4.4.1.1.1.1.m1.1a"><mo id="S7.T17.4.4.1.1.1.1.m1.1.1" xref="S7.T17.4.4.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.4.4.1.1.1.1.m1.1b"><ci id="S7.T17.4.4.1.1.1.1.m1.1.1.cmml" xref="S7.T17.4.4.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.4.4.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n Question: Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as <math id="S7.T17.5.5.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.5.5.2.2.2.2.m2.1a"><mo id="S7.T17.5.5.2.2.2.2.m2.1.1" xref="S7.T17.5.5.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.5.5.2.2.2.2.m2.1b"><ci id="S7.T17.5.5.2.2.2.2.m2.1.1.cmml" xref="S7.T17.5.5.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.5.5.2.2.2.2.m2.1c">\backslash</annotation></semantics></math>n Choices: <math id="S7.T17.6.6.3.3.3.3.m3.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.6.6.3.3.3.3.m3.1a"><mo id="S7.T17.6.6.3.3.3.3.m3.1.1" xref="S7.T17.6.6.3.3.3.3.m3.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.6.6.3.3.3.3.m3.1b"><ci id="S7.T17.6.6.3.3.3.3.m3.1.1.cmml" xref="S7.T17.6.6.3.3.3.3.m3.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.6.6.3.3.3.3.m3.1c">\backslash</annotation></semantics></math>n A. Deep sea animals <math id="S7.T17.7.7.4.4.4.4.m4.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.7.7.4.4.4.4.m4.1a"><mo id="S7.T17.7.7.4.4.4.4.m4.1.1" xref="S7.T17.7.7.4.4.4.4.m4.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.7.7.4.4.4.4.m4.1b"><ci id="S7.T17.7.7.4.4.4.4.m4.1.1.cmml" xref="S7.T17.7.7.4.4.4.4.m4.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.7.7.4.4.4.4.m4.1c">\backslash</annotation></semantics></math>n B. fish <math id="S7.T17.8.8.5.5.5.5.m5.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.8.8.5.5.5.5.m5.1a"><mo id="S7.T17.8.8.5.5.5.5.m5.1.1" xref="S7.T17.8.8.5.5.5.5.m5.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.8.8.5.5.5.5.m5.1b"><ci id="S7.T17.8.8.5.5.5.5.m5.1.1.cmml" xref="S7.T17.8.8.5.5.5.5.m5.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.8.8.5.5.5.5.m5.1c">\backslash</annotation></semantics></math>n C. Long Sea Fish <math id="S7.T17.9.9.6.6.6.6.m6.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.9.9.6.6.6.6.m6.1a"><mo id="S7.T17.9.9.6.6.6.6.m6.1.1" xref="S7.T17.9.9.6.6.6.6.m6.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.9.9.6.6.6.6.m6.1b"><ci id="S7.T17.9.9.6.6.6.6.m6.1.1.cmml" xref="S7.T17.9.9.6.6.6.6.m6.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.9.9.6.6.6.6.m6.1c">\backslash</annotation></semantics></math>n D. Far Sea Animals <math id="S7.T17.10.10.7.7.7.7.m7.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.10.10.7.7.7.7.m7.1a"><mo id="S7.T17.10.10.7.7.7.7.m7.1.1" xref="S7.T17.10.10.7.7.7.7.m7.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.10.10.7.7.7.7.m7.1b"><ci id="S7.T17.10.10.7.7.7.7.m7.1.1.cmml" xref="S7.T17.10.10.7.7.7.7.m7.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.10.10.7.7.7.7.m7.1c">\backslash</annotation></semantics></math>n You must only output A, B, C, or D without any extra explanation. The answer is</span></span>
</span>
</td>
<td id="S7.T17.10.10.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.10.10.8.1" class="ltx_text" style="font-size:70%;">82.20</span></td>
</tr>
<tr id="S7.T17.39.46" class="ltx_tr">
<td id="S7.T17.39.46.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S7.T17.39.46.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.46.1.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.46.1.1.1.1" class="ltx_text" style="font-size:70%;">Fact Extraction</span></span>
</span>
</td>
<td id="S7.T17.39.46.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.39.46.2.1" class="ltx_text" style="font-size:70%;">WikiF</span></td>
<td id="S7.T17.39.46.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.46.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.46.3.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.46.3.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Complete the sentence with one or a few words.</span></span>
</span>
</td>
<td id="S7.T17.39.46.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.46.4.1" class="ltx_text" style="font-size:70%;">29.25</span></td>
<td id="S7.T17.39.46.5" class="ltx_td ltx_align_right ltx_border_t" rowspan="2"><span id="S7.T17.39.46.5.1" class="ltx_text" style="font-size:70%;">34.20&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib520" title="" class="ltx_ref">520</a>]</cite></span></td>
</tr>
<tr id="S7.T17.39.47" class="ltx_tr">
<td id="S7.T17.39.47.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.47.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.47.1.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.47.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Complete the given sentence with one entity name in Wikipedia (MUST be a noun) as short as possible, and ensure that the completed sentence conforms to the facts.</span></span>
</span>
</td>
<td id="S7.T17.39.47.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.47.2.1" class="ltx_text" style="font-size:70%;">31.21</span></td>
</tr>
<tr id="S7.T17.11.11" class="ltx_tr">
<td id="S7.T17.11.11.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S7.T17.11.11.2.1" class="ltx_text" style="font-size:70%;">CR</span></td>
<td id="S7.T17.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S7.T17.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.11.11.3.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.11.11.3.1.1.1" class="ltx_text" style="font-size:70%;">Symbolic Reasoning</span></span>
</span>
</td>
<td id="S7.T17.11.11.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.11.11.4.1" class="ltx_text" style="font-size:70%;">C-Objects</span></td>
<td id="S7.T17.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.11.11.1.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.11.11.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Problem: {problem}<math id="S7.T17.11.11.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.11.11.1.1.1.1.m1.1a"><mo id="S7.T17.11.11.1.1.1.1.m1.1.1" xref="S7.T17.11.11.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.11.11.1.1.1.1.m1.1b"><ci id="S7.T17.11.11.1.1.1.1.m1.1.1.cmml" xref="S7.T17.11.11.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.11.11.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n
Answer:</span></span>
</span>
</td>
<td id="S7.T17.11.11.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.11.11.5.1" class="ltx_text" style="font-size:70%;">53.20</span></td>
<td id="S7.T17.11.11.6" class="ltx_td ltx_align_right ltx_border_t" rowspan="2"><span id="S7.T17.11.11.6.1" class="ltx_text" style="font-size:70%;">—</span></td>
</tr>
<tr id="S7.T17.39.48" class="ltx_tr">
<td id="S7.T17.39.48.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.48.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.48.1.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.48.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">You are an expert in reasoning problem. Here are some examples about symbolic reasoning. You can use the knowledge in examples and solve the last problem. You should follow the examples and generate the final answer without external solution or words.</span></span>
</span>
</td>
<td id="S7.T17.39.48.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.48.2.1" class="ltx_text" style="font-size:70%;">66.75</span></td>
</tr>
<tr id="S7.T17.12.12" class="ltx_tr">
<td id="S7.T17.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S7.T17.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.12.12.2.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.12.12.2.1.1.1" class="ltx_text" style="font-size:70%;">Math Word Problems</span></span>
</span>
</td>
<td id="S7.T17.12.12.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S7.T17.12.12.3.1" class="ltx_text" style="font-size:70%;">GSM8k</span></td>
<td id="S7.T17.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S7.T17.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.12.12.1.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.12.12.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Problem: {problem}<math id="S7.T17.12.12.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.12.12.1.1.1.1.m1.1a"><mo id="S7.T17.12.12.1.1.1.1.m1.1.1" xref="S7.T17.12.12.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.12.12.1.1.1.1.m1.1b"><ci id="S7.T17.12.12.1.1.1.1.m1.1.1.cmml" xref="S7.T17.12.12.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.12.12.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n
Solution: Let’s think step by step.</span></span>
</span>
</td>
<td id="S7.T17.12.12.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S7.T17.12.12.4.1" class="ltx_text" style="font-size:70%;">78.47</span></td>
<td id="S7.T17.12.12.5" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S7.T17.12.12.5.1" class="ltx_text" style="font-size:70%;">63.20&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib742" title="" class="ltx_ref">742</a>]</cite></span></td>
</tr>
<tr id="S7.T17.25.25" class="ltx_tr">
<td id="S7.T17.25.25.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.25.25.13.13" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.25.25.13.13.13" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.25.25.13.13.13.13" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Let’s use python to solve math problems. Here are three examples how to do it,<math id="S7.T17.13.13.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.13.13.1.1.1.1.m1.1a"><mo id="S7.T17.13.13.1.1.1.1.m1.1.1" xref="S7.T17.13.13.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.13.13.1.1.1.1.m1.1b"><ci id="S7.T17.13.13.1.1.1.1.m1.1.1.cmml" xref="S7.T17.13.13.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.13.13.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?<math id="S7.T17.14.14.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.14.14.2.2.2.2.m2.1a"><mo id="S7.T17.14.14.2.2.2.2.m2.1.1" xref="S7.T17.14.14.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.14.14.2.2.2.2.m2.1b"><ci id="S7.T17.14.14.2.2.2.2.m2.1.1.cmml" xref="S7.T17.14.14.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.14.14.2.2.2.2.m2.1c">\backslash</annotation></semantics></math>n‘‘‘def solution():<math id="S7.T17.15.15.3.3.3.3.m3.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.15.15.3.3.3.3.m3.1a"><mo id="S7.T17.15.15.3.3.3.3.m3.1.1" xref="S7.T17.15.15.3.3.3.3.m3.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.15.15.3.3.3.3.m3.1b"><ci id="S7.T17.15.15.3.3.3.3.m3.1.1.cmml" xref="S7.T17.15.15.3.3.3.3.m3.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.15.15.3.3.3.3.m3.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;"""Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"""<math id="S7.T17.16.16.4.4.4.4.m4.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.16.16.4.4.4.4.m4.1a"><mo id="S7.T17.16.16.4.4.4.4.m4.1.1" xref="S7.T17.16.16.4.4.4.4.m4.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.16.16.4.4.4.4.m4.1b"><ci id="S7.T17.16.16.4.4.4.4.m4.1.1.cmml" xref="S7.T17.16.16.4.4.4.4.m4.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.16.16.4.4.4.4.m4.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;money_initial = 23<math id="S7.T17.17.17.5.5.5.5.m5.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.17.17.5.5.5.5.m5.1a"><mo id="S7.T17.17.17.5.5.5.5.m5.1.1" xref="S7.T17.17.17.5.5.5.5.m5.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.17.17.5.5.5.5.m5.1b"><ci id="S7.T17.17.17.5.5.5.5.m5.1.1.cmml" xref="S7.T17.17.17.5.5.5.5.m5.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.17.17.5.5.5.5.m5.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;bagels = 5<math id="S7.T17.18.18.6.6.6.6.m6.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.18.18.6.6.6.6.m6.1a"><mo id="S7.T17.18.18.6.6.6.6.m6.1.1" xref="S7.T17.18.18.6.6.6.6.m6.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.18.18.6.6.6.6.m6.1b"><ci id="S7.T17.18.18.6.6.6.6.m6.1.1.cmml" xref="S7.T17.18.18.6.6.6.6.m6.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.18.18.6.6.6.6.m6.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;bagel_cost = 3<math id="S7.T17.19.19.7.7.7.7.m7.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.19.19.7.7.7.7.m7.1a"><mo id="S7.T17.19.19.7.7.7.7.m7.1.1" xref="S7.T17.19.19.7.7.7.7.m7.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.19.19.7.7.7.7.m7.1b"><ci id="S7.T17.19.19.7.7.7.7.m7.1.1.cmml" xref="S7.T17.19.19.7.7.7.7.m7.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.19.19.7.7.7.7.m7.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;money_spent = bagels * bagel_cost<math id="S7.T17.20.20.8.8.8.8.m8.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.20.20.8.8.8.8.m8.1a"><mo id="S7.T17.20.20.8.8.8.8.m8.1.1" xref="S7.T17.20.20.8.8.8.8.m8.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.20.20.8.8.8.8.m8.1b"><ci id="S7.T17.20.20.8.8.8.8.m8.1.1.cmml" xref="S7.T17.20.20.8.8.8.8.m8.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.20.20.8.8.8.8.m8.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;money_left = money_initial - money_spent<math id="S7.T17.21.21.9.9.9.9.m9.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.21.21.9.9.9.9.m9.1a"><mo id="S7.T17.21.21.9.9.9.9.m9.1.1" xref="S7.T17.21.21.9.9.9.9.m9.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.21.21.9.9.9.9.m9.1b"><ci id="S7.T17.21.21.9.9.9.9.m9.1.1.cmml" xref="S7.T17.21.21.9.9.9.9.m9.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.21.21.9.9.9.9.m9.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;result = money_left<math id="S7.T17.22.22.10.10.10.10.m10.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.22.22.10.10.10.10.m10.1a"><mo id="S7.T17.22.22.10.10.10.10.m10.1.1" xref="S7.T17.22.22.10.10.10.10.m10.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.22.22.10.10.10.10.m10.1b"><ci id="S7.T17.22.22.10.10.10.10.m10.1.1.cmml" xref="S7.T17.22.22.10.10.10.10.m10.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.22.22.10.10.10.10.m10.1c">\backslash</annotation></semantics></math>n &nbsp;&nbsp;&nbsp;&nbsp;return result‘‘‘<math id="S7.T17.23.23.11.11.11.11.m11.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.23.23.11.11.11.11.m11.1a"><mo id="S7.T17.23.23.11.11.11.11.m11.1.1" xref="S7.T17.23.23.11.11.11.11.m11.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.23.23.11.11.11.11.m11.1b"><ci id="S7.T17.23.23.11.11.11.11.m11.1.1.cmml" xref="S7.T17.23.23.11.11.11.11.m11.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.23.23.11.11.11.11.m11.1c">\backslash</annotation></semantics></math>n ...... <math id="S7.T17.24.24.12.12.12.12.m12.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.24.24.12.12.12.12.m12.1a"><mo id="S7.T17.24.24.12.12.12.12.m12.1.1" xref="S7.T17.24.24.12.12.12.12.m12.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.24.24.12.12.12.12.m12.1b"><ci id="S7.T17.24.24.12.12.12.12.m12.1.1.cmml" xref="S7.T17.24.24.12.12.12.12.m12.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.24.24.12.12.12.12.m12.1c">\backslash</annotation></semantics></math>n How about this question?<math id="S7.T17.25.25.13.13.13.13.m13.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.25.25.13.13.13.13.m13.1a"><mo id="S7.T17.25.25.13.13.13.13.m13.1.1" xref="S7.T17.25.25.13.13.13.13.m13.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.25.25.13.13.13.13.m13.1b"><ci id="S7.T17.25.25.13.13.13.13.m13.1.1.cmml" xref="S7.T17.25.25.13.13.13.13.m13.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.25.25.13.13.13.13.m13.1c">\backslash</annotation></semantics></math>n Q:</span></span>
</span>
</td>
<td id="S7.T17.25.25.14" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.25.25.14.1" class="ltx_text" style="font-size:70%;">79.30</span></td>
</tr>
<tr id="S7.T17.39.49" class="ltx_tr">
<td id="S7.T17.39.49.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T17.39.49.1.1" class="ltx_text" style="font-size:70%;">SDG</span></td>
<td id="S7.T17.39.49.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.49.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.49.2.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.49.2.1.1.1" class="ltx_text" style="font-size:70%;">Code Synthesis</span></span>
</span>
</td>
<td id="S7.T17.39.49.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T17.39.49.3.1" class="ltx_text" style="font-size:70%;">HumanEval</span></td>
<td id="S7.T17.39.49.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.49.4.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.49.4.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.49.4.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">I want you act as a code completer. Given a code snippet, your objective is to complete the code and ensure that it can achieve the described functionality.</span></span>
</span>
</td>
<td id="S7.T17.39.49.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.49.5.1" class="ltx_text" style="font-size:70%;">79.88</span></td>
<td id="S7.T17.39.49.6" class="ltx_td ltx_align_right ltx_border_t">
<span id="S7.T17.39.49.6.1" class="ltx_text" style="font-size:70%;">48.20&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T17.39.49.6.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib743" title="" class="ltx_ref">743</a><span id="S7.T17.39.49.6.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T17.33.33" class="ltx_tr">
<td id="S7.T17.33.33.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.33.33.9.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.33.33.9.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.33.33.9.1.1.1" class="ltx_text"></span><span id="S7.T17.33.33.9.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S7.T17.33.33.9.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S7.T17.33.33.9.1.1.2.1.1" class="ltx_tr">
<span id="S7.T17.33.33.9.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Text-to-SQL</span></span>
</span></span><span id="S7.T17.33.33.9.1.1.3" class="ltx_text"></span><span id="S7.T17.33.33.9.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S7.T17.33.33.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T17.33.33.10.1" class="ltx_text" style="font-size:70%;">Spider</span></td>
<td id="S7.T17.33.33.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.33.33.8.8" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.33.33.8.8.8" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.33.33.8.8.8.8" class="ltx_text ltx_font_typewriter" style="font-size:70%;">### Complete sqlite SQL query only and with no explanation.<math id="S7.T17.26.26.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.26.26.1.1.1.1.m1.1a"><mo id="S7.T17.26.26.1.1.1.1.m1.1.1" xref="S7.T17.26.26.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.26.26.1.1.1.1.m1.1b"><ci id="S7.T17.26.26.1.1.1.1.m1.1.1.cmml" xref="S7.T17.26.26.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.26.26.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n #<math id="S7.T17.27.27.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.27.27.2.2.2.2.m2.1a"><mo id="S7.T17.27.27.2.2.2.2.m2.1.1" xref="S7.T17.27.27.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.27.27.2.2.2.2.m2.1b"><ci id="S7.T17.27.27.2.2.2.2.m2.1.1.cmml" xref="S7.T17.27.27.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.27.27.2.2.2.2.m2.1c">\backslash</annotation></semantics></math>n### Sqlite SQL tables, with their properties: <math id="S7.T17.28.28.3.3.3.3.m3.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.28.28.3.3.3.3.m3.1a"><mo id="S7.T17.28.28.3.3.3.3.m3.1.1" xref="S7.T17.28.28.3.3.3.3.m3.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.28.28.3.3.3.3.m3.1b"><ci id="S7.T17.28.28.3.3.3.3.m3.1.1.cmml" xref="S7.T17.28.28.3.3.3.3.m3.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.28.28.3.3.3.3.m3.1c">\backslash</annotation></semantics></math>n#<math id="S7.T17.29.29.4.4.4.4.m4.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.29.29.4.4.4.4.m4.1a"><mo id="S7.T17.29.29.4.4.4.4.m4.1.1" xref="S7.T17.29.29.4.4.4.4.m4.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.29.29.4.4.4.4.m4.1b"><ci id="S7.T17.29.29.4.4.4.4.m4.1.1.cmml" xref="S7.T17.29.29.4.4.4.4.m4.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.29.29.4.4.4.4.m4.1c">\backslash</annotation></semantics></math>n{table}<math id="S7.T17.30.30.5.5.5.5.m5.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.30.30.5.5.5.5.m5.1a"><mo id="S7.T17.30.30.5.5.5.5.m5.1.1" xref="S7.T17.30.30.5.5.5.5.m5.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.30.30.5.5.5.5.m5.1b"><ci id="S7.T17.30.30.5.5.5.5.m5.1.1.cmml" xref="S7.T17.30.30.5.5.5.5.m5.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.30.30.5.5.5.5.m5.1c">\backslash</annotation></semantics></math>n# {foreign_key}<math id="S7.T17.31.31.6.6.6.6.m6.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.31.31.6.6.6.6.m6.1a"><mo id="S7.T17.31.31.6.6.6.6.m6.1.1" xref="S7.T17.31.31.6.6.6.6.m6.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.31.31.6.6.6.6.m6.1b"><ci id="S7.T17.31.31.6.6.6.6.m6.1.1.cmml" xref="S7.T17.31.31.6.6.6.6.m6.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.31.31.6.6.6.6.m6.1c">\backslash</annotation></semantics></math>n#<math id="S7.T17.32.32.7.7.7.7.m7.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.32.32.7.7.7.7.m7.1a"><mo id="S7.T17.32.32.7.7.7.7.m7.1.1" xref="S7.T17.32.32.7.7.7.7.m7.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.32.32.7.7.7.7.m7.1b"><ci id="S7.T17.32.32.7.7.7.7.m7.1.1.cmml" xref="S7.T17.32.32.7.7.7.7.m7.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.32.32.7.7.7.7.m7.1c">\backslash</annotation></semantics></math>n### {question}<math id="S7.T17.33.33.8.8.8.8.m8.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.33.33.8.8.8.8.m8.1a"><mo id="S7.T17.33.33.8.8.8.8.m8.1.1" xref="S7.T17.33.33.8.8.8.8.m8.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.33.33.8.8.8.8.m8.1b"><ci id="S7.T17.33.33.8.8.8.8.m8.1.1.cmml" xref="S7.T17.33.33.8.8.8.8.m8.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.33.33.8.8.8.8.m8.1c">\backslash</annotation></semantics></math>n SELECT</span></span>
</span>
</td>
<td id="S7.T17.33.33.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.33.33.11.1" class="ltx_text" style="font-size:70%;">70.10</span></td>
<td id="S7.T17.33.33.12" class="ltx_td ltx_align_right ltx_border_t">
<span id="S7.T17.33.33.12.1" class="ltx_text" style="font-size:70%;">84.10&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T17.33.33.12.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib744" title="" class="ltx_ref">744</a><span id="S7.T17.33.33.12.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T17.39.39" class="ltx_tr">
<td id="S7.T17.39.39.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="15"><span id="S7.T17.39.39.7.1" class="ltx_text" style="font-size:70%;">IR</span></td>
<td id="S7.T17.39.39.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.39.8.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.39.8.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.39.8.1.1.1" class="ltx_text" style="font-size:70%;">Recommendation</span></span>
</span>
</td>
<td id="S7.T17.39.39.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T17.39.39.9.1" class="ltx_text" style="font-size:70%;">MovieLens</span></td>
<td id="S7.T17.39.39.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S7.T17.39.39.6.6" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.39.6.6.6" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.39.6.6.6.6" class="ltx_text ltx_font_typewriter" style="font-size:70%;">I’ve watched the following movies in the past in order: <math id="S7.T17.34.34.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.34.34.1.1.1.1.m1.1a"><mo id="S7.T17.34.34.1.1.1.1.m1.1.1" xref="S7.T17.34.34.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.34.34.1.1.1.1.m1.1b"><ci id="S7.T17.34.34.1.1.1.1.m1.1.1.cmml" xref="S7.T17.34.34.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.34.34.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n {user_his_text} <math id="S7.T17.35.35.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.35.35.2.2.2.2.m2.1a"><mo id="S7.T17.35.35.2.2.2.2.m2.1.1" xref="S7.T17.35.35.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.35.35.2.2.2.2.m2.1b"><ci id="S7.T17.35.35.2.2.2.2.m2.1.1.cmml" xref="S7.T17.35.35.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.35.35.2.2.2.2.m2.1c">\backslash</annotation></semantics></math>n<math id="S7.T17.36.36.3.3.3.3.m3.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.36.36.3.3.3.3.m3.1a"><mo id="S7.T17.36.36.3.3.3.3.m3.1.1" xref="S7.T17.36.36.3.3.3.3.m3.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.36.36.3.3.3.3.m3.1b"><ci id="S7.T17.36.36.3.3.3.3.m3.1.1.cmml" xref="S7.T17.36.36.3.3.3.3.m3.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.36.36.3.3.3.3.m3.1c">\backslash</annotation></semantics></math>n Now there are {recall_budget} candidate movies that I can watch next: <math id="S7.T17.37.37.4.4.4.4.m4.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.37.37.4.4.4.4.m4.1a"><mo id="S7.T17.37.37.4.4.4.4.m4.1.1" xref="S7.T17.37.37.4.4.4.4.m4.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.37.37.4.4.4.4.m4.1b"><ci id="S7.T17.37.37.4.4.4.4.m4.1.1.cmml" xref="S7.T17.37.37.4.4.4.4.m4.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.37.37.4.4.4.4.m4.1c">\backslash</annotation></semantics></math>n {candidate_text_order} <math id="S7.T17.38.38.5.5.5.5.m5.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.38.38.5.5.5.5.m5.1a"><mo id="S7.T17.38.38.5.5.5.5.m5.1.1" xref="S7.T17.38.38.5.5.5.5.m5.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.38.38.5.5.5.5.m5.1b"><ci id="S7.T17.38.38.5.5.5.5.m5.1.1.cmml" xref="S7.T17.38.38.5.5.5.5.m5.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.38.38.5.5.5.5.m5.1c">\backslash</annotation></semantics></math>n Please rank these {recall_budget} movies by measuring the possibilities that I would like to watch next most, according to my watching history. Please think step by step. <math id="S7.T17.39.39.6.6.6.6.m6.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S7.T17.39.39.6.6.6.6.m6.1a"><mo id="S7.T17.39.39.6.6.6.6.m6.1.1" xref="S7.T17.39.39.6.6.6.6.m6.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S7.T17.39.39.6.6.6.6.m6.1b"><ci id="S7.T17.39.39.6.6.6.6.m6.1.1.cmml" xref="S7.T17.39.39.6.6.6.6.m6.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T17.39.39.6.6.6.6.m6.1c">\backslash</annotation></semantics></math>n Note that my most recently watched movie is {recent_item}. Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate movies. You can not generate movies that are not in the given candidate list.</span></span>
</span>
</td>
<td id="S7.T17.39.39.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S7.T17.39.39.10.1" class="ltx_text" style="font-size:70%;">48.80</span></td>
<td id="S7.T17.39.39.11" class="ltx_td ltx_align_right ltx_border_t">
<span id="S7.T17.39.39.11.1" class="ltx_text" style="font-size:70%;">76.25&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T17.39.39.11.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib745" title="" class="ltx_ref">745</a><span id="S7.T17.39.39.11.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T17.39.50" class="ltx_tr">
<td id="S7.T17.39.50.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S7.T17.39.50.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.50.1.1.1" class="ltx_p" style="width:43.4pt;"><span id="S7.T17.39.50.1.1.1.1" class="ltx_text" style="font-size:70%;">Conversational  Recommendation</span></span>
</span>
</td>
<td id="S7.T17.39.50.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S7.T17.39.50.2.1" class="ltx_text" style="font-size:70%;">ReDial</span></td>
<td id="S7.T17.39.50.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S7.T17.39.50.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T17.39.50.3.1.1" class="ltx_p" style="width:238.5pt;"><span id="S7.T17.39.50.3.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Recommend 10 items that are consistent with user preference. The recommendation list can contain items that the dialog mentioned before. The format of the recommendation list is: no. title (year). Don’t mention anything other than the title of items in your recommendation list</span></span>
</span>
</td>
<td id="S7.T17.39.50.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S7.T17.39.50.4.1" class="ltx_text" style="font-size:70%;">17.20</span></td>
<td id="S7.T17.39.50.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">
<span id="S7.T17.39.50.5.1" class="ltx_text" style="font-size:70%;">25.60&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T17.39.50.5.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib746" title="" class="ltx_ref">746</a><span id="S7.T17.39.50.5.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span><span id="S7.SS4.1.1" class="ltx_text ltx_font_italic">Empirical Evaluation</span>
</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">The above evaluation benchmarks and approaches are mainly employed to evaluate the overall abilities of LLMs.
In this part, we conduct a fine-grained evaluation of the abilities discussed in Section&nbsp;<a href="#S7.SS1" title="7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a> and Section&nbsp;<a href="#S7.SS2" title="7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>.
For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the corresponding performance of LLMs.</p>
</div>
<section id="S7.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.1 </span>Experimental Settings</h4>

<div id="S7.SS4.SSS1.p1" class="ltx_para">
<p id="S7.SS4.SSS1.p1.1" class="ltx_p">In this part, we introduce the experimental settings for our evaluation.</p>
</div>
<div id="S7.SS4.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S7.SS4.SSS1.p2.1" class="ltx_p"><span id="S7.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Evaluation Models.</span>
To conduct the evaluation, we consider representative LLMs from open-source models to closed-source API-accessing models as follows:</p>
</div>
<div id="S7.SS4.SSS1.p3" class="ltx_para">
<p id="S7.SS4.SSS1.p3.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">Open-source models.</em>
Existing open-source models can be categorized into base models and instruction-tuned models. Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning. In our evaluation, we select four representative base models including LLaMA (7B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, LLaMA 2 (7B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, Pythia (7B and 12B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>, and Falcon (7B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib747" title="" class="ltx_ref">747</a>]</cite><span id="footnote48" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">48</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">48</sup><span class="ltx_tag ltx_tag_note">48</span>Experiments with larger models are still in schedule due to the limit of computational resources. </span></span></span>.
Instruction-tuned models are those fine-tuned using instructions (<em id="S7.SS4.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> task datasets, daily chat, or synthetic instructions). In our experiments, we select four representative instruction-tuned models including Vicuna (7B and 13B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>, Alpaca (7B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>, and ChatGLM (6B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.
In addition, we also include LLaMA 2-Chat (7B)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B).</p>
</div>
<div id="S7.SS4.SSS1.p4" class="ltx_para">
<p id="S7.SS4.SSS1.p4.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Closed-source models.</em>
In addition to the open-source models, there are also closed-source models that can only be accessed via APIs, which have gained much attention from both developers and researchers.
Here, we select four representative closed-source models including text-davinci-002/003 (short as <em id="S7.SS4.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">Davinci002/003</em>), ChatGPT, Claude, and Claude 2, where the first three models are developed by OpenAI and the other two are developed by Anthropic.</p>
</div>
<div id="S7.SS4.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S7.SS4.SSS1.p5.1" class="ltx_p"><span id="S7.SS4.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Tasks and Datasets.</span> Next, we set up the evaluation tasks and datasets for the abilities discussed in Section&nbsp;<a href="#S7.SS1" title="7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a> and Section&nbsp;<a href="#S7.SS2" title="7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>. We mainly evaluate the zero-shot performance of LLMs on these datasets. For more complex tasks that are hard to be solved in the zero-shot manner (<em id="S7.SS4.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> mathematical reasoning and tool manipulation), we mainly report the 3-shot performance, considering the context length limit of open-source models.</p>
</div>
<div id="S7.SS4.SSS1.p6" class="ltx_para">
<p id="S7.SS4.SSS1.p6.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p6.1.2" class="ltx_emph ltx_font_italic">Language generation.</em> As discussed before, for language generation, we consider evaluating three kinds of tasks, <em id="S7.SS4.SSS1.p6.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> language modeling, conditional text generation, and code synthesis. Specially, we select four commonly-used datasets, namely LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> (language modeling), WMT’22&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib545" title="" class="ltx_ref">545</a>]</cite> (machine translation), XSum&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib549" title="" class="ltx_ref">549</a>]</cite> (text summarization), and HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> (code synthesis) for evaluation.

In WMT’22, we construct a new evaluation set by selecting 1000 examples for each language pair from the original large-scale test set to examine the average performance of LLMs in machine translation.
We evaluate the zero-shot performance of LLMs on these datasets, and compute the <em id="S7.SS4.SSS1.p6.1.4" class="ltx_emph ltx_font_italic">accuracy</em> of predicting words for LAMBADA, <em id="S7.SS4.SSS1.p6.1.5" class="ltx_emph ltx_font_italic">BLEU-4</em> for WMT’22, <em id="S7.SS4.SSS1.p6.1.6" class="ltx_emph ltx_font_italic">ROUGE-L</em> for XSum, and <em id="S7.SS4.SSS1.p6.1.1" class="ltx_emph ltx_font_italic">pass@<math id="S7.SS4.SSS1.p6.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.SS4.SSS1.p6.1.1.m1.1a"><mn id="S7.SS4.SSS1.p6.1.1.m1.1.1" xref="S7.SS4.SSS1.p6.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p6.1.1.m1.1b"><cn type="integer" id="S7.SS4.SSS1.p6.1.1.m1.1.1.cmml" xref="S7.SS4.SSS1.p6.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p6.1.1.m1.1c">10</annotation></semantics></math></em> for HumanEval.</p>
</div>
<div id="S7.SS4.SSS1.p7" class="ltx_para">
<p id="S7.SS4.SSS1.p7.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p7.1.1" class="ltx_emph ltx_font_italic">Knowledge utilization.</em>
To evaluate the ability of knowledge utilization, we select four question answering datasets (<em id="S7.SS4.SSS1.p7.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib558" title="" class="ltx_ref">558</a>]</cite>, Natural Questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib554" title="" class="ltx_ref">554</a>]</cite>, Web Questions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib557" title="" class="ltx_ref">557</a>]</cite>, and ARC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib555" title="" class="ltx_ref">555</a>]</cite>), and a fact extraction dataset, WikiFact&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib571" title="" class="ltx_ref">571</a>]</cite>.
We also report the zero-shot performance of LLMs on these datasets, and compute <em id="S7.SS4.SSS1.p7.1.3" class="ltx_emph ltx_font_italic">accuracy</em> for ARC and <em id="S7.SS4.SSS1.p7.1.4" class="ltx_emph ltx_font_italic">exact match</em> for other datasets.</p>
</div>
<div id="S7.SS4.SSS1.p8" class="ltx_para">
<p id="S7.SS4.SSS1.p8.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p8.1.1" class="ltx_emph ltx_font_italic">Complex reasoning.</em> For complex reasoning, we evaluate the comparison models on OpenbookQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib566" title="" class="ltx_ref">566</a>]</cite>, HellaSwag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib582" title="" class="ltx_ref">582</a>]</cite>, and SocialIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib581" title="" class="ltx_ref">581</a>]</cite> for knowledge reasoning; Colored Objects&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and Penguins in the Table&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> for symbolic reasoning; GSM8k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> and MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> for mathematical reasoning. We compute the <em id="S7.SS4.SSS1.p8.1.2" class="ltx_emph ltx_font_italic">accuracy</em> for OpenbookQA, HellaSwag, and SocialIQA; <em id="S7.SS4.SSS1.p8.1.3" class="ltx_emph ltx_font_italic">solve rate</em> for Colored Objects and Penguins in the Table; and <em id="S7.SS4.SSS1.p8.1.4" class="ltx_emph ltx_font_italic">accuracy</em> for GSM8k and MATH.
For knowledge reasoning tasks, we evaluate the zero-shot performance, since they are all QA tasks that can be solved in a zero-shot setting.
For complex symbolic reasoning and mathematical reasoning tasks, we leverage 3-shot in-context exemplars to better elicit LLMs to accomplish them.
Following existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, we also utilize the chain-of-thought prompting strategy for better solving the mathematical reasoning tasks.</p>
</div>
<div id="S7.SS4.SSS1.p9" class="ltx_para">
<p id="S7.SS4.SSS1.p9.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p9.1.1" class="ltx_emph ltx_font_italic">Human alignment.</em> For human alignment, we select TruthfulQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib556" title="" class="ltx_ref">556</a>]</cite> to measure whether a LLM is truthful in generating answers to questions, CrowS-Pairs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib603" title="" class="ltx_ref">603</a>]</cite> and WinoGender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib604" title="" class="ltx_ref">604</a>]</cite> to assess the stereotypes in LLMs, RealToxityPrompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib605" title="" class="ltx_ref">605</a>]</cite> to evaluate the extent to which LLMs generate toxic language, and HaluEval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib602" title="" class="ltx_ref">602</a>]</cite> to test the ability of LLMs to recognize hallucination.
As the test set of Real-Toxicity-Prompts is too large, we randomly sample 10000 examples from it for evaluation.
We follow LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> to report the zero-shot performance, and compute the <em id="S7.SS4.SSS1.p9.1.2" class="ltx_emph ltx_font_italic">accuracy</em> of identifying a claim as true for TruthfulQA, <em id="S7.SS4.SSS1.p9.1.3" class="ltx_emph ltx_font_italic">accuracy</em> of recognizing biased sentences (high perplexity) for CrowS-Pairs, <em id="S7.SS4.SSS1.p9.1.4" class="ltx_emph ltx_font_italic">coreference resolution accuracy (he/she/they)</em> for WinoGender, <em id="S7.SS4.SSS1.p9.1.5" class="ltx_emph ltx_font_italic">toxicity score</em> for RealToxityPrompts, and <em id="S7.SS4.SSS1.p9.1.6" class="ltx_emph ltx_font_italic">average accuracy</em> of recognizing hallucinations for HaluEval.
For TruthfulQA, we follow existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> that utilizes text-davinci-003 to replace humans for scoring.
For Crows-Pairs and WinoGender, we follow the experimental settings of LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> to compute the perplexity and coreference resolution score.
For RealToxityPrompts, we utilize the Perspective-API<span id="footnote49" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">49</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">49</sup><span class="ltx_tag ltx_tag_note">49</span><a target="_blank" href="https://perspectiveapi.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://perspectiveapi.com/</a></span></span></span> for toxicity evaluation.</p>
</div>
<div id="S7.SS4.SSS1.p10" class="ltx_para">
<p id="S7.SS4.SSS1.p10.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p10.1.1" class="ltx_emph ltx_font_italic">Interaction with environment.</em> To test this ability, we select ALFWorld&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib609" title="" class="ltx_ref">609</a>]</cite> and WebShop&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib610" title="" class="ltx_ref">610</a>]</cite> for evaluation, which simulate real-world scenarios such as household and e-commerce environments.
We follow the setting of ReAct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref">449</a>]</cite> that evaluate the 1-shot and 2-shot performance of LLMs on WebShop and ALFWorld respectively, and compute <em id="S7.SS4.SSS1.p10.1.2" class="ltx_emph ltx_font_italic">success rate</em> for ALFWorld and <em id="S7.SS4.SSS1.p10.1.3" class="ltx_emph ltx_font_italic">average score/success rate</em> for WebShop.
Further, we also follow ReAct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref">449</a>]</cite> to reduce the length of the input prompt and utilize line break as the EOS token.</p>
</div>
<div id="S7.SS4.SSS1.p11" class="ltx_para">
<p id="S7.SS4.SSS1.p11.1" class="ltx_p">•&nbsp;<em id="S7.SS4.SSS1.p11.1.1" class="ltx_emph ltx_font_italic">Tool manipulation.</em> For tool manipulation, we consider two kinds of tools including search engine and model interfaces. Therefore, we adopt two tool manipulation benchmarks, <em id="S7.SS4.SSS1.p11.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> HotpotQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib579" title="" class="ltx_ref">579</a>]</cite> and Gorilla&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib617" title="" class="ltx_ref">617</a>]</cite>. HotpotQA requires LLMs to use search engine to retrieve documents from the web, and Gorilla to invoke model APIs from three hubs of TorchHub, TensorHub and HuggingFace. We compute <em id="S7.SS4.SSS1.p11.1.3" class="ltx_emph ltx_font_italic">exact match</em> for HotpotQA and <em id="S7.SS4.SSS1.p11.1.4" class="ltx_emph ltx_font_italic">accuracy</em> for Gorilla.
For HotpotQA, we follow ReAct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref">449</a>]</cite> to report the 3-shot performance.
For Gorilla, we follow the code released by its paper&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib617" title="" class="ltx_ref">617</a>]</cite>, and evaluate the zero-shot performance.</p>
</div>
<div id="S7.SS4.SSS1.p12" class="ltx_para ltx_noindent">
<p id="S7.SS4.SSS1.p12.1" class="ltx_p"><span id="S7.SS4.SSS1.p12.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span> For each task and dataset, we evaluate the compared LLMs using the same prompts and results parsing method provided by existing work (<em id="S7.SS4.SSS1.p12.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> TruthfulQA, HotPotQA, Gorilla, HaluEval) or designed according to our empirical experience (<em id="S7.SS4.SSS1.p12.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT’22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt).
Specifically, all the experiments about closed-source models are based on invoking their official APIs, while for open-source models, we utilize their publicly available code and model parameters, and perform the inference on 8 A800-80G GPUs.
For TriviaQA, OpenbookQA, HellaSwag, and SocialIQA, we experiment on the development set since the test set is not publicly released. While for other datasets, we experiment on the test set. To reproduce our experiments, we also publicly release our experimental code and data in <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments</a>.</p>
</div>
</section>
<section id="S7.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.2 </span>Results Analysis and Findings</h4>

<div id="S7.SS4.SSS2.p1" class="ltx_para">
<p id="S7.SS4.SSS2.p1.1" class="ltx_p">We report the experimental results in Table&nbsp;<a href="#S7.T16" title="TABLE XVI ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVI</span></a>, and analyze the results in the following.</p>
</div>
<div id="S7.SS4.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S7.SS4.SSS2.p2.1" class="ltx_p"><span id="S7.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Analysis of Closed-Source Models.</span>
We summarize our analysis and findings of the four closed-source models (<em id="S7.SS4.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> ChatGPT, Claude, Davinci003 and Davinci002) as follows:</p>
</div>
<div id="S7.SS4.SSS2.p3" class="ltx_para">
<p id="S7.SS4.SSS2.p3.1" class="ltx_p"><math id="S7.SS4.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p3.1.m1.1a"><mo id="S7.SS4.SSS2.p3.1.m1.1.1" xref="S7.SS4.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p3.1.m1.1b"><ci id="S7.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">These five closed-source models achieve promising results as general-purpose task solvers, in which ChatGPT mostly performs the best.</em> ChatGPT, Claude, Claude 2, Davinci003 and Davinci002 perform well in most of tasks, including complex tasks (<em id="S7.SS4.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GSM8k), which have shown great potential to be general-purpose task solvers. Among them, ChatGPT exhibits a more superior model capacity on the evaluation tasks, winning the most across all tasks.
In some evaluation tasks, the performance gap between ChatGPT and other closed-source models is very large, especially for complex tasks <em id="S7.SS4.SSS2.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> 78.47 (ChatGPT) <em id="S7.SS4.SSS2.p3.1.4" class="ltx_emph ltx_font_italic">v.s.</em> 49.96 (Davinci002) on GSM8k, and 79.88 (ChatGPT) <em id="S7.SS4.SSS2.p3.1.5" class="ltx_emph ltx_font_italic">v.s.</em> 51.22 (Claude) on HumanEval.</p>
</div>
<div id="S7.SS4.SSS2.p4" class="ltx_para">
<p id="S7.SS4.SSS2.p4.1" class="ltx_p"><math id="S7.SS4.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p4.1.m1.1a"><mo id="S7.SS4.SSS2.p4.1.m1.1.1" xref="S7.SS4.SSS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p4.1.m1.1b"><ci id="S7.SS4.SSS2.p4.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">Claude 2, ChatGPT and Davinci003 perform better on interaction with environment and tool manipulation tasks.</em>
On the two evaluation tasks, Claude 2, ChatGPT and Davinci003, perform better than other models by a large margin, <em id="S7.SS4.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> 36.40 (Claude 2) <em id="S7.SS4.SSS2.p4.1.3" class="ltx_emph ltx_font_italic">v.s.</em> 26.00 (Davinci002) on HotpotQA, 44.53 (ChatGPT) <em id="S7.SS4.SSS2.p4.1.4" class="ltx_emph ltx_font_italic">v.s.</em> 7.74 (Claude) on Gorilla-TF, and 72.58 (Davinci003) <em id="S7.SS4.SSS2.p4.1.5" class="ltx_emph ltx_font_italic">v.s.</em> 22.04 (Claude) on Gorilla-TH.
A possible reason is that these three models have been specially optimized towards these advanced abilities, <em id="S7.SS4.SSS2.p4.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> supporting the use of external plugins.</p>
</div>
<div id="S7.SS4.SSS2.p5" class="ltx_para">
<p id="S7.SS4.SSS2.p5.1" class="ltx_p"><math id="S7.SS4.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p5.1.m1.1a"><mo id="S7.SS4.SSS2.p5.1.m1.1.1" xref="S7.SS4.SSS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p5.1.m1.1b"><ci id="S7.SS4.SSS2.p5.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p5.1.m1.1c">\bullet</annotation></semantics></math>
<em id="S7.SS4.SSS2.p5.1.1" class="ltx_emph ltx_font_italic">All the comparison models perform not well on very difficult reasoning tasks.</em>
On MATH and HotpotQA, all models (including ChatGPT) perform not well. The two tasks are very difficult to solve, requiring accurate understanding of complex mathematical knowledge and performing multi-hop reasoning across documents, respectively.
Further, these models also have a relatively weak performance on machine translation task (WMT).
A possible reason is that WMT also contains many evaluation examples in minor languages, which might not be well covered in the pre-training data of these LLMs.</p>
</div>
<div id="S7.SS4.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S7.SS4.SSS2.p6.1" class="ltx_p"><span id="S7.SS4.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Analysis of Open-Source Models.</span>
Next, we continue to show our analysis and findings about eight open-source models (<em id="S7.SS4.SSS2.p6.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM, LLaMA 2, LLaMA, Pythia and Falcon) as follows:</p>
</div>
<div id="S7.SS4.SSS2.p7" class="ltx_para">
<p id="S7.SS4.SSS2.p7.1" class="ltx_p"><math id="S7.SS4.SSS2.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p7.1.m1.1a"><mo id="S7.SS4.SSS2.p7.1.m1.1.1" xref="S7.SS4.SSS2.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p7.1.m1.1b"><ci id="S7.SS4.SSS2.p7.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p7.1.1" class="ltx_emph ltx_font_italic">Instruction-tuned models mostly perform better than the base models.</em>
Among all the compared open-source methods, the instruction-tuned models (<em id="S7.SS4.SSS2.p7.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> LLaMA 2-Chat, Vicuna, Alpaca and ChatGLM) mostly perform better than non-instruction-tuned models (<em id="S7.SS4.SSS2.p7.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> LLaMA 2, LLaMA, Pythia and Falcon).
It indicates that instruction tuning is generally capable of improving the few-shot or zero-shot ability of LLMs in solving various tasks.
However, after instruction tuning, Vicuna (7B) and Alpaca (7B) suffer from performance degradations on LAMBADA, a language modeling task.
The reason may be that the instruction data mainly focuses on enabling LLMs to follow human instructions, which is not always useful for the general language generation task.</p>
</div>
<div id="S7.SS4.SSS2.p8" class="ltx_para">
<p id="S7.SS4.SSS2.p8.1" class="ltx_p"><math id="S7.SS4.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p8.1.m1.1a"><mo id="S7.SS4.SSS2.p8.1.m1.1.1" xref="S7.SS4.SSS2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p8.1.m1.1b"><ci id="S7.SS4.SSS2.p8.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p8.1.1" class="ltx_emph ltx_font_italic">These small-sized open-source models perform not well on mathematical reasoning, interaction with environment, and tool manipulation tasks.</em>
On the tasks of mathematical reasoning, interaction with environment and tool manipulation, all these evaluated open-source models perform not well, including instruction-tuned ones. A possible reason is that the instruction data for fine-tuning these models is not specifically designed for these tasks. In addition, these closed-source models may have limited model capacities due to small model sizes.</p>
</div>
<div id="S7.SS4.SSS2.p9" class="ltx_para">
<p id="S7.SS4.SSS2.p9.1" class="ltx_p"><math id="S7.SS4.SSS2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p9.1.m1.1a"><mo id="S7.SS4.SSS2.p9.1.m1.1.1" xref="S7.SS4.SSS2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p9.1.m1.1b"><ci id="S7.SS4.SSS2.p9.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p9.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p9.1.1" class="ltx_emph ltx_font_italic">The top-performing model varies on different human alignment tasks.</em>
For different human alignment tasks, we can see that these models achieve inconsistent performance rankings.
For example, LLaMA 2-Chat (7B) performs the best among the compared open-source models on TruthfulQA, while Vicuna (13B) performs the best on CrowS-Pairs. A possible reason is that these tasks are designed with specific purposes for evaluating different aspects of human alignment, and these models exhibit varied performance on different tasks, even for the variants of the same model (<em id="S7.SS4.SSS2.p9.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> Pythia (7B) and Pythia (12B)). More experiments and analysis on human alignment evaluation are needed to reveal more detailed findings.</p>
</div>
<div id="S7.SS4.SSS2.p10" class="ltx_para">
<p id="S7.SS4.SSS2.p10.1" class="ltx_p"><math id="S7.SS4.SSS2.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p10.1.m1.1a"><mo id="S7.SS4.SSS2.p10.1.m1.1.1" xref="S7.SS4.SSS2.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p10.1.m1.1b"><ci id="S7.SS4.SSS2.p10.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p10.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p10.1.1" class="ltx_emph ltx_font_italic">As a more recently released model, LLaMA 2 (7B) overall achieves a good performance, especially on complex reasoning tasks.</em>
For complex reasoning tasks, LLaMA 2 (7B) mostly performs better than other base models, <em id="S7.SS4.SSS2.p10.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> 43.95 (LLaMA 2 (7B)) <em id="S7.SS4.SSS2.p10.1.3" class="ltx_emph ltx_font_italic">v.s.</em> 29.80 (Falcon (7B)) in C-Objects.
For other tasks (<em id="S7.SS4.SSS2.p10.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> language generation and knowledge utilization), LLaMA 2 (7B) can also achieve comparable performance as the best-performing base models. It has used more data for pre-training (<em id="S7.SS4.SSS2.p10.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> about 2 trillion tokens), which mainly contributes to the excellent performance. Furthermore, it also conducts a more robust data cleaning process.</p>
</div>
<div id="S7.SS4.SSS2.p11" class="ltx_para">
<p id="S7.SS4.SSS2.p11.1" class="ltx_p"><math id="S7.SS4.SSS2.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S7.SS4.SSS2.p11.1.m1.1a"><mo id="S7.SS4.SSS2.p11.1.m1.1.1" xref="S7.SS4.SSS2.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p11.1.m1.1b"><ci id="S7.SS4.SSS2.p11.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p11.1.m1.1c">\bullet</annotation></semantics></math> <em id="S7.SS4.SSS2.p11.1.1" class="ltx_emph ltx_font_italic">Scaling the open-source modes can improve the performance consistently.</em>
By comparing the performance of Vicuna (7B) and Vicuna (13B), Pythia (7B) and Pythia (13B), we can see that the models with larger scales mostly perform better than smaller ones on these evaluation tasks, indicating the effectiveness of scaling up the model size.
Across different tasks, scaling model is more beneficial for more complex tasks (<em id="S7.SS4.SSS2.p11.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> symbolic and mathematical reasoning), where the larger models mostly outperform smaller ones in a large margin.</p>
</div>
<div id="S7.SS4.SSS2.p12" class="ltx_para">
<p id="S7.SS4.SSS2.p12.1" class="ltx_p">The readers should be note that these findings about open-source language models are limited to the model sizes. We will continually update this part by including the results of larger versions of these models, and also call for the support of computational resources for more experiments.</p>
</div>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Applications</span>
</h2>

<figure id="S8.F18" class="ltx_figure"><img src="/html/2303.18223/assets/x18.png" id="S8.F18.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>The applications of LLMs in representative research directions and downstream domains. </figcaption>
</figure>
<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this section, we briefly review the recent progress on the applications of LLMs in two aspects, namely the impact to research community and representative domains.
Figure&nbsp;<a href="#S8.F18" title="Figure 18 ‣ 8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> shows a content organization of this section<span id="footnote50" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">50</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">50</sup><span class="ltx_tag ltx_tag_note">50</span>Note that we don’t aim to cover all the related research directions or domains, but instead demonstrating the use or impact of LLMs via these selected examples. </span></span></span>.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span><span id="S8.SS1.1.1" class="ltx_text ltx_font_italic">LLM for Research Community</span>
</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">As LLMs have revolutionized the way how we develop AI algorithms, it poses significant impact on the research community. In this part, we briefly review the advances that led by LLMs for several representative research directions.</p>
</div>
<section id="S8.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.1 </span>LLM for Classic NLP Tasks</h4>

<div id="S8.SS1.SSS1.p1" class="ltx_para">
<p id="S8.SS1.SSS1.p1.1" class="ltx_p">As pre-trained language models (<em id="S8.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> BERT) have originated in the field of NLP, the technical advances of language models has an important impact on the research of NLP. In this part, we discuss the application of LLMs on five kinds of classic NLP tasks, including word-level, sentence-level, sequence tagging, relation extraction, and text generation tasks, which had been the foundation of many existing NLP systems and applications.
Note that we do not intend to comprehensively cover all NLP tasks, but instead try to analyze the impact of LLMs for fundamental NLP research through the basic tasks.
We also omit the discussion of several tasks (<em id="S8.SS1.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> language modeling) that have been discussed early in this survey.</p>
</div>
<div id="S8.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS1.p2.1" class="ltx_p"><span id="S8.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Word/Sentence-level Tasks.</span>
As long-standing NLP tasks, word-level (<em id="S8.SS1.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> word clustering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib748" title="" class="ltx_ref">748</a>]</cite> and sense disambiguation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib749" title="" class="ltx_ref">749</a>]</cite>) and sentence-level tasks (sentence matching&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib750" title="" class="ltx_ref">750</a>]</cite> and sentiment classification&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib751" title="" class="ltx_ref">751</a>]</cite>) have been widely studied in the literature and applied in real-world platforms.
To solve these tasks, the key is to accurately understand the semantic information about the words or sentences.
As rich high-quality labeled data about these tasks has been accumulated so far, existing work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> finds that small language models can achieve very good performance by fine-tuning on it.
Recent studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib752" title="" class="ltx_ref">752</a>]</cite> have also tested the performance of LLMs on these tasks, showing that LLMs can also perform well via in-context learning (with very few examples).
Whereas, as small models can be specially optimized on these tasks to learn the specific task requirement and domain knowledge, full-data fine-tuned small models can mostly outperform LLMs using in-context learning on several classic tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib753" title="" class="ltx_ref">753</a>, <a href="#bib.bib754" title="" class="ltx_ref">754</a>]</cite>, <em id="S8.SS1.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> semantic matching and sentiment analysis.</p>
</div>
<div id="S8.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS1.p3.1" class="ltx_p"><span id="S8.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Sequence Tagging.</span>
The sequence tagging tasks, <em id="S8.SS1.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> named entity recognition&nbsp;(NER)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib755" title="" class="ltx_ref">755</a>]</cite> and part-of-speech&nbsp;(POS) tagging&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib756" title="" class="ltx_ref">756</a>]</cite>, are also fundamental tasks.
Typically, such tasks require assigning each token in the input sequence a proper semantic category label, <em id="S8.SS1.SSS1.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> the classic B-I-O (<em id="S8.SS1.SSS1.p3.1.4" class="ltx_emph ltx_font_italic">Beginning</em>, <em id="S8.SS1.SSS1.p3.1.5" class="ltx_emph ltx_font_italic">Inside</em> and <em id="S8.SS1.SSS1.p3.1.6" class="ltx_emph ltx_font_italic">Outside</em>) tagging scheme for NER tasks.
In the era of deep learning, early efforts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib757" title="" class="ltx_ref">757</a>, <a href="#bib.bib758" title="" class="ltx_ref">758</a>]</cite> mainly integrate the learned sequence representations (<em id="S8.SS1.SSS1.p3.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> using CNN, LSTM, and BERT) into the classic conditional random field model&nbsp;(CRF), which performs the tagging task based on structural prediction.
Recently, researchers have tested the performance of LLMs in sequence tagging tasks, but observed that LLMs still face challenges in solving them using in-context learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib753" title="" class="ltx_ref">753</a>]</cite>, especially for special categories with ambiguous or rare names, <em id="S8.SS1.SSS1.p3.1.8" class="ltx_emph ltx_font_italic">e.g.,</em> the “MISC” (<em id="S8.SS1.SSS1.p3.1.9" class="ltx_emph ltx_font_italic">miscellaneous entity</em>) and “ORG” (<em id="S8.SS1.SSS1.p3.1.10" class="ltx_emph ltx_font_italic">organization</em>) classes.
A possible reason is that LLMs may misunderstand the meanings of these classes in the human-annotated dataset, making it difficult to accurately understand their semantics according to the instruction and limited examples in the context.</p>
</div>
<div id="S8.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS1.p4.1" class="ltx_p"><span id="S8.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Information Extraction.</span>
The information extraction task focuses on automatically extracting useful structured information from unstructured text data, such as relation extraction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib759" title="" class="ltx_ref">759</a>]</cite> and event extraction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib760" title="" class="ltx_ref">760</a>]</cite>, which is also a crucial task relating to many NLP applications.
Typically, previous studies formulate this task as a text classification task or a sequential labeling task.
As information extraction often needs to accurately understand and process complex semantic relations (multiple relations within one sentence), in-context learning with LLMs typically underperform state-of-the-art full-data fine-tuning methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib761" title="" class="ltx_ref">761</a>, <a href="#bib.bib762" title="" class="ltx_ref">762</a>]</cite>.
Whereas, it is shown that enabling collaboration between LLMs and small models can further boost the performance of specific tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib763" title="" class="ltx_ref">763</a>, <a href="#bib.bib762" title="" class="ltx_ref">762</a>]</cite>.
In addition, a recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib425" title="" class="ltx_ref">425</a>]</cite> also reveals that LLMs can achieve competitive zero-shot performance for information extraction with a two-stage workflow, making this approach attractive in future applications.</p>
</div>
<div id="S8.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS1.p5.1" class="ltx_p"><span id="S8.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Text Generation.</span>
Text generation tasks, <em id="S8.SS1.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> machine translation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib624" title="" class="ltx_ref">624</a>]</cite> and automatic summarization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib548" title="" class="ltx_ref">548</a>]</cite>, are long-standing NLP tasks that have been widely studied, and there have been a number of deployed products and systems based on fine-tuned small models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib311" title="" class="ltx_ref">311</a>, <a href="#bib.bib764" title="" class="ltx_ref">764</a>]</cite>.
Since the pre-training of LLMs is established on text prediction, they exhibit strong language generation abilities as commercial products&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib627" title="" class="ltx_ref">627</a>]</cite> and humans&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib628" title="" class="ltx_ref">628</a>]</cite>, with the help of proper prompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib765" title="" class="ltx_ref">765</a>, <a href="#bib.bib766" title="" class="ltx_ref">766</a>]</cite>.
Additionally, LLMs are flexible to effectively handle special requirement in real-world application scenarios, <em id="S8.SS1.SSS1.p5.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> document-level translation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib767" title="" class="ltx_ref">767</a>]</cite>, and also enable natural language interaction with users to further improve the generation quality&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib768" title="" class="ltx_ref">768</a>]</cite>.
Despite the above success, recent work also reveals that LLMs are hard to well address the generation tasks about low-resource languages and domains, <em id="S8.SS1.SSS1.p5.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> Marathi-to-English translation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib769" title="" class="ltx_ref">769</a>]</cite>, due to their unbalanced training data across different languages.</p>
</div>
<div id="S8.SS1.SSS1.p6" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS1.p6.1" class="ltx_p"><span id="S8.SS1.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Summary</span>.
Based on the above discussion, we summarize the suggestions, and future direction about the use of LLMs in classic NLP tasks as follows:</p>
</div>
<div id="S8.SS1.SSS1.p7" class="ltx_para">
<p id="S8.SS1.SSS1.p7.1" class="ltx_p"><math id="S8.SS1.SSS1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S8.SS1.SSS1.p7.1.m1.1a"><mo id="S8.SS1.SSS1.p7.1.m1.1.1" xref="S8.SS1.SSS1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S8.SS1.SSS1.p7.1.m1.1b"><ci id="S8.SS1.SSS1.p7.1.m1.1.1.cmml" xref="S8.SS1.SSS1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.SSS1.p7.1.m1.1c">\bullet</annotation></semantics></math> <em id="S8.SS1.SSS1.p7.1.1" class="ltx_emph ltx_font_italic">Suggestions:</em>
LLMs and small models have their own merits in different aspects: LLMs are can provide unified solutions to various NLP tasks and achieve competitive performance (especially in the zero/few-shot setting), while small models are economical to develop and can be specially tuned according to target tasks, which can achieve good performance with sufficient high-quality labeled data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib753" title="" class="ltx_ref">753</a>, <a href="#bib.bib754" title="" class="ltx_ref">754</a>, <a href="#bib.bib770" title="" class="ltx_ref">770</a>, <a href="#bib.bib771" title="" class="ltx_ref">771</a>]</cite>.
In applications, one can make suitable choices based on the actual needs, comprehensively considering flexibility, data availability, training compute, and efficiency.</p>
</div>
<div id="S8.SS1.SSS1.p8" class="ltx_para">
<p id="S8.SS1.SSS1.p8.1" class="ltx_p"><math id="S8.SS1.SSS1.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S8.SS1.SSS1.p8.1.m1.1a"><mo id="S8.SS1.SSS1.p8.1.m1.1.1" xref="S8.SS1.SSS1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S8.SS1.SSS1.p8.1.m1.1b"><ci id="S8.SS1.SSS1.p8.1.m1.1.1.cmml" xref="S8.SS1.SSS1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.SSS1.p8.1.m1.1c">\bullet</annotation></semantics></math> <em id="S8.SS1.SSS1.p8.1.1" class="ltx_emph ltx_font_italic">Future direction:</em>
Despite the excellent general capacities, LLMs still cannot effectively process the NLP tasks in low-resource domains, <em id="S8.SS1.SSS1.p8.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> minor language translation.
To tackle such tasks, it needs to develop effective approaches to injecting necessary task information or domain-specific knowledge into LLMs, either through fine-tuning or prompting. In addition, it is still challenging for LLMs to handle complex semantic relations in classic NLP tasks (<em id="S8.SS1.SSS1.p8.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> nested entity extraction), which is worth more exploration from the underlying working mechanism of LLMs.

It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib772" title="" class="ltx_ref">772</a>]</cite>.
Another promising direction is to conduct human-machine collaborative research (<em id="S8.SS1.SSS1.p8.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> conversational translation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib768" title="" class="ltx_ref">768</a>]</cite>) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses.</p>
</div>
</section>
<section id="S8.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.2 </span>LLM for Information Retrieval</h4>

<div id="S8.SS1.SSS2.p1" class="ltx_para">
<p id="S8.SS1.SSS2.p1.1" class="ltx_p">The goal of information retrieval&nbsp;(IR) systems is to assist users in discovering ideal information resources&nbsp;(typically documents) and mitigating the information overload issue.
Typically, contemporary IR systems adopt a retrieve-then-rerank pipeline framework&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Within this framework, the retriever initially retrieves relevant information from a large-scale corpus, and the reranker subsequently performs multi-stage ranking procedure to acquire the most relevant information&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib773" title="" class="ltx_ref">773</a>]</cite>.
Since the advent of LLMs has significant impact on the way of information access, we discuss how it advances the development of IR from two main aspects, namely LLMs as IR models and LLM-enhanced IR models.</p>
</div>
<div id="S8.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS2.p2.1" class="ltx_p"><span id="S8.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">LLMs as IR Models.</span>
Existing IR models can be overall categorized into <em id="S8.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">sparse models</em> (relying on term-based lexical similarity) and <em id="S8.SS1.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">dense models</em> (relying on embedding based semantic similarity)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib740" title="" class="ltx_ref">740</a>]</cite>. Specially, dense models are mainly implemented by fine-tuned PLMs (<em id="S8.SS1.SSS2.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> BERT).
Compared to PLMs, LLMs have more strong model capacities in capturing text semantics, thus having the potential to improve existing dense IR models.
However, due to the high overhead of LLMs, the majority of studies concentrate on employing LLMs as rerankers, aiming to refine the ranking of retrieved candidates. To achieve this, recent efforts often formulate special instructions that enable LLMs to perform reranking on a small set of provided candidate documents. Typically, such an approach does not necessitate model training, and achieve promising results compared with well-trained reranking methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib774" title="" class="ltx_ref">774</a>, <a href="#bib.bib775" title="" class="ltx_ref">775</a>]</cite>.
Specially, the LLM-based reranking approach can be implemented in different ways by zero-shot or few-shot instruction, including pointwise (<em id="S8.SS1.SSS2.p2.1.5" class="ltx_emph ltx_font_italic">estimating the relevance scores for query-document pairs</em>)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib776" title="" class="ltx_ref">776</a>]</cite>, pairwise (<em id="S8.SS1.SSS2.p2.1.6" class="ltx_emph ltx_font_italic">determining the relevance order of two documents</em>)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib775" title="" class="ltx_ref">775</a>]</cite>, or listwise ranking (<em id="S8.SS1.SSS2.p2.1.7" class="ltx_emph ltx_font_italic">sorting a subset of candidate documents</em>)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib777" title="" class="ltx_ref">777</a>]</cite>.
The essence of these methods lies in the special design of instructions for text reranking, such as sliding window strategy for document lists&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib774" title="" class="ltx_ref">774</a>, <a href="#bib.bib778" title="" class="ltx_ref">778</a>]</cite>, setwise selection prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib779" title="" class="ltx_ref">779</a>]</cite>, fine-grained relevance labels incorporation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib780" title="" class="ltx_ref">780</a>]</cite>, and pairwise comparison prompting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib775" title="" class="ltx_ref">775</a>]</cite>.
In addition, recent efforts employ LLMs to generate intermediate texts (<em id="S8.SS1.SSS2.p2.1.8" class="ltx_emph ltx_font_italic">e.g.,</em> URLs) as retrieval results using few-shot demonstrations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib781" title="" class="ltx_ref">781</a>]</cite>.
To further enhance the model performance, LLMs can be specially fine-tuned as backbones for reranking&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib782" title="" class="ltx_ref">782</a>, <a href="#bib.bib783" title="" class="ltx_ref">783</a>]</cite> or retrieval&nbsp;(including dense retrieval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> and model-based retrieval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib784" title="" class="ltx_ref">784</a>, <a href="#bib.bib785" title="" class="ltx_ref">785</a>]</cite>), similar to the fine-tuning process for traditional PLM-based IR models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib782" title="" class="ltx_ref">782</a>]</cite>. However, fine-tuning LLMs as IR models entails considerable expenses given the huge parameter scale of LLMs.</p>
</div>
<div id="S8.SS1.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS2.p3.1" class="ltx_p"><span id="S8.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">LLM-Enhanced IR Models.</span>
As another major research direction, LLMs can be employed to improve existing IR models (<em id="S8.SS1.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> small models).
A common challenge faced by existing IR models is the lack of relevant judgment annotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib786" title="" class="ltx_ref">786</a>, <a href="#bib.bib787" title="" class="ltx_ref">787</a>]</cite>.
To tackle this problem, LLMs can be instructed to annotate positive or negative documents for a given query&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib788" title="" class="ltx_ref">788</a>]</cite>, or to generate corresponding queries based on a set of documents in the corpus by referring to a few demonstrations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib789" title="" class="ltx_ref">789</a>, <a href="#bib.bib790" title="" class="ltx_ref">790</a>]</cite>.
In addition to training data augmentation, LLM has the potential to improve existing IR models by refining the search-oriented informativeness of both queries and documents.
In IR systems, the input queries may be constrained by a user’s cognitive and cultural competency, making it challenging to accurately express the real intent, and irrelevant content present in documents can also impact the relevance evaluation with the query.
As a solution, LLM can be utilized to rewrite the query for enhancing the understanding of the query intent and incorporating additional knowledge into the query through well-designed instructions. The rewritten query can take the form of an improved version of the original query&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib791" title="" class="ltx_ref">791</a>]</cite>, a document in the corpus that related to the query&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib792" title="" class="ltx_ref">792</a>]</cite>, or an expansion of the query that concatenated with a pseudo generated document&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib793" title="" class="ltx_ref">793</a>]</cite>.
In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib794" title="" class="ltx_ref">794</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS2.p4.1" class="ltx_p"><span id="S8.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Remaining Issues.</span>

In this part, we further discuss several important issues to apply LLMs to improve IR systems.
First, though LLMs are capable of being as general-purpose task solvers, they are not directly well suited for existing IR systems: they require high overhead for inference&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib774" title="" class="ltx_ref">774</a>, <a href="#bib.bib782" title="" class="ltx_ref">782</a>]</cite>, have limitations in modeling long texts or document lists&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib778" title="" class="ltx_ref">778</a>]</cite>, and need special adaptation (<em id="S8.SS1.SSS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> instruction tuning) to perform the text ranking task&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib795" title="" class="ltx_ref">795</a>]</cite>.
Therefore, more systematic approaches to adapt LLMs for modern IR systems should be investigated, to leverage their benefits and meanwhile overcome these limitations.
Secondly, the advent of LLMs sheds lights on the development of new information seeking ways (<em id="S8.SS1.SSS2.p4.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> New Bing).
It is meaningful to explore how to reshape the architecture and paradigm of IR by integrating the LLMs’ capacities and the merits of existing IR systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib796" title="" class="ltx_ref">796</a>]</cite>.
Thirdly, existing work mainly focuses on text retrieval tasks, lacking a comprehensive consideration of multimodal information sources.
As will be discussed in Section&nbsp;<a href="#S8.SS1.SSS4" title="8.1.4 Multimodal Large Language Model ‣ 8.1 LLM for Research Community ‣ 8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8.1.4</span></a>, multimodal large language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib797" title="" class="ltx_ref">797</a>]</cite>
are also widely studied, making it feasible to develop more powerful multimedia retrieval systems.</p>
</div>
</section>
<section id="S8.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.3 </span>LLM for Recommender Systems</h4>

<div id="S8.SS1.SSS3.p1" class="ltx_para">
<p id="S8.SS1.SSS3.p1.1" class="ltx_p">Unlike IR systems that analyze user search queries to retrieve relevant documents, recommender systems (RS) aim to capture the underlying user preference and provide appropriate information resources to users&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib798" title="" class="ltx_ref">798</a>, <a href="#bib.bib799" title="" class="ltx_ref">799</a>, <a href="#bib.bib800" title="" class="ltx_ref">800</a>, <a href="#bib.bib801" title="" class="ltx_ref">801</a>]</cite>.
Typically, existing studies train a recommendation model (either classic or deep learning model) by fitting it over the user’s logged data (<em id="S8.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> click data)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib802" title="" class="ltx_ref">802</a>, <a href="#bib.bib745" title="" class="ltx_ref">745</a>]</cite>.
However, these models often suffer from a series of technical issues, <em id="S8.SS1.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> cold-start recommendation, domain transfer, and poor explainability.
Recently, LLMs have demonstrated the potential to alleviate these issues of recommendation models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref">357</a>, <a href="#bib.bib803" title="" class="ltx_ref">803</a>, <a href="#bib.bib804" title="" class="ltx_ref">804</a>]</cite>, due to the strong capacities of domain generalization and language generation.
In this part, we briefly
review the recent progress of LLMs in recommender systems, from the following three aspects, namely LLMs as recommendation models, LLM-enhanced recommendation models, and LLMs as recommendation simulators.</p>
</div>
<div id="S8.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS3.p2.1" class="ltx_p"><span id="S8.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">LLMs as Recommendation Models.</span>

With specific methods or mechanisms, LLMs can be adapted to serve as recommendation models.
Existing work along this line can be generally divided into two main categories. First, some methods prompt LLMs for completing the recommendation task in a zero-shot paradigm (<em id="S8.SS1.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> without parameter tuning)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib805" title="" class="ltx_ref">805</a>, <a href="#bib.bib806" title="" class="ltx_ref">806</a>]</cite>. A series of prompt engineering methods like recency-focused and in-context learning are introduced to improve recommendation performance as well as alleviate the potential model biases&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib807" title="" class="ltx_ref">807</a>, <a href="#bib.bib808" title="" class="ltx_ref">808</a>]</cite>.
Second, another category of studies aim to specialize LLMs for personalized recommendation through instruction tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref">357</a>, <a href="#bib.bib809" title="" class="ltx_ref">809</a>]</cite>. Specially, high-quality instruction data is key to adapt LLMs to the recommendation tasks, which can be constructed based on user-item interactions with heuristic templates. To further improve the instruction diversity,
InstructRec&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref">357</a>]</cite> employs self-instruct technique to simulate large amounts of potential user instructions in various scenarios like product search and personalized recommendations. In addition to representing each item by its text description, there is also growing attention on extending LLM’s vocabulary with semantic identifiers in recommender systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib810" title="" class="ltx_ref">810</a>, <a href="#bib.bib811" title="" class="ltx_ref">811</a>]</cite>, to incorporate collaborative semantics into LLMs.</p>
</div>
<div id="S8.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS3.p3.1" class="ltx_p"><span id="S8.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_bold">LLM-enhanced Recommendation Models.</span>

In addition to instructing LLMs to directly provide recommendations, researchers also propose leveraging the universal knowledge encoded in LLMs to improve traditional recommender systems.
Existing approaches in this line can be divided into three main categories. The first category employs LLMs to infer users’ potential intention from their historical interaction data. Furthermore, traditional recommendation/search models employ the inferred intentions to improve the retrieval of relevant items&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib812" title="" class="ltx_ref">812</a>, <a href="#bib.bib813" title="" class="ltx_ref">813</a>]</cite>. Additionally, several studies explore the use of LLMs as feature encoders. They employ LLMs to encode the side information of items and users (<em id="S8.SS1.SSS3.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> item’s descriptions and user’s reviews), thus deriving more informative representations of users and items. These representations are then fed into traditional recommender systems as augmented input&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib814" title="" class="ltx_ref">814</a>, <a href="#bib.bib815" title="" class="ltx_ref">815</a>]</cite>. 
As another alternative approach,
several studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib816" title="" class="ltx_ref">816</a>, <a href="#bib.bib817" title="" class="ltx_ref">817</a>]</cite> adopt a distillation-like way to transfer LLM’s capacities (<em id="S8.SS1.SSS3.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> semantic encoding) to improve traditional recommenders (<em id="S8.SS1.SSS3.p3.1.4" class="ltx_emph ltx_font_italic">i.e.,</em> small models).
Specially, they align the hidden states of LLMs and traditional recommendation models via joint training. After training, since only the enhanced small model will be deployed online, it can avoid the huge overhead of LLMs in online service.</p>
</div>
<div id="S8.SS1.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS3.p4.1" class="ltx_p"><span id="S8.SS1.SSS3.p4.1.1" class="ltx_text ltx_font_bold">LLM as Recommendation Simulator.</span>
Inspired by the recent success of autonomous AI agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib818" title="" class="ltx_ref">818</a>]</cite>, LLMs have been also utilized to develop recommendation simulators&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib819" title="" class="ltx_ref">819</a>, <a href="#bib.bib820" title="" class="ltx_ref">820</a>]</cite> (exemplified by RecAgent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib819" title="" class="ltx_ref">819</a>]</cite>), showing great potential to simulate user real behaviors in recommender systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib819" title="" class="ltx_ref">819</a>, <a href="#bib.bib821" title="" class="ltx_ref">821</a>, <a href="#bib.bib822" title="" class="ltx_ref">822</a>]</cite>.
Specifically, to make personalized simulation, an agent will be equipped with a profiling module that encompasses relevant identity information. Then, a memory module is introduced to store agents’ past interaction experiences.
During the process of simulation, agents are further prompted to conduct self-reflection based on their past experiences, to capture their underlying user preference.
Most of existing recommendation simulators are conducted in a user-oriented way, without explicitly modeling the items in the interaction process.
To address this, AgentCF&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib821" title="" class="ltx_ref">821</a>]</cite> models both users and items as agents, and further facilitates collaborative reflections to simulate user-item interactions, so as to capturing the two-sided relations between users and items.</p>
</div>
<div id="S8.SS1.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS3.p5.1" class="ltx_p"><span id="S8.SS1.SSS3.p5.1.1" class="ltx_text ltx_font_bold">Remaining Issues.</span>

Despite these efforts, there are still several challenges to address when applying LLMs in recommender systems. First, existing studies have shown that LLM-based recommendation models in zero/few-shot settings tend to perform worse than traditional ID-based recommenders&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib807" title="" class="ltx_ref">807</a>, <a href="#bib.bib806" title="" class="ltx_ref">806</a>]</cite>. This indicates that LLMs might lack an understanding of personalized user behaviors and domain-specific collaborative semantics. Although instruction tuning alleviates this issue to some extent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib809" title="" class="ltx_ref">809</a>, <a href="#bib.bib357" title="" class="ltx_ref">357</a>]</cite>, it can’t fully reduce the semantic gap between LLMs and recommender systems, and also suffers from high tuning costs.
Furthermore, recommender systems prioritize minimizing inference latency to enhance users’ experience in low-resourced environments (<em id="S8.SS1.SSS3.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> phones), which poses a challenge to LLMs’ inference speed as well as memory overhead. Therefore, it is important to explore improvement techniques, such as efficient tuning and quantization methods, to deploy LLMs efficiently and effectively in real-world recommender systems. In addition, existing LLMs
have limited capacities in long context modeling, make it difficult to process the huge amount of user-item interaction data. Improved context length extension and context information utilization approaches should be developed to improve the modeling capacities of LLMs in long interaction sequences.</p>
</div>
</section>
<section id="S8.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.4 </span>Multimodal Large Language Model</h4>

<div id="S8.SS1.SSS4.p1" class="ltx_para">
<p id="S8.SS1.SSS4.p1.1" class="ltx_p">In existing literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib823" title="" class="ltx_ref">823</a>, <a href="#bib.bib824" title="" class="ltx_ref">824</a>]</cite>, multimodal models mainly refer to the models that can process and integrate information of various modalities (<em id="S8.SS1.SSS4.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> text, image, and audio) from input, and further produce corresponding output in certain modalities.
In this part, we mainly focus on the multimodal extension of LLMs by enabling the information modeling of non-textual modalities, especially the vision modality, called <em id="S8.SS1.SSS4.p1.1.2" class="ltx_emph ltx_font_italic">multimodal large language models&nbsp;(MLLMs)</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib797" title="" class="ltx_ref">797</a>]</cite><span id="footnote51" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">51</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">51</sup><span class="ltx_tag ltx_tag_note">51</span>In existing work, large vision language models&nbsp;(LVLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib662" title="" class="ltx_ref">662</a>]</cite> are also used to term such bimodal models that are developed based on LLMs. We use the naming of MLLMs in this part due to its wide use in existing literature. </span></span></span>.
To start our discussion, we specify the input to be text-image pairs and the output to be text responses. Similar discussions can be made for other modalities, <em id="S8.SS1.SSS4.p1.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> language-audio models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib825" title="" class="ltx_ref">825</a>]</cite>, which is beyond our scope here.
In essence, MLLMs are developed by adapting the information from other modalities to the text modality, so as to leverage the excellent model capacities of LLMs that are learned based on world text. Typically, a MLLM comprises an image encoder for image encoding and a LLM for text generation, associated by a connection module that aligns vision and language representations. During generation, the image is first split into patches, and then transformed into patch embeddings by the image encoder and the connection module, to derive a visual representation that can be understood by the LLM. Subsequently, the patch embeddings and text embeddings are concatenated, and fed into the MLLM, allowing the language model to generate the response autoregressively. In the following, we will discuss the training, evaluation, and key points to develop capable MLLMs.</p>
</div>
<div id="S8.SS1.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS4.p2.1" class="ltx_p"><span id="S8.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Training Process.</span>
The training process of the MLLM includes two major stages: vision-language alignment pre-training and visual instruction tuning.</p>
</div>
<div id="S8.SS1.SSS4.p3" class="ltx_para">
<p id="S8.SS1.SSS4.p3.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p3.1.1" class="ltx_emph ltx_font_italic">Vision-language alignment pre-training.</em>
To develop MLLMs, existing work mostly initializes the vision encoder and the LLM with pre-trained models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib826" title="" class="ltx_ref">826</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite>. These models retain excellent vision and language capacities, but span different semantic spaces. Thus, the goal of vision-language alignment pre-training (<em id="S8.SS1.SSS4.p3.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> the first-stage training) is to align the vision encoder and the LLM through end-to-end training on large-scale image-text pairs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib827" title="" class="ltx_ref">827</a>, <a href="#bib.bib828" title="" class="ltx_ref">828</a>]</cite>. However, directly tuning these two models on image-text pairs may cause the degradation of the original representation capacities. To improve the alignment performance, it is crucial to design effective training strategies and select appropriate pre-training data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib829" title="" class="ltx_ref">829</a>, <a href="#bib.bib830" title="" class="ltx_ref">830</a>]</cite>.
Existing work mainly employs the following strategies for cross-modality alignment: (1) if the number of image-text pairs is not sufficiently large&nbsp;(<em id="S8.SS1.SSS4.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> less than 1M), it is often suggested to only update the connection module&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib831" title="" class="ltx_ref">831</a>]</cite>; (2) if the training data includes high-quality text corpora&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib832" title="" class="ltx_ref">832</a>]</cite> or image-text pairs with fine-grained annotations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib833" title="" class="ltx_ref">833</a>]</cite>, fine-tuning the LLM can be conducted to boost the performance; (3) if the number of image-text pairs is very large&nbsp;(<em id="S8.SS1.SSS4.p3.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> about 1B), fine-tuning the vision encoder is also plausible&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib829" title="" class="ltx_ref">829</a>, <a href="#bib.bib830" title="" class="ltx_ref">830</a>]</cite>, but the benefit remains further verification.</p>
</div>
<div id="S8.SS1.SSS4.p4" class="ltx_para">
<p id="S8.SS1.SSS4.p4.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p4.1.1" class="ltx_emph ltx_font_italic">Visual instruction tuning.</em> After vision-language pre-training, the second-stage training, <em id="S8.SS1.SSS4.p4.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> visual instruction tuning, aims to improve the instruction-following and task-solving abilities of MLLMs. Generally, the input of visual instruction tuning consists of an image and a task description, and the task is to generate a corresponding text output.
To boost the performance, high-quality visual instruction data is key to eliciting and enhancing the abilities of MLLMs.
Therefore, most studies are dedicated to constructing various visual instruction datasets. As the basic approaches, early studies construct visual instructions by distilling from GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> or reformulating vision-language task datasets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>. To enhance the quality of instruction data, recent work further proposes improved strategies by increasing the instruction diversity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib834" title="" class="ltx_ref">834</a>]</cite>, incorporating fine-grained information&nbsp;(<em id="S8.SS1.SSS4.p4.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> coordinate of objects) into the instruction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib833" title="" class="ltx_ref">833</a>]</cite>, or synthesizing complex visual reasoning instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib835" title="" class="ltx_ref">835</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS4.p5" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS4.p5.1" class="ltx_p"><span id="S8.SS1.SSS4.p5.1.1" class="ltx_text ltx_font_bold">Evaluation of MLLM.</span> After introducing the approaches to developing MLLMs, we further discuss how to effectively assess the multimodal capabilities of MLLMs from the following three aspects.</p>
</div>
<div id="S8.SS1.SSS4.p6" class="ltx_para">
<p id="S8.SS1.SSS4.p6.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p6.1.1" class="ltx_emph ltx_font_italic">Evaluation perspectives.</em>
The evaluation tasks for MLLMs can be categorized into two main types: <em id="S8.SS1.SSS4.p6.1.2" class="ltx_emph ltx_font_italic">perception</em> and <em id="S8.SS1.SSS4.p6.1.3" class="ltx_emph ltx_font_italic">cognition</em> tasks. Specifically, <em id="S8.SS1.SSS4.p6.1.4" class="ltx_emph ltx_font_italic">perception</em> tasks aim to assess the model’s abilities in understanding the basic semantics of the image content, while <em id="S8.SS1.SSS4.p6.1.5" class="ltx_emph ltx_font_italic">cognition</em> tasks evaluate models with more complex tasks that require reasoning based on perception results.
The perception ability is typically evaluated through classification tasks about attributes of image (<em id="S8.SS1.SSS4.p6.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> topic and style) and object (<em id="S8.SS1.SSS4.p6.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> existence and color) or OCR-related tasks, based on existing datasets or new datasets derived from existing images with annotations by humans or LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib836" title="" class="ltx_ref">836</a>, <a href="#bib.bib837" title="" class="ltx_ref">837</a>, <a href="#bib.bib838" title="" class="ltx_ref">838</a>, <a href="#bib.bib839" title="" class="ltx_ref">839</a>]</cite>.
A notable perception issue is hallucination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib840" title="" class="ltx_ref">840</a>]</cite>, where the model’s responses contain inconsistent content with the image.
Among existing studies about hallucination in MLLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib834" title="" class="ltx_ref">834</a>, <a href="#bib.bib841" title="" class="ltx_ref">841</a>, <a href="#bib.bib842" title="" class="ltx_ref">842</a>]</cite>, object hallucination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib843" title="" class="ltx_ref">843</a>]</cite> has received much research attention.
To conduct a stable, robust evaluation of object hallucination, POPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib844" title="" class="ltx_ref">844</a>]</cite> proposes a polling-based object probing approach for converting object recognition into a series of binary questions, and the results indicate that current MLLMs often struggle with object hallucination.
Cognition tasks, on the other hand, require MLLMs to perform reasoning based on image perception. A common reasoning task is visual question answering (VQA), where models answer questions about images that demand reasoning about spatial relationships&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib845" title="" class="ltx_ref">845</a>]</cite>, general knowledge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib846" title="" class="ltx_ref">846</a>]</cite>, or scene text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib847" title="" class="ltx_ref">847</a>]</cite>. To fully explore the capabilities of MLLMs, HallusionBench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib848" title="" class="ltx_ref">848</a>]</cite> collects 200 sophisticated visual dependent or supplement questions, on which even the most advanced MLLMs like LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib831" title="" class="ltx_ref">831</a>]</cite> and GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> fail to achieve good performance.</p>
</div>
<div id="S8.SS1.SSS4.p7" class="ltx_para">
<p id="S8.SS1.SSS4.p7.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p7.1.1" class="ltx_emph ltx_font_italic">Evaluation paradigms.</em>
The responses of MLLMs can be evaluated either in a closed-ended or an open-ended manner.
Traditional multimodal tasks often rely on a closed-ended evaluation framework, where the assessment is based on the exact match between the model’s response and the ground-truth answer. Examples include the VQA score&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib849" title="" class="ltx_ref">849</a>]</cite> for visual question answering tasks and the CIDEr&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib850" title="" class="ltx_ref">850</a>]</cite> score for captioning tasks. However, MLLMs generate responses in an open-ended way, which may contain the correct answer but not exactly match the ground-truth perfectly.
This discrepancy can lead to the underestimation of the model’s performance in previous evaluation paradigms. To address this issue, recent approaches have incorporated humans or LLMs as evaluators&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib829" title="" class="ltx_ref">829</a>]</cite>. For instance, MMBench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib838" title="" class="ltx_ref">838</a>]</cite> employs ChatGPT to align the model responses with the most relevant option in a set of multiple-choice questions. Similarly, LLaVA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib851" title="" class="ltx_ref">851</a>]</cite> utilizes GPT-4 for evaluating MLLMs’ output, where GPT-4 takes the generated image captions and object bounding boxes as visual inputs for assessment. Such open-ended evaluation methods can improve assessment accuracy while incurring higher costs due to the involvement of humans or LLMs.</p>
</div>
<div id="S8.SS1.SSS4.p8" class="ltx_para">
<p id="S8.SS1.SSS4.p8.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p8.1.1" class="ltx_emph ltx_font_italic">Evaluation benchmarks.</em> To facilitate a more thorough evaluation of MLLMs, various benchmarks have been developed. Part of them collect existing vision-language tasks for comprehensive evaluation. For instance, LVLM-eHub&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib852" title="" class="ltx_ref">852</a>]</cite> aggregates 47 existing text-related visual tasks to assess six distinct capabilities of MLLMs, and Reform-Eval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib853" title="" class="ltx_ref">853</a>]</cite> takes this a step further by standardizing questions from existing benchmarks into a uniform format and discusses how the backbone models influence MLLMs’ performance. In addition to incorporating existing tasks, several work also derives new questions annotated by humans or with the help of LLMs. MME&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib839" title="" class="ltx_ref">839</a>]</cite> creates a dataset by pairing images from public sources with manually-collected text instructions for perception and cognition evaluations. MMBench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib838" title="" class="ltx_ref">838</a>]</cite> transforms these instructions into multiple-choice questions and introduces CircularEval to ensure evaluation consistency. SEED-Bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib854" title="" class="ltx_ref">854</a>]</cite> further considers temporal understanding tasks and enlarges the evaluation scale to 19K multiple-choice questions with the assistance of LLMs. MM-Vet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib855" title="" class="ltx_ref">855</a>]</cite> presents more complex tasks to assess the integrated multimodal capabilities of MLLMs. It starts by defining six essential multimodal abilities and then creates intricate questions by combining multiple abilities. In summary, the above benchmarks collectively contribute to the comprehensive evaluation and improved development of MLLMs.</p>
</div>
<div id="S8.SS1.SSS4.p9" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS4.p9.1" class="ltx_p"><span id="S8.SS1.SSS4.p9.1.1" class="ltx_text ltx_font_bold">Key Points for Improving MLLMs.</span>
To develop capable MLLMs, we continue to discuss three key points to improve the model capacities, from the perspectives of instruction data, training strategy, and safety and alignment.</p>
</div>
<div id="S8.SS1.SSS4.p10" class="ltx_para">
<p id="S8.SS1.SSS4.p10.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p10.1.1" class="ltx_emph ltx_font_italic">Visual instruction data</em>.
Extensive work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib831" title="" class="ltx_ref">831</a>, <a href="#bib.bib856" title="" class="ltx_ref">856</a>]</cite> has empirically found that both quantity and quality of visual instructions have an important impact on model performance of MLLMs. One basic way to construct visual instructions is to leverage the exceptional capability of LLMs to synthesize instructions based on text descriptions of images&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib851" title="" class="ltx_ref">851</a>]</cite>. To further enhance the quality of instructions, one can
construct fine-grained visual instructions with the help of human annotation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib833" title="" class="ltx_ref">833</a>, <a href="#bib.bib857" title="" class="ltx_ref">857</a>]</cite> or synthesize more complex data through carefully-designed prompts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib835" title="" class="ltx_ref">835</a>]</cite>.
Despite the effectiveness of the above LLM-based approaches,
one primary question emerges as to whether a LLM (<em id="S8.SS1.SSS4.p10.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> text generation model without training on any images) possesses the ability to generate sufficiently good visual instructions solely based on verbalized visual information&nbsp;(<em id="S8.SS1.SSS4.p10.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> captions and coordinates). Specially, existing work has also revealed that visual instructions generated by LLMs sometimes contain misinterpretations about the visual information, <em id="S8.SS1.SSS4.p10.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> object hallucination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib844" title="" class="ltx_ref">844</a>]</cite>. Therefore, it is crucial to design effective verification methods to control the quality of instruction data generated by LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib835" title="" class="ltx_ref">835</a>]</cite>. Furthermore, it still needs more investigation about what makes good visual instructions and how visual instructions elicit specific multimodal abilities in MLLMs.</p>
</div>
<div id="S8.SS1.SSS4.p11" class="ltx_para">
<p id="S8.SS1.SSS4.p11.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p11.1.1" class="ltx_emph ltx_font_italic">Model training.</em> Different from LLMs, MLLMs are not trained from scratch, but instead developed based on pre-trained language and vision models. Existing work employs a typical two-stage approach for training MLLMs, <em id="S8.SS1.SSS4.p11.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> vision-language alignment pre-training and visual instruction tuning.
In essence, existing MLLMs aim to (1) preserve the inherent capabilities and parametric knowledge of LLMs as possible, and meanwhile (2) effectively adapt to multimodal tasks by leveraging the pre-trained LLMs and visual encoders.
To achieve the above two goals, two typical training strategies are often employed for visual instruction tuning, either only optimizing the connection module&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> or fine-tuning both the connector module and LLM component&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib851" title="" class="ltx_ref">851</a>]</cite>.
As we can see, the former can reserve the original capacities of LLMs but likely have a weak an adaptation performance, while the latter can fully adapt to multimodal tasks but suffer from the loss of original capacities of LLMs.
More efforts should be made to investigate how to effectively balance the two aspects, so as to achieving improved multimodal capacities.
In addition, existing MLLMs are still overly dependent on the capacities of LLMs, which pose the limits on many multimodal tasks (<em id="S8.SS1.SSS4.p11.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> space positioning).
It will be meaningful to explore improved training approaches of language models, so that multimodal information can be also utilized in this process.</p>
</div>
<div id="S8.SS1.SSS4.p12" class="ltx_para">
<p id="S8.SS1.SSS4.p12.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS4.p12.1.1" class="ltx_emph ltx_font_italic">Safety and alignment.</em>

Safety and alignment has been widely discussed in LLMs, which aim to regulate the behaviors of models by technical approaches&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. This topic is also important to MLLMs.
Even a highly advanced MLLM&nbsp;(<em id="S8.SS1.SSS4.p12.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>) can be susceptible to safety issues. For example, GPT-4V might occasionally exhibit factual inaccuracies and baseless inferences about images. In some cases, it may even generate harmful content targeting specific individuals or groups&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>. Furthermore, open-sourced MLLMs are also prone to generate hallucinated response&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib844" title="" class="ltx_ref">844</a>]</cite> and can be easily manipulated to produce harmful content&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib858" title="" class="ltx_ref">858</a>]</cite>. To address the aforementioned issues, some studies collect specialized visual instructions to mitigate the problem of hallucination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib834" title="" class="ltx_ref">834</a>]</cite>. Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib859" title="" class="ltx_ref">859</a>]</cite>. Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib860" title="" class="ltx_ref">860</a>]</cite>. Despite these efforts, existing alignment techniques for MLLMs mainly concentrate on several specific aspects&nbsp;(<em id="S8.SS1.SSS4.p12.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> hallucination), lacking a comprehensive consideration of alignment criteria.
More efforts should be made to promote the research of safety and alignment for MLLMs.</p>
</div>
</section>
<section id="S8.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.5 </span>KG-Enhanced LLM</h4>

<div id="S8.SS1.SSS5.p1" class="ltx_para">
<p id="S8.SS1.SSS5.p1.2" class="ltx_p">Despite the excellent capacities, LLMs often suffer from challenges on knowledge-intensive tasks, such as
the potential to generate hallucinated content&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib602" title="" class="ltx_ref">602</a>]</cite> and the lack of domain-specific knowledge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib861" title="" class="ltx_ref">861</a>]</cite>.
As a promising solution, knowledge graphs (KGs), which store enormous knowledge in the triple format, <em id="S8.SS1.SSS5.p1.2.2" class="ltx_emph ltx_font_italic">i.e.,</em> <math id="S8.SS1.SSS5.p1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S8.SS1.SSS5.p1.1.m1.1a"><mo stretchy="false" id="S8.SS1.SSS5.p1.1.m1.1.1" xref="S8.SS1.SSS5.p1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S8.SS1.SSS5.p1.1.m1.1b"><ci id="S8.SS1.SSS5.p1.1.m1.1.1.cmml" xref="S8.SS1.SSS5.p1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.SSS5.p1.1.m1.1c">\langle</annotation></semantics></math><span id="S8.SS1.SSS5.p1.2.1" class="ltx_text ltx_font_italic"> head_entity, relation, tail_entity <math id="S8.SS1.SSS5.p1.2.1.m1.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S8.SS1.SSS5.p1.2.1.m1.1a"><mo stretchy="false" id="S8.SS1.SSS5.p1.2.1.m1.1.1" xref="S8.SS1.SSS5.p1.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S8.SS1.SSS5.p1.2.1.m1.1b"><ci id="S8.SS1.SSS5.p1.2.1.m1.1.1.cmml" xref="S8.SS1.SSS5.p1.2.1.m1.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.SSS5.p1.2.1.m1.1c">\rangle</annotation></semantics></math></span>, can be utilized to enhance the task performance of LLMs by providing precise and necessary knowledge.
Generally, knowledge enhanced approaches can be expanded into other forms of structured data (<em id="S8.SS1.SSS5.p1.2.3" class="ltx_emph ltx_font_italic">e.g.,</em> tables and databases)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib862" title="" class="ltx_ref">862</a>]</cite>, while we limit our discussion to the integration of KG for improving LLMs, which are detailed in two aspects, namely retrieval-augmented LLM and synergy-augmented LLM.</p>
</div>
<div id="S8.SS1.SSS5.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS5.p2.1" class="ltx_p"><span id="S8.SS1.SSS5.p2.1.1" class="ltx_text ltx_font_bold">Retrieval-Augmented LLM.</span>

Due to the huge amount of fact records in a KG, existing work typically adopts a retrieval model to first obtain a relatively small subgraph from KG, and then leverages it to enhance LLMs by enriching the relevant knowledge.
Before the advent of LLMs, the retrieved subgraphs are often supplemented into training data, injecting knowledge information into PLMs via parameter learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib863" title="" class="ltx_ref">863</a>, <a href="#bib.bib864" title="" class="ltx_ref">864</a>, <a href="#bib.bib865" title="" class="ltx_ref">865</a>]</cite>.
In contrast, to leverage the retrieved knowledge, LLMs mainly incorporate it as part of the prompt, without parameter update.
To implement this approach, there are two main technical problems, <em id="S8.SS1.SSS5.p2.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> how to retrieve relevant knowledge from KGs and how to make better use of the structured data by LLMs.
For the first issue (<em id="S8.SS1.SSS5.p2.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> retrieving relevant knowledge),
a typical approach is to train a small language model (<em id="S8.SS1.SSS5.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> RoBERTa) to identify question-related fact triples&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib866" title="" class="ltx_ref">866</a>]</cite>.
To further improve the retrieval performance, several studies also propose an iterative reading-then-reasoning framework, enabling the LLM to interact with the KG multiple times and acquire the required knowledge in a more accurate way&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref">458</a>]</cite>.
For the second issue (<em id="S8.SS1.SSS5.p2.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> utilizing retrieved knowledge), a straightforward approach is to serialize the retrieved subgraph and craft specific prompts to include it as the input of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib651" title="" class="ltx_ref">651</a>, <a href="#bib.bib471" title="" class="ltx_ref">471</a>]</cite>.
However, due to the loss of structured information in knowledge serialization, LLMs cannot fully capture the structural semantics conveyed by original KGs.
To address this issue, several model-based approaches train a specialized language model (<em id="S8.SS1.SSS5.p2.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> T5) to transform the subgraph into the natural language text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib867" title="" class="ltx_ref">867</a>]</cite>.
To guarantee the transformation accuracy, it relies on sufficient training pairs (often unsupervised constructed)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib868" title="" class="ltx_ref">868</a>]</cite> and excellent model capability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib869" title="" class="ltx_ref">869</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS5.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS5.p3.1" class="ltx_p"><span id="S8.SS1.SSS5.p3.1.1" class="ltx_text ltx_font_bold">Synergy-Augmented LLM.</span>

To solve complex tasks (<em id="S8.SS1.SSS5.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> multi-hop question answering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib656" title="" class="ltx_ref">656</a>]</cite>), it often requires LLMs to query a KG multiple times, following a systematic solution plan.
We call such a multi-turn interaction approach to enhancing LLM <em id="S8.SS1.SSS5.p3.1.3" class="ltx_emph ltx_font_italic">synergy-augmented LLM</em>.
To better synergize the LLM and KG in a complementary manner, recent studies propose to decompose the complex task into multiple sub-goals and iteratively solve each one by leveraging the necessary knowledge from KG&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref">458</a>, <a href="#bib.bib870" title="" class="ltx_ref">870</a>, <a href="#bib.bib871" title="" class="ltx_ref">871</a>]</cite>.
In this process, the LLM can be regarded as an autonomous agent&nbsp;(detailed in Section&nbsp;<a href="#S8.SS1.SSS6" title="8.1.6 LLM-based Agent ‣ 8.1 LLM for Research Community ‣ 8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8.1.6</span></a>), which automatically generates the plan and executes it through interaction with the KG environment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib870" title="" class="ltx_ref">870</a>]</cite>.
Specially, the mainstream approaches typically start by enumerating the candidates using the available knowledge information at the current step, and then retrieve the most appropriate candidates for the next step according to the question&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib870" title="" class="ltx_ref">870</a>, <a href="#bib.bib871" title="" class="ltx_ref">871</a>]</cite>.
By iterating the above two steps, LLMs can gradually collect relevant evidence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib870" title="" class="ltx_ref">870</a>, <a href="#bib.bib871" title="" class="ltx_ref">871</a>]</cite>, and finally approach the correct solution. 
Despite the effectiveness, enumeration of the candidates over the KG would lead to a vast search space&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib872" title="" class="ltx_ref">872</a>]</cite>.
To address it, StructGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref">458</a>]</cite> proposes a more efficient way to access knowledge information using the specialized interfaces for KGs.
Specifically, it carefully designs the specialized interfaces according to the common data operations on KG (<em id="S8.SS1.SSS5.p3.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> relation extraction and triple extraction), to ensure efficient and accurate data extraction.
In this way, LLMs can be instructed to better manipulate and process the structural information of KGs, thus achieving improved task performance.</p>
</div>
<div id="S8.SS1.SSS5.p4" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS5.p4.1" class="ltx_p"><span id="S8.SS1.SSS5.p4.1.1" class="ltx_text ltx_font_bold">Future Directions.</span>

Besides the above approaches, there are several promising directions for KG-enhanced LLM remaining underexplored.

First, due to the variety of structured data, it is still difficult for LLMs to directly leverage various kinds of knowledge sources, <em id="S8.SS1.SSS5.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> domain-specific KGs.
Therefore, it is essential to explore the unified way to manipulate and utilize different knowledge sources by LLMs.
As a potential solution, it is promising to develop effective approaches to help LLMs comprehend and make use of the access interfaces provided by specific knowledge sources to acquire precise knowledge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref">458</a>]</cite>, while more efforts should be made to investigate how to adapt to the data variety in a cost-effective way.

Second, with the evolution of real-world information, the knowledge stored in LLMs may become outdated or incorrect.
It is necessary to explore how to synchronize the updated knowledge into LLMs through a cost-effective manner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib873" title="" class="ltx_ref">873</a>, <a href="#bib.bib874" title="" class="ltx_ref">874</a>]</cite>.
Third, it is promising to investigate the use of factual information from KG to align LLMs in generating more faithful content&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib875" title="" class="ltx_ref">875</a>, <a href="#bib.bib876" title="" class="ltx_ref">876</a>]</cite>, which can help reduce the hallucination of LLMs.</p>
</div>
<div id="S8.SS1.SSS5.p5" class="ltx_para">
<p id="S8.SS1.SSS5.p5.1" class="ltx_p">In addition to exploring KG-enhanced LLMs, it is also meaningful to leverage LLMs to improve the tasks on the KG side (<em id="S8.SS1.SSS5.p5.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> LLM4KG)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib861" title="" class="ltx_ref">861</a>, <a href="#bib.bib877" title="" class="ltx_ref">877</a>]</cite>.
A typical example is that LLMs can help supplement or construct the KG.
We omit the discussion of this part, since it is beyond our scope.</p>
</div>
</section>
<section id="S8.SS1.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.6 </span>LLM-based Agent</h4>

<div id="S8.SS1.SSS6.p1" class="ltx_para">
<p id="S8.SS1.SSS6.p1.1" class="ltx_p">The research on agents in AI aims to develop entities that can perceive the environment, make decisions, and take actions to achieve specific goals&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib878" title="" class="ltx_ref">878</a>]</cite>.
However, traditional agents are often limited to heuristic rules or specific environments, which constrain their generalization to open-domain scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib879" title="" class="ltx_ref">879</a>]</cite>.
Given that LLMs possess excellent capacities in solving complex tasks, they have rapidly emerged as promising solutions for serving as the core computation unit of agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib818" title="" class="ltx_ref">818</a>]</cite>.
In this part, we will first introduce the framework for LLM-based agents and then discuss their applications.</p>
</div>
<div id="S8.SS1.SSS6.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS6.p2.1" class="ltx_p"><span id="S8.SS1.SSS6.p2.1.1" class="ltx_text ltx_font_bold">Overall Framework.</span>
Next, we first detail the key components of an LLM-based agent and then present the typical workflow.</p>
</div>
<div id="S8.SS1.SSS6.p3" class="ltx_para">
<p id="S8.SS1.SSS6.p3.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS6.p3.1.1" class="ltx_emph ltx_font_italic">Components.</em>

Typically, there are three main components in an LLM-based agent: <span id="S8.SS1.SSS6.p3.1.2" class="ltx_text ltx_font_italic">memory</span>, <span id="S8.SS1.SSS6.p3.1.3" class="ltx_text ltx_font_italic">planning<span id="footnote52" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">52</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">52</sup><span class="ltx_tag ltx_tag_note"><span id="footnote52.1.1.1" class="ltx_text ltx_font_upright">52</span></span><span id="footnote52.5" class="ltx_text ltx_font_upright">
Section&nbsp;</span><a href="#S6.SS4" title="6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref ltx_font_upright"><span class="ltx_text ltx_ref_tag">6.4</span></a><span id="footnote52.6" class="ltx_text ltx_font_upright"> introduces planning as a utilization approach for LLMs, while in this section, we describe its utilization as a functional component in LLM-based agents.
</span></span></span></span></span>, and <span id="S8.SS1.SSS6.p3.1.4" class="ltx_text ltx_font_italic">execution</span>.
Specifically, the <span id="S8.SS1.SSS6.p3.1.5" class="ltx_text ltx_font_italic">memory</span> component aims to store the information perceived from the environment and can be utilized to support decision-making.
In particular, LLM-based agents usually maintain information in both short-term memory and long-term memory with the operations of reading and writing.
Short-term memory usually refers to the internal context window of LLMs (<em id="S8.SS1.SSS6.p3.1.6" class="ltx_emph ltx_font_italic">i.e.,</em> input), where LLMs can read and write through actions like reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib880" title="" class="ltx_ref">880</a>]</cite>.
While long-term memory can be mapped to the external storage like vector databases&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib537" title="" class="ltx_ref">537</a>]</cite>, where LLMs can read through retrieval and write with reflection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib686" title="" class="ltx_ref">686</a>]</cite>.
Specially, profiles are usually implemented with long-term memory, which is an important feature for an agent that specifies its role and function&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib818" title="" class="ltx_ref">818</a>]</cite>.
The <span id="S8.SS1.SSS6.p3.1.7" class="ltx_text ltx_font_italic">planning</span> component is responsible for generating the action plan based on the information from the memory component.
In data format, the plan usually takes the form of text-based instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib441" title="" class="ltx_ref">441</a>]</cite> or code-based programs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib443" title="" class="ltx_ref">443</a>]</cite>.
To generate it, LLM-based agents will first propose several candidates and then select a more suitable one among them&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib436" title="" class="ltx_ref">436</a>]</cite>.
The initial plan can be further refined with execution feedback from the environment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib528" title="" class="ltx_ref">528</a>]</cite>.
The <span id="S8.SS1.SSS6.p3.1.8" class="ltx_text ltx_font_italic">execution</span> component is in charge of carrying out the plan from the planning component,
which can be fulfilled by the internal LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib441" title="" class="ltx_ref">441</a>]</cite> or external tools&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib880" title="" class="ltx_ref">880</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS6.p4" class="ltx_para">
<p id="S8.SS1.SSS6.p4.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS6.p4.1.1" class="ltx_emph ltx_font_italic">Workflow.</em>

With the three components mentioned above, a typical workflow of an LLM-based agent is as follows.
First, it receives information from the environment and writes it into short-term memory.
Then, the agent processes the newly received information in the short-term memory.
Such a process can be enhanced with information retrieved from long-term memory.
Subsequently, the planning component utilizes the processed information from short-term memory to generate the next plan.
Finally, the execution component carries out the plan generated from the planning component, which can be further assisted with external tools.
By repeating the aforementioned process, the LLM-based agent can autonomously adjust its behavior in response to feedback from the environment and ultimately achieve its goal.
Once LLM-based agents receive user requests or are assigned goals, they follow the above workflow to accomplish tasks through multi-turn interactions with the environment.</p>
</div>
<div id="S8.SS1.SSS6.p5" class="ltx_para">
<p id="S8.SS1.SSS6.p5.1" class="ltx_p">To summarize, in an LLM-based agent, the LLM serves as the core computation unit and is equipped with components including <span id="S8.SS1.SSS6.p5.1.1" class="ltx_text ltx_font_italic">memory</span>, <span id="S8.SS1.SSS6.p5.1.2" class="ltx_text ltx_font_italic">planning</span>, and <span id="S8.SS1.SSS6.p5.1.3" class="ltx_text ltx_font_italic">execution</span>.
These components are integrated in a systematic way under the control of the LLM during interactions with the environment.
For more details, the readers might refer to the comprehensive survey for LLM-based AI agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib818" title="" class="ltx_ref">818</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS6.p6" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS6.p6.1" class="ltx_p"><span id="S8.SS1.SSS6.p6.1.1" class="ltx_text ltx_font_bold">Applications.</span>
Recently, LLM-based agents have shown great potential in autonomously solving complex tasks, making it feasible to rapidly develop capable applications for specific domains or tasks. In this section, we will discuss the applications in single-agent and multi-agent scenarios.</p>
</div>
<div id="S8.SS1.SSS6.p7" class="ltx_para">
<p id="S8.SS1.SSS6.p7.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS6.p7.1.1" class="ltx_emph ltx_font_italic">Single-agent based applications.</em>

Applications based on a single-agent mode mainly aim to develop capable task solvers that can autonomously complete user requests.
A large number of single-agent projects have been developed, which focus on general-purpose task solving. As a representative project, AutoGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib534" title="" class="ltx_ref">534</a>]</cite> empowers LLMs with long/short-term memory management and external tools like search engines.
In order to autonomously address a user request, AutoGPT understands the request with knowledge from its memory and actions like reasoning, decomposes it into a detailed plan, executes the plan step-by-step with the assistance of tools, and refines the rest plan based on feedback from the environment.
Such an iterative process continues until the user request is successfully resolved.
Other similar projects include GPT-Engineer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib881" title="" class="ltx_ref">881</a>]</cite> and XAgent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib882" title="" class="ltx_ref">882</a>]</cite>.
In addition, there is also some work that aims to develop autonomous agents for specific domains, such as WebGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> for the web-browsing environment, ProgPrompt&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib530" title="" class="ltx_ref">530</a>]</cite> for the real-life environment, and Voyager&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib697" title="" class="ltx_ref">697</a>]</cite> for the Minecraft environment.</p>
</div>
<div id="S8.SS1.SSS6.p8" class="ltx_para">
<p id="S8.SS1.SSS6.p8.1" class="ltx_p">•&nbsp;<em id="S8.SS1.SSS6.p8.1.1" class="ltx_emph ltx_font_italic">Multi-agent based applications.</em>

Different from single-agent systems where agents work independently, multi-agent systems work in collaboration to unleash collective intelligence.
Typically, multiple agents can be instantiated from the same or different LLMs, each with their respective roles and functions.
According to the coordinating strategies among these agents, multi-agent systems can be divided into two categories: cooperation-based and competition-based.
In the cooperation-based mode, to share information and seek collaborative actions among agents, various communication protocols have been proposed, including free-form dialogue&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib883" title="" class="ltx_ref">883</a>]</cite>, structured document&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib884" title="" class="ltx_ref">884</a>]</cite>, and data embedding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib885" title="" class="ltx_ref">885</a>]</cite>.
Based on the communication protocol, agents can be effectively organized for downstream applications, such as software engineering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib884" title="" class="ltx_ref">884</a>]</cite>, user behavior analysis&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib819" title="" class="ltx_ref">819</a>, <a href="#bib.bib821" title="" class="ltx_ref">821</a>]</cite>, and society simulation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib533" title="" class="ltx_ref">533</a>]</cite>.
In the competition-based mode, debate serves as one of the popular communication protocols to foster divergent thinking and elicit valuable external feedback among agents.
Such a way is beneficial for domains that demand precise decision-making and accurate responses, such as mathematical reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib886" title="" class="ltx_ref">886</a>]</cite> and evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib732" title="" class="ltx_ref">732</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS6.p9" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS6.p9.1" class="ltx_p"><span id="S8.SS1.SSS6.p9.1.1" class="ltx_text ltx_font_bold">Remaining Issues.</span>

Despite the huge success, there are still several issues that limit the development and applications of LLM-based agents.
First, with the explosive growth of the model scale, the efficiency of LLM-based agents, including both the time and memory overhead, becomes an important issue for large-scale deployment, especially for multi-agent systems with numerous instances of LLMs.
Second, with the scaling of the number of LLM-based agents, more effective and efficient communication protocols and architectures are required to support the increased complexity of coordination among agents.
Furthermore, building capable agents poses technical challenges for the capacities of LLMs like instruction following and long text modeling.
Since existing LLMs are not specially optimized for instantiating agents, most public-sourced LLMs like LLaMA cannot effectively facilitate the development of agents.
Therefore, it is crucial to develop capable, specialized models to serve as the core computation unit of agents.</p>
</div>
</section>
<section id="S8.SS1.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.7 </span>LLM for Evaluation</h4>

<div id="S8.SS1.SSS7.p1" class="ltx_para">
<p id="S8.SS1.SSS7.p1.1" class="ltx_p">While human evaluation can generally offer reliable quality assessment, it is also often hindered by high annotation costs, significant time requirements, and annotation inconsistencies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib887" title="" class="ltx_ref">887</a>]</cite>.
In contrast, automatic evaluation can be employed as a scalable alternative to human evaluation.
Traditional automatic evaluations have relied on reference-based metrics (<em id="S8.SS1.SSS7.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> BLEU and ROUGE).
Recently, with the emergence of LLMs as general task solvers highlights their potential as automatic evaluators&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib647" title="" class="ltx_ref">647</a>]</cite>, making it promising to conduct LLM based evaluation.
In the following part, we will introduce the recent progress on LLM for evaluation, including evaluation formats, methods, meta-evaluation, and the remaining issues.</p>
</div>
<div id="S8.SS1.SSS7.p2" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS7.p2.1" class="ltx_p"><span id="S8.SS1.SSS7.p2.1.1" class="ltx_text ltx_font_bold">Evaluation Formats.</span>

Depending on the type of evaluation outcome, the evaluation format can be categorized into <em id="S8.SS1.SSS7.p2.1.2" class="ltx_emph ltx_font_italic">score-based evaluation</em> and <em id="S8.SS1.SSS7.p2.1.3" class="ltx_emph ltx_font_italic">language-based evaluation</em>.
Score-based evaluation employs measurable metrics to assign quality scores (<em id="S8.SS1.SSS7.p2.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> ratings or rankings) for evaluated texts.
A prevalent way is to conduct pairwise comparison, where LLMs are used to determine the partial order relation of candidate texts following specific guidelines&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib354" title="" class="ltx_ref">354</a>, <a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib647" title="" class="ltx_ref">647</a>]</cite>, which greatly simplifies the evaluation task.
However, it may face the inefficiency issue when scaling up the number of candidates&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite>.
When high-quality reference texts are available during evaluation, LLMs can be instructed to score texts under the guidance provided by references&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib728" title="" class="ltx_ref">728</a>, <a href="#bib.bib716" title="" class="ltx_ref">716</a>]</cite>.
On the other hand, language-based evaluation focuses on generating critiques and suggestions, offering qualitative explanation beyond simple quantitative scoring&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>, <a href="#bib.bib888" title="" class="ltx_ref">888</a>, <a href="#bib.bib889" title="" class="ltx_ref">889</a>, <a href="#bib.bib890" title="" class="ltx_ref">890</a>]</cite>.
It is particularly useful for gathering language feedback signals for human alignment tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>, <a href="#bib.bib888" title="" class="ltx_ref">888</a>]</cite>.
Furthermore, it can evolve into a multi-turn interaction framework, where LLM-based evaluators provide natural language feedback to existing solutions from task solvers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib891" title="" class="ltx_ref">891</a>]</cite>.
This framework evaluates the ability of LLMs to leverage language feedback for refining self-generated solutions.</p>
</div>
<div id="S8.SS1.SSS7.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS7.p3.1" class="ltx_p"><span id="S8.SS1.SSS7.p3.1.1" class="ltx_text ltx_font_bold">Evaluation Methods.</span>

A common method for LLM-based evaluation involves prompting LLMs with specific instructions.
To further improve the quality of LLM-based evaluation, recent work proposes to prompt LLMs with varied contexts to generate diverse evaluation feedback.
These contexts vary in aspects such as the candidate order&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib647" title="" class="ltx_ref">647</a>]</cite>, evaluation perspectives&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib892" title="" class="ltx_ref">892</a>, <a href="#bib.bib893" title="" class="ltx_ref">893</a>]</cite> (<em id="S8.SS1.SSS7.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> relevance, clarity, originality), and evaluation explanation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib647" title="" class="ltx_ref">647</a>]</cite>.
The generated multiple evaluation feedbacks are then aggregated to produce a final evaluation result, which makes the evaluation process less prone to biases from individual feedback and allows for a more thorough evaluation by covering a wider range of evaluation aspects.
To further improve the quality of the single-model evaluation, recent studies also develop multi-agent collaboration frameworks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib894" title="" class="ltx_ref">894</a>, <a href="#bib.bib895" title="" class="ltx_ref">895</a>, <a href="#bib.bib893" title="" class="ltx_ref">893</a>]</cite> or fine-tune LLMs as specified evaluators&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>, <a href="#bib.bib888" title="" class="ltx_ref">888</a>, <a href="#bib.bib889" title="" class="ltx_ref">889</a>, <a href="#bib.bib890" title="" class="ltx_ref">890</a>, <a href="#bib.bib896" title="" class="ltx_ref">896</a>]</cite>.
In a multi-model collaboration mode, different LLMs evaluate the candidates by engaging in discussions to align preferences and reach a consensus&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib894" title="" class="ltx_ref">894</a>, <a href="#bib.bib895" title="" class="ltx_ref">895</a>]</cite>.
This method helps reduce the potential biases in individual models through the consensus reached by multiple agents.
Another approach to improving single-model evaluation is to specialize LLMs as scores or critics through fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib371" title="" class="ltx_ref">371</a>, <a href="#bib.bib888" title="" class="ltx_ref">888</a>, <a href="#bib.bib889" title="" class="ltx_ref">889</a>, <a href="#bib.bib890" title="" class="ltx_ref">890</a>, <a href="#bib.bib896" title="" class="ltx_ref">896</a>]</cite>.
This process involves creating datasets annotated with preferences and feedback from humans or proficient LLMs. These datasets are then used to train evaluation-oriented models, enabling them to generate pairwise preference or language feedback.
The specialized LLM evaluators demonstrate competitive performance with fewer parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib889" title="" class="ltx_ref">889</a>, <a href="#bib.bib890" title="" class="ltx_ref">890</a>, <a href="#bib.bib896" title="" class="ltx_ref">896</a>]</cite>.</p>
</div>
<div id="S8.SS1.SSS7.p4" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS7.p4.1" class="ltx_p"><span id="S8.SS1.SSS7.p4.1.1" class="ltx_text ltx_font_bold">Meta-Evaluation.</span>

To effectively assess the quality of LLM-based evaluators, meta-evaluation benchmarks have been introduced, for gauging the agreement with human preferences and the fairness of the evaluations made by LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib647" title="" class="ltx_ref">647</a>, <a href="#bib.bib897" title="" class="ltx_ref">897</a>, <a href="#bib.bib893" title="" class="ltx_ref">893</a>, <a href="#bib.bib898" title="" class="ltx_ref">898</a>]</cite>.
As a representative benchmark, MT-Bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>]</cite> evaluates the agreement between LLMs and human judgments, demonstrating that GPT-4 aligns closely with human preferences in no-tie comparisons on 80 multi-turn questions.
In addition, to address potential biases arising from subjective human evaluations, LLMBar&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib897" title="" class="ltx_ref">897</a>]</cite> manually designs outputs that are objectively worse but superficially appealing, which could mislead evaluators.
The evaluation results reveal that even the most advanced LLMs still fall short of human-level evaluation in the challenging setting.</p>
</div>
<div id="S8.SS1.SSS7.p5" class="ltx_para ltx_noindent">
<p id="S8.SS1.SSS7.p5.1" class="ltx_p"><span id="S8.SS1.SSS7.p5.1.1" class="ltx_text ltx_font_bold">Remaining Issues.</span>

As discussed in Section&nbsp;<a href="#S7.SS1.SSS1" title="7.1.1 Language Generation ‣ 7.1 Basic Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1.1</span></a>, recent studies demonstrate that LLM-based evaluators expose multiple types of bias, such as order bias, self-preference bias, and length bias&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib727" title="" class="ltx_ref">727</a>, <a href="#bib.bib647" title="" class="ltx_ref">647</a>]</cite>.
Although some biases can be mitigated through methods like multi-path ensemble or multi-agent collaboration, they remain inherent to LLM-based evaluators. Consequently, addressing these biases intrinsically within the models continues to be an a challenging issue.
In addition, recent work has revealed that LLMs may be incapable of understanding the self-generated content, exhibiting a weaker understanding capacity compared to their generation capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib899" title="" class="ltx_ref">899</a>]</cite>.
Even the most advanced LLMs still struggle identifying their reasoning or factual errors without external feedback&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib900" title="" class="ltx_ref">900</a>, <a href="#bib.bib901" title="" class="ltx_ref">901</a>]</cite>.
Consequently, current LLM-based evaluators might not be adequate for evaluating top-tier LLMs or complex tasks.
This underscores the importance of improvement approaches for LLM-based evaluators, especially for evaluating capable LLMs and complex tasks demanding sophisticated reasoning, planning, and domain-specific knowledge.</p>
</div>
</section>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span><span id="S8.SS2.1.1" class="ltx_text ltx_font_italic">LLM for Specific Domains</span>
</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">In this part, we discuss the applications of LLMs on several representative domains, including healthcare, education, law, finance, and scientific research assistance.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para ltx_noindent">
<p id="S8.SS2.p2.1" class="ltx_p"><span id="S8.SS2.p2.1.1" class="ltx_text ltx_font_bold">Healthcare</span> is a vital application field closely related to human life. Ever since the advent of ChatGPT, a number of studies have applied ChatGPT or other LLMs to the medical domain.
It has been shown that LLMs are capable of handling a variety of healthcare tasks, <em id="S8.SS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> biology information extraction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib763" title="" class="ltx_ref">763</a>]</cite>, medical advice consultation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib902" title="" class="ltx_ref">902</a>]</cite>, mental health analysis&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib903" title="" class="ltx_ref">903</a>]</cite>, and report simplification&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib904" title="" class="ltx_ref">904</a>]</cite>.
As the major technical approach, researchers typically design specific prompts or instructions to guide LLMs to perform a wide range of medical tasks. 
To further harness the power of LLMs in the healthcare domain, researchers propose to develop healthcare-related LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>, <a href="#bib.bib905" title="" class="ltx_ref">905</a>, <a href="#bib.bib906" title="" class="ltx_ref">906</a>]</cite>.
Specifically, the Med-PaLM models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>, <a href="#bib.bib905" title="" class="ltx_ref">905</a>]</cite> achieves expert-level performance on the United States Medical Licensing Examination (USMLE), and earns greater approval from physicians in answering consumer’s medical questions.
However, LLMs may fabricate medical misinformation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib904" title="" class="ltx_ref">904</a>, <a href="#bib.bib907" title="" class="ltx_ref">907</a>]</cite>, <em id="S8.SS2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> misinterpreting medical terms and suggesting advice inconsistent with medical guidelines. In addition, it would also raise privacy concerns to upload the health information of patients&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib763" title="" class="ltx_ref">763</a>]</cite> into a commercial server that support the LLM.</p>
</div>
<div id="S8.SS2.p3" class="ltx_para ltx_noindent">
<p id="S8.SS2.p3.1" class="ltx_p"><span id="S8.SS2.p3.1.1" class="ltx_text ltx_font_bold">Education</span>
is also an important application domain where LLMs potentially exert significant influence. Existing work has found that LLMs can achieve student-level performance on standardized tests&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> in a variety of subjects of mathematics (<em id="S8.SS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> physics, computer science) on both multiple-choice and free-response problems.
In addition, empirical studies have shown that LLMs can serve as writing or reading assistant for education&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib908" title="" class="ltx_ref">908</a>, <a href="#bib.bib909" title="" class="ltx_ref">909</a>]</cite>.
A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib909" title="" class="ltx_ref">909</a>]</cite> reveals that
ChatGPT is capable of generating logically consistent answers across disciplines, balancing both depth and breadth.
Another quantitative analysis&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib908" title="" class="ltx_ref">908</a>]</cite> shows that students utilizing ChatGPT (either keeping or refining the results from LLMs as their own answers) perform better than average students in some courses from the computer security field.
Recently, several perspective papers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib910" title="" class="ltx_ref">910</a>, <a href="#bib.bib911" title="" class="ltx_ref">911</a>]</cite> also explore various application scenarios of LLMs in classroom teaching, such as teacher-student collaboration, personalized learning, and assessment automation.
However, the application of LLMs in education may lead to a series of practical issues, <em id="S8.SS2.p3.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> plagiarism, potential bias in AI-generated content, overreliance on LLMs, and inequitable access for non-English speaking individuals&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib912" title="" class="ltx_ref">912</a>]</cite>.</p>
</div>
<div id="S8.SS2.p4" class="ltx_para ltx_noindent">
<p id="S8.SS2.p4.1" class="ltx_p"><span id="S8.SS2.p4.1.1" class="ltx_text ltx_font_bold">Law</span>
is a specialized domain that is built on professional domain knowledge.
Recently, a number of studies have applied LLMs to solve various legal tasks, <em id="S8.SS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> legal document analysis&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib913" title="" class="ltx_ref">913</a>]</cite>, legal judgment prediction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib914" title="" class="ltx_ref">914</a>]</cite>, and legal document writing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib915" title="" class="ltx_ref">915</a>]</cite>. A recent study&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib916" title="" class="ltx_ref">916</a>]</cite> has found that
LLMs exhibit powerful abilities of legal interpretation and reasoning.
Moreover, the latest GPT-4 model achieves a top 10% score in a simulated bar exam compared with human test-takers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.

To further improve the performance of LLMs in the law domain, specially designed legal prompt engineering are employed to yield advanced performance in long legal document comprehension and complex legal reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib917" title="" class="ltx_ref">917</a>, <a href="#bib.bib918" title="" class="ltx_ref">918</a>]</cite>.
To summarize the progress, LLMs can act as helpful assistants to legal profession.
Despite the progress, the use of LLMs in law raises concerns about legal challenges, including copyright issues&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib919" title="" class="ltx_ref">919</a>]</cite>, personal information leakage&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib920" title="" class="ltx_ref">920</a>]</cite>, or bias and discrimination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib921" title="" class="ltx_ref">921</a>]</cite>.</p>
</div>
<div id="S8.SS2.p5" class="ltx_para ltx_noindent">
<p id="S8.SS2.p5.1" class="ltx_p"><span id="S8.SS2.p5.1.1" class="ltx_text ltx_font_bold">Finance</span>
is an important field where LLMs have promising application prospects.
LLMs have been employed on various finance related tasks, such as numerical claim detection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib922" title="" class="ltx_ref">922</a>]</cite>, financial sentiment analysis&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib923" title="" class="ltx_ref">923</a>]</cite>, financial named entity recognition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib924" title="" class="ltx_ref">924</a>]</cite>, and financial reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib925" title="" class="ltx_ref">925</a>]</cite>.
Despite the competitive zero-shot performance exhibited by general-purpose LLMs in the finance tasks, they still underperform domain-specific PLMs containing million-scale parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib922" title="" class="ltx_ref">922</a>]</cite>.
To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs (<em id="S8.SS2.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>, XuanYuan 2.0&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib926" title="" class="ltx_ref">926</a>]</cite>, and FinGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib927" title="" class="ltx_ref">927</a>]</cite>).
BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>.
Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>.
Therefore, it needs more strict reviewing and monitoring on the use of LLMs in the financial field.</p>
</div>
<div id="S8.SS2.p6" class="ltx_para ltx_noindent">
<p id="S8.SS2.p6.1" class="ltx_p"><span id="S8.SS2.p6.1.1" class="ltx_text ltx_font_bold">Scientific research</span> is another
promising field that LLMs can empower the development progress. Prior research demonstrates the effectiveness of LLMs in handling knowledge-intensive scientific tasks (<em id="S8.SS2.p6.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> PubMedQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib928" title="" class="ltx_ref">928</a>]</cite>, BioASQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib929" title="" class="ltx_ref">929</a>]</cite>), especially for LLMs that are trained on scientific-related corpora&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>, <a href="#bib.bib930" title="" class="ltx_ref">930</a>]</cite>.
Given the excellent general abilities and broad scientific knowledge, LLMs hold significant potential as helpful assistants across various stages of the scientific research pipeline&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib931" title="" class="ltx_ref">931</a>]</cite>.
First, during the literature survey stage, LLMs can help conduct a comprehensive overview of the progress in a specific research field&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib932" title="" class="ltx_ref">932</a>, <a href="#bib.bib933" title="" class="ltx_ref">933</a>]</cite>.
Second, during the research idea generation stage, LLMs demonstrate the ability to generate intriguing scientific hypotheses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib934" title="" class="ltx_ref">934</a>]</cite>.
Third, during the data analysis stage, LLMs can be employed to conduct automatic approaches to analyzing the data characteristics, including data exploration, visualization, and deriving analytical conclusions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib935" title="" class="ltx_ref">935</a>, <a href="#bib.bib936" title="" class="ltx_ref">936</a>]</cite>.
Fourth, during the paper writing stage, researchers can also benefit from the assistance of LLMs in scientific writing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib937" title="" class="ltx_ref">937</a>, <a href="#bib.bib938" title="" class="ltx_ref">938</a>]</cite>, in which LLMs can offer valuable support for scientific writing through diverse means, such as summarizing the existing content and polishing the writing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib939" title="" class="ltx_ref">939</a>]</cite>. 
In addition, LLMs can aid in the automated paper review process, encompassing tasks such as error detection, checklist verification, and candidate ranking&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib940" title="" class="ltx_ref">940</a>]</cite>.
Despite these advances,
there is much room for improving the capacities of LLMs to serve as helpful, trustworthy scientific assistants, to both increase the quality of the generated scientific content and reduce the harmful hallucinations.</p>
</div>
<div id="S8.SS2.p7" class="ltx_para">
<p id="S8.SS2.p7.1" class="ltx_p"><em id="S8.SS2.p7.1.1" class="ltx_emph ltx_font_italic">Summary</em>.
In addition to the aforementioned work,
the applications of LLMs have been also discussed in several other domains.
For instance, in the psychologic domain, some recent work has studied the human-like characteristics of LLMs, such as self-awareness, theory of mind&nbsp;(ToM), and affective computing&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib941" title="" class="ltx_ref">941</a>, <a href="#bib.bib942" title="" class="ltx_ref">942</a>]</cite>.
In particular, an empirical evaluation of ToM conducted on two classic false-belief tasks speculates that LLMs may have ToM-like abilities since the model in the GPT-3.5 series achieves comparable performance with nine-year-old children in ToM task&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib941" title="" class="ltx_ref">941</a>]</cite>.
In addition, another line of work has investigated applying LLMs into the software development domain, <em id="S8.SS2.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> code suggestion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib943" title="" class="ltx_ref">943</a>]</cite>, code summarization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib944" title="" class="ltx_ref">944</a>]</cite>, and automated program repair&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib945" title="" class="ltx_ref">945</a>]</cite>.
To summarize, to assist humans by LLMs in real-world tasks has become a significant area of research.
However, it also presents challenges. Ensuring the accuracy of LLM-generated content, addressing biases, and maintaining user privacy and data security are crucial considerations when applying LLMs to real-world scenarios.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Directions</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">In this survey, we have reviewed the recent progress of large language models&nbsp;(LLMs), and introduced the key concepts, findings, and techniques for understanding and utilizing LLMs. We focus on the large-sized models (<em id="S9.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> having a size larger than 10B) while excluding the contents of early pre-trained language models (<em id="S9.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> BERT and GPT-2) that have been well covered in the existing literature.
In particular, our survey has discussed four important aspects of LLMs, <em id="S9.p1.1.3" class="ltx_emph ltx_font_italic">i.e.,</em> pre-training, adaptation, utilization, and evaluation. For each aspect, we highlight the techniques or findings that are key to the success of LLMs.
Furthermore, we also summarize the available resources for developing LLMs and discuss important implementation guidelines for reproducing LLMs.
This survey tries to cover the most recent literature about LLMs and provides a good reference resource on this topic for both researchers and engineers.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p id="S9.p2.1" class="ltx_p">Next, we summarize the discussions of this survey, and introduce the challenges and future directions for LLMs, in the following aspects.</p>
</div>
<div id="S9.p3" class="ltx_para ltx_noindent">
<p id="S9.p3.1" class="ltx_p"><span id="S9.p3.1.1" class="ltx_text ltx_font_bold">Basics and Principles.</span>
Instead of training on specific task goals, LLMs learn from unsupervised pre-training on large-scale text data. This is quite different from previous multi-task learning approaches, which aim to extend the training tasks as possible to achieve sufficient generalization.
Thus, it is essential to reveal the basic principles or elements that establish the foundation of the abilities of LLMs.
Although the basic idea of language models is intuitive, it is still challenging to formally explain why LLMs trained by simple language modeling objectives (<em id="S9.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> next token prediction) can become capable of solving various real-world tasks.
To investigate this problem, a promising approach is to study the
capacity learning (or selection) mechanism based on unsupervised pre-training, since
the model capacity of LLMs strongly depends on pre-training data.
In addition, <em id="S9.p3.1.3" class="ltx_emph ltx_font_italic">scaling</em> plays an important role in improving the capacity of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, and it is very useful to conduct more theoretical analysis about how the behaviors of large models relate to those of small models, <em id="S9.p3.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> what behaviors of large models can be inferred from small models and what can’t be predicted indeed.
Another research direction is to explore more deep analysis on model generalization for LLMs, since increasing concerns have been raised about whether LLMs can generalize beyond the knowledge encoded by pre-training data. Furthermore,
data contamination has become a severe issue for fairly assessing the performance of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib738" title="" class="ltx_ref">738</a>]</cite>, and thus
setting appropriate evaluation protocol will be the basis to investigate and analyze the model capacity of LLMs.</p>
</div>
<div id="S9.p4" class="ltx_para ltx_noindent">
<p id="S9.p4.1" class="ltx_p"><span id="S9.p4.1.1" class="ltx_text ltx_font_bold">Model Architecture.</span> Due to the scalability and effectiveness,
Transformer
has become the de facto architecture for building LLMs. Various strategies have been proposed to improve the performance of this architecture, such as neural network configuration and scalable parallel training (see discussions in Section&nbsp;<a href="#S4.SS2.SSS2" title="4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>).
However, Transformer still suffers from high training costs and slow inference rates. More efforts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref">251</a>, <a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite> are still in need to develop improved model architectures for large-scale pre-training.
Specially, system-level or hardware-level optimization (<em id="S9.p4.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> FlashAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite>) is worth more exploration to improve the efficiency of Transformer architectures. In addition, as an important basic capacity, existing LLMs typically maintain a long context window. For example, the most recent GPT-4 Turbo enables a long context of 128K tokens, and Claude 2.1 also supports the input up to 200K tokens. Although many efforts have been made to enhance the long context modeling ability of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib291" title="" class="ltx_ref">291</a>, <a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite>, the resulting models still can’t well process the information in the context window&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib299" title="" class="ltx_ref">299</a>]</cite>. To address this issue, specific architecture adaptations or algorithms might be needed to enhance the modeling and utilization of long context information. Another worrying concern is that existing work mostly focuses on training LLMs with decoder-only Transformers. Despite the effectiveness, it severely limits the more wide, diverse explorations on alternative model architectures.</p>
</div>
<div id="S9.p5" class="ltx_para ltx_noindent">
<p id="S9.p5.1" class="ltx_p"><span id="S9.p5.1.1" class="ltx_text ltx_font_bold">Model Training.</span>
For pre-training, it is essential to establish a data-centric infrastructure and training procedure for LLM optimization, which can effectively support a systematic process of data collection, data cleaning, data mixture, and data curriculum. Furthermore, it also calls for more flexible mechanisms of hardware support or resource schedule, so as to better organize and utilize the resources in a computing cluster.
In practice, it is very challenging to pre-train capable LLMs, due to the huge compute consumption and the sensitivity to data quality and training tricks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.
Thus, it becomes particularly important to develop systemic, economical pre-training approaches for optimizing LLMs, <em id="S9.p5.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> predictable scaling&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and proxy model training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. More training recipes or principles should be investigated and shared to reduce the potential risk of degradation or failure in large-scale model optimization.
Although increasingly more model checkpoints and cleaned datasets have been released, there still lacks reproducible work on pre-training data preparation (<em id="S9.p5.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> detailed cleaning strategies) and data scheduling (<em id="S9.p5.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> data mixture and curriculum). Since it is very costly to pre-train a LLM from scratch, it is important to design suitable mechanisms for continually pre-training or fine-tuning the LLM based on publicly available model checkpoints (<em id="S9.p5.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and Flan-T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>).
For this purpose, a number of technical issues have to be resolved, <em id="S9.p5.1.6" class="ltx_emph ltx_font_italic">e.g.,</em> catastrophic forgetting and task specialization.
Furthermore, it is also useful to develop effective tuning strategies that effectively inject or edit specific knowledge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib672" title="" class="ltx_ref">672</a>]</cite>, <em id="S9.p5.1.7" class="ltx_emph ltx_font_italic">e.g.,</em> correcting the outdated facts.</p>
</div>
<div id="S9.p6" class="ltx_para ltx_noindent">
<p id="S9.p6.1" class="ltx_p"><span id="S9.p6.1.1" class="ltx_text ltx_font_bold">Model Utilization.</span>
Based on the natural language interface, <em id="S9.p6.1.2" class="ltx_emph ltx_font_italic">prompting</em> has become the prominent approach for using LLMs to solving various tasks. By combining task descriptions and demonstration examples into prompts, in-context learning&nbsp;(ICL) endows LLMs with the ability to perform well on new tasks, even outperforming full-data fine-tuned models in some cases.
To enhance the ability of complex reasoning, advanced prompting techniques have been proposed, exemplified by the chain-of-thought&nbsp;(CoT) strategy, which includes the intermediate reasoning steps into prompts.
Furthermore, planning is a promising approach for solving complex tasks, which iteratively invokes LLMs by leveraging tool use capacities. Despite these efforts, several basic problems related to prompting are still under-explored: why a good prompt can elicit the correct answer but a bad prompt cannot, how to reveal the working principles of advanced prompting methods (<em id="S9.p6.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> ICL and CoT) and further improve these existing approaches, and how to efficiently find the effective prompts for LLMs on specific tasks.
Furthermore, from a practical perspective, it has become a fundamental challenge to reduce the inference cost of LLMs, especially in large-scale deployment.
Another popular research direction is retrieval-augmented generation, where retrieved contexts from supporting sources are included into prompts for task solving. It has been shown that retrieval augmentation can extend the knowledge boundary and improve the question answering capacity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib461" title="" class="ltx_ref">461</a>]</cite>, but may suffer from the effectiveness of long context utilization by LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib299" title="" class="ltx_ref">299</a>]</cite>.</p>
</div>
<div id="S9.p7" class="ltx_para ltx_noindent">
<p id="S9.p7.1" class="ltx_p"><span id="S9.p7.1.1" class="ltx_text ltx_font_bold">Safety and Alignment.</span>
Despite the capacities, LLMs are faced with great safety challenges in practical use.
As a fundamental issue of probabilistic modeling nature, LLMs exhibit a tendency to generate hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib638" title="" class="ltx_ref">638</a>]</cite>, referring to texts that seem plausible but may be factually incorrect&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
What is worse,
LLMs might be elicited by intentional instructions to produce harmful, biased, or toxic texts for malicious systems, leading to the potential risks of misuse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
To have a detailed discussion of the safety issues of LLMs (<em id="S9.p7.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> privacy, overreliance, disinformation, and influence operations), the readers can refer to the GPT-3/4 technical reports&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. As the major technical approach to averting these issues, alignment methods (<em id="S9.p7.1.3" class="ltx_emph ltx_font_italic">e.g.,</em> RLHF)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> have been widely used by leveraging human feedback for developing well-aligned LLMs.
However, RLHF heavily relies on high-quality human feedback data from professional labelers, which is costly and time-consuming to recruit qualified human annotators. Therefore, it is necessary to improve the RLHF framework for reducing the efforts of human labelers and seek a more efficient annotation approach with guaranteed data quality, <em id="S9.p7.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> LLMs can be employed to assist the labeling work.
Furthermore, it is also suggested to develop simplified optimization algorithms for alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib389" title="" class="ltx_ref">389</a>, <a href="#bib.bib386" title="" class="ltx_ref">386</a>]</cite>, to reduce the training difficulty and unstability of RLHF.
As another practical approach, red teaming&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib369" title="" class="ltx_ref">369</a>]</cite> has been adopted for improving the model safety of LLMs, which utilizes the collected adversarial prompts to refine the LLMs (<em id="S9.p7.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> avoiding the attacks from red teaming).
In addition,
privacy concerns are also important to consider when fine-tuning LLMs with domain-specific data, and thus federated based learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib946" title="" class="ltx_ref">946</a>]</cite> can be useful in privacy-restricted scenarios.</p>
</div>
<div id="S9.p8" class="ltx_para ltx_noindent">
<p id="S9.p8.1" class="ltx_p"><span id="S9.p8.1.1" class="ltx_text ltx_font_bold">Application and Ecosystem.</span>
As LLMs have shown strong capacities in solving various tasks, they can be applied in a broad range of real-world applications (<em id="S9.p8.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> following task-specific natural language instructions). As a remarkable progress, ChatGPT has potentially changed the way how humans access information, which has been additionally integrated in the release of <em id="S9.p8.1.3" class="ltx_emph ltx_font_italic">New Bing</em>. Generally, in the near future, it can be foreseen that LLMs would have a significant impact on information-seeking techniques, including both search engines and recommender systems.
Furthermore, LLMs make it possible to develop more intelligent systems (<em id="S9.p8.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> autonomous AI agents) to tackle various complex tasks in real-world scenarios.
Specially, Assistants API has been launched by OpenAI (featured by instructions, knowledge and tool use), enabling rapid development of agent-like assistants within the applications.
This wave of technical innovation would lead to an ecosystem of LLM-empowered applications (<em id="S9.p8.1.5" class="ltx_emph ltx_font_italic">e.g.,</em> OpenAI’s GPT Store), which has a close connection with human life.
Lastly, the rise of LLMs sheds light on the exploration of artificial general intelligence&nbsp;(AGI). It is promising to develop more smart AI systems than ever. However, in this development process, AI safety should be one of the primary concerns, <em id="S9.p8.1.6" class="ltx_emph ltx_font_italic">i.e.,</em> making AI lead to good for humanity but not bad&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Coda</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">It is not an easy job to write this long survey and update its content with timely work. First of all, we would like to sincerely thank the support from the readers and our team members. We work very hard on this survey, and hope that it can present a comprehensive, timely reference for LLMs.</p>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p"><span id="Sx1.p2.1.1" class="ltx_text ltx_font_bold">Survey Writing</span>. This survey was planned during a discussion meeting held by our research team, and we aimed to summarize the recent advances of large language models as a highly readable report for our team members. The first draft was finished on March 13, 2023, in which our team members tried their best to include the related studies about LLMs in a relatively objective, comprehensive way.
Then, we have extensively revised the writing and contents in several passes.
Due to the space limit, we can only include a fraction of existing LLMs in Figure&nbsp;<a href="#S2.F3" title="Figure 3 ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table&nbsp;<a href="#S2.T1" title="TABLE I ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, by setting the selection criterion.
However, we set a more relaxed criterion for model selection on our GitHub page (<a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey</a>), which will be regularly maintained.
We release the initial version on March 31, 2023, the major revision on June 29, 2023, and second version on September 10, 2023, and this latest version (major revision) on November 23, 2023.</p>
</div>
<div id="Sx1.p3" class="ltx_para ltx_noindent">
<p id="Sx1.p3.1" class="ltx_p"><span id="Sx1.p3.1.1" class="ltx_text ltx_font_bold">Seeking for Advice</span>. Despite all our efforts, this survey is still far from perfect: we are likely to miss important references or topics, and might also have non-rigorous expressions or discussions.
We will continuously update this survey, and improve the quality as much as we can.
For us, survey writing is also a learning process for LLMs by ourselves.
For readers with constructive suggestions to improve this survey, you are welcome to leave comments on the GitHub page of our survey or directly email our authors.
We will make revisions following the received comments or suggestions in a future version, and acknowledge the readers who have contributed constructive suggestions in our survey.</p>
</div>
<div id="Sx1.p4" class="ltx_para ltx_noindent">
<p id="Sx1.p4.1" class="ltx_p"><span id="Sx1.p4.1.1" class="ltx_text ltx_font_bold">Update log</span>. In this part, we regularly maintain an update log for the submissions of this survey to arXiv:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">First release on March 31, 2023: the initial version.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Update on April 9, 2023: add the affiliation information, revise Figure&nbsp;<a href="#S2.F3" title="Figure 3 ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table&nbsp;<a href="#S2.T1" title="TABLE I ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and clarify the corresponding selection criterion for LLMs, improve the writing, and correct some minor errors.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Update on April 11, 2023: correct the errors for library resources.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">Update on April 12, 2023: revise Figure&nbsp;<a href="#S2.F3" title="Figure 3 ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table&nbsp;<a href="#S2.T1" title="TABLE I ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, and clarify the release date of LLMs.</p>
</div>
</li>
<li id="Sx1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i5.p1.1" class="ltx_p">Update on April 16, 2023: add a new Section&nbsp;<a href="#S2.SS2" title="2.2 Technical Evolution of GPT-series Models ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> about the technical evolution of GPT-series models.</p>
</div>
</li>
<li id="Sx1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i6.p1" class="ltx_para">
<p id="Sx1.I1.i6.p1.1" class="ltx_p">Update on April 24, 2023: add the discussion about scaling laws and add some explanations about the model sizes for emergent abilities (Section&nbsp;<a href="#S2.SS1" title="2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>); add an illustrative figure for the attention patterns for different architectures in Figure&nbsp;<a href="#S4.F9" title="Figure 9 ‣ 4.2.1 Typical Architectures ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, and add the detailed formulas in Table&nbsp;<a href="#S4.T6" title="TABLE VI ‣ 4.2.2 Detailed Configuration ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</li>
<li id="Sx1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i7.p1" class="ltx_para">
<p id="Sx1.I1.i7.p1.1" class="ltx_p">Update on April 25, 2023: revise some copy errors in figures and tables.</p>
</div>
</li>
<li id="Sx1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i8.p1" class="ltx_para">
<p id="Sx1.I1.i8.p1.1" class="ltx_p">Update on April 27, 2023: add efficient tuning in Section&nbsp;<a href="#S5.SS3" title="5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
</li>
<li id="Sx1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i9.p1" class="ltx_para">
<p id="Sx1.I1.i9.p1.1" class="ltx_p">Update on April 28, 2023: revise Section&nbsp;<a href="#S5.SS3" title="5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
</li>
<li id="Sx1.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i10.p1" class="ltx_para">
<p id="Sx1.I1.i10.p1.1" class="ltx_p">Update on May 7, 2023: revise Table&nbsp;<a href="#S2.T1" title="TABLE I ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, Table&nbsp;<a href="#S3.T2" title="TABLE II ‣ 3.1 Publicly Available Model Checkpoints or APIs ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, and some minor points.</p>
</div>
</li>
<li id="Sx1.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i11.p1" class="ltx_para">
<p id="Sx1.I1.i11.p1.1" class="ltx_p">Update on June 29, 2023 (major revision):</p>
<ul id="Sx1.I1.i11.I1" class="ltx_itemize">
<li id="Sx1.I1.i11.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i1.p1.1" class="ltx_p">Section&nbsp;<a href="#S1" title="1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: add Figure&nbsp;<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the trends of published LLM papers in arXiv;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i2.p1.1" class="ltx_p">Section&nbsp;<a href="#S2" title="2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>: add Figure&nbsp;<a href="#S2.F4" title="Figure 4 ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for GPT’s evolution and the corresponding discussion;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i3.p1.1" class="ltx_p">Section&nbsp;<a href="#S3" title="3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>: add Figure&nbsp;<a href="#S2.F5" title="Figure 5 ‣ 2.2 Technical Evolution of GPT-series Models ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for LLaMA family and the corresponding discussion;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i4.p1.1" class="ltx_p">Section&nbsp;<a href="#S5" title="5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>: add latest discussion about the synthetic data formatting of instruction tuning in Section&nbsp;<a href="#S5.SS1.SSS1" title="5.1.1 Formatted Instance Construction ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>, the empirical analysis for instruction tuning in Section&nbsp;<a href="#S5.SS1.SSS4" title="5.1.4 Empirical Analysis for Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.4</span></a>, parameter-efficient model adaptation in Section&nbsp;<a href="#S5.SS3" title="5.3 Parameter-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a> and memory-efficient adaptation in Section&nbsp;<a href="#S5.SS4" title="5.4 Memory-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i5.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i5.p1.1" class="ltx_p">Section&nbsp;<a href="#S6" title="6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>: add latest discussion about the underlying mechanism of ICL&nbsp;<a href="#S6.SS2.SSS3" title="6.2.3 Underlying Mechanism ‣ 6.2 In-Context Learning ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2.3</span></a>, planning for complex task solving in Section&nbsp;<a href="#S6.SS4" title="6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i6.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i6.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i6.p1.1" class="ltx_p">Section&nbsp;<a href="#S7" title="7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>: update Table&nbsp;<a href="#S7.T14" title="TABLE XIV ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XIV</span></a> for representative datasets for evaluating advanced abilities of LLMs, and empirical ability evaluation in Section&nbsp;<a href="#S7.SS4" title="7.4 Empirical Evaluation ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i7.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i7.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i7.p1.1" class="ltx_p">Section&nbsp;<a href="#S6.SS1.SSS1" title="6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>: add prompt design;</p>
</div>
</li>
<li id="Sx1.I1.i11.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i11.I1.i8.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i11.I1.i8.p1" class="ltx_para">
<p id="Sx1.I1.i11.I1.i8.p1.1" class="ltx_p">Section&nbsp;<a href="#S8" title="8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>: add the discussions on applications of LLMs in finance and scientific research domains;</p>
</div>
</li>
</ul>
</div>
</li>
<li id="Sx1.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i12.p1" class="ltx_para">
<p id="Sx1.I1.i12.p1.1" class="ltx_p">Update on September 10, 2023 (major revision):</p>
<ul id="Sx1.I1.i12.I1" class="ltx_itemize">
<li id="Sx1.I1.i12.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i1.p1.1" class="ltx_p">Claim the copyrights of the figures and tables in this paper.</p>
</div>
</li>
<li id="Sx1.I1.i12.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i2.p1.1" class="ltx_p">Add latest LLMs, techniques and their descriptions in Section&nbsp;<a href="#S3" title="3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Section&nbsp;<a href="#S4" title="4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Section&nbsp;<a href="#S5" title="5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, Section&nbsp;<a href="#S6" title="6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Section&nbsp;<a href="#S7" title="7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i12.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i3.p1.1" class="ltx_p">Section&nbsp;<a href="#S4" title="4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>: add latest discussion about the decoding strategy in Section&nbsp;<a href="#S4.SS2.SSS5" title="4.2.5 Decoding Strategy ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.5</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i12.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i4.p1.1" class="ltx_p">Section&nbsp;<a href="#S5" title="5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>: add latest discussion about the practical tricks for instruction tuning in Section&nbsp;<a href="#S5.SS1.SSS2" title="5.1.2 Instruction Tuning Strategies ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>, the empirical analysis on LLaMA (13B) for instruction tuning in Section&nbsp;<a href="#S5.SS1.SSS4" title="5.1.4 Empirical Analysis for Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.4</span></a>, practical strategies for RLHF in Section&nbsp;<a href="#S5.SS2.SSS3" title="5.2.3 Reinforcement Learning from Human Feedback ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>, alignment without RLHF in Section&nbsp;<a href="#S5.SS2.SSS4" title="5.2.4 Alignment without RLHF ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.4</span></a> and remarks on SFT and RLHF in Section&nbsp;<a href="#S5.SS2.SSS5" title="5.2.5 Remarks on SFT and RLHF ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.5</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i12.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i5.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i5.p1.1" class="ltx_p">Section&nbsp;<a href="#S6" title="6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>: update the content about the planning for complex task solving in Section&nbsp;<a href="#S6.SS4" title="6.4 Planning for Complex Task Solving ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i12.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i6.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i6.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i6.p1.1" class="ltx_p">Section&nbsp;<a href="#S7" title="7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>: add discussions about evaluation approaches in Section&nbsp;<a href="#S7.SS3.SSS2" title="7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3.2</span></a>, Table&nbsp;<a href="#S7.T15" title="TABLE XV ‣ 7.2.3 Tool Manipulation ‣ 7.2 Advanced Ability ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XV</span></a> for the category of existing evaluation work, and update empirical ability evaluation in Section&nbsp;<a href="#S7.SS4" title="7.4 Empirical Evaluation ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a> and the results on Table&nbsp;<a href="#S7.T16" title="TABLE XVI ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVI</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i12.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i12.I1.i7.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i12.I1.i7.p1" class="ltx_para">
<p id="Sx1.I1.i12.I1.i7.p1.1" class="ltx_p">Section&nbsp;<a href="#S6.SS1.SSS1" title="6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>: add new prompt examples in Table&nbsp;<a href="#S6.T12" title="TABLE XII ‣ 6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XII</span></a>;</p>
</div>
</li>
</ul>
</div>
</li>
<li id="Sx1.I1.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i13.p1" class="ltx_para">
<p id="Sx1.I1.i13.p1.1" class="ltx_p">Update on November 23, 2023 (this version):</p>
<ul id="Sx1.I1.i13.I1" class="ltx_itemize">
<li id="Sx1.I1.i13.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i1.p1.1" class="ltx_p">Section&nbsp;<a href="#S1" title="1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: add Figure&nbsp;<a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for the evolution process of four generations of language models;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i2.p1.1" class="ltx_p">Section&nbsp;<a href="#S2" title="2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>: add more discussion about scaling laws and how emergent abilities relate to scaling laws;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i3.p1.1" class="ltx_p">Section&nbsp;<a href="#S3" title="3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>: add latest LLMs in Figure&nbsp;<a href="#S2.F3" title="Figure 3 ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table&nbsp;<a href="#S2.T1" title="TABLE I ‣ 2.1 Background for LLMs ‣ 2 Overview ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, latest APIs in Section&nbsp;<a href="#S3.SS1" title="3.1 Publicly Available Model Checkpoints or APIs ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, commonly used datasets for instruction tuning and alignment tuning in Section&nbsp;<a href="#S3.SS3" title="3.3 Commonly Used Datasets for Fine-tuning ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, and several libraries in Section&nbsp;<a href="#S3.SS4" title="3.4 Library Resource ‣ 3 Resources of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i4.p1.1" class="ltx_p">Section&nbsp;<a href="#S4" title="4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>: add latest discussion about the data scheduling, including data mixtures and data curriculum in Section&nbsp;<a href="#S4.SS1.SSS3" title="4.1.3 Data Scheduling ‣ 4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>; add summary of data preparation in Section&nbsp;<a href="#S4.SS1.SSS4" title="4.1.4 Summary of Data Preparation ‣ 4.1 Data Collection and Preparation ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.4</span></a>; add discussion about modeling long context in Section&nbsp;<a href="#S4.SS2.SSS4" title="4.2.4 Long Context Modeling ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>; add discussion about decoding efficiency issues and add latest decoding strategies in Section&nbsp;<a href="#S4.SS2.SSS5" title="4.2.5 Decoding Strategy ‣ 4.2 Architecture ‣ 4 Pre-training ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.5</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i5.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i5.p1.1" class="ltx_p">Section&nbsp;<a href="#S5" title="5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>: add latest discussion about instance construction and tuning strategies in Section&nbsp;<a href="#S5.SS1" title="5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>; add latest discussion about process-supervised RLHF in Section&nbsp;<a href="#S5.SS2.SSS3" title="5.2.3 Reinforcement Learning from Human Feedback ‣ 5.2 Alignment Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>, and the empirical study on quantized LLaMA models (7B and 13B) in Section&nbsp;<a href="#S5.SS4.SSS3" title="5.4.3 Empirical Analysis and Findings ‣ 5.4 Memory-Efficient Model Adaptation ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4.3</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i6.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i6.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i6.p1.1" class="ltx_p">Section&nbsp;<a href="#S6" title="6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>: add latest discussion about prompt optimization in Section&nbsp;<a href="#S6.SS1.SSS2" title="6.1.2 Prompt Optimization ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1.2</span></a>, and update the content about chain-of-thought prompting in Section&nbsp;<a href="#S6.SS3" title="6.3 Chain-of-Thought Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i7.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i7.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i7.p1.1" class="ltx_p">Section&nbsp;<a href="#S8" title="8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>: add latest discussion about LLM for research directions in Section&nbsp;<a href="#S8.SS1" title="8.1 LLM for Research Community ‣ 8 Applications ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8.1</span></a>;</p>
</div>
</li>
<li id="Sx1.I1.i13.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="Sx1.I1.i13.I1.i8.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="Sx1.I1.i13.I1.i8.p1" class="ltx_para">
<p id="Sx1.I1.i13.I1.i8.p1.1" class="ltx_p">Section&nbsp;<a href="#S9" title="9 Conclusion and Future Directions ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>: revise the content in the several aspects.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="Sx1.p5" class="ltx_para ltx_noindent">
<p id="Sx1.p5.1" class="ltx_p"><span id="Sx1.p5.1.1" class="ltx_text ltx_font_bold">Planning Content</span>. We will regularly include new content into this survey, to make it more self-contained and up-to-date. Here, we list several potential topics that might appear in the next major version(s): (1) more experiments with larger language models for both instruction tuning and ability evaluation; (2) more detailed prompting practice; (3) training recipe; (4) more theoretical analysis and discussion; (5) more discussions on applications.</p>
</div>
<div id="Sx1.p6" class="ltx_para ltx_noindent">
<p id="Sx1.p6.1" class="ltx_p"><span id="Sx1.p6.1.1" class="ltx_text ltx_font_bold">Clarifications on Experiments</span>. In this version, we have included a number experiments on instruction-tuning (Table&nbsp;<a href="#S5.T9" title="TABLE IX ‣ 5.1.3 The Effect of Instruction Tuning ‣ 5.1 Instruction Tuning ‣ 5 Adaptation of LLMs ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>), overall ability evaluation (Table&nbsp;<a href="#S7.T16" title="TABLE XVI ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVI</span></a>), and prompt engineering (Table&nbsp;<a href="#S7.T17" title="TABLE XVII ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVII</span></a>). Due to the limit of computational resources, our experiments are not complete, limited to small-sized models or a few comparisons. Despite that, we feel that it might be meaningful to share the partial results to the public. We will try to include the missing results of larger models or more comparisons in the future versions. <span id="Sx1.p6.1.2" class="ltx_text ltx_font_bold">We also call for support of computing power for conducting more comprehensive experiments.</span></p>
</div>
<div id="Sx1.p7" class="ltx_para ltx_noindent">
<p id="Sx1.p7.1" class="ltx_p"><span id="Sx1.p7.1.1" class="ltx_text ltx_font_bold">Chinese Version</span>. We also provide a translated Chinese version (corresponding to the first release) of this survey paper at the link: <a target="_blank" href="https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf</a>. Four volunteers contribute to check and revise the content, and they are Yiwen Hu, Xin Deng, Xinming Hou, Yanbin Yin, and Zhanshuo Cao (in order of contribution). We will also continuously update the Chinese version, but it may not be as timely as the latest English version.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The authors would like to thank Yankai Lin and Yutao Zhu for proofreading this paper.
Since the first release of this paper, we have received a number of valuable comments from the readers.
We sincerely thank the readers who have written to us with constructive suggestions and comments: Tyler Suard, Damai Dai, Liang Ding, Stella Biderman, Kevin Gray, Jay Alammar, Yubo Feng, Mark Holmstrom, Xingdong Liu, Il-Seok Oh, Yiting Liu, Shaojun Wang, Gaoyan Ou, Todd Morrill, Hao Liu, Zhenyu Zhang, and Xinlin Zhuang.

<br class="ltx_break"></p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Since the v11 version (June 29, 2023), we have been adding a large number of experiments and prompt practices. These new contents are completed by a number of volunteers in our team. Here, we add a special part to thank all the students who have worked very hard on this part (also including the ones on our author list).</p>
</div>
<div id="Sx2.p3" class="ltx_para ltx_noindent">
<p id="Sx2.p3.1" class="ltx_p"><span id="Sx2.p3.1.1" class="ltx_text ltx_font_bold">Contribution on Experiments.</span>
We would like to sincerely thank the following people for their hard work involved in experiments shown in Table&nbsp;<a href="#S7.T16" title="TABLE XVI ‣ 7.3.2 Evaluation Approaches ‣ 7.3 Benchmarks and Evaluation Approaches ‣ 7 Capacity and Evaluation ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XVI</span></a>.</p>
</div>
<div id="Sx2.p4" class="ltx_para">
<p id="Sx2.p4.1" class="ltx_p"><math id="Sx2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p4.1.m1.1a"><mo id="Sx2.p4.1.m1.1.1" xref="Sx2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p4.1.m1.1b"><ci id="Sx2.p4.1.m1.1.1.cmml" xref="Sx2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p4.1.m1.1c">\bullet</annotation></semantics></math> Xiaoxue Cheng: implement the experiments for evaluation on Language Generation and HaluEval tasks.</p>
</div>
<div id="Sx2.p5" class="ltx_para">
<p id="Sx2.p5.1" class="ltx_p"><math id="Sx2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p5.1.m1.1a"><mo id="Sx2.p5.1.m1.1.1" xref="Sx2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p5.1.m1.1b"><ci id="Sx2.p5.1.m1.1.1.cmml" xref="Sx2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p5.1.m1.1c">\bullet</annotation></semantics></math> Yuhao Wang: implement the experiments for evaluation on interaction with environment tasks.</p>
</div>
<div id="Sx2.p6" class="ltx_para">
<p id="Sx2.p6.1" class="ltx_p"><math id="Sx2.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p6.1.m1.1a"><mo id="Sx2.p6.1.m1.1.1" xref="Sx2.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p6.1.m1.1b"><ci id="Sx2.p6.1.m1.1.1.cmml" xref="Sx2.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p6.1.m1.1c">\bullet</annotation></semantics></math> Bowen Zheng: implement the experiments for evaluation on tool manipulation tasks.</p>
</div>
<div id="Sx2.p7" class="ltx_para ltx_noindent">
<p id="Sx2.p7.1" class="ltx_p"><span id="Sx2.p7.1.1" class="ltx_text ltx_font_bold">Contribution on Tips.</span>
We list the following guys for their contributions on the corresponding numbers of provided tips for designing prompts in Table&nbsp;<a href="#S6.T12" title="TABLE XII ‣ 6.1.1 Prompt Creation ‣ 6.1 Prompting ‣ 6 Utilization ‣ A Survey of Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XII</span></a>.</p>
</div>
<div id="Sx2.p8" class="ltx_para">
<p id="Sx2.p8.1" class="ltx_p"><math id="Sx2.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p8.1.m1.1a"><mo id="Sx2.p8.1.m1.1.1" xref="Sx2.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p8.1.m1.1b"><ci id="Sx2.p8.1.m1.1.1.cmml" xref="Sx2.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p8.1.m1.1c">\bullet</annotation></semantics></math> Xiaolei Wang: T3, O3</p>
</div>
<div id="Sx2.p9" class="ltx_para">
<p id="Sx2.p9.1" class="ltx_p"><math id="Sx2.p9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p9.1.m1.1a"><mo id="Sx2.p9.1.m1.1.1" xref="Sx2.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p9.1.m1.1b"><ci id="Sx2.p9.1.m1.1.1.cmml" xref="Sx2.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p9.1.m1.1c">\bullet</annotation></semantics></math> Beichen Zhang: D2, D5</p>
</div>
<div id="Sx2.p10" class="ltx_para">
<p id="Sx2.p10.1" class="ltx_p"><math id="Sx2.p10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p10.1.m1.1a"><mo id="Sx2.p10.1.m1.1.1" xref="Sx2.p10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p10.1.m1.1b"><ci id="Sx2.p10.1.m1.1.1.cmml" xref="Sx2.p10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p10.1.m1.1c">\bullet</annotation></semantics></math> Zhipeng Chen: D3, D4</p>
</div>
<div id="Sx2.p11" class="ltx_para">
<p id="Sx2.p11.1" class="ltx_p"><math id="Sx2.p11.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p11.1.m1.1a"><mo id="Sx2.p11.1.m1.1.1" xref="Sx2.p11.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p11.1.m1.1b"><ci id="Sx2.p11.1.m1.1.1.cmml" xref="Sx2.p11.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p11.1.m1.1c">\bullet</annotation></semantics></math> Junjie Zhang: D6</p>
</div>
<div id="Sx2.p12" class="ltx_para">
<p id="Sx2.p12.1" class="ltx_p"><math id="Sx2.p12.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p12.1.m1.1a"><mo id="Sx2.p12.1.m1.1.1" xref="Sx2.p12.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p12.1.m1.1b"><ci id="Sx2.p12.1.m1.1.1.cmml" xref="Sx2.p12.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p12.1.m1.1c">\bullet</annotation></semantics></math> Bowen Zheng: D7</p>
</div>
<div id="Sx2.p13" class="ltx_para">
<p id="Sx2.p13.1" class="ltx_p"><math id="Sx2.p13.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p13.1.m1.1a"><mo id="Sx2.p13.1.m1.1.1" xref="Sx2.p13.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p13.1.m1.1b"><ci id="Sx2.p13.1.m1.1.1.cmml" xref="Sx2.p13.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p13.1.m1.1c">\bullet</annotation></semantics></math> Zican Dong: D8</p>
</div>
<div id="Sx2.p14" class="ltx_para">
<p id="Sx2.p14.1" class="ltx_p"><math id="Sx2.p14.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p14.1.m1.1a"><mo id="Sx2.p14.1.m1.1.1" xref="Sx2.p14.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p14.1.m1.1b"><ci id="Sx2.p14.1.m1.1.1.cmml" xref="Sx2.p14.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p14.1.m1.1c">\bullet</annotation></semantics></math> Xinyu Tang: C2</p>
</div>
<div id="Sx2.p15" class="ltx_para">
<p id="Sx2.p15.1" class="ltx_p"><math id="Sx2.p15.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p15.1.m1.1a"><mo id="Sx2.p15.1.m1.1.1" xref="Sx2.p15.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p15.1.m1.1b"><ci id="Sx2.p15.1.m1.1.1.cmml" xref="Sx2.p15.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p15.1.m1.1c">\bullet</annotation></semantics></math> Yifan Du: T4</p>
</div>
<div id="Sx2.p16" class="ltx_para">
<p id="Sx2.p16.1" class="ltx_p"><math id="Sx2.p16.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p16.1.m1.1a"><mo id="Sx2.p16.1.m1.1.1" xref="Sx2.p16.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p16.1.m1.1b"><ci id="Sx2.p16.1.m1.1.1.cmml" xref="Sx2.p16.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p16.1.m1.1c">\bullet</annotation></semantics></math> Tianyi Tang: O6, O7, D9</p>
</div>
<div id="Sx2.p17" class="ltx_para">
<p id="Sx2.p17.1" class="ltx_p"><math id="Sx2.p17.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p17.1.m1.1a"><mo id="Sx2.p17.1.m1.1.1" xref="Sx2.p17.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p17.1.m1.1b"><ci id="Sx2.p17.1.m1.1.1.cmml" xref="Sx2.p17.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p17.1.m1.1c">\bullet</annotation></semantics></math> Yupeng Hou: O8, C3</p>
</div>
<div id="Sx2.p18" class="ltx_para">
<p id="Sx2.p18.1" class="ltx_p"><math id="Sx2.p18.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="Sx2.p18.1.m1.1a"><mo id="Sx2.p18.1.m1.1.1" xref="Sx2.p18.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="Sx2.p18.1.m1.1b"><ci id="Sx2.p18.1.m1.1.1.cmml" xref="Sx2.p18.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p18.1.m1.1c">\bullet</annotation></semantics></math> Salvatore Raieli: C4</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bengio, R.&nbsp;Ducharme, P.&nbsp;Vincent, and C.&nbsp;Janvin, “A neural probabilistic
language model,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, vol.&nbsp;3, pp. 1137–1155, 2003.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R.&nbsp;Collobert, J.&nbsp;Weston, L.&nbsp;Bottou, M.&nbsp;Karlen, K.&nbsp;Kavukcuoglu, and P.&nbsp;P. Kuksa,
“Natural language processing (almost) from scratch,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn.
Res.</em>, vol.&nbsp;12, pp. 2493–2537, 2011.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.&nbsp;Pinker, <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">The Language Instinct: How the Mind Creates Language</em>.&nbsp;&nbsp;&nbsp;Brilliance Audio; Unabridged edition, 2014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M.&nbsp;D. Hauser, N.&nbsp;Chomsky, and W.&nbsp;T. Fitch, “The faculty of language: what is
it, who has it, and how did it evolve?” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">science</em>, vol. 298, no. 5598,
pp. 1569–1579, 2002.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.&nbsp;M. Turing, “Computing machinery and intelligence,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Mind</em>, vol.
LIX, no. 236, pp. 433–460, 1950.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F.&nbsp;Jelinek, <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Statistical Methods for Speech Recognition</em>.&nbsp;&nbsp;&nbsp;MIT Press, 1998.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J.&nbsp;Gao and C.&nbsp;Lin, “Introduction to the special issue on statistical language
modeling,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Asian Lang. Inf. Process.</em>, vol.&nbsp;3, no.&nbsp;2, pp.
87–93, 2004.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R.&nbsp;Rosenfeld, “Two decades of statistical language modeling: Where do we go
from here?” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol.&nbsp;88, no.&nbsp;8, pp. 1270–1278,
2000.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.&nbsp;Stolcke, “Srilm-an extensible language modeling toolkit,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Seventh
international conference on spoken language processing</em>, 2002.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X.&nbsp;Liu and W.&nbsp;B. Croft, “Statistical language modeling for information
retrieval,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Annu. Rev. Inf. Sci. Technol.</em>, vol.&nbsp;39, no.&nbsp;1, pp. 1–31,
2005.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C.&nbsp;Zhai, <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Statistical Language Models for Information Retrieval</em>, ser.
Synthesis Lectures on Human Language Technologies.&nbsp;&nbsp;&nbsp;Morgan &amp; Claypool Publishers, 2008.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Thede and M.&nbsp;P. Harper, “A second-order hidden markov model for
part-of-speech tagging,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">27th Annual Meeting of the Association for
Computational Linguistics, University of Maryland, College Park, Maryland,
USA, 20-26 June 1999</em>, R.&nbsp;Dale and K.&nbsp;W. Church, Eds.&nbsp;&nbsp;&nbsp;ACL, 1999, pp. 175–182.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L.&nbsp;R. Bahl, P.&nbsp;F. Brown, P.&nbsp;V. de&nbsp;Souza, and R.&nbsp;L. Mercer, “A tree-based
statistical language model for natural language speech recognition,”
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Acoustics, Speech, and Signal Processing</em>,
vol.&nbsp;37, no.&nbsp;7, pp. 1001–1008, 1989.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T.&nbsp;Brants, A.&nbsp;C. Popat, P.&nbsp;Xu, F.&nbsp;J. Och, and J.&nbsp;Dean, “Large language models
in machine translation,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">EMNLP-CoNLL 2007, Proceedings of the 2007
Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning, June 28-30, 2007, Prague, Czech
Republic</em>, J.&nbsp;Eisner, Ed.&nbsp;&nbsp;&nbsp;ACL, 2007,
pp. 858–867.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Katz, “Estimation of probabilities from sparse data for the language
model component of a speech recognizer,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Acoust. Speech
Signal Process.</em>, vol.&nbsp;35, no.&nbsp;3, pp. 400–401, 1987.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
W.&nbsp;A. Gale and G.&nbsp;Sampson, “Good-turing frequency estimation without tears,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">J. Quant. Linguistics</em>, vol.&nbsp;2, no.&nbsp;3, pp. 217–237, 1995.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T.&nbsp;Mikolov, M.&nbsp;Karafiát, L.&nbsp;Burget, J.&nbsp;Cernocký, and S.&nbsp;Khudanpur,
“Recurrent neural network based language model,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH
2010, 11th Annual Conference of the International Speech Communication
Association, Makuhari, Chiba, Japan, September 26-30, 2010</em>, T.&nbsp;Kobayashi,
K.&nbsp;Hirose, and S.&nbsp;Nakamura, Eds.&nbsp;&nbsp;&nbsp;ISCA, 2010, pp. 1045–1048.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.&nbsp;Kombrink, T.&nbsp;Mikolov, M.&nbsp;Karafiát, and L.&nbsp;Burget, “Recurrent neural
network based language modeling in meeting recognition,” in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH 2011, 12th Annual Conference of the International Speech
Communication Association, Florence, Italy, August 27-31, 2011</em>.&nbsp;&nbsp;&nbsp;ISCA, 2011, pp. 2877–2880.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T.&nbsp;Mikolov, I.&nbsp;Sutskever, K.&nbsp;Chen, G.&nbsp;S. Corrado, and J.&nbsp;Dean, “Distributed
representations of words and phrases and their compositionality,” in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Systems 2013. Proceedings of a
meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States</em>, C.&nbsp;J.&nbsp;C.
Burges, L.&nbsp;Bottou, Z.&nbsp;Ghahramani, and K.&nbsp;Q. Weinberger, Eds., 2013, pp.
3111–3119.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T.&nbsp;Mikolov, K.&nbsp;Chen, G.&nbsp;Corrado, and J.&nbsp;Dean, “Efficient estimation of word
representations in vector space,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">1st International Conference on
Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4,
2013, Workshop Track Proceedings</em>, Y.&nbsp;Bengio and Y.&nbsp;LeCun, Eds., 2013.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M.&nbsp;E. Peters, M.&nbsp;Neumann, M.&nbsp;Iyyer, M.&nbsp;Gardner, C.&nbsp;Clark, K.&nbsp;Lee, and
L.&nbsp;Zettlemoyer, “Deep contextualized word representations,” in
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long
Papers)</em>, M.&nbsp;A. Walker, H.&nbsp;Ji, and A.&nbsp;Stent, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2018, pp. 2227–2237.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.&nbsp;Vaswani, N.&nbsp;Shazeer, N.&nbsp;Parmar, J.&nbsp;Uszkoreit, L.&nbsp;Jones, A.&nbsp;N. Gomez,
L.&nbsp;Kaiser, and I.&nbsp;Polosukhin, “Attention is all you need,” in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA</em>, 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J.&nbsp;Devlin, M.&nbsp;Chang, K.&nbsp;Lee, and K.&nbsp;Toutanova, “BERT: pre-training of deep
bidirectional transformers for language understanding,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>,
J.&nbsp;Burstein, C.&nbsp;Doran, and T.&nbsp;Solorio, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2019, pp. 4171–4186.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M.&nbsp;Lewis, Y.&nbsp;Liu, N.&nbsp;Goyal, M.&nbsp;Ghazvininejad, A.&nbsp;Mohamed, O.&nbsp;Levy, V.&nbsp;Stoyanov,
and L.&nbsp;Zettlemoyer, “BART: denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, 2020, pp.
7871–7880.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
W.&nbsp;Fedus, B.&nbsp;Zoph, and N.&nbsp;Shazeer, “Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn.
Res</em>, pp. 1–40, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;Wu, R.&nbsp;Child, D.&nbsp;Luan, D.&nbsp;Amodei, I.&nbsp;Sutskever <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>,
“Language models are unsupervised multitask learners,” <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">OpenAI blog</em>,
p.&nbsp;9, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, M.&nbsp;Ott, N.&nbsp;Goyal, J.&nbsp;Du, M.&nbsp;Joshi, D.&nbsp;Chen, O.&nbsp;Levy, M.&nbsp;Lewis,
L.&nbsp;Zettlemoyer, and V.&nbsp;Stoyanov, “Roberta: A robustly optimized BERT
pretraining approach,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1907.11692, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
V.&nbsp;Sanh, A.&nbsp;Webson, C.&nbsp;Raffel, S.&nbsp;H. Bach, L.&nbsp;Sutawika, Z.&nbsp;Alyafeai,
A.&nbsp;Chaffin, A.&nbsp;Stiegler, A.&nbsp;Raja, M.&nbsp;Dey, M.&nbsp;S. Bari, C.&nbsp;Xu, U.&nbsp;Thakker,
S.&nbsp;S. Sharma, E.&nbsp;Szczechla, T.&nbsp;Kim, G.&nbsp;Chhablani, N.&nbsp;V. Nayak, D.&nbsp;Datta,
J.&nbsp;Chang, M.&nbsp;T. Jiang, H.&nbsp;Wang, M.&nbsp;Manica, S.&nbsp;Shen, Z.&nbsp;X. Yong, H.&nbsp;Pandey,
R.&nbsp;Bawden, T.&nbsp;Wang, T.&nbsp;Neeraj, J.&nbsp;Rozen, A.&nbsp;Sharma, A.&nbsp;Santilli,
T.&nbsp;Févry, J.&nbsp;A. Fries, R.&nbsp;Teehan, T.&nbsp;L. Scao, S.&nbsp;Biderman, L.&nbsp;Gao,
T.&nbsp;Wolf, and A.&nbsp;M. Rush, “Multitask prompted training enables zero-shot task
generalization,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T.&nbsp;Wang, A.&nbsp;Roberts, D.&nbsp;Hesslow, T.&nbsp;L. Scao, H.&nbsp;W. Chung, I.&nbsp;Beltagy,
J.&nbsp;Launay, and C.&nbsp;Raffel, “What language model architecture and pretraining
objective works best for zero-shot generalization?” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
Maryland, USA</em>, ser. Proceedings of Machine Learning Research, vol. 162,
2022, pp. 22 964–22 984.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child,
S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei, “Scaling laws for neural language
models,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2001.08361, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, Y.&nbsp;Tay, R.&nbsp;Bommasani, C.&nbsp;Raffel, B.&nbsp;Zoph, S.&nbsp;Borgeaud, D.&nbsp;Yogatama,
M.&nbsp;Bosma, D.&nbsp;Zhou, D.&nbsp;Metzler, E.&nbsp;H. Chi, T.&nbsp;Hashimoto, O.&nbsp;Vinyals, P.&nbsp;Liang,
J.&nbsp;Dean, and W.&nbsp;Fedus, “Emergent abilities of large language models,”
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2206.07682, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M.&nbsp;Shanahan, “Talking about large language models,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2212.03551, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, X.&nbsp;Wang, D.&nbsp;Schuurmans, M.&nbsp;Bosma, E.&nbsp;H. Chi, Q.&nbsp;Le, and D.&nbsp;Zhou,
“Chain of thought prompting elicits reasoning in large language models,”
<em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2201.11903, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford,
D.&nbsp;de&nbsp;Las&nbsp;Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark, T.&nbsp;Hennigan, E.&nbsp;Noland,
K.&nbsp;Millican, G.&nbsp;van&nbsp;den Driessche, B.&nbsp;Damoc, A.&nbsp;Guy, S.&nbsp;Osindero,
K.&nbsp;Simonyan, E.&nbsp;Elsen, J.&nbsp;W. Rae, O.&nbsp;Vinyals, and L.&nbsp;Sifre, “Training
compute-optimal large language models,” vol. abs/2203.15556, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
R.&nbsp;Taylor, M.&nbsp;Kardas, G.&nbsp;Cucurull, T.&nbsp;Scialom, A.&nbsp;Hartshorn, E.&nbsp;Saravia,
A.&nbsp;Poulton, V.&nbsp;Kerkez, and R.&nbsp;Stojnic, “Galactica: A large language model
for science,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.09085, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
P.&nbsp;Liu, W.&nbsp;Yuan, J.&nbsp;Fu, Z.&nbsp;Jiang, H.&nbsp;Hayashi, and G.&nbsp;Neubig, “Pre-train,
prompt, and predict: A systematic survey of prompting methods in natural
language processing,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, pp. 195:1–195:35, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
C.&nbsp;Zhou, Q.&nbsp;Li, C.&nbsp;Li, J.&nbsp;Yu, Y.&nbsp;Liu, G.&nbsp;Wang, K.&nbsp;Zhang, C.&nbsp;Ji, Q.&nbsp;Yan, L.&nbsp;He,
H.&nbsp;Peng, J.&nbsp;Li, J.&nbsp;Wu, Z.&nbsp;Liu, P.&nbsp;Xie, C.&nbsp;Xiong, J.&nbsp;Pei, P.&nbsp;S. Yu, and
L.&nbsp;Sun, “A comprehensive survey on pretrained foundation models: A history
from BERT to chatgpt,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.09419, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
X.&nbsp;Han, Z.&nbsp;Zhang, N.&nbsp;Ding, Y.&nbsp;Gu, X.&nbsp;Liu, Y.&nbsp;Huo, J.&nbsp;Qiu, Y.&nbsp;Yao, A.&nbsp;Zhang,
L.&nbsp;Zhang, W.&nbsp;Han, M.&nbsp;Huang, Q.&nbsp;Jin, Y.&nbsp;Lan, Y.&nbsp;Liu, Z.&nbsp;Liu, Z.&nbsp;Lu, X.&nbsp;Qiu,
R.&nbsp;Song, J.&nbsp;Tang, J.&nbsp;Wen, J.&nbsp;Yuan, W.&nbsp;X. Zhao, and J.&nbsp;Zhu, “Pre-trained
models: Past, present and future,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">AI Open</em>, vol.&nbsp;2, pp. 225–250,
2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
X.&nbsp;Qiu, T.&nbsp;Sun, Y.&nbsp;Xu, Y.&nbsp;Shao, N.&nbsp;Dai, and X.&nbsp;Huang, “Pre-trained models for
natural language processing: A survey,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2003.08271,
2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S.&nbsp;Altman, “Planning for agi and beyond,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, February 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
S.&nbsp;Bubeck, V.&nbsp;Chandrasekaran, R.&nbsp;Eldan, J.&nbsp;Gehrke, E.&nbsp;Horvitz, E.&nbsp;Kamar,
P.&nbsp;Lee, Y.&nbsp;T. Lee, Y.&nbsp;Li, S.&nbsp;Lundberg, H.&nbsp;Nori, H.&nbsp;Palangi, M.&nbsp;T. Ribeiro,
and Y.&nbsp;Zhang, “Sparks of artificial general intelligence: Early experiments
with gpt-4,” vol. abs/2303.12712, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S.&nbsp;Huang, L.&nbsp;Dong, W.&nbsp;Wang, Y.&nbsp;Hao, S.&nbsp;Singhal, S.&nbsp;Ma, T.&nbsp;Lv, L.&nbsp;Cui, O.&nbsp;K.
Mohammed, B.&nbsp;Patra, Q.&nbsp;Liu, K.&nbsp;Aggarwal, Z.&nbsp;Chi, J.&nbsp;Bjorck, V.&nbsp;Chaudhary,
S.&nbsp;Som, X.&nbsp;Song, and F.&nbsp;Wei, “Language is not all you need: Aligning
perception with language models,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.14045, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Y.&nbsp;Cao, S.&nbsp;Li, Y.&nbsp;Liu, Z.&nbsp;Yan, Y.&nbsp;Dai, P.&nbsp;S. Yu, and L.&nbsp;Sun, “A comprehensive
survey of ai-generated content (aigc): A history of generative ai from gan to
chatgpt,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04226</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D.&nbsp;Driess, F.&nbsp;Xia, M.&nbsp;S. Sajjadi, C.&nbsp;Lynch, A.&nbsp;Chowdhery, B.&nbsp;Ichter, A.&nbsp;Wahid,
J.&nbsp;Tompson, Q.&nbsp;Vuong, T.&nbsp;Yu <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Palm-e: An embodied multimodal
language model,” <em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.03378</em>, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
C.&nbsp;Wu, S.&nbsp;Yin, W.&nbsp;Qi, X.&nbsp;Wang, Z.&nbsp;Tang, and N.&nbsp;Duan, “Visual chatgpt: Talking,
drawing and editing with visual foundation models,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2303.04671</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
OpenAI, “Gpt-4 technical report,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">OpenAI</em>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Y.&nbsp;Fu, H.&nbsp;Peng, and T.&nbsp;Khot, “How does gpt obtain its ability? tracing
emergent abilities of language models to their sources,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Yao Fu’s
Notion</em>, Dec 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J.&nbsp;Li, T.&nbsp;Tang, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “Pretrained language model for text
generation: A survey,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirtieth International
Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event /
Montreal, Canada, 19-27 August 2021</em>, Z.&nbsp;Zhou, Ed.&nbsp;&nbsp;&nbsp;ijcai.org, 2021, pp. 4492–4499.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
P.&nbsp;Lu, L.&nbsp;Qiu, W.&nbsp;Yu, S.&nbsp;Welleck, and K.&nbsp;Chang, “A survey of deep learning for
mathematical reasoning,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.10535, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Q.&nbsp;Dong, L.&nbsp;Li, D.&nbsp;Dai, C.&nbsp;Zheng, Z.&nbsp;Wu, B.&nbsp;Chang, X.&nbsp;Sun, J.&nbsp;Xu, L.&nbsp;Li, and
Z.&nbsp;Sui, “A survey for in-context learning,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2301.00234, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J.&nbsp;Huang and K.&nbsp;C. Chang, “Towards reasoning in large language models: A
survey,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.10403, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S.&nbsp;Qiao, Y.&nbsp;Ou, N.&nbsp;Zhang, X.&nbsp;Chen, Y.&nbsp;Yao, S.&nbsp;Deng, C.&nbsp;Tan, F.&nbsp;Huang, and
H.&nbsp;Chen, “Reasoning with language model prompting: A survey,”
<em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.09597, 2022.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
J.&nbsp;Zhou, P.&nbsp;Ke, X.&nbsp;Qiu, M.&nbsp;Huang, and J.&nbsp;Zhang, “Chatgpt: potential,
prospects, and limitations,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Frontiers of Information Technology &amp;
Electronic Engineering</em>, 2023, pp. 1–6.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
W.&nbsp;X. Zhao, J.&nbsp;Liu, R.&nbsp;Ren, and J.&nbsp;Wen, “Dense text retrieval based on
pretrained language models: A survey,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.14876,
2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
T.&nbsp;B. Brown, B.&nbsp;Mann, N.&nbsp;Ryder, M.&nbsp;Subbiah, J.&nbsp;Kaplan, P.&nbsp;Dhariwal,
A.&nbsp;Neelakantan, P.&nbsp;Shyam, G.&nbsp;Sastry, A.&nbsp;Askell, S.&nbsp;Agarwal,
A.&nbsp;Herbert-Voss, G.&nbsp;Krueger, T.&nbsp;Henighan, R.&nbsp;Child, A.&nbsp;Ramesh, D.&nbsp;M.
Ziegler, J.&nbsp;Wu, C.&nbsp;Winter, C.&nbsp;Hesse, M.&nbsp;Chen, E.&nbsp;Sigler, M.&nbsp;Litwin, S.&nbsp;Gray,
B.&nbsp;Chess, J.&nbsp;Clark, C.&nbsp;Berner, S.&nbsp;McCandlish, A.&nbsp;Radford, I.&nbsp;Sutskever, and
D.&nbsp;Amodei, “Language models are few-shot learners,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual</em>, H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin, Eds.,
2020.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A.&nbsp;Chowdhery, S.&nbsp;Narang, J.&nbsp;Devlin, M.&nbsp;Bosma, G.&nbsp;Mishra, A.&nbsp;Roberts, P.&nbsp;Barham,
H.&nbsp;W. Chung, C.&nbsp;Sutton, S.&nbsp;Gehrmann, P.&nbsp;Schuh, K.&nbsp;Shi, S.&nbsp;Tsvyashchenko,
J.&nbsp;Maynez, A.&nbsp;Rao, P.&nbsp;Barnes, Y.&nbsp;Tay, N.&nbsp;Shazeer, V.&nbsp;Prabhakaran, E.&nbsp;Reif,
N.&nbsp;Du, B.&nbsp;Hutchinson, R.&nbsp;Pope, J.&nbsp;Bradbury, J.&nbsp;Austin, M.&nbsp;Isard,
G.&nbsp;Gur-Ari, P.&nbsp;Yin, T.&nbsp;Duke, A.&nbsp;Levskaya, S.&nbsp;Ghemawat, S.&nbsp;Dev,
H.&nbsp;Michalewski, X.&nbsp;Garcia, V.&nbsp;Misra, K.&nbsp;Robinson, L.&nbsp;Fedus, D.&nbsp;Zhou,
D.&nbsp;Ippolito, D.&nbsp;Luan, H.&nbsp;Lim, B.&nbsp;Zoph, A.&nbsp;Spiridonov, R.&nbsp;Sepassi, D.&nbsp;Dohan,
S.&nbsp;Agrawal, M.&nbsp;Omernick, A.&nbsp;M. Dai, T.&nbsp;S. Pillai, M.&nbsp;Pellat, A.&nbsp;Lewkowycz,
E.&nbsp;Moreira, R.&nbsp;Child, O.&nbsp;Polozov, K.&nbsp;Lee, Z.&nbsp;Zhou, X.&nbsp;Wang, B.&nbsp;Saeta,
M.&nbsp;Diaz, O.&nbsp;Firat, M.&nbsp;Catasta, J.&nbsp;Wei, K.&nbsp;Meier-Hellstern, D.&nbsp;Eck, J.&nbsp;Dean,
S.&nbsp;Petrov, and N.&nbsp;Fiedel, “Palm: Scaling language modeling with pathways,”
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2204.02311, 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.&nbsp;Lachaux, T.&nbsp;Lacroix,
B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar, A.&nbsp;Rodriguez, A.&nbsp;Joulin,
E.&nbsp;Grave, and G.&nbsp;Lample, “Llama: Open and efficient foundation language
models,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
T.&nbsp;Henighan, J.&nbsp;Kaplan, M.&nbsp;Katz, M.&nbsp;Chen, C.&nbsp;Hesse, J.&nbsp;Jackson, H.&nbsp;Jun, T.&nbsp;B.
Brown, P.&nbsp;Dhariwal, S.&nbsp;Gray <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Scaling laws for autoregressive
generative modeling,” <em id="bib.bib58.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.14701</em>, 2020.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Xie, H.&nbsp;Pham, X.&nbsp;Dong, N.&nbsp;Du, H.&nbsp;Liu, Y.&nbsp;Lu, P.&nbsp;Liang, Q.&nbsp;V. Le, T.&nbsp;Ma,
and A.&nbsp;W. Yu, “Doremi: Optimizing data mixtures speeds up language model
pretraining,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10429</em>, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
P.&nbsp;Villalobos, J.&nbsp;Sevilla, L.&nbsp;Heim, T.&nbsp;Besiroglu, M.&nbsp;Hobbhahn, and A.&nbsp;Ho,
“Will we run out of data? an analysis of the limits of scaling datasets in
machine learning,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.04325, 2022. [Online].
Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2211.04325" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2211.04325</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
N.&nbsp;Muennighoff, A.&nbsp;M. Rush, B.&nbsp;Barak, T.&nbsp;L. Scao, A.&nbsp;Piktus, N.&nbsp;Tazi,
S.&nbsp;Pyysalo, T.&nbsp;Wolf, and C.&nbsp;Raffel, “Scaling data-constrained language
models,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16264</em>, 2023.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
I.&nbsp;McKenzie, A.&nbsp;Lyzhov, A.&nbsp;Parrish, A.&nbsp;Prabhu, A.&nbsp;Mueller, N.&nbsp;Kim, S.&nbsp;Bowman,
and E.&nbsp;Perez, “The inverse scaling prize,” 2022. [Online]. Available:
<a target="_blank" href="https://github.com/inverse-scaling/prize" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/inverse-scaling/prize</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
B.&nbsp;A. Huberman and T.&nbsp;Hogg, “Phase transitions in artificial intelligence
systems,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence</em>, vol.&nbsp;33, no.&nbsp;2, pp. 155–171,
1987.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, S.&nbsp;Borgeaud, T.&nbsp;Cai, K.&nbsp;Millican, J.&nbsp;Hoffmann, H.&nbsp;F. Song,
J.&nbsp;Aslanides, S.&nbsp;Henderson, R.&nbsp;Ring, S.&nbsp;Young, E.&nbsp;Rutherford, T.&nbsp;Hennigan,
J.&nbsp;Menick, A.&nbsp;Cassirer, R.&nbsp;Powell, G.&nbsp;van&nbsp;den Driessche, L.&nbsp;A. Hendricks,
M.&nbsp;Rauh, P.&nbsp;Huang, A.&nbsp;Glaese, J.&nbsp;Welbl, S.&nbsp;Dathathri, S.&nbsp;Huang, J.&nbsp;Uesato,
J.&nbsp;Mellor, I.&nbsp;Higgins, A.&nbsp;Creswell, N.&nbsp;McAleese, A.&nbsp;Wu, E.&nbsp;Elsen, S.&nbsp;M.
Jayakumar, E.&nbsp;Buchatskaya, D.&nbsp;Budden, E.&nbsp;Sutherland, K.&nbsp;Simonyan,
M.&nbsp;Paganini, L.&nbsp;Sifre, L.&nbsp;Martens, X.&nbsp;L. Li, A.&nbsp;Kuncoro, A.&nbsp;Nematzadeh,
E.&nbsp;Gribovskaya, D.&nbsp;Donato, A.&nbsp;Lazaridou, A.&nbsp;Mensch, J.&nbsp;Lespiau,
M.&nbsp;Tsimpoukelli, N.&nbsp;Grigorev, D.&nbsp;Fritz, T.&nbsp;Sottiaux, M.&nbsp;Pajarskas, T.&nbsp;Pohlen,
Z.&nbsp;Gong, D.&nbsp;Toyama, C.&nbsp;de&nbsp;Masson&nbsp;d’Autume, Y.&nbsp;Li, T.&nbsp;Terzi, V.&nbsp;Mikulik,
I.&nbsp;Babuschkin, A.&nbsp;Clark, D.&nbsp;de&nbsp;Las&nbsp;Casas, A.&nbsp;Guy, C.&nbsp;Jones, J.&nbsp;Bradbury,
M.&nbsp;J. Johnson, B.&nbsp;A. Hechtman, L.&nbsp;Weidinger, I.&nbsp;Gabriel, W.&nbsp;S. Isaac,
E.&nbsp;Lockhart, S.&nbsp;Osindero, L.&nbsp;Rimell, C.&nbsp;Dyer, O.&nbsp;Vinyals, K.&nbsp;Ayoub,
J.&nbsp;Stanway, L.&nbsp;Bennett, D.&nbsp;Hassabis, K.&nbsp;Kavukcuoglu, and G.&nbsp;Irving, “Scaling
language models: Methods, analysis &amp; insights from training gopher,”
<em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2112.11446, 2021.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
D.&nbsp;Dai, Y.&nbsp;Sun, L.&nbsp;Dong, Y.&nbsp;Hao, Z.&nbsp;Sui, and F.&nbsp;Wei, “Why can GPT learn
in-context? language models secretly perform gradient descent as
meta-optimizers,” <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.10559, 2022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
L.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;L. Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang,
S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray, J.&nbsp;Schulman, J.&nbsp;Hilton, F.&nbsp;Kelton, L.&nbsp;Miller,
M.&nbsp;Simens, A.&nbsp;Askell, P.&nbsp;Welinder, P.&nbsp;F. Christiano, J.&nbsp;Leike, and R.&nbsp;Lowe,
“Training language models to follow instructions with human feedback,”
<em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2203.02155, 2022.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, M.&nbsp;Bosma, V.&nbsp;Y. Zhao, K.&nbsp;Guu, A.&nbsp;W. Yu, B.&nbsp;Lester, N.&nbsp;Du, A.&nbsp;M. Dai,
and Q.&nbsp;V. Le, “Finetuned language models are zero-shot learners,” in
<em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
R.&nbsp;Thoppilan, D.&nbsp;D. Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.&nbsp;Cheng,
A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du, Y.&nbsp;Li, H.&nbsp;Lee, H.&nbsp;S. Zheng, A.&nbsp;Ghafouri,
M.&nbsp;Menegali, Y.&nbsp;Huang, M.&nbsp;Krikun, D.&nbsp;Lepikhin, J.&nbsp;Qin, D.&nbsp;Chen, Y.&nbsp;Xu,
Z.&nbsp;Chen, A.&nbsp;Roberts, M.&nbsp;Bosma, Y.&nbsp;Zhou, C.&nbsp;Chang, I.&nbsp;Krivokon, W.&nbsp;Rusch,
M.&nbsp;Pickett, K.&nbsp;S. Meier-Hellstern, M.&nbsp;R. Morris, T.&nbsp;Doshi, R.&nbsp;D. Santos,
T.&nbsp;Duke, J.&nbsp;Soraker, B.&nbsp;Zevenbergen, V.&nbsp;Prabhakaran, M.&nbsp;Diaz, B.&nbsp;Hutchinson,
K.&nbsp;Olson, A.&nbsp;Molina, E.&nbsp;Hoffman-John, J.&nbsp;Lee, L.&nbsp;Aroyo, R.&nbsp;Rajakumar,
A.&nbsp;Butryna, M.&nbsp;Lamm, V.&nbsp;Kuzmina, J.&nbsp;Fenton, A.&nbsp;Cohen, R.&nbsp;Bernstein,
R.&nbsp;Kurzweil, B.&nbsp;Aguera-Arcas, C.&nbsp;Cui, M.&nbsp;Croak, E.&nbsp;H. Chi, and Q.&nbsp;Le,
“Lamda: Language models for dialog applications,” <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2201.08239, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
H.&nbsp;W. Chung, L.&nbsp;Hou, S.&nbsp;Longpre, B.&nbsp;Zoph, Y.&nbsp;Tay, W.&nbsp;Fedus, E.&nbsp;Li, X.&nbsp;Wang,
M.&nbsp;Dehghani, S.&nbsp;Brahma, A.&nbsp;Webson, S.&nbsp;S. Gu, Z.&nbsp;Dai, M.&nbsp;Suzgun, X.&nbsp;Chen,
A.&nbsp;Chowdhery, S.&nbsp;Narang, G.&nbsp;Mishra, A.&nbsp;Yu, V.&nbsp;Y. Zhao, Y.&nbsp;Huang, A.&nbsp;M. Dai,
H.&nbsp;Yu, S.&nbsp;Petrov, E.&nbsp;H. Chi, J.&nbsp;Dean, J.&nbsp;Devlin, A.&nbsp;Roberts, D.&nbsp;Zhou, Q.&nbsp;V.
Le, and J.&nbsp;Wei, “Scaling instruction-finetuned language models,”
<em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.11416, 2022.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
A.&nbsp;Srivastava, A.&nbsp;Rastogi, A.&nbsp;Rao, A.&nbsp;A.&nbsp;M. Shoeb, A.&nbsp;Abid, A.&nbsp;Fisch, A.&nbsp;R.
Brown, A.&nbsp;Santoro, A.&nbsp;Gupta, A.&nbsp;Garriga-Alonso, A.&nbsp;Kluska, A.&nbsp;Lewkowycz,
A.&nbsp;Agarwal, A.&nbsp;Power, A.&nbsp;Ray, A.&nbsp;Warstadt, A.&nbsp;W. Kocurek, A.&nbsp;Safaya,
A.&nbsp;Tazarv, A.&nbsp;Xiang, A.&nbsp;Parrish, A.&nbsp;Nie, A.&nbsp;Hussain, A.&nbsp;Askell, A.&nbsp;Dsouza,
A.&nbsp;Rahane, A.&nbsp;S. Iyer, A.&nbsp;Andreassen, A.&nbsp;Santilli, A.&nbsp;Stuhlmüller,
A.&nbsp;M. Dai, A.&nbsp;La, A.&nbsp;K. Lampinen, A.&nbsp;Zou, A.&nbsp;Jiang, A.&nbsp;Chen, A.&nbsp;Vuong,
A.&nbsp;Gupta, A.&nbsp;Gottardi, A.&nbsp;Norelli, A.&nbsp;Venkatesh, A.&nbsp;Gholamidavoodi,
A.&nbsp;Tabassum, A.&nbsp;Menezes, A.&nbsp;Kirubarajan, A.&nbsp;Mullokandov, A.&nbsp;Sabharwal,
A.&nbsp;Herrick, A.&nbsp;Efrat, A.&nbsp;Erdem, A.&nbsp;Karakas, and et&nbsp;al., “Beyond the
imitation game: Quantifying and extrapolating the capabilities of language
models,” <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2206.04615, 2022.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
R.&nbsp;Schaeffer, B.&nbsp;Miranda, and S.&nbsp;Koyejo, “Are emergent abilities of large
language models a mirage?” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.15004</em>, 2023.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
S.&nbsp;Hu, X.&nbsp;Liu, X.&nbsp;Han, X.&nbsp;Zhang, C.&nbsp;He, W.&nbsp;Zhao, Y.&nbsp;Lin, N.&nbsp;Ding, Z.&nbsp;Ou,
G.&nbsp;Zeng, Z.&nbsp;Liu, and M.&nbsp;Sun, “Unlock predictable scaling from emergent
abilities,” 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
A.&nbsp;Power, Y.&nbsp;Burda, H.&nbsp;Edwards, I.&nbsp;Babuschkin, and V.&nbsp;Misra, “Grokking:
Generalization beyond overfitting on small algorithmic datasets,”
<em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.02177</em>, 2022.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
J.&nbsp;Rasley, S.&nbsp;Rajbhandari, O.&nbsp;Ruwase, and Y.&nbsp;He, “Deepspeed: System
optimizations enable training deep learning models with over 100 billion
parameters,” in <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">KDD</em>, 2020, pp. 3505–3506.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
M.&nbsp;Shoeybi, M.&nbsp;Patwary, R.&nbsp;Puri, P.&nbsp;LeGresley, J.&nbsp;Casper, and B.&nbsp;Catanzaro,
“Megatron-lm: Training multi-billion parameter language models using model
parallelism,” <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1909.08053, 2019.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
D.&nbsp;Narayanan, M.&nbsp;Shoeybi, J.&nbsp;Casper, P.&nbsp;LeGresley, M.&nbsp;Patwary, V.&nbsp;Korthikanti,
D.&nbsp;Vainbrand, P.&nbsp;Kashinkunti, J.&nbsp;Bernauer, B.&nbsp;Catanzaro, A.&nbsp;Phanishayee, and
M.&nbsp;Zaharia, “Efficient large-scale language model training on GPU clusters
using megatron-lm,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">International Conference for High Performance
Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Missouri,
USA, November 14-19, 2021</em>.&nbsp;&nbsp;&nbsp;ACM,
2021, p.&nbsp;58.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
V.&nbsp;Korthikanti, J.&nbsp;Casper, S.&nbsp;Lym, L.&nbsp;McAfee, M.&nbsp;Andersch, M.&nbsp;Shoeybi, and
B.&nbsp;Catanzaro, “Reducing activation recomputation in large transformer
models,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2205.05198, 2022.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
T.&nbsp;L. Scao, A.&nbsp;Fan, C.&nbsp;Akiki, E.&nbsp;Pavlick, S.&nbsp;Ilic, D.&nbsp;Hesslow,
R.&nbsp;Castagné, A.&nbsp;S. Luccioni, F.&nbsp;Yvon, M.&nbsp;Gallé, J.&nbsp;Tow, A.&nbsp;M.
Rush, S.&nbsp;Biderman, A.&nbsp;Webson, P.&nbsp;S. Ammanamanchi, T.&nbsp;Wang, B.&nbsp;Sagot,
N.&nbsp;Muennighoff, A.&nbsp;V. del Moral, O.&nbsp;Ruwase, R.&nbsp;Bawden, S.&nbsp;Bekman,
A.&nbsp;McMillan-Major, I.&nbsp;Beltagy, H.&nbsp;Nguyen, L.&nbsp;Saulnier, S.&nbsp;Tan, P.&nbsp;O.
Suarez, V.&nbsp;Sanh, H.&nbsp;Laurençon, Y.&nbsp;Jernite, J.&nbsp;Launay, M.&nbsp;Mitchell,
C.&nbsp;Raffel, A.&nbsp;Gokaslan, A.&nbsp;Simhi, A.&nbsp;Soroa, A.&nbsp;F. Aji, A.&nbsp;Alfassy, A.&nbsp;Rogers,
A.&nbsp;K. Nitzav, C.&nbsp;Xu, C.&nbsp;Mou, C.&nbsp;Emezue, C.&nbsp;Klamm, C.&nbsp;Leong, D.&nbsp;van Strien,
D.&nbsp;I. Adelani, and et&nbsp;al., “BLOOM: A 176b-parameter open-access
multilingual language model,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.05100, 2022.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
P.&nbsp;F. Christiano, J.&nbsp;Leike, T.&nbsp;B. Brown, M.&nbsp;Martic, S.&nbsp;Legg, and D.&nbsp;Amodei,
“Deep reinforcement learning from human preferences,” in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA</em>, I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, Eds., 2017, pp. 4299–4307.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli,
L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom, “Toolformer: Language models
can teach themselves to use tools,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.04761, 2023.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
R.&nbsp;Nakano, J.&nbsp;Hilton, S.&nbsp;Balaji, J.&nbsp;Wu, L.&nbsp;Ouyang, C.&nbsp;Kim, C.&nbsp;Hesse, S.&nbsp;Jain,
V.&nbsp;Kosaraju, W.&nbsp;Saunders, X.&nbsp;Jiang, K.&nbsp;Cobbe, T.&nbsp;Eloundou, G.&nbsp;Krueger,
K.&nbsp;Button, M.&nbsp;Knight, B.&nbsp;Chess, and J.&nbsp;Schulman, “Webgpt: Browser-assisted
question-answering with human feedback,” <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2112.09332,
2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
C.&nbsp;Raffel, N.&nbsp;Shazeer, A.&nbsp;Roberts, K.&nbsp;Lee, S.&nbsp;Narang, M.&nbsp;Matena, Y.&nbsp;Zhou,
W.&nbsp;Li, and P.&nbsp;J. Liu, “Exploring the limits of transfer learning with a
unified text-to-text transformer,” <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, pp.
140:1–140:67, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
L.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua,
and C.&nbsp;Raffel, “mt5: A massively multilingual pre-trained text-to-text
transformer,” in <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, 2021, pp.
483–498.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
W.&nbsp;Zeng, X.&nbsp;Ren, T.&nbsp;Su, H.&nbsp;Wang, Y.&nbsp;Liao, Z.&nbsp;Wang, X.&nbsp;Jiang, Z.&nbsp;Yang, K.&nbsp;Wang,
X.&nbsp;Zhang, C.&nbsp;Li, Z.&nbsp;Gong, Y.&nbsp;Yao, X.&nbsp;Huang, J.&nbsp;Wang, J.&nbsp;Yu, Q.&nbsp;Guo, Y.&nbsp;Yu,
Y.&nbsp;Zhang, J.&nbsp;Wang, H.&nbsp;Tao, D.&nbsp;Yan, Z.&nbsp;Yi, F.&nbsp;Peng, F.&nbsp;Jiang, H.&nbsp;Zhang,
L.&nbsp;Deng, Y.&nbsp;Zhang, Z.&nbsp;Lin, C.&nbsp;Zhang, S.&nbsp;Zhang, M.&nbsp;Guo, S.&nbsp;Gu, G.&nbsp;Fan,
Y.&nbsp;Wang, X.&nbsp;Jin, Q.&nbsp;Liu, and Y.&nbsp;Tian, “Pangu-<math id="bib.bib84.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib84.1.m1.1a"><mi id="bib.bib84.1.m1.1.1" xref="bib.bib84.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="bib.bib84.1.m1.1b"><ci id="bib.bib84.1.m1.1.1.cmml" xref="bib.bib84.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib84.1.m1.1c">\alpha</annotation></semantics></math>: Large-scale
autoregressive pretrained chinese language models with auto-parallel
computation,” <em id="bib.bib84.2.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2104.12369, 2021.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, Y.&nbsp;Gu, X.&nbsp;Han, S.&nbsp;Chen, C.&nbsp;Xiao, Z.&nbsp;Sun, Y.&nbsp;Yao, F.&nbsp;Qi, J.&nbsp;Guan,
P.&nbsp;Ke, Y.&nbsp;Cai, G.&nbsp;Zeng, Z.&nbsp;Tan, Z.&nbsp;Liu, M.&nbsp;Huang, W.&nbsp;Han, Y.&nbsp;Liu, X.&nbsp;Zhu, and
M.&nbsp;Sun, “CPM-2: large-scale cost-effective pre-trained language models,”
<em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2106.10715, 2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
E.&nbsp;Nijkamp, B.&nbsp;Pang, H.&nbsp;Hayashi, L.&nbsp;Tu, H.&nbsp;Wang, Y.&nbsp;Zhou, S.&nbsp;Savarese, and
C.&nbsp;Xiong, “Codegen: An open large language model for code with mtulti-turn
program synthesis,” <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.13474</em>, 2022.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
S.&nbsp;Black, S.&nbsp;Biderman, E.&nbsp;Hallahan, Q.&nbsp;Anthony, L.&nbsp;Gao, L.&nbsp;Golding, H.&nbsp;He,
C.&nbsp;Leahy, K.&nbsp;McDonell, J.&nbsp;Phang, M.&nbsp;Pieler, U.&nbsp;S. Prashanth, S.&nbsp;Purohit,
L.&nbsp;Reynolds, J.&nbsp;Tow, B.&nbsp;Wang, and S.&nbsp;Weinbach, “Gpt-neox-20b: An open-source
autoregressive language model,” <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2204.06745, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, S.&nbsp;Mishra, P.&nbsp;Alipoormolabashi, Y.&nbsp;Kordi, A.&nbsp;Mirzaei, A.&nbsp;Naik,
A.&nbsp;Ashok, A.&nbsp;S. Dhanasekaran, A.&nbsp;Arunkumar, D.&nbsp;Stap, E.&nbsp;Pathak,
G.&nbsp;Karamanolakis, H.&nbsp;G. Lai, I.&nbsp;Purohit, I.&nbsp;Mondal, J.&nbsp;Anderson, K.&nbsp;Kuznia,
K.&nbsp;Doshi, K.&nbsp;K. Pal, M.&nbsp;Patel, M.&nbsp;Moradshahi, M.&nbsp;Parmar, M.&nbsp;Purohit,
N.&nbsp;Varshney, P.&nbsp;R. Kaza, P.&nbsp;Verma, R.&nbsp;S. Puri, R.&nbsp;Karia, S.&nbsp;Doshi, S.&nbsp;K.
Sampat, S.&nbsp;Mishra, S.&nbsp;R. A, S.&nbsp;Patro, T.&nbsp;Dixit, and X.&nbsp;Shen,
“Super-naturalinstructions: Generalization via declarative instructions on
1600+ NLP tasks,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab
Emirates, December 7-11, 2022</em>, 2022, pp. 5085–5109.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, M.&nbsp;Dehghani, V.&nbsp;Q. Tran, X.&nbsp;García, J.&nbsp;Wei, X.&nbsp;Wang, H.&nbsp;W. Chung,
D.&nbsp;Bahri, T.&nbsp;Schuster, H.&nbsp;Zheng, D.&nbsp;Zhou, N.&nbsp;Houlsby, and D.&nbsp;Metzler, “Ul2:
Unifying language learning paradigms,” 2022.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, S.&nbsp;Roller, N.&nbsp;Goyal, M.&nbsp;Artetxe, M.&nbsp;Chen, S.&nbsp;Chen, C.&nbsp;Dewan, M.&nbsp;T.
Diab, X.&nbsp;Li, X.&nbsp;V. Lin, T.&nbsp;Mihaylov, M.&nbsp;Ott, S.&nbsp;Shleifer, K.&nbsp;Shuster,
D.&nbsp;Simig, P.&nbsp;S. Koura, A.&nbsp;Sridhar, T.&nbsp;Wang, and L.&nbsp;Zettlemoyer, “OPT: open
pre-trained transformer language models,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2205.01068,
2022.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
M.&nbsp;R. Costa-jussà, J.&nbsp;Cross, O.&nbsp;Çelebi, M.&nbsp;Elbayad, K.&nbsp;Heafield,
K.&nbsp;Heffernan, E.&nbsp;Kalbassi, J.&nbsp;Lam, D.&nbsp;Licht, J.&nbsp;Maillard, A.&nbsp;Sun, S.&nbsp;Wang,
G.&nbsp;Wenzek, A.&nbsp;Youngblood, B.&nbsp;Akula, L.&nbsp;Barrault, G.&nbsp;M. Gonzalez, P.&nbsp;Hansanti,
J.&nbsp;Hoffman, S.&nbsp;Jarrett, K.&nbsp;R. Sadagopan, D.&nbsp;Rowe, S.&nbsp;Spruit, C.&nbsp;Tran,
P.&nbsp;Andrews, N.&nbsp;F. Ayan, S.&nbsp;Bhosale, S.&nbsp;Edunov, A.&nbsp;Fan, C.&nbsp;Gao, V.&nbsp;Goswami,
F.&nbsp;Guzmán, P.&nbsp;Koehn, A.&nbsp;Mourachko, C.&nbsp;Ropers, S.&nbsp;Saleem, H.&nbsp;Schwenk,
and J.&nbsp;Wang, “No language left behind: Scaling human-centered machine
translation,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2207.04672, 2022.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Q.&nbsp;Zheng, X.&nbsp;Xia, X.&nbsp;Zou, Y.&nbsp;Dong, S.&nbsp;Wang, Y.&nbsp;Xue, Z.&nbsp;Wang, L.&nbsp;Shen, A.&nbsp;Wang,
Y.&nbsp;Li <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Codegeex: A pre-trained model for code generation with
multilingual evaluations on humaneval-x,” <em id="bib.bib92.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2303.17568</em>, 2023.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
A.&nbsp;Zeng, X.&nbsp;Liu, Z.&nbsp;Du, Z.&nbsp;Wang, H.&nbsp;Lai, M.&nbsp;Ding, Z.&nbsp;Yang, Y.&nbsp;Xu, W.&nbsp;Zheng,
X.&nbsp;Xia, W.&nbsp;L. Tam, Z.&nbsp;Ma, Y.&nbsp;Xue, J.&nbsp;Zhai, W.&nbsp;Chen, P.&nbsp;Zhang, Y.&nbsp;Dong, and
J.&nbsp;Tang, “GLM-130B: an open bilingual pre-trained model,” vol.
abs/2210.02414, 2022.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
N.&nbsp;Muennighoff, T.&nbsp;Wang, L.&nbsp;Sutawika, A.&nbsp;Roberts, S.&nbsp;Biderman, T.&nbsp;L. Scao,
M.&nbsp;S. Bari, S.&nbsp;Shen, Z.&nbsp;X. Yong, H.&nbsp;Schoelkopf, X.&nbsp;Tang, D.&nbsp;Radev, A.&nbsp;F. Aji,
K.&nbsp;Almubarak, S.&nbsp;Albanie, Z.&nbsp;Alyafeai, A.&nbsp;Webson, E.&nbsp;Raff, and C.&nbsp;Raffel,
“Crosslingual generalization through multitask finetuning,” <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2211.01786, 2022.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
S.&nbsp;Iyer, X.&nbsp;V. Lin, R.&nbsp;Pasunuru, T.&nbsp;Mihaylov, D.&nbsp;Simig, P.&nbsp;Yu, K.&nbsp;Shuster,
T.&nbsp;Wang, Q.&nbsp;Liu, P.&nbsp;S. Koura, X.&nbsp;Li, B.&nbsp;O’Horo, G.&nbsp;Pereyra, J.&nbsp;Wang,
C.&nbsp;Dewan, A.&nbsp;Celikyilmaz, L.&nbsp;Zettlemoyer, and V.&nbsp;Stoyanov, “OPT-IML:
scaling language model instruction meta learning through the lens of
generalization,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.12017, 2022.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
S.&nbsp;Biderman, H.&nbsp;Schoelkopf, Q.&nbsp;Anthony, H.&nbsp;Bradley, K.&nbsp;O’Brien, E.&nbsp;Hallahan,
M.&nbsp;A. Khan, S.&nbsp;Purohit, U.&nbsp;S. Prashanth, E.&nbsp;Raff <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Pythia: A
suite for analyzing large language models across training and scaling,”
<em id="bib.bib96.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01373</em>, 2023.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
E.&nbsp;Nijkamp, H.&nbsp;Hayashi, C.&nbsp;Xiong, S.&nbsp;Savarese, and Y.&nbsp;Zhou, “Codegen2: Lessons
for training llms on programming and natural languages,” <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.02309, 2023.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
R.&nbsp;Li, L.&nbsp;B. Allal, Y.&nbsp;Zi, N.&nbsp;Muennighoff, D.&nbsp;Kocetkov, C.&nbsp;Mou, M.&nbsp;Marone,
C.&nbsp;Akiki, J.&nbsp;Li, J.&nbsp;Chim, Q.&nbsp;Liu, E.&nbsp;Zheltonozhskii, T.&nbsp;Y. Zhuo, T.&nbsp;Wang,
O.&nbsp;Dehaene, M.&nbsp;Davaadorj, J.&nbsp;Lamy-Poirier, J.&nbsp;Monteiro, O.&nbsp;Shliazhko,
N.&nbsp;Gontier, N.&nbsp;Meade, A.&nbsp;Zebaze, M.&nbsp;Yee, L.&nbsp;K. Umapathi, J.&nbsp;Zhu, B.&nbsp;Lipkin,
M.&nbsp;Oblokulov, Z.&nbsp;Wang, R.&nbsp;M. V, J.&nbsp;Stillerman, S.&nbsp;S. Patel, D.&nbsp;Abulkhanov,
M.&nbsp;Zocca, M.&nbsp;Dey, Z.&nbsp;Zhang, N.&nbsp;Fahmy, U.&nbsp;Bhattacharyya, W.&nbsp;Yu, S.&nbsp;Singh,
S.&nbsp;Luccioni, P.&nbsp;Villegas, M.&nbsp;Kunakov, F.&nbsp;Zhdanov, M.&nbsp;Romero, T.&nbsp;Lee,
N.&nbsp;Timor, J.&nbsp;Ding, C.&nbsp;Schlesinger, H.&nbsp;Schoelkopf, J.&nbsp;Ebert, T.&nbsp;Dao,
M.&nbsp;Mishra, A.&nbsp;Gu, J.&nbsp;Robinson, C.&nbsp;J. Anderson, B.&nbsp;Dolan-Gavitt,
D.&nbsp;Contractor, S.&nbsp;Reddy, D.&nbsp;Fried, D.&nbsp;Bahdanau, Y.&nbsp;Jernite, C.&nbsp;M. Ferrandis,
S.&nbsp;Hughes, T.&nbsp;Wolf, A.&nbsp;Guha, L.&nbsp;von Werra, and H.&nbsp;de&nbsp;Vries, “Starcoder: may
the source be with you!” <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.06161, 2023. [Online].
Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2305.06161" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.06161</a>

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei,
N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Llama 2:
Open foundation and fine-tuned chat models,” <em id="bib.bib99.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
A.&nbsp;Yang, B.&nbsp;Xiao, B.&nbsp;Wang, B.&nbsp;Zhang, C.&nbsp;Yin, C.&nbsp;Lv, D.&nbsp;Pan, D.&nbsp;Wang, D.&nbsp;Yan,
F.&nbsp;Yang <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Baichuan 2: Open large-scale language models,”
<em id="bib.bib100.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10305</em>, 2023.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J.&nbsp;Bai, S.&nbsp;Bai, Y.&nbsp;Chu, Z.&nbsp;Cui, K.&nbsp;Dang, X.&nbsp;Deng, Y.&nbsp;Fan, W.&nbsp;Ge, Y.&nbsp;Han,
F.&nbsp;Huang <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Qwen technical report,” <em id="bib.bib101.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
X.&nbsp;Li, Y.&nbsp;Yao, X.&nbsp;Jiang, X.&nbsp;Fang, X.&nbsp;Meng, S.&nbsp;Fan, P.&nbsp;Han, J.&nbsp;Li, L.&nbsp;Du, B.&nbsp;Qin
<em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Flm-101b: An open llm and how to train it with $100 k
budget,” <em id="bib.bib102.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.03852</em>, 2023.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
T.&nbsp;Wei, L.&nbsp;Zhao, L.&nbsp;Zhang, B.&nbsp;Zhu, L.&nbsp;Wang, H.&nbsp;Yang, B.&nbsp;Li, C.&nbsp;Cheng,
W.&nbsp;Lü, R.&nbsp;Hu <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Skywork: A more open bilingual foundation
model,” <em id="bib.bib103.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.19341</em>, 2023.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
D.&nbsp;Lepikhin, H.&nbsp;Lee, Y.&nbsp;Xu, D.&nbsp;Chen, O.&nbsp;Firat, Y.&nbsp;Huang, M.&nbsp;Krikun, N.&nbsp;Shazeer,
and Z.&nbsp;Chen, “Gshard: Scaling giant models with conditional computation and
automatic sharding,” in <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>, 2021.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
M.&nbsp;Chen, J.&nbsp;Tworek, H.&nbsp;Jun, Q.&nbsp;Yuan, H.&nbsp;P. de&nbsp;Oliveira&nbsp;Pinto, J.&nbsp;Kaplan,
H.&nbsp;Edwards, Y.&nbsp;Burda, N.&nbsp;Joseph, G.&nbsp;Brockman, A.&nbsp;Ray, R.&nbsp;Puri, G.&nbsp;Krueger,
M.&nbsp;Petrov, H.&nbsp;Khlaaf, G.&nbsp;Sastry, P.&nbsp;Mishkin, B.&nbsp;Chan, S.&nbsp;Gray, N.&nbsp;Ryder,
M.&nbsp;Pavlov, A.&nbsp;Power, L.&nbsp;Kaiser, M.&nbsp;Bavarian, C.&nbsp;Winter, P.&nbsp;Tillet, F.&nbsp;P.
Such, D.&nbsp;Cummings, M.&nbsp;Plappert, F.&nbsp;Chantzis, E.&nbsp;Barnes, A.&nbsp;Herbert-Voss,
W.&nbsp;H. Guss, A.&nbsp;Nichol, A.&nbsp;Paino, N.&nbsp;Tezak, J.&nbsp;Tang, I.&nbsp;Babuschkin, S.&nbsp;Balaji,
S.&nbsp;Jain, W.&nbsp;Saunders, C.&nbsp;Hesse, A.&nbsp;N. Carr, J.&nbsp;Leike, J.&nbsp;Achiam, V.&nbsp;Misra,
E.&nbsp;Morikawa, A.&nbsp;Radford, M.&nbsp;Knight, M.&nbsp;Brundage, M.&nbsp;Murati, K.&nbsp;Mayer,
P.&nbsp;Welinder, B.&nbsp;McGrew, D.&nbsp;Amodei, S.&nbsp;McCandlish, I.&nbsp;Sutskever, and
W.&nbsp;Zaremba, “Evaluating large language models trained on code,”
<em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2107.03374, 2021.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Y.&nbsp;Sun, S.&nbsp;Wang, S.&nbsp;Feng, S.&nbsp;Ding, C.&nbsp;Pang, J.&nbsp;Shang, J.&nbsp;Liu, X.&nbsp;Chen, Y.&nbsp;Zhao,
Y.&nbsp;Lu, W.&nbsp;Liu, Z.&nbsp;Wu, W.&nbsp;Gong, J.&nbsp;Liang, Z.&nbsp;Shang, P.&nbsp;Sun, W.&nbsp;Liu, X.&nbsp;Ouyang,
D.&nbsp;Yu, H.&nbsp;Tian, H.&nbsp;Wu, and H.&nbsp;Wang, “ERNIE 3.0: Large-scale knowledge
enhanced pre-training for language understanding and generation,”
<em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2107.02137, 2021.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
O.&nbsp;Lieber, O.&nbsp;Sharir, B.&nbsp;Lenz, and Y.&nbsp;Shoham, “Jurassic-1: Technical details
and evaluation,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">White Paper. AI21 Labs</em>, vol.&nbsp;1, 2021.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
B.&nbsp;Kim, H.&nbsp;Kim, S.&nbsp;Lee, G.&nbsp;Lee, D.&nbsp;Kwak, D.&nbsp;H. Jeon, S.&nbsp;Park, S.&nbsp;Kim, S.&nbsp;Kim,
D.&nbsp;Seo, H.&nbsp;Lee, M.&nbsp;Jeong, S.&nbsp;Lee, M.&nbsp;Kim, S.&nbsp;Ko, S.&nbsp;Kim, T.&nbsp;Park, J.&nbsp;Kim,
S.&nbsp;Kang, N.&nbsp;Ryu, K.&nbsp;M. Yoo, M.&nbsp;Chang, S.&nbsp;Suh, S.&nbsp;In, J.&nbsp;Park, K.&nbsp;Kim, H.&nbsp;Kim,
J.&nbsp;Jeong, Y.&nbsp;G. Yeo, D.&nbsp;Ham, D.&nbsp;Park, M.&nbsp;Y. Lee, J.&nbsp;Kang, I.&nbsp;Kang, J.&nbsp;Ha,
W.&nbsp;Park, and N.&nbsp;Sung, “What changes can large-scale language models bring?
intensive study on hyperclova: Billions-scale korean generative pretrained
transformers,” in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta
Cana, Dominican Republic, 7-11 November, 2021</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
S.&nbsp;Wu, X.&nbsp;Zhao, T.&nbsp;Yu, R.&nbsp;Zhang, C.&nbsp;Shen, H.&nbsp;Liu, F.&nbsp;Li, H.&nbsp;Zhu, J.&nbsp;Luo, L.&nbsp;Xu
<em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Yuan 1.0: Large-scale pre-trained language model in
zero-shot and few-shot learning,” <em id="bib.bib109.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.04725</em>,
2021.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma, N.&nbsp;Elhage, Z.&nbsp;Hatfield-Dodds,
D.&nbsp;Hernandez, J.&nbsp;Kernion, K.&nbsp;Ndousse, C.&nbsp;Olsson, D.&nbsp;Amodei, T.&nbsp;B. Brown,
J.&nbsp;Clark, S.&nbsp;McCandlish, C.&nbsp;Olah, and J.&nbsp;Kaplan, “A general language
assistant as a laboratory for alignment,” <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2112.00861,
2021.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
S.&nbsp;Wang, Y.&nbsp;Sun, Y.&nbsp;Xiang, Z.&nbsp;Wu, S.&nbsp;Ding, W.&nbsp;Gong, S.&nbsp;Feng, J.&nbsp;Shang, Y.&nbsp;Zhao,
C.&nbsp;Pang, J.&nbsp;Liu, X.&nbsp;Chen, Y.&nbsp;Lu, W.&nbsp;Liu, X.&nbsp;Wang, Y.&nbsp;Bai, Q.&nbsp;Chen, L.&nbsp;Zhao,
S.&nbsp;Li, P.&nbsp;Sun, D.&nbsp;Yu, Y.&nbsp;Ma, H.&nbsp;Tian, H.&nbsp;Wu, T.&nbsp;Wu, W.&nbsp;Zeng, G.&nbsp;Li, W.&nbsp;Gao,
and H.&nbsp;Wang, “ERNIE 3.0 titan: Exploring larger-scale knowledge enhanced
pre-training for language understanding and generation,” <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2112.12731, 2021.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
N.&nbsp;Du, Y.&nbsp;Huang, A.&nbsp;M. Dai, S.&nbsp;Tong, D.&nbsp;Lepikhin, Y.&nbsp;Xu, M.&nbsp;Krikun, Y.&nbsp;Zhou,
A.&nbsp;W. Yu, O.&nbsp;Firat, B.&nbsp;Zoph, L.&nbsp;Fedus, M.&nbsp;P. Bosma, Z.&nbsp;Zhou, T.&nbsp;Wang, Y.&nbsp;E.
Wang, K.&nbsp;Webster, M.&nbsp;Pellat, K.&nbsp;Robinson, K.&nbsp;S. Meier-Hellstern, T.&nbsp;Duke,
L.&nbsp;Dixon, K.&nbsp;Zhang, Q.&nbsp;V. Le, Y.&nbsp;Wu, Z.&nbsp;Chen, and C.&nbsp;Cui, “Glam: Efficient
scaling of language models with mixture-of-experts,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
Maryland, USA</em>, 2022, pp. 5547–5569.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
S.&nbsp;Smith, M.&nbsp;Patwary, B.&nbsp;Norick, P.&nbsp;LeGresley, S.&nbsp;Rajbhandari, J.&nbsp;Casper,
Z.&nbsp;Liu, S.&nbsp;Prabhumoye, G.&nbsp;Zerveas, V.&nbsp;Korthikanti, E.&nbsp;Zheng, R.&nbsp;Child, R.&nbsp;Y.
Aminabadi, J.&nbsp;Bernauer, X.&nbsp;Song, M.&nbsp;Shoeybi, Y.&nbsp;He, M.&nbsp;Houston, S.&nbsp;Tiwary,
and B.&nbsp;Catanzaro, “Using deepspeed and megatron to train megatron-turing
NLG 530b, A large-scale generative language model,” <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2201.11990, 2022.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y.&nbsp;Li, D.&nbsp;H. Choi, J.&nbsp;Chung, N.&nbsp;Kushman, J.&nbsp;Schrittwieser, R.&nbsp;Leblond,
T.&nbsp;Eccles, J.&nbsp;Keeling, F.&nbsp;Gimeno, A.&nbsp;D. Lago, T.&nbsp;Hubert, P.&nbsp;Choy,
C.&nbsp;de&nbsp;Masson&nbsp;d’Autume, I.&nbsp;Babuschkin, X.&nbsp;Chen, P.&nbsp;Huang, J.&nbsp;Welbl, S.&nbsp;Gowal,
A.&nbsp;Cherepanov, J.&nbsp;Molloy, D.&nbsp;J. Mankowitz, E.&nbsp;S. Robson, P.&nbsp;Kohli,
N.&nbsp;de&nbsp;Freitas, K.&nbsp;Kavukcuoglu, and O.&nbsp;Vinyals, “Competition-level code
generation with alphacode,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Science</em>, 2022.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
S.&nbsp;Soltan, S.&nbsp;Ananthakrishnan, J.&nbsp;FitzGerald, R.&nbsp;Gupta, W.&nbsp;Hamza, H.&nbsp;Khan,
C.&nbsp;Peris, S.&nbsp;Rawls, A.&nbsp;Rosenbaum, A.&nbsp;Rumshisky, C.&nbsp;S. Prakash, M.&nbsp;Sridhar,
F.&nbsp;Triefenbach, A.&nbsp;Verma, G.&nbsp;Tür, and P.&nbsp;Natarajan, “Alexatm 20b:
Few-shot learning using a large-scale multilingual seq2seq model,”
<em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2208.01448, 2022.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
A.&nbsp;Glaese, N.&nbsp;McAleese, M.&nbsp;Trebacz, J.&nbsp;Aslanides, V.&nbsp;Firoiu, T.&nbsp;Ewalds,
M.&nbsp;Rauh, L.&nbsp;Weidinger, M.&nbsp;Chadwick, P.&nbsp;Thacker, L.&nbsp;Campbell-Gillingham,
J.&nbsp;Uesato, P.&nbsp;Huang, R.&nbsp;Comanescu, F.&nbsp;Yang, A.&nbsp;See, S.&nbsp;Dathathri, R.&nbsp;Greig,
C.&nbsp;Chen, D.&nbsp;Fritz, J.&nbsp;S. Elias, R.&nbsp;Green, S.&nbsp;Mokrá, N.&nbsp;Fernando, B.&nbsp;Wu,
R.&nbsp;Foley, S.&nbsp;Young, I.&nbsp;Gabriel, W.&nbsp;Isaac, J.&nbsp;Mellor, D.&nbsp;Hassabis,
K.&nbsp;Kavukcuoglu, L.&nbsp;A. Hendricks, and G.&nbsp;Irving, “Improving alignment of
dialogue agents via targeted human judgements,” <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2209.14375, 2022.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
H.&nbsp;Su, X.&nbsp;Zhou, H.&nbsp;Yu, Y.&nbsp;Chen, Z.&nbsp;Zhu, Y.&nbsp;Yu, and J.&nbsp;Zhou, “Welm: A
well-read pre-trained language model for chinese,” <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2209.10372, 2022.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, J.&nbsp;Wei, H.&nbsp;W. Chung, V.&nbsp;Q. Tran, D.&nbsp;R. So, S.&nbsp;Shakeri, X.&nbsp;Garcia, H.&nbsp;S.
Zheng, J.&nbsp;Rao, A.&nbsp;Chowdhery, D.&nbsp;Zhou, D.&nbsp;Metzler, S.&nbsp;Petrov, N.&nbsp;Houlsby,
Q.&nbsp;V. Le, and M.&nbsp;Dehghani, “Transcending scaling laws with 0.1% extra
compute,” <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.11399, 2022.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
X.&nbsp;Ren, P.&nbsp;Zhou, X.&nbsp;Meng, X.&nbsp;Huang, Y.&nbsp;Wang, W.&nbsp;Wang, P.&nbsp;Li, X.&nbsp;Zhang,
A.&nbsp;Podolskiy, G.&nbsp;Arshinov, A.&nbsp;Bout, I.&nbsp;Piontkovskaya, J.&nbsp;Wei, X.&nbsp;Jiang,
T.&nbsp;Su, Q.&nbsp;Liu, and J.&nbsp;Yao, “Pangu-<math id="bib.bib119.1.m1.1" class="ltx_Math" alttext="\Sigma" display="inline"><semantics id="bib.bib119.1.m1.1a"><mi mathvariant="normal" id="bib.bib119.1.m1.1.1" xref="bib.bib119.1.m1.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="bib.bib119.1.m1.1b"><ci id="bib.bib119.1.m1.1.1.cmml" xref="bib.bib119.1.m1.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib119.1.m1.1c">\Sigma</annotation></semantics></math>: Towards trillion parameter
language model with sparse heterogeneous computing,” <em id="bib.bib119.2.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2303.10845, 2023.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
R.&nbsp;Anil, A.&nbsp;M. Dai, O.&nbsp;Firat, M.&nbsp;Johnson, D.&nbsp;Lepikhin, A.&nbsp;Passos, S.&nbsp;Shakeri,
E.&nbsp;Taropa, P.&nbsp;Bailey, Z.&nbsp;Chen <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Palm 2 technical report,”
<em id="bib.bib120.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10403</em>, 2023.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
A.&nbsp;Radford, R.&nbsp;Józefowicz, and I.&nbsp;Sutskever, “Learning to generate
reviews and discovering sentiment,” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1704.01444, 2017.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
A.&nbsp;Radford, K.&nbsp;Narasimhan, T.&nbsp;Salimans, I.&nbsp;Sutskever <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Improving
language understanding by generative pre-training,” 2018.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
B.&nbsp;McCann, N.&nbsp;S. Keskar, C.&nbsp;Xiong, and R.&nbsp;Socher, “The natural language
decathlon: Multitask learning as question answering,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/1806.08730, 2018.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, S.&nbsp;Sun, M.&nbsp;Galley, Y.&nbsp;Chen, C.&nbsp;Brockett, X.&nbsp;Gao, J.&nbsp;Gao, J.&nbsp;Liu, and
B.&nbsp;Dolan, “DIALOGPT : Large-scale generative pre-training for
conversational response generation,” in <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics: System
Demonstrations, ACL 2020, Online, July 5-10, 2020</em>, A.&nbsp;Celikyilmaz and
T.&nbsp;Wen, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2020, pp. 270–278.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
D.&nbsp;Ham, J.&nbsp;Lee, Y.&nbsp;Jang, and K.&nbsp;Kim, “End-to-end neural pipeline for
goal-oriented dialogue systems using GPT-2,” in <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020, pp. 583–592.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
I.&nbsp;Drori, S.&nbsp;Tran, R.&nbsp;Wang, N.&nbsp;Cheng, K.&nbsp;Liu, L.&nbsp;Tang, E.&nbsp;Ke, N.&nbsp;Singh, T.&nbsp;L.
Patti, J.&nbsp;Lynch, A.&nbsp;Shporer, N.&nbsp;Verma, E.&nbsp;Wu, and G.&nbsp;Strang, “A neural
network solves and generates mathematics problems by program synthesis:
Calculus, differential equations, linear algebra, and more,” <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2112.15594, 2021.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
A.&nbsp;Neelakantan, T.&nbsp;Xu, R.&nbsp;Puri, A.&nbsp;Radford, J.&nbsp;M. Han, J.&nbsp;Tworek, Q.&nbsp;Yuan,
N.&nbsp;Tezak, J.&nbsp;W. Kim, C.&nbsp;Hallacy, J.&nbsp;Heidecke, P.&nbsp;Shyam, B.&nbsp;Power, T.&nbsp;E.
Nekoul, G.&nbsp;Sastry, G.&nbsp;Krueger, D.&nbsp;Schnurr, F.&nbsp;P. Such, K.&nbsp;Hsu, M.&nbsp;Thompson,
T.&nbsp;Khan, T.&nbsp;Sherbakov, J.&nbsp;Jang, P.&nbsp;Welinder, and L.&nbsp;Weng, “Text and code
embeddings by contrastive pre-training,” <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2201.10005,
2022.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
J.&nbsp;Schulman, F.&nbsp;Wolski, P.&nbsp;Dhariwal, A.&nbsp;Radford, and O.&nbsp;Klimov, “Proximal
policy optimization algorithms,” <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.06347</em>,
2017.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
N.&nbsp;Stiennon, L.&nbsp;Ouyang, J.&nbsp;Wu, D.&nbsp;M. Ziegler, R.&nbsp;Lowe, C.&nbsp;Voss, A.&nbsp;Radford,
D.&nbsp;Amodei, and P.&nbsp;F. Christiano, “Learning to summarize from human
feedback,” <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2009.01325, 2020.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
OpenAI, “Our approach to alignment research,” <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, August
2022.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
——, “Introducing chatgpt,” <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, November 2022.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
D.&nbsp;Ganguli, L.&nbsp;Lovitt, J.&nbsp;Kernion, A.&nbsp;Askell, Y.&nbsp;Bai, S.&nbsp;Kadavath, B.&nbsp;Mann,
E.&nbsp;Perez, N.&nbsp;Schiefer, K.&nbsp;Ndousse, A.&nbsp;Jones, S.&nbsp;Bowman, A.&nbsp;Chen, T.&nbsp;Conerly,
N.&nbsp;DasSarma, D.&nbsp;Drain, N.&nbsp;Elhage, S.&nbsp;E. Showk, S.&nbsp;Fort, Z.&nbsp;Hatfield-Dodds,
T.&nbsp;Henighan, D.&nbsp;Hernandez, T.&nbsp;Hume, J.&nbsp;Jacobson, S.&nbsp;Johnston, S.&nbsp;Kravec,
C.&nbsp;Olsson, S.&nbsp;Ringer, E.&nbsp;Tran-Johnson, D.&nbsp;Amodei, T.&nbsp;Brown, N.&nbsp;Joseph,
S.&nbsp;McCandlish, C.&nbsp;Olah, J.&nbsp;Kaplan, and J.&nbsp;Clark, “Red teaming language
models to reduce harms: Methods, scaling behaviors, and lessons learned,”
<em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.07858, 2022.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
OpenAI, “Gpt-4v(ision) system card,” <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">OpenAI</em>, 2023.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
——, “Lessons learned on language model safety and misuse,” <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">OpenAI
blog</em>, 2022.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
E.&nbsp;Almazrouei, H.&nbsp;Alobeidli, A.&nbsp;Alshamsi, A.&nbsp;Cappelli, R.&nbsp;Cojocaru, M.&nbsp;Debbah,
E.&nbsp;Goffinet, D.&nbsp;Heslow, J.&nbsp;Launay, Q.&nbsp;Malartic, B.&nbsp;Noune, B.&nbsp;Pannier, and
G.&nbsp;Penedo, “Falcon-40B: an open large language model with state-of-the-art
performance,” 2023.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
L.&nbsp;Huawei Technologies&nbsp;Co., “Huawei mindspore ai development framework,” in
<em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence Technology</em>.&nbsp;&nbsp;&nbsp;Springer, 2022, pp. 137–162.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto, “Stanford alpaca: An instruction-following llama model,”
<a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
W.-L. Chiang, Z.&nbsp;Li, Z.&nbsp;Lin, Y.&nbsp;Sheng, Z.&nbsp;Wu, H.&nbsp;Zhang, L.&nbsp;Zheng, S.&nbsp;Zhuang,
Y.&nbsp;Zhuang, J.&nbsp;E. Gonzalez, I.&nbsp;Stoica, and E.&nbsp;P. Xing, “Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” 2023.
[Online]. Available: <a target="_blank" href="https://vicuna.lmsys.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vicuna.lmsys.org</a>

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
2023. [Online]. Available:
<a target="_blank" href="https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama</a>

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Y.&nbsp;You, “Colossalchat: An open-source solution for cloning chatgpt with a
complete rlhf pipeline,” 2023. [Online]. Available:
<a target="_blank" href="https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b</a>

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
G.&nbsp;Penedo, Q.&nbsp;Malartic, D.&nbsp;Hesslow, R.&nbsp;Cojocaru, A.&nbsp;Cappelli, H.&nbsp;Alobeidli,
B.&nbsp;Pannier, E.&nbsp;Almazrouei, and J.&nbsp;Launay, “The RefinedWeb dataset for
Falcon LLM: outperforming curated corpora with web data, and web data
only,” <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.01116</em>, 2023.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto, “Stanford alpaca: An instruction-following llama model,”
<a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, Y.&nbsp;Kordi, S.&nbsp;Mishra, A.&nbsp;Liu, N.&nbsp;A. Smith, D.&nbsp;Khashabi, and
H.&nbsp;Hajishirzi, “Self-instruct: Aligning language model with self generated
instructions,” <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.10560, 2022.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Alpaca-LoRA, “Instruct-tune llama on consumer hardware,”
<a target="_blank" href="https://github.com/tloen/alpaca-lora" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tloen/alpaca-lora</a>, 2023.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
E.&nbsp;J. Hu, Y.&nbsp;Shen, P.&nbsp;Wallis, Z.&nbsp;Allen-Zhu, Y.&nbsp;Li, S.&nbsp;Wang, L.&nbsp;Wang, and
W.&nbsp;Chen, “Lora: Low-rank adaptation of large language models,” in <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">The
Tenth International Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
X.&nbsp;Geng, A.&nbsp;Gudibande, H.&nbsp;Liu, E.&nbsp;Wallace, P.&nbsp;Abbeel, S.&nbsp;Levine, and D.&nbsp;Song,
“Koala: A dialogue model for academic research,” Blog post, April 2023.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Y.&nbsp;Ji, Y.&nbsp;Deng, Y.&nbsp;Gong, Y.&nbsp;Peng, Q.&nbsp;Niu, B.&nbsp;Ma, and X.&nbsp;Li, “Belle: Be
everyone’s large language model engine,”
<a target="_blank" href="https://github.com/LianjiaTech/BELLE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LianjiaTech/BELLE</a>, 2023.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
D.&nbsp;Eccleston, “Sharegpt,” <a target="_blank" href="https://sharegpt.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sharegpt.com/</a>, 2023.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee, “Visual instruction tuning,”
<em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.08485, 2023.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
D.&nbsp;Zhu, J.&nbsp;Chen, X.&nbsp;Shen, X.&nbsp;Li, and M.&nbsp;Elhoseiny, “Minigpt-4: Enhancing
vision-language understanding with advanced large language models,”
<em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.10592, 2023.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
W.&nbsp;Dai, J.&nbsp;Li, D.&nbsp;Li, A.&nbsp;M.&nbsp;H. Tiong, J.&nbsp;Zhao, W.&nbsp;Wang, B.&nbsp;Li, P.&nbsp;Fung, and
S.&nbsp;C.&nbsp;H. Hoi, “Instructblip: Towards general-purpose vision-language models
with instruction tuning,” <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.06500, 2023.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Y.&nbsp;Su, T.&nbsp;Lan, H.&nbsp;Li, J.&nbsp;Xu, Y.&nbsp;Wang, and D.&nbsp;Cai, “Pandagpt: One model to
instruction-follow them all,” 2023.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhu, R.&nbsp;Kiros, R.&nbsp;S. Zemel, R.&nbsp;Salakhutdinov, R.&nbsp;Urtasun, A.&nbsp;Torralba, and
S.&nbsp;Fidler, “Aligning books and movies: Towards story-like visual
explanations by watching movies and reading books,” in <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">2015 IEEE
International Conference on Computer Vision, ICCV 2015, Santiago, Chile,
December 7-13, 2015</em>.&nbsp;&nbsp;&nbsp;IEEE Computer
Society, 2015, pp. 19–27.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
“Project gutenberg.” [Online]. Available: <a target="_blank" href="https://www.gutenberg.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.gutenberg.org/</a>

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
T.&nbsp;H. Trinh and Q.&nbsp;V. Le, “A simple method for commonsense reasoning,”
<em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1806.02847, 2018.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
R.&nbsp;Zellers, A.&nbsp;Holtzman, H.&nbsp;Rashkin, Y.&nbsp;Bisk, A.&nbsp;Farhadi, F.&nbsp;Roesner, and
Y.&nbsp;Choi, “Defending against neural fake news,” in <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada</em>, H.&nbsp;M. Wallach, H.&nbsp;Larochelle, A.&nbsp;Beygelzimer,
F.&nbsp;d’Alché-Buc, E.&nbsp;B. Fox, and R.&nbsp;Garnett, Eds., 2019, pp.
9051–9062.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
A.&nbsp;Gokaslan, V.&nbsp;C.&nbsp;E. Pavlick, and S.&nbsp;Tellex, “Openwebtext corpus,”
<a target="_blank" href="http://Skylion007.github.io/OpenWebTextCorpus" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://Skylion007.github.io/OpenWebTextCorpus</a>, 2019.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
J.&nbsp;Baumgartner, S.&nbsp;Zannettou, B.&nbsp;Keegan, M.&nbsp;Squire, and J.&nbsp;Blackburn, “The
pushshift reddit dataset,” in <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourteenth
International AAAI Conference on Web and Social Media, ICWSM 2020, Held
Virtually, Original Venue: Atlanta, Georgia, USA, June 8-11, 2020</em>.&nbsp;&nbsp;&nbsp;AAAI Press, 2020, pp. 830–839.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
“Wikipedia.” [Online]. Available:
<a target="_blank" href="https://en.wikipedia.org/wiki/Main_Page" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Main_Page</a>

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
“Bigquery dataset.” [Online]. Available:
<a target="_blank" href="https://cloud.google.com/bigquery?hl=zh-cn" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/bigquery?hl=zh-cn</a>

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, S.&nbsp;Biderman, S.&nbsp;Black, L.&nbsp;Golding, T.&nbsp;Hoppe, C.&nbsp;Foster, J.&nbsp;Phang,
H.&nbsp;He, A.&nbsp;Thite, N.&nbsp;Nabeshima, S.&nbsp;Presser, and C.&nbsp;Leahy, “The pile: An 800gb
dataset of diverse text for language modeling,” <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2101.00027, 2021.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
H.&nbsp;Laurençon, L.&nbsp;Saulnier, T.&nbsp;Wang, C.&nbsp;Akiki, A.&nbsp;V. del Moral,
T.&nbsp;Le&nbsp;Scao, L.&nbsp;Von&nbsp;Werra, C.&nbsp;Mou, E.&nbsp;G. Ponferrada, H.&nbsp;Nguyen <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>,
“The bigscience roots corpus: A 1.6 tb composite multilingual dataset,” in
<em id="bib.bib162.2.2" class="ltx_emph ltx_font_italic">Thirty-sixth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track</em>, 2022.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
“Common crawl.” [Online]. Available: <a target="_blank" href="https://commoncrawl.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://commoncrawl.org/</a>

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
“A reproduction version of cc-stories on hugging face.” [Online]. Available:
<a target="_blank" href="https://huggingface.co/datasets/spacemanidol/cc-stories" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/spacemanidol/cc-stories</a>

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
B.&nbsp;Wang and A.&nbsp;Komatsuzaki, “GPT-J-6B: A 6 Billion Parameter Autoregressive
Language Model,” <a target="_blank" href="https://github.com/kingoflolz/mesh-transformer-jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kingoflolz/mesh-transformer-jax</a>,
2021.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
S.&nbsp;Mishra, D.&nbsp;Khashabi, C.&nbsp;Baral, and H.&nbsp;Hajishirzi, “Cross-task
generalization via natural language crowdsourcing instructions,” in
<em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022</em>, S.&nbsp;Muresan, P.&nbsp;Nakov, and A.&nbsp;Villavicencio, Eds.,
2022, pp. 3470–3487.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
S.&nbsp;H. Bach, V.&nbsp;Sanh, Z.&nbsp;X. Yong, A.&nbsp;Webson, C.&nbsp;Raffel, N.&nbsp;V. Nayak, A.&nbsp;Sharma,
T.&nbsp;Kim, M.&nbsp;S. Bari, T.&nbsp;Févry, Z.&nbsp;Alyafeai, M.&nbsp;Dey, A.&nbsp;Santilli, Z.&nbsp;Sun,
S.&nbsp;Ben-David, C.&nbsp;Xu, G.&nbsp;Chhablani, H.&nbsp;Wang, J.&nbsp;A. Fries, M.&nbsp;S. AlShaibani,
S.&nbsp;Sharma, U.&nbsp;Thakker, K.&nbsp;Almubarak, X.&nbsp;Tang, D.&nbsp;R. Radev, M.&nbsp;T. Jiang, and
A.&nbsp;M. Rush, “Promptsource: An integrated development environment and
repository for natural language prompts,” in <em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">ACL (demo)</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022,
pp. 93–104.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
T.&nbsp;Tang, J.&nbsp;Li, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “MVP: multi-task supervised
pre-training for natural language generation,” <em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2206.12131, 2022.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
H.&nbsp;Nguyen, S.&nbsp;Suri, K.&nbsp;Tsui, Shahules786, T.&nbsp;team, and C.&nbsp;Schuhmann, “The oig
dataset,” <a target="_blank" href="https://laion.ai/blog/oig-dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://laion.ai/blog/oig-dataset/</a>, 2023.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, A.&nbsp;Jones, K.&nbsp;Ndousse, A.&nbsp;Askell, A.&nbsp;Chen, N.&nbsp;DasSarma, D.&nbsp;Drain,
S.&nbsp;Fort, D.&nbsp;Ganguli, T.&nbsp;Henighan, N.&nbsp;Joseph, S.&nbsp;Kadavath, J.&nbsp;Kernion,
T.&nbsp;Conerly, S.&nbsp;E. Showk, N.&nbsp;Elhage, Z.&nbsp;Hatfield-Dodds, D.&nbsp;Hernandez,
T.&nbsp;Hume, S.&nbsp;Johnston, S.&nbsp;Kravec, L.&nbsp;Lovitt, N.&nbsp;Nanda, C.&nbsp;Olsson, D.&nbsp;Amodei,
T.&nbsp;B. Brown, J.&nbsp;Clark, S.&nbsp;McCandlish, C.&nbsp;Olah, B.&nbsp;Mann, and J.&nbsp;Kaplan,
“Training a helpful and harmless assistant with reinforcement learning from
human feedback,” <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2204.05862, 2022. [Online].
Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2204.05862" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2204.05862</a>

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
B.&nbsp;Guo, X.&nbsp;Zhang, Z.&nbsp;Wang, M.&nbsp;Jiang, J.&nbsp;Nie, Y.&nbsp;Ding, J.&nbsp;Yue, and Y.&nbsp;Wu, “How
close is chatgpt to human experts? comparison corpus, evaluation, and
detection,” <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.07597</em>, 2023.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
M.&nbsp;Conover, M.&nbsp;Hayes, A.&nbsp;Mathur, J.&nbsp;Xie, J.&nbsp;Wan, S.&nbsp;Shah, A.&nbsp;Ghodsi,
P.&nbsp;Wendell, M.&nbsp;Zaharia, and R.&nbsp;Xin. (2023) Free dolly: Introducing the
world’s first truly open instruction-tuned llm.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
A.&nbsp;Köpf, Y.&nbsp;Kilcher, D.&nbsp;von Rütte, S.&nbsp;Anagnostidis, Z.-R. Tam,
K.&nbsp;Stevens, A.&nbsp;Barhoum, N.&nbsp;M. Duc, O.&nbsp;Stanley, R.&nbsp;Nagyfi <em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>,
“Openassistant conversations–democratizing large language model
alignment,” <em id="bib.bib173.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07327</em>, 2023.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
J.&nbsp;Cheung, “Guanaco - generative universal assistant for natural-language
adaptive context-aware omnilingual outputs,”
<a target="_blank" href="https://guanaco-model.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://guanaco-model.github.io/</a>, 2023.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
C.&nbsp;Xu, D.&nbsp;Guo, N.&nbsp;Duan, and J.&nbsp;McAuley, “Baize: An open-source chat model with
parameter-efficient tuning on self-chat data,” <em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2304.01196</em>, 2023.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Y.&nbsp;Ji, Y.&nbsp;Gong, Y.&nbsp;Deng, Y.&nbsp;Peng, Q.&nbsp;Niu, B.&nbsp;Ma, and X.&nbsp;Li, “Towards better
instruction following language models for chinese: Investigating the impact
of training data and evaluation,” <em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07854</em>,
2023.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
K.&nbsp;Ethayarajh, Y.&nbsp;Choi, and S.&nbsp;Swayamdipta, “Understanding dataset difficulty
with <math id="bib.bib177.1.m1.1" class="ltx_Math" alttext="\mathcal{V}" display="inline"><semantics id="bib.bib177.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="bib.bib177.1.m1.1.1" xref="bib.bib177.1.m1.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="bib.bib177.1.m1.1b"><ci id="bib.bib177.1.m1.1.1.cmml" xref="bib.bib177.1.m1.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib177.1.m1.1c">\mathcal{V}</annotation></semantics></math>-usable information,” in <em id="bib.bib177.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th
International Conference on Machine Learning</em>, 2022, pp. 5988–6008.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
N.&nbsp;Lambert, L.&nbsp;Tunstall, N.&nbsp;Rajani, and T.&nbsp;Thrush. (2023) Huggingface h4 stack
exchange preference dataset. [Online]. Available:
<a target="_blank" href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences</a>

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
R.&nbsp;Liu, R.&nbsp;Yang, C.&nbsp;Jia, G.&nbsp;Zhang, D.&nbsp;Zhou, A.&nbsp;M. Dai, D.&nbsp;Yang, and
S.&nbsp;Vosoughi, “Training socially aligned language models in simulated human
society,” <em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.16960, 2023.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
G.&nbsp;Xu, J.&nbsp;Liu, M.&nbsp;Yan, H.&nbsp;Xu, J.&nbsp;Si, Z.&nbsp;Zhou, P.&nbsp;Yi, X.&nbsp;Gao, J.&nbsp;Sang, R.&nbsp;Zhang,
J.&nbsp;Zhang, C.&nbsp;Peng, F.&nbsp;Huang, and J.&nbsp;Zhou, “Cvalues: Measuring the values of
chinese large language models from safety to responsibility,” 2023.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
J.&nbsp;Dai, X.&nbsp;Pan, R.&nbsp;Sun, J.&nbsp;Ji, X.&nbsp;Xu, M.&nbsp;Liu, Y.&nbsp;Wang, and Y.&nbsp;Yang, “Safe
rlhf: Safe reinforcement learning from human feedback,” <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2310.12773</em>, 2023.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
V.&nbsp;Sanh, A.&nbsp;Webson, C.&nbsp;Raffel, S.&nbsp;H. Bach, L.&nbsp;Sutawika, Z.&nbsp;Alyafeai,
A.&nbsp;Chaffin, A.&nbsp;Stiegler, A.&nbsp;Raja, M.&nbsp;Dey, M.&nbsp;S. Bari, C.&nbsp;Xu, U.&nbsp;Thakker,
S.&nbsp;S. Sharma, E.&nbsp;Szczechla, T.&nbsp;Kim, G.&nbsp;Chhablani, N.&nbsp;V. Nayak, D.&nbsp;Datta,
J.&nbsp;Chang, M.&nbsp;T. Jiang, H.&nbsp;Wang, M.&nbsp;Manica, S.&nbsp;Shen, Z.&nbsp;X. Yong, H.&nbsp;Pandey,
R.&nbsp;Bawden, T.&nbsp;Wang, T.&nbsp;Neeraj, J.&nbsp;Rozen, A.&nbsp;Sharma, A.&nbsp;Santilli,
T.&nbsp;Févry, J.&nbsp;A. Fries, R.&nbsp;Teehan, T.&nbsp;L. Scao, S.&nbsp;Biderman, L.&nbsp;Gao,
T.&nbsp;Wolf, and A.&nbsp;M. Rush, “Multitask prompted training enables zero-shot task
generalization,” in <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
S.&nbsp;Longpre, L.&nbsp;Hou, T.&nbsp;Vu, A.&nbsp;Webson, H.&nbsp;W. Chung, Y.&nbsp;Tay, D.&nbsp;Zhou, Q.&nbsp;V. Le,
B.&nbsp;Zoph, J.&nbsp;Wei <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “The flan collection: Designing data and
methods for effective instruction tuning,” <em id="bib.bib183.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2301.13688</em>, 2023.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
K.&nbsp;Cobbe, V.&nbsp;Kosaraju, M.&nbsp;Bavarian, J.&nbsp;Hilton, R.&nbsp;Nakano, C.&nbsp;Hesse, and
J.&nbsp;Schulman, “Training verifiers to solve math word problems,” <em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2110.14168, 2021.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
M.&nbsp;Geva, D.&nbsp;Khashabi, E.&nbsp;Segal, T.&nbsp;Khot, D.&nbsp;Roth, and J.&nbsp;Berant, “Did
aristotle use a laptop? A question answering benchmark with implicit
reasoning strategies,” <em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, vol.&nbsp;9, pp.
346–361, 2021.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
O.&nbsp;Camburu, B.&nbsp;Shillingford, P.&nbsp;Minervini, T.&nbsp;Lukasiewicz, and P.&nbsp;Blunsom,
“Make up your mind! adversarial generation of inconsistent natural language
explanations,” in <em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, ACL 2020, Online, July 5-10,
2020</em>, D.&nbsp;Jurafsky, J.&nbsp;Chai, N.&nbsp;Schluter, and J.&nbsp;R. Tetreault, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020,
pp. 4157–4165.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
T.&nbsp;Wolf, L.&nbsp;Debut, V.&nbsp;Sanh, J.&nbsp;Chaumond, C.&nbsp;Delangue, A.&nbsp;Moi, P.&nbsp;Cistac,
T.&nbsp;Rault, R.&nbsp;Louf, M.&nbsp;Funtowicz, J.&nbsp;Davison, S.&nbsp;Shleifer, P.&nbsp;von Platen,
C.&nbsp;Ma, Y.&nbsp;Jernite, J.&nbsp;Plu, C.&nbsp;Xu, T.&nbsp;L. Scao, S.&nbsp;Gugger, M.&nbsp;Drame, Q.&nbsp;Lhoest,
and A.&nbsp;M. Rush, “Transformers: State-of-the-art natural language
processing,” in <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -
Demos, Online, November 16-20, 2020</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020, pp. 38–45.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
J.&nbsp;Bradbury, R.&nbsp;Frostig, P.&nbsp;Hawkins, M.&nbsp;J. Johnson, C.&nbsp;Leary, D.&nbsp;Maclaurin,
G.&nbsp;Necula, A.&nbsp;Paszke, J.&nbsp;VanderPlas, S.&nbsp;Wanderman-Milne, and Q.&nbsp;Zhang,
“JAX: composable transformations of Python+NumPy programs,” 2018.
[Online]. Available: <a target="_blank" href="http://github.com/google/jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/google/jax</a>

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Z.&nbsp;Bian, H.&nbsp;Liu, B.&nbsp;Wang, H.&nbsp;Huang, Y.&nbsp;Li, C.&nbsp;Wang, F.&nbsp;Cui, and Y.&nbsp;You,
“Colossal-ai: A unified deep learning system for large-scale parallel
training,” <em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2110.14883, 2021.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
J.&nbsp;Fang, Y.&nbsp;Yu, S.&nbsp;Li, Y.&nbsp;You, and J.&nbsp;Zhou, “Patrickstar: Parallel training of
pre-trained models via a chunk-based memory management,” <em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2108.05818, 2021.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
“Bmtrain: Effient training for big models.” [Online]. Available:
<a target="_blank" href="https://github.com/OpenBMB/BMTrain" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenBMB/BMTrain</a>

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
J.&nbsp;He, J.&nbsp;Qiu, A.&nbsp;Zeng, Z.&nbsp;Yang, J.&nbsp;Zhai, and J.&nbsp;Tang, “Fastmoe: A fast
mixture-of-expert training system,” <em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2103.13262, 2021.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
W.&nbsp;Kwon, Z.&nbsp;Li, S.&nbsp;Zhuang, Y.&nbsp;Sheng, L.&nbsp;Zheng, C.&nbsp;H. Yu, J.&nbsp;E. Gonzalez,
H.&nbsp;Zhang, and I.&nbsp;Stoica, “Efficient memory management for large language
model serving with pagedattention,” in <em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGOPS
29th Symposium on Operating Systems Principles</em>, 2023.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
(2023) Deepspeed-mii. [Online]. Available:
<a target="_blank" href="https://github.com/microsoft/DeepSpeed-MII" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/microsoft/DeepSpeed-MII</a>

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Mensch, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;de&nbsp;las
Casas, F.&nbsp;Bressand, G.&nbsp;Lengyel, G.&nbsp;Lample, L.&nbsp;Saulnier, L.&nbsp;R. Lavaud, M.-A.
Lachaux, P.&nbsp;Stock, T.&nbsp;L. Scao, T.&nbsp;Lavril, T.&nbsp;Wang, T.&nbsp;Lacroix, and W.&nbsp;E.
Sayed, “Mistral 7b,” 2023.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yao, R.&nbsp;Y. Aminabadi, O.&nbsp;Ruwase, S.&nbsp;Rajbhandari, X.&nbsp;Wu, A.&nbsp;A. Awan,
J.&nbsp;Rasley, M.&nbsp;Zhang, C.&nbsp;Li, C.&nbsp;Holmes, Z.&nbsp;Zhou, M.&nbsp;Wyatt, M.&nbsp;Smith,
L.&nbsp;Kurilenko, H.&nbsp;Qin, M.&nbsp;Tanaka, S.&nbsp;Che, S.&nbsp;L. Song, and Y.&nbsp;He,
“DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like
Models at All Scales,” <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.01320</em>, 2023.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
A.&nbsp;Paszke, S.&nbsp;Gross, F.&nbsp;Massa, A.&nbsp;Lerer, J.&nbsp;Bradbury, G.&nbsp;Chanan, T.&nbsp;Killeen,
Z.&nbsp;Lin, N.&nbsp;Gimelshein, L.&nbsp;Antiga, A.&nbsp;Desmaison, A.&nbsp;Köpf, E.&nbsp;Z. Yang,
Z.&nbsp;DeVito, M.&nbsp;Raison, A.&nbsp;Tejani, S.&nbsp;Chilamkurthy, B.&nbsp;Steiner, L.&nbsp;Fang,
J.&nbsp;Bai, and S.&nbsp;Chintala, “Pytorch: An imperative style, high-performance
deep learning library,” in <em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</em>, H.&nbsp;M. Wallach,
H.&nbsp;Larochelle, A.&nbsp;Beygelzimer, F.&nbsp;d’Alché-Buc, E.&nbsp;B. Fox, and
R.&nbsp;Garnett, Eds., 2019, pp. 8024–8035.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
M.&nbsp;Abadi, P.&nbsp;Barham, J.&nbsp;Chen, Z.&nbsp;Chen, A.&nbsp;Davis, J.&nbsp;Dean, M.&nbsp;Devin,
S.&nbsp;Ghemawat, G.&nbsp;Irving, M.&nbsp;Isard, M.&nbsp;Kudlur, J.&nbsp;Levenberg, R.&nbsp;Monga,
S.&nbsp;Moore, D.&nbsp;G. Murray, B.&nbsp;Steiner, P.&nbsp;A. Tucker, V.&nbsp;Vasudevan, P.&nbsp;Warden,
M.&nbsp;Wicke, Y.&nbsp;Yu, and X.&nbsp;Zheng, “Tensorflow: A system for large-scale
machine learning,” in <em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">12th USENIX Symposium on Operating Systems
Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4,
2016</em>, K.&nbsp;Keeton and T.&nbsp;Roscoe, Eds.&nbsp;&nbsp;&nbsp;USENIX Association, 2016, pp. 265–283.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
T.&nbsp;Chen, M.&nbsp;Li, Y.&nbsp;Li, M.&nbsp;Lin, N.&nbsp;Wang, M.&nbsp;Wang, T.&nbsp;Xiao, B.&nbsp;Xu, C.&nbsp;Zhang, and
Z.&nbsp;Zhang, “Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems,” <em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1512.01274, 2015.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Y.&nbsp;Ma, D.&nbsp;Yu, T.&nbsp;Wu, and H.&nbsp;Wang, “Paddlepaddle: An open-source deep learning
platform from industrial practice,” <em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">Frontiers of Data and Domputing</em>,
vol.&nbsp;1, no.&nbsp;1, p. 105, 2019.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
J.&nbsp;Yuan, X.&nbsp;Li, C.&nbsp;Cheng, J.&nbsp;Liu, R.&nbsp;Guo, S.&nbsp;Cai, C.&nbsp;Yao, F.&nbsp;Yang, X.&nbsp;Yi,
C.&nbsp;Wu, H.&nbsp;Zhang, and J.&nbsp;Zhao, “Oneflow: Redesign the distributed deep
learning framework from scratch,” <em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2110.15032, 2021.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
S.&nbsp;Roller, E.&nbsp;Dinan, N.&nbsp;Goyal, D.&nbsp;Ju, M.&nbsp;Williamson, Y.&nbsp;Liu, J.&nbsp;Xu, M.&nbsp;Ott,
E.&nbsp;M. Smith, Y.&nbsp;Boureau, and J.&nbsp;Weston, “Recipes for building an open-domain
chatbot,” in <em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main Volume, EACL
2021, Online, April 19 - 23, 2021</em>, 2021, pp. 300–325.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
A.&nbsp;Lewkowycz, A.&nbsp;Andreassen, D.&nbsp;Dohan, E.&nbsp;Dyer, H.&nbsp;Michalewski, V.&nbsp;V. Ramasesh,
A.&nbsp;Slone, C.&nbsp;Anil, I.&nbsp;Schlag, T.&nbsp;Gutman-Solo, Y.&nbsp;Wu, B.&nbsp;Neyshabur,
G.&nbsp;Gur-Ari, and V.&nbsp;Misra, “Solving quantitative reasoning problems with
language models,” <em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2206.14858, 2022.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
T.&nbsp;Saier, J.&nbsp;Krause, and M.&nbsp;Färber, “unarxive 2022: All arxiv publications
pre-processed for nlp, including structured full-text and citation network,”
<em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14957</em>, 2023.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
H.&nbsp;A. Simon, “Experiments with a heuristic compiler,” <em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">J. ACM</em>,
vol.&nbsp;10, no.&nbsp;4, pp. 493–506, 1963.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Z.&nbsp;Manna and R.&nbsp;J. Waldinger, “Toward automatic program synthesis,”
<em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em>, vol.&nbsp;14, no.&nbsp;3, pp. 151–165, 1971.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Z.&nbsp;Feng, D.&nbsp;Guo, D.&nbsp;Tang, N.&nbsp;Duan, X.&nbsp;Feng, M.&nbsp;Gong, L.&nbsp;Shou, B.&nbsp;Qin, T.&nbsp;Liu,
D.&nbsp;Jiang, and M.&nbsp;Zhou, “Codebert: A pre-trained model for programming and
natural languages,” in <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
J.&nbsp;Austin, A.&nbsp;Odena, M.&nbsp;I. Nye, M.&nbsp;Bosma, H.&nbsp;Michalewski, D.&nbsp;Dohan, E.&nbsp;Jiang,
C.&nbsp;J. Cai, M.&nbsp;Terry, Q.&nbsp;V. Le, and C.&nbsp;Sutton, “Program synthesis with large
language models,” <em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2108.07732, 2021.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
S.&nbsp;Black, L.&nbsp;Gao, P.&nbsp;Wang, C.&nbsp;Leahy, and S.&nbsp;Biderman, “GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorflow,” 2021.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
F.&nbsp;F. Xu, U.&nbsp;Alon, G.&nbsp;Neubig, and V.&nbsp;J. Hellendoorn, “A systematic evaluation
of large language models of code,” in <em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">MAPS@PLDI</em>, 2022.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
A.&nbsp;Madaan, S.&nbsp;Zhou, U.&nbsp;Alon, Y.&nbsp;Yang, and G.&nbsp;Neubig, “Language models of code
are few-shot commonsense learners,” in <em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022
Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,
Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, Y.&nbsp;Goldberg,
Z.&nbsp;Kozareva, and Y.&nbsp;Zhang, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 1384–1403.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
S.&nbsp;Longpre, G.&nbsp;Yauney, E.&nbsp;Reif, K.&nbsp;Lee, A.&nbsp;Roberts, B.&nbsp;Zoph, D.&nbsp;Zhou, J.&nbsp;Wei,
K.&nbsp;Robinson, D.&nbsp;Mimno <em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “A pretrainer’s guide to training data:
Measuring the effects of data age, domain coverage, quality, &amp; toxicity,”
<em id="bib.bib212.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13169</em>, 2023.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
D.&nbsp;Chen, Y.&nbsp;Huang, Z.&nbsp;Ma, H.&nbsp;Chen, X.&nbsp;Pan, C.&nbsp;Ge, D.&nbsp;Gao, Y.&nbsp;Xie, Z.&nbsp;Liu,
J.&nbsp;Gao, Y.&nbsp;Li, B.&nbsp;Ding, and J.&nbsp;Zhou, “Data-juicer: A one-stop data
processing system for large language models,” 2023.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
D.&nbsp;Hernandez, T.&nbsp;B. Brown, T.&nbsp;Conerly, N.&nbsp;DasSarma, D.&nbsp;Drain, S.&nbsp;E. Showk,
N.&nbsp;Elhage, Z.&nbsp;Hatfield-Dodds, T.&nbsp;Henighan, T.&nbsp;Hume, S.&nbsp;Johnston, B.&nbsp;Mann,
C.&nbsp;Olah, C.&nbsp;Olsson, D.&nbsp;Amodei, N.&nbsp;Joseph, J.&nbsp;Kaplan, and S.&nbsp;McCandlish,
“Scaling laws and interpretability of learning from repeated data,”
<em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2205.10487, 2022.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
A.&nbsp;Holtzman, J.&nbsp;Buys, L.&nbsp;Du, M.&nbsp;Forbes, and Y.&nbsp;Choi, “The curious case of
neural text degeneration,” in <em id="bib.bib215.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2020.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
K.&nbsp;Lee, D.&nbsp;Ippolito, A.&nbsp;Nystrom, C.&nbsp;Zhang, D.&nbsp;Eck, C.&nbsp;Callison-Burch, and
N.&nbsp;Carlini, “Deduplicating training data makes language models better,” in
<em id="bib.bib216.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022</em>, 2022, pp. 8424–8445.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
N.&nbsp;Carlini, D.&nbsp;Ippolito, M.&nbsp;Jagielski, K.&nbsp;Lee, F.&nbsp;Tramèr, and C.&nbsp;Zhang,
“Quantifying memorization across neural language models,” <em id="bib.bib217.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
2022.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
N.&nbsp;Carlini, F.&nbsp;Tramèr, E.&nbsp;Wallace, M.&nbsp;Jagielski, A.&nbsp;Herbert-Voss,
K.&nbsp;Lee, A.&nbsp;Roberts, T.&nbsp;B. Brown, D.&nbsp;Song, Ú.&nbsp;Erlingsson, A.&nbsp;Oprea, and
C.&nbsp;Raffel, “Extracting training data from large language models,” in
<em id="bib.bib218.1.1" class="ltx_emph ltx_font_italic">30th USENIX Security Symposium, USENIX Security 2021, August 11-13,
2021</em>, 2021, pp. 2633–2650.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
N.&nbsp;Kandpal, E.&nbsp;Wallace, and C.&nbsp;Raffel, “Deduplicating training data mitigates
privacy risks in language models,” in <em id="bib.bib219.1.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp.
10 697–10 707.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
J.&nbsp;D. Lafferty, A.&nbsp;McCallum, and F.&nbsp;C.&nbsp;N. Pereira, “Conditional random fields:
Probabilistic models for segmenting and labeling sequence data,” in
<em id="bib.bib220.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighteenth International Conference on Machine
Learning (ICML 2001), Williams College, Williamstown, MA, USA, June 28 -
July 1, 2001</em>, C.&nbsp;E. Brodley and A.&nbsp;P. Danyluk, Eds.&nbsp;&nbsp;&nbsp;Morgan Kaufmann, 2001, pp. 282–289.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
P.&nbsp;Gage, “A new algorithm for data compression,” <em id="bib.bib221.1.1" class="ltx_emph ltx_font_italic">C Users Journal</em>,
vol.&nbsp;12, no.&nbsp;2, pp. 23–38, 1994.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
R.&nbsp;Sennrich, B.&nbsp;Haddow, and A.&nbsp;Birch, “Neural machine translation of rare
words with subword units,” in <em id="bib.bib222.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers</em>.&nbsp;&nbsp;&nbsp;The Association for Computer Linguistics, 2016.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
M.&nbsp;Schuster and K.&nbsp;Nakajima, “Japanese and korean voice search,” in
<em id="bib.bib223.1.1" class="ltx_emph ltx_font_italic">2012 IEEE international conference on acoustics, speech and signal
processing (ICASSP)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2012, pp.
5149–5152.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wu, M.&nbsp;Schuster, Z.&nbsp;Chen, Q.&nbsp;V. Le, M.&nbsp;Norouzi, W.&nbsp;Macherey, M.&nbsp;Krikun,
Y.&nbsp;Cao, Q.&nbsp;Gao, K.&nbsp;Macherey, J.&nbsp;Klingner, A.&nbsp;Shah, M.&nbsp;Johnson, X.&nbsp;Liu,
L.&nbsp;Kaiser, S.&nbsp;Gouws, Y.&nbsp;Kato, T.&nbsp;Kudo, H.&nbsp;Kazawa, K.&nbsp;Stevens, G.&nbsp;Kurian,
N.&nbsp;Patil, W.&nbsp;Wang, C.&nbsp;Young, J.&nbsp;Smith, J.&nbsp;Riesa, A.&nbsp;Rudnick, O.&nbsp;Vinyals,
G.&nbsp;Corrado, M.&nbsp;Hughes, and J.&nbsp;Dean, “Google’s neural machine translation
system: Bridging the gap between human and machine translation,”
<em id="bib.bib224.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1609.08144, 2016.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
T.&nbsp;Kudo, “Subword regularization: Improving neural network translation models
with multiple subword candidates,” in <em id="bib.bib225.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics, ACL 2018,
Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers</em>, I.&nbsp;Gurevych
and Y.&nbsp;Miyao, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2018, pp. 66–75.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
T.&nbsp;Kudo and J.&nbsp;Richardson, “Sentencepiece: A simple and language independent
subword tokenizer and detokenizer for neural text processing,” in
<em id="bib.bib226.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium,
October 31 - November 4, 2018</em>, E.&nbsp;Blanco and W.&nbsp;Lu, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2018.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
M.&nbsp;Davis and M.&nbsp;Dürst, “Unicode normalization forms,” 2001.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
P.&nbsp;Nakkiran, G.&nbsp;Kaplun, Y.&nbsp;Bansal, T.&nbsp;Yang, B.&nbsp;Barak, and I.&nbsp;Sutskever, “Deep
double descent: Where bigger models and more data hurt,” in <em id="bib.bib228.1.1" class="ltx_emph ltx_font_italic">8th
International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2020.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
K.&nbsp;Tirumala, D.&nbsp;Simig, A.&nbsp;Aghajanyan, and A.&nbsp;S. Morcos, “D4: Improving llm
pretraining via document de-duplication and diversification,” <em id="bib.bib229.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2308.12284</em>, 2023.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Z.&nbsp;Shen, T.&nbsp;Tao, L.&nbsp;Ma, W.&nbsp;Neiswanger, J.&nbsp;Hestness, N.&nbsp;Vassilieva, D.&nbsp;Soboleva,
and E.&nbsp;Xing, “Slimpajama-dc: Understanding data combinations for llm
training,” <em id="bib.bib230.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10818</em>, 2023.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Xie, S.&nbsp;Santurkar, T.&nbsp;Ma, and P.&nbsp;Liang, “Data selection for language
models via importance resampling,” <em id="bib.bib231.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.03169</em>,
2023.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, W.&nbsp;Zhou, Q.&nbsp;Zhang, J.&nbsp;Zhou, S.&nbsp;Gao, J.&nbsp;Wang, M.&nbsp;Zhang, X.&nbsp;Gao,
Y.&nbsp;Chen, and T.&nbsp;Gui, “Farewell to aimless large-scale pretraining:
Influential subset selection for language model,” <em id="bib.bib232.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.12816</em>, 2023.

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
D.&nbsp;Paperno, G.&nbsp;Kruszewski, A.&nbsp;Lazaridou, Q.&nbsp;N. Pham, R.&nbsp;Bernardi, S.&nbsp;Pezzelle,
M.&nbsp;Baroni, G.&nbsp;Boleda, and R.&nbsp;Fernández, “The LAMBADA dataset: Word
prediction requiring a broad discourse context,” in <em id="bib.bib233.1.1" class="ltx_emph ltx_font_italic">ACL
(1)</em>.&nbsp;&nbsp;&nbsp;The Association for Computer
Linguistics, 2016.

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
M.&nbsp;F. Chen, N.&nbsp;Roberts, K.&nbsp;Bhatia, J.&nbsp;Wang, C.&nbsp;Zhang, F.&nbsp;Sala, and C.&nbsp;Ré,
“Skill-it! a data-driven skills framework for understanding and training
language models,” <em id="bib.bib234.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.14430</em>, 2023.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
B.&nbsp;Rozière, J.&nbsp;Gehring, F.&nbsp;Gloeckle, S.&nbsp;Sootla, I.&nbsp;Gat, X.&nbsp;E. Tan,
Y.&nbsp;Adi, J.&nbsp;Liu, T.&nbsp;Remez, J.&nbsp;Rapin, A.&nbsp;Kozhevnikov, I.&nbsp;Evtimov, J.&nbsp;Bitton,
M.&nbsp;Bhatt, C.&nbsp;Canton-Ferrer, A.&nbsp;Grattafiori, W.&nbsp;Xiong, A.&nbsp;Défossez,
J.&nbsp;Copet, F.&nbsp;Azhar, H.&nbsp;Touvron, L.&nbsp;Martin, N.&nbsp;Usunier, T.&nbsp;Scialom, and
G.&nbsp;Synnaeve, “Code llama: Open foundation models for code,” <em id="bib.bib235.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2308.12950, 2023.

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bengio, J.&nbsp;Louradour, R.&nbsp;Collobert, and J.&nbsp;Weston, “Curriculum learning,”
in <em id="bib.bib236.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2009, pp. 41–48.

</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
C.&nbsp;Xu, C.&nbsp;Rosset, L.&nbsp;Del&nbsp;Corro, S.&nbsp;Mahajan, J.&nbsp;McAuley, J.&nbsp;Neville, A.&nbsp;H.
Awadallah, and N.&nbsp;Rao, “Contrastive post-training large language models on
data curriculum,” <em id="bib.bib237.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.02263</em>, 2023.

</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
S.&nbsp;Tworkowski, K.&nbsp;Staniszewski, M.&nbsp;Pacek, Y.&nbsp;Wu, H.&nbsp;Michalewski, and P.&nbsp;Milos,
“Focused transformer: Contrastive training for context scaling,”
<em id="bib.bib238.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.03170, 2023.

</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
Z.&nbsp;Azerbayev, H.&nbsp;Schoelkopf, K.&nbsp;Paster, M.&nbsp;D. Santos, S.&nbsp;McAleer, A.&nbsp;Q. Jiang,
J.&nbsp;Deng, S.&nbsp;Biderman, and S.&nbsp;Welleck, “Llemma: An open language model for
mathematics,” <em id="bib.bib239.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10631</em>, 2023.

</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
S.&nbsp;Chen, S.&nbsp;Wong, L.&nbsp;Chen, and Y.&nbsp;Tian, “Extending context window of large
language models via positional interpolation,” <em id="bib.bib240.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.15595, 2023.

</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
G.&nbsp;Wenzek, M.-A. Lachaux, A.&nbsp;Conneau, V.&nbsp;Chaudhary, F.&nbsp;Guzmán, A.&nbsp;Joulin,
and É.&nbsp;Grave, “Ccnet: Extracting high quality monolingual datasets from
web crawl data,” in <em id="bib.bib241.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twelfth Language Resources and
Evaluation Conference</em>, 2020, pp. 4003–4012.

</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
A.&nbsp;Joulin, E.&nbsp;Grave, P.&nbsp;Bojanowski, and T.&nbsp;Mikolov, “Bag of tricks for
efficient text classification,” in <em id="bib.bib242.1.1" class="ltx_emph ltx_font_italic">EACL</em>, 2017, pp. 427–431.

</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
D.&nbsp;Chen, Y.&nbsp;Huang, Z.&nbsp;Ma, H.&nbsp;Chen, X.&nbsp;Pan, C.&nbsp;Ge, D.&nbsp;Gao, Y.&nbsp;Xie, Z.&nbsp;Liu,
J.&nbsp;Gao <em id="bib.bib243.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Data-juicer: A one-stop data processing system for
large language models,” <em id="bib.bib243.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.02033</em>, 2023.

</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
B.&nbsp;Zhang, B.&nbsp;Ghorbani, A.&nbsp;Bapna, Y.&nbsp;Cheng, X.&nbsp;Garcia, J.&nbsp;Shen, and O.&nbsp;Firat,
“Examining scaling and transfer of language model architectures for machine
translation,” in <em id="bib.bib244.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML
2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, 2022, pp.
26 176–26 192.

</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
L.&nbsp;Dong, N.&nbsp;Yang, W.&nbsp;Wang, F.&nbsp;Wei, X.&nbsp;Liu, Y.&nbsp;Wang, J.&nbsp;Gao, M.&nbsp;Zhou, and
H.&nbsp;Hon, “Unified language model pre-training for natural language
understanding and generation,” in <em id="bib.bib245.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</em>,
2019, pp. 13 042–13 054.

</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
A.&nbsp;Clark, D.&nbsp;de&nbsp;Las&nbsp;Casas, A.&nbsp;Guy, A.&nbsp;Mensch, M.&nbsp;Paganini, J.&nbsp;Hoffmann,
B.&nbsp;Damoc, B.&nbsp;A. Hechtman, T.&nbsp;Cai, S.&nbsp;Borgeaud, G.&nbsp;van&nbsp;den Driessche,
E.&nbsp;Rutherford, T.&nbsp;Hennigan, M.&nbsp;J. Johnson, A.&nbsp;Cassirer, C.&nbsp;Jones,
E.&nbsp;Buchatskaya, D.&nbsp;Budden, L.&nbsp;Sifre, S.&nbsp;Osindero, O.&nbsp;Vinyals, M.&nbsp;Ranzato,
J.&nbsp;W. Rae, E.&nbsp;Elsen, K.&nbsp;Kavukcuoglu, and K.&nbsp;Simonyan, “Unified scaling laws
for routed language models,” in <em id="bib.bib246.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, 2022,
pp. 4057–4086.

</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
A.&nbsp;Gu, K.&nbsp;Goel, and C.&nbsp;Ré, “Efficiently modeling long sequences with
structured state spaces,” in <em id="bib.bib247.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event, April 25-29,
2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022. [Online].
Available: <a target="_blank" href="https://openreview.net/forum?id=uYLFoz1vlAC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=uYLFoz1vlAC</a>

</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
H.&nbsp;Mehta, A.&nbsp;Gupta, A.&nbsp;Cutkosky, and B.&nbsp;Neyshabur, “Long range language
modeling via gated state spaces,” <em id="bib.bib248.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2206.13947, 2022.
[Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2206.13947" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2206.13947</a>

</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
T.&nbsp;Dao, D.&nbsp;Y. Fu, K.&nbsp;K. Saab, A.&nbsp;W. Thomas, A.&nbsp;Rudra, and C.&nbsp;Ré, “Hungry
hungry hippos: Towards language modeling with state space models,”
<em id="bib.bib249.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.14052, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2212.14052" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.14052</a>

</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
M.&nbsp;Poli, S.&nbsp;Massaroli, E.&nbsp;Nguyen, D.&nbsp;Y. Fu, T.&nbsp;Dao, S.&nbsp;Baccus, Y.&nbsp;Bengio,
S.&nbsp;Ermon, and C.&nbsp;Ré, “Hyena hierarchy: Towards larger convolutional
language models,” in <em id="bib.bib250.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023.

</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
B.&nbsp;Peng, E.&nbsp;Alcaide, Q.&nbsp;Anthony, A.&nbsp;Albalak, S.&nbsp;Arcadinho, H.&nbsp;Cao, X.&nbsp;Cheng,
M.&nbsp;Chung, M.&nbsp;Grella, K.&nbsp;K.&nbsp;G. V., X.&nbsp;He, H.&nbsp;Hou, P.&nbsp;Kazienko, J.&nbsp;Kocon,
J.&nbsp;Kong, B.&nbsp;Koptyra, H.&nbsp;Lau, K.&nbsp;S.&nbsp;I. Mantri, F.&nbsp;Mom, A.&nbsp;Saito, X.&nbsp;Tang,
B.&nbsp;Wang, J.&nbsp;S. Wind, S.&nbsp;Wozniak, R.&nbsp;Zhang, Z.&nbsp;Zhang, Q.&nbsp;Zhao, P.&nbsp;Zhou,
J.&nbsp;Zhu, and R.&nbsp;Zhu, “RWKV: reinventing rnns for the transformer era,”
<em id="bib.bib251.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13048, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.13048" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.13048</a>

</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Y.&nbsp;Sun, L.&nbsp;Dong, S.&nbsp;Huang, S.&nbsp;Ma, Y.&nbsp;Xia, J.&nbsp;Xue, J.&nbsp;Wang, and F.&nbsp;Wei,
“Retentive network: A successor to transformer for large language models,”
<em id="bib.bib252.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.08621</em>, 2023.

</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
J.&nbsp;T. Smith, A.&nbsp;Warrington, and S.&nbsp;Linderman, “Simplified state space layers
for sequence modeling,” in <em id="bib.bib253.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2023.

</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
A.&nbsp;Orvieto, S.&nbsp;L. Smith, A.&nbsp;Gu, A.&nbsp;Fernando, C.&nbsp;Gulcehre, R.&nbsp;Pascanu, and
S.&nbsp;De, “Resurrecting recurrent neural networks for long sequences,” in
<em id="bib.bib254.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023.

</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
M.&nbsp;Ding, Z.&nbsp;Yang, W.&nbsp;Hong, W.&nbsp;Zheng, C.&nbsp;Zhou, D.&nbsp;Yin, J.&nbsp;Lin, X.&nbsp;Zou, Z.&nbsp;Shao,
H.&nbsp;Yang, and J.&nbsp;Tang, “Cogview: Mastering text-to-image generation via
transformers,” in <em id="bib.bib255.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual</em>, 2021, pp. 19 822–19 835.

</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
L.&nbsp;J. Ba, J.&nbsp;R. Kiros, and G.&nbsp;E. Hinton, “Layer normalization,” vol.
abs/1607.06450, 2016.

</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
B.&nbsp;Zhang and R.&nbsp;Sennrich, “Root mean square layer normalization,” in
<em id="bib.bib257.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada</em>, 2019, pp. 12 360–12 371.

</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
H.&nbsp;Wang, S.&nbsp;Ma, L.&nbsp;Dong, S.&nbsp;Huang, D.&nbsp;Zhang, and F.&nbsp;Wei, “Deepnet: Scaling
transformers to 1, 000 layers,” vol. abs/2203.00555, 2022.

</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
V.&nbsp;Nair and G.&nbsp;E. Hinton, “Rectified linear units improve restricted boltzmann
machines,” in <em id="bib.bib259.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th international conference on
machine learning (ICML-10)</em>, 2010, pp. 807–814.

</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
A.&nbsp;Wang, A.&nbsp;Singh, J.&nbsp;Michael, F.&nbsp;Hill, O.&nbsp;Levy, and S.&nbsp;R. Bowman, “GLUE:
A multi-task benchmark and analysis platform for natural language
understanding,” in <em id="bib.bib260.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop: Analyzing and
Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels,
Belgium, November 1, 2018</em>, T.&nbsp;Linzen, G.&nbsp;Chrupala, and A.&nbsp;Alishahi,
Eds.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2018, pp. 353–355.

</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
P.&nbsp;Ramachandran, B.&nbsp;Zoph, and Q.&nbsp;V. Le, “Searching for activation functions,”
<em id="bib.bib261.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.05941</em>, 2017.

</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer, “GLU variants improve transformer,” vol. abs/2002.05202, 2020.

</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
J.&nbsp;Su, Y.&nbsp;Lu, S.&nbsp;Pan, B.&nbsp;Wen, and Y.&nbsp;Liu, “Roformer: Enhanced transformer with
rotary position embedding,” vol. abs/2104.09864, 2021.

</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
O.&nbsp;Press, N.&nbsp;A. Smith, and M.&nbsp;Lewis, “Train short, test long: Attention with
linear biases enables input length extrapolation,” in <em id="bib.bib264.1.1" class="ltx_emph ltx_font_italic">The Tenth
International Conference on Learning Representations, ICLR 2022, Virtual
Event, April 25-29, 2022</em>, 2022.

</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
S.&nbsp;Ioffe and C.&nbsp;Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” in <em id="bib.bib265.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
32nd International Conference on Machine Learning, ICML 2015, Lille,
France, 6-11 July 2015</em>, ser. JMLR Workshop and Conference Proceedings,
F.&nbsp;R. Bach and D.&nbsp;M. Blei, Eds., vol.&nbsp;37.&nbsp;&nbsp;&nbsp;JMLR.org, 2015, pp. 448–456. [Online]. Available:
<a target="_blank" href="http://proceedings.mlr.press/v37/ioffe15.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v37/ioffe15.html</a>

</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
S.&nbsp;Narang, H.&nbsp;W. Chung, Y.&nbsp;Tay, L.&nbsp;Fedus, T.&nbsp;Févry, M.&nbsp;Matena, K.&nbsp;Malkan,
N.&nbsp;Fiedel, N.&nbsp;Shazeer, Z.&nbsp;Lan, Y.&nbsp;Zhou, W.&nbsp;Li, N.&nbsp;Ding, J.&nbsp;Marcus,
A.&nbsp;Roberts, and C.&nbsp;Raffel, “Do transformer modifications transfer across
implementations and applications?” in <em id="bib.bib266.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, 2021,
pp. 5758–5773.

</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
R.&nbsp;Xiong, Y.&nbsp;Yang, D.&nbsp;He, K.&nbsp;Zheng, S.&nbsp;Zheng, C.&nbsp;Xing, H.&nbsp;Zhang, Y.&nbsp;Lan,
L.&nbsp;Wang, and T.&nbsp;Liu, “On layer normalization in the transformer
architecture,” in <em id="bib.bib267.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
A.&nbsp;Baevski and M.&nbsp;Auli, “Adaptive input representations for neural language
modeling,” in <em id="bib.bib268.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2019.

</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
L.&nbsp;Liu, X.&nbsp;Liu, J.&nbsp;Gao, W.&nbsp;Chen, and J.&nbsp;Han, “Understanding the difficulty of
training transformers,” in <em id="bib.bib269.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2020, pp. 5747–5763.

</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks and K.&nbsp;Gimpel, “Gaussian error linear units (gelus),”
<em id="bib.bib270.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.08415</em>, 2016.

</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
Y.&nbsp;N. Dauphin, A.&nbsp;Fan, M.&nbsp;Auli, and D.&nbsp;Grangier, “Language modeling with gated
convolutional networks,” in <em id="bib.bib271.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017</em>, 2017, pp. 933–941.

</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
T.&nbsp;L. Scao, T.&nbsp;Wang, D.&nbsp;Hesslow, S.&nbsp;Bekman, M.&nbsp;S. Bari, S.&nbsp;Biderman,
H.&nbsp;Elsahar, N.&nbsp;Muennighoff, J.&nbsp;Phang, O.&nbsp;Press, C.&nbsp;Raffel, V.&nbsp;Sanh, S.&nbsp;Shen,
L.&nbsp;Sutawika, J.&nbsp;Tae, Z.&nbsp;X. Yong, J.&nbsp;Launay, and I.&nbsp;Beltagy, “What language
model to train if you have one million GPU hours?” in <em id="bib.bib272.1.1" class="ltx_emph ltx_font_italic">Findings of
the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi,
United Arab Emirates, December 7-11, 2022</em>, 2022, pp. 765–782.

</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
P.&nbsp;Shaw, J.&nbsp;Uszkoreit, and A.&nbsp;Vaswani, “Self-attention with relative position
representations,” in <em id="bib.bib273.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6,
2018, Volume 2 (Short Papers)</em>, M.&nbsp;A. Walker, H.&nbsp;Ji, and A.&nbsp;Stent, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics,
2018, pp. 464–468. [Online]. Available:
<a target="_blank" href="https://doi.org/10.18653/v1/n18-2074" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/n18-2074</a>

</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
Z.&nbsp;Dai, Z.&nbsp;Yang, Y.&nbsp;Yang, J.&nbsp;G. Carbonell, Q.&nbsp;V. Le, and R.&nbsp;Salakhutdinov,
“Transformer-xl: Attentive language models beyond a fixed-length context,”
in <em id="bib.bib274.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Conference of the Association for
Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers</em>, A.&nbsp;Korhonen, D.&nbsp;R. Traum, and L.&nbsp;Màrquez,
Eds.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2019, pp. 2978–2988. [Online]. Available:
<a target="_blank" href="https://doi.org/10.18653/v1/p19-1285" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/p19-1285</a>

</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yang, Z.&nbsp;Dai, Y.&nbsp;Yang, J.&nbsp;Carbonell, R.&nbsp;R. Salakhutdinov, and Q.&nbsp;V. Le,
“Xlnet: Generalized autoregressive pretraining for language understanding,”
<em id="bib.bib275.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.&nbsp;32, 2019.

</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
B.&nbsp;Peng, J.&nbsp;Quesnelle, H.&nbsp;Fan, and E.&nbsp;Shippole, “Yarn: Efficient context
window extension of large language models,” <em id="bib.bib276.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2309.00071, 2023.

</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
Y.&nbsp;Sun, L.&nbsp;Dong, B.&nbsp;Patra, S.&nbsp;Ma, S.&nbsp;Huang, A.&nbsp;Benhaim, V.&nbsp;Chaudhary, X.&nbsp;Song,
and F.&nbsp;Wei, “A length-extrapolatable transformer,” <em id="bib.bib277.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2212.10554, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2212.10554" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.10554</a>

</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
H.&nbsp;Peng, N.&nbsp;Pappas, D.&nbsp;Yogatama, R.&nbsp;Schwartz, N.&nbsp;A. Smith, and L.&nbsp;Kong,
“Random feature attention,” in <em id="bib.bib278.1.1" class="ltx_emph ltx_font_italic">9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021</em>.

</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
M.&nbsp;Zaheer, G.&nbsp;Guruganesh, K.&nbsp;A. Dubey, J.&nbsp;Ainslie, C.&nbsp;Alberti,
S.&nbsp;Ontañón, P.&nbsp;Pham, A.&nbsp;Ravula, Q.&nbsp;Wang, L.&nbsp;Yang, and A.&nbsp;Ahmed,
“Big bird: Transformers for longer sequences,” in <em id="bib.bib279.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, and I.&nbsp;Sutskever, “Generating long sequences
with sparse transformers,” <em id="bib.bib280.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1904.10509, 2019.

</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer, “Fast transformer decoding: One write-head is all you need,”
<em id="bib.bib281.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1911.02150, 2019. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1911.02150" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1911.02150</a>

</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
J.&nbsp;Ainslie, J.&nbsp;Lee-Thorp, M.&nbsp;de&nbsp;Jong, Y.&nbsp;Zemlyanskiy, F.&nbsp;Lebrón, and
S.&nbsp;Sanghai, “Gqa: Training generalized multi-query transformer models from
multi-head checkpoints,” <em id="bib.bib282.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
T.&nbsp;Dao, D.&nbsp;Y. Fu, S.&nbsp;Ermon, A.&nbsp;Rudra, and C.&nbsp;Re, “Flashattention: Fast and
memory-efficient exact attention with IO-awareness,” in <em id="bib.bib283.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>,
2022.

</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
T.&nbsp;Dao, “Flashattention-2: Faster attention with better parallelism and work
partitioning,” <em id="bib.bib284.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.08691</em>, 2023.

</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
“vllm: Easy, fast, and cheap llm serving with pagedattention.” [Online].
Available: <a target="_blank" href="https://vllm.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vllm.ai/</a>

</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
A.&nbsp;Yuan, A.&nbsp;Coenen, E.&nbsp;Reif, and D.&nbsp;Ippolito, “Wordcraft: story writing with
large language models,” in <em id="bib.bib286.1.1" class="ltx_emph ltx_font_italic">27th International Conference on
Intelligent User Interfaces</em>, 2022, pp. 841–852.

</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
A.&nbsp;Kazemnejad, I.&nbsp;Padhi, K.&nbsp;N. Ramamurthy, P.&nbsp;Das, and S.&nbsp;Reddy, “The impact
of positional encoding on length generalization in transformers,”
<em id="bib.bib287.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.19466, 2023.

</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
W.&nbsp;Xiong, J.&nbsp;Liu, I.&nbsp;Molybog, H.&nbsp;Zhang, P.&nbsp;Bhargava, R.&nbsp;Hou, L.&nbsp;Martin,
R.&nbsp;Rungta, K.&nbsp;A. Sankararaman, B.&nbsp;Oguz, M.&nbsp;Khabsa, H.&nbsp;Fang, Y.&nbsp;Mehdad,
S.&nbsp;Narang, K.&nbsp;Malik, A.&nbsp;Fan, S.&nbsp;Bhosale, S.&nbsp;Edunov, M.&nbsp;Lewis, S.&nbsp;Wang, and
H.&nbsp;Ma, “Effective long-context scaling of foundation models,” <em id="bib.bib288.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2309.16039, 2023.

</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
kaiokendev, “Things I’m learning while training superhot.” 2023.

</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
Z.&nbsp;Dong, T.&nbsp;Tang, J.&nbsp;Li, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “BAMBOO: A comprehensive
benchmark for evaluating long text modeling capacities of large language
models,” <em id="bib.bib290.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.13345, 2023.

</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
J.&nbsp;Su. (2023) Transformer upgrade path: 12, infinite extrapolation of rerope?

</span>
</li>
<li id="bib.bib292" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[292]</span>
<span class="ltx_bibblock">
X.&nbsp;Liu, H.&nbsp;Yan, S.&nbsp;Zhang, C.&nbsp;An, X.&nbsp;Qiu, and D.&nbsp;Lin, “Scaling laws of
rope-based extrapolation,” <em id="bib.bib292.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.05209, 2023.

</span>
</li>
<li id="bib.bib293" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[293]</span>
<span class="ltx_bibblock">
A.&nbsp;Pal, D.&nbsp;Karkhanis, M.&nbsp;Roberts, S.&nbsp;Dooley, A.&nbsp;Sundararajan, and S.&nbsp;Naidu,
“Giraffe: Adventures in expanding context lengths in llms,” <em id="bib.bib293.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2308.10882, 2023.

</span>
</li>
<li id="bib.bib294" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[294]</span>
<span class="ltx_bibblock">
G.&nbsp;Izacard and E.&nbsp;Grave, “Leveraging passage retrieval with generative models
for open domain question answering,” in <em id="bib.bib294.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th
Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics,
2021, pp. 874–880.

</span>
</li>
<li id="bib.bib295" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[295]</span>
<span class="ltx_bibblock">
N.&nbsp;Ratner, Y.&nbsp;Levine, Y.&nbsp;Belinkov, O.&nbsp;Ram, I.&nbsp;Magar, O.&nbsp;Abend, E.&nbsp;Karpas,
A.&nbsp;Shashua, K.&nbsp;Leyton-Brown, and Y.&nbsp;Shoham, “Parallel context windows for
large language models,” in <em id="bib.bib295.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, July 9-14, 2023</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2023, pp. 6383–6402.

</span>
</li>
<li id="bib.bib296" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[296]</span>
<span class="ltx_bibblock">
Y.&nbsp;Hao, Y.&nbsp;Sun, L.&nbsp;Dong, Z.&nbsp;Han, Y.&nbsp;Gu, and F.&nbsp;Wei, “Structured prompting:
Scaling in-context learning to 1, 000 examples,” <em id="bib.bib296.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2022.

</span>
</li>
<li id="bib.bib297" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[297]</span>
<span class="ltx_bibblock">
I.&nbsp;Beltagy, M.&nbsp;E. Peters, and A.&nbsp;Cohan, “Longformer: The long-document
transformer,” <em id="bib.bib297.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2004.05150, 2020.

</span>
</li>
<li id="bib.bib298" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[298]</span>
<span class="ltx_bibblock">
G.&nbsp;Xiao, Y.&nbsp;Tian, B.&nbsp;Chen, S.&nbsp;Han, and M.&nbsp;Lewis, “Efficient streaming language
models with attention sinks,” <em id="bib.bib298.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.17453, 2023.

</span>
</li>
<li id="bib.bib299" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[299]</span>
<span class="ltx_bibblock">
N.&nbsp;F. Liu, K.&nbsp;Lin, J.&nbsp;Hewitt, A.&nbsp;Paranjape, M.&nbsp;Bevilacqua, F.&nbsp;Petroni, and
P.&nbsp;Liang, “Lost in the middle: How language models use long contexts,”
<em id="bib.bib299.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.03172, 2023.

</span>
</li>
<li id="bib.bib300" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[300]</span>
<span class="ltx_bibblock">
C.&nbsp;Han, Q.&nbsp;Wang, W.&nbsp;Xiong, Y.&nbsp;Chen, H.&nbsp;Ji, and S.&nbsp;Wang, “Lm-infinite: Simple
on-the-fly length generalization for large language models,” <em id="bib.bib300.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2308.16137, 2023.

</span>
</li>
<li id="bib.bib301" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[301]</span>
<span class="ltx_bibblock">
A.&nbsp;Bertsch, U.&nbsp;Alon, G.&nbsp;Neubig, and M.&nbsp;R. Gormley, “Unlimiformer: Long-range
transformers with unlimited length input,” <em id="bib.bib301.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.01625,
2023.

</span>
</li>
<li id="bib.bib302" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[302]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wu, M.&nbsp;N. Rabe, D.&nbsp;Hutchins, and C.&nbsp;Szegedy, “Memorizing transformers,” in
<em id="bib.bib302.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib303" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[303]</span>
<span class="ltx_bibblock">
H.&nbsp;Chen, R.&nbsp;Pasunuru, J.&nbsp;Weston, and A.&nbsp;Celikyilmaz, “Walking down the memory
maze: Beyond context limit through interactive reading,” <em id="bib.bib303.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2310.05029, 2023.

</span>
</li>
<li id="bib.bib304" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[304]</span>
<span class="ltx_bibblock">
W.&nbsp;Zhou, Y.&nbsp;E. Jiang, P.&nbsp;Cui, T.&nbsp;Wang, Z.&nbsp;Xiao, Y.&nbsp;Hou, R.&nbsp;Cotterell, and
M.&nbsp;Sachan, “Recurrentgpt: Interactive generation of (arbitrarily) long
text,” <em id="bib.bib304.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13304, 2023.

</span>
</li>
<li id="bib.bib305" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[305]</span>
<span class="ltx_bibblock">
C.&nbsp;Packer, V.&nbsp;Fang, S.&nbsp;G. Patil, K.&nbsp;Lin, S.&nbsp;Wooders, and J.&nbsp;E. Gonzalez,
“Memgpt: Towards llms as operating systems,” <em id="bib.bib305.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2310.08560, 2023.

</span>
</li>
<li id="bib.bib306" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[306]</span>
<span class="ltx_bibblock">
P.&nbsp;Xu, W.&nbsp;Ping, X.&nbsp;Wu, L.&nbsp;McAfee, C.&nbsp;Zhu, Z.&nbsp;Liu, S.&nbsp;Subramanian,
E.&nbsp;Bakhturina, M.&nbsp;Shoeybi, and B.&nbsp;Catanzaro, “Retrieval meets long context
large language models,” <em id="bib.bib306.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.03025, 2023.

</span>
</li>
<li id="bib.bib307" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[307]</span>
<span class="ltx_bibblock">
K.&nbsp;Murray and D.&nbsp;Chiang, “Correcting length bias in neural machine
translation,” in <em id="bib.bib307.1.1" class="ltx_emph ltx_font_italic">WMT</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2018, pp. 212–223.

</span>
</li>
<li id="bib.bib308" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[308]</span>
<span class="ltx_bibblock">
A.&nbsp;Holtzman, J.&nbsp;Buys, L.&nbsp;Du, M.&nbsp;Forbes, and Y.&nbsp;Choi, “The curious case of
neural text degeneration,” in <em id="bib.bib308.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2020.

</span>
</li>
<li id="bib.bib309" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[309]</span>
<span class="ltx_bibblock">
C.-M. U. P. P. D. O.&nbsp;C. SCIENCE, <em id="bib.bib309.1.1" class="ltx_emph ltx_font_italic">Speech Understanding Systems. Summary of
Results of the Five-Year Research Effort at Carnegie-Mellon University</em>,
1977.

</span>
</li>
<li id="bib.bib310" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[310]</span>
<span class="ltx_bibblock">
P.&nbsp;Koehn and R.&nbsp;Knowles, “Six challenges for neural machine translation,” in
<em id="bib.bib310.1.1" class="ltx_emph ltx_font_italic">NMT@ACL</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2017, pp. 28–39.

</span>
</li>
<li id="bib.bib311" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[311]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wu, M.&nbsp;Schuster, Z.&nbsp;Chen, Q.&nbsp;V. Le, M.&nbsp;Norouzi, W.&nbsp;Macherey, M.&nbsp;Krikun,
Y.&nbsp;Cao, Q.&nbsp;Gao, K.&nbsp;Macherey, J.&nbsp;Klingner, A.&nbsp;Shah, M.&nbsp;Johnson, X.&nbsp;Liu,
L.&nbsp;Kaiser, S.&nbsp;Gouws, Y.&nbsp;Kato, T.&nbsp;Kudo, H.&nbsp;Kazawa, K.&nbsp;Stevens, G.&nbsp;Kurian,
N.&nbsp;Patil, W.&nbsp;Wang, C.&nbsp;Young, J.&nbsp;Smith, J.&nbsp;Riesa, A.&nbsp;Rudnick, O.&nbsp;Vinyals,
G.&nbsp;Corrado, M.&nbsp;Hughes, and J.&nbsp;Dean, “Google’s neural machine translation
system: Bridging the gap between human and machine translation,”
<em id="bib.bib311.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1609.08144, 2016.

</span>
</li>
<li id="bib.bib312" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[312]</span>
<span class="ltx_bibblock">
R.&nbsp;Paulus, C.&nbsp;Xiong, and R.&nbsp;Socher, “A deep reinforced model for abstractive
summarization,” in <em id="bib.bib312.1.1" class="ltx_emph ltx_font_italic">ICLR (Poster)</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2018.

</span>
</li>
<li id="bib.bib313" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[313]</span>
<span class="ltx_bibblock">
A.&nbsp;K. Vijayakumar, M.&nbsp;Cogswell, R.&nbsp;R. Selvaraju, Q.&nbsp;Sun, S.&nbsp;Lee, D.&nbsp;J.
Crandall, and D.&nbsp;Batra, “Diverse beam search: Decoding diverse solutions
from neural sequence models,” <em id="bib.bib313.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1610.02424, 2016.

</span>
</li>
<li id="bib.bib314" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[314]</span>
<span class="ltx_bibblock">
A.&nbsp;Fan, M.&nbsp;Lewis, and Y.&nbsp;N. Dauphin, “Hierarchical neural story generation,”
in <em id="bib.bib314.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2018, pp. 889–898.

</span>
</li>
<li id="bib.bib315" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[315]</span>
<span class="ltx_bibblock">
J.&nbsp;Hewitt, C.&nbsp;D. Manning, and P.&nbsp;Liang, “Truncation sampling as language model
desmoothing,” in <em id="bib.bib315.1.1" class="ltx_emph ltx_font_italic">EMNLP (Findings)</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 3414–3427.

</span>
</li>
<li id="bib.bib316" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[316]</span>
<span class="ltx_bibblock">
Y.&nbsp;Su, T.&nbsp;Lan, Y.&nbsp;Wang, D.&nbsp;Yogatama, L.&nbsp;Kong, and N.&nbsp;Collier, “A contrastive
framework for neural text generation,” in <em id="bib.bib316.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib317" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[317]</span>
<span class="ltx_bibblock">
C.&nbsp;Meister, T.&nbsp;Pimentel, G.&nbsp;Wiher, and R.&nbsp;Cotterell, “Locally typical
sampling,” <em id="bib.bib317.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, 2023.

</span>
</li>
<li id="bib.bib318" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[318]</span>
<span class="ltx_bibblock">
X.&nbsp;L. Li, A.&nbsp;Holtzman, D.&nbsp;Fried, P.&nbsp;Liang, J.&nbsp;Eisner, T.&nbsp;Hashimoto,
L.&nbsp;Zettlemoyer, and M.&nbsp;Lewis, “Contrastive decoding: Open-ended text
generation as optimization,” in <em id="bib.bib318.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2023, pp.
12 286–12 312.

</span>
</li>
<li id="bib.bib319" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[319]</span>
<span class="ltx_bibblock">
Y.&nbsp;Chuang, Y.&nbsp;Xie, H.&nbsp;Luo, Y.&nbsp;Kim, J.&nbsp;R. Glass, and P.&nbsp;He, “Dola: Decoding by
contrasting layers improves factuality in large language models,”
<em id="bib.bib319.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.03883, 2023.

</span>
</li>
<li id="bib.bib320" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[320]</span>
<span class="ltx_bibblock">
L.&nbsp;Chen, “Dissecting batching effects in gpt inference,” 2023. [Online].
Available: <a target="_blank" href="https://le.qun.ch/en/blog/2023/05/13/transformer-batching/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://le.qun.ch/en/blog/2023/05/13/transformer-batching/</a>

</span>
</li>
<li id="bib.bib321" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[321]</span>
<span class="ltx_bibblock">
Y.&nbsp;Sheng, L.&nbsp;Zheng, B.&nbsp;Yuan, Z.&nbsp;Li, M.&nbsp;Ryabinin, B.&nbsp;Chen, P.&nbsp;Liang,
C.&nbsp;Ré, I.&nbsp;Stoica, and C.&nbsp;Zhang, “Flexgen: High-throughput generative
inference of large language models with a single GPU,” in <em id="bib.bib321.1.1" class="ltx_emph ltx_font_italic">ICML</em>,
ser. Proceedings of Machine Learning Research, vol. 202.&nbsp;&nbsp;&nbsp;PMLR, 2023, pp. 31 094–31 116.

</span>
</li>
<li id="bib.bib322" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[322]</span>
<span class="ltx_bibblock">
T.&nbsp;Dao, D.&nbsp;Haziza, F.&nbsp;Massa, and G.&nbsp;Sizov, “Flash-decoding for long-context
inference,” <a target="_blank" href="https://crfm.stanford.edu/2023/10/12/flashdecoding.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://crfm.stanford.edu/2023/10/12/flashdecoding.html</a>,
2023.

</span>
</li>
<li id="bib.bib323" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[323]</span>
<span class="ltx_bibblock">
Y.&nbsp;Leviathan, M.&nbsp;Kalman, and Y.&nbsp;Matias, “Fast inference from transformers via
speculative decoding,” in <em id="bib.bib323.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>, 2023.

</span>
</li>
<li id="bib.bib324" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[324]</span>
<span class="ltx_bibblock">
C.&nbsp;Chen, S.&nbsp;Borgeaud, G.&nbsp;Irving, J.&nbsp;Lespiau, L.&nbsp;Sifre, and J.&nbsp;Jumper,
“Accelerating large language model decoding with speculative sampling,”
<em id="bib.bib324.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.01318, 2023.

</span>
</li>
<li id="bib.bib325" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[325]</span>
<span class="ltx_bibblock">
X.&nbsp;Miao, G.&nbsp;Oliaro, Z.&nbsp;Zhang, X.&nbsp;Cheng, Z.&nbsp;Wang, R.&nbsp;Y.&nbsp;Y. Wong, Z.&nbsp;Chen,
D.&nbsp;Arfeen, R.&nbsp;Abhyankar, and Z.&nbsp;Jia, “Specinfer: Accelerating generative
LLM serving with speculative inference and token tree verification,”
<em id="bib.bib325.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.09781, 2023.

</span>
</li>
<li id="bib.bib326" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[326]</span>
<span class="ltx_bibblock">
B.&nbsp;Spector and C.&nbsp;Ré, “Accelerating LLM inference with staged
speculative decoding,” <em id="bib.bib326.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.04623, 2023.

</span>
</li>
<li id="bib.bib327" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[327]</span>
<span class="ltx_bibblock">
L.&nbsp;D. Corro, A.&nbsp;D. Giorno, S.&nbsp;Agarwal, B.&nbsp;Yu, A.&nbsp;H. Awadallah, and
S.&nbsp;Mukherjee, “Skipdecode: Autoregressive skip decoding with batching and
caching for efficient LLM inference,” <em id="bib.bib327.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.02628,
2023.

</span>
</li>
<li id="bib.bib328" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[328]</span>
<span class="ltx_bibblock">
D.&nbsp;P. Kingma and J.&nbsp;Ba, “Adam: A method for stochastic optimization,” in
<em id="bib.bib328.1.1" class="ltx_emph ltx_font_italic">3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, Y.&nbsp;Bengio
and Y.&nbsp;LeCun, Eds., 2015.

</span>
</li>
<li id="bib.bib329" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[329]</span>
<span class="ltx_bibblock">
I.&nbsp;Loshchilov and F.&nbsp;Hutter, “Fixing weight decay regularization in adam,”
<em id="bib.bib329.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1711.05101, 2017.

</span>
</li>
<li id="bib.bib330" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[330]</span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer and M.&nbsp;Stern, “Adafactor: Adaptive learning rates with sublinear
memory cost,” in <em id="bib.bib330.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
July 10-15, 2018</em>, ser. Proceedings of Machine Learning Research, J.&nbsp;G. Dy
and A.&nbsp;Krause, Eds., vol.&nbsp;80.&nbsp;&nbsp;&nbsp;PMLR,
2018, pp. 4603–4611.

</span>
</li>
<li id="bib.bib331" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[331]</span>
<span class="ltx_bibblock">
Y.&nbsp;Huang, Y.&nbsp;Cheng, A.&nbsp;Bapna, O.&nbsp;Firat, D.&nbsp;Chen, M.&nbsp;X. Chen, H.&nbsp;Lee, J.&nbsp;Ngiam,
Q.&nbsp;V. Le, Y.&nbsp;Wu, and Z.&nbsp;Chen, “Gpipe: Efficient training of giant neural
networks using pipeline parallelism,” in <em id="bib.bib331.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada</em>, H.&nbsp;M. Wallach, H.&nbsp;Larochelle, A.&nbsp;Beygelzimer,
F.&nbsp;d’Alché-Buc, E.&nbsp;B. Fox, and R.&nbsp;Garnett, Eds., 2019, pp. 103–112.

</span>
</li>
<li id="bib.bib332" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[332]</span>
<span class="ltx_bibblock">
A.&nbsp;Harlap, D.&nbsp;Narayanan, A.&nbsp;Phanishayee, V.&nbsp;Seshadri, N.&nbsp;R. Devanur, G.&nbsp;R.
Ganger, and P.&nbsp;B. Gibbons, “Pipedream: Fast and efficient pipeline parallel
DNN training,” <em id="bib.bib332.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1806.03377, 2018.

</span>
</li>
<li id="bib.bib333" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[333]</span>
<span class="ltx_bibblock">
S.&nbsp;Rajbhandari, J.&nbsp;Rasley, O.&nbsp;Ruwase, and Y.&nbsp;He, “Zero: memory optimizations
toward training trillion parameter models,” in <em id="bib.bib333.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
International Conference for High Performance Computing, Networking, Storage
and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November
9-19, 2020</em>, C.&nbsp;Cuicchi, I.&nbsp;Qualters, and W.&nbsp;T. Kramer, Eds.&nbsp;&nbsp;&nbsp;IEEE/ACM, 2020, p.&nbsp;20.

</span>
</li>
<li id="bib.bib334" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[334]</span>
<span class="ltx_bibblock">
P.&nbsp;Micikevicius, S.&nbsp;Narang, J.&nbsp;Alben, G.&nbsp;F. Diamos, E.&nbsp;Elsen, D.&nbsp;García,
B.&nbsp;Ginsburg, M.&nbsp;Houston, O.&nbsp;Kuchaiev, G.&nbsp;Venkatesh, and H.&nbsp;Wu, “Mixed
precision training,” <em id="bib.bib334.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1710.03740, 2017.

</span>
</li>
<li id="bib.bib335" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[335]</span>
<span class="ltx_bibblock">
Q.&nbsp;Xu, S.&nbsp;Li, C.&nbsp;Gong, and Y.&nbsp;You, “An efficient 2d method for training
super-large deep learning models,” <em id="bib.bib335.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2104.05343, 2021.

</span>
</li>
<li id="bib.bib336" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[336]</span>
<span class="ltx_bibblock">
B.&nbsp;Wang, Q.&nbsp;Xu, Z.&nbsp;Bian, and Y.&nbsp;You, “Tesseract: Parallelize the tensor
parallelism efficiently,” in <em id="bib.bib336.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 51st International
Conference on Parallel Processing, ICPP 2022, Bordeaux, France, 29 August
2022 - 1 September 2022</em>.&nbsp;&nbsp;&nbsp;ACM, 2022.

</span>
</li>
<li id="bib.bib337" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[337]</span>
<span class="ltx_bibblock">
Z.&nbsp;Bian, Q.&nbsp;Xu, B.&nbsp;Wang, and Y.&nbsp;You, “Maximizing parallelism in distributed
training for huge neural networks,” <em id="bib.bib337.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2105.14450, 2021.

</span>
</li>
<li id="bib.bib338" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[338]</span>
<span class="ltx_bibblock">
S.&nbsp;Li, F.&nbsp;Xue, C.&nbsp;Baranwal, Y.&nbsp;Li, and Y.&nbsp;You, “Sequence parallelism: Long
sequence training from system perspective,” <em id="bib.bib338.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pp.
arXiv–2105, 2021.

</span>
</li>
<li id="bib.bib339" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[339]</span>
<span class="ltx_bibblock">
FairScale authors, “Fairscale: A general purpose modular pytorch library for
high performance and large scale training,”
<a target="_blank" href="https://github.com/facebookresearch/fairscale" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/fairscale</a>, 2021.

</span>
</li>
<li id="bib.bib340" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[340]</span>
<span class="ltx_bibblock">
L.&nbsp;Zheng, Z.&nbsp;Li, H.&nbsp;Zhang, Y.&nbsp;Zhuang, Z.&nbsp;Chen, Y.&nbsp;Huang, Y.&nbsp;Wang, Y.&nbsp;Xu,
D.&nbsp;Zhuo, E.&nbsp;P. Xing <em id="bib.bib340.3.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Alpa: Automating inter-and
<math id="bib.bib340.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib340.1.m1.1a"><mo stretchy="false" id="bib.bib340.1.m1.1.1" xref="bib.bib340.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib340.1.m1.1b"><ci id="bib.bib340.1.m1.1.1.cmml" xref="bib.bib340.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib340.1.m1.1c">\{</annotation></semantics></math>Intra-Operator<math id="bib.bib340.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib340.2.m2.1a"><mo stretchy="false" id="bib.bib340.2.m2.1.1" xref="bib.bib340.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib340.2.m2.1b"><ci id="bib.bib340.2.m2.1.1.cmml" xref="bib.bib340.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib340.2.m2.1c">\}</annotation></semantics></math> parallelism for distributed deep learning,” in
<em id="bib.bib340.4.2" class="ltx_emph ltx_font_italic">OSDI</em>, 2022, pp. 559–578.

</span>
</li>
<li id="bib.bib341" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[341]</span>
<span class="ltx_bibblock">
T.&nbsp;Chen, B.&nbsp;Xu, C.&nbsp;Zhang, and C.&nbsp;Guestrin, “Training deep nets with sublinear
memory cost,” <em id="bib.bib341.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1604.06174, 2016.

</span>
</li>
<li id="bib.bib342" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[342]</span>
<span class="ltx_bibblock">
R.&nbsp;Lou, K.&nbsp;Zhang, and W.&nbsp;Yin, “Is prompt all you need? no. A comprehensive
and broader view of instruction learning,” <em id="bib.bib342.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.10475,
2023.

</span>
</li>
<li id="bib.bib343" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[343]</span>
<span class="ltx_bibblock">
X.&nbsp;Liu, P.&nbsp;He, W.&nbsp;Chen, and J.&nbsp;Gao, “Multi-task deep neural networks for
natural language understanding,” in <em id="bib.bib343.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2019, pp.
4487–4496.

</span>
</li>
<li id="bib.bib344" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[344]</span>
<span class="ltx_bibblock">
A.&nbsp;Aghajanyan, A.&nbsp;Gupta, A.&nbsp;Shrivastava, X.&nbsp;Chen, L.&nbsp;Zettlemoyer, and S.&nbsp;Gupta,
“Muppet: Massive multi-task representations with pre-finetuning,” in
<em id="bib.bib344.1.1" class="ltx_emph ltx_font_italic">EMNLP (1)</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2021, pp. 5799–5811.

</span>
</li>
<li id="bib.bib345" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[345]</span>
<span class="ltx_bibblock">
S.&nbsp;Longpre, L.&nbsp;Hou, T.&nbsp;Vu, A.&nbsp;Webson, H.&nbsp;W. Chung, Y.&nbsp;Tay, D.&nbsp;Zhou, Q.&nbsp;V. Le,
B.&nbsp;Zoph, J.&nbsp;Wei, and A.&nbsp;Roberts, “The flan collection: Designing data and
methods for effective instruction tuning,” <em id="bib.bib345.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2301.13688,
2023.

</span>
</li>
<li id="bib.bib346" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[346]</span>
<span class="ltx_bibblock">
C.&nbsp;Xu, Q.&nbsp;Sun, K.&nbsp;Zheng, X.&nbsp;Geng, P.&nbsp;Zhao, J.&nbsp;Feng, C.&nbsp;Tao, and D.&nbsp;Jiang,
“Wizardlm: Empowering large language models to follow complex
instructions,” <em id="bib.bib346.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.12244, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2304.12244" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2304.12244</a>

</span>
</li>
<li id="bib.bib347" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[347]</span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, Y.&nbsp;Shen, Q.&nbsp;Zhou, H.&nbsp;Zhang, Z.&nbsp;Chen, D.&nbsp;Cox, Y.&nbsp;Yang, and C.&nbsp;Gan,
“Principle-driven self-alignment of language models from scratch with
minimal human supervision,” <em id="bib.bib347.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.03047</em>, 2023.

</span>
</li>
<li id="bib.bib348" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[348]</span>
<span class="ltx_bibblock">
X.&nbsp;Li, P.&nbsp;Yu, C.&nbsp;Zhou, T.&nbsp;Schick, L.&nbsp;Zettlemoyer, O.&nbsp;Levy, J.&nbsp;Weston, and
M.&nbsp;Lewis, “Self-alignment with instruction backtranslation,” <em id="bib.bib348.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2308.06259, 2023.

</span>
</li>
<li id="bib.bib349" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[349]</span>
<span class="ltx_bibblock">
C.&nbsp;Zhou, P.&nbsp;Liu, P.&nbsp;Xu, S.&nbsp;Iyer, J.&nbsp;Sun, Y.&nbsp;Mao, X.&nbsp;Ma, A.&nbsp;Efrat, P.&nbsp;Yu, L.&nbsp;Yu
<em id="bib.bib349.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Lima: Less is more for alignment,” <em id="bib.bib349.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.11206</em>, 2023.

</span>
</li>
<li id="bib.bib350" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[350]</span>
<span class="ltx_bibblock">
L.&nbsp;Chen, S.&nbsp;Li, J.&nbsp;Yan, H.&nbsp;Wang, K.&nbsp;Gunaratna, V.&nbsp;Yadav, Z.&nbsp;Tang,
V.&nbsp;Srinivasan, T.&nbsp;Zhou, H.&nbsp;Huang, and H.&nbsp;Jin, “Alpagasus: Training A
better alpaca with fewer data,” <em id="bib.bib350.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.08701, 2023.

</span>
</li>
<li id="bib.bib351" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[351]</span>
<span class="ltx_bibblock">
S.&nbsp;Mukherjee, A.&nbsp;Mitra, G.&nbsp;Jawahar, S.&nbsp;Agarwal, H.&nbsp;Palangi, and A.&nbsp;H.
Awadallah, “Orca: Progressive learning from complex explanation traces of
GPT-4,” <em id="bib.bib351.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.02707, 2023.

</span>
</li>
<li id="bib.bib352" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[352]</span>
<span class="ltx_bibblock">
YuLan-Chat-Team, “Yulan-chat: An open-source bilingual chatbot,”
<a target="_blank" href="https://github.com/RUC-GSAI/YuLan-Chat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUC-GSAI/YuLan-Chat</a>, 2023.

</span>
</li>
<li id="bib.bib353" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[353]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, H.&nbsp;Ivison, P.&nbsp;Dasigi, J.&nbsp;Hessel, T.&nbsp;Khot, K.&nbsp;R. Chandu, D.&nbsp;Wadden,
K.&nbsp;MacMillan, N.&nbsp;A. Smith, I.&nbsp;Beltagy, and H.&nbsp;Hajishirzi, “How far can
camels go? exploring the state of instruction tuning on open resources,”
<em id="bib.bib353.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.04751, 2023.

</span>
</li>
<li id="bib.bib354" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[354]</span>
<span class="ltx_bibblock">
B.&nbsp;Peng, C.&nbsp;Li, P.&nbsp;He, M.&nbsp;Galley, and J.&nbsp;Gao, “Instruction tuning with
GPT-4,” <em id="bib.bib354.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.03277, 2023.

</span>
</li>
<li id="bib.bib355" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[355]</span>
<span class="ltx_bibblock">
M.&nbsp;M. Krell, M.&nbsp;Kosec, S.&nbsp;P. Perez, and A.&nbsp;Fitzgibbon, “Efficient sequence
packing without cross-contamination: Accelerating large language models
without impacting performance,” <em id="bib.bib355.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.02027</em>,
2021.

</span>
</li>
<li id="bib.bib356" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[356]</span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, S.&nbsp;Azizi, T.&nbsp;Tu, S.&nbsp;S. Mahdavi, J.&nbsp;Wei, H.&nbsp;W. Chung, N.&nbsp;Scales,
A.&nbsp;Tanwani, H.&nbsp;Cole-Lewis, S.&nbsp;Pfohl <em id="bib.bib356.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Large language models
encode clinical knowledge,” <em id="bib.bib356.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.13138</em>, 2022.

</span>
</li>
<li id="bib.bib357" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[357]</span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, R.&nbsp;Xie, Y.&nbsp;Hou, W.&nbsp;X. Zhao, L.&nbsp;Lin, and J.&nbsp;Wen, “Recommendation as
instruction following: A large language model empowered recommendation
approach,” <em id="bib.bib357.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.07001, 2023.

</span>
</li>
<li id="bib.bib358" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[358]</span>
<span class="ltx_bibblock">
H.&nbsp;Wang, C.&nbsp;Liu, N.&nbsp;Xi, Z.&nbsp;Qiang, S.&nbsp;Zhao, B.&nbsp;Qin, and T.&nbsp;Liu, “Huatuo: Tuning
llama model with chinese medical knowledge,” <em id="bib.bib358.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2304.06975</em>, 2023.

</span>
</li>
<li id="bib.bib359" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[359]</span>
<span class="ltx_bibblock">
Q.&nbsp;Huang, M.&nbsp;Tao, Z.&nbsp;An, C.&nbsp;Zhang, C.&nbsp;Jiang, Z.&nbsp;Chen, Z.&nbsp;Wu, and Y.&nbsp;Feng,
“Lawyer llama technical report,” <em id="bib.bib359.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15062</em>,
2023.

</span>
</li>
<li id="bib.bib360" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[360]</span>
<span class="ltx_bibblock">
S.&nbsp;Wu, O.&nbsp;Irsoy, S.&nbsp;Lu, V.&nbsp;Dabravolski, M.&nbsp;Dredze, S.&nbsp;Gehrmann, P.&nbsp;Kambadur,
D.&nbsp;Rosenberg, and G.&nbsp;Mann, “Bloomberggpt: A large language model for
finance,” <em id="bib.bib360.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17564</em>, 2023.

</span>
</li>
<li id="bib.bib361" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[361]</span>
<span class="ltx_bibblock">
T.&nbsp;Liu and B.&nbsp;K.&nbsp;H. Low, “Goat: Fine-tuned llama outperforms gpt-4 on
arithmetic tasks,” <em id="bib.bib361.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14201</em>, 2023.

</span>
</li>
<li id="bib.bib362" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[362]</span>
<span class="ltx_bibblock">
T.&nbsp;Sun, X.&nbsp;Zhang, Z.&nbsp;He, P.&nbsp;Li, Q.&nbsp;Cheng, H.&nbsp;Yan, X.&nbsp;Liu, Y.&nbsp;Shao, Q.&nbsp;Tang,
X.&nbsp;Zhao, K.&nbsp;Chen, Y.&nbsp;Zheng, Z.&nbsp;Zhou, R.&nbsp;Li, J.&nbsp;Zhan, Y.&nbsp;Zhou, L.&nbsp;Li, X.&nbsp;Yang,
L.&nbsp;Wu, Z.&nbsp;Yin, X.&nbsp;Huang, and X.&nbsp;Qiu, “Moss: Training conversational language
models from synthetic data,” 2023.

</span>
</li>
<li id="bib.bib363" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[363]</span>
<span class="ltx_bibblock">
Y.&nbsp;Dubois, X.&nbsp;Li, R.&nbsp;Taori, T.&nbsp;Zhang, I.&nbsp;Gulrajani, J.&nbsp;Ba, C.&nbsp;Guestrin,
P.&nbsp;Liang, and T.&nbsp;B. Hashimoto, “Alpacafarm: A simulation framework for
methods that learn from human feedback,” <em id="bib.bib363.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.14387,
2023. [Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2305.14387" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.14387</a>

</span>
</li>
<li id="bib.bib364" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[364]</span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, C.&nbsp;Burns, S.&nbsp;Basart, A.&nbsp;Zou, M.&nbsp;Mazeika, D.&nbsp;Song, and
J.&nbsp;Steinhardt, “Measuring massive multitask language understanding,” in
<em id="bib.bib364.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2021.

</span>
</li>
<li id="bib.bib365" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[365]</span>
<span class="ltx_bibblock">
M.&nbsp;Suzgun, N.&nbsp;Scales, N.&nbsp;Schärli, S.&nbsp;Gehrmann, Y.&nbsp;Tay, H.&nbsp;W. Chung,
A.&nbsp;Chowdhery, Q.&nbsp;V. Le, E.&nbsp;H. Chi, D.&nbsp;Zhou, and J.&nbsp;Wei, “Challenging
big-bench tasks and whether chain-of-thought can solve them,” <em id="bib.bib365.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2210.09261, 2022.

</span>
</li>
<li id="bib.bib366" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[366]</span>
<span class="ltx_bibblock">
Z.&nbsp;Kenton, T.&nbsp;Everitt, L.&nbsp;Weidinger, I.&nbsp;Gabriel, V.&nbsp;Mikulik, and G.&nbsp;Irving,
“Alignment of language agents,” <em id="bib.bib366.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2103.14659, 2021.

</span>
</li>
<li id="bib.bib367" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[367]</span>
<span class="ltx_bibblock">
D.&nbsp;M. Ziegler, N.&nbsp;Stiennon, J.&nbsp;Wu, T.&nbsp;B. Brown, A.&nbsp;Radford, D.&nbsp;Amodei, P.&nbsp;F.
Christiano, and G.&nbsp;Irving, “Fine-tuning language models from human
preferences,” <em id="bib.bib367.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1909.08593, 2019.

</span>
</li>
<li id="bib.bib368" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[368]</span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma, N.&nbsp;Elhage, Z.&nbsp;Hatfield-Dodds,
D.&nbsp;Hernandez, J.&nbsp;Kernion, K.&nbsp;Ndousse, C.&nbsp;Olsson, D.&nbsp;Amodei, T.&nbsp;B. Brown,
J.&nbsp;Clark, S.&nbsp;McCandlish, C.&nbsp;Olah, and J.&nbsp;Kaplan, “A general language
assistant as a laboratory for alignment,” <em id="bib.bib368.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2112.00861,
2021.

</span>
</li>
<li id="bib.bib369" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[369]</span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Huang, H.&nbsp;F. Song, T.&nbsp;Cai, R.&nbsp;Ring, J.&nbsp;Aslanides, A.&nbsp;Glaese,
N.&nbsp;McAleese, and G.&nbsp;Irving, “Red teaming language models with language
models,” in <em id="bib.bib369.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,
December 7-11, 2022</em>, Y.&nbsp;Goldberg, Z.&nbsp;Kozareva, and Y.&nbsp;Zhang, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022,
pp. 3419–3448.

</span>
</li>
<li id="bib.bib370" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[370]</span>
<span class="ltx_bibblock">
J.&nbsp;Menick, M.&nbsp;Trebacz, V.&nbsp;Mikulik, J.&nbsp;Aslanides, H.&nbsp;F. Song, M.&nbsp;Chadwick,
M.&nbsp;Glaese, S.&nbsp;Young, L.&nbsp;Campbell-Gillingham, G.&nbsp;Irving, and N.&nbsp;McAleese,
“Teaching language models to support answers with verified quotes,”
<em id="bib.bib370.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2203.11147, 2022.

</span>
</li>
<li id="bib.bib371" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[371]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, S.&nbsp;Kadavath, S.&nbsp;Kundu, A.&nbsp;Askell, J.&nbsp;Kernion, A.&nbsp;Jones, A.&nbsp;Chen,
A.&nbsp;Goldie, A.&nbsp;Mirhoseini, C.&nbsp;McKinnon, C.&nbsp;Chen, C.&nbsp;Olsson, C.&nbsp;Olah,
D.&nbsp;Hernandez, D.&nbsp;Drain, D.&nbsp;Ganguli, D.&nbsp;Li, E.&nbsp;Tran-Johnson, E.&nbsp;Perez,
J.&nbsp;Kerr, J.&nbsp;Mueller, J.&nbsp;Ladish, J.&nbsp;Landau, K.&nbsp;Ndousse, K.&nbsp;Lukosiute,
L.&nbsp;Lovitt, M.&nbsp;Sellitto, N.&nbsp;Elhage, N.&nbsp;Schiefer, N.&nbsp;Mercado, N.&nbsp;DasSarma,
R.&nbsp;Lasenby, R.&nbsp;Larson, S.&nbsp;Ringer, S.&nbsp;Johnston, S.&nbsp;Kravec, S.&nbsp;E. Showk,
S.&nbsp;Fort, T.&nbsp;Lanham, T.&nbsp;Telleen-Lawton, T.&nbsp;Conerly, T.&nbsp;Henighan, T.&nbsp;Hume,
S.&nbsp;R. Bowman, Z.&nbsp;Hatfield-Dodds, B.&nbsp;Mann, D.&nbsp;Amodei, N.&nbsp;Joseph,
S.&nbsp;McCandlish, T.&nbsp;Brown, and J.&nbsp;Kaplan, “Constitutional AI: harmlessness
from AI feedback,” <em id="bib.bib371.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.08073, 2022. [Online].
Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2212.08073" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.08073</a>

</span>
</li>
<li id="bib.bib372" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[372]</span>
<span class="ltx_bibblock">
H.&nbsp;Lee, S.&nbsp;Phatale, H.&nbsp;Mansoor, K.&nbsp;Lu, T.&nbsp;Mesnard, C.&nbsp;Bishop, V.&nbsp;Carbune, and
A.&nbsp;Rastogi, “RLAIF: scaling reinforcement learning from human feedback
with AI feedback,” <em id="bib.bib372.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.00267, 2023.

</span>
</li>
<li id="bib.bib373" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[373]</span>
<span class="ltx_bibblock">
H.&nbsp;Dong, W.&nbsp;Xiong, D.&nbsp;Goyal, R.&nbsp;Pan, S.&nbsp;Diao, J.&nbsp;Zhang, K.&nbsp;Shum, and T.&nbsp;Zhang,
“RAFT: reward ranked finetuning for generative foundation model
alignment,” <em id="bib.bib373.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.06767, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2304.06767" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2304.06767</a>

</span>
</li>
<li id="bib.bib374" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[374]</span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma <em id="bib.bib374.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “A general language assistant
as a laboratory for alignment,” <em id="bib.bib374.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.00861</em>,
2021.

</span>
</li>
<li id="bib.bib375" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[375]</span>
<span class="ltx_bibblock">
R.&nbsp;Zheng, S.&nbsp;Dou, S.&nbsp;Gao, W.&nbsp;Shen, B.&nbsp;Wang, Y.&nbsp;Liu, S.&nbsp;Jin, Q.&nbsp;Liu, L.&nbsp;Xiong,
L.&nbsp;Chen <em id="bib.bib375.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Secrets of rlhf in large language models part i:
Ppo,” <em id="bib.bib375.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.04964</em>, 2023.

</span>
</li>
<li id="bib.bib376" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[376]</span>
<span class="ltx_bibblock">
J.&nbsp;Uesato, N.&nbsp;Kushman, R.&nbsp;Kumar, H.&nbsp;F. Song, N.&nbsp;Y. Siegel, L.&nbsp;Wang,
A.&nbsp;Creswell, G.&nbsp;Irving, and I.&nbsp;Higgins, “Solving math word problems with
process- and outcome-based feedback,” <em id="bib.bib376.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.14275,
2022.

</span>
</li>
<li id="bib.bib377" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[377]</span>
<span class="ltx_bibblock">
H.&nbsp;Lightman, V.&nbsp;Kosaraju, Y.&nbsp;Burda, H.&nbsp;Edwards, B.&nbsp;Baker, T.&nbsp;Lee, J.&nbsp;Leike,
J.&nbsp;Schulman, I.&nbsp;Sutskever, and K.&nbsp;Cobbe, “Let’s verify step by step,”
<em id="bib.bib377.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.20050, 2023.

</span>
</li>
<li id="bib.bib378" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[378]</span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, S.&nbsp;Basart, S.&nbsp;Kadavath, M.&nbsp;Mazeika, A.&nbsp;Arora, E.&nbsp;Guo, C.&nbsp;Burns,
S.&nbsp;Puranik, H.&nbsp;He, D.&nbsp;Song, and J.&nbsp;Steinhardt, “Measuring coding challenge
competence with APPS,” in <em id="bib.bib378.1.1" class="ltx_emph ltx_font_italic">NeurIPS Datasets and Benchmarks</em>, 2021.

</span>
</li>
<li id="bib.bib379" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[379]</span>
<span class="ltx_bibblock">
Q.&nbsp;Ma, H.&nbsp;Zhou, T.&nbsp;Liu, J.&nbsp;Yuan, P.&nbsp;Liu, Y.&nbsp;You, and H.&nbsp;Yang, “Let’s reward
step by step: Step-level reward model as the navigators for reasoning,”
<em id="bib.bib379.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.10080, 2023.

</span>
</li>
<li id="bib.bib380" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[380]</span>
<span class="ltx_bibblock">
D.&nbsp;Silver, J.&nbsp;Schrittwieser, K.&nbsp;Simonyan, I.&nbsp;Antonoglou, A.&nbsp;Huang, A.&nbsp;Guez,
T.&nbsp;Hubert, L.&nbsp;Baker, M.&nbsp;Lai, A.&nbsp;Bolton, Y.&nbsp;Chen, T.&nbsp;P. Lillicrap, F.&nbsp;Hui,
L.&nbsp;Sifre, G.&nbsp;van&nbsp;den Driessche, T.&nbsp;Graepel, and D.&nbsp;Hassabis, “Mastering the
game of go without human knowledge,” <em id="bib.bib380.1.1" class="ltx_emph ltx_font_italic">Nat.</em>, pp. 354–359, 2017.

</span>
</li>
<li id="bib.bib381" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[381]</span>
<span class="ltx_bibblock">
T.&nbsp;Anthony, Z.&nbsp;Tian, and D.&nbsp;Barber, “Thinking fast and slow with deep learning
and tree search,” in <em id="bib.bib381.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems 2017, December
4-9, 2017, Long Beach, CA, USA</em>, 2017, pp. 5360–5370.

</span>
</li>
<li id="bib.bib382" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[382]</span>
<span class="ltx_bibblock">
H.&nbsp;Luo, Q.&nbsp;Sun, C.&nbsp;Xu, P.&nbsp;Zhao, J.&nbsp;Lou, C.&nbsp;Tao, X.&nbsp;Geng, Q.&nbsp;Lin, S.&nbsp;Chen, and
D.&nbsp;Zhang, “Wizardmath: Empowering mathematical reasoning for large language
models via reinforced evol-instruct,” <em id="bib.bib382.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.09583,
2023.

</span>
</li>
<li id="bib.bib383" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[383]</span>
<span class="ltx_bibblock">
R.&nbsp;Liu, C.&nbsp;Jia, G.&nbsp;Zhang, Z.&nbsp;Zhuang, T.&nbsp;X. Liu, and S.&nbsp;Vosoughi, “Second
thoughts are best: Learning to re-align with human values from text edits,”
in <em id="bib.bib383.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib384" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[384]</span>
<span class="ltx_bibblock">
X.&nbsp;Lu, S.&nbsp;Welleck, J.&nbsp;Hessel, L.&nbsp;Jiang, L.&nbsp;Qin, P.&nbsp;West, P.&nbsp;Ammanabrolu, and
Y.&nbsp;Choi, “QUARK: controllable text generation with reinforced
unlearning,” in <em id="bib.bib384.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib385" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[385]</span>
<span class="ltx_bibblock">
J.&nbsp;Scheurer, J.&nbsp;A. Campos, T.&nbsp;Korbak, J.&nbsp;S. Chan, A.&nbsp;Chen, K.&nbsp;Cho, and
E.&nbsp;Perez, “Training language models with language feedback at scale,”
<em id="bib.bib385.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.16755, 2023.

</span>
</li>
<li id="bib.bib386" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[386]</span>
<span class="ltx_bibblock">
G.&nbsp;Guo, R.&nbsp;Zhao, T.&nbsp;Tang, W.&nbsp;X. Zhao, and J.-R. Wen, “Beyond imitation:
Leveraging fine-grained quality signals for alignment,” <em id="bib.bib386.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2311.04072</em>, 2023.

</span>
</li>
<li id="bib.bib387" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[387]</span>
<span class="ltx_bibblock">
R.&nbsp;Krishna, D.&nbsp;Lee, L.&nbsp;Fei-Fei, and M.&nbsp;S. Bernstein, “Socially situated
artificial intelligence enables learning from human interaction,”
<em id="bib.bib387.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences of the United States of
America</em>, vol. 119, 2022. [Online]. Available:
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:252381954" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:252381954</a>

</span>
</li>
<li id="bib.bib388" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[388]</span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Sferrazza, and P.&nbsp;Abbeel, “Chain of hindsight aligns language
models with feedback,” <em id="bib.bib388.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.02676, 2023.

</span>
</li>
<li id="bib.bib389" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[389]</span>
<span class="ltx_bibblock">
R.&nbsp;Rafailov, A.&nbsp;Sharma, E.&nbsp;Mitchell, S.&nbsp;Ermon, C.&nbsp;D. Manning, and C.&nbsp;Finn,
“Direct preference optimization: Your language model is secretly a reward
model,” <em id="bib.bib389.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.18290, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.18290" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.18290</a>

</span>
</li>
<li id="bib.bib390" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[390]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Tan, W.&nbsp;Wang, S.&nbsp;Huang, and F.&nbsp;Huang, “RRHF: rank
responses to align language models with human feedback without tears,”
<em id="bib.bib390.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.05302, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2304.05302" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2304.05302</a>

</span>
</li>
<li id="bib.bib391" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[391]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhao, R.&nbsp;Joshi, T.&nbsp;Liu, M.&nbsp;Khalman, M.&nbsp;Saleh, and P.&nbsp;J. Liu, “Slic-hf:
Sequence likelihood calibration with human feedback,” <em id="bib.bib391.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.10425, 2023.

</span>
</li>
<li id="bib.bib392" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[392]</span>
<span class="ltx_bibblock">
T.&nbsp;Zhang, F.&nbsp;Liu, J.&nbsp;Wong, P.&nbsp;Abbeel, and J.&nbsp;E. Gonzalez, “The wisdom of
hindsight makes language models better instruction followers,” <em id="bib.bib392.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2302.05206, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2302.05206" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2302.05206</a>

</span>
</li>
<li id="bib.bib393" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[393]</span>
<span class="ltx_bibblock">
A.&nbsp;Hussein, M.&nbsp;M. Gaber, E.&nbsp;Elyan, and C.&nbsp;Jayne, “Imitation learning: A survey
of learning methods,” <em id="bib.bib393.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, vol.&nbsp;50, no.&nbsp;2, apr 2017.
[Online]. Available: <a target="_blank" href="https://doi.org/10.1145/3054912" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3054912</a>

</span>
</li>
<li id="bib.bib394" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[394]</span>
<span class="ltx_bibblock">
S.&nbsp;Levine, “Should i imitate or reinforce,” 2022. [Online]. Available:
<a target="_blank" href="https://www.youtube.com/watch?v=sVPm7zOrBxM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=sVPm7zOrBxM</a>

</span>
</li>
<li id="bib.bib395" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[395]</span>
<span class="ltx_bibblock">
J.&nbsp;Schulman, “Reinforcement learning from human feedback: Progress and
challenges,” 2023. [Online]. Available:
<a target="_blank" href="https://www.youtube.com/watch?v=hhiLw5Q_UFg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=hhiLw5Q_UFg</a>

</span>
</li>
<li id="bib.bib396" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[396]</span>
<span class="ltx_bibblock">
X.&nbsp;L. Li and P.&nbsp;Liang, “Prefix-tuning: Optimizing continuous prompts for
generation,” in <em id="bib.bib396.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long
Papers), Virtual Event, August 1-6, 2021</em>, C.&nbsp;Zong, F.&nbsp;Xia, W.&nbsp;Li, and
R.&nbsp;Navigli, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2021, pp. 4582–4597.

</span>
</li>
<li id="bib.bib397" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[397]</span>
<span class="ltx_bibblock">
B.&nbsp;Lester, R.&nbsp;Al-Rfou, and N.&nbsp;Constant, “The power of scale for
parameter-efficient prompt tuning,” in <em id="bib.bib397.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>,
M.&nbsp;Moens, X.&nbsp;Huang, L.&nbsp;Specia, and S.&nbsp;W. Yih, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 3045–3059.

</span>
</li>
<li id="bib.bib398" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[398]</span>
<span class="ltx_bibblock">
N.&nbsp;Houlsby, A.&nbsp;Giurgiu, S.&nbsp;Jastrzebski, B.&nbsp;Morrone, Q.&nbsp;de&nbsp;Laroussilhe,
A.&nbsp;Gesmundo, M.&nbsp;Attariyan, and S.&nbsp;Gelly, “Parameter-efficient transfer
learning for NLP,” in <em id="bib.bib398.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA</em>, 2019, pp. 2790–2799.

</span>
</li>
<li id="bib.bib399" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[399]</span>
<span class="ltx_bibblock">
Z.&nbsp;Hu, Y.&nbsp;Lan, L.&nbsp;Wang, W.&nbsp;Xu, E.&nbsp;Lim, R.&nbsp;K. Lee, L.&nbsp;Bing, and S.&nbsp;Poria,
“Llm-adapters: An adapter family for parameter-efficient fine-tuning of
large language models,” <em id="bib.bib399.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.01933, 2023.

</span>
</li>
<li id="bib.bib400" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[400]</span>
<span class="ltx_bibblock">
J.&nbsp;He, C.&nbsp;Zhou, X.&nbsp;Ma, T.&nbsp;Berg-Kirkpatrick, and G.&nbsp;Neubig, “Towards a
unified view of parameter-efficient transfer learning,” in <em id="bib.bib400.1.1" class="ltx_emph ltx_font_italic">The Tenth
International Conference on Learning Representations, ICLR 2022, Virtual
Event, April 25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib401" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[401]</span>
<span class="ltx_bibblock">
X.&nbsp;Liu, K.&nbsp;Ji, Y.&nbsp;Fu, Z.&nbsp;Du, Z.&nbsp;Yang, and J.&nbsp;Tang, “P-tuning v2: Prompt tuning
can be comparable to fine-tuning universally across scales and tasks,”
<em id="bib.bib401.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2110.07602, 2021.

</span>
</li>
<li id="bib.bib402" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[402]</span>
<span class="ltx_bibblock">
X.&nbsp;Liu, Y.&nbsp;Zheng, Z.&nbsp;Du, M.&nbsp;Ding, Y.&nbsp;Qian, Z.&nbsp;Yang, and J.&nbsp;Tang, “GPT
understands, too,” <em id="bib.bib402.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2103.10385, 2021.

</span>
</li>
<li id="bib.bib403" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[403]</span>
<span class="ltx_bibblock">
Y.&nbsp;Gu, X.&nbsp;Han, Z.&nbsp;Liu, and M.&nbsp;Huang, “Ppt: Pre-trained prompt tuning for
few-shot learning,” in <em id="bib.bib403.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2022, pp.
8410–8423.

</span>
</li>
<li id="bib.bib404" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[404]</span>
<span class="ltx_bibblock">
Z.&nbsp;Jiang, F.&nbsp;F. Xu, J.&nbsp;Araki, and G.&nbsp;Neubig, “How can we know what language
models know?” <em id="bib.bib404.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational
Linguistics</em>, vol.&nbsp;8, pp. 423–438, 2020.

</span>
</li>
<li id="bib.bib405" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[405]</span>
<span class="ltx_bibblock">
T.&nbsp;Shin, Y.&nbsp;Razeghi, R.&nbsp;L. Logan&nbsp;IV, E.&nbsp;Wallace, and S.&nbsp;Singh, “Autoprompt:
Eliciting knowledge from language models with automatically generated
prompts,” in <em id="bib.bib405.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP)</em>, 2020, pp. 4222–4235.

</span>
</li>
<li id="bib.bib406" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[406]</span>
<span class="ltx_bibblock">
Q.&nbsp;Zhang, M.&nbsp;Chen, A.&nbsp;Bukharin, P.&nbsp;He, Y.&nbsp;Cheng, W.&nbsp;Chen, and T.&nbsp;Zhao,
“Adaptive budget allocation for parameter-efficient fine-tuning,”
<em id="bib.bib406.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.10512, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2303.10512" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2303.10512</a>

</span>
</li>
<li id="bib.bib407" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[407]</span>
<span class="ltx_bibblock">
M.&nbsp;Valipour, M.&nbsp;Rezagholizadeh, I.&nbsp;Kobyzev, and A.&nbsp;Ghodsi, “Dylora: Parameter
efficient tuning of pre-trained models using dynamic search-free low-rank
adaptation,” <em id="bib.bib407.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.07558, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2210.07558" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2210.07558</a>

</span>
</li>
<li id="bib.bib408" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[408]</span>
<span class="ltx_bibblock">
N.&nbsp;Ding, Y.&nbsp;Qin, G.&nbsp;Yang, F.&nbsp;Wei, Y.&nbsp;Zonghan, Y.&nbsp;Su, S.&nbsp;Hu, Y.&nbsp;Chen, C.-M.
Chan, W.&nbsp;Chen, J.&nbsp;Yi, W.&nbsp;Zhao, X.&nbsp;Wang, Z.&nbsp;Liu, H.-T. Zheng, J.&nbsp;Chen, Y.&nbsp;Liu,
J.&nbsp;Tang, J.&nbsp;Li, and M.&nbsp;Sun, “Parameter-efficient fine-tuning of large-scale
pre-trained language models,” <em id="bib.bib408.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, vol.&nbsp;5,
pp. 1–16, 03 2023.

</span>
</li>
<li id="bib.bib409" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[409]</span>
<span class="ltx_bibblock">
R.&nbsp;Zhang, J.&nbsp;Han, A.&nbsp;Zhou, X.&nbsp;Hu, S.&nbsp;Yan, P.&nbsp;Lu, H.&nbsp;Li, P.&nbsp;Gao, and Y.&nbsp;Qiao,
“Llama-adapter: Efficient fine-tuning of language models with zero-init
attention,” <em id="bib.bib409.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.16199, 2023.

</span>
</li>
<li id="bib.bib410" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[410]</span>
<span class="ltx_bibblock">
J.&nbsp;Pfeiffer, I.&nbsp;Vulic, I.&nbsp;Gurevych, and S.&nbsp;Ruder, “MAD-X: an adapter-based
framework for multi-task cross-lingual transfer,” in <em id="bib.bib410.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2020 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2020, Online, November 16-20, 2020</em>, B.&nbsp;Webber, T.&nbsp;Cohn, Y.&nbsp;He, and
Y.&nbsp;Liu, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2020, pp. 7654–7673.

</span>
</li>
<li id="bib.bib411" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[411]</span>
<span class="ltx_bibblock">
S.&nbsp;Mangrulkar, S.&nbsp;Gugger, L.&nbsp;Debut, Y.&nbsp;Belkada, and S.&nbsp;Paul, “Peft:
State-of-the-art parameter-efficient fine-tuning methods,”
<a target="_blank" href="https://github.com/huggingface/peft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/peft</a>, 2022.

</span>
</li>
<li id="bib.bib412" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[412]</span>
<span class="ltx_bibblock">
A.&nbsp;Gholami, S.&nbsp;Kim, Z.&nbsp;Dong, Z.&nbsp;Yao, M.&nbsp;W. Mahoney, and K.&nbsp;Keutzer, “A survey
of quantization methods for efficient neural network inference,”
<em id="bib.bib412.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2103.13630, 2021. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2103.13630" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2103.13630</a>

</span>
</li>
<li id="bib.bib413" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[413]</span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, M.&nbsp;Lewis, Y.&nbsp;Belkada, and L.&nbsp;Zettlemoyer, “Llm.int8(): 8-bit
matrix multiplication for transformers at scale,” <em id="bib.bib413.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2208.07339, 2022.

</span>
</li>
<li id="bib.bib414" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[414]</span>
<span class="ltx_bibblock">
G.&nbsp;Xiao, J.&nbsp;Lin, M.&nbsp;Seznec, J.&nbsp;Demouth, and S.&nbsp;Han, “Smoothquant: Accurate and
efficient post-training quantization for large language models,”
<em id="bib.bib414.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.10438, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2211.10438" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2211.10438</a>

</span>
</li>
<li id="bib.bib415" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[415]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yao, R.&nbsp;Y. Aminabadi, M.&nbsp;Zhang, X.&nbsp;Wu, C.&nbsp;Li, and Y.&nbsp;He, “Zeroquant:
Efficient and affordable post-training quantization for large-scale
transformers,” in <em id="bib.bib415.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib416" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[416]</span>
<span class="ltx_bibblock">
J.&nbsp;Lin, J.&nbsp;Tang, H.&nbsp;Tang, S.&nbsp;Yang, X.&nbsp;Dang, and S.&nbsp;Han, “Awq: Activation-aware
weight quantization for llm compression and acceleration,” 2023.

</span>
</li>
<li id="bib.bib417" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[417]</span>
<span class="ltx_bibblock">
E.&nbsp;Frantar, S.&nbsp;Ashkboos, T.&nbsp;Hoefler, and D.&nbsp;Alistarh, “Gptq: Accurate
post-training quantization for generative pre-trained transformers,”
<em id="bib.bib417.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li id="bib.bib418" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[418]</span>
<span class="ltx_bibblock">
E.&nbsp;Frantar and D.&nbsp;Alistarh, “Optimal brain compression: A framework for
accurate post-training quantization and pruning,” in <em id="bib.bib418.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib419" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[419]</span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, A.&nbsp;Pagnoni, A.&nbsp;Holtzman, and L.&nbsp;Zettlemoyer, “Qlora: Efficient
finetuning of quantized llms,” <em id="bib.bib419.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li id="bib.bib420" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[420]</span>
<span class="ltx_bibblock">
Z.&nbsp;Liu, B.&nbsp;Oguz, C.&nbsp;Zhao, E.&nbsp;Chang, P.&nbsp;Stock, Y.&nbsp;Mehdad, Y.&nbsp;Shi,
R.&nbsp;Krishnamoorthi, and V.&nbsp;Chandra, “Llm-qat: Data-free quantization aware
training for large language models,” 2023.

</span>
</li>
<li id="bib.bib421" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[421]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yao, X.&nbsp;Wu, C.&nbsp;Li, S.&nbsp;Youn, and Y.&nbsp;He, “Zeroquant-v2: Exploring
post-training quantization in llms from comprehensive study to low rank
compensation,” 2023.

</span>
</li>
<li id="bib.bib422" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[422]</span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers and L.&nbsp;Zettlemoyer, “The case for 4-bit precision: k-bit inference
scaling laws,” <em id="bib.bib422.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.09720, 2022.

</span>
</li>
<li id="bib.bib423" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[423]</span>
<span class="ltx_bibblock">
L.&nbsp;Peiyu, L.&nbsp;Zikang, G.&nbsp;Ze-Feng, G.&nbsp;Dawei, Z.&nbsp;W. Xin, L.&nbsp;Yaliang, D.&nbsp;Bolin, and
W.&nbsp;Ji-Rong, “Do emergent abilities exist in quantized large language models:
An empirical study,” <em id="bib.bib423.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.08072</em>, 2023.

</span>
</li>
<li id="bib.bib424" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[424]</span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, M.&nbsp;Lewis, Y.&nbsp;Belkada, and L.&nbsp;Zettlemoyer, “Llm.int8(): 8-bit
matrix multiplication for transformers at scale,” <em id="bib.bib424.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2208.07339, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2208.07339" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2208.07339</a>

</span>
</li>
<li id="bib.bib425" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[425]</span>
<span class="ltx_bibblock">
X.&nbsp;Wei, X.&nbsp;Cui, N.&nbsp;Cheng, X.&nbsp;Wang, X.&nbsp;Zhang, S.&nbsp;Huang, P.&nbsp;Xie, J.&nbsp;Xu, Y.&nbsp;Chen,
M.&nbsp;Zhang <em id="bib.bib425.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Zero-shot information extraction via chatting with
chatgpt,” <em id="bib.bib425.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.10205</em>, 2023.

</span>
</li>
<li id="bib.bib426" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[426]</span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, M.&nbsp;Lewis, S.&nbsp;Shleifer, and L.&nbsp;Zettlemoyer, “8-bit optimizers via
block-wise quantization,” <em id="bib.bib426.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning
Representations, ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib427" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[427]</span>
<span class="ltx_bibblock">
C.&nbsp;Tao, L.&nbsp;Hou, W.&nbsp;Zhang, L.&nbsp;Shang, X.&nbsp;Jiang, Q.&nbsp;Liu, P.&nbsp;Luo, and N.&nbsp;Wong,
“Compression of generative pre-trained language models via quantization,”
in <em id="bib.bib427.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022</em>, S.&nbsp;Muresan, P.&nbsp;Nakov, and A.&nbsp;Villavicencio,
Eds.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2022, pp. 4821–4836.

</span>
</li>
<li id="bib.bib428" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[428]</span>
<span class="ltx_bibblock">
J.&nbsp;Liu, D.&nbsp;Shen, Y.&nbsp;Zhang, B.&nbsp;Dolan, L.&nbsp;Carin, and W.&nbsp;Chen, “What makes good
in-context examples for gpt-3?” in <em id="bib.bib428.1.1" class="ltx_emph ltx_font_italic">Proceedings of Deep Learning Inside
Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep
Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27,
2022</em>, 2022, pp. 100–114.

</span>
</li>
<li id="bib.bib429" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[429]</span>
<span class="ltx_bibblock">
O.&nbsp;Rubin, J.&nbsp;Herzig, and J.&nbsp;Berant, “Learning to retrieve prompts for
in-context learning,” in <em id="bib.bib429.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the
North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL 2022, Seattle, WA, United States, July
10-15, 2022</em>, 2022, pp. 2655–2671.

</span>
</li>
<li id="bib.bib430" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[430]</span>
<span class="ltx_bibblock">
H.&nbsp;J. Kim, H.&nbsp;Cho, J.&nbsp;Kim, T.&nbsp;Kim, K.&nbsp;M. Yoo, and S.&nbsp;Lee, “Self-generated
in-context learning: Leveraging auto-regressive language models as a
demonstration generator,” <em id="bib.bib430.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2206.08082, 2022.

</span>
</li>
<li id="bib.bib431" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[431]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhou, A.&nbsp;I. Muresanu, Z.&nbsp;Han, K.&nbsp;Paster, S.&nbsp;Pitis, H.&nbsp;Chan, and J.&nbsp;Ba,
“Large language models are human-level prompt engineers,” in <em id="bib.bib431.1.1" class="ltx_emph ltx_font_italic">Proc. of
ICLR</em>, 2023.

</span>
</li>
<li id="bib.bib432" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[432]</span>
<span class="ltx_bibblock">
Y.&nbsp;Lu, M.&nbsp;Bartolo, A.&nbsp;Moore, S.&nbsp;Riedel, and P.&nbsp;Stenetorp, “Fantastically
ordered prompts and where to find them: Overcoming few-shot prompt order
sensitivity,” in <em id="bib.bib432.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022</em>, S.&nbsp;Muresan, P.&nbsp;Nakov, and
A.&nbsp;Villavicencio, Eds., 2022, pp. 8086–8098.

</span>
</li>
<li id="bib.bib433" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[433]</span>
<span class="ltx_bibblock">
Y.&nbsp;Fu, H.&nbsp;Peng, A.&nbsp;Sabharwal, P.&nbsp;Clark, and T.&nbsp;Khot, “Complexity-based
prompting for multi-step reasoning,” <em id="bib.bib433.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.00720, 2022.

</span>
</li>
<li id="bib.bib434" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[434]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, A.&nbsp;Zhang, M.&nbsp;Li, and A.&nbsp;Smola, “Automatic chain of thought prompting
in large language models,” <em id="bib.bib434.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.03493, 2022.

</span>
</li>
<li id="bib.bib435" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[435]</span>
<span class="ltx_bibblock">
A.&nbsp;Creswell, M.&nbsp;Shanahan, and I.&nbsp;Higgins, “Selection-inference: Exploiting
large language models for interpretable logical reasoning,” <em id="bib.bib435.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2205.09712, 2022.

</span>
</li>
<li id="bib.bib436" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[436]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;V. Le, E.&nbsp;H. Chi, and D.&nbsp;Zhou,
“Self-consistency improves chain of thought reasoning in language models,”
<em id="bib.bib436.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2203.11171, 2022.

</span>
</li>
<li id="bib.bib437" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[437]</span>
<span class="ltx_bibblock">
Y.&nbsp;Li, Z.&nbsp;Lin, S.&nbsp;Zhang, Q.&nbsp;Fu, B.&nbsp;Chen, J.&nbsp;Lou, and W.&nbsp;Chen, “On the advance
of making language models better reasoners,” <em id="bib.bib437.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2206.02336, 2022.

</span>
</li>
<li id="bib.bib438" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[438]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;V. Le, E.&nbsp;H. Chi, and D.&nbsp;Zhou,
“Rationale-augmented ensembles in language models,” <em id="bib.bib438.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2022.

</span>
</li>
<li id="bib.bib439" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[439]</span>
<span class="ltx_bibblock">
D.&nbsp;Zhou, N.&nbsp;Schärli, L.&nbsp;Hou, J.&nbsp;Wei, N.&nbsp;Scales, X.&nbsp;Wang, D.&nbsp;Schuurmans,
O.&nbsp;Bousquet, Q.&nbsp;Le, and E.&nbsp;H. Chi, “Least-to-most prompting enables complex
reasoning in large language models,” <em id="bib.bib439.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2205.10625, 2022.

</span>
</li>
<li id="bib.bib440" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[440]</span>
<span class="ltx_bibblock">
T.&nbsp;Khot, H.&nbsp;Trivedi, M.&nbsp;Finlayson, Y.&nbsp;Fu, K.&nbsp;Richardson, P.&nbsp;Clark, and
A.&nbsp;Sabharwal, “Decomposed prompting: A modular approach for solving
complex tasks,” <em id="bib.bib440.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.02406, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2210.02406" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2210.02406</a>

</span>
</li>
<li id="bib.bib441" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[441]</span>
<span class="ltx_bibblock">
L.&nbsp;Wang, W.&nbsp;Xu, Y.&nbsp;Lan, Z.&nbsp;Hu, Y.&nbsp;Lan, R.&nbsp;K. Lee, and E.&nbsp;Lim, “Plan-and-solve
prompting: Improving zero-shot chain-of-thought reasoning by large language
models,” <em id="bib.bib441.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.04091, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.04091" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.04091</a>

</span>
</li>
<li id="bib.bib442" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[442]</span>
<span class="ltx_bibblock">
Q.&nbsp;Lyu, S.&nbsp;Havaldar, A.&nbsp;Stein, L.&nbsp;Zhang, D.&nbsp;Rao, E.&nbsp;Wong, M.&nbsp;Apidianaki, and
C.&nbsp;Callison-Burch, “Faithful chain-of-thought reasoning,” <em id="bib.bib442.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2301.13379, 2023.

</span>
</li>
<li id="bib.bib443" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[443]</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, A.&nbsp;Madaan, S.&nbsp;Zhou, U.&nbsp;Alon, P.&nbsp;Liu, Y.&nbsp;Yang, J.&nbsp;Callan, and G.&nbsp;Neubig,
“PAL: program-aided language models,” <em id="bib.bib443.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.10435,
2022.

</span>
</li>
<li id="bib.bib444" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[444]</span>
<span class="ltx_bibblock">
Y.&nbsp;Shen, K.&nbsp;Song, X.&nbsp;Tan, D.&nbsp;Li, W.&nbsp;Lu, and Y.&nbsp;Zhuang, “Hugginggpt: Solving ai
tasks with chatgpt and its friends in huggingface,” <em id="bib.bib444.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2303.17580</em>, 2023.

</span>
</li>
<li id="bib.bib445" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[445]</span>
<span class="ltx_bibblock">
H.&nbsp;Sun, Y.&nbsp;Zhuang, L.&nbsp;Kong, B.&nbsp;Dai, and C.&nbsp;Zhang, “Adaplanner: Adaptive
planning from feedback with language models,” <em id="bib.bib445.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.16653</em>, 2023.

</span>
</li>
<li id="bib.bib446" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[446]</span>
<span class="ltx_bibblock">
Y.&nbsp;Lu, P.&nbsp;Lu, Z.&nbsp;Chen, W.&nbsp;Zhu, X.&nbsp;E. Wang, and W.&nbsp;Y. Wang, “Multimodal
procedural planning via dual text-image prompting,” <em id="bib.bib446.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.01795, 2023.

</span>
</li>
<li id="bib.bib447" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[447]</span>
<span class="ltx_bibblock">
S.&nbsp;Hao, Y.&nbsp;Gu, H.&nbsp;Ma, J.&nbsp;J. Hong, Z.&nbsp;Wang, D.&nbsp;Z. Wang, and Z.&nbsp;Hu, “Reasoning
with language model is planning with world model,” <em id="bib.bib447.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.14992, 2023.

</span>
</li>
<li id="bib.bib448" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[448]</span>
<span class="ltx_bibblock">
Z.&nbsp;Chen, K.&nbsp;Zhou, B.&nbsp;Zhang, Z.&nbsp;Gong, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “Chatcot:
Tool-augmented chain-of-thought reasoning on chat-based large language
models,” <em id="bib.bib448.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.14323, 2023.

</span>
</li>
<li id="bib.bib449" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[449]</span>
<span class="ltx_bibblock">
S.&nbsp;Yao, J.&nbsp;Zhao, D.&nbsp;Yu, N.&nbsp;Du, I.&nbsp;Shafran, K.&nbsp;Narasimhan, and Y.&nbsp;Cao, “React:
Synergizing reasoning and acting in language models,” <em id="bib.bib449.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2210.03629, 2022.

</span>
</li>
<li id="bib.bib450" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[450]</span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, F.&nbsp;Cassano, B.&nbsp;Labash, A.&nbsp;Gopinath, K.&nbsp;Narasimhan, and S.&nbsp;Yao,
“Reflexion: Language agents with verbal reinforcement learning,” 2023.

</span>
</li>
<li id="bib.bib451" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[451]</span>
<span class="ltx_bibblock">
S.&nbsp;Yao, D.&nbsp;Yu, J.&nbsp;Zhao, I.&nbsp;Shafran, T.&nbsp;L. Griffiths, Y.&nbsp;Cao, and K.&nbsp;Narasimhan,
“Tree of thoughts: Deliberate problem solving with large language models,”
<em id="bib.bib451.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.10601, 2023.

</span>
</li>
<li id="bib.bib452" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[452]</span>
<span class="ltx_bibblock">
V.&nbsp;Liu and L.&nbsp;B. Chilton, “Design guidelines for prompt engineering
text-to-image generative models,” in <em id="bib.bib452.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 CHI
Conference on Human Factors in Computing Systems</em>, 2022, pp. 1–23.

</span>
</li>
<li id="bib.bib453" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[453]</span>
<span class="ltx_bibblock">
J.&nbsp;White, Q.&nbsp;Fu, S.&nbsp;Hays, M.&nbsp;Sandborn, C.&nbsp;Olea, H.&nbsp;Gilbert, A.&nbsp;Elnashar,
J.&nbsp;Spencer-Smith, and D.&nbsp;C. Schmidt, “A prompt pattern catalog to enhance
prompt engineering with chatgpt,” <em id="bib.bib453.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.11382</em>,
2023.

</span>
</li>
<li id="bib.bib454" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[454]</span>
<span class="ltx_bibblock">
S.&nbsp;K.&nbsp;K. Santu and D.&nbsp;Feng, “Teler: A general taxonomy of LLM prompts for
benchmarking complex tasks,” <em id="bib.bib454.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.11430, 2023.
[Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2305.11430" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.11430</a>

</span>
</li>
<li id="bib.bib455" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[455]</span>
<span class="ltx_bibblock">
OpenAI, “Gpt best practices,” <em id="bib.bib455.1.1" class="ltx_emph ltx_font_italic">OpenAI</em>, 2023. [Online]. Available:
<a target="_blank" href="https://platform.openai.com/docs/guides/gpt-best-practices" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/guides/gpt-best-practices</a>

</span>
</li>
<li id="bib.bib456" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[456]</span>
<span class="ltx_bibblock">
Contributors, “Ai short,” 2023. [Online]. Available:
<a target="_blank" href="https://www.aishort.top/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aishort.top/</a>

</span>
</li>
<li id="bib.bib457" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[457]</span>
<span class="ltx_bibblock">
——, “Awesome chatgpt prompts,” <em id="bib.bib457.1.1" class="ltx_emph ltx_font_italic">Github</em>, 2023. [Online]. Available:
<a target="_blank" href="https://github.com/f/awesome-chatgpt-prompts/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/f/awesome-chatgpt-prompts/</a>

</span>
</li>
<li id="bib.bib458" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[458]</span>
<span class="ltx_bibblock">
J.&nbsp;Jiang, K.&nbsp;Zhou, Z.&nbsp;Dong, K.&nbsp;Ye, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “Structgpt: A
general framework for large language model to reason over structured data,”
<em id="bib.bib458.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.09645, 2023.

</span>
</li>
<li id="bib.bib459" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[459]</span>
<span class="ltx_bibblock">
L.&nbsp;Beurer-Kellner, M.&nbsp;Fischer, and M.&nbsp;Vechev, “Prompting is programming: A
query language for large language models,” <em id="bib.bib459.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on
Programming Languages</em>, vol.&nbsp;7, no. PLDI, pp. 1946–1969, 2023.

</span>
</li>
<li id="bib.bib460" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[460]</span>
<span class="ltx_bibblock">
P.&nbsp;Lu, B.&nbsp;Peng, H.&nbsp;Cheng, M.&nbsp;Galley, K.-W. Chang, Y.&nbsp;N. Wu, S.-C. Zhu, and
J.&nbsp;Gao, “Chameleon: Plug-and-play compositional reasoning with large
language models,” <em id="bib.bib460.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.09842</em>, 2023.

</span>
</li>
<li id="bib.bib461" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[461]</span>
<span class="ltx_bibblock">
R.&nbsp;Ren, Y.&nbsp;Wang, Y.&nbsp;Qu, W.&nbsp;X. Zhao, J.&nbsp;Liu, H.&nbsp;Tian, H.&nbsp;Wu, J.-R. Wen, and
H.&nbsp;Wang, “Investigating the factual knowledge boundary of large language
models with retrieval augmentation,” <em id="bib.bib461.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.11019</em>,
2023.

</span>
</li>
<li id="bib.bib462" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[462]</span>
<span class="ltx_bibblock">
Y.&nbsp;Hou, J.&nbsp;Zhang, Z.&nbsp;Lin, H.&nbsp;Lu, R.&nbsp;Xie, J.&nbsp;J. McAuley, and W.&nbsp;X. Zhao, “Large
language models are zero-shot rankers for recommender systems,” <em id="bib.bib462.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.08845, 2023.

</span>
</li>
<li id="bib.bib463" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[463]</span>
<span class="ltx_bibblock">
S.&nbsp;Chang and E.&nbsp;Fosler-Lussier, “How to prompt llms for text-to-sql: A
study in zero-shot, single-domain, and cross-domain settings,” <em id="bib.bib463.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.11853, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.11853" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.11853</a>

</span>
</li>
<li id="bib.bib464" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[464]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wen, N.&nbsp;Jain, J.&nbsp;Kirchenbauer, M.&nbsp;Goldblum, J.&nbsp;Geiping, and T.&nbsp;Goldstein,
“Hard prompts made easy: Gradient-based discrete optimization for prompt
tuning and discovery,” <em id="bib.bib464.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.03668, 2023. [Online].
Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2302.03668" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2302.03668</a>

</span>
</li>
<li id="bib.bib465" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[465]</span>
<span class="ltx_bibblock">
T.&nbsp;Gao, A.&nbsp;Fisch, and D.&nbsp;Chen, “Making pre-trained language models better
few-shot learners,” in <em id="bib.bib465.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long
Papers), Virtual Event, August 1-6, 2021</em>, C.&nbsp;Zong, F.&nbsp;Xia, W.&nbsp;Li, and
R.&nbsp;Navigli, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2021, pp. 3816–3830.

</span>
</li>
<li id="bib.bib466" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[466]</span>
<span class="ltx_bibblock">
L.&nbsp;Chen, J.&nbsp;Chen, T.&nbsp;Goldstein, H.&nbsp;Huang, and T.&nbsp;Zhou, “Instructzero:
Efficient instruction optimization for black-box large language models,”
<em id="bib.bib466.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.03082, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2306.03082" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2306.03082</a>

</span>
</li>
<li id="bib.bib467" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[467]</span>
<span class="ltx_bibblock">
M.&nbsp;Deng, J.&nbsp;Wang, C.&nbsp;Hsieh, Y.&nbsp;Wang, H.&nbsp;Guo, T.&nbsp;Shu, M.&nbsp;Song, E.&nbsp;P. Xing, and
Z.&nbsp;Hu, “Rlprompt: Optimizing discrete text prompts with reinforcement
learning,” in <em id="bib.bib467.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab
Emirates, December 7-11, 2022</em>, Y.&nbsp;Goldberg, Z.&nbsp;Kozareva, and Y.&nbsp;Zhang,
Eds.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2022, pp. 3369–3391.

</span>
</li>
<li id="bib.bib468" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[468]</span>
<span class="ltx_bibblock">
T.&nbsp;Zhang, X.&nbsp;Wang, D.&nbsp;Zhou, D.&nbsp;Schuurmans, and J.&nbsp;E. Gonzalez, “TEMPERA:
test-time prompt editing via reinforcement learning,” in <em id="bib.bib468.1.1" class="ltx_emph ltx_font_italic">The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2023.

</span>
</li>
<li id="bib.bib469" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[469]</span>
<span class="ltx_bibblock">
H.&nbsp;Xu, Y.&nbsp;Chen, Y.&nbsp;Du, N.&nbsp;Shao, Y.&nbsp;Wang, H.&nbsp;Li, and Z.&nbsp;Yang, “GPS: genetic
prompt search for efficient few-shot learning,” in <em id="bib.bib469.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing, EMNLP
2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, Y.&nbsp;Goldberg,
Z.&nbsp;Kozareva, and Y.&nbsp;Zhang, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 8162–8171.

</span>
</li>
<li id="bib.bib470" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[470]</span>
<span class="ltx_bibblock">
A.&nbsp;Prasad, P.&nbsp;Hase, X.&nbsp;Zhou, and M.&nbsp;Bansal, “Grips: Gradient-free, edit-based
instruction search for prompting large language models,” in
<em id="bib.bib470.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia,
May 2-6, 2023</em>, A.&nbsp;Vlachos and I.&nbsp;Augenstein, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2023, pp. 3827–3846.

</span>
</li>
<li id="bib.bib471" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[471]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhou, A.&nbsp;I. Muresanu, Z.&nbsp;Han, K.&nbsp;Paster, S.&nbsp;Pitis, H.&nbsp;Chan, and J.&nbsp;Ba,
“Large language models are human-level prompt engineers,” in <em id="bib.bib471.1.1" class="ltx_emph ltx_font_italic">The
Eleventh International Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2023.

</span>
</li>
<li id="bib.bib472" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[472]</span>
<span class="ltx_bibblock">
R.&nbsp;Pryzant, D.&nbsp;Iter, J.&nbsp;Li, Y.&nbsp;T. Lee, C.&nbsp;Zhu, and M.&nbsp;Zeng, “Automatic prompt
optimization with ”gradient descent” and beam search,” <em id="bib.bib472.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.03495, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.03495" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.03495</a>

</span>
</li>
<li id="bib.bib473" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[473]</span>
<span class="ltx_bibblock">
C.&nbsp;Yang, X.&nbsp;Wang, Y.&nbsp;Lu, H.&nbsp;Liu, Q.&nbsp;V. Le, D.&nbsp;Zhou, and X.&nbsp;Chen, “Large
language models as optimizers,” <em id="bib.bib473.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.03409, 2023.
[Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2309.03409" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2309.03409</a>

</span>
</li>
<li id="bib.bib474" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[474]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, C.&nbsp;Li, Z.&nbsp;Wang, F.&nbsp;Bai, H.&nbsp;Luo, J.&nbsp;Zhang, N.&nbsp;Jojic, E.&nbsp;P. Xing, and
Z.&nbsp;Hu, “Promptagent: Strategic planning with language models enables
expert-level prompt optimization,” <em id="bib.bib474.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.16427, 2023.
[Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2310.16427" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2310.16427</a>

</span>
</li>
<li id="bib.bib475" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[475]</span>
<span class="ltx_bibblock">
T.&nbsp;Tang, J.&nbsp;Li, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “Context-tuning: Learning
contextualized prompts for natural language generation,” in
<em id="bib.bib475.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on Computational
Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17,
2022</em>, N.&nbsp;Calzolari, C.&nbsp;Huang, H.&nbsp;Kim, J.&nbsp;Pustejovsky, L.&nbsp;Wanner, K.&nbsp;Choi,
P.&nbsp;Ryu, H.&nbsp;Chen, L.&nbsp;Donatelli, H.&nbsp;Ji, S.&nbsp;Kurohashi, P.&nbsp;Paggio, N.&nbsp;Xue,
S.&nbsp;Kim, Y.&nbsp;Hahm, Z.&nbsp;He, T.&nbsp;K. Lee, E.&nbsp;Santus, F.&nbsp;Bond, and S.&nbsp;Na, Eds.&nbsp;&nbsp;&nbsp;International Committee on Computational
Linguistics, 2022, pp. 6340–6354.

</span>
</li>
<li id="bib.bib476" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[476]</span>
<span class="ltx_bibblock">
T.&nbsp;Vu, B.&nbsp;Lester, N.&nbsp;Constant, R.&nbsp;Al-Rfou’, and D.&nbsp;Cer, “Spot: Better frozen
model adaptation through soft prompt transfer,” in <em id="bib.bib476.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, S.&nbsp;Muresan,
P.&nbsp;Nakov, and A.&nbsp;Villavicencio, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 5039–5059.

</span>
</li>
<li id="bib.bib477" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[477]</span>
<span class="ltx_bibblock">
J.&nbsp;Li, T.&nbsp;Tang, J.&nbsp;Nie, J.&nbsp;Wen, and X.&nbsp;Zhao, “Learning to transfer prompts for
text generation,” in <em id="bib.bib477.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15,
2022</em>, M.&nbsp;Carpuat, M.&nbsp;de&nbsp;Marneffe, and I.&nbsp;V.&nbsp;M. Ruíz, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022,
pp. 3506–3518.

</span>
</li>
<li id="bib.bib478" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[478]</span>
<span class="ltx_bibblock">
S.&nbsp;Min, X.&nbsp;Lyu, A.&nbsp;Holtzman, M.&nbsp;Artetxe, M.&nbsp;Lewis, H.&nbsp;Hajishirzi, and
L.&nbsp;Zettlemoyer, “Rethinking the role of demonstrations: What makes
in-context learning work?” in <em id="bib.bib478.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,
United Arab Emirates, December 7-11, 2022</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp.
11 048–11 064.

</span>
</li>
<li id="bib.bib479" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[479]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zhao, E.&nbsp;Wallace, S.&nbsp;Feng, D.&nbsp;Klein, and S.&nbsp;Singh, “Calibrate before use:
Improving few-shot performance of language models,” in <em id="bib.bib479.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event</em>, M.&nbsp;Meila and T.&nbsp;Zhang, Eds., 2021, pp.
12 697–12 706.

</span>
</li>
<li id="bib.bib480" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[480]</span>
<span class="ltx_bibblock">
Y.&nbsp;Lee, C.&nbsp;Lim, and H.&nbsp;Choi, “Does GPT-3 generate empathetic dialogues? A
novel in-context example selection method and automatic evaluation metric for
empathetic dialogue generation,” in <em id="bib.bib480.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th
International Conference on Computational Linguistics, COLING 2022,
Gyeongju, Republic of Korea, October 12-17, 2022</em>, N.&nbsp;Calzolari, C.&nbsp;Huang,
H.&nbsp;Kim, J.&nbsp;Pustejovsky, L.&nbsp;Wanner, K.&nbsp;Choi, P.&nbsp;Ryu, H.&nbsp;Chen, L.&nbsp;Donatelli,
H.&nbsp;Ji, S.&nbsp;Kurohashi, P.&nbsp;Paggio, N.&nbsp;Xue, S.&nbsp;Kim, Y.&nbsp;Hahm, Z.&nbsp;He, T.&nbsp;K. Lee,
E.&nbsp;Santus, F.&nbsp;Bond, and S.&nbsp;Na, Eds.&nbsp;&nbsp;&nbsp;International Committee on Computational Linguistics, 2022, pp. 669–683.

</span>
</li>
<li id="bib.bib481" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[481]</span>
<span class="ltx_bibblock">
I.&nbsp;Levy, B.&nbsp;Bogin, and J.&nbsp;Berant, “Diverse demonstrations improve in-context
compositional generalization,” <em id="bib.bib481.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.06800, 2022.

</span>
</li>
<li id="bib.bib482" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[482]</span>
<span class="ltx_bibblock">
H.&nbsp;Su, J.&nbsp;Kasai, C.&nbsp;H. Wu, W.&nbsp;Shi, T.&nbsp;Wang, J.&nbsp;Xin, R.&nbsp;Zhang, M.&nbsp;Ostendorf,
L.&nbsp;Zettlemoyer, N.&nbsp;A. Smith, and T.&nbsp;Yu, “Selective annotation makes language
models better few-shot learners,” <em id="bib.bib482.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2022.

</span>
</li>
<li id="bib.bib483" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[483]</span>
<span class="ltx_bibblock">
X.&nbsp;Ye, S.&nbsp;Iyer, A.&nbsp;Celikyilmaz, V.&nbsp;Stoyanov, G.&nbsp;Durrett, and R.&nbsp;Pasunuru,
“Complementary explanations for effective in-context learning,”
<em id="bib.bib483.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2022.

</span>
</li>
<li id="bib.bib484" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[484]</span>
<span class="ltx_bibblock">
X.&nbsp;Li and X.&nbsp;Qiu, “Finding supporting examples for in-context learning,”
<em id="bib.bib484.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib485" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[485]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, S.&nbsp;Feng, and C.&nbsp;Tan, “Active example selection for in-context
learning,” in <em id="bib.bib485.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab
Emirates, December 7-11, 2022</em>, 2022, pp. 9134–9148.

</span>
</li>
<li id="bib.bib486" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[486]</span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli, “Chatgpt outperforms crowd-workers for
text-annotation tasks,” 2023.

</span>
</li>
<li id="bib.bib487" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[487]</span>
<span class="ltx_bibblock">
H.&nbsp;J. Kim, H.&nbsp;Cho, J.&nbsp;Kim, T.&nbsp;Kim, K.&nbsp;M. Yoo, and S.&nbsp;Lee, “Self-generated
in-context learning: Leveraging auto-regressive language models as a
demonstration generator,” <em id="bib.bib487.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2206.08082, 2022.

</span>
</li>
<li id="bib.bib488" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[488]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Xie, A.&nbsp;Raghunathan, P.&nbsp;Liang, and T.&nbsp;Ma, “An explanation of in-context
learning as implicit bayesian inference,” in <em id="bib.bib488.1.1" class="ltx_emph ltx_font_italic">The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event, April
25-29, 2022</em>, 2022.

</span>
</li>
<li id="bib.bib489" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[489]</span>
<span class="ltx_bibblock">
Z.&nbsp;Wu, Y.&nbsp;Wang, J.&nbsp;Ye, and L.&nbsp;Kong, “Self-adaptive in-context learning,”
<em id="bib.bib489.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.10375, 2022.

</span>
</li>
<li id="bib.bib490" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[490]</span>
<span class="ltx_bibblock">
Y.&nbsp;Gu, L.&nbsp;Dong, F.&nbsp;Wei, and M.&nbsp;Huang, “Pre-training to learn in context,”
<em id="bib.bib490.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.09137, 2023.

</span>
</li>
<li id="bib.bib491" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[491]</span>
<span class="ltx_bibblock">
S.&nbsp;Min, M.&nbsp;Lewis, L.&nbsp;Zettlemoyer, and H.&nbsp;Hajishirzi, “Metaicl: Learning to
learn in context,” in <em id="bib.bib491.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15,
2022</em>, M.&nbsp;Carpuat, M.&nbsp;de&nbsp;Marneffe, and I.&nbsp;V.&nbsp;M. Ruíz, Eds., 2022, pp.
2791–2809.

</span>
</li>
<li id="bib.bib492" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[492]</span>
<span class="ltx_bibblock">
M.&nbsp;Hahn and N.&nbsp;Goyal, “A theory of emergent in-context learning as implicit
structure induction,” <em id="bib.bib492.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.07971, 2023.

</span>
</li>
<li id="bib.bib493" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[493]</span>
<span class="ltx_bibblock">
J.&nbsp;Pan, T.&nbsp;Gao, H.&nbsp;Chen, and D.&nbsp;Chen, “What in-context learning ”learns”
in-context: Disentangling task recognition and task learning,” <em id="bib.bib493.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.09731, 2023.

</span>
</li>
<li id="bib.bib494" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[494]</span>
<span class="ltx_bibblock">
N.&nbsp;Wies, Y.&nbsp;Levine, and A.&nbsp;Shashua, “The learnability of in-context
learning,” <em id="bib.bib494.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.07895, 2023.

</span>
</li>
<li id="bib.bib495" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[495]</span>
<span class="ltx_bibblock">
A.&nbsp;Webson and E.&nbsp;Pavlick, “Do prompt-based models really understand the
meaning of their prompts?” in <em id="bib.bib495.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of
the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL 2022, Seattle, WA, United States, July
10-15, 2022</em>, 2022, pp. 2300–2344.

</span>
</li>
<li id="bib.bib496" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[496]</span>
<span class="ltx_bibblock">
J.&nbsp;von Oswald, E.&nbsp;Niklasson, E.&nbsp;Randazzo, J.&nbsp;Sacramento, A.&nbsp;Mordvintsev,
A.&nbsp;Zhmoginov, and M.&nbsp;Vladymyrov, “Transformers learn in-context by gradient
descent,” <em id="bib.bib496.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.07677, 2022.

</span>
</li>
<li id="bib.bib497" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[497]</span>
<span class="ltx_bibblock">
C.&nbsp;Olsson, N.&nbsp;Elhage, N.&nbsp;Nanda, N.&nbsp;Joseph, N.&nbsp;DasSarma, T.&nbsp;Henighan, B.&nbsp;Mann,
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, T.&nbsp;Conerly, D.&nbsp;Drain, D.&nbsp;Ganguli,
Z.&nbsp;Hatfield-Dodds, D.&nbsp;Hernandez, S.&nbsp;Johnston, A.&nbsp;Jones, J.&nbsp;Kernion,
L.&nbsp;Lovitt, K.&nbsp;Ndousse, D.&nbsp;Amodei, T.&nbsp;Brown, J.&nbsp;Clark, J.&nbsp;Kaplan,
S.&nbsp;McCandlish, and C.&nbsp;Olah, “In-context learning and induction heads,”
<em id="bib.bib497.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.11895, 2022.

</span>
</li>
<li id="bib.bib498" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[498]</span>
<span class="ltx_bibblock">
E.&nbsp;Akyürek, D.&nbsp;Schuurmans, J.&nbsp;Andreas, T.&nbsp;Ma, and D.&nbsp;Zhou, “What
learning algorithm is in-context learning? investigations with linear
models,” <em id="bib.bib498.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.15661, 2022.

</span>
</li>
<li id="bib.bib499" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[499]</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, J.&nbsp;Wei, Y.&nbsp;Tay, D.&nbsp;Tran, A.&nbsp;Webson, Y.&nbsp;Lu, X.&nbsp;Chen, H.&nbsp;Liu, D.&nbsp;Huang,
D.&nbsp;Zhou <em id="bib.bib499.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Larger language models do in-context learning
differently,” <em id="bib.bib499.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.03846</em>, 2023.

</span>
</li>
<li id="bib.bib500" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[500]</span>
<span class="ltx_bibblock">
J.&nbsp;Coda-Forno, M.&nbsp;Binz, Z.&nbsp;Akata, M.&nbsp;M. Botvinick, J.&nbsp;X. Wang, and E.&nbsp;Schulz,
“Meta-in-context learning in large language models,” <em id="bib.bib500.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.12907, 2023.

</span>
</li>
<li id="bib.bib501" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[501]</span>
<span class="ltx_bibblock">
J.&nbsp;W. Wei, L.&nbsp;Hou, A.&nbsp;K. Lampinen, X.&nbsp;Chen, D.&nbsp;Huang, Y.&nbsp;Tay, X.&nbsp;Chen, Y.&nbsp;Lu,
D.&nbsp;Zhou, T.&nbsp;Ma, and Q.&nbsp;V. Le, “Symbol tuning improves in-context learning in
language models,” <em id="bib.bib501.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.08298, 2023.

</span>
</li>
<li id="bib.bib502" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[502]</span>
<span class="ltx_bibblock">
Z.&nbsp;Chu, J.&nbsp;Chen, Q.&nbsp;Chen, W.&nbsp;Yu, T.&nbsp;He, H.&nbsp;Wang, W.&nbsp;Peng, M.&nbsp;Liu, B.&nbsp;Qin, and
T.&nbsp;Liu, “A survey of chain of thought reasoning: Advances, frontiers and
future,” <em id="bib.bib502.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.15402, 2023.

</span>
</li>
<li id="bib.bib503" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[503]</span>
<span class="ltx_bibblock">
S.&nbsp;Miao, C.&nbsp;Liang, and K.&nbsp;Su, “A diverse corpus for evaluating and developing
english math word problem solvers,” in <em id="bib.bib503.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, ACL 2020, Online,
July 5-10, 2020</em>, D.&nbsp;Jurafsky, J.&nbsp;Chai, N.&nbsp;Schluter, and J.&nbsp;R. Tetreault,
Eds.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2020, pp. 975–984.

</span>
</li>
<li id="bib.bib504" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[504]</span>
<span class="ltx_bibblock">
A.&nbsp;Talmor, J.&nbsp;Herzig, N.&nbsp;Lourie, and J.&nbsp;Berant, “Commonsenseqa: A question
answering challenge targeting commonsense knowledge,” in <em id="bib.bib504.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>,
J.&nbsp;Burstein, C.&nbsp;Doran, and T.&nbsp;Solorio, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2019, pp. 4149–4158.

</span>
</li>
<li id="bib.bib505" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[505]</span>
<span class="ltx_bibblock">
T.&nbsp;Kojima, S.&nbsp;S. Gu, M.&nbsp;Reid, Y.&nbsp;Matsuo, and Y.&nbsp;Iwasawa, “Large language
models are zero-shot reasoners,” <em id="bib.bib505.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2205.11916, 2022.

</span>
</li>
<li id="bib.bib506" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[506]</span>
<span class="ltx_bibblock">
W.&nbsp;Chen, X.&nbsp;Ma, X.&nbsp;Wang, and W.&nbsp;W. Cohen, “Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks,”
<em id="bib.bib506.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.12588, 2022.

</span>
</li>
<li id="bib.bib507" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[507]</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, A.&nbsp;Madaan, S.&nbsp;Zhou, U.&nbsp;Alon, P.&nbsp;Liu, Y.&nbsp;Yang, J.&nbsp;Callan, and G.&nbsp;Neubig,
“PAL: program-aided language models,” in <em id="bib.bib507.1.1" class="ltx_emph ltx_font_italic">International Conference
on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>,
A.&nbsp;Krause, E.&nbsp;Brunskill, K.&nbsp;Cho, B.&nbsp;Engelhardt, S.&nbsp;Sabato, and J.&nbsp;Scarlett,
Eds., 2023.

</span>
</li>
<li id="bib.bib508" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[508]</span>
<span class="ltx_bibblock">
X.&nbsp;Zhao, Y.&nbsp;Xie, K.&nbsp;Kawaguchi, J.&nbsp;He, and Q.&nbsp;Xie, “Automatic model selection
with large language models for reasoning,” <em id="bib.bib508.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.14333,
2023.

</span>
</li>
<li id="bib.bib509" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[509]</span>
<span class="ltx_bibblock">
Y.&nbsp;Li, Z.&nbsp;Lin, S.&nbsp;Zhang, Q.&nbsp;Fu, B.&nbsp;Chen, J.-G. Lou, and W.&nbsp;Chen, “Making large
language models better reasoners with step-aware verifier,” 2023.

</span>
</li>
<li id="bib.bib510" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[510]</span>
<span class="ltx_bibblock">
O.&nbsp;Yoran, T.&nbsp;Wolfson, B.&nbsp;Bogin, U.&nbsp;Katz, D.&nbsp;Deutch, and J.&nbsp;Berant, “Answering
questions by meta-reasoning over multiple chains of thought,” <em id="bib.bib510.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2304.13007, 2023.

</span>
</li>
<li id="bib.bib511" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[511]</span>
<span class="ltx_bibblock">
Z.&nbsp;Ling, Y.&nbsp;Fang, X.&nbsp;Li, Z.&nbsp;Huang, M.&nbsp;Lee, R.&nbsp;Memisevic, and H.&nbsp;Su, “Deductive
verification of chain-of-thought reasoning,” <em id="bib.bib511.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.03872, 2023.

</span>
</li>
<li id="bib.bib512" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[512]</span>
<span class="ltx_bibblock">
T.&nbsp;Xue, Z.&nbsp;Wang, Z.&nbsp;Wang, C.&nbsp;Han, P.&nbsp;Yu, and H.&nbsp;Ji, “RCOT: detecting and
rectifying factual inconsistency in reasoning by reversing
chain-of-thought,” <em id="bib.bib512.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.11499, 2023.

</span>
</li>
<li id="bib.bib513" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[513]</span>
<span class="ltx_bibblock">
Y.&nbsp;Weng, M.&nbsp;Zhu, F.&nbsp;Xia, B.&nbsp;Li, S.&nbsp;He, K.&nbsp;Liu, and J.&nbsp;Zhao, “Large language
models are better reasoners with self-verification,” <em id="bib.bib513.1.1" class="ltx_emph ltx_font_italic">CoRR,
abs/2212.09561</em>, 2023.

</span>
</li>
<li id="bib.bib514" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[514]</span>
<span class="ltx_bibblock">
W.&nbsp;Jiang, H.&nbsp;Shi, L.&nbsp;Yu, Z.&nbsp;Liu, Y.&nbsp;Zhang, Z.&nbsp;Li, and J.&nbsp;T. Kwok,
“Forward-backward reasoning in large language models for mathematical
verification,” 2023.

</span>
</li>
<li id="bib.bib515" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[515]</span>
<span class="ltx_bibblock">
J.&nbsp;Long, “Large language model guided tree-of-thought,” <em id="bib.bib515.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.08291, 2023.

</span>
</li>
<li id="bib.bib516" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[516]</span>
<span class="ltx_bibblock">
S.&nbsp;Mo and M.&nbsp;Xin, “Tree of uncertain thoughts reasoning for large language
models,” <em id="bib.bib516.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.07694, 2023.

</span>
</li>
<li id="bib.bib517" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[517]</span>
<span class="ltx_bibblock">
M.&nbsp;Besta, N.&nbsp;Blach, A.&nbsp;Kubicek, R.&nbsp;Gerstenberger, L.&nbsp;Gianinazzi, J.&nbsp;Gajda,
T.&nbsp;Lehmann, M.&nbsp;Podstawski, H.&nbsp;Niewiadomski, P.&nbsp;Nyczyk, and T.&nbsp;Hoefler,
“Graph of thoughts: Solving elaborate problems with large language models,”
<em id="bib.bib517.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.09687, 2023.

</span>
</li>
<li id="bib.bib518" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[518]</span>
<span class="ltx_bibblock">
B.&nbsp;Lei, P.&nbsp;Lin, C.&nbsp;Liao, and C.&nbsp;Ding, “Boosting logical reasoning in large
language models through a new framework: The graph of thought,” <em id="bib.bib518.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2308.08614, 2023.

</span>
</li>
<li id="bib.bib519" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[519]</span>
<span class="ltx_bibblock">
R.&nbsp;Ding, C.&nbsp;Zhang, L.&nbsp;Wang, Y.&nbsp;Xu, M.&nbsp;Ma, W.&nbsp;Zhang, S.&nbsp;Qin, S.&nbsp;Rajmohan,
Q.&nbsp;Lin, and D.&nbsp;Zhang, “Everything of thoughts: Defying the law of penrose
triangle for thought generation,” <em id="bib.bib519.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.04254</em>,
2023.

</span>
</li>
<li id="bib.bib520" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[520]</span>
<span class="ltx_bibblock">
P.&nbsp;Liang, R.&nbsp;Bommasani, T.&nbsp;Lee, D.&nbsp;Tsipras, D.&nbsp;Soylu, M.&nbsp;Yasunaga, Y.&nbsp;Zhang,
D.&nbsp;Narayanan, Y.&nbsp;Wu, A.&nbsp;Kumar, B.&nbsp;Newman, B.&nbsp;Yuan, B.&nbsp;Yan, C.&nbsp;Zhang,
C.&nbsp;Cosgrove, C.&nbsp;D. Manning, C.&nbsp;Ré, D.&nbsp;Acosta-Navas, D.&nbsp;A. Hudson,
E.&nbsp;Zelikman, E.&nbsp;Durmus, F.&nbsp;Ladhak, F.&nbsp;Rong, H.&nbsp;Ren, H.&nbsp;Yao, J.&nbsp;Wang,
K.&nbsp;Santhanam, L.&nbsp;J. Orr, L.&nbsp;Zheng, M.&nbsp;Yüksekgönül,
M.&nbsp;Suzgun, N.&nbsp;Kim, N.&nbsp;Guha, N.&nbsp;S. Chatterji, O.&nbsp;Khattab, P.&nbsp;Henderson,
Q.&nbsp;Huang, R.&nbsp;Chi, S.&nbsp;M. Xie, S.&nbsp;Santurkar, S.&nbsp;Ganguli, T.&nbsp;Hashimoto,
T.&nbsp;Icard, T.&nbsp;Zhang, V.&nbsp;Chaudhary, W.&nbsp;Wang, X.&nbsp;Li, Y.&nbsp;Mai, Y.&nbsp;Zhang, and
Y.&nbsp;Koreeda, “Holistic evaluation of language models,” <em id="bib.bib520.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2211.09110, 2022.

</span>
</li>
<li id="bib.bib521" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[521]</span>
<span class="ltx_bibblock">
Z.&nbsp;Bi, N.&nbsp;Zhang, Y.&nbsp;Jiang, S.&nbsp;Deng, G.&nbsp;Zheng, and H.&nbsp;Chen, “When do
program-of-thoughts work for reasoning?” <em id="bib.bib521.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.15452,
2023.

</span>
</li>
<li id="bib.bib522" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[522]</span>
<span class="ltx_bibblock">
A.&nbsp;Madaan and A.&nbsp;Yazdanbakhsh, “Text and patterns: For effective chain of
thought, it takes two to tango,” <em id="bib.bib522.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.07686, 2022.

</span>
</li>
<li id="bib.bib523" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[523]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, A.&nbsp;Zhang, M.&nbsp;Li, H.&nbsp;Zhao, G.&nbsp;Karypis, and A.&nbsp;Smola, “Multimodal
chain-of-thought reasoning in language models,” <em id="bib.bib523.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2302.00923, 2023.

</span>
</li>
<li id="bib.bib524" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[524]</span>
<span class="ltx_bibblock">
F.&nbsp;Shi, M.&nbsp;Suzgun, M.&nbsp;Freitag, X.&nbsp;Wang, S.&nbsp;Srivats, S.&nbsp;Vosoughi, H.&nbsp;W. Chung,
Y.&nbsp;Tay, S.&nbsp;Ruder, D.&nbsp;Zhou, D.&nbsp;Das, and J.&nbsp;Wei, “Language models are
multilingual chain-of-thought reasoners,” <em id="bib.bib524.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.03057,
2022.

</span>
</li>
<li id="bib.bib525" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[525]</span>
<span class="ltx_bibblock">
J.&nbsp;Qian, H.&nbsp;Wang, Z.&nbsp;Li, S.&nbsp;Li, and X.&nbsp;Yan, “Limitations of language models in
arithmetic and symbolic induction,” <em id="bib.bib525.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2208.05051, 2022.

</span>
</li>
<li id="bib.bib526" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[526]</span>
<span class="ltx_bibblock">
N.&nbsp;Bian, X.&nbsp;Han, L.&nbsp;Sun, H.&nbsp;Lin, Y.&nbsp;Lu, and B.&nbsp;He, “ChatGPT is a
Knowledgeable but Inexperienced Solver: An Investigation of Commonsense
Problem in Large Language Models,” <em id="bib.bib526.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib527" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[527]</span>
<span class="ltx_bibblock">
S.&nbsp;Yao, D.&nbsp;Yu, J.&nbsp;Zhao, I.&nbsp;Shafran, T.&nbsp;L. Griffiths, Y.&nbsp;Cao, and K.&nbsp;Narasimhan,
“Tree of thoughts: Deliberate problem solving with large language models,”
<em id="bib.bib527.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.10601, 2023.

</span>
</li>
<li id="bib.bib528" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[528]</span>
<span class="ltx_bibblock">
G.&nbsp;Wang, Y.&nbsp;Xie, Y.&nbsp;Jiang, A.&nbsp;Mandlekar, C.&nbsp;Xiao, Y.&nbsp;Zhu, L.&nbsp;Fan, and
A.&nbsp;Anandkumar, “Voyager: An open-ended embodied agent with large language
models,” <em id="bib.bib528.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16291</em>, 2023.

</span>
</li>
<li id="bib.bib529" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[529]</span>
<span class="ltx_bibblock">
X.&nbsp;Jiang, Y.&nbsp;Dong, L.&nbsp;Wang, Q.&nbsp;Shang, and G.&nbsp;Li, “Self-planning code
generation with large language model,” <em id="bib.bib529.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.06689,
2023. [Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2303.06689" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2303.06689</a>

</span>
</li>
<li id="bib.bib530" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[530]</span>
<span class="ltx_bibblock">
I.&nbsp;Singh, V.&nbsp;Blukis, A.&nbsp;Mousavian, A.&nbsp;Goyal, D.&nbsp;Xu, J.&nbsp;Tremblay, D.&nbsp;Fox,
J.&nbsp;Thomason, and A.&nbsp;Garg, “Progprompt: Generating situated robot task plans
using large language models,” <em id="bib.bib530.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.11302, 2022.

</span>
</li>
<li id="bib.bib531" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[531]</span>
<span class="ltx_bibblock">
B.&nbsp;Liu, Y.&nbsp;Jiang, X.&nbsp;Zhang, Q.&nbsp;Liu, S.&nbsp;Zhang, J.&nbsp;Biswas, and P.&nbsp;Stone,
“LLM+P: empowering large language models with optimal planning
proficiency,” <em id="bib.bib531.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.11477, 2023. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2304.11477" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2304.11477</a>

</span>
</li>
<li id="bib.bib532" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[532]</span>
<span class="ltx_bibblock">
R.&nbsp;Rombach, A.&nbsp;Blattmann, D.&nbsp;Lorenz, P.&nbsp;Esser, and B.&nbsp;Ommer, “High-resolution
image synthesis with latent diffusion models,” in <em id="bib.bib532.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF
Conference on Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022</em>, 2022, pp. 10 674–10 685.

</span>
</li>
<li id="bib.bib533" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[533]</span>
<span class="ltx_bibblock">
J.&nbsp;S. Park, J.&nbsp;C. O’Brien, C.&nbsp;J. Cai, M.&nbsp;R. Morris, P.&nbsp;Liang, and M.&nbsp;S.
Bernstein, “Generative agents: Interactive simulacra of human behavior,”
<em id="bib.bib533.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.03442, 2023.

</span>
</li>
<li id="bib.bib534" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[534]</span>
<span class="ltx_bibblock">
2023. [Online]. Available:
<a target="_blank" href="https://github.com/Significant-Gravitas/Auto-GPT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Significant-Gravitas/Auto-GPT</a>

</span>
</li>
<li id="bib.bib535" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[535]</span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, S.&nbsp;Cai, A.&nbsp;Liu, X.&nbsp;Ma, and Y.&nbsp;Liang, “Describe, explain, plan and
select: Interactive planning with large language models enables open-world
multi-task agents,” <em id="bib.bib535.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.01560, 2023.

</span>
</li>
<li id="bib.bib536" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[536]</span>
<span class="ltx_bibblock">
J.&nbsp;Wang, X.&nbsp;Yi, R.&nbsp;Guo, H.&nbsp;Jin, P.&nbsp;Xu, S.&nbsp;Li, X.&nbsp;Wang, X.&nbsp;Guo, C.&nbsp;Li, X.&nbsp;Xu
<em id="bib.bib536.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Milvus: A purpose-built vector data management system,” in
<em id="bib.bib536.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 2021 International Conference on Management of
Data</em>, 2021, pp. 2614–2627.

</span>
</li>
<li id="bib.bib537" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[537]</span>
<span class="ltx_bibblock">
W.&nbsp;Zhong, L.&nbsp;Guo, Q.&nbsp;Gao, H.&nbsp;Ye, and Y.&nbsp;Wang, “Memorybank: Enhancing large
language models with long-term memory,” <em id="bib.bib537.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.10250,
2023.

</span>
</li>
<li id="bib.bib538" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[538]</span>
<span class="ltx_bibblock">
M.&nbsp;P. Marcus, B.&nbsp;Santorini, and M.&nbsp;A. Marcinkiewicz, “Building a large
annotated corpus of english: The penn treebank,” <em id="bib.bib538.1.1" class="ltx_emph ltx_font_italic">Comput. Linguistics</em>,
vol.&nbsp;19, no.&nbsp;2, pp. 313–330, 1993.

</span>
</li>
<li id="bib.bib539" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[539]</span>
<span class="ltx_bibblock">
S.&nbsp;Merity, C.&nbsp;Xiong, J.&nbsp;Bradbury, and R.&nbsp;Socher, “Pointer sentinel mixture
models,” in <em id="bib.bib539.1.1" class="ltx_emph ltx_font_italic">ICLR (Poster)</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2017.

</span>
</li>
<li id="bib.bib540" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[540]</span>
<span class="ltx_bibblock">
O.&nbsp;Bojar, C.&nbsp;Buck, C.&nbsp;Federmann, B.&nbsp;Haddow, P.&nbsp;Koehn, J.&nbsp;Leveling, C.&nbsp;Monz,
P.&nbsp;Pecina, M.&nbsp;Post, H.&nbsp;Saint-Amand, R.&nbsp;Soricut, L.&nbsp;Specia, and A.&nbsp;Tamchyna,
“Findings of the 2014 workshop on statistical machine translation,” in
<em id="bib.bib540.1.1" class="ltx_emph ltx_font_italic">WMT@ACL</em>.&nbsp;&nbsp;&nbsp;The Association for
Computer Linguistics, 2014, pp. 12–58.

</span>
</li>
<li id="bib.bib541" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[541]</span>
<span class="ltx_bibblock">
O.&nbsp;Bojar, R.&nbsp;Chatterjee, C.&nbsp;Federmann, Y.&nbsp;Graham, B.&nbsp;Haddow, M.&nbsp;Huck,
A.&nbsp;Jimeno-Yepes, P.&nbsp;Koehn, V.&nbsp;Logacheva, C.&nbsp;Monz, M.&nbsp;Negri,
A.&nbsp;Névéol, M.&nbsp;L. Neves, M.&nbsp;Popel, M.&nbsp;Post, R.&nbsp;Rubino, C.&nbsp;Scarton,
L.&nbsp;Specia, M.&nbsp;Turchi, K.&nbsp;Verspoor, and M.&nbsp;Zampieri, “Findings of the 2016
conference on machine translation,” in <em id="bib.bib541.1.1" class="ltx_emph ltx_font_italic">WMT</em>.&nbsp;&nbsp;&nbsp;The Association for Computer Linguistics, 2016, pp.
131–198.

</span>
</li>
<li id="bib.bib542" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[542]</span>
<span class="ltx_bibblock">
L.&nbsp;Barrault, O.&nbsp;Bojar, M.&nbsp;R. Costa-jussà, C.&nbsp;Federmann, M.&nbsp;Fishel,
Y.&nbsp;Graham, B.&nbsp;Haddow, M.&nbsp;Huck, P.&nbsp;Koehn, S.&nbsp;Malmasi, C.&nbsp;Monz,
M.&nbsp;Müller, S.&nbsp;Pal, M.&nbsp;Post, and M.&nbsp;Zampieri, “Findings of the 2019
conference on machine translation (WMT19),” in <em id="bib.bib542.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
Fourth Conference on Machine Translation, WMT 2019, Florence, Italy, August
1-2, 2019 - Volume 2: Shared Task Papers, Day 1</em>, O.&nbsp;Bojar, R.&nbsp;Chatterjee,
C.&nbsp;Federmann, M.&nbsp;Fishel, Y.&nbsp;Graham, B.&nbsp;Haddow, M.&nbsp;Huck, A.&nbsp;Jimeno-Yepes,
P.&nbsp;Koehn, A.&nbsp;Martins, C.&nbsp;Monz, M.&nbsp;Negri, A.&nbsp;Névéol, M.&nbsp;L. Neves,
M.&nbsp;Post, M.&nbsp;Turchi, and K.&nbsp;Verspoor, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2019, pp. 1–61.

</span>
</li>
<li id="bib.bib543" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[543]</span>
<span class="ltx_bibblock">
L.&nbsp;Barrault, M.&nbsp;Biesialska, O.&nbsp;Bojar, M.&nbsp;R. Costa-jussà, C.&nbsp;Federmann,
Y.&nbsp;Graham, R.&nbsp;Grundkiewicz, B.&nbsp;Haddow, M.&nbsp;Huck, E.&nbsp;Joanis, T.&nbsp;Kocmi,
P.&nbsp;Koehn, C.&nbsp;Lo, N.&nbsp;Ljubesic, C.&nbsp;Monz, M.&nbsp;Morishita, M.&nbsp;Nagata, T.&nbsp;Nakazawa,
S.&nbsp;Pal, M.&nbsp;Post, and M.&nbsp;Zampieri, “Findings of the 2020 conference on
machine translation (WMT20),” in <em id="bib.bib543.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Conference
on Machine Translation, WMT@EMNLP 2020, Online, November 19-20, 2020</em>,
L.&nbsp;Barrault, O.&nbsp;Bojar, F.&nbsp;Bougares, R.&nbsp;Chatterjee, M.&nbsp;R. Costa-jussà,
C.&nbsp;Federmann, M.&nbsp;Fishel, A.&nbsp;Fraser, Y.&nbsp;Graham, P.&nbsp;Guzman, B.&nbsp;Haddow, M.&nbsp;Huck,
A.&nbsp;Jimeno-Yepes, P.&nbsp;Koehn, A.&nbsp;Martins, M.&nbsp;Morishita, C.&nbsp;Monz, M.&nbsp;Nagata,
T.&nbsp;Nakazawa, and M.&nbsp;Negri, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020, pp. 1–55.

</span>
</li>
<li id="bib.bib544" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[544]</span>
<span class="ltx_bibblock">
F.&nbsp;Akhbardeh, A.&nbsp;Arkhangorodsky, M.&nbsp;Biesialska, O.&nbsp;Bojar, R.&nbsp;Chatterjee,
V.&nbsp;Chaudhary, M.&nbsp;R. Costa-jussà, C.&nbsp;España-Bonet, A.&nbsp;Fan,
C.&nbsp;Federmann, M.&nbsp;Freitag, Y.&nbsp;Graham, R.&nbsp;Grundkiewicz, B.&nbsp;Haddow, L.&nbsp;Harter,
K.&nbsp;Heafield, C.&nbsp;Homan, M.&nbsp;Huck, K.&nbsp;Amponsah-Kaakyire, J.&nbsp;Kasai,
D.&nbsp;Khashabi, K.&nbsp;Knight, T.&nbsp;Kocmi, P.&nbsp;Koehn, N.&nbsp;Lourie, C.&nbsp;Monz, M.&nbsp;Morishita,
M.&nbsp;Nagata, A.&nbsp;Nagesh, T.&nbsp;Nakazawa, M.&nbsp;Negri, S.&nbsp;Pal, A.&nbsp;A. Tapo, M.&nbsp;Turchi,
V.&nbsp;Vydrin, and M.&nbsp;Zampieri, “Findings of the 2021 conference on machine
translation (WMT21),” in <em id="bib.bib544.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth Conference on
Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021</em>,
L.&nbsp;Barrault, O.&nbsp;Bojar, F.&nbsp;Bougares, R.&nbsp;Chatterjee, M.&nbsp;R. Costa-jussà,
C.&nbsp;Federmann, M.&nbsp;Fishel, A.&nbsp;Fraser, M.&nbsp;Freitag, Y.&nbsp;Graham, R.&nbsp;Grundkiewicz,
P.&nbsp;Guzman, B.&nbsp;Haddow, M.&nbsp;Huck, A.&nbsp;Jimeno-Yepes, P.&nbsp;Koehn, T.&nbsp;Kocmi,
A.&nbsp;Martins, M.&nbsp;Morishita, and C.&nbsp;Monz, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 1–88.

</span>
</li>
<li id="bib.bib545" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[545]</span>
<span class="ltx_bibblock">
T.&nbsp;Kocmi, R.&nbsp;Bawden, O.&nbsp;Bojar, A.&nbsp;Dvorkovich, C.&nbsp;Federmann, M.&nbsp;Fishel,
T.&nbsp;Gowda, Y.&nbsp;Graham, R.&nbsp;Grundkiewicz, B.&nbsp;Haddow, R.&nbsp;Knowles, P.&nbsp;Koehn,
C.&nbsp;Monz, M.&nbsp;Morishita, M.&nbsp;Nagata, T.&nbsp;Nakazawa, M.&nbsp;Novák, M.&nbsp;Popel, and
M.&nbsp;Popovic, “Findings of the 2022 conference on machine translation
(WMT22),” in <em id="bib.bib545.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh Conference on Machine
Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December
7-8, 2022</em>, P.&nbsp;Koehn, L.&nbsp;Barrault, O.&nbsp;Bojar, F.&nbsp;Bougares, R.&nbsp;Chatterjee,
M.&nbsp;R. Costa-jussà, C.&nbsp;Federmann, M.&nbsp;Fishel, A.&nbsp;Fraser, M.&nbsp;Freitag,
Y.&nbsp;Graham, R.&nbsp;Grundkiewicz, P.&nbsp;Guzman, B.&nbsp;Haddow, M.&nbsp;Huck, A.&nbsp;Jimeno-Yepes,
T.&nbsp;Kocmi, A.&nbsp;Martins, M.&nbsp;Morishita, C.&nbsp;Monz, M.&nbsp;Nagata, T.&nbsp;Nakazawa,
M.&nbsp;Negri, A.&nbsp;Névéol, M.&nbsp;Neves, M.&nbsp;Popel, M.&nbsp;Turchi, and
M.&nbsp;Zampieri, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2022, pp. 1–45.

</span>
</li>
<li id="bib.bib546" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[546]</span>
<span class="ltx_bibblock">
N.&nbsp;Goyal, C.&nbsp;Gao, V.&nbsp;Chaudhary, P.&nbsp;Chen, G.&nbsp;Wenzek, D.&nbsp;Ju, S.&nbsp;Krishnan,
M.&nbsp;Ranzato, F.&nbsp;Guzmán, and A.&nbsp;Fan, “The flores-101 evaluation
benchmark for low-resource and multilingual machine translation,”
<em id="bib.bib546.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, vol.&nbsp;10, pp. 522–538, 2022.

</span>
</li>
<li id="bib.bib547" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[547]</span>
<span class="ltx_bibblock">
R.&nbsp;Bawden, E.&nbsp;Bilinski, T.&nbsp;Lavergne, and S.&nbsp;Rosset, “Diabla: a corpus of
bilingual spontaneous written dialogues for machine translation,”
<em id="bib.bib547.1.1" class="ltx_emph ltx_font_italic">Lang. Resour. Evaluation</em>, vol.&nbsp;55, no.&nbsp;3, pp. 635–660, 2021.

</span>
</li>
<li id="bib.bib548" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[548]</span>
<span class="ltx_bibblock">
R.&nbsp;Nallapati, B.&nbsp;Zhou, C.&nbsp;N. dos Santos, Ç.&nbsp;Gülçehre, and
B.&nbsp;Xiang, “Abstractive text summarization using sequence-to-sequence rnns
and beyond,” in <em id="bib.bib548.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th SIGNLL Conference on
Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August
11-12, 2016</em>, Y.&nbsp;Goldberg and S.&nbsp;Riezler, Eds.&nbsp;&nbsp;&nbsp;ACL, 2016, pp. 280–290.

</span>
</li>
<li id="bib.bib549" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[549]</span>
<span class="ltx_bibblock">
S.&nbsp;Narayan, S.&nbsp;B. Cohen, and M.&nbsp;Lapata, “Don’t give me the details, just the
summary! topic-aware convolutional neural networks for extreme
summarization,” in <em id="bib.bib549.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2018, pp. 1797–1807.

</span>
</li>
<li id="bib.bib550" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[550]</span>
<span class="ltx_bibblock">
F.&nbsp;Ladhak, E.&nbsp;Durmus, C.&nbsp;Cardie, and K.&nbsp;Mckeown, “Wikilingua: A new benchmark
dataset for cross-lingual abstractive summarization,” in <em id="bib.bib550.1.1" class="ltx_emph ltx_font_italic">Findings of
the Association for Computational Linguistics: EMNLP 2020</em>, 2020, pp.
4034–4048.

</span>
</li>
<li id="bib.bib551" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[551]</span>
<span class="ltx_bibblock">
S.&nbsp;Moon, P.&nbsp;Shah, A.&nbsp;Kumar, and R.&nbsp;Subba, “Opendialkg: Explainable
conversational reasoning with attention-based walks over knowledge graphs,”
in <em id="bib.bib551.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2019, pp. 845–854.

</span>
</li>
<li id="bib.bib552" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[552]</span>
<span class="ltx_bibblock">
Y.&nbsp;Lai, C.&nbsp;Li, Y.&nbsp;Wang, T.&nbsp;Zhang, R.&nbsp;Zhong, L.&nbsp;Zettlemoyer, S.&nbsp;W. Yih,
D.&nbsp;Fried, S.&nbsp;I. Wang, and T.&nbsp;Yu, “DS-1000: A natural and reliable
benchmark for data science code generation,” <em id="bib.bib552.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2211.11501, 2022.

</span>
</li>
<li id="bib.bib553" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[553]</span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, S.&nbsp;Zhou, D.&nbsp;Fried, and G.&nbsp;Neubig, “Execution-based evaluation for
open-domain code generation,” <em id="bib.bib553.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.10481, 2022.

</span>
</li>
<li id="bib.bib554" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[554]</span>
<span class="ltx_bibblock">
T.&nbsp;Kwiatkowski, J.&nbsp;Palomaki, O.&nbsp;Redfield, M.&nbsp;Collins, A.&nbsp;P. Parikh, C.&nbsp;Alberti,
D.&nbsp;Epstein, I.&nbsp;Polosukhin, J.&nbsp;Devlin, K.&nbsp;Lee, K.&nbsp;Toutanova, L.&nbsp;Jones,
M.&nbsp;Kelcey, M.&nbsp;Chang, A.&nbsp;M. Dai, J.&nbsp;Uszkoreit, Q.&nbsp;Le, and S.&nbsp;Petrov, “Natural
questions: a benchmark for question answering research,” <em id="bib.bib554.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc.
Comput. Linguistics</em>, pp. 452–466, 2019.

</span>
</li>
<li id="bib.bib555" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[555]</span>
<span class="ltx_bibblock">
P.&nbsp;Clark, I.&nbsp;Cowhey, O.&nbsp;Etzioni, T.&nbsp;Khot, A.&nbsp;Sabharwal, C.&nbsp;Schoenick, and
O.&nbsp;Tafjord, “Think you have solved question answering? try arc, the AI2
reasoning challenge,” <em id="bib.bib555.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1803.05457, 2018.

</span>
</li>
<li id="bib.bib556" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[556]</span>
<span class="ltx_bibblock">
S.&nbsp;Lin, J.&nbsp;Hilton, and O.&nbsp;Evans, “Truthfulqa: Measuring how models mimic human
falsehoods,” in <em id="bib.bib556.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022</em>, 2022, pp. 3214–3252.

</span>
</li>
<li id="bib.bib557" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[557]</span>
<span class="ltx_bibblock">
J.&nbsp;Berant, A.&nbsp;Chou, R.&nbsp;Frostig, and P.&nbsp;Liang, “Semantic parsing on freebase
from question-answer pairs,” in <em id="bib.bib557.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October
2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a
Special Interest Group of the ACL</em>, 2013, pp. 1533–1544.

</span>
</li>
<li id="bib.bib558" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[558]</span>
<span class="ltx_bibblock">
M.&nbsp;Joshi, E.&nbsp;Choi, D.&nbsp;S. Weld, and L.&nbsp;Zettlemoyer, “Triviaqa: A large scale
distantly supervised challenge dataset for reading comprehension,” in
<em id="bib.bib558.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4,
Volume 1: Long Papers</em>, 2017, pp. 1601–1611.

</span>
</li>
<li id="bib.bib559" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[559]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bisk, R.&nbsp;Zellers, R.&nbsp;L. Bras, J.&nbsp;Gao, and Y.&nbsp;Choi, “PIQA: reasoning about
physical commonsense in natural language,” in <em id="bib.bib559.1.1" class="ltx_emph ltx_font_italic">The Thirty-Fourth AAAI
Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Conference, IAAI 2020,
The Tenth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 2020, pp.
7432–7439.

</span>
</li>
<li id="bib.bib560" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[560]</span>
<span class="ltx_bibblock">
M.&nbsp;Dubey, D.&nbsp;Banerjee, A.&nbsp;Abdelkawi, and J.&nbsp;Lehmann, “Lc-quad 2.0: A large
dataset for complex question answering over wikidata and dbpedia,” in
<em id="bib.bib560.1.1" class="ltx_emph ltx_font_italic">The Semantic Web - ISWC 2019 - 18th International Semantic Web
Conference, Auckland, New Zealand, October 26-30, 2019, Proceedings, Part
II</em>, 2019, pp. 69–78.

</span>
</li>
<li id="bib.bib561" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[561]</span>
<span class="ltx_bibblock">
Y.&nbsp;Gu, S.&nbsp;Kase, M.&nbsp;Vanni, B.&nbsp;M. Sadler, P.&nbsp;Liang, X.&nbsp;Yan, and Y.&nbsp;Su, “Beyond
I.I.D.: three levels of generalization for question answering on knowledge
bases,” in <em id="bib.bib561.1.1" class="ltx_emph ltx_font_italic">WWW ’21: The Web Conference 2021, Virtual Event /
Ljubljana, Slovenia, April 19-23, 2021</em>, 2021, pp. 3477–3488.

</span>
</li>
<li id="bib.bib562" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[562]</span>
<span class="ltx_bibblock">
S.&nbsp;Cao, J.&nbsp;Shi, L.&nbsp;Pan, L.&nbsp;Nie, Y.&nbsp;Xiang, L.&nbsp;Hou, J.&nbsp;Li, B.&nbsp;He, and H.&nbsp;Zhang,
“KQA pro: A dataset with explicit compositional programs for complex
question answering over knowledge base,” in <em id="bib.bib562.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, 2022, pp.
6101–6119.

</span>
</li>
<li id="bib.bib563" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[563]</span>
<span class="ltx_bibblock">
X.&nbsp;Hu, X.&nbsp;Wu, Y.&nbsp;Shu, and Y.&nbsp;Qu, “Logical form generation via multi-task
learning for complex question answering over knowledge bases,” in
<em id="bib.bib563.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on Computational
Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17,
2022</em>, 2022, pp. 1687–1696.

</span>
</li>
<li id="bib.bib564" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[564]</span>
<span class="ltx_bibblock">
S.&nbsp;Longpre, Y.&nbsp;Lu, and J.&nbsp;Daiber, “MKQA: A linguistically diverse
benchmark for multilingual open domain question answering,” <em id="bib.bib564.1.1" class="ltx_emph ltx_font_italic">Trans.
Assoc. Comput. Linguistics</em>, vol.&nbsp;9, pp. 1389–1406, 2021.

</span>
</li>
<li id="bib.bib565" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[565]</span>
<span class="ltx_bibblock">
T.&nbsp;Saikh, T.&nbsp;Ghosal, A.&nbsp;Mittal, A.&nbsp;Ekbal, and P.&nbsp;Bhattacharyya, “Scienceqa: a
novel resource for question answering on scholarly articles,” <em id="bib.bib565.1.1" class="ltx_emph ltx_font_italic">Int. J.
Digit. Libr.</em>, vol.&nbsp;23, no.&nbsp;3, pp. 289–301, 2022.

</span>
</li>
<li id="bib.bib566" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[566]</span>
<span class="ltx_bibblock">
T.&nbsp;Mihaylov, P.&nbsp;Clark, T.&nbsp;Khot, and A.&nbsp;Sabharwal, “Can a suit of armor conduct
electricity? A new dataset for open book question answering,” in
<em id="bib.bib566.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>, 2018,
pp. 2381–2391.

</span>
</li>
<li id="bib.bib567" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[567]</span>
<span class="ltx_bibblock">
T.&nbsp;Nguyen, M.&nbsp;Rosenberg, X.&nbsp;Song, J.&nbsp;Gao, S.&nbsp;Tiwary, R.&nbsp;Majumder, and L.&nbsp;Deng,
“MS MARCO: A human generated machine reading comprehension dataset,”
in <em id="bib.bib567.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Cognitive Computation: Integrating
neural and symbolic approaches 2016 co-located with the 30th Annual
Conference on Neural Information Processing Systems (NIPS 2016), Barcelona,
Spain, December 9, 2016</em>, 2016.

</span>
</li>
<li id="bib.bib568" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[568]</span>
<span class="ltx_bibblock">
T.&nbsp;Khot, P.&nbsp;Clark, M.&nbsp;Guerquin, P.&nbsp;Jansen, and A.&nbsp;Sabharwal, “QASC: A
dataset for question answering via sentence composition,” in <em id="bib.bib568.1.1" class="ltx_emph ltx_font_italic">The
Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial Intelligence Conference,
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 2020, pp.
8082–8090.

</span>
</li>
<li id="bib.bib569" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[569]</span>
<span class="ltx_bibblock">
P.&nbsp;Rajpurkar, J.&nbsp;Zhang, K.&nbsp;Lopyrev, and P.&nbsp;Liang, “Squad: 100, 000+ questions
for machine comprehension of text,” in <em id="bib.bib569.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016</em>, 2016, pp. 2383–2392.

</span>
</li>
<li id="bib.bib570" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[570]</span>
<span class="ltx_bibblock">
A.&nbsp;H. Miller, A.&nbsp;Fisch, J.&nbsp;Dodge, A.&nbsp;Karimi, A.&nbsp;Bordes, and J.&nbsp;Weston,
“Key-value memory networks for directly reading documents,” in
<em id="bib.bib570.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016</em>,
2016, pp. 1400–1409.

</span>
</li>
<li id="bib.bib571" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[571]</span>
<span class="ltx_bibblock">
B.&nbsp;Goodrich, V.&nbsp;Rao, P.&nbsp;J. Liu, and M.&nbsp;Saleh, “Assessing the factual accuracy
of generated text,” in <em id="bib.bib571.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019,
Anchorage, AK, USA, August 4-8, 2019</em>, 2019, pp. 166–175.

</span>
</li>
<li id="bib.bib572" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[572]</span>
<span class="ltx_bibblock">
K.&nbsp;Toutanova and D.&nbsp;Chen, “Observed versus latent features for knowledge base
and text inference,” in <em id="bib.bib572.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Continuous
Vector Space Models and their Compositionality, CVSC 2015, Beijing, China,
July 26-31, 2015</em>, 2015, pp. 57–66.

</span>
</li>
<li id="bib.bib573" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[573]</span>
<span class="ltx_bibblock">
K.&nbsp;D. Bollacker, C.&nbsp;Evans, P.&nbsp;K. Paritosh, T.&nbsp;Sturge, and J.&nbsp;Taylor,
“Freebase: a collaboratively created graph database for structuring human
knowledge,” in <em id="bib.bib573.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGMOD International
Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June
10-12, 2008</em>, 2008, pp. 1247–1250.

</span>
</li>
<li id="bib.bib574" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[574]</span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, P.&nbsp;Minervini, P.&nbsp;Stenetorp, and S.&nbsp;Riedel, “Convolutional 2d
knowledge graph embeddings,” in <em id="bib.bib574.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative
Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI
Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018</em>, 2018, pp. 1811–1818.

</span>
</li>
<li id="bib.bib575" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[575]</span>
<span class="ltx_bibblock">
G.&nbsp;A. Miller, “Wordnet: A lexical database for english,” <em id="bib.bib575.1.1" class="ltx_emph ltx_font_italic">Commun.
ACM</em>, pp. 39–41, 1995.

</span>
</li>
<li id="bib.bib576" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[576]</span>
<span class="ltx_bibblock">
F.&nbsp;Petroni, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, P.&nbsp;S.&nbsp;H. Lewis, A.&nbsp;Bakhtin, Y.&nbsp;Wu,
and A.&nbsp;H. Miller, “Language models as knowledge bases?” in
<em id="bib.bib576.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,
2019</em>, 2019, pp. 2463–2473.

</span>
</li>
<li id="bib.bib577" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[577]</span>
<span class="ltx_bibblock">
F.&nbsp;Mahdisoltani, J.&nbsp;Biega, and F.&nbsp;M. Suchanek, “YAGO3: A knowledge base
from multilingual wikipedias,” in <em id="bib.bib577.1.1" class="ltx_emph ltx_font_italic">Seventh Biennial Conference on
Innovative Data Systems Research, CIDR 2015, Asilomar, CA, USA, January
4-7, 2015, Online Proceedings</em>, 2015.

</span>
</li>
<li id="bib.bib578" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[578]</span>
<span class="ltx_bibblock">
F.&nbsp;M. Suchanek, G.&nbsp;Kasneci, and G.&nbsp;Weikum, “Yago: a core of semantic
knowledge,” in <em id="bib.bib578.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th International Conference on
World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007</em>, 2007,
pp. 697–706.

</span>
</li>
<li id="bib.bib579" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[579]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yang, P.&nbsp;Qi, S.&nbsp;Zhang, Y.&nbsp;Bengio, W.&nbsp;W. Cohen, R.&nbsp;Salakhutdinov, and C.&nbsp;D.
Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question
answering,” in <em id="bib.bib579.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing, Brussels, Belgium, October 31 - November 4,
2018</em>.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2018, pp. 2369–2380.

</span>
</li>
<li id="bib.bib580" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[580]</span>
<span class="ltx_bibblock">
C.&nbsp;Clark, K.&nbsp;Lee, M.&nbsp;Chang, T.&nbsp;Kwiatkowski, M.&nbsp;Collins, and K.&nbsp;Toutanova,
“Boolq: Exploring the surprising difficulty of natural yes/no questions,”
in <em id="bib.bib580.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
Short Papers)</em>, J.&nbsp;Burstein, C.&nbsp;Doran, and T.&nbsp;Solorio, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2019, pp.
2924–2936.

</span>
</li>
<li id="bib.bib581" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[581]</span>
<span class="ltx_bibblock">
M.&nbsp;Sap, H.&nbsp;Rashkin, D.&nbsp;Chen, R.&nbsp;L. Bras, and Y.&nbsp;Choi, “Socialiqa: Commonsense
reasoning about social interactions,” <em id="bib.bib581.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1904.09728,
2019.

</span>
</li>
<li id="bib.bib582" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[582]</span>
<span class="ltx_bibblock">
R.&nbsp;Zellers, A.&nbsp;Holtzman, Y.&nbsp;Bisk, A.&nbsp;Farhadi, and Y.&nbsp;Choi, “Hellaswag: Can a
machine really finish your sentence?” in <em id="bib.bib582.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019,
Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>,
A.&nbsp;Korhonen, D.&nbsp;R. Traum, and L.&nbsp;Màrquez, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2019, pp.
4791–4800.

</span>
</li>
<li id="bib.bib583" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[583]</span>
<span class="ltx_bibblock">
K.&nbsp;Sakaguchi, R.&nbsp;L. Bras, C.&nbsp;Bhagavatula, and Y.&nbsp;Choi, “Winogrande: An
adversarial winograd schema challenge at scale,” in <em id="bib.bib583.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.&nbsp;&nbsp;&nbsp;AAAI Press, 2020, pp. 8732–8740.

</span>
</li>
<li id="bib.bib584" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[584]</span>
<span class="ltx_bibblock">
M.&nbsp;Roemmele, C.&nbsp;A. Bejan, and A.&nbsp;S. Gordon, “Choice of plausible alternatives:
An evaluation of commonsense causal reasoning,” in <em id="bib.bib584.1.1" class="ltx_emph ltx_font_italic">Logical
Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring
Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23,
2011</em>.&nbsp;&nbsp;&nbsp;AAAI, 2011.

</span>
</li>
<li id="bib.bib585" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[585]</span>
<span class="ltx_bibblock">
K.&nbsp;Sakaguchi, C.&nbsp;Bhagavatula, R.&nbsp;L. Bras, N.&nbsp;Tandon, P.&nbsp;Clark, and Y.&nbsp;Choi,
“proscript: Partially ordered scripts generation,” in <em id="bib.bib585.1.1" class="ltx_emph ltx_font_italic">Findings of the
Association for Computational Linguistics: EMNLP 2021, Virtual Event /
Punta Cana, Dominican Republic, 16-20 November, 2021</em>, M.&nbsp;Moens, X.&nbsp;Huang,
L.&nbsp;Specia, and S.&nbsp;W. Yih, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 2138–2149.

</span>
</li>
<li id="bib.bib586" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[586]</span>
<span class="ltx_bibblock">
B.&nbsp;Dalvi, L.&nbsp;Huang, N.&nbsp;Tandon, W.&nbsp;Yih, and P.&nbsp;Clark, “Tracking state changes
in procedural text: a challenge dataset and models for process paragraph
comprehension,” in <em id="bib.bib586.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June
1-6, 2018, Volume 1 (Long Papers)</em>, M.&nbsp;A. Walker, H.&nbsp;Ji, and A.&nbsp;Stent,
Eds.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2018, pp. 1595–1604.

</span>
</li>
<li id="bib.bib587" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[587]</span>
<span class="ltx_bibblock">
S.&nbsp;Saha, P.&nbsp;Yadav, L.&nbsp;Bauer, and M.&nbsp;Bansal, “Explagraphs: An explanation graph
generation task for structured commonsense reasoning,” in <em id="bib.bib587.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
2021</em>, M.&nbsp;Moens, X.&nbsp;Huang, L.&nbsp;Specia, and S.&nbsp;W. Yih, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp.
7716–7740.

</span>
</li>
<li id="bib.bib588" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[588]</span>
<span class="ltx_bibblock">
O.&nbsp;Tafjord, B.&nbsp;Dalvi, and P.&nbsp;Clark, “Proofwriter: Generating implications,
proofs, and abductive statements over natural language,” in <em id="bib.bib588.1.1" class="ltx_emph ltx_font_italic">Findings
of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online
Event, August 1-6, 2021</em>, ser. Findings of ACL, C.&nbsp;Zong, F.&nbsp;Xia, W.&nbsp;Li, and
R.&nbsp;Navigli, Eds., vol. ACL/IJCNLP 2021.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 3621–3634.

</span>
</li>
<li id="bib.bib589" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[589]</span>
<span class="ltx_bibblock">
B.&nbsp;Dalvi, P.&nbsp;Jansen, O.&nbsp;Tafjord, Z.&nbsp;Xie, H.&nbsp;Smith, L.&nbsp;Pipatanangkura, and
P.&nbsp;Clark, “Explaining answers with entailment trees,” in <em id="bib.bib589.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
2021</em>, M.&nbsp;Moens, X.&nbsp;Huang, L.&nbsp;Specia, and S.&nbsp;W. Yih, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp.
7358–7370.

</span>
</li>
<li id="bib.bib590" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[590]</span>
<span class="ltx_bibblock">
A.&nbsp;Saparov and H.&nbsp;He, “Language models are greedy reasoners: A systematic
formal analysis of chain-of-thought,” <em id="bib.bib590.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.01240,
2022.

</span>
</li>
<li id="bib.bib591" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[591]</span>
<span class="ltx_bibblock">
C.&nbsp;Anil, Y.&nbsp;Wu, A.&nbsp;Andreassen, A.&nbsp;Lewkowycz, V.&nbsp;Misra, V.&nbsp;V. Ramasesh,
A.&nbsp;Slone, G.&nbsp;Gur-Ari, E.&nbsp;Dyer, and B.&nbsp;Neyshabur, “Exploring length
generalization in large language models,” <em id="bib.bib591.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2207.04901,
2022.

</span>
</li>
<li id="bib.bib592" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[592]</span>
<span class="ltx_bibblock">
A.&nbsp;Patel, S.&nbsp;Bhattamishra, and N.&nbsp;Goyal, “Are NLP models really able to
solve simple math word problems?” in <em id="bib.bib592.1.1" class="ltx_emph ltx_font_italic">NAACL-HLT</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp.
2080–2094.

</span>
</li>
<li id="bib.bib593" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[593]</span>
<span class="ltx_bibblock">
S.&nbsp;Roy and D.&nbsp;Roth, “Solving general arithmetic word problems,” in
<em id="bib.bib593.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015</em>,
L.&nbsp;Màrquez, C.&nbsp;Callison-Burch, J.&nbsp;Su, D.&nbsp;Pighin, and Y.&nbsp;Marton,
Eds.&nbsp;&nbsp;&nbsp;The Association for Computational
Linguistics, 2015, pp. 1743–1752.

</span>
</li>
<li id="bib.bib594" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[594]</span>
<span class="ltx_bibblock">
A.&nbsp;Amini, S.&nbsp;Gabriel, S.&nbsp;Lin, R.&nbsp;Koncel-Kedziorski, Y.&nbsp;Choi, and
H.&nbsp;Hajishirzi, “Mathqa: Towards interpretable math word problem solving with
operation-based formalisms,” in <em id="bib.bib594.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June
2-7, 2019, Volume 1 (Long and Short Papers)</em>, J.&nbsp;Burstein, C.&nbsp;Doran, and
T.&nbsp;Solorio, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2019, pp. 2357–2367.

</span>
</li>
<li id="bib.bib595" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[595]</span>
<span class="ltx_bibblock">
W.&nbsp;Ling, D.&nbsp;Yogatama, C.&nbsp;Dyer, and P.&nbsp;Blunsom, “Program induction by rationale
generation: Learning to solve and explain algebraic word problems,” in
<em id="bib.bib595.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4,
Volume 1: Long Papers</em>, R.&nbsp;Barzilay and M.&nbsp;Kan, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2017, pp.
158–167.

</span>
</li>
<li id="bib.bib596" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[596]</span>
<span class="ltx_bibblock">
R.&nbsp;Koncel-Kedziorski, S.&nbsp;Roy, A.&nbsp;Amini, N.&nbsp;Kushman, and H.&nbsp;Hajishirzi, “Mawps:
A math word problem repository,” in <em id="bib.bib596.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 conference
of the north american chapter of the association for computational
linguistics: human language technologies</em>, 2016, pp. 1152–1157.

</span>
</li>
<li id="bib.bib597" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[597]</span>
<span class="ltx_bibblock">
D.&nbsp;Dua, Y.&nbsp;Wang, P.&nbsp;Dasigi, G.&nbsp;Stanovsky, S.&nbsp;Singh, and M.&nbsp;Gardner, “DROP:
A reading comprehension benchmark requiring discrete reasoning over
paragraphs,” in <em id="bib.bib597.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers)</em>, 2019, pp. 2368–2378.

</span>
</li>
<li id="bib.bib598" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[598]</span>
<span class="ltx_bibblock">
S.&nbsp;Welleck, J.&nbsp;Liu, R.&nbsp;L. Bras, H.&nbsp;Hajishirzi, Y.&nbsp;Choi, and K.&nbsp;Cho,
“Naturalproofs: Mathematical theorem proving in natural language,” in
<em id="bib.bib598.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on
Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
2021, virtual</em>, J.&nbsp;Vanschoren and S.&nbsp;Yeung, Eds., 2021.

</span>
</li>
<li id="bib.bib599" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[599]</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, W.&nbsp;Li, J.&nbsp;M. Han, and Y.&nbsp;Wu, “Lisa: Language models of isabelle
proofs,” in <em id="bib.bib599.1.1" class="ltx_emph ltx_font_italic">6th Conference on Artificial Intelligence and Theorem
Proving</em>, 2021, pp. 378–392.

</span>
</li>
<li id="bib.bib600" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[600]</span>
<span class="ltx_bibblock">
K.&nbsp;Zheng, J.&nbsp;M. Han, and S.&nbsp;Polu, “minif2f: a cross-system benchmark for
formal olympiad-level mathematics,” in <em id="bib.bib600.1.1" class="ltx_emph ltx_font_italic">The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event, April
25-29, 2022</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib601" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[601]</span>
<span class="ltx_bibblock">
Z.&nbsp;Azerbayev, B.&nbsp;Piotrowski, H.&nbsp;Schoelkopf, E.&nbsp;W. Ayers, D.&nbsp;Radev, and
J.&nbsp;Avigad, “Proofnet: Autoformalizing and formally proving
undergraduate-level mathematics,” <em id="bib.bib601.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.12433, 2023.

</span>
</li>
<li id="bib.bib602" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[602]</span>
<span class="ltx_bibblock">
J.&nbsp;Li, X.&nbsp;Cheng, W.&nbsp;X. Zhao, J.&nbsp;Nie, and J.&nbsp;Wen, “Halueval: A large-scale
hallucination evaluation benchmark for large language models,” <em id="bib.bib602.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.11747, 2023.

</span>
</li>
<li id="bib.bib603" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[603]</span>
<span class="ltx_bibblock">
N.&nbsp;Nangia, C.&nbsp;Vania, R.&nbsp;Bhalerao, and S.&nbsp;R. Bowman, “Crows-pairs: A
challenge dataset for measuring social biases in masked language models,” in
<em id="bib.bib603.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, 2020, pp.
1953–1967.

</span>
</li>
<li id="bib.bib604" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[604]</span>
<span class="ltx_bibblock">
R.&nbsp;Rudinger, J.&nbsp;Naradowsky, B.&nbsp;Leonard, and B.&nbsp;V. Durme, “Gender bias in
coreference resolution,” in <em id="bib.bib604.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June
1-6, 2018, Volume 2 (Short Papers)</em>, 2018, pp. 8–14.

</span>
</li>
<li id="bib.bib605" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[605]</span>
<span class="ltx_bibblock">
S.&nbsp;Gehman, S.&nbsp;Gururangan, M.&nbsp;Sap, Y.&nbsp;Choi, and N.&nbsp;A. Smith,
“Realtoxicityprompts: Evaluating neural toxic degeneration in language
models,” in <em id="bib.bib605.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020, Online Event, 16-20 November 2020</em>, ser. Findings of ACL,
T.&nbsp;Cohn, Y.&nbsp;He, and Y.&nbsp;Liu, Eds., vol. EMNLP 2020.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020, pp.
3356–3369.

</span>
</li>
<li id="bib.bib606" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[606]</span>
<span class="ltx_bibblock">
X.&nbsp;Puig, K.&nbsp;Ra, M.&nbsp;Boben, J.&nbsp;Li, T.&nbsp;Wang, S.&nbsp;Fidler, and A.&nbsp;Torralba,
“Virtualhome: Simulating household activities via programs,” in
<em id="bib.bib606.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.&nbsp;&nbsp;&nbsp;Computer Vision
Foundation / IEEE Computer Society, 2018, pp. 8494–8502.

</span>
</li>
<li id="bib.bib607" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[607]</span>
<span class="ltx_bibblock">
S.&nbsp;Srivastava, C.&nbsp;Li, M.&nbsp;Lingelbach, R.&nbsp;Martín-Martín, F.&nbsp;Xia,
K.&nbsp;E. Vainio, Z.&nbsp;Lian, C.&nbsp;Gokmen, S.&nbsp;Buch, C.&nbsp;K. Liu, S.&nbsp;Savarese, H.&nbsp;Gweon,
J.&nbsp;Wu, and L.&nbsp;Fei-Fei, “BEHAVIOR: benchmark for everyday household
activities in virtual, interactive, and ecological environments,” in
<em id="bib.bib607.1.1" class="ltx_emph ltx_font_italic">CoRL</em>, ser. Proceedings of Machine Learning Research, vol. 164.&nbsp;&nbsp;&nbsp;PMLR, 2021, pp. 477–490.

</span>
</li>
<li id="bib.bib608" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[608]</span>
<span class="ltx_bibblock">
M.&nbsp;Shridhar, J.&nbsp;Thomason, D.&nbsp;Gordon, Y.&nbsp;Bisk, W.&nbsp;Han, R.&nbsp;Mottaghi,
L.&nbsp;Zettlemoyer, and D.&nbsp;Fox, “ALFRED: A benchmark for interpreting
grounded instructions for everyday tasks,” in <em id="bib.bib608.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.&nbsp;&nbsp;&nbsp;Computer Vision Foundation / IEEE, 2020, pp.
10 737–10 746.

</span>
</li>
<li id="bib.bib609" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[609]</span>
<span class="ltx_bibblock">
M.&nbsp;Shridhar, X.&nbsp;Yuan, M.&nbsp;Côté, Y.&nbsp;Bisk, A.&nbsp;Trischler, and M.&nbsp;J.
Hausknecht, “Alfworld: Aligning text and embodied environments for
interactive learning,” in <em id="bib.bib609.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2021.

</span>
</li>
<li id="bib.bib610" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[610]</span>
<span class="ltx_bibblock">
S.&nbsp;Yao, H.&nbsp;Chen, J.&nbsp;Yang, and K.&nbsp;Narasimhan, “Webshop: Towards scalable
real-world web interaction with grounded language agents,” in
<em id="bib.bib610.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib611" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[611]</span>
<span class="ltx_bibblock">
X.&nbsp;Deng, Y.&nbsp;Gu, B.&nbsp;Zheng, S.&nbsp;Chen, S.&nbsp;Stevens, B.&nbsp;Wang, H.&nbsp;Sun, and Y.&nbsp;Su,
“Mind2web: Towards a generalist agent for the web,” <em id="bib.bib611.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.06070, 2023.

</span>
</li>
<li id="bib.bib612" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[612]</span>
<span class="ltx_bibblock">
W.&nbsp;H. Guss, B.&nbsp;Houghton, N.&nbsp;Topin, P.&nbsp;Wang, C.&nbsp;Codel, M.&nbsp;Veloso, and
R.&nbsp;Salakhutdinov, “Minerl: A large-scale dataset of minecraft
demonstrations,” in <em id="bib.bib612.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China,
August 10-16, 2019</em>, S.&nbsp;Kraus, Ed.&nbsp;&nbsp;&nbsp;ijcai.org, 2019, pp. 2442–2448.

</span>
</li>
<li id="bib.bib613" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[613]</span>
<span class="ltx_bibblock">
L.&nbsp;Fan, G.&nbsp;Wang, Y.&nbsp;Jiang, A.&nbsp;Mandlekar, Y.&nbsp;Yang, H.&nbsp;Zhu, A.&nbsp;Tang, D.&nbsp;Huang,
Y.&nbsp;Zhu, and A.&nbsp;Anandkumar, “Minedojo: Building open-ended embodied agents
with internet-scale knowledge,” in <em id="bib.bib613.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib614" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[614]</span>
<span class="ltx_bibblock">
P.&nbsp;Lu, L.&nbsp;Qiu, K.&nbsp;Chang, Y.&nbsp;N. Wu, S.&nbsp;Zhu, T.&nbsp;Rajpurohit, P.&nbsp;Clark, and
A.&nbsp;Kalyan, “Dynamic prompt learning via policy gradient for semi-structured
mathematical reasoning,” <em id="bib.bib614.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.14610, 2022.

</span>
</li>
<li id="bib.bib615" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[615]</span>
<span class="ltx_bibblock">
B.&nbsp;Zhang, K.&nbsp;Zhou, X.&nbsp;Wei, W.&nbsp;X. Zhao, J.&nbsp;Sha, S.&nbsp;Wang, and J.&nbsp;rong Wen,
“Evaluating and improving tool-augmented computation-intensive math
reasoning,” <em id="bib.bib615.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.02408, 2023.

</span>
</li>
<li id="bib.bib616" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[616]</span>
<span class="ltx_bibblock">
R.&nbsp;Yang, L.&nbsp;Song, Y.&nbsp;Li, S.&nbsp;Zhao, Y.&nbsp;Ge, X.&nbsp;Li, and Y.&nbsp;Shan, “Gpt4tools:
Teaching large language model to use tools via self-instruction,”
<em id="bib.bib616.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.18752, 2023.

</span>
</li>
<li id="bib.bib617" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[617]</span>
<span class="ltx_bibblock">
S.&nbsp;G. Patil, T.&nbsp;Zhang, X.&nbsp;Wang, and J.&nbsp;E. Gonzalez, “Gorilla: Large language
model connected with massive apis,” <em id="bib.bib617.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.15334, 2023.

</span>
</li>
<li id="bib.bib618" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[618]</span>
<span class="ltx_bibblock">
W.&nbsp;Yih, M.&nbsp;Richardson, C.&nbsp;Meek, M.&nbsp;Chang, and J.&nbsp;Suh, “The value of semantic
parse labeling for knowledge base question answering,” in <em id="bib.bib618.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics,
ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short
Papers</em>.&nbsp;&nbsp;&nbsp;The Association for Computer
Linguistics, 2016.

</span>
</li>
<li id="bib.bib619" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[619]</span>
<span class="ltx_bibblock">
H.&nbsp;Puerto, G.&nbsp;G. Sahin, and I.&nbsp;Gurevych, “Metaqa: Combining expert agents for
multi-skill question answering,” in <em id="bib.bib619.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference
of the European Chapter of the Association for Computational Linguistics,
EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>, A.&nbsp;Vlachos and
I.&nbsp;Augenstein, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2023, pp. 3548–3562.

</span>
</li>
<li id="bib.bib620" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[620]</span>
<span class="ltx_bibblock">
P.&nbsp;Pasupat and P.&nbsp;Liang, “Compositional semantic parsing on semi-structured
tables,” in <em id="bib.bib620.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long
Papers</em>.&nbsp;&nbsp;&nbsp;The Association for Computer
Linguistics, 2015, pp. 1470–1480.

</span>
</li>
<li id="bib.bib621" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[621]</span>
<span class="ltx_bibblock">
V.&nbsp;Zhong, C.&nbsp;Xiong, and R.&nbsp;Socher, “Seq2sql: Generating structured queries
from natural language using reinforcement learning,” <em id="bib.bib621.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/1709.00103, 2017.

</span>
</li>
<li id="bib.bib622" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[622]</span>
<span class="ltx_bibblock">
W.&nbsp;Chen, H.&nbsp;Wang, J.&nbsp;Chen, Y.&nbsp;Zhang, H.&nbsp;Wang, S.&nbsp;Li, X.&nbsp;Zhou, and W.&nbsp;Y. Wang,
“Tabfact: A large-scale dataset for table-based fact verification,” in
<em id="bib.bib622.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020</em>.&nbsp;&nbsp;&nbsp;OpenReview.net, 2020.

</span>
</li>
<li id="bib.bib623" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[623]</span>
<span class="ltx_bibblock">
T.&nbsp;Yu, R.&nbsp;Zhang, K.&nbsp;Yang, M.&nbsp;Yasunaga, D.&nbsp;Wang, Z.&nbsp;Li, J.&nbsp;Ma, I.&nbsp;Li, Q.&nbsp;Yao,
S.&nbsp;Roman, Z.&nbsp;Zhang, and D.&nbsp;R. Radev, “Spider: A large-scale human-labeled
dataset for complex and cross-domain semantic parsing and text-to-sql task,”
in <em id="bib.bib623.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>,
E.&nbsp;Riloff, D.&nbsp;Chiang, J.&nbsp;Hockenmaier, and J.&nbsp;Tsujii, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2018, pp.
3911–3921.

</span>
</li>
<li id="bib.bib624" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[624]</span>
<span class="ltx_bibblock">
D.&nbsp;Bahdanau, K.&nbsp;Cho, and Y.&nbsp;Bengio, “Neural machine translation by jointly
learning to align and translate,” in <em id="bib.bib624.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2015.

</span>
</li>
<li id="bib.bib625" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[625]</span>
<span class="ltx_bibblock">
K.&nbsp;Papineni, S.&nbsp;Roukos, T.&nbsp;Ward, and W.&nbsp;Zhu, “Bleu: a method for automatic
evaluation of machine translation,” in <em id="bib.bib625.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual
Meeting of the Association for Computational Linguistics, July 6-12, 2002,
Philadelphia, PA, USA</em>.&nbsp;&nbsp;&nbsp;ACL, 2002,
pp. 311–318.

</span>
</li>
<li id="bib.bib626" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[626]</span>
<span class="ltx_bibblock">
C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in
<em id="bib.bib626.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, Jul. 2004, pp. 74–81.

</span>
</li>
<li id="bib.bib627" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[627]</span>
<span class="ltx_bibblock">
W.&nbsp;Jiao, W.&nbsp;Wang, J.-t. Huang, X.&nbsp;Wang, and Z.&nbsp;Tu, “Is chatgpt a good
translator? a preliminary study,” <em id="bib.bib627.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.08745</em>,
2023.

</span>
</li>
<li id="bib.bib628" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[628]</span>
<span class="ltx_bibblock">
T.&nbsp;Zhang, F.&nbsp;Ladhak, E.&nbsp;Durmus, P.&nbsp;Liang, K.&nbsp;R. McKeown, and T.&nbsp;B. Hashimoto,
“Benchmarking large language models for news summarization,” <em id="bib.bib628.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2301.13848, 2023.

</span>
</li>
<li id="bib.bib629" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[629]</span>
<span class="ltx_bibblock">
T.&nbsp;Goyal, J.&nbsp;J. Li, and G.&nbsp;Durrett, “News summarization and evaluation in the
era of GPT-3,” <em id="bib.bib629.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.12356, 2022.

</span>
</li>
<li id="bib.bib630" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[630]</span>
<span class="ltx_bibblock">
S.&nbsp;Gehrmann, E.&nbsp;Clark, and T.&nbsp;Sellam, “Repairing the cracked foundation: A
survey of obstacles in evaluation practices for generated text,”
<em id="bib.bib630.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2202.06935, 2022.

</span>
</li>
<li id="bib.bib631" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[631]</span>
<span class="ltx_bibblock">
J.&nbsp;Wang, Y.&nbsp;Liang, F.&nbsp;Meng, H.&nbsp;Shi, Z.&nbsp;Li, J.&nbsp;Xu, J.&nbsp;Qu, and J.&nbsp;Zhou, “Is
chatgpt a good NLG evaluator? A preliminary study,” <em id="bib.bib631.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2303.04048, 2023.

</span>
</li>
<li id="bib.bib632" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[632]</span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, D.&nbsp;Iter, Y.&nbsp;Xu, S.&nbsp;Wang, R.&nbsp;Xu, and C.&nbsp;Zhu, “G-eval: NLG evaluation
using GPT-4 with better human alignment,” <em id="bib.bib632.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2303.16634, 2023.

</span>
</li>
<li id="bib.bib633" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[633]</span>
<span class="ltx_bibblock">
K.&nbsp;Yang, Y.&nbsp;Tian, N.&nbsp;Peng, and D.&nbsp;Klein, “Re3: Generating longer stories with
recursive reprompting and revision,” in <em id="bib.bib633.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022
Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,
Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, Y.&nbsp;Goldberg,
Z.&nbsp;Kozareva, and Y.&nbsp;Zhang, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 4393–4479.

</span>
</li>
<li id="bib.bib634" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[634]</span>
<span class="ltx_bibblock">
W.&nbsp;Zhou, Y.&nbsp;E. Jiang, P.&nbsp;Cui, T.&nbsp;Wang, Z.&nbsp;Xiao, Y.&nbsp;Hou, R.&nbsp;Cotterell, and
M.&nbsp;Sachan, “Recurrentgpt: Interactive generation of (arbitrarily) long
text,” <em id="bib.bib634.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13304, 2023.

</span>
</li>
<li id="bib.bib635" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[635]</span>
<span class="ltx_bibblock">
S.&nbsp;Gulwani, O.&nbsp;Polozov, and R.&nbsp;Singh, “Program synthesis,” <em id="bib.bib635.1.1" class="ltx_emph ltx_font_italic">Found.
Trends Program. Lang.</em>, vol.&nbsp;4, no. 1-2, pp. 1–119, 2017.

</span>
</li>
<li id="bib.bib636" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[636]</span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, Z.&nbsp;Chen, Y.&nbsp;Shen, M.&nbsp;Ding, J.&nbsp;B. Tenenbaum, and C.&nbsp;Gan, “Planning
with large language models for code generation,” 2023.

</span>
</li>
<li id="bib.bib637" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[637]</span>
<span class="ltx_bibblock">
M.&nbsp;Welsh, “The end of programming,” <em id="bib.bib637.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em>, vol.&nbsp;66, no.&nbsp;1, pp.
34–35, 2023.

</span>
</li>
<li id="bib.bib638" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[638]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bang, S.&nbsp;Cahyawijaya, N.&nbsp;Lee, W.&nbsp;Dai, D.&nbsp;Su, B.&nbsp;Wilie, H.&nbsp;Lovenia, Z.&nbsp;Ji,
T.&nbsp;Yu, W.&nbsp;Chung, Q.&nbsp;V. Do, Y.&nbsp;Xu, and P.&nbsp;Fung, “A multitask, multilingual,
multimodal evaluation of chatgpt on reasoning, hallucination, and
interactivity,” <em id="bib.bib638.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.04023, 2023.

</span>
</li>
<li id="bib.bib639" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[639]</span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, A.&nbsp;R. Fabbri, P.&nbsp;Liu, Y.&nbsp;Zhao, L.&nbsp;Nan, R.&nbsp;Han, S.&nbsp;Han, S.&nbsp;R. Joty,
C.&nbsp;Wu, C.&nbsp;Xiong, and D.&nbsp;Radev, “Revisiting the gold standard: Grounding
summarization evaluation with robust human evaluation,” <em id="bib.bib639.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2212.07981, 2022.

</span>
</li>
<li id="bib.bib640" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[640]</span>
<span class="ltx_bibblock">
A.&nbsp;R. Fabbri, W.&nbsp;Kryscinski, B.&nbsp;McCann, C.&nbsp;Xiong, R.&nbsp;Socher, and D.&nbsp;R. Radev,
“Summeval: Re-evaluating summarization evaluation,” <em id="bib.bib640.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc.
Comput. Linguistics</em>, vol.&nbsp;9, pp. 391–409, 2021.

</span>
</li>
<li id="bib.bib641" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[641]</span>
<span class="ltx_bibblock">
T.&nbsp;Tang, H.&nbsp;Lu, Y.&nbsp;E. Jiang, H.&nbsp;Huang, D.&nbsp;Zhang, W.&nbsp;X. Zhao, and F.&nbsp;Wei, “Not
all metrics are guilty: Improving NLG evaluation with LLM paraphrasing,”
<em id="bib.bib641.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.15067, 2023.

</span>
</li>
<li id="bib.bib642" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[642]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, X.&nbsp;Tang, W.&nbsp;X. Zhao, J.&nbsp;Wang, and J.&nbsp;Wen, “Rethinking the evaluation
for conversational recommendation in the era of large language models,”
<em id="bib.bib642.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13112, 2023.

</span>
</li>
<li id="bib.bib643" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[643]</span>
<span class="ltx_bibblock">
M.&nbsp;Gao, J.&nbsp;Ruan, R.&nbsp;Sun, X.&nbsp;Yin, S.&nbsp;Yang, and X.&nbsp;Wan, “Human-like
summarization evaluation with chatgpt,” <em id="bib.bib643.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.02554,
2023.

</span>
</li>
<li id="bib.bib644" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[644]</span>
<span class="ltx_bibblock">
Y.&nbsp;Ji, Y.&nbsp;Gong, Y.&nbsp;Peng, C.&nbsp;Ni, P.&nbsp;Sun, D.&nbsp;Pan, B.&nbsp;Ma, and X.&nbsp;Li, “Exploring
chatgpt’s ability to rank content: A preliminary study on consistency with
human preferences,” <em id="bib.bib644.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.07610, 2023.

</span>
</li>
<li id="bib.bib645" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[645]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, J.&nbsp;Ying, Y.&nbsp;Cao, X.&nbsp;Lv, Y.&nbsp;He, X.&nbsp;Wang, J.&nbsp;Yu, K.&nbsp;Zeng, Y.&nbsp;Xiao,
H.&nbsp;Lyu, J.&nbsp;Zhang, J.&nbsp;Li, and L.&nbsp;Hou, “Benchmarking foundation models with
language-model-as-an-examiner,” <em id="bib.bib645.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.04181, 2023.

</span>
</li>
<li id="bib.bib646" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[646]</span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, S.&nbsp;Feng, D.&nbsp;Wang, Y.&nbsp;Zhang, and H.&nbsp;Schütze, “Evaluate what you
can’t evaluate: Unassessable generated responses quality,” <em id="bib.bib646.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.14658, 2023.

</span>
</li>
<li id="bib.bib647" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[647]</span>
<span class="ltx_bibblock">
P.&nbsp;Wang, L.&nbsp;Li, L.&nbsp;Chen, D.&nbsp;Zhu, B.&nbsp;Lin, Y.&nbsp;Cao, Q.&nbsp;Liu, T.&nbsp;Liu, and Z.&nbsp;Sui,
“Large language models are not fair evaluators,” <em id="bib.bib647.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.17926, 2023.

</span>
</li>
<li id="bib.bib648" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[648]</span>
<span class="ltx_bibblock">
J.&nbsp;Ye, X.&nbsp;Chen, N.&nbsp;Xu, C.&nbsp;Zu, Z.&nbsp;Shao, S.&nbsp;Liu, Y.&nbsp;Cui, Z.&nbsp;Zhou, C.&nbsp;Gong,
Y.&nbsp;Shen, J.&nbsp;Zhou, S.&nbsp;Chen, T.&nbsp;Gui, Q.&nbsp;Zhang, and X.&nbsp;Huang, “A comprehensive
capability analysis of gpt-3 and gpt-3.5 series models,” <em id="bib.bib648.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2303.10420</em>, 2023.

</span>
</li>
<li id="bib.bib649" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[649]</span>
<span class="ltx_bibblock">
M.&nbsp;McCloskey and N.&nbsp;J. Cohen, “Catastrophic interference in connectionist
networks: The sequential learning problem,” in <em id="bib.bib649.1.1" class="ltx_emph ltx_font_italic">Psychology of learning
and motivation</em>, 1989, pp. 109–165.

</span>
</li>
<li id="bib.bib650" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[650]</span>
<span class="ltx_bibblock">
R.&nbsp;Kemker, M.&nbsp;McClure, A.&nbsp;Abitino, T.&nbsp;L. Hayes, and C.&nbsp;Kanan, “Measuring
catastrophic forgetting in neural networks,” in <em id="bib.bib650.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the
30th innovative Applications of Artificial Intelligence (IAAI-18), and the
8th AAAI Symposium on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018</em>, 2018, pp.
3390–3398.

</span>
</li>
<li id="bib.bib651" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[651]</span>
<span class="ltx_bibblock">
T.&nbsp;Xie, C.&nbsp;H. Wu, P.&nbsp;Shi, R.&nbsp;Zhong, T.&nbsp;Scholak, M.&nbsp;Yasunaga, C.&nbsp;Wu, M.&nbsp;Zhong,
P.&nbsp;Yin, S.&nbsp;I. Wang, V.&nbsp;Zhong, B.&nbsp;Wang, C.&nbsp;Li, C.&nbsp;Boyle, A.&nbsp;Ni, Z.&nbsp;Yao,
D.&nbsp;Radev, C.&nbsp;Xiong, L.&nbsp;Kong, R.&nbsp;Zhang, N.&nbsp;A. Smith, L.&nbsp;Zettlemoyer, and
T.&nbsp;Yu, “Unifiedskg: Unifying and multi-tasking structured knowledge
grounding with text-to-text language models,” in <em id="bib.bib651.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022,
pp. 602–631.

</span>
</li>
<li id="bib.bib652" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[652]</span>
<span class="ltx_bibblock">
A.&nbsp;Roberts, C.&nbsp;Raffel, and N.&nbsp;Shazeer, “How much knowledge can you pack into
the parameters of a language model?” in <em id="bib.bib652.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020</em>, 2020, pp. 5418–5426.

</span>
</li>
<li id="bib.bib653" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[653]</span>
<span class="ltx_bibblock">
G.&nbsp;Izacard, P.&nbsp;S.&nbsp;H. Lewis, M.&nbsp;Lomeli, L.&nbsp;Hosseini, F.&nbsp;Petroni, T.&nbsp;Schick,
J.&nbsp;Dwivedi-Yu, A.&nbsp;Joulin, S.&nbsp;Riedel, and E.&nbsp;Grave, “Few-shot learning with
retrieval augmented language models,” <em id="bib.bib653.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2208.03299,
2022.

</span>
</li>
<li id="bib.bib654" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[654]</span>
<span class="ltx_bibblock">
K.&nbsp;Guu, K.&nbsp;Lee, Z.&nbsp;Tung, P.&nbsp;Pasupat, and M.&nbsp;Chang, “Retrieval augmented
language model pre-training,” in <em id="bib.bib654.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event</em>,
2020, pp. 3929–3938.

</span>
</li>
<li id="bib.bib655" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[655]</span>
<span class="ltx_bibblock">
P.&nbsp;S.&nbsp;H. Lewis, E.&nbsp;Perez, A.&nbsp;Piktus, F.&nbsp;Petroni, V.&nbsp;Karpukhin, N.&nbsp;Goyal,
H.&nbsp;Küttler, M.&nbsp;Lewis, W.&nbsp;Yih, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, and
D.&nbsp;Kiela, “Retrieval-augmented generation for knowledge-intensive NLP
tasks,” in <em id="bib.bib655.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual</em>, 2020.

</span>
</li>
<li id="bib.bib656" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[656]</span>
<span class="ltx_bibblock">
Y.&nbsp;Lan, G.&nbsp;He, J.&nbsp;Jiang, J.&nbsp;Jiang, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “Complex knowledge
base question answering: A survey,” <em id="bib.bib656.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2108.06688,
2021.

</span>
</li>
<li id="bib.bib657" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[657]</span>
<span class="ltx_bibblock">
S.&nbsp;Borgeaud, A.&nbsp;Mensch, J.&nbsp;Hoffmann, T.&nbsp;Cai, E.&nbsp;Rutherford, K.&nbsp;Millican,
G.&nbsp;van&nbsp;den Driessche, J.&nbsp;Lespiau, B.&nbsp;Damoc, A.&nbsp;Clark, D.&nbsp;de&nbsp;Las&nbsp;Casas,
A.&nbsp;Guy, J.&nbsp;Menick, R.&nbsp;Ring, T.&nbsp;Hennigan, S.&nbsp;Huang, L.&nbsp;Maggiore, C.&nbsp;Jones,
A.&nbsp;Cassirer, A.&nbsp;Brock, M.&nbsp;Paganini, G.&nbsp;Irving, O.&nbsp;Vinyals, S.&nbsp;Osindero,
K.&nbsp;Simonyan, J.&nbsp;W. Rae, E.&nbsp;Elsen, and L.&nbsp;Sifre, “Improving language models
by retrieving from trillions of tokens,” in <em id="bib.bib657.1.1" class="ltx_emph ltx_font_italic">International Conference
on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA</em>, ser. Proceedings of Machine Learning Research, K.&nbsp;Chaudhuri,
S.&nbsp;Jegelka, L.&nbsp;Song, C.&nbsp;Szepesvári, G.&nbsp;Niu, and S.&nbsp;Sabato, Eds., vol.
162.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 2206–2240.

</span>
</li>
<li id="bib.bib658" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[658]</span>
<span class="ltx_bibblock">
S.&nbsp;Xu, L.&nbsp;Pang, H.&nbsp;Shen, X.&nbsp;Cheng, and T.-S. Chua, “Search-in-the-chain:
Towards accurate, credible and traceable large language models for
knowledge-intensive tasks,” <em id="bib.bib658.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.14732, 2023.

</span>
</li>
<li id="bib.bib659" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[659]</span>
<span class="ltx_bibblock">
B.&nbsp;Peng, M.&nbsp;Galley, P.&nbsp;He, H.&nbsp;Cheng, Y.&nbsp;Xie, Y.&nbsp;Hu, Q.&nbsp;Huang, L.&nbsp;Liden, Z.&nbsp;Yu,
W.&nbsp;Chen, and J.&nbsp;Gao, “Check your facts and try again: Improving large
language models with external knowledge and automated feedback,”
<em id="bib.bib659.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.12813, 2023.

</span>
</li>
<li id="bib.bib660" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[660]</span>
<span class="ltx_bibblock">
Z.&nbsp;Jiang, F.&nbsp;F. Xu, L.&nbsp;Gao, Z.&nbsp;Sun, Q.&nbsp;Liu, J.&nbsp;Dwivedi-Yu, Y.&nbsp;Yang,
J.&nbsp;Callan, and G.&nbsp;Neubig, “Active retrieval augmented generation,”
<em id="bib.bib660.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.06983, 2023.

</span>
</li>
<li id="bib.bib661" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[661]</span>
<span class="ltx_bibblock">
L.&nbsp;Huang, W.&nbsp;Yu, W.&nbsp;Ma, W.&nbsp;Zhong, Z.&nbsp;Feng, H.&nbsp;Wang, Q.&nbsp;Chen, W.&nbsp;Peng, X.&nbsp;Feng,
B.&nbsp;Qin, and T.&nbsp;Liu, “A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions,” <em id="bib.bib661.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2311.05232, 2023.

</span>
</li>
<li id="bib.bib662" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[662]</span>
<span class="ltx_bibblock">
Y.&nbsp;Li, Y.&nbsp;Du, K.&nbsp;Zhou, J.&nbsp;Wang, W.&nbsp;X. Zhao, and J.&nbsp;Wen, “Evaluating object
hallucination in large vision-language models,” <em id="bib.bib662.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.10355, 2023.

</span>
</li>
<li id="bib.bib663" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[663]</span>
<span class="ltx_bibblock">
S.&nbsp;Kadavath, T.&nbsp;Conerly, A.&nbsp;Askell, T.&nbsp;J. Henighan, D.&nbsp;Drain, E.&nbsp;Perez,
N.&nbsp;Schiefer, Z.&nbsp;Dodds, N.&nbsp;DasSarma, E.&nbsp;Tran-Johnson, S.&nbsp;Johnston,
S.&nbsp;El-Showk, A.&nbsp;Jones, N.&nbsp;Elhage, T.&nbsp;Hume, A.&nbsp;Chen, Y.&nbsp;Bai, S.&nbsp;Bowman,
S.&nbsp;Fort, D.&nbsp;Ganguli, D.&nbsp;Hernandez, J.&nbsp;Jacobson, J.&nbsp;Kernion, S.&nbsp;Kravec,
L.&nbsp;Lovitt, K.&nbsp;Ndousse, C.&nbsp;Olsson, S.&nbsp;Ringer, D.&nbsp;Amodei, T.&nbsp;B. Brown,
J.&nbsp;Clark, N.&nbsp;Joseph, B.&nbsp;Mann, S.&nbsp;McCandlish, C.&nbsp;Olah, and J.&nbsp;Kaplan,
“Language models (mostly) know what they know,” <em id="bib.bib663.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2207.05221, 2022.

</span>
</li>
<li id="bib.bib664" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[664]</span>
<span class="ltx_bibblock">
P.&nbsp;Manakul, A.&nbsp;Liusie, and M.&nbsp;J.&nbsp;F. Gales, “Selfcheckgpt: Zero-resource
black-box hallucination detection for generative large language models,”
<em id="bib.bib664.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/2305.06983, 2023.

</span>
</li>
<li id="bib.bib665" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[665]</span>
<span class="ltx_bibblock">
S.&nbsp;Agarwal, I.&nbsp;Akkaya, V.&nbsp;Balcom, M.&nbsp;Bavarian, G.&nbsp;Bernadett-Shapiro,
G.&nbsp;Brockman, M.&nbsp;Brundage, J.&nbsp;Chan, F.&nbsp;Chantzis, N.&nbsp;Deutsch, B.&nbsp;Eastman,
A.&nbsp;Eleti, N.&nbsp;Felix, S.&nbsp;P. Fishman, I.&nbsp;Fulford, C.&nbsp;Gibson, J.&nbsp;Gross,
M.&nbsp;Heaton, J.&nbsp;Hilton, X.&nbsp;Hu, S.&nbsp;Jain, H.&nbsp;Jin, L.&nbsp;Kilpatrick, C.&nbsp;Kim,
M.&nbsp;Kolhede, A.&nbsp;Mayne, P.&nbsp;McMillan, D.&nbsp;Medina, J.&nbsp;Menick, A.&nbsp;Mishchenko,
A.&nbsp;Nair, R.&nbsp;Nayak, A.&nbsp;Neelakantan, R.&nbsp;Nuttall, J.&nbsp;Parish, A.&nbsp;T. Passos,
A.&nbsp;Perelman, F.&nbsp;de&nbsp;Avila Belbute&nbsp;Peres, V.&nbsp;Pong, J.&nbsp;Schulman, E.&nbsp;Sigler,
N.&nbsp;Staudacher, N.&nbsp;Turley, J.&nbsp;Tworek, R.&nbsp;Greene, A.&nbsp;Vijayvergiya, C.&nbsp;Voss,
J.&nbsp;Weng, M.&nbsp;Wiethoff, S.&nbsp;Yoo, K.&nbsp;Yu, W.&nbsp;Zaremba, S.&nbsp;Zhao, W.&nbsp;Zhuk, and
B.&nbsp;Zoph, “Chatgpt plugins,” <em id="bib.bib665.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, March 2023.

</span>
</li>
<li id="bib.bib666" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[666]</span>
<span class="ltx_bibblock">
A.&nbsp;Lazaridou, E.&nbsp;Gribovskaya, W.&nbsp;Stokowiec, and N.&nbsp;Grigorev,
“Internet-augmented language models through few-shot prompting for
open-domain question answering,” <em id="bib.bib666.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2203.05115, 2022.

</span>
</li>
<li id="bib.bib667" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[667]</span>
<span class="ltx_bibblock">
H.&nbsp;Qian, Y.&nbsp;Zhu, Z.&nbsp;Dou, H.&nbsp;Gu, X.&nbsp;Zhang, Z.&nbsp;Liu, R.&nbsp;Lai, Z.&nbsp;Cao, J.&nbsp;Nie, and
J.&nbsp;Wen, “Webbrain: Learning to generate factually correct articles for
queries by grounding on large web corpus,” <em id="bib.bib667.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.04358,
2023.

</span>
</li>
<li id="bib.bib668" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[668]</span>
<span class="ltx_bibblock">
J.&nbsp;Liu, J.&nbsp;Jin, Z.&nbsp;Wang, J.&nbsp;Cheng, Z.&nbsp;Dou, and J.&nbsp;Wen, “RETA-LLM: A
retrieval-augmented large language model toolkit,” <em id="bib.bib668.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.05212, 2023.

</span>
</li>
<li id="bib.bib669" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[669]</span>
<span class="ltx_bibblock">
D.&nbsp;Dai, L.&nbsp;Dong, Y.&nbsp;Hao, Z.&nbsp;Sui, B.&nbsp;Chang, and F.&nbsp;Wei, “Knowledge neurons in
pretrained transformers,” in <em id="bib.bib669.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022</em>, S.&nbsp;Muresan, P.&nbsp;Nakov, and
A.&nbsp;Villavicencio, Eds.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2022, pp. 8493–8502.

</span>
</li>
<li id="bib.bib670" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[670]</span>
<span class="ltx_bibblock">
K.&nbsp;Meng, D.&nbsp;Bau, A.&nbsp;J. Andonian, and Y.&nbsp;Belinkov, “Locating and editing
factual associations in gpt,” in <em id="bib.bib670.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib671" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[671]</span>
<span class="ltx_bibblock">
M.&nbsp;Geva, R.&nbsp;Schuster, J.&nbsp;Berant, and O.&nbsp;Levy, “Transformer feed-forward layers
are key-value memories,” in <em id="bib.bib671.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11 November, 2021</em>, M.&nbsp;Moens, X.&nbsp;Huang,
L.&nbsp;Specia, and S.&nbsp;W. Yih, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 5484–5495.

</span>
</li>
<li id="bib.bib672" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[672]</span>
<span class="ltx_bibblock">
Y.&nbsp;Yao, P.&nbsp;Wang, B.&nbsp;Tian, S.&nbsp;Cheng, Z.&nbsp;Li, S.&nbsp;Deng, H.&nbsp;Chen, and N.&nbsp;Zhang,
“Editing large language models: Problems, methods, and opportunities,”
<em id="bib.bib672.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13172, 2023.

</span>
</li>
<li id="bib.bib673" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[673]</span>
<span class="ltx_bibblock">
P.&nbsp;Wang, N.&nbsp;Zhang, X.&nbsp;Xie, Y.&nbsp;Yao, B.&nbsp;Tian, M.&nbsp;Wang, Z.&nbsp;Xi, S.&nbsp;Cheng, K.&nbsp;Liu,
G.&nbsp;Zheng, and H.&nbsp;Chen, “Easyedit: An easy-to-use knowledge editing framework
for large language models,” <em id="bib.bib673.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.07269, 2023.

</span>
</li>
<li id="bib.bib674" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[674]</span>
<span class="ltx_bibblock">
Z.&nbsp;Shao, Y.&nbsp;Gong, Y.&nbsp;Shen, M.&nbsp;Huang, N.&nbsp;Duan, and W.&nbsp;Chen, “Synthetic
prompting: Generating chain-of-thought demonstrations for large language
models,” <em id="bib.bib674.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.00618, 2023.

</span>
</li>
<li id="bib.bib675" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[675]</span>
<span class="ltx_bibblock">
Sifatkaur, M.&nbsp;Singh, V.&nbsp;S. B, and N.&nbsp;Malviya, “Mind meets machine: Unravelling
gpt-4’s cognitive psychology,” <em id="bib.bib675.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.11436, 2023.

</span>
</li>
<li id="bib.bib676" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[676]</span>
<span class="ltx_bibblock">
M.&nbsp;I. Nye, A.&nbsp;J. Andreassen, G.&nbsp;Gur-Ari, H.&nbsp;Michalewski, J.&nbsp;Austin,
D.&nbsp;Bieber, D.&nbsp;Dohan, A.&nbsp;Lewkowycz, M.&nbsp;Bosma, D.&nbsp;Luan, C.&nbsp;Sutton, and
A.&nbsp;Odena, “Show your work: Scratchpads for intermediate computation with
language models,” <em id="bib.bib676.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2112.00114, 2021.

</span>
</li>
<li id="bib.bib677" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[677]</span>
<span class="ltx_bibblock">
J.&nbsp;Qian, H.&nbsp;Wang, Z.&nbsp;Li, S.&nbsp;Li, and X.&nbsp;Yan, “Limitations of language models in
arithmetic and symbolic induction,” <em id="bib.bib677.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2208.05051, 2022.

</span>
</li>
<li id="bib.bib678" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[678]</span>
<span class="ltx_bibblock">
W.&nbsp;X. Zhao, K.&nbsp;Zhou, Z.&nbsp;Gong, B.&nbsp;Zhang, Y.&nbsp;Zhou, J.&nbsp;Sha, Z.&nbsp;Chen, S.&nbsp;Wang,
C.&nbsp;Liu, and J.&nbsp;Wen, “Jiuzhang: A chinese pre-trained language model for
mathematical problem understanding,” in <em id="bib.bib678.1.1" class="ltx_emph ltx_font_italic">KDD ’22: The 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC,
USA, August 14 - 18, 2022</em>, A.&nbsp;Zhang and H.&nbsp;Rangwala, Eds.&nbsp;&nbsp;&nbsp;ACM, 2022, pp. 4571–4581.

</span>
</li>
<li id="bib.bib679" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[679]</span>
<span class="ltx_bibblock">
Q.&nbsp;Wang, C.&nbsp;Kaliszyk, and J.&nbsp;Urban, “First experiments with neural translation
of informal to formal mathematics,” in <em id="bib.bib679.1.1" class="ltx_emph ltx_font_italic">Intelligent Computer
Mathematics - 11th International Conference, CICM 2018, Hagenberg, Austria,
August 13-17, 2018, Proceedings</em>, ser. Lecture Notes in Computer Science,
F.&nbsp;Rabe, W.&nbsp;M. Farmer, G.&nbsp;O. Passmore, and A.&nbsp;Youssef, Eds., vol.
11006.&nbsp;&nbsp;&nbsp;Springer, 2018, pp. 255–270.

</span>
</li>
<li id="bib.bib680" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[680]</span>
<span class="ltx_bibblock">
S.&nbsp;Polu and I.&nbsp;Sutskever, “Generative language modeling for automated theorem
proving,” <em id="bib.bib680.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2009.03393, 2020.

</span>
</li>
<li id="bib.bib681" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[681]</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, W.&nbsp;Li, S.&nbsp;Tworkowski, K.&nbsp;Czechowski, T.&nbsp;Odrzygózdz,
P.&nbsp;Milos, Y.&nbsp;Wu, and M.&nbsp;Jamnik, “Thor: Wielding hammers to integrate
language models and automated theorem provers,” <em id="bib.bib681.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2205.10893, 2022.

</span>
</li>
<li id="bib.bib682" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[682]</span>
<span class="ltx_bibblock">
S.&nbsp;Polu, J.&nbsp;M. Han, K.&nbsp;Zheng, M.&nbsp;Baksys, I.&nbsp;Babuschkin, and I.&nbsp;Sutskever,
“Formal mathematics statement curriculum learning,” <em id="bib.bib682.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2202.01344, 2022.

</span>
</li>
<li id="bib.bib683" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[683]</span>
<span class="ltx_bibblock">
Y.&nbsp;Wu, A.&nbsp;Q. Jiang, W.&nbsp;Li, M.&nbsp;N. Rabe, C.&nbsp;Staats, M.&nbsp;Jamnik, and C.&nbsp;Szegedy,
“Autoformalization with large language models,” <em id="bib.bib683.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2205.12615, 2022.

</span>
</li>
<li id="bib.bib684" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[684]</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, S.&nbsp;Welleck, J.&nbsp;P. Zhou, W.&nbsp;Li, J.&nbsp;Liu, M.&nbsp;Jamnik, T.&nbsp;Lacroix,
Y.&nbsp;Wu, and G.&nbsp;Lample, “Draft, sketch, and prove: Guiding formal theorem
provers with informal proofs,” <em id="bib.bib684.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.12283, 2022.

</span>
</li>
<li id="bib.bib685" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[685]</span>
<span class="ltx_bibblock">
A.&nbsp;Madaan, N.&nbsp;Tandon, P.&nbsp;Gupta, S.&nbsp;Hallinan, L.&nbsp;Gao, S.&nbsp;Wiegreffe, U.&nbsp;Alon,
N.&nbsp;Dziri, S.&nbsp;Prabhumoye, Y.&nbsp;Yang, S.&nbsp;Welleck, B.&nbsp;P. Majumder, S.&nbsp;Gupta,
A.&nbsp;Yazdanbakhsh, and P.&nbsp;Clark, “Self-refine: Iterative refinement with
self-feedback,” <em id="bib.bib685.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.17651, 2023.

</span>
</li>
<li id="bib.bib686" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[686]</span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, B.&nbsp;Labash, and A.&nbsp;Gopinath, “Reflexion: an autonomous agent with
dynamic memory and self-reflection,” <em id="bib.bib686.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.11366, 2023.

</span>
</li>
<li id="bib.bib687" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[687]</span>
<span class="ltx_bibblock">
Z.&nbsp;Gou, Z.&nbsp;Shao, Y.&nbsp;Gong, Y.&nbsp;Shen, Y.&nbsp;Yang, N.&nbsp;Duan, and W.&nbsp;Chen, “CRITIC:
large language models can self-correct with tool-interactive critiquing,”
<em id="bib.bib687.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.11738, 2023.

</span>
</li>
<li id="bib.bib688" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[688]</span>
<span class="ltx_bibblock">
J.&nbsp;Uesato, N.&nbsp;Kushman, R.&nbsp;Kumar, H.&nbsp;F. Song, N.&nbsp;Y. Siegel, L.&nbsp;Wang,
A.&nbsp;Creswell, G.&nbsp;Irving, and I.&nbsp;Higgins, “Solving math word problems with
process- and outcome-based feedback,” <em id="bib.bib688.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2211.14275,
2022.

</span>
</li>
<li id="bib.bib689" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[689]</span>
<span class="ltx_bibblock">
H.&nbsp;Lightman, V.&nbsp;Kosaraju, Y.&nbsp;Burda, H.&nbsp;Edwards, B.&nbsp;Baker, T.&nbsp;Lee, J.&nbsp;Leike,
J.&nbsp;Schulman, I.&nbsp;Sutskever, and K.&nbsp;Cobbe, “Let’s verify step by step,”
<em id="bib.bib689.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.20050, 2023.

</span>
</li>
<li id="bib.bib690" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[690]</span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Tan, W.&nbsp;Wang, and S.&nbsp;Huang, “How well do large language
models perform in arithmetic tasks?” <em id="bib.bib690.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.02015, 2023.

</span>
</li>
<li id="bib.bib691" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[691]</span>
<span class="ltx_bibblock">
X.&nbsp;Pi, Q.&nbsp;Liu, B.&nbsp;Chen, M.&nbsp;Ziyadi, Z.&nbsp;Lin, Q.&nbsp;Fu, Y.&nbsp;Gao, J.&nbsp;Lou, and W.&nbsp;Chen,
“Reasoning like program executors,” in <em id="bib.bib691.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022
Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,
Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, 2022, pp. 761–779.

</span>
</li>
<li id="bib.bib692" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[692]</span>
<span class="ltx_bibblock">
H.&nbsp;Zhou, A.&nbsp;Nova, H.&nbsp;Larochelle, A.&nbsp;C. Courville, B.&nbsp;Neyshabur, and H.&nbsp;Sedghi,
“Teaching algorithmic reasoning via in-context learning,” <em id="bib.bib692.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2211.09066, 2022.

</span>
</li>
<li id="bib.bib693" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[693]</span>
<span class="ltx_bibblock">
A.&nbsp;Parisi, Y.&nbsp;Zhao, and N.&nbsp;Fiedel, “TALM: tool augmented language models,”
<em id="bib.bib693.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2205.12255, 2022.

</span>
</li>
<li id="bib.bib694" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[694]</span>
<span class="ltx_bibblock">
W.&nbsp;Huang, P.&nbsp;Abbeel, D.&nbsp;Pathak, and I.&nbsp;Mordatch, “Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents,” in
<em id="bib.bib694.1.1" class="ltx_emph ltx_font_italic">ICML</em>, ser. Proceedings of Machine Learning Research, vol. 162.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 9118–9147.

</span>
</li>
<li id="bib.bib695" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[695]</span>
<span class="ltx_bibblock">
T.&nbsp;Carta, C.&nbsp;Romac, T.&nbsp;Wolf, S.&nbsp;Lamprier, O.&nbsp;Sigaud, and P.&nbsp;Oudeyer,
“Grounding large language models in interactive environments with online
reinforcement learning,” <em id="bib.bib695.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.02662, 2023.

</span>
</li>
<li id="bib.bib696" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[696]</span>
<span class="ltx_bibblock">
X.&nbsp;Zhu, Y.&nbsp;Chen, H.&nbsp;Tian, C.&nbsp;Tao, W.&nbsp;Su, C.&nbsp;Yang, G.&nbsp;Huang, B.&nbsp;Li, L.&nbsp;Lu,
X.&nbsp;Wang, Y.&nbsp;Qiao, Z.&nbsp;Zhang, and J.&nbsp;Dai, “Ghost in the minecraft: Generally
capable agents for open-world environments via large language models with
text-based knowledge and memory,” <em id="bib.bib696.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.17144, 2023.

</span>
</li>
<li id="bib.bib697" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[697]</span>
<span class="ltx_bibblock">
G.&nbsp;Wang, Y.&nbsp;Xie, Y.&nbsp;Jiang, A.&nbsp;Mandlekar, C.&nbsp;Xiao, Y.&nbsp;Zhu, L.&nbsp;Fan, and
A.&nbsp;Anandkumar, “Voyager: An open-ended embodied agent with large language
models,” <em id="bib.bib697.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.16291, 2023.

</span>
</li>
<li id="bib.bib698" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[698]</span>
<span class="ltx_bibblock">
M.&nbsp;Ahn, A.&nbsp;Brohan, N.&nbsp;Brown, Y.&nbsp;Chebotar, O.&nbsp;Cortes, B.&nbsp;David, C.&nbsp;Finn,
K.&nbsp;Gopalakrishnan, K.&nbsp;Hausman, A.&nbsp;Herzog, D.&nbsp;Ho, J.&nbsp;Hsu, J.&nbsp;Ibarz, B.&nbsp;Ichter,
A.&nbsp;Irpan, E.&nbsp;Jang, R.&nbsp;J. Ruano, K.&nbsp;Jeffrey, S.&nbsp;Jesmonth, N.&nbsp;J. Joshi,
R.&nbsp;Julian, D.&nbsp;Kalashnikov, Y.&nbsp;Kuang, K.&nbsp;Lee, S.&nbsp;Levine, Y.&nbsp;Lu, L.&nbsp;Luu,
C.&nbsp;Parada, P.&nbsp;Pastor, J.&nbsp;Quiambao, K.&nbsp;Rao, J.&nbsp;Rettinghouse, D.&nbsp;Reyes,
P.&nbsp;Sermanet, N.&nbsp;Sievers, C.&nbsp;Tan, A.&nbsp;Toshev, V.&nbsp;Vanhoucke, F.&nbsp;Xia, T.&nbsp;Xiao,
P.&nbsp;Xu, S.&nbsp;Xu, and M.&nbsp;Yan, “Do as I can, not as I say: Grounding language
in robotic affordances,” <em id="bib.bib698.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2204.01691, 2022.

</span>
</li>
<li id="bib.bib699" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[699]</span>
<span class="ltx_bibblock">
J.&nbsp;Liang, W.&nbsp;Huang, F.&nbsp;Xia, P.&nbsp;Xu, K.&nbsp;Hausman, B.&nbsp;Ichter, P.&nbsp;Florence, and
A.&nbsp;Zeng, “Code as policies: Language model programs for embodied control,”
<em id="bib.bib699.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.07753, 2022.

</span>
</li>
<li id="bib.bib700" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[700]</span>
<span class="ltx_bibblock">
Y.&nbsp;Fu, H.&nbsp;Peng, T.&nbsp;Khot, and M.&nbsp;Lapata, “Improving language model negotiation
with self-play and in-context learning from AI feedback,” <em id="bib.bib700.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.10142, 2023.

</span>
</li>
<li id="bib.bib701" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[701]</span>
<span class="ltx_bibblock">
N.&nbsp;Mehta, M.&nbsp;Teruel, P.&nbsp;F. Sanz, X.&nbsp;Deng, A.&nbsp;H. Awadallah, and J.&nbsp;Kiseleva,
“Improving grounded language understanding in a collaborative environment by
interacting with agents through help feedback,” <em id="bib.bib701.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2304.10750, 2023.

</span>
</li>
<li id="bib.bib702" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[702]</span>
<span class="ltx_bibblock">
S.&nbsp;G. Patil, T.&nbsp;Zhang, X.&nbsp;Wang, and J.&nbsp;E. Gonzalez, “Gorilla: Large language
model connected with massive apis,” <em id="bib.bib702.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.15334, 2023.

</span>
</li>
<li id="bib.bib703" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[703]</span>
<span class="ltx_bibblock">
S.&nbsp;Hao, T.&nbsp;Liu, Z.&nbsp;Wang, and Z.&nbsp;Hu, “Toolkengpt: Augmenting frozen language
models with massive tools via tool embeddings,” <em id="bib.bib703.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.11554, 2023.

</span>
</li>
<li id="bib.bib704" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[704]</span>
<span class="ltx_bibblock">
Y.&nbsp;Liang, C.&nbsp;Wu, T.&nbsp;Song, W.&nbsp;Wu, Y.&nbsp;Xia, Y.&nbsp;Liu, Y.&nbsp;Ou, S.&nbsp;Lu, L.&nbsp;Ji, S.&nbsp;Mao,
Y.&nbsp;Wang, L.&nbsp;Shou, M.&nbsp;Gong, and N.&nbsp;Duan, “Taskmatrix.ai: Completing tasks by
connecting foundation models with millions of apis,” <em id="bib.bib704.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2303.16434, 2023.

</span>
</li>
<li id="bib.bib705" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[705]</span>
<span class="ltx_bibblock">
T.&nbsp;Cai, X.&nbsp;Wang, T.&nbsp;Ma, X.&nbsp;Chen, and D.&nbsp;Zhou, “Large language models as tool
makers,” <em id="bib.bib705.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.17126, 2023.

</span>
</li>
<li id="bib.bib706" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[706]</span>
<span class="ltx_bibblock">
J.&nbsp;Huang, S.&nbsp;S. Gu, L.&nbsp;Hou, Y.&nbsp;Wu, X.&nbsp;Wang, H.&nbsp;Yu, and J.&nbsp;Han, “Large language
models can self-improve,” <em id="bib.bib706.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.11610, 2022.

</span>
</li>
<li id="bib.bib707" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[707]</span>
<span class="ltx_bibblock">
E.&nbsp;Beeching, C.&nbsp;Fourrier, N.&nbsp;Habib, S.&nbsp;Han, N.&nbsp;Lambert, N.&nbsp;Rajani,
O.&nbsp;Sanseviero, L.&nbsp;Tunstall, and T.&nbsp;Wolf, “Open llm leaderboard,”
<a target="_blank" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>, 2023.

</span>
</li>
<li id="bib.bib708" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[708]</span>
<span class="ltx_bibblock">
W.&nbsp;Zhong, R.&nbsp;Cui, Y.&nbsp;Guo, Y.&nbsp;Liang, S.&nbsp;Lu, Y.&nbsp;Wang, A.&nbsp;Saied, W.&nbsp;Chen, and
N.&nbsp;Duan, “Agieval: A human-centric benchmark for evaluating foundation
models,” <em id="bib.bib708.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.06364, 2023.

</span>
</li>
<li id="bib.bib709" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[709]</span>
<span class="ltx_bibblock">
H.&nbsp;Zeng, “Measuring massive multitask chinese understanding,” <em id="bib.bib709.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2304.12986, 2023.

</span>
</li>
<li id="bib.bib710" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[710]</span>
<span class="ltx_bibblock">
C.&nbsp;Liu, R.&nbsp;Jin, Y.&nbsp;Ren, L.&nbsp;Yu, T.&nbsp;Dong, X.&nbsp;Peng, S.&nbsp;Zhang, J.&nbsp;Peng, P.&nbsp;Zhang,
Q.&nbsp;Lyu, X.&nbsp;Su, Q.&nbsp;Liu, and D.&nbsp;Xiong, “M3KE: A massive multi-level
multi-subject knowledge evaluation benchmark for chinese large language
models,” <em id="bib.bib710.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.10263, 2023.

</span>
</li>
<li id="bib.bib711" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[711]</span>
<span class="ltx_bibblock">
Y.&nbsp;Huang, Y.&nbsp;Bai, Z.&nbsp;Zhu, J.&nbsp;Zhang, J.&nbsp;Zhang, T.&nbsp;Su, J.&nbsp;Liu, C.&nbsp;Lv, Y.&nbsp;Zhang,
J.&nbsp;Lei, Y.&nbsp;Fu, M.&nbsp;Sun, and J.&nbsp;He, “C-eval: A multi-level multi-discipline
chinese evaluation suite for foundation models,” <em id="bib.bib711.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.08322, 2023.

</span>
</li>
<li id="bib.bib712" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[712]</span>
<span class="ltx_bibblock">
Z.&nbsp;Gu, X.&nbsp;Zhu, H.&nbsp;Ye, L.&nbsp;Zhang, J.&nbsp;Wang, S.&nbsp;Jiang, Z.&nbsp;Xiong, Z.&nbsp;Li, Q.&nbsp;He,
R.&nbsp;Xu, W.&nbsp;Huang, W.&nbsp;Zheng, H.&nbsp;Feng, and Y.&nbsp;Xiao, “Xiezhi: An ever-updating
benchmark for holistic domain knowledge evaluation,” <em id="bib.bib712.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.05783, 2023.

</span>
</li>
<li id="bib.bib713" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[713]</span>
<span class="ltx_bibblock">
O.&nbsp;Contributors, “Opencompass: A universal evaluation platform for foundation
models,” <a target="_blank" href="https://github.com/InternLM/OpenCompass" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/InternLM/OpenCompass</a>, 2023.

</span>
</li>
<li id="bib.bib714" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[714]</span>
<span class="ltx_bibblock">
Y.&nbsp;Fu, L.&nbsp;Ou, M.&nbsp;Chen, Y.&nbsp;Wan, H.&nbsp;Peng, and T.&nbsp;Khot, “Chain-of-thought hub:
A continuous effort to measure large language models’ reasoning
performance,” <em id="bib.bib714.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.17306, 2023.

</span>
</li>
<li id="bib.bib715" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[715]</span>
<span class="ltx_bibblock">
J.&nbsp;Yu, X.&nbsp;Wang, S.&nbsp;Tu, S.&nbsp;Cao, D.&nbsp;Zhang-li, X.&nbsp;Lv, H.&nbsp;Peng, Z.&nbsp;Yao, X.&nbsp;Zhang,
H.&nbsp;Li, C.&nbsp;Li, Z.&nbsp;Zhang, Y.&nbsp;Bai, Y.&nbsp;Liu, A.&nbsp;Xin, N.&nbsp;Lin, K.&nbsp;Yun, L.&nbsp;Gong,
J.&nbsp;Chen, Z.&nbsp;Wu, Y.&nbsp;Qi, W.&nbsp;Li, Y.&nbsp;Guan, K.&nbsp;Zeng, J.&nbsp;Qi, H.&nbsp;Jin, J.&nbsp;Liu, Y.&nbsp;Gu,
Y.&nbsp;Yao, N.&nbsp;Ding, L.&nbsp;Hou, Z.&nbsp;Liu, B.&nbsp;Xu, J.&nbsp;Tang, and J.&nbsp;Li, “Kola: Carefully
benchmarking world knowledge of large language models,” <em id="bib.bib715.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.09296, 2023.

</span>
</li>
<li id="bib.bib716" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[716]</span>
<span class="ltx_bibblock">
T.&nbsp;Sawada, D.&nbsp;Paleka, A.&nbsp;Havrilla, P.&nbsp;Tadepalli, P.&nbsp;Vidas, A.&nbsp;Kranias, J.&nbsp;J.
Nay, K.&nbsp;Gupta, and A.&nbsp;Komatsuzaki, “ARB: advanced reasoning benchmark for
large language models,” <em id="bib.bib716.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.13692, 2023.

</span>
</li>
<li id="bib.bib717" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[717]</span>
<span class="ltx_bibblock">
Y.&nbsp;Peng, S.&nbsp;Li, W.&nbsp;Gu, Y.&nbsp;Li, W.&nbsp;Wang, C.&nbsp;Gao, and M.&nbsp;R. Lyu, “Revisiting,
benchmarking and exploring API recommendation: How far are we?”
<em id="bib.bib717.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Software Eng.</em>, vol.&nbsp;49, no.&nbsp;4, pp. 1876–1897, 2023.

</span>
</li>
<li id="bib.bib718" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[718]</span>
<span class="ltx_bibblock">
M.&nbsp;Li, F.&nbsp;Song, B.&nbsp;Yu, H.&nbsp;Yu, Z.&nbsp;Li, F.&nbsp;Huang, and Y.&nbsp;Li, “Api-bank: A
benchmark for tool-augmented llms,” <em id="bib.bib718.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.08244, 2023.

</span>
</li>
<li id="bib.bib719" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[719]</span>
<span class="ltx_bibblock">
Q.&nbsp;Tang, Z.&nbsp;Deng, H.&nbsp;Lin, X.&nbsp;Han, Q.&nbsp;Liang, and L.&nbsp;Sun, “Toolalpaca:
Generalized tool learning for language models with 3000 simulated cases,”
<em id="bib.bib719.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.05301, 2023.

</span>
</li>
<li id="bib.bib720" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[720]</span>
<span class="ltx_bibblock">
Q.&nbsp;Xu, F.&nbsp;Hong, B.&nbsp;Li, C.&nbsp;Hu, Z.&nbsp;Chen, and J.&nbsp;Zhang, “On the tool manipulation
capability of open-source large language models,” <em id="bib.bib720.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.16504, 2023.

</span>
</li>
<li id="bib.bib721" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[721]</span>
<span class="ltx_bibblock">
Y.&nbsp;Qin, S.&nbsp;Liang, Y.&nbsp;Ye, K.&nbsp;Zhu, L.&nbsp;Yan, Y.&nbsp;Lu, Y.&nbsp;Lin, X.&nbsp;Cong, X.&nbsp;Tang,
B.&nbsp;Qian, S.&nbsp;Zhao, R.&nbsp;Tian, R.&nbsp;Xie, J.&nbsp;Zhou, M.&nbsp;Gerstein, D.&nbsp;Li, Z.&nbsp;Liu, and
M.&nbsp;Sun, “Toolllm: Facilitating large language models to master 16000+
real-world apis,” <em id="bib.bib721.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.16789, 2023.

</span>
</li>
<li id="bib.bib722" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[722]</span>
<span class="ltx_bibblock">
Z.&nbsp;Liu, W.&nbsp;Yao, J.&nbsp;Zhang, L.&nbsp;Xue, S.&nbsp;Heinecke, R.&nbsp;Murthy, Y.&nbsp;Feng, Z.&nbsp;Chen,
J.&nbsp;C. Niebles, D.&nbsp;Arpit, R.&nbsp;Xu, P.&nbsp;Mui, H.&nbsp;Wang, C.&nbsp;Xiong, and S.&nbsp;Savarese,
“BOLAA: benchmarking and orchestrating llm-augmented autonomous agents,”
<em id="bib.bib722.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.05960, 2023.

</span>
</li>
<li id="bib.bib723" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[723]</span>
<span class="ltx_bibblock">
X.&nbsp;Liu, H.&nbsp;Yu, H.&nbsp;Zhang, Y.&nbsp;Xu, X.&nbsp;Lei, H.&nbsp;Lai, Y.&nbsp;Gu, H.&nbsp;Ding, K.&nbsp;Men,
K.&nbsp;Yang, S.&nbsp;Zhang, X.&nbsp;Deng, A.&nbsp;Zeng, Z.&nbsp;Du, C.&nbsp;Zhang, S.&nbsp;Shen, T.&nbsp;Zhang,
Y.&nbsp;Su, H.&nbsp;Sun, M.&nbsp;Huang, Y.&nbsp;Dong, and J.&nbsp;Tang, “Agentbench: Evaluating llms
as agents,” <em id="bib.bib723.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.03688, 2023.

</span>
</li>
<li id="bib.bib724" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[724]</span>
<span class="ltx_bibblock">
K.&nbsp;Zhu, J.&nbsp;Wang, J.&nbsp;Zhou, Z.&nbsp;Wang, H.&nbsp;Chen, Y.&nbsp;Wang, L.&nbsp;Yang, W.&nbsp;Ye, N.&nbsp;Z.
Gong, Y.&nbsp;Zhang, and X.&nbsp;Xie, “Promptbench: Towards evaluating the robustness
of large language models on adversarial prompts,” <em id="bib.bib724.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.04528, 2023.

</span>
</li>
<li id="bib.bib725" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[725]</span>
<span class="ltx_bibblock">
R.&nbsp;S. Shah, K.&nbsp;Chawla, D.&nbsp;Eidnani, A.&nbsp;Shah, W.&nbsp;Du, S.&nbsp;Chava, N.&nbsp;Raman,
C.&nbsp;Smiley, J.&nbsp;Chen, and D.&nbsp;Yang, “WHEN FLUE MEETS FLANG: benchmarks
and large pre-trained language model for financial domain,” <em id="bib.bib725.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2211.00083, 2022.

</span>
</li>
<li id="bib.bib726" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[726]</span>
<span class="ltx_bibblock">
N.&nbsp;Guha, D.&nbsp;E. Ho, J.&nbsp;Nyarko, and C.&nbsp;Ré, “Legalbench: Prototyping a
collaborative benchmark for legal reasoning,” <em id="bib.bib726.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2209.06120, 2022.

</span>
</li>
<li id="bib.bib727" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[727]</span>
<span class="ltx_bibblock">
L.&nbsp;Zheng, W.&nbsp;Chiang, Y.&nbsp;Sheng, S.&nbsp;Zhuang, Z.&nbsp;Wu, Y.&nbsp;Zhuang, Z.&nbsp;Lin, Z.&nbsp;Li,
D.&nbsp;Li, E.&nbsp;P. Xing, H.&nbsp;Zhang, J.&nbsp;E. Gonzalez, and I.&nbsp;Stoica, “Judging
llm-as-a-judge with mt-bench and chatbot arena,” <em id="bib.bib727.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.05685, 2023.

</span>
</li>
<li id="bib.bib728" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[728]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, Z.&nbsp;Hu, P.&nbsp;Lu, Y.&nbsp;Zhu, J.&nbsp;Zhang, S.&nbsp;Subramaniam, A.&nbsp;R. Loomba,
S.&nbsp;Zhang, Y.&nbsp;Sun, and W.&nbsp;Wang, “Scibench: Evaluating college-level
scientific problem-solving abilities of large language models,” <em id="bib.bib728.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2307.10635, 2023.

</span>
</li>
<li id="bib.bib729" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[729]</span>
<span class="ltx_bibblock">
X.&nbsp;Li, T.&nbsp;Zhang, Y.&nbsp;Dubois, R.&nbsp;Taori, I.&nbsp;Gulrajani, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto, “Alpacaeval: An automatic evaluator of
instruction-following models,”
<a target="_blank" href="https://github.com/tatsu-lab/alpaca_eval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/alpaca_eval</a>, 2023.

</span>
</li>
<li id="bib.bib730" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[730]</span>
<span class="ltx_bibblock">
Y.&nbsp;Huang, Q.&nbsp;Zhang, P.&nbsp;S. Yu, and L.&nbsp;Sun, “Trustgpt: A benchmark for
trustworthy and responsible large language models,” <em id="bib.bib730.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.11507, 2023.

</span>
</li>
<li id="bib.bib731" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[731]</span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, J.&nbsp;Ying, Y.&nbsp;Cao, X.&nbsp;Lv, Y.&nbsp;He, X.&nbsp;Wang, J.&nbsp;Yu, K.&nbsp;Zeng, Y.&nbsp;Xiao,
H.&nbsp;Lyu, J.&nbsp;Zhang, J.&nbsp;Li, and L.&nbsp;Hou, “Benchmarking foundation models with
language-model-as-an-examiner,” <em id="bib.bib731.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.04181, 2023.

</span>
</li>
<li id="bib.bib732" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[732]</span>
<span class="ltx_bibblock">
C.&nbsp;Chan, W.&nbsp;Chen, Y.&nbsp;Su, J.&nbsp;Yu, W.&nbsp;Xue, S.&nbsp;Zhang, J.&nbsp;Fu, and Z.&nbsp;Liu,
“Chateval: Towards better llm-based evaluators through multi-agent debate,”
<em id="bib.bib732.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.07201, 2023.

</span>
</li>
<li id="bib.bib733" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[733]</span>
<span class="ltx_bibblock">
Y.&nbsp;Chang, X.&nbsp;Wang, J.&nbsp;Wang, Y.&nbsp;Wu, K.&nbsp;Zhu, H.&nbsp;Chen, L.&nbsp;Yang, X.&nbsp;Yi, C.&nbsp;Wang,
Y.&nbsp;Wang, W.&nbsp;Ye, Y.&nbsp;Zhang, Y.&nbsp;Chang, P.&nbsp;S. Yu, Q.&nbsp;Yang, and X.&nbsp;Xie, “A survey
on evaluation of large language models,” <em id="bib.bib733.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.03109,
2023.

</span>
</li>
<li id="bib.bib734" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[734]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zhuang, Q.&nbsp;Chen, L.&nbsp;Ma, M.&nbsp;Li, Y.&nbsp;Han, Y.&nbsp;Qian, H.&nbsp;Bai, Z.&nbsp;Feng, W.&nbsp;Zhang,
and T.&nbsp;Liu, “Through the lens of core competency: Survey on evaluation of
large language models,” <em id="bib.bib734.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.07902, 2023.

</span>
</li>
<li id="bib.bib735" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[735]</span>
<span class="ltx_bibblock">
J.&nbsp;H. Clark, J.&nbsp;Palomaki, V.&nbsp;Nikolaev, E.&nbsp;Choi, D.&nbsp;Garrette, M.&nbsp;Collins, and
T.&nbsp;Kwiatkowski, “Tydi QA: A benchmark for information-seeking question
answering in typologically diverse languages,” <em id="bib.bib735.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput.
Linguistics</em>, vol.&nbsp;8, pp. 454–470, 2020.

</span>
</li>
<li id="bib.bib736" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[736]</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, J.&nbsp;Tow, S.&nbsp;Biderman, S.&nbsp;Black, A.&nbsp;DiPofi, C.&nbsp;Foster, L.&nbsp;Golding,
J.&nbsp;Hsu, K.&nbsp;McDonell, N.&nbsp;Muennighoff, J.&nbsp;Phang, L.&nbsp;Reynolds, E.&nbsp;Tang,
A.&nbsp;Thite, B.&nbsp;Wang, K.&nbsp;Wang, and A.&nbsp;Zou, “A framework for few-shot language
model evaluation,” Sep. 2021.

</span>
</li>
<li id="bib.bib737" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[737]</span>
<span class="ltx_bibblock">
R.&nbsp;Shah, K.&nbsp;Chawla, D.&nbsp;Eidnani, A.&nbsp;Shah, W.&nbsp;Du, S.&nbsp;Chava, N.&nbsp;Raman, C.&nbsp;Smiley,
J.&nbsp;Chen, and D.&nbsp;Yang, “When flue meets flang: Benchmarks and large
pretrained language model for financial domain,” in <em id="bib.bib737.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022,
pp. 2322–2335.

</span>
</li>
<li id="bib.bib738" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[738]</span>
<span class="ltx_bibblock">
K.&nbsp;Zhou, Y.&nbsp;Zhu, Z.&nbsp;Chen, W.&nbsp;Chen, W.&nbsp;X. Zhao, X.&nbsp;Chen, Y.&nbsp;Lin, J.-R. Wen, and
J.&nbsp;Han, “Don’t make your llm an evaluation benchmark cheater,” <em id="bib.bib738.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2311.01964</em>, 2023.

</span>
</li>
<li id="bib.bib739" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[739]</span>
<span class="ltx_bibblock">
C.&nbsp;Zan, K.&nbsp;Peng, L.&nbsp;Ding, B.&nbsp;Qiu, B.&nbsp;Liu, S.&nbsp;He, Q.&nbsp;Lu, Z.&nbsp;Zhang, C.&nbsp;Liu,
W.&nbsp;Liu, Y.&nbsp;Zhan, and D.&nbsp;Tao, “Vega-mt: The JD explore academy machine
translation system for WMT22,” in <em id="bib.bib739.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh
Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab
Emirates (Hybrid), December 7-8, 2022</em>, P.&nbsp;Koehn, L.&nbsp;Barrault, O.&nbsp;Bojar,
F.&nbsp;Bougares, R.&nbsp;Chatterjee, M.&nbsp;R. Costa-jussà, C.&nbsp;Federmann,
M.&nbsp;Fishel, A.&nbsp;Fraser, M.&nbsp;Freitag, Y.&nbsp;Graham, R.&nbsp;Grundkiewicz, P.&nbsp;Guzman,
B.&nbsp;Haddow, M.&nbsp;Huck, A.&nbsp;Jimeno-Yepes, T.&nbsp;Kocmi, A.&nbsp;Martins, M.&nbsp;Morishita,
C.&nbsp;Monz, M.&nbsp;Nagata, T.&nbsp;Nakazawa, M.&nbsp;Negri, A.&nbsp;Névéol, M.&nbsp;Neves,
M.&nbsp;Popel, M.&nbsp;Turchi, and M.&nbsp;Zampieri, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 411–422.

</span>
</li>
<li id="bib.bib740" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[740]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhao, M.&nbsp;Khalman, R.&nbsp;Joshi, S.&nbsp;Narayan, M.&nbsp;Saleh, and P.&nbsp;J. Liu,
“Calibrating sequence likelihood improves conditional language generation,”
<em id="bib.bib740.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2210.00045, 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.48550/arXiv.2210.00045" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2210.00045</a>

</span>
</li>
<li id="bib.bib741" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[741]</span>
<span class="ltx_bibblock">
D.&nbsp;Khashabi, S.&nbsp;Min, T.&nbsp;Khot, A.&nbsp;Sabharwal, O.&nbsp;Tafjord, P.&nbsp;Clark, and
H.&nbsp;Hajishirzi, “Unifiedqa: Crossing format boundaries with a single QA
system,” in <em id="bib.bib741.1.1" class="ltx_emph ltx_font_italic">EMNLP (Findings)</em>, ser. Findings of ACL, vol. EMNLP
2020.&nbsp;&nbsp;&nbsp;Association for Computational
Linguistics, 2020, pp. 1896–1907.

</span>
</li>
<li id="bib.bib742" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[742]</span>
<span class="ltx_bibblock">
X.&nbsp;Zhu, J.&nbsp;Wang, L.&nbsp;Zhang, Y.&nbsp;Zhang, R.&nbsp;Gan, J.&nbsp;Zhang, and Y.&nbsp;Yang, “Solving
math word problem via cooperative reasoning induced language models,”
<em id="bib.bib742.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.16257</em>, 2022.

</span>
</li>
<li id="bib.bib743" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[743]</span>
<span class="ltx_bibblock">
A.&nbsp;Nguyen, N.&nbsp;Karampatziakis, and W.&nbsp;Chen, “Meet in the middle: A new
pre-training paradigm,” <em id="bib.bib743.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.07295, 2023. [Online].
Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2303.07295" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2303.07295</a>

</span>
</li>
<li id="bib.bib744" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[744]</span>
<span class="ltx_bibblock">
H.&nbsp;Li, J.&nbsp;Zhang, C.&nbsp;Li, and H.&nbsp;Chen, “RESDSQL: decoupling schema linking and
skeleton parsing for text-to-sql,” <em id="bib.bib744.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.05965, 2023.
[Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2302.05965" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2302.05965</a>

</span>
</li>
<li id="bib.bib745" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[745]</span>
<span class="ltx_bibblock">
W.&nbsp;Kang and J.&nbsp;J. McAuley, “Self-attentive sequential recommendation,” in
<em id="bib.bib745.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Data Mining, ICDM 2018, Singapore,
November 17-20, 2018</em>.&nbsp;&nbsp;&nbsp;IEEE Computer
Society, 2018, pp. 197–206.

</span>
</li>
<li id="bib.bib746" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[746]</span>
<span class="ltx_bibblock">
B.&nbsp;Yang, C.&nbsp;Han, Y.&nbsp;Li, L.&nbsp;Zuo, and Z.&nbsp;Yu, “Improving conversational
recommendation systems’ quality with context-aware item meta-information,”
in <em id="bib.bib746.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: NAACL
2022, Seattle, WA, United States, July 10-15, 2022</em>, M.&nbsp;Carpuat,
M.&nbsp;de&nbsp;Marneffe, and I.&nbsp;V.&nbsp;M. Ruíz, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2022, pp. 38–48.

</span>
</li>
<li id="bib.bib747" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[747]</span>
<span class="ltx_bibblock">
E.&nbsp;Almazrouei, H.&nbsp;Alobeidli, A.&nbsp;Alshamsi, A.&nbsp;Cappelli, R.&nbsp;Cojocaru, M.&nbsp;Debbah,
E.&nbsp;Goffinet, D.&nbsp;Heslow, J.&nbsp;Launay, Q.&nbsp;Malartic, B.&nbsp;Noune, B.&nbsp;Pannier, and
G.&nbsp;Penedo, “Falcon-40B: an open large language model with state-of-the-art
performance,” 2023.

</span>
</li>
<li id="bib.bib748" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[748]</span>
<span class="ltx_bibblock">
S.&nbsp;Martin, J.&nbsp;Liermann, and H.&nbsp;Ney, “Algorithms for bigram and trigram word
clustering,” <em id="bib.bib748.1.1" class="ltx_emph ltx_font_italic">Speech communication</em>, vol.&nbsp;24, no.&nbsp;1, pp. 19–37, 1998.

</span>
</li>
<li id="bib.bib749" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[749]</span>
<span class="ltx_bibblock">
R.&nbsp;Navigli, “Word sense disambiguation: A survey,” <em id="bib.bib749.1.1" class="ltx_emph ltx_font_italic">ACM computing
surveys (CSUR)</em>, vol.&nbsp;41, no.&nbsp;2, pp. 1–69, 2009.

</span>
</li>
<li id="bib.bib750" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[750]</span>
<span class="ltx_bibblock">
W.&nbsp;H. Gomaa, A.&nbsp;A. Fahmy <em id="bib.bib750.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “A survey of text similarity
approaches,” <em id="bib.bib750.2.2" class="ltx_emph ltx_font_italic">international journal of Computer Applications</em>, vol.&nbsp;68,
no.&nbsp;13, pp. 13–18, 2013.

</span>
</li>
<li id="bib.bib751" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[751]</span>
<span class="ltx_bibblock">
S.&nbsp;Minaee, N.&nbsp;Kalchbrenner, E.&nbsp;Cambria, N.&nbsp;Nikzad, M.&nbsp;Chenaghlu, and J.&nbsp;Gao,
“Deep learning–based text classification: a comprehensive review,”
<em id="bib.bib751.1.1" class="ltx_emph ltx_font_italic">ACM computing surveys (CSUR)</em>, vol.&nbsp;54, no.&nbsp;3, pp. 1–40, 2021.

</span>
</li>
<li id="bib.bib752" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[752]</span>
<span class="ltx_bibblock">
N.&nbsp;Alex, E.&nbsp;Lifland, L.&nbsp;Tunstall, A.&nbsp;Thakur, P.&nbsp;Maham, C.&nbsp;J. Riedel, E.&nbsp;Hine,
C.&nbsp;Ashurst, P.&nbsp;Sedille, A.&nbsp;Carlier, M.&nbsp;Noetel, and A.&nbsp;Stuhlmüller,
“RAFT: A real-world few-shot text classification benchmark,” in
<em id="bib.bib752.1.1" class="ltx_emph ltx_font_italic">NeurIPS Datasets and Benchmarks</em>, 2021.

</span>
</li>
<li id="bib.bib753" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[753]</span>
<span class="ltx_bibblock">
C.&nbsp;Qin, A.&nbsp;Zhang, Z.&nbsp;Zhang, J.&nbsp;Chen, M.&nbsp;Yasunaga, and D.&nbsp;Yang, “Is chatgpt a
general-purpose natural language processing task solver?” <em id="bib.bib753.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2302.06476, 2023.

</span>
</li>
<li id="bib.bib754" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[754]</span>
<span class="ltx_bibblock">
X.&nbsp;Chen, J.&nbsp;Ye, C.&nbsp;Zu, N.&nbsp;Xu, R.&nbsp;Zheng, M.&nbsp;Peng, J.&nbsp;Zhou, T.&nbsp;Gui, Q.&nbsp;Zhang, and
X.&nbsp;Huang, “How robust is gpt-3.5 to predecessors? a comprehensive study on
language understanding tasks,” 2023.

</span>
</li>
<li id="bib.bib755" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[755]</span>
<span class="ltx_bibblock">
D.&nbsp;Nadeau and S.&nbsp;Sekine, “A survey of named entity recognition and
classification,” <em id="bib.bib755.1.1" class="ltx_emph ltx_font_italic">Lingvisticae Investigationes</em>, vol.&nbsp;30, no.&nbsp;1, pp.
3–26, 2007.

</span>
</li>
<li id="bib.bib756" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[756]</span>
<span class="ltx_bibblock">
A.&nbsp;Ratnaparkhi, “A maximum entropy model for part-of-speech tagging,” in
<em id="bib.bib756.1.1" class="ltx_emph ltx_font_italic">Conference on empirical methods in natural language processing</em>, 1996.

</span>
</li>
<li id="bib.bib757" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[757]</span>
<span class="ltx_bibblock">
V.&nbsp;Yadav and S.&nbsp;Bethard, “A survey on recent advances in named entity
recognition from deep learning models,” in <em id="bib.bib757.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th
International Conference on Computational Linguistics</em>, 2018, pp. 2145–2158.

</span>
</li>
<li id="bib.bib758" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[758]</span>
<span class="ltx_bibblock">
F.&nbsp;Souza, R.&nbsp;Nogueira, and R.&nbsp;Lotufo, “Portuguese named entity recognition
using bert-crf,” <em id="bib.bib758.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.10649</em>, 2019.

</span>
</li>
<li id="bib.bib759" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[759]</span>
<span class="ltx_bibblock">
S.&nbsp;Pawar, G.&nbsp;K. Palshikar, and P.&nbsp;Bhattacharyya, “Relation extraction: A
survey,” <em id="bib.bib759.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.05191</em>, 2017.

</span>
</li>
<li id="bib.bib760" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[760]</span>
<span class="ltx_bibblock">
C.&nbsp;Walker and et&nbsp;al., “Ace 2005 multilingual training corpus ldc2006t06,”
Philadelphia, 2006.

</span>
</li>
<li id="bib.bib761" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[761]</span>
<span class="ltx_bibblock">
J.&nbsp;Gao, H.&nbsp;Zhao, C.&nbsp;Yu, and R.&nbsp;Xu, “Exploring the feasibility of chatgpt for
event extraction,” <em id="bib.bib761.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.03836, 2023.

</span>
</li>
<li id="bib.bib762" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[762]</span>
<span class="ltx_bibblock">
Y.&nbsp;Ma, Y.&nbsp;Cao, Y.&nbsp;Hong, and A.&nbsp;Sun, “Large language model is not a good
few-shot information extractor, but a good reranker for hard samples!”
<em id="bib.bib762.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.08559, 2023.

</span>
</li>
<li id="bib.bib763" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[763]</span>
<span class="ltx_bibblock">
R.&nbsp;Tang, X.&nbsp;Han, X.&nbsp;Jiang, and X.&nbsp;Hu, “Does synthetic data generation of llms
help clinical text mining?” <em id="bib.bib763.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04360</em>, 2023.

</span>
</li>
<li id="bib.bib764" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[764]</span>
<span class="ltx_bibblock">
A.&nbsp;Vaswani, S.&nbsp;Bengio, E.&nbsp;Brevdo, F.&nbsp;Chollet, A.&nbsp;Gomez, S.&nbsp;Gouws, L.&nbsp;Jones,
Ł.&nbsp;Kaiser, N.&nbsp;Kalchbrenner, N.&nbsp;Parmar <em id="bib.bib764.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Tensor2tensor for
neural machine translation,” in <em id="bib.bib764.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 13th Conference of
the Association for Machine Translation in the Americas (Volume 1: Research
Track)</em>, 2018, pp. 193–199.

</span>
</li>
<li id="bib.bib765" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[765]</span>
<span class="ltx_bibblock">
B.&nbsp;Zhang, B.&nbsp;Haddow, and A.&nbsp;Birch, “Prompting large language model for machine
translation: A case study,” <em id="bib.bib765.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.07069</em>, 2023.

</span>
</li>
<li id="bib.bib766" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[766]</span>
<span class="ltx_bibblock">
M.&nbsp;Ghazvininejad, H.&nbsp;Gonen, and L.&nbsp;Zettlemoyer, “Dictionary-based phrase-level
prompting of large language models for machine translation,” <em id="bib.bib766.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2302.07856</em>, 2023.

</span>
</li>
<li id="bib.bib767" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[767]</span>
<span class="ltx_bibblock">
L.&nbsp;Wang, C.&nbsp;Lyu, T.&nbsp;Ji, Z.&nbsp;Zhang, D.&nbsp;Yu, S.&nbsp;Shi, and Z.&nbsp;Tu, “Document-level
machine translation with large language models,” <em id="bib.bib767.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2304.02210</em>, 2023.

</span>
</li>
<li id="bib.bib768" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[768]</span>
<span class="ltx_bibblock">
W.&nbsp;Jiao, J.-t. Huang, W.&nbsp;Wang, X.&nbsp;Wang, S.&nbsp;Shi, and Z.&nbsp;Tu, “Parrot:
Translating during chat using large language models,” <em id="bib.bib768.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2304.02426</em>, 2023.

</span>
</li>
<li id="bib.bib769" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[769]</span>
<span class="ltx_bibblock">
W.&nbsp;Yang, C.&nbsp;Li, J.&nbsp;Zhang, and C.&nbsp;Zong, “Bigtrans: Augmenting large language
models with multilingual translation capability over 100 languages,”
<em id="bib.bib769.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18098</em>, 2023.

</span>
</li>
<li id="bib.bib770" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[770]</span>
<span class="ltx_bibblock">
J.&nbsp;Kocon, I.&nbsp;Cichecki, O.&nbsp;Kaszyca, M.&nbsp;Kochanek, D.&nbsp;Szydlo, J.&nbsp;Baran,
J.&nbsp;Bielaniewicz, M.&nbsp;Gruza, A.&nbsp;Janz, K.&nbsp;Kanclerz, A.&nbsp;Kocon, B.&nbsp;Koptyra,
W.&nbsp;Mieleszczenko-Kowszewicz, P.&nbsp;Milkowski, M.&nbsp;Oleksy, M.&nbsp;Piasecki,
L.&nbsp;Radlinski, K.&nbsp;Wojtasik, S.&nbsp;Wozniak, and P.&nbsp;Kazienko, “Chatgpt: Jack of
all trades, master of none,” <em id="bib.bib770.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.10724, 2023.

</span>
</li>
<li id="bib.bib771" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[771]</span>
<span class="ltx_bibblock">
Q.&nbsp;Zhong, L.&nbsp;Ding, J.&nbsp;Liu, B.&nbsp;Du, and D.&nbsp;Tao, “Can chatgpt understand too? A
comparative study on chatgpt and fine-tuned BERT,” <em id="bib.bib771.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2302.10198, 2023.

</span>
</li>
<li id="bib.bib772" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[772]</span>
<span class="ltx_bibblock">
D.&nbsp;Cheng, S.&nbsp;Huang, J.&nbsp;Bi, Y.&nbsp;Zhan, J.&nbsp;Liu, Y.&nbsp;Wang, H.&nbsp;Sun, F.&nbsp;Wei, D.&nbsp;Deng,
and Q.&nbsp;Zhang, “Uprise: Universal prompt retrieval for improving zero-shot
evaluation,” <em id="bib.bib772.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08518</em>, 2023.

</span>
</li>
<li id="bib.bib773" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[773]</span>
<span class="ltx_bibblock">
R.&nbsp;Ren, Y.&nbsp;Qu, J.&nbsp;Liu, W.&nbsp;X. Zhao, Q.&nbsp;She, H.&nbsp;Wu, H.&nbsp;Wang, and J.-R. Wen,
“Rocketqav2: A joint training method for dense passage retrieval and passage
re-ranking,” in <em id="bib.bib773.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing</em>, 2021, pp. 2825–2835.

</span>
</li>
<li id="bib.bib774" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[774]</span>
<span class="ltx_bibblock">
W.&nbsp;Sun, L.&nbsp;Yan, X.&nbsp;Ma, P.&nbsp;Ren, D.&nbsp;Yin, and Z.&nbsp;Ren, “Is chatgpt good at search?
investigating large language models as re-ranking agent,” <em id="bib.bib774.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2304.09542</em>, 2023.

</span>
</li>
<li id="bib.bib775" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[775]</span>
<span class="ltx_bibblock">
Z.&nbsp;Qin, R.&nbsp;Jagerman, K.&nbsp;Hui, H.&nbsp;Zhuang, J.&nbsp;Wu, J.&nbsp;Shen, T.&nbsp;Liu, J.&nbsp;Liu,
D.&nbsp;Metzler, X.&nbsp;Wang <em id="bib.bib775.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Large language models are effective text
rankers with pairwise ranking prompting,” <em id="bib.bib775.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2306.17563</em>, 2023.

</span>
</li>
<li id="bib.bib776" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[776]</span>
<span class="ltx_bibblock">
S.&nbsp;Cho, S.&nbsp;Jeong, J.&nbsp;Seo, and J.&nbsp;C. Park, “Discrete prompt optimization via
constrained generation for zero-shot re-ranker,” <em id="bib.bib776.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.13729</em>, 2023.

</span>
</li>
<li id="bib.bib777" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[777]</span>
<span class="ltx_bibblock">
R.&nbsp;Tang, X.&nbsp;Zhang, X.&nbsp;Ma, J.&nbsp;Lin, and F.&nbsp;Ture, “Found in the middle:
Permutation self-consistency improves listwise ranking in large language
models,” <em id="bib.bib777.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07712</em>, 2023.

</span>
</li>
<li id="bib.bib778" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[778]</span>
<span class="ltx_bibblock">
X.&nbsp;Ma, X.&nbsp;Zhang, R.&nbsp;Pradeep, and J.&nbsp;Lin, “Zero-shot listwise document
reranking with a large language model,” <em id="bib.bib778.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.02156</em>, 2023.

</span>
</li>
<li id="bib.bib779" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[779]</span>
<span class="ltx_bibblock">
S.&nbsp;Zhuang, H.&nbsp;Zhuang, B.&nbsp;Koopman, and G.&nbsp;Zuccon, “A setwise approach for
effective and highly efficient zero-shot ranking with large language
models,” <em id="bib.bib779.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.09497</em>, 2023.

</span>
</li>
<li id="bib.bib780" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[780]</span>
<span class="ltx_bibblock">
H.&nbsp;Zhuang, Z.&nbsp;Qin, K.&nbsp;Hui, J.&nbsp;Wu, L.&nbsp;Yan, X.&nbsp;Wang, and M.&nbsp;Berdersky, “Beyond
yes and no: Improving zero-shot llm rankers via scoring fine-grained
relevance labels,” <em id="bib.bib780.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.14122</em>, 2023.

</span>
</li>
<li id="bib.bib781" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[781]</span>
<span class="ltx_bibblock">
N.&nbsp;Ziems, W.&nbsp;Yu, Z.&nbsp;Zhang, and M.&nbsp;Jiang, “Large language models are built-in
autoregressive search engines,” <em id="bib.bib781.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.09612</em>,
2023.

</span>
</li>
<li id="bib.bib782" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[782]</span>
<span class="ltx_bibblock">
X.&nbsp;Ma, L.&nbsp;Wang, N.&nbsp;Yang, F.&nbsp;Wei, and J.&nbsp;Lin, “Fine-tuning llama for
multi-stage text retrieval,” <em id="bib.bib782.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.08319</em>, 2023.

</span>
</li>
<li id="bib.bib783" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[783]</span>
<span class="ltx_bibblock">
R.&nbsp;Pradeep, S.&nbsp;Sharifymoghaddam, and J.&nbsp;Lin, “Rankvicuna: Zero-shot listwise
document reranking with open-source large language models,” <em id="bib.bib783.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2309.15088</em>, 2023.

</span>
</li>
<li id="bib.bib784" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[784]</span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, V.&nbsp;Q. Tran, M.&nbsp;Dehghani, J.&nbsp;Ni, D.&nbsp;Bahri, H.&nbsp;Mehta, Z.&nbsp;Qin, K.&nbsp;Hui,
Z.&nbsp;Zhao, J.&nbsp;Gupta <em id="bib.bib784.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Transformer memory as a differentiable
search index,” in <em id="bib.bib784.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
2022.

</span>
</li>
<li id="bib.bib785" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[785]</span>
<span class="ltx_bibblock">
R.&nbsp;Ren, W.&nbsp;X. Zhao, J.&nbsp;Liu, H.&nbsp;Wu, J.-R. Wen, and H.&nbsp;Wang, “TOME: A
two-stage approach for model-based retrieval,” in <em id="bib.bib785.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers)</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2023, pp. 6102–6114. [Online]. Available:
<a target="_blank" href="https://aclanthology.org/2023.acl-long.336" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.acl-long.336</a>

</span>
</li>
<li id="bib.bib786" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[786]</span>
<span class="ltx_bibblock">
Y.&nbsp;Qu, Y.&nbsp;Ding, J.&nbsp;Liu, K.&nbsp;Liu, R.&nbsp;Ren, W.&nbsp;X. Zhao, D.&nbsp;Dong, H.&nbsp;Wu, and
H.&nbsp;Wang, “Rocketqa: An optimized training approach to dense passage
retrieval for open-domain question answering,” in <em id="bib.bib786.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies</em>, 2021, pp.
5835–5847.

</span>
</li>
<li id="bib.bib787" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[787]</span>
<span class="ltx_bibblock">
R.&nbsp;Ren, S.&nbsp;Lv, Y.&nbsp;Qu, J.&nbsp;Liu, W.&nbsp;X. Zhao, Q.&nbsp;She, H.&nbsp;Wu, H.&nbsp;Wang, and J.-R.
Wen, “Pair: Leveraging passage-centric similarity relation for improving
dense passage retrieval,” in <em id="bib.bib787.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for
Computational Linguistics: ACL-IJCNLP 2021</em>, 2021, pp. 2173–2183.

</span>
</li>
<li id="bib.bib788" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[788]</span>
<span class="ltx_bibblock">
Z.&nbsp;Peng, X.&nbsp;Wu, and Y.&nbsp;Fang, “Soft prompt tuning for augmenting dense
retrieval with large language models,” <em id="bib.bib788.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2307.08303</em>, 2023.

</span>
</li>
<li id="bib.bib789" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[789]</span>
<span class="ltx_bibblock">
Z.&nbsp;Dai, V.&nbsp;Y. Zhao, J.&nbsp;Ma, Y.&nbsp;Luan, J.&nbsp;Ni, J.&nbsp;Lu, A.&nbsp;Bakalov, K.&nbsp;Guu, K.&nbsp;Hall,
and M.-W. Chang, “Promptagator: Few-shot dense retrieval from 8 examples,”
in <em id="bib.bib789.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>,
2023.

</span>
</li>
<li id="bib.bib790" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[790]</span>
<span class="ltx_bibblock">
A.&nbsp;Askari, M.&nbsp;Aliannejadi, E.&nbsp;Kanoulas, and S.&nbsp;Verberne, “Generating synthetic
documents for cross-encoder re-rankers: A comparative study of chatgpt and
human experts,” <em id="bib.bib790.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.02320</em>, 2023.

</span>
</li>
<li id="bib.bib791" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[791]</span>
<span class="ltx_bibblock">
K.&nbsp;Mao, Z.&nbsp;Dou, H.&nbsp;Chen, F.&nbsp;Mo, and H.&nbsp;Qian, “Large language models know your
contextual search intent: A prompting framework for conversational search,”
<em id="bib.bib791.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.06573</em>, 2023.

</span>
</li>
<li id="bib.bib792" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[792]</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, X.&nbsp;Ma, J.&nbsp;Lin, and J.&nbsp;Callan, “Precise zero-shot dense retrieval
without relevance labels,” in <em id="bib.bib792.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers)</em>.&nbsp;&nbsp;&nbsp;Association for
Computational Linguistics, 2023, pp. 1762–1777.

</span>
</li>
<li id="bib.bib793" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[793]</span>
<span class="ltx_bibblock">
L.&nbsp;Wang, N.&nbsp;Yang, and F.&nbsp;Wei, “Query2doc: Query expansion with large language
models,” <em id="bib.bib793.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.07678</em>, 2023.

</span>
</li>
<li id="bib.bib794" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[794]</span>
<span class="ltx_bibblock">
G.&nbsp;Ma, X.&nbsp;Wu, P.&nbsp;Wang, Z.&nbsp;Lin, and S.&nbsp;Hu, “Pre-training with large language
model-based document expansion for dense passage retrieval,” <em id="bib.bib794.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2308.08285</em>, 2023.

</span>
</li>
<li id="bib.bib795" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[795]</span>
<span class="ltx_bibblock">
W.&nbsp;Sun, Z.&nbsp;Chen, X.&nbsp;Ma, L.&nbsp;Yan, S.&nbsp;Wang, P.&nbsp;Ren, Z.&nbsp;Chen, D.&nbsp;Yin, and Z.&nbsp;Ren,
“Instruction distillation makes large language models efficient zero-shot
rankers,” <em id="bib.bib795.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.01555</em>, 2023.

</span>
</li>
<li id="bib.bib796" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[796]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, W.&nbsp;Zhu, and W.&nbsp;Y. Wang, “Large language models are implicitly topic
models: Explaining and finding good demonstrations for in-context learning,”
<em id="bib.bib796.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2301.11916, 2023.

</span>
</li>
<li id="bib.bib797" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[797]</span>
<span class="ltx_bibblock">
C.&nbsp;Li, Z.&nbsp;Gan, Z.&nbsp;Yang, J.&nbsp;Yang, L.&nbsp;Li, L.&nbsp;Wang, and J.&nbsp;Gao, “Multimodal
foundation models: From specialists to general-purpose assistants,”
<em id="bib.bib797.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.10020, 2023.

</span>
</li>
<li id="bib.bib798" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[798]</span>
<span class="ltx_bibblock">
W.&nbsp;X. Zhao, S.&nbsp;Mu, Y.&nbsp;Hou, Z.&nbsp;Lin, Y.&nbsp;Chen, X.&nbsp;Pan, K.&nbsp;Li, Y.&nbsp;Lu, H.&nbsp;Wang,
C.&nbsp;Tian, Y.&nbsp;Min, Z.&nbsp;Feng, X.&nbsp;Fan, X.&nbsp;Chen, P.&nbsp;Wang, W.&nbsp;Ji, Y.&nbsp;Li, X.&nbsp;Wang,
and J.&nbsp;Wen, “Recbole: Towards a unified, comprehensive and efficient
framework for recommendation algorithms,” in <em id="bib.bib798.1.1" class="ltx_emph ltx_font_italic">CIKM</em>, G.&nbsp;Demartini,
G.&nbsp;Zuccon, J.&nbsp;S. Culpepper, Z.&nbsp;Huang, and H.&nbsp;Tong, Eds.&nbsp;&nbsp;&nbsp;ACM, 2021, pp. 4653–4664.

</span>
</li>
<li id="bib.bib799" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[799]</span>
<span class="ltx_bibblock">
K.&nbsp;Zhou, H.&nbsp;Wang, W.&nbsp;X. Zhao, Y.&nbsp;Zhu, S.&nbsp;Wang, F.&nbsp;Zhang, Z.&nbsp;Wang, and J.&nbsp;Wen,
“S3-rec: Self-supervised learning for sequential recommendation with mutual
information maximization,” in <em id="bib.bib799.1.1" class="ltx_emph ltx_font_italic">CIKM</em>, M.&nbsp;d’Aquin, S.&nbsp;Dietze,
C.&nbsp;Hauff, E.&nbsp;Curry, and P.&nbsp;Cudré-Mauroux, Eds.&nbsp;&nbsp;&nbsp;ACM, 2020, pp. 1893–1902.

</span>
</li>
<li id="bib.bib800" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[800]</span>
<span class="ltx_bibblock">
W.&nbsp;X. Zhao, Y.&nbsp;Hou, X.&nbsp;Pan, C.&nbsp;Yang, Z.&nbsp;Zhang, Z.&nbsp;Lin, J.&nbsp;Zhang, S.&nbsp;Bian,
J.&nbsp;Tang, W.&nbsp;Sun, Y.&nbsp;Chen, L.&nbsp;Xu, G.&nbsp;Zhang, Z.&nbsp;Tian, C.&nbsp;Tian, S.&nbsp;Mu, X.&nbsp;Fan,
X.&nbsp;Chen, and J.&nbsp;Wen, “Recbole 2.0: Towards a more up-to-date recommendation
library,” in <em id="bib.bib800.1.1" class="ltx_emph ltx_font_italic">CIKM</em>, M.&nbsp;A. Hasan and L.&nbsp;Xiong, Eds.&nbsp;&nbsp;&nbsp;ACM, 2022, pp. 4722–4726.

</span>
</li>
<li id="bib.bib801" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[801]</span>
<span class="ltx_bibblock">
L.&nbsp;Xu, Z.&nbsp;Tian, G.&nbsp;Zhang, J.&nbsp;Zhang, L.&nbsp;Wang, B.&nbsp;Zheng, Y.&nbsp;Li, J.&nbsp;Tang,
Z.&nbsp;Zhang, Y.&nbsp;Hou, X.&nbsp;Pan, W.&nbsp;X. Zhao, X.&nbsp;Chen, and J.&nbsp;Wen, “Towards a more
user-friendly and easy-to-use benchmark library for recommender systems,” in
<em id="bib.bib801.1.1" class="ltx_emph ltx_font_italic">SIGIR</em>, H.&nbsp;Chen, W.&nbsp;E. Duh, H.&nbsp;Huang, M.&nbsp;P. Kato, J.&nbsp;Mothe, and
B.&nbsp;Poblete, Eds.&nbsp;&nbsp;&nbsp;ACM, 2023, pp.
2837–2847.

</span>
</li>
<li id="bib.bib802" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[802]</span>
<span class="ltx_bibblock">
S.&nbsp;Rendle, C.&nbsp;Freudenthaler, Z.&nbsp;Gantner, and L.&nbsp;Schmidt-Thieme, “BPR:
bayesian personalized ranking from implicit feedback,” <em id="bib.bib802.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/1205.2618, 2012.

</span>
</li>
<li id="bib.bib803" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[803]</span>
<span class="ltx_bibblock">
W.&nbsp;Fan, Z.&nbsp;Zhao, J.&nbsp;Li, Y.&nbsp;Liu, X.&nbsp;Mei, Y.&nbsp;Wang, J.&nbsp;Tang, and Q.&nbsp;Li,
“Recommender systems in the era of large language models (llms),”
<em id="bib.bib803.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib804" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[804]</span>
<span class="ltx_bibblock">
L.&nbsp;Wu, Z.&nbsp;Zheng, Z.&nbsp;Qiu, H.&nbsp;Wang, H.&nbsp;Gu, T.&nbsp;Shen, C.&nbsp;Qin, C.&nbsp;Zhu, H.&nbsp;Zhu,
Q.&nbsp;Liu, H.&nbsp;Xiong, and E.&nbsp;Chen, “A survey on large language models for
recommendation,” <em id="bib.bib804.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib805" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[805]</span>
<span class="ltx_bibblock">
Y.&nbsp;Gao, T.&nbsp;Sheng, Y.&nbsp;Xiang, Y.&nbsp;Xiong, H.&nbsp;Wang, and J.&nbsp;Zhang, “Chat-rec:
Towards interactive and explainable llms-augmented recommender system,”
<em id="bib.bib805.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.14524, 2023.

</span>
</li>
<li id="bib.bib806" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[806]</span>
<span class="ltx_bibblock">
S.&nbsp;Dai, N.&nbsp;Shao, H.&nbsp;Zhao, W.&nbsp;Yu, Z.&nbsp;Si, C.&nbsp;Xu, Z.&nbsp;Sun, X.&nbsp;Zhang, and J.&nbsp;Xu,
“Uncovering chatgpt’s capabilities in recommender systems,” in
<em id="bib.bib806.1.1" class="ltx_emph ltx_font_italic">RecSys</em>, J.&nbsp;Zhang, L.&nbsp;Chen, S.&nbsp;Berkovsky, M.&nbsp;Zhang, T.&nbsp;D. Noia,
J.&nbsp;Basilico, L.&nbsp;Pizzato, and Y.&nbsp;Song, Eds.&nbsp;&nbsp;&nbsp;ACM, 2023, pp. 1126–1132.

</span>
</li>
<li id="bib.bib807" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[807]</span>
<span class="ltx_bibblock">
Y.&nbsp;Hou, J.&nbsp;Zhang, Z.&nbsp;Lin, H.&nbsp;Lu, R.&nbsp;Xie, J.&nbsp;J. McAuley, and W.&nbsp;X. Zhao, “Large
language models are zero-shot rankers for recommender systems,”
<em id="bib.bib807.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib808" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[808]</span>
<span class="ltx_bibblock">
J.&nbsp;Liu, C.&nbsp;Liu, R.&nbsp;Lv, K.&nbsp;Zhou, and Y.&nbsp;Zhang, “Is chatgpt a good recommender?
A preliminary study,” <em id="bib.bib808.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.10149, 2023.

</span>
</li>
<li id="bib.bib809" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[809]</span>
<span class="ltx_bibblock">
K.&nbsp;Bao, J.&nbsp;Zhang, Y.&nbsp;Zhang, W.&nbsp;Wang, F.&nbsp;Feng, and X.&nbsp;He, “Tallrec: An
effective and efficient tuning framework to align large language model with
recommendation,” in <em id="bib.bib809.1.1" class="ltx_emph ltx_font_italic">RecSys</em>, J.&nbsp;Zhang, L.&nbsp;Chen, S.&nbsp;Berkovsky,
M.&nbsp;Zhang, T.&nbsp;D. Noia, J.&nbsp;Basilico, L.&nbsp;Pizzato, and Y.&nbsp;Song, Eds.&nbsp;&nbsp;&nbsp;ACM, 2023, pp. 1007–1014.

</span>
</li>
<li id="bib.bib810" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[810]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhu, L.&nbsp;Wu, Q.&nbsp;Guo, L.&nbsp;Hong, and J.&nbsp;Li, “Collaborative large language model
for recommender systems,” <em id="bib.bib810.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.01343</em>, 2023.

</span>
</li>
<li id="bib.bib811" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[811]</span>
<span class="ltx_bibblock">
B.&nbsp;Zheng, Y.&nbsp;Hou, H.&nbsp;Lu, Y.&nbsp;Chen, W.&nbsp;X. Zhao, and J.-R. Wen, “Adapting large
language models by integrating collaborative semantics for recommendation,”
2023. [Online]. Available:
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:265213194" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:265213194</a>

</span>
</li>
<li id="bib.bib812" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[812]</span>
<span class="ltx_bibblock">
Y.&nbsp;Xi, W.&nbsp;Liu, J.&nbsp;Lin, J.&nbsp;Zhu, B.&nbsp;Chen, R.&nbsp;Tang, W.&nbsp;Zhang, R.&nbsp;Zhang, and Y.&nbsp;Yu,
“Towards open-world recommendation with knowledge augmentation from large
language models,” <em id="bib.bib812.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.10933, 2023.

</span>
</li>
<li id="bib.bib813" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[813]</span>
<span class="ltx_bibblock">
Q.&nbsp;Liu, N.&nbsp;Chen, T.&nbsp;Sakai, and X.&nbsp;Wu, “A first look at llm-powered generative
news recommendation,” <em id="bib.bib813.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.06566, 2023.

</span>
</li>
<li id="bib.bib814" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[814]</span>
<span class="ltx_bibblock">
R.&nbsp;Li, W.&nbsp;Deng, Y.&nbsp;Cheng, Z.&nbsp;Yuan, J.&nbsp;Zhang, and F.&nbsp;Yuan, “Exploring the upper
limits of text-based collaborative filtering using large language models:
Discoveries and insights,” <em id="bib.bib814.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.11700, 2023.

</span>
</li>
<li id="bib.bib815" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[815]</span>
<span class="ltx_bibblock">
W.&nbsp;Wei, X.&nbsp;Ren, J.&nbsp;Tang, Q.&nbsp;Wang, L.&nbsp;Su, S.&nbsp;Cheng, J.&nbsp;Wang, D.&nbsp;Yin, and
C.&nbsp;Huang, “Llmrec: Large language models with graph augmentation for
recommendation,” <em id="bib.bib815.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2311.00423, 2023.

</span>
</li>
<li id="bib.bib816" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[816]</span>
<span class="ltx_bibblock">
X.&nbsp;Li, B.&nbsp;Chen, L.&nbsp;Hou, and R.&nbsp;Tang, “Ctrl: Connect tabular and language model
for ctr prediction,” <em id="bib.bib816.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02841</em>, 2023.

</span>
</li>
<li id="bib.bib817" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[817]</span>
<span class="ltx_bibblock">
A.&nbsp;Muhamed, I.&nbsp;Keivanloo, S.&nbsp;Perera, J.&nbsp;Mracek, Y.&nbsp;Xu, Q.&nbsp;Cui, S.&nbsp;Rajagopalan,
B.&nbsp;Zeng, and T.&nbsp;Chilimbi, “Ctr-bert: Cost-effective knowledge distillation
for billion-parameter teacher models,” in <em id="bib.bib817.1.1" class="ltx_emph ltx_font_italic">NeurIPS Efficient Natural
Language and Speech Processing Workshop</em>, 2021.

</span>
</li>
<li id="bib.bib818" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[818]</span>
<span class="ltx_bibblock">
L.&nbsp;Wang, C.&nbsp;Ma, X.&nbsp;Feng, Z.&nbsp;Zhang, H.&nbsp;Yang, J.&nbsp;Zhang, Z.&nbsp;Chen, J.&nbsp;Tang,
X.&nbsp;Chen, Y.&nbsp;Lin, W.&nbsp;X. Zhao, Z.&nbsp;Wei, and J.&nbsp;Wen, “A survey on large language
model based autonomous agents,” <em id="bib.bib818.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.11432, 2023.

</span>
</li>
<li id="bib.bib819" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[819]</span>
<span class="ltx_bibblock">
L.&nbsp;Wang, J.&nbsp;Zhang, X.&nbsp;Chen, Y.&nbsp;Lin, R.&nbsp;Song, W.&nbsp;X. Zhao, and J.&nbsp;Wen,
“Recagent: A novel simulation paradigm for recommender systems,”
<em id="bib.bib819.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.02552, 2023.

</span>
</li>
<li id="bib.bib820" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[820]</span>
<span class="ltx_bibblock">
E.&nbsp;Ie, C.&nbsp;Hsu, M.&nbsp;Mladenov, V.&nbsp;Jain, S.&nbsp;Narvekar, J.&nbsp;Wang, R.&nbsp;Wu, and
C.&nbsp;Boutilier, “Recsim: A configurable simulation platform for recommender
systems,” <em id="bib.bib820.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1909.04847, 2019.

</span>
</li>
<li id="bib.bib821" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[821]</span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, Y.&nbsp;Hou, R.&nbsp;Xie, W.&nbsp;Sun, J.&nbsp;J. McAuley, W.&nbsp;X. Zhao, L.&nbsp;Lin, and
J.&nbsp;Wen, “Agentcf: Collaborative learning with autonomous language agents for
recommender systems,” <em id="bib.bib821.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.09233, 2023.

</span>
</li>
<li id="bib.bib822" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[822]</span>
<span class="ltx_bibblock">
A.&nbsp;Zhang, L.&nbsp;Sheng, Y.&nbsp;Chen, H.&nbsp;Li, Y.&nbsp;Deng, X.&nbsp;Wang, and T.&nbsp;Chua, “On
generative agents in recommendation,” <em id="bib.bib822.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.10108,
2023.

</span>
</li>
<li id="bib.bib823" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[823]</span>
<span class="ltx_bibblock">
Y.&nbsp;Du, Z.&nbsp;Liu, J.&nbsp;Li, and W.&nbsp;X. Zhao, “A survey of vision-language pre-trained
models,” in <em id="bib.bib823.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-First International Joint
Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29
July 2022</em>, L.&nbsp;D. Raedt, Ed.&nbsp;&nbsp;&nbsp;ijcai.org, 2022, pp. 5436–5443.

</span>
</li>
<li id="bib.bib824" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[824]</span>
<span class="ltx_bibblock">
Z.&nbsp;Gan, L.&nbsp;Li, C.&nbsp;Li, L.&nbsp;Wang, Z.&nbsp;Liu, and J.&nbsp;Gao, “Vision-language
pre-training: Basics, recent advances, and future trends,” <em id="bib.bib824.1.1" class="ltx_emph ltx_font_italic">Found.
Trends Comput. Graph. Vis.</em>, vol.&nbsp;14, no. 3-4, pp. 163–352, 2022.

</span>
</li>
<li id="bib.bib825" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[825]</span>
<span class="ltx_bibblock">
P.&nbsp;K. Rubenstein, C.&nbsp;Asawaroengchai, D.&nbsp;D. Nguyen, A.&nbsp;Bapna, Z.&nbsp;Borsos,
F.&nbsp;de&nbsp;Chaumont&nbsp;Quitry, P.&nbsp;Chen, D.&nbsp;E. Badawy, W.&nbsp;Han, E.&nbsp;Kharitonov
<em id="bib.bib825.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Audiopalm: A large language model that can speak and
listen,” <em id="bib.bib825.2.2" class="ltx_emph ltx_font_italic">CoRR</em>, 2023.

</span>
</li>
<li id="bib.bib826" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[826]</span>
<span class="ltx_bibblock">
J.&nbsp;Alayrac, J.&nbsp;Donahue, P.&nbsp;Luc, A.&nbsp;Miech, I.&nbsp;Barr, Y.&nbsp;Hasson, K.&nbsp;Lenc,
A.&nbsp;Mensch, K.&nbsp;Millican, M.&nbsp;Reynolds, R.&nbsp;Ring, E.&nbsp;Rutherford, S.&nbsp;Cabi, T.&nbsp;Han,
Z.&nbsp;Gong, S.&nbsp;Samangooei, M.&nbsp;Monteiro, J.&nbsp;L. Menick, S.&nbsp;Borgeaud, A.&nbsp;Brock,
A.&nbsp;Nematzadeh, S.&nbsp;Sharifzadeh, M.&nbsp;Binkowski, R.&nbsp;Barreira, O.&nbsp;Vinyals,
A.&nbsp;Zisserman, and K.&nbsp;Simonyan, “Flamingo: a visual language model for
few-shot learning,” in <em id="bib.bib826.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib827" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[827]</span>
<span class="ltx_bibblock">
C.&nbsp;Schuhmann, R.&nbsp;Beaumont, R.&nbsp;Vencu, C.&nbsp;Gordon, R.&nbsp;Wightman, M.&nbsp;Cherti,
T.&nbsp;Coombes, A.&nbsp;Katta, C.&nbsp;Mullis, M.&nbsp;Wortsman, P.&nbsp;Schramowski, S.&nbsp;Kundurthy,
K.&nbsp;Crowson, L.&nbsp;Schmidt, R.&nbsp;Kaczmarczyk, and J.&nbsp;Jitsev, “LAION-5B: an open
large-scale dataset for training next generation image-text models,” in
<em id="bib.bib827.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib828" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[828]</span>
<span class="ltx_bibblock">
S.&nbsp;Changpinyo, P.&nbsp;Sharma, N.&nbsp;Ding, and R.&nbsp;Soricut, “Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts,”
in <em id="bib.bib828.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2021, virtual, June 19-25, 2021</em>.&nbsp;&nbsp;&nbsp;Computer Vision Foundation / IEEE, 2021, pp. 3558–3568.

</span>
</li>
<li id="bib.bib829" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[829]</span>
<span class="ltx_bibblock">
Q.&nbsp;Ye, H.&nbsp;Xu, G.&nbsp;Xu, J.&nbsp;Ye, M.&nbsp;Yan, Y.&nbsp;Zhou, J.&nbsp;Wang, A.&nbsp;Hu, P.&nbsp;Shi, Y.&nbsp;Shi,
C.&nbsp;Li, Y.&nbsp;Xu, H.&nbsp;Chen, J.&nbsp;Tian, Q.&nbsp;Qi, J.&nbsp;Zhang, and F.&nbsp;Huang, “mplug-owl:
Modularization empowers large language models with multimodality,”
<em id="bib.bib829.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.14178, 2023.

</span>
</li>
<li id="bib.bib830" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[830]</span>
<span class="ltx_bibblock">
J.&nbsp;Bai, S.&nbsp;Bai, S.&nbsp;Yang, S.&nbsp;Wang, S.&nbsp;Tan, P.&nbsp;Wang, J.&nbsp;Lin, C.&nbsp;Zhou, and
J.&nbsp;Zhou, “Qwen-vl: A frontier large vision-language model with versatile
abilities,” <em id="bib.bib830.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.12966, 2023.

</span>
</li>
<li id="bib.bib831" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[831]</span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Y.&nbsp;Li, and Y.&nbsp;J. Lee, “Improved baselines with visual
instruction tuning,” <em id="bib.bib831.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.03744, 2023.

</span>
</li>
<li id="bib.bib832" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[832]</span>
<span class="ltx_bibblock">
P.&nbsp;Zhang, X.&nbsp;Dong, B.&nbsp;Wang, Y.&nbsp;Cao, C.&nbsp;Xu, L.&nbsp;Ouyang, Z.&nbsp;Zhao, S.&nbsp;Ding,
S.&nbsp;Zhang, H.&nbsp;Duan, W.&nbsp;Zhang, H.&nbsp;Yan, X.&nbsp;Zhang, W.&nbsp;Li, J.&nbsp;Li, K.&nbsp;Chen, C.&nbsp;He,
X.&nbsp;Zhang, Y.&nbsp;Qiao, D.&nbsp;Lin, and J.&nbsp;Wang, “Internlm-xcomposer: A
vision-language large model for advanced text-image comprehension and
composition,” <em id="bib.bib832.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.15112, 2023.

</span>
</li>
<li id="bib.bib833" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[833]</span>
<span class="ltx_bibblock">
K.&nbsp;Chen, Z.&nbsp;Zhang, W.&nbsp;Zeng, R.&nbsp;Zhang, F.&nbsp;Zhu, and R.&nbsp;Zhao, “Shikra: Unleashing
multimodal llm’s referential dialogue magic,” <em id="bib.bib833.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.15195, 2023.

</span>
</li>
<li id="bib.bib834" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[834]</span>
<span class="ltx_bibblock">
F.&nbsp;Liu, K.&nbsp;Lin, L.&nbsp;Li, J.&nbsp;Wang, Y.&nbsp;Yacoob, and L.&nbsp;Wang, “Aligning large
multi-modal model with robust instruction tuning,” <em id="bib.bib834.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.14565, 2023.

</span>
</li>
<li id="bib.bib835" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[835]</span>
<span class="ltx_bibblock">
Y.&nbsp;Du, H.&nbsp;Guo, K.&nbsp;Zhou, W.&nbsp;X. Zhao, J.&nbsp;Wang, C.&nbsp;Wang, M.&nbsp;Cai, R.&nbsp;Song, and
J.-R. Wen, “What makes for good visual instructions? synthesizing complex
visual reasoning instructions for visual instruction tuning,” 2023.

</span>
</li>
<li id="bib.bib836" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[836]</span>
<span class="ltx_bibblock">
D.&nbsp;Gurari, Q.&nbsp;Li, A.&nbsp;J. Stangl, A.&nbsp;Guo, C.&nbsp;Lin, K.&nbsp;Grauman, J.&nbsp;Luo, and J.&nbsp;P.
Bigham, “Vizwiz grand challenge: Answering visual questions from blind
people,” in <em id="bib.bib836.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.&nbsp;&nbsp;&nbsp;Computer
Vision Foundation / IEEE Computer Society, 2018, pp. 3608–3617.

</span>
</li>
<li id="bib.bib837" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[837]</span>
<span class="ltx_bibblock">
A.&nbsp;Mishra, K.&nbsp;Alahari, and C.&nbsp;V. Jawahar, “Top-down and bottom-up cues for
scene text recognition,” in <em id="bib.bib837.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.&nbsp;&nbsp;&nbsp;IEEE Computer Society, 2012, pp. 2687–2694.

</span>
</li>
<li id="bib.bib838" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[838]</span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, H.&nbsp;Duan, Y.&nbsp;Zhang, B.&nbsp;Li, S.&nbsp;Zhang, W.&nbsp;Zhao, Y.&nbsp;Yuan, J.&nbsp;Wang, C.&nbsp;He,
Z.&nbsp;Liu, K.&nbsp;Chen, and D.&nbsp;Lin, “Mmbench: Is your multi-modal model an
all-around player?” <em id="bib.bib838.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.06281, 2023.

</span>
</li>
<li id="bib.bib839" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[839]</span>
<span class="ltx_bibblock">
C.&nbsp;Fu, P.&nbsp;Chen, Y.&nbsp;Shen, Y.&nbsp;Qin, M.&nbsp;Zhang, X.&nbsp;Lin, Z.&nbsp;Qiu, W.&nbsp;Lin, J.&nbsp;Yang,
X.&nbsp;Zheng, K.&nbsp;Li, X.&nbsp;Sun, and R.&nbsp;Ji, “MME: A comprehensive evaluation
benchmark for multimodal large language models,” <em id="bib.bib839.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2306.13394, 2023.

</span>
</li>
<li id="bib.bib840" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[840]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, Y.&nbsp;Li, L.&nbsp;Cui, D.&nbsp;Cai, L.&nbsp;Liu, T.&nbsp;Fu, X.&nbsp;Huang, E.&nbsp;Zhao, Y.&nbsp;Zhang,
Y.&nbsp;Chen, L.&nbsp;Wang, A.&nbsp;T. Luu, W.&nbsp;Bi, F.&nbsp;Shi, and S.&nbsp;Shi, “Siren’s song in the
AI ocean: A survey on hallucination in large language models,”
<em id="bib.bib840.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.01219, 2023.

</span>
</li>
<li id="bib.bib841" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[841]</span>
<span class="ltx_bibblock">
A.&nbsp;Gunjal, J.&nbsp;Yin, and E.&nbsp;Bas, “Detecting and preventing hallucinations in
large vision language models,” <em id="bib.bib841.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.06394, 2023.

</span>
</li>
<li id="bib.bib842" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[842]</span>
<span class="ltx_bibblock">
J.&nbsp;Lu, J.&nbsp;Rao, K.&nbsp;Chen, X.&nbsp;Guo, Y.&nbsp;Zhang, B.&nbsp;Sun, C.&nbsp;Yang, and J.&nbsp;Yang,
“Evaluation and mitigation of agnosia in multimodal large language models,”
<em id="bib.bib842.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.04041, 2023.

</span>
</li>
<li id="bib.bib843" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[843]</span>
<span class="ltx_bibblock">
A.&nbsp;Rohrbach, L.&nbsp;A. Hendricks, K.&nbsp;Burns, T.&nbsp;Darrell, and K.&nbsp;Saenko, “Object
hallucination in image captioning,” in <em id="bib.bib843.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2018, pp.
4035–4045.

</span>
</li>
<li id="bib.bib844" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[844]</span>
<span class="ltx_bibblock">
Y.&nbsp;Li, Y.&nbsp;Du, K.&nbsp;Zhou, J.&nbsp;Wang, W.&nbsp;X. Zhao, and J.-R. Wen, “Evaluating object
hallucination in large vision-language models,” in <em id="bib.bib844.1.1" class="ltx_emph ltx_font_italic">The 2023 Conference
on Empirical Methods in Natural Language Processing</em>, 2023. [Online].
Available: <a target="_blank" href="https://openreview.net/forum?id=xozJw0kZXF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=xozJw0kZXF</a>

</span>
</li>
<li id="bib.bib845" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[845]</span>
<span class="ltx_bibblock">
D.&nbsp;A. Hudson and C.&nbsp;D. Manning, “GQA: A new dataset for real-world visual
reasoning and compositional question answering,” in <em id="bib.bib845.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.&nbsp;&nbsp;&nbsp;Computer Vision Foundation / IEEE, 2019, pp.
6700–6709.

</span>
</li>
<li id="bib.bib846" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[846]</span>
<span class="ltx_bibblock">
P.&nbsp;Lu, S.&nbsp;Mishra, T.&nbsp;Xia, L.&nbsp;Qiu, K.&nbsp;Chang, S.&nbsp;Zhu, O.&nbsp;Tafjord, P.&nbsp;Clark, and
A.&nbsp;Kalyan, “Learn to explain: Multimodal reasoning via thought chains for
science question answering,” in <em id="bib.bib846.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib847" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[847]</span>
<span class="ltx_bibblock">
A.&nbsp;Singh, V.&nbsp;Natarjan, M.&nbsp;Shah, Y.&nbsp;Jiang, X.&nbsp;Chen, D.&nbsp;Parikh, and M.&nbsp;Rohrbach,
“Towards vqa models that can read,” in <em id="bib.bib847.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 8317–8326.

</span>
</li>
<li id="bib.bib848" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[848]</span>
<span class="ltx_bibblock">
F.&nbsp;Liu, T.&nbsp;Guan, Z.&nbsp;Li, L.&nbsp;Chen, Y.&nbsp;Yacoob, D.&nbsp;Manocha, and T.&nbsp;Zhou,
“Hallusionbench: You see what you think? or you think what you see? an
image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5,
and other multi-modality models,” <em id="bib.bib848.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.14566, 2023.

</span>
</li>
<li id="bib.bib849" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[849]</span>
<span class="ltx_bibblock">
S.&nbsp;Antol, A.&nbsp;Agrawal, J.&nbsp;Lu, M.&nbsp;Mitchell, D.&nbsp;Batra, C.&nbsp;L. Zitnick, and
D.&nbsp;Parikh, “VQA: visual question answering,” in <em id="bib.bib849.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.&nbsp;&nbsp;&nbsp;IEEE Computer Society, 2015, pp. 2425–2433.

</span>
</li>
<li id="bib.bib850" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[850]</span>
<span class="ltx_bibblock">
R.&nbsp;Vedantam, C.&nbsp;L. Zitnick, and D.&nbsp;Parikh, “Cider: Consensus-based image
description evaluation,” in <em id="bib.bib850.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.&nbsp;&nbsp;&nbsp;IEEE Computer Society, 2015, pp. 4566–4575.

</span>
</li>
<li id="bib.bib851" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[851]</span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee, “Visual instruction tuning,”
<em id="bib.bib851.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.08485, 2023.

</span>
</li>
<li id="bib.bib852" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[852]</span>
<span class="ltx_bibblock">
P.&nbsp;Xu, W.&nbsp;Shao, K.&nbsp;Zhang, P.&nbsp;Gao, S.&nbsp;Liu, M.&nbsp;Lei, F.&nbsp;Meng, S.&nbsp;Huang, Y.&nbsp;Qiao,
and P.&nbsp;Luo, “Lvlm-ehub: A comprehensive evaluation benchmark for large
vision-language models,” <em id="bib.bib852.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.09265, 2023.

</span>
</li>
<li id="bib.bib853" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[853]</span>
<span class="ltx_bibblock">
Z.&nbsp;Li, Y.&nbsp;Wang, M.&nbsp;Du, Q.&nbsp;Liu, B.&nbsp;Wu, J.&nbsp;Zhang, C.&nbsp;Zhou, Z.&nbsp;Fan, J.&nbsp;Fu,
J.&nbsp;Chen, X.&nbsp;Huang, and Z.&nbsp;Wei, “Reform-eval: Evaluating large vision
language models via unified re-formulation of task-oriented benchmarks,”
<em id="bib.bib853.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.02569, 2023.

</span>
</li>
<li id="bib.bib854" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[854]</span>
<span class="ltx_bibblock">
B.&nbsp;Li, R.&nbsp;Wang, G.&nbsp;Wang, Y.&nbsp;Ge, Y.&nbsp;Ge, and Y.&nbsp;Shan, “Seed-bench: Benchmarking
multimodal llms with generative comprehension,” <em id="bib.bib854.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2307.16125, 2023.

</span>
</li>
<li id="bib.bib855" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[855]</span>
<span class="ltx_bibblock">
W.&nbsp;Yu, Z.&nbsp;Yang, L.&nbsp;Li, J.&nbsp;Wang, K.&nbsp;Lin, Z.&nbsp;Liu, X.&nbsp;Wang, and L.&nbsp;Wang, “Mm-vet:
Evaluating large multimodal models for integrated capabilities,”
<em id="bib.bib855.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.02490, 2023.

</span>
</li>
<li id="bib.bib856" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[856]</span>
<span class="ltx_bibblock">
J.&nbsp;Wang, L.&nbsp;Meng, Z.&nbsp;Weng, B.&nbsp;He, Z.&nbsp;Wu, and Y.&nbsp;Jiang, “To see is to believe:
Prompting GPT-4V for better visual instruction tuning,” <em id="bib.bib856.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2311.07574, 2023.

</span>
</li>
<li id="bib.bib857" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[857]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, R.&nbsp;Zhang, J.&nbsp;Gu, Y.&nbsp;Zhou, N.&nbsp;Lipka, D.&nbsp;Yang, and T.&nbsp;Sun, “Llavar:
Enhanced visual instruction tuning for text-rich image understanding,”
<em id="bib.bib857.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.17107</em>, 2023.

</span>
</li>
<li id="bib.bib858" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[858]</span>
<span class="ltx_bibblock">
X.&nbsp;Qi, K.&nbsp;Huang, A.&nbsp;Panda, M.&nbsp;Wang, and P.&nbsp;Mittal, “Visual adversarial
examples jailbreak aligned large language models,” in <em id="bib.bib858.1.1" class="ltx_emph ltx_font_italic">The Second
Workshop on New Frontiers in Adversarial Machine Learning</em>, 2023.

</span>
</li>
<li id="bib.bib859" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[859]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhou, C.&nbsp;Cui, J.&nbsp;Yoon, L.&nbsp;Zhang, Z.&nbsp;Deng, C.&nbsp;Finn, M.&nbsp;Bansal, and H.&nbsp;Yao,
“Analyzing and mitigating object hallucination in large vision-language
models,” <em id="bib.bib859.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.00754</em>, 2023.

</span>
</li>
<li id="bib.bib860" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[860]</span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, S.&nbsp;Shen, S.&nbsp;Cao, H.&nbsp;Liu, C.&nbsp;Li, Y.&nbsp;Shen, C.&nbsp;Gan, L.-Y. Gui, Y.-X. Wang,
Y.&nbsp;Yang <em id="bib.bib860.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Aligning large multimodal models with factually
augmented rlhf,” <em id="bib.bib860.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.14525</em>, 2023.

</span>
</li>
<li id="bib.bib861" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[861]</span>
<span class="ltx_bibblock">
S.&nbsp;Pan, L.&nbsp;Luo, Y.&nbsp;Wang, C.&nbsp;Chen, J.&nbsp;Wang, and X.&nbsp;Wu, “Unifying large language
models and knowledge graphs: A roadmap,” <em id="bib.bib861.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.08302,
2023.

</span>
</li>
<li id="bib.bib862" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[862]</span>
<span class="ltx_bibblock">
E.&nbsp;Jiménez-Ruiz, O.&nbsp;Hassanzadeh, V.&nbsp;Efthymiou, J.&nbsp;Chen, and
K.&nbsp;Srinivas, “Semtab 2019: Resources to benchmark tabular data to knowledge
graph matching systems,” in <em id="bib.bib862.1.1" class="ltx_emph ltx_font_italic">The Semantic Web - 17th International
Conference, ESWC 2020, Heraklion, Crete, Greece, May 31-June 4, 2020,
Proceedings</em>, ser. Lecture Notes in Computer Science, vol. 12123.&nbsp;&nbsp;&nbsp;Springer, 2020, pp. 514–530.

</span>
</li>
<li id="bib.bib863" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[863]</span>
<span class="ltx_bibblock">
Y.&nbsp;Sun, S.&nbsp;Wang, S.&nbsp;Feng, S.&nbsp;Ding, C.&nbsp;Pang, J.&nbsp;Shang, J.&nbsp;Liu, X.&nbsp;Chen, Y.&nbsp;Zhao,
Y.&nbsp;Lu, W.&nbsp;Liu, Z.&nbsp;Wu, W.&nbsp;Gong, J.&nbsp;Liang, Z.&nbsp;Shang, P.&nbsp;Sun, W.&nbsp;Liu, X.&nbsp;Ouyang,
D.&nbsp;Yu, H.&nbsp;Tian, H.&nbsp;Wu, and H.&nbsp;Wang, “ERNIE 3.0: Large-scale knowledge
enhanced pre-training for language understanding and generation,”
<em id="bib.bib863.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2107.02137, 2021. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2107.02137" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2107.02137</a>

</span>
</li>
<li id="bib.bib864" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[864]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, X.&nbsp;Han, Z.&nbsp;Liu, X.&nbsp;Jiang, M.&nbsp;Sun, and Q.&nbsp;Liu, “ERNIE: enhanced
language representation with informative entities,” in <em id="bib.bib864.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 57th Conference of the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics,
2019, pp. 1441–1451.

</span>
</li>
<li id="bib.bib865" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[865]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, T.&nbsp;Gao, Z.&nbsp;Zhu, Z.&nbsp;Zhang, Z.&nbsp;Liu, J.&nbsp;Li, and J.&nbsp;Tang, “KEPLER: A
unified model for knowledge embedding and pre-trained language
representation,” <em id="bib.bib865.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, vol.&nbsp;9, pp.
176–194, 2021.

</span>
</li>
<li id="bib.bib866" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[866]</span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, X.&nbsp;Zhang, J.&nbsp;Yu, J.&nbsp;Tang, J.&nbsp;Tang, C.&nbsp;Li, and H.&nbsp;Chen, “Subgraph
retrieval enhanced model for multi-hop knowledge base question answering,”
in <em id="bib.bib866.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022</em>.&nbsp;&nbsp;&nbsp;Association
for Computational Linguistics, 2022, pp. 5773–5784.

</span>
</li>
<li id="bib.bib867" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[867]</span>
<span class="ltx_bibblock">
P.&nbsp;Ke, H.&nbsp;Ji, Y.&nbsp;Ran, X.&nbsp;Cui, L.&nbsp;Wang, L.&nbsp;Song, X.&nbsp;Zhu, and M.&nbsp;Huang,
“Jointgt: Graph-text joint representation learning for text generation from
knowledge graphs,” in <em id="bib.bib867.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational
Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser.
Findings of ACL, vol. ACL/IJCNLP 2021.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 2526–2538.

</span>
</li>
<li id="bib.bib868" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[868]</span>
<span class="ltx_bibblock">
O.&nbsp;Agarwal, H.&nbsp;Ge, S.&nbsp;Shakeri, and R.&nbsp;Al-Rfou, “Large scale knowledge graph
based synthetic corpus generation for knowledge-enhanced language model
pre-training,” <em id="bib.bib868.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2010.12688, 2020.

</span>
</li>
<li id="bib.bib869" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[869]</span>
<span class="ltx_bibblock">
W.&nbsp;Chen, Y.&nbsp;Su, X.&nbsp;Yan, and W.&nbsp;Y. Wang, “KGPT: knowledge-grounded
pre-training for data-to-text generation,” in <em id="bib.bib869.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020, pp. 8635–8648.

</span>
</li>
<li id="bib.bib870" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[870]</span>
<span class="ltx_bibblock">
Y.&nbsp;Gu, X.&nbsp;Deng, and Y.&nbsp;Su, “Don’t generate, discriminate: A proposal for
grounding language models to real-world environments,” in <em id="bib.bib870.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics,
2023, pp. 4928–4949.

</span>
</li>
<li id="bib.bib871" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[871]</span>
<span class="ltx_bibblock">
L.&nbsp;Luo, Y.&nbsp;Li, G.&nbsp;Haffari, and S.&nbsp;Pan, “Reasoning on graphs: Faithful and
interpretable large language model reasoning,” <em id="bib.bib871.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2310.01061, 2023.

</span>
</li>
<li id="bib.bib872" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[872]</span>
<span class="ltx_bibblock">
Y.&nbsp;Lan and J.&nbsp;Jiang, “Query graph generation for answering multi-hop complex
questions from knowledge bases,” in <em id="bib.bib872.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, ACL 2020, Online,
July 5-10, 2020</em>, D.&nbsp;J. and, Ed.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2020, pp. 969–974.

</span>
</li>
<li id="bib.bib873" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[873]</span>
<span class="ltx_bibblock">
P.&nbsp;Wang, N.&nbsp;Zhang, X.&nbsp;Xie, Y.&nbsp;Yao, B.&nbsp;Tian, M.&nbsp;Wang, Z.&nbsp;Xi, S.&nbsp;Cheng, K.&nbsp;Liu,
G.&nbsp;Zheng, and H.&nbsp;Chen, “Easyedit: An easy-to-use knowledge editing framework
for large language models,” <em id="bib.bib873.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.07269, 2023.

</span>
</li>
<li id="bib.bib874" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[874]</span>
<span class="ltx_bibblock">
Y.&nbsp;Yao, P.&nbsp;Wang, B.&nbsp;Tian, S.&nbsp;Cheng, Z.&nbsp;Li, S.&nbsp;Deng, H.&nbsp;Chen, and N.&nbsp;Zhang,
“Editing large language models: Problems, methods, and opportunities,”
<em id="bib.bib874.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13172, 2023.

</span>
</li>
<li id="bib.bib875" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[875]</span>
<span class="ltx_bibblock">
S.&nbsp;Choi, T.&nbsp;Fang, Z.&nbsp;Wang, and Y.&nbsp;Song, “KCTS: knowledge-constrained tree
search decoding with token-level hallucination detection,” <em id="bib.bib875.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2310.09044, 2023.

</span>
</li>
<li id="bib.bib876" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[876]</span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, L.&nbsp;Pan, J.&nbsp;Zhao, and W.&nbsp;Y. Wang, “Mitigating language model
hallucination with interactive question-knowledge alignment,” <em id="bib.bib876.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.13669, 2023.

</span>
</li>
<li id="bib.bib877" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[877]</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhu, X.&nbsp;Wang, J.&nbsp;Chen, S.&nbsp;Qiao, Y.&nbsp;Ou, Y.&nbsp;Yao, S.&nbsp;Deng, H.&nbsp;Chen, and
N.&nbsp;Zhang, “Llms for knowledge graph construction and reasoning: Recent
capabilities and future opportunities,” <em id="bib.bib877.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13168,
2023. [Online]. Available: <a target="_blank" href="https://doi.org/10.48550/arXiv.2305.13168" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2305.13168</a>

</span>
</li>
<li id="bib.bib878" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[878]</span>
<span class="ltx_bibblock">
S.&nbsp;Russell and P.&nbsp;Norvig, <em id="bib.bib878.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence: A Modern Approach
(4th Edition)</em>.&nbsp;&nbsp;&nbsp;Pearson, 2020.
[Online]. Available: <a target="_blank" href="http://aima.cs.berkeley.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://aima.cs.berkeley.edu/</a>

</span>
</li>
<li id="bib.bib879" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[879]</span>
<span class="ltx_bibblock">
B.&nbsp;M. Lake, T.&nbsp;D. Ullman, J.&nbsp;B. Tenenbaum, and S.&nbsp;J. Gershman, “Building
machines that learn and think like people,” <em id="bib.bib879.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/1604.00289, 2016.

</span>
</li>
<li id="bib.bib880" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[880]</span>
<span class="ltx_bibblock">
S.&nbsp;Yao, J.&nbsp;Zhao, D.&nbsp;Yu, N.&nbsp;Du, I.&nbsp;Shafran, K.&nbsp;Narasimhan, and Y.&nbsp;Cao, “React:
Synergizing reasoning and acting in language models,” <em id="bib.bib880.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2210.03629, 2022.

</span>
</li>
<li id="bib.bib881" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[881]</span>
<span class="ltx_bibblock">
2023. [Online]. Available: <a target="_blank" href="https://github.com/AntonOsika/gpt-engineer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AntonOsika/gpt-engineer</a>

</span>
</li>
<li id="bib.bib882" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[882]</span>
<span class="ltx_bibblock">
X.&nbsp;Team, “Xagent: An autonomous agent for complex task solving,” 2023.

</span>
</li>
<li id="bib.bib883" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[883]</span>
<span class="ltx_bibblock">
G.&nbsp;Li, H.&nbsp;A. A.&nbsp;K. Hammoud, H.&nbsp;Itani, D.&nbsp;Khizbullin, and B.&nbsp;Ghanem, “CAMEL:
communicative agents for ”mind” exploration of large scale language model
society,” <em id="bib.bib883.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.17760, 2023.

</span>
</li>
<li id="bib.bib884" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[884]</span>
<span class="ltx_bibblock">
S.&nbsp;Hong, X.&nbsp;Zheng, J.&nbsp;Chen, Y.&nbsp;Cheng, J.&nbsp;Wang, C.&nbsp;Zhang, Z.&nbsp;Wang, S.&nbsp;K.&nbsp;S. Yau,
Z.&nbsp;Lin, L.&nbsp;Zhou, C.&nbsp;Ran, L.&nbsp;Xiao, and C.&nbsp;Wu, “Metagpt: Meta programming for
multi-agent collaborative framework,” <em id="bib.bib884.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.00352,
2023.

</span>
</li>
<li id="bib.bib885" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[885]</span>
<span class="ltx_bibblock">
C.&nbsp;Pham, B.&nbsp;Liu, Y.&nbsp;Yang, Z.&nbsp;Chen, T.&nbsp;Liu, J.&nbsp;Yuan, B.&nbsp;A. Plummer, Z.&nbsp;Wang, and
H.&nbsp;Yang, “Let models speak ciphers: Multiagent debate through embeddings,”
<em id="bib.bib885.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.06272, 2023.

</span>
</li>
<li id="bib.bib886" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[886]</span>
<span class="ltx_bibblock">
Y.&nbsp;Du, S.&nbsp;Li, A.&nbsp;Torralba, J.&nbsp;B. Tenenbaum, and I.&nbsp;Mordatch, “Improving
factuality and reasoning in language models through multiagent debate,”
<em id="bib.bib886.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.14325, 2023.

</span>
</li>
<li id="bib.bib887" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[887]</span>
<span class="ltx_bibblock">
M.&nbsp;Karpinska, N.&nbsp;Akoury, and M.&nbsp;Iyyer, “The perils of using mechanical turk to
evaluate open-ended text generation,” in <em id="bib.bib887.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>,
M.&nbsp;Moens, X.&nbsp;Huang, L.&nbsp;Specia, and S.&nbsp;W. Yih, Eds.&nbsp;&nbsp;&nbsp;Association for Computational Linguistics, 2021, pp. 1265–1285.

</span>
</li>
<li id="bib.bib888" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[888]</span>
<span class="ltx_bibblock">
H.&nbsp;Lee, S.&nbsp;Phatale, H.&nbsp;Mansoor, K.&nbsp;Lu, T.&nbsp;Mesnard, C.&nbsp;Bishop, V.&nbsp;Carbune, and
A.&nbsp;Rastogi, “RLAIF: scaling reinforcement learning from human feedback
with AI feedback,” <em id="bib.bib888.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.00267, 2023.

</span>
</li>
<li id="bib.bib889" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[889]</span>
<span class="ltx_bibblock">
T.&nbsp;Wang, P.&nbsp;Yu, X.&nbsp;E. Tan, S.&nbsp;O’Brien, R.&nbsp;Pasunuru, J.&nbsp;Dwivedi-Yu,
O.&nbsp;Golovneva, L.&nbsp;Zettlemoyer, M.&nbsp;Fazel-Zarandi, and A.&nbsp;Celikyilmaz,
“Shepherd: A critic for language model generation,” <em id="bib.bib889.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2308.04592, 2023.

</span>
</li>
<li id="bib.bib890" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[890]</span>
<span class="ltx_bibblock">
G.&nbsp;Cui, L.&nbsp;Yuan, N.&nbsp;Ding, G.&nbsp;Yao, W.&nbsp;Zhu, Y.&nbsp;Ni, G.&nbsp;Xie, Z.&nbsp;Liu, and M.&nbsp;Sun,
“Ultrafeedback: Boosting language models with high-quality feedback,”
<em id="bib.bib890.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.01377, 2023.

</span>
</li>
<li id="bib.bib891" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[891]</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, Z.&nbsp;Wang, J.&nbsp;Liu, Y.&nbsp;Chen, L.&nbsp;Yuan, H.&nbsp;Peng, and H.&nbsp;Ji, “MINT:
evaluating llms in multi-turn interaction with tools and language feedback,”
<em id="bib.bib891.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2309.10691, 2023.

</span>
</li>
<li id="bib.bib892" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[892]</span>
<span class="ltx_bibblock">
S.&nbsp;Saha, O.&nbsp;Levy, A.&nbsp;Celikyilmaz, M.&nbsp;Bansal, J.&nbsp;Weston, and X.&nbsp;Li,
“Branch-solve-merge improves large language model evaluation and
generation,” <em id="bib.bib892.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.15123, 2023.

</span>
</li>
<li id="bib.bib893" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[893]</span>
<span class="ltx_bibblock">
X.&nbsp;Zhang, B.&nbsp;Yu, H.&nbsp;Yu, Y.&nbsp;Lv, T.&nbsp;Liu, F.&nbsp;Huang, H.&nbsp;Xu, and Y.&nbsp;Li, “Wider and
deeper LLM networks are fairer LLM evaluators,” <em id="bib.bib893.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2308.01862, 2023.

</span>
</li>
<li id="bib.bib894" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[894]</span>
<span class="ltx_bibblock">
C.&nbsp;Chan, W.&nbsp;Chen, Y.&nbsp;Su, J.&nbsp;Yu, W.&nbsp;Xue, S.&nbsp;Zhang, J.&nbsp;Fu, and Z.&nbsp;Liu,
“Chateval: Towards better llm-based evaluators through multi-agent debate,”
<em id="bib.bib894.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2308.07201, 2023.

</span>
</li>
<li id="bib.bib895" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[895]</span>
<span class="ltx_bibblock">
R.&nbsp;Li, T.&nbsp;Patel, and X.&nbsp;Du, “PRD: peer rank and discussion improve large
language model based evaluations,” <em id="bib.bib895.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2307.02762, 2023.

</span>
</li>
<li id="bib.bib896" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[896]</span>
<span class="ltx_bibblock">
L.&nbsp;Zhu, X.&nbsp;Wang, and X.&nbsp;Wang, “Judgelm: Fine-tuned large language models are
scalable judges,” <em id="bib.bib896.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2310.17631, 2023.

</span>
</li>
<li id="bib.bib897" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[897]</span>
<span class="ltx_bibblock">
Z.&nbsp;Zeng, J.&nbsp;Yu, T.&nbsp;Gao, Y.&nbsp;Meng, T.&nbsp;Goyal, and D.&nbsp;Chen, “Evaluating large
language models at evaluating instruction following,” <em id="bib.bib897.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2310.07641, 2023.

</span>
</li>
<li id="bib.bib898" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[898]</span>
<span class="ltx_bibblock">
R.&nbsp;Koo, M.&nbsp;Lee, V.&nbsp;Raheja, J.&nbsp;I. Park, Z.&nbsp;M. Kim, and D.&nbsp;Kang, “Benchmarking
cognitive biases in large language models as evaluators,” <em id="bib.bib898.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2309.17012, 2023.

</span>
</li>
<li id="bib.bib899" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[899]</span>
<span class="ltx_bibblock">
P.&nbsp;West, X.&nbsp;Lu, N.&nbsp;Dziri, F.&nbsp;Brahman, L.&nbsp;Li, J.&nbsp;D. Hwang, L.&nbsp;Jiang, J.&nbsp;Fisher,
A.&nbsp;Ravichander, K.&nbsp;Chandu, B.&nbsp;Newman, P.&nbsp;W. Koh, A.&nbsp;Ettinger, and Y.&nbsp;Choi,
“The generative AI paradox: ”what it can create, it may not understand”,”
<em id="bib.bib899.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2311.00059, 2023.

</span>
</li>
<li id="bib.bib900" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[900]</span>
<span class="ltx_bibblock">
J.&nbsp;Huang, X.&nbsp;Chen, S.&nbsp;Mishra, H.&nbsp;S. Zheng, A.&nbsp;W. Yu, X.&nbsp;Song, and D.&nbsp;Zhou,
“Large language models cannot self-correct reasoning yet,” <em id="bib.bib900.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2310.01798, 2023.

</span>
</li>
<li id="bib.bib901" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[901]</span>
<span class="ltx_bibblock">
K.&nbsp;Stechly, M.&nbsp;Marquez, and S.&nbsp;Kambhampati, “GPT-4 doesn’t know it’s wrong:
An analysis of iterative prompting for reasoning problems,” <em id="bib.bib901.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2310.12397, 2023.

</span>
</li>
<li id="bib.bib902" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[902]</span>
<span class="ltx_bibblock">
O.&nbsp;Nov, N.&nbsp;Singh, and D.&nbsp;M. Mann, “Putting chatgpt’s medical advice to the
(turing) test,” <em id="bib.bib902.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2301.10035, 2023.

</span>
</li>
<li id="bib.bib903" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[903]</span>
<span class="ltx_bibblock">
K.&nbsp;Yang, S.&nbsp;Ji, T.&nbsp;Zhang, Q.&nbsp;Xie, and S.&nbsp;Ananiadou, “On the evaluations of
chatgpt and emotion-enhanced prompting for mental health analysis,”
<em id="bib.bib903.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.03347, 2023.

</span>
</li>
<li id="bib.bib904" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[904]</span>
<span class="ltx_bibblock">
K.&nbsp;Jeblick, B.&nbsp;Schachtner, J.&nbsp;Dexl, A.&nbsp;Mittermeier, A.&nbsp;T. Stüber,
J.&nbsp;Topalis, T.&nbsp;Weber, P.&nbsp;Wesp, B.&nbsp;O. Sabel, J.&nbsp;Ricke, and M.&nbsp;Ingrisch,
“Chatgpt makes medicine easy to swallow: An exploratory case study on
simplified radiology reports,” <em id="bib.bib904.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.14882, 2022.

</span>
</li>
<li id="bib.bib905" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[905]</span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, T.&nbsp;Tu, J.&nbsp;Gottweis, R.&nbsp;Sayres, E.&nbsp;Wulczyn, L.&nbsp;Hou, K.&nbsp;Clark,
S.&nbsp;Pfohl, H.&nbsp;Cole-Lewis, D.&nbsp;Neal, M.&nbsp;Schaekermann, A.&nbsp;Wang, M.&nbsp;Amin,
S.&nbsp;Lachgar, P.&nbsp;A. Mansfield, S.&nbsp;Prakash, B.&nbsp;Green, E.&nbsp;Dominowska, B.&nbsp;A.
y&nbsp;Arcas, N.&nbsp;Tomasev, Y.&nbsp;Liu, R.&nbsp;Wong, C.&nbsp;Semturs, S.&nbsp;S. Mahdavi, J.&nbsp;K.
Barral, D.&nbsp;R. Webster, G.&nbsp;S. Corrado, Y.&nbsp;Matias, S.&nbsp;Azizi,
A.&nbsp;Karthikesalingam, and V.&nbsp;Natarajan, “Towards expert-level medical
question answering with large language models,” <em id="bib.bib905.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.09617, 2023.

</span>
</li>
<li id="bib.bib906" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[906]</span>
<span class="ltx_bibblock">
S.&nbsp;Yang, H.&nbsp;Zhao, S.&nbsp;Zhu, G.&nbsp;Zhou, H.&nbsp;Xu, Y.&nbsp;Jia, and H.&nbsp;Zan, “Zhongjing:
Enhancing the chinese medical capabilities of large language model through
expert feedback and real-world multi-turn dialogue,” <em id="bib.bib906.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2308.03549, 2023.

</span>
</li>
<li id="bib.bib907" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[907]</span>
<span class="ltx_bibblock">
S.&nbsp;Chen, B.&nbsp;H. Kann, M.&nbsp;B. Foote, H.&nbsp;J. Aerts, G.&nbsp;K. Savova, R.&nbsp;H. Mak, and
D.&nbsp;S. Bitterman, “The utility of chatgpt for cancer treatment information,”
<em id="bib.bib907.1.1" class="ltx_emph ltx_font_italic">medRxiv</em>, 2023.

</span>
</li>
<li id="bib.bib908" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[908]</span>
<span class="ltx_bibblock">
K.&nbsp;Malinka, M.&nbsp;Peresíni, A.&nbsp;Firc, O.&nbsp;Hujnak, and F.&nbsp;Janus, “On the
educational impact of chatgpt: Is artificial intelligence ready to obtain a
university degree?” <em id="bib.bib908.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.11146, 2023.

</span>
</li>
<li id="bib.bib909" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[909]</span>
<span class="ltx_bibblock">
T.&nbsp;Susnjak, “Chatgpt: The end of online exam integrity?” <em id="bib.bib909.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2212.09292, 2022.

</span>
</li>
<li id="bib.bib910" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[910]</span>
<span class="ltx_bibblock">
K.&nbsp;Tan, T.&nbsp;Pang, and C.&nbsp;Fan, “Towards applying powerful large ai models in
classroom teaching: Opportunities, challenges and prospects,” 2023.

</span>
</li>
<li id="bib.bib911" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[911]</span>
<span class="ltx_bibblock">
F.&nbsp;Kamalov and I.&nbsp;Gurrib, “A new era of artificial intelligence in education:
A multifaceted revolution,” <em id="bib.bib911.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.18303, 2023.

</span>
</li>
<li id="bib.bib912" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[912]</span>
<span class="ltx_bibblock">
E.&nbsp;Kasneci, K.&nbsp;Seßler, S.&nbsp;Küchemann, M.&nbsp;Bannert, D.&nbsp;Dementieva,
F.&nbsp;Fischer, U.&nbsp;Gasser, G.&nbsp;Groh, S.&nbsp;Günnemann, E.&nbsp;Hüllermeier
<em id="bib.bib912.1.1" class="ltx_emph ltx_font_italic">et&nbsp;al.</em>, “Chatgpt for good? on opportunities and challenges of large
language models for education,” <em id="bib.bib912.2.2" class="ltx_emph ltx_font_italic">Learning and Individual Differences</em>,
vol. 103, p. 102274, 2023.

</span>
</li>
<li id="bib.bib913" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[913]</span>
<span class="ltx_bibblock">
A.&nbsp;Blair-Stanek, N.&nbsp;Holzenberger, and B.&nbsp;V. Durme, “Can GPT-3 perform
statutory reasoning?” <em id="bib.bib913.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.06100, 2023.

</span>
</li>
<li id="bib.bib914" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[914]</span>
<span class="ltx_bibblock">
D.&nbsp;Trautmann, A.&nbsp;Petrova, and F.&nbsp;Schilder, “Legal prompt engineering for
multilingual legal judgement prediction,” <em id="bib.bib914.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.02199,
2022.

</span>
</li>
<li id="bib.bib915" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[915]</span>
<span class="ltx_bibblock">
J.&nbsp;H. Choi, K.&nbsp;E. Hickman, A.&nbsp;Monahan, and D.&nbsp;Schwarcz, “Chatgpt goes to law
school,” <em id="bib.bib915.1.1" class="ltx_emph ltx_font_italic">Available at SSRN</em>, 2023.

</span>
</li>
<li id="bib.bib916" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[916]</span>
<span class="ltx_bibblock">
J.&nbsp;J. Nay, “Law informs code: A legal informatics approach to aligning
artificial intelligence with humans,” <em id="bib.bib916.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2209.13020,
2022.

</span>
</li>
<li id="bib.bib917" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[917]</span>
<span class="ltx_bibblock">
F.&nbsp;Yu, L.&nbsp;Quartey, and F.&nbsp;Schilder, “Legal prompting: Teaching a language
model to think like a lawyer,” <em id="bib.bib917.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.01326, 2022.

</span>
</li>
<li id="bib.bib918" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[918]</span>
<span class="ltx_bibblock">
D.&nbsp;Trautmann, A.&nbsp;Petrova, and F.&nbsp;Schilder, “Legal prompt engineering for
multilingual legal judgement prediction,” <em id="bib.bib918.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2212.02199,
2022.

</span>
</li>
<li id="bib.bib919" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[919]</span>
<span class="ltx_bibblock">
A.&nbsp;Tamkin, M.&nbsp;Brundage, J.&nbsp;Clark, and D.&nbsp;Ganguli, “Understanding the
capabilities, limitations, and societal impact of large language models,”
<em id="bib.bib919.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2102.02503, 2021.

</span>
</li>
<li id="bib.bib920" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[920]</span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, “A short survey of viewing large language models in legal aspect,”
<em id="bib.bib920.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.09136, 2023.

</span>
</li>
<li id="bib.bib921" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[921]</span>
<span class="ltx_bibblock">
A.&nbsp;Abid, M.&nbsp;Farooqi, and J.&nbsp;Zou, “Persistent anti-muslim bias in large
language models,” in <em id="bib.bib921.1.1" class="ltx_emph ltx_font_italic">AIES ’21: AAAI/ACM Conference on AI, Ethics,
and Society, Virtual Event, USA, May 19-21, 2021</em>, M.&nbsp;Fourcade, B.&nbsp;Kuipers,
S.&nbsp;Lazar, and D.&nbsp;K. Mulligan, Eds.&nbsp;&nbsp;&nbsp;ACM, 2021, pp. 298–306.

</span>
</li>
<li id="bib.bib922" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[922]</span>
<span class="ltx_bibblock">
A.&nbsp;Shah and S.&nbsp;Chava, “Zero is not hero yet: Benchmarking zero-shot
performance of llms for financial tasks,” <em id="bib.bib922.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.16633,
2023.

</span>
</li>
<li id="bib.bib923" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[923]</span>
<span class="ltx_bibblock">
D.&nbsp;Araci, “Finbert: Financial sentiment analysis with pre-trained language
models,” <em id="bib.bib923.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1908.10063, 2019.

</span>
</li>
<li id="bib.bib924" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[924]</span>
<span class="ltx_bibblock">
J.&nbsp;C.&nbsp;S. Alvarado, K.&nbsp;Verspoor, and T.&nbsp;Baldwin, “Domain adaption of named
entity recognition to support credit risk assessment,” in <em id="bib.bib924.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the Australasian Language Technology Association Workshop, ALTA 2015,
Parramatta, Australia, December 8 - 9, 2015</em>, B.&nbsp;Hachey and K.&nbsp;Webster,
Eds.&nbsp;&nbsp;&nbsp;ACL, 2015, pp. 84–90.

</span>
</li>
<li id="bib.bib925" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[925]</span>
<span class="ltx_bibblock">
G.&nbsp;Son, H.&nbsp;Jung, M.&nbsp;Hahm, K.&nbsp;Na, and S.&nbsp;Jin, “Beyond classification: Financial
reasoning in state-of-the-art language models,” <em id="bib.bib925.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2305.01505, 2023.

</span>
</li>
<li id="bib.bib926" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[926]</span>
<span class="ltx_bibblock">
X.&nbsp;Zhang, Q.&nbsp;Yang, and D.&nbsp;Xu, “Xuanyuan 2.0: A large chinese financial chat
model with hundreds of billions parameters,” <em id="bib.bib926.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.12002</em>, 2023.

</span>
</li>
<li id="bib.bib927" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[927]</span>
<span class="ltx_bibblock">
H.&nbsp;Yang, X.-Y. Liu, and C.&nbsp;D. Wang, “Fingpt: Open-source financial large
language models,” <em id="bib.bib927.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.06031, 2023.

</span>
</li>
<li id="bib.bib928" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[928]</span>
<span class="ltx_bibblock">
Q.&nbsp;Jin, B.&nbsp;Dhingra, Z.&nbsp;Liu, W.&nbsp;W. Cohen, and X.&nbsp;Lu, “Pubmedqa: A dataset for
biomedical research question answering,” in <em id="bib.bib928.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
2019, Hong Kong, China, November 3-7, 2019</em>, 2019, pp. 2567–2577.

</span>
</li>
<li id="bib.bib929" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[929]</span>
<span class="ltx_bibblock">
A.&nbsp;Krithara, A.&nbsp;Nentidis, K.&nbsp;Bougiatiotis, and G.&nbsp;Paliouras, “Bioasq-qa: A
manually curated corpus for biomedical question answering,” 2022.

</span>
</li>
<li id="bib.bib930" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[930]</span>
<span class="ltx_bibblock">
Z.&nbsp;Bi, N.&nbsp;Zhang, Y.&nbsp;Xue, Y.&nbsp;Ou, D.&nbsp;Ji, G.&nbsp;Zheng, and H.&nbsp;Chen, “Oceangpt: A
large language model for ocean science tasks,” <em id="bib.bib930.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2310.02031, 2023.

</span>
</li>
<li id="bib.bib931" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[931]</span>
<span class="ltx_bibblock">
C.&nbsp;Zhang, C.&nbsp;Zhang, C.&nbsp;Li, Y.&nbsp;Qiao, S.&nbsp;Zheng, S.&nbsp;K. Dam, M.&nbsp;Zhang, J.&nbsp;U. Kim,
S.&nbsp;T. Kim, J.&nbsp;Choi, G.&nbsp;Park, S.&nbsp;Bae, L.&nbsp;Lee, P.&nbsp;Hui, I.&nbsp;S. Kweon, and C.&nbsp;S.
Hong, “One small step for generative ai, one giant leap for AGI: A
complete survey on chatgpt in AIGC era,” <em id="bib.bib931.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.06488,
2023.

</span>
</li>
<li id="bib.bib932" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[932]</span>
<span class="ltx_bibblock">
M.&nbsp;Haman and M.&nbsp;Skolnik, “Using chatgpt to conduct a literature review.”
<em id="bib.bib932.1.1" class="ltx_emph ltx_font_italic">Accountability in research</em>, 2023.

</span>
</li>
<li id="bib.bib933" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[933]</span>
<span class="ltx_bibblock">
Ö.&nbsp;Aydın and E.&nbsp;Karaarslan, “Openai chatgpt generated literature review:
Digital twin in healthcare,” <em id="bib.bib933.1.1" class="ltx_emph ltx_font_italic">SSRN Electronic Journal</em>, 2022.

</span>
</li>
<li id="bib.bib934" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[934]</span>
<span class="ltx_bibblock">
Y.&nbsp;J. Park, D.&nbsp;Kaplan, Z.&nbsp;Ren, C.&nbsp;Hsu, C.&nbsp;Li, H.&nbsp;Xu, S.&nbsp;Li, and J.&nbsp;Li, “Can
chatgpt be used to generate scientific hypotheses?” <em id="bib.bib934.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2304.12208, 2023.

</span>
</li>
<li id="bib.bib935" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[935]</span>
<span class="ltx_bibblock">
M.&nbsp;M. Hassan, R.&nbsp;A. Knipper, and S.&nbsp;K.&nbsp;K. Santu, “Chatgpt as your personal
data scientist,” <em id="bib.bib935.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.13657, 2023.

</span>
</li>
<li id="bib.bib936" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[936]</span>
<span class="ltx_bibblock">
L.&nbsp;Cheng, X.&nbsp;Li, and L.&nbsp;Bing, “Is GPT-4 a good data analyst?” <em id="bib.bib936.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2305.15038, 2023.

</span>
</li>
<li id="bib.bib937" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[937]</span>
<span class="ltx_bibblock">
S.&nbsp;I.&nbsp;M. Hussam&nbsp;Alkaissi, “Artificial hallucinations in chatgpt: Implications
in scientific writing,” <em id="bib.bib937.1.1" class="ltx_emph ltx_font_italic">PubMed</em>, 2023.

</span>
</li>
<li id="bib.bib938" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[938]</span>
<span class="ltx_bibblock">
A.&nbsp;Azaria, R.&nbsp;Azoulay, and S.&nbsp;Reches, “Chatgpt is a remarkable tool – for
experts,” <em id="bib.bib938.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.03102, 2023.

</span>
</li>
<li id="bib.bib939" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[939]</span>
<span class="ltx_bibblock">
O.&nbsp;O. Buruk, “Academic writing with GPT-3.5: reflections on practices,
efficacy and transparency,” <em id="bib.bib939.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2304.11079, 2023.

</span>
</li>
<li id="bib.bib940" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[940]</span>
<span class="ltx_bibblock">
R.&nbsp;Liu and N.&nbsp;B. Shah, “Reviewergpt? an exploratory study on using large
language models for paper reviewing,” <em id="bib.bib940.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2306.00622,
2023.

</span>
</li>
<li id="bib.bib941" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[941]</span>
<span class="ltx_bibblock">
M.&nbsp;Kosinski, “Theory of mind may have spontaneously emerged in large language
models,” <em id="bib.bib941.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2302.02083, 2023.

</span>
</li>
<li id="bib.bib942" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[942]</span>
<span class="ltx_bibblock">
M.&nbsp;M. Amin, E.&nbsp;Cambria, and B.&nbsp;W. Schuller, “Will affective computing emerge
from foundation models and general ai? A first evaluation on chatgpt,”
<em id="bib.bib942.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2303.03186, 2023.

</span>
</li>
<li id="bib.bib943" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[943]</span>
<span class="ltx_bibblock">
G.&nbsp;Sridhara, R.&nbsp;H. G., and S.&nbsp;Mazumdar, “Chatgpt: A study on its utility for
ubiquitous software engineering tasks,” <em id="bib.bib943.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.16837,
2023.

</span>
</li>
<li id="bib.bib944" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[944]</span>
<span class="ltx_bibblock">
W.&nbsp;Sun, C.&nbsp;Fang, Y.&nbsp;You, Y.&nbsp;Miao, Y.&nbsp;Liu, Y.&nbsp;Li, G.&nbsp;Deng, S.&nbsp;Huang, Y.&nbsp;Chen,
Q.&nbsp;Zhang, H.&nbsp;Qian, Y.&nbsp;Liu, and Z.&nbsp;Chen, “Automatic code summarization via
chatgpt: How far are we?” <em id="bib.bib944.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2305.12865, 2023.

</span>
</li>
<li id="bib.bib945" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[945]</span>
<span class="ltx_bibblock">
C.&nbsp;S. Xia and L.&nbsp;Zhang, “Conversational automated program repair,”
<em id="bib.bib945.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2301.13246, 2023.

</span>
</li>
<li id="bib.bib946" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[946]</span>
<span class="ltx_bibblock">
W.&nbsp;Kuang, B.&nbsp;Qian, Z.&nbsp;Li, D.&nbsp;Chen, D.&nbsp;Gao, X.&nbsp;Pan, Y.&nbsp;Xie, Y.&nbsp;Li, B.&nbsp;Ding, and
J.&nbsp;Zhou, “Federatedscope-llm: A comprehensive package for fine-tuning large
language models in federated learning,” 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.18222" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.18223" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2303.18223">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.18223" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.18224" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 17:19:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
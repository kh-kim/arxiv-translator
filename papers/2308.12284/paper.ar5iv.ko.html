<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.12284] D4: Improving LLM Pretraining via Document De-Duplication and Diversification</title><meta property="og:description" content="Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scal…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="D4: Improving LLM Pretraining via Document De-Duplication and Diversification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="D4: Improving LLM Pretraining via Document De-Duplication and Diversification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.12284">

<!--Generated on Wed Feb 28 12:01:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution. Correspondence emails: ktirumala@meta.com, simigd@gmail.com</span></span></span>
<h1 class="ltx_title ltx_title_document">D4: Improving LLM Pretraining via Document De-Duplication and Diversification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kushal Tirumala* 
<br class="ltx_break">Meta AI Research
<br class="ltx_break">&amp;Daniel Simig*
<br class="ltx_break">Meta AI Research 
<br class="ltx_break">&amp;Armen Aghajanyan 
<br class="ltx_break">Meta AI Research 
<br class="ltx_break">&amp;Ari S. Morcos 
<br class="ltx_break">Meta AI Research 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">최근 몇 년 동안 대규모 웹 말뭉치에서 무작위로 선택된 가능한 많은 토큰에 대해 원패스 학습을 수행하여 대규모 언어 모델(LLM)을 훈련하는 데 컴퓨팅 및 데이터의 양이 증가하고 있다. 인터넷의 점점 더 큰 부분에 대한 훈련은 일관된 성능 개선으로 이어지지만, 이러한 개선의 크기는 규모에 따라 감소하며, MinHash와 같은 단순한 중복 제거 방법을 넘어 사전 훈련 및 다운스트림 성능에 대한 데이터 선택의 영향을 탐구하는 작업은 거의 없다. 본 논문에서는 6.7B 모델 스케일에서 16개의 NLP 태스크(최대 2%)에 대해 사전 훈련된 모델 임베딩을 통해 (중복되지 않은 데이터 위에) 신중하게 데이터를 선택하는 것이 훈련 속도를 높이고(효율성 이득 20%) 평균 다운스트림 정확도를 향상시킬 수 있음을 보여준다. 또한, 반복 데이터가 지능적으로 일관되게 <span class="ltx_text ltx_font_italic" id="id1.id1.1">outperforms</span> 베이스라인 트레이닝(반복 랜덤 데이터가 베이스라인 트레이닝보다 더 나쁜 성능을 수행하는 동안)을 보여준다. 우리의 결과는 영리한 데이터 선택이 LLM 사전 트레이닝을 크게 개선할 수 있고, 가능한 한 많은 데이터에 대해 단일 에포크에 대한 트레이닝의 일반적인 관행에 의문을 제기하며, 웹 데이터를 무작위로 샘플링하는 한계를 넘어 모델을 계속 개선하는 경로를 보여준다.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">계산적 한계로 인해 언어 모델 사전 훈련에 대한 초기 작업은 북코퍼스 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib61" title="">61</a>]</cite> 및 위키피디아 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>와 같은 작고 고품질의 텍스트 데이터 세트에 대한 훈련 모델에 초점을 맞췄다. 그러나 최근에는 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite>와 같은 작업에 의해 촉매되는 대규모 언어 모델(LLM)의 발전은 인터넷의 스냅샷(CommonCrawl <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>, <a class="ltx_ref" href="#bib.bib39" title="">39</a>, <a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>)에서 파생된 레이블이 지정되지 않은 분류되지 않은 데이터의 대규모 모음을 활용하여 대량의 분류되지 않은 데이터와 대량의 분류되지 않은 데이터를 거래함으로써 주도되었다. 데이터 양의 급격한 증가로 인해 이러한 전략은 더 높은 성능 모델을 가져왔고 대규모의 여과되지 않은 데이터 세트가 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib46" title="">46</a>, <a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite> 학습에 활용되는 새로운 패러다임을 촉발했다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">현재 LM 사전 훈련에서 대규모 웹 데이터가 수행하는 필수적인 역할에도 불구하고 대규모 웹 데이터에 대한 데이터 큐레이션 및 선택은 철저히 탐구되지 않았다. 이는 주로 계산 및 데이터 스케일링 법칙 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib20" title="">20</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>의 보편성에 기인하며, 이는 실무자에게 반드시 "올바른" 데이터가 아닌 "더 많은" 데이터를 추가하여 LM 성능을 안정적으로 개선하는 저위험 방법을 제공하기 때문이다. 실제로, 스케일링 법칙들을 모델링하기 위해 사용되는 데이터 선택 방법(대부분의 LLM 사전 트레이닝 파이프라인들에서 사용되는 데이터 선택 방법들과 함께)은 간단한 휴리스틱 필터링(예를 들어, 매우 짧은 스트링들을 제거하기 위해) 및 매우 거의 일치하는 중복 제거 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite>의 조합을 통해 투입된 웹 데이터 덤프들로부터 토큰들을 단순히 랜덤하게 샘플링하는 것을 포함한다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">LLM을 개선하기 위해 스케일링 법칙에 계속 의존하면 스케일링 법칙의 멱법칙 특성으로 인해 수익 체감에 빠르게 도달할 것이다. 따라서 우리는 일관된 한계 개선을 유지하기 위해 기하급수적으로 더 많은 데이터가 필요할 것이며, 이는 인간이 생성한 텍스트 데이터 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib51" title="">51</a>]</cite>의 한계에 빠르게 접근하고 있기 때문에 특히 어려운 것으로 판명될 수 있다. 비전의 맥락에서 <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>는 비용이 많이 드는 멱법칙 스케일링을 극복하기 위해 간단한 데이터 선택 전략을 활용할 수 있음을 보여주었다. 그들은 수많은 데이터 선택 방법을 비교하고 미리 훈련된 임베딩 공간에서 데이터 포인트를 클러스터링하고 클러스터 중심(cluster centroid; "SSL Prototype")까지의 거리에 따라 순위를 매기는 것이 비전 모델의 데이터 효율성을 크게 향상시킨다는 것을 발견한다. 최근 <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>는 데이터를 디-중복하기 위해 사전 훈련된 임베딩 공간을 사용하는 것("SemDeDup")이 CLIP와 같은 비전 언어 모델의 효율성과 성능을 모두 향상시킨다는 것을 입증했다. 그러나 LLM을 대규모로 훈련하는 데 있어 이러한 접근법이나 관련 접근법에 대한 탐색은 거의 없다. 이를 통해 이러한 접근 방식을 결합하여 LLM에 적용함으로써 사전 훈련된 임베딩을 활용하는 비교적 간단한 데이터 선택 전략이 LLM 훈련을 크게 개선할 수 있다고 주장한다. 구체적으로, 우리의 기여는 다음과 같다:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1">데이터를 이미 수동으로 필터링/중복 제거(예: MinHash)했으며 성능을 최적화하는 목표 분포를 알지 못하는 표준 LLM 사전 훈련 설정에 대한 다양한 데이터 선택 전략을 조사합니다. 우리는 SSL 프로토타입의 성능이 임베딩 공간에서 중복 구동 클러스터에 의해 영향을 받는다고 주장한다. 섹션 <a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a>에서는 이러한 클러스터의 영향을 받지 않기 위해 SemDeDup을 활용하는 새로운 데이터 선택 전략 <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">D4</span>을 제안한다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1">섹션 <a class="ltx_ref" href="#S4.SS1" title="4.1 Fixed compute regime: can data selection help on fixed token budgets? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.1</span></a>에서는 <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">compute-limited regime</span>에서 "infinite" 소스 데이터를 가지고 고정된 토큰 예산으로 모델을 학습하면 무작위 iid 데이터 선택 및 이전에 설정된 방법보다 더 나은 사전 학습 복잡도와 다운스트림 정확도를 달성할 수 있음을 보여준다. 또한, 6.7b 모델 규모에서 D4 방법이 약 20%의 효율 이득을 얻을 수 있음을 보이고, 모델 규모에 따라 효율 이득의 크기가 증가함을 보인다.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">data-limited regime</span>에서 데이터가 부족하고 데이터를 에포크해야 하며, 교묘하게 반복할 데이터를 선택하면 무작위로 선택된 새 데이터에 대한 트레이닝을 이길 수 있는 반면, 반복할 데이터를 무작위로 선택하면 새 데이터를 추가하는 성능이 떨어진다(Section <a class="ltx_ref" href="#S4.SS2" title="4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.2</span></a>). 이것은 단일 epoch LLM 훈련의 표준 관행에 의문을 제기하며 지능적으로 하위 선택된 데이터에 대한 epoching이 더 나은 접근법이 될 수 있음을 시사한다.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">그림 1</span>:</span><span class="ltx_text" id="S1.F1.4.2" style="font-size:90%;">Learning curves for 6.7B OPT model pretraining on 100B tokens, with data selected with D4 (pink line) and randomly (gray line). D4는 16개의 NLP 작업에서 검증 복잡도에 대해 18-20%의 효율성 증가와 평균 0-샷 다운스트림 정확도의 2% 증가를 가져 베이스라인 트레이닝을 상당히 능가한다. 전체 학습 곡선은 Section <a class="ltx_ref" href="#A1.SS2" title="A.2 Efficiency gains across model scales and training ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.2</span></a>를 참조한다. </span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">비텍스트 도메인에서 데이터 선택:</span> 수많은 작업이 비전 모델 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>, <a class="ltx_ref" href="#bib.bib10" title="">10</a>, <a class="ltx_ref" href="#bib.bib23" title="">23</a>, <a class="ltx_ref" href="#bib.bib31" title="">31</a>, <a class="ltx_ref" href="#bib.bib34" title="">34</a>, <a class="ltx_ref" href="#bib.bib38" title="">38</a>, <a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>에서 데이터 선택 기술을 성공적으로 사용했지만 대부분 하위 이미지넷 규모였다. 이들 작업 중 일부는 개별 데이터 포인트(예를 들어, EL2N from <cite class="ltx_cite ltx_citemacro_citet">Paul et al. [<a class="ltx_ref" href="#bib.bib38" title="">38</a>]</cite>)를 점수화하는 가지치기 메트릭을 개발하는 반면, 일부는 데이터 효율성에 초점을 맞추고 모델이 더 적은 데이터 포인트, 예를 들어, 코어셋 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>, <a class="ltx_ref" href="#bib.bib35" title="">35</a>, <a class="ltx_ref" href="#bib.bib44" title="">44</a>, <a class="ltx_ref" href="#bib.bib60" title="">60</a>]</cite>로 기준 성능에 도달할 수 있도록 하는 포인트 그룹을 찾으려고 시도한다. <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>는 ImageNet 규모에서 기존의 많은 개별 점수 방법을 비교하여 SSL 프로토타입 메트릭과 <cite class="ltx_cite ltx_citemacro_citet">Feldman and Zhang [<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>의 (금지적으로 비싼) 암기 메트릭이 일반적으로 다른 방법보다 우수하다는 것을 발견했다. 오디오 도메인에서 <cite class="ltx_cite ltx_citemacro_citet">Dong et al. [<a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite>는 중요도 임베딩을 계산하여 오디오 장면 분류를 위한 중요한 훈련 샘플을 찾는다. 보다 최근에, <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>는 SSL 프로토타입과 유사한 방법이지만 의미적 중복 제거에 초점을 맞춘 SemDeDup을 사용하는 비전 언어 모델(CLIP 모델)에 대해 매우 고무적인 결과를 보여주었다. 우리의 작업은 이러한 접근법을 결합하고 대규모 LLM에 적용한다.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Pre-training data on LM performance effect:</span> <cite class="ltx_cite ltx_citemacro_citet">Gao et al. [<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite> trains variants of GPT-2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite> models from scratch to compare the "Pile" dataset to CommonCrawl-derived corpora. <cite class="ltx_cite ltx_citemacro_citet">Radford et al. [<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite>는 1.4B 매개 변수 모델을 처음부터 훈련하여 MassiveWeb을 큐레이트하는 데 사용되는 품질 필터 및 데이터 중복 제거 방법의 긍정적인 영향을 보여줍니다. <cite class="ltx_cite ltx_citemacro_citet">Hernandez et al. [<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite>는 다양한 양의 인위적으로 생성된 데이터 복제의 영향을 정량화하고 복제된 데이터에 대해 훈련된 모델의 행동 변화를 해석하는 데 분석을 제공한다. 이와 함께 <cite class="ltx_cite ltx_citemacro_citet">Xie et al. [<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite>는 Wikipedia와 같은 고품질 참조 말뭉치에 웹 데이터의 분포를 정렬하기 위해 중요도 재샘플링을 사용하는 것을 제안한다. 유사하게, <cite class="ltx_cite ltx_citemacro_citet">Gururangan et al. [<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>는 LMs를 태스크-특정 코퍼스에 적응시키기 위한 데이터 선택 전략을 탐구한다. 최근 연구의 또 다른 라인은 <cite class="ltx_cite ltx_citemacro_citet">Xie et al. [<a class="ltx_ref" href="#bib.bib55" title="">55</a>]</cite>가 파일에서 훈련된 8B 매개변수 모델에 대해 모든 데이터 세트에 걸쳐 다운스트림 정확도와 복잡도에서 인상적인 개선을 보여주면서 데이터 혼합이 사전 훈련에 어떻게 영향을 미치는지 탐구한다. 마찬가지로 <cite class="ltx_cite ltx_citemacro_citet">Longpre et al. [<a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite>는 LLM 성능에 대한 학습 데이터의 텍스트 품질, 독성, 연령 및 도메인 분포의 역할을 탐구한다. 데이터 큐레이션 외에도 최근 반복 데이터 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>, <a class="ltx_ref" href="#bib.bib37" title="">37</a>, <a class="ltx_ref" href="#bib.bib57" title="">57</a>]</cite>의 영향을 탐구하는 작업이 급증했으며 일반적으로 반복 토큰이 새 토큰에 대한 훈련보다 나쁘다고 결론지었다(섹션 <a class="ltx_ref" href="#S4.SS2" title="4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.2</span></a>에서 질문한다).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Notation</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.17" class="ltx_p">Given a source dataset, <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1c" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1d" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝐷</ci><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.4">𝑢</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.5">𝑟</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.6">𝑐</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">D_{source}</annotation></semantics></math>, of documents (crawled web pages) and model architecture, <math id="S3.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.1c">M</annotation></semantics></math>, we aim to find a strategy <math id="S3.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.1c">S</annotation></semantics></math> for selecting a subset of these documents that maximizes some evaluation metric <math id="S3.SS0.SSS0.Px1.p1.4.m4.3" class="ltx_Math" alttext="E(M(D_{S,R}))" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.4.m4.3a"><mrow id="S3.SS0.SSS0.Px1.p1.4.m4.3.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.2.cmml">​</mo><mrow id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.4" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.cmml">S</mi><mo id="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.4.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.2.cmml">R</mi></mrow></msub><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.4.m4.3b"><apply id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3"><times id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.2"></times><ci id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.3">𝐸</ci><apply id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1"><times id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.2"></times><ci id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.3">𝑀</ci><apply id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.3.3.1.1.1.1.1.1.2">𝐷</ci><list id="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.4"><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.1.1">𝑆</ci><ci id="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2.2.2">𝑅</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.4.m4.3c">E(M(D_{S,R}))</annotation></semantics></math>. <math id="S3.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.5.m5.1a"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.5.m5.1b"><ci id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.5.m5.1c">R</annotation></semantics></math> indicates the proportion of remaining documents from the source dataset <math id="S3.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">D</mi><mrow id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1c" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.6" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1d" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.7" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2">𝐷</ci><apply id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.2">𝑠</ci><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.3">𝑜</ci><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.4">𝑢</ci><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.5">𝑟</ci><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.6">𝑐</ci><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.6.m6.1c">D_{source}</annotation></semantics></math> after selecting data with strategy <math id="S3.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.7.m7.1a"><mi id="S3.SS0.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.7.m7.1b"><ci id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.7.m7.1c">S</annotation></semantics></math>. For this reason, we refer to <math id="S3.SS0.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.8.m8.1a"><mi id="S3.SS0.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.8.m8.1b"><ci id="S3.SS0.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.8.m8.1c">R</annotation></semantics></math> throughout this work as the <span id="S3.SS0.SSS0.Px1.p1.17.1" class="ltx_text ltx_font_italic">selection ratio</span>: for example, if <math id="S3.SS0.SSS0.Px1.p1.9.m9.1" class="ltx_Math" alttext="R=0.25" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.9.m9.1a"><mrow id="S3.SS0.SSS0.Px1.p1.9.m9.1.1" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.2" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.2.cmml">R</mi><mo id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.1" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.1.cmml">=</mo><mn id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.3" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.9.m9.1b"><apply id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1"><eq id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.1"></eq><ci id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.2">𝑅</ci><cn type="float" id="S3.SS0.SSS0.Px1.p1.9.m9.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.9.m9.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.9.m9.1c">R=0.25</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="|D_{source}|=100" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.10.m10.1a"><mrow id="S3.SS0.SSS0.Px1.p1.10.m10.1.1" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.cmml"><mrow id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.2.1.cmml">|</mo><msub id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1c" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.6" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1d" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.7" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.7.cmml">e</mi></mrow></msub><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.2" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.2.cmml">=</mo><mn id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.3" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.10.m10.1b"><apply id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1"><eq id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.2"></eq><apply id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1"><abs id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.2"></abs><apply id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.2">𝐷</ci><apply id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.2">𝑠</ci><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.3">𝑜</ci><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.4">𝑢</ci><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.5">𝑟</ci><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.6">𝑐</ci><ci id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.1.1.1.3.7">𝑒</ci></apply></apply></apply><cn type="integer" id="S3.SS0.SSS0.Px1.p1.10.m10.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.10.m10.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.10.m10.1c">|D_{source}|=100</annotation></semantics></math> million, then we <span id="S3.SS0.SSS0.Px1.p1.17.2" class="ltx_text ltx_font_italic">select</span> 25% of documents from a source dataset of size <math id="S3.SS0.SSS0.Px1.p1.11.m11.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.11.m11.1a"><mn id="S3.SS0.SSS0.Px1.p1.11.m11.1.1" xref="S3.SS0.SSS0.Px1.p1.11.m11.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.11.m11.1b"><cn type="integer" id="S3.SS0.SSS0.Px1.p1.11.m11.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.11.m11.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.11.m11.1c">100</annotation></semantics></math>M documents to arrive at a a training dataset with <math id="S3.SS0.SSS0.Px1.p1.12.m12.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.12.m12.1a"><mn id="S3.SS0.SSS0.Px1.p1.12.m12.1.1" xref="S3.SS0.SSS0.Px1.p1.12.m12.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.12.m12.1b"><cn type="integer" id="S3.SS0.SSS0.Px1.p1.12.m12.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.12.m12.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.12.m12.1c">25</annotation></semantics></math>M documents. We operate at the granularity of a single document, independently of how the model trainer would pack these documents into batches later. Throughout the paper, we use random selection as the baseline for <math id="S3.SS0.SSS0.Px1.p1.13.m13.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.13.m13.1a"><mi id="S3.SS0.SSS0.Px1.p1.13.m13.1.1" xref="S3.SS0.SSS0.Px1.p1.13.m13.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.13.m13.1b"><ci id="S3.SS0.SSS0.Px1.p1.13.m13.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.13.m13.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.13.m13.1c">S</annotation></semantics></math>, as it is the most common method for selecting data for language model pre-training. In the rest of this section, we describe our choices of source dataset (<math id="S3.SS0.SSS0.Px1.p1.14.m14.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.14.m14.1a"><msub id="S3.SS0.SSS0.Px1.p1.14.m14.1.1" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.2" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.2.cmml">D</mi><mrow id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1c" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.6" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1d" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.7" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.14.m14.1b"><apply id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.2">𝐷</ci><apply id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.2">𝑠</ci><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.3">𝑜</ci><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.4">𝑢</ci><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.5">𝑟</ci><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.6.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.6">𝑐</ci><ci id="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.7.cmml" xref="S3.SS0.SSS0.Px1.p1.14.m14.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.14.m14.1c">D_{source}</annotation></semantics></math>), model (<math id="S3.SS0.SSS0.Px1.p1.15.m15.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.15.m15.1a"><mi id="S3.SS0.SSS0.Px1.p1.15.m15.1.1" xref="S3.SS0.SSS0.Px1.p1.15.m15.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.15.m15.1b"><ci id="S3.SS0.SSS0.Px1.p1.15.m15.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.15.m15.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.15.m15.1c">M</annotation></semantics></math>), evaluation metric (<math id="S3.SS0.SSS0.Px1.p1.16.m16.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.16.m16.1a"><mi id="S3.SS0.SSS0.Px1.p1.16.m16.1.1" xref="S3.SS0.SSS0.Px1.p1.16.m16.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.16.m16.1b"><ci id="S3.SS0.SSS0.Px1.p1.16.m16.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.16.m16.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.16.m16.1c">E</annotation></semantics></math>), and, most importantly, our suggestions for the selection strategy (<math id="S3.SS0.SSS0.Px1.p1.17.m17.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.17.m17.1a"><mi id="S3.SS0.SSS0.Px1.p1.17.m17.1.1" xref="S3.SS0.SSS0.Px1.p1.17.m17.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.17.m17.1b"><ci id="S3.SS0.SSS0.Px1.p1.17.m17.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.17.m17.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.17.m17.1c">S</annotation></semantics></math>).</p>
</div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training Dataset (choice for <math id="S3.SS1.1.m1.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S3.SS1.1.m1.1b"><msub id="S3.SS1.1.m1.1.1" xref="S3.SS1.1.m1.1.1.cmml"><mi id="S3.SS1.1.m1.1.1.2" xref="S3.SS1.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS1.1.m1.1.1.3" xref="S3.SS1.1.m1.1.1.3.cmml"><mi id="S3.SS1.1.m1.1.1.3.2" xref="S3.SS1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS1.1.m1.1.1.3.1" xref="S3.SS1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.1.m1.1.1.3.3" xref="S3.SS1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.1.m1.1.1.3.1b" xref="S3.SS1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.1.m1.1.1.3.4" xref="S3.SS1.1.m1.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.1.m1.1.1.3.1c" xref="S3.SS1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.1.m1.1.1.3.5" xref="S3.SS1.1.m1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.1.m1.1.1.3.1d" xref="S3.SS1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.1.m1.1.1.3.6" xref="S3.SS1.1.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.1.m1.1.1.3.1e" xref="S3.SS1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.1.m1.1.1.3.7" xref="S3.SS1.1.m1.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.1.m1.1c"><apply id="S3.SS1.1.m1.1.1.cmml" xref="S3.SS1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.1.m1.1.1.1.cmml" xref="S3.SS1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.1.m1.1.1.2.cmml" xref="S3.SS1.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.1.m1.1.1.3.cmml" xref="S3.SS1.1.m1.1.1.3"><times id="S3.SS1.1.m1.1.1.3.1.cmml" xref="S3.SS1.1.m1.1.1.3.1"></times><ci id="S3.SS1.1.m1.1.1.3.2.cmml" xref="S3.SS1.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS1.1.m1.1.1.3.3.cmml" xref="S3.SS1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS1.1.m1.1.1.3.4.cmml" xref="S3.SS1.1.m1.1.1.3.4">𝑢</ci><ci id="S3.SS1.1.m1.1.1.3.5.cmml" xref="S3.SS1.1.m1.1.1.3.5">𝑟</ci><ci id="S3.SS1.1.m1.1.1.3.6.cmml" xref="S3.SS1.1.m1.1.1.3.6">𝑐</ci><ci id="S3.SS1.1.m1.1.1.3.7.cmml" xref="S3.SS1.1.m1.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.1.m1.1d">D_{source}</annotation></semantics></math>)</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">우리는 CCNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib54" title="">54</a>]</cite> 파이프라인과 동일한 CCNet <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite> 파이프라인으로 전처리된 CommonCrawl 버전에서 모든 훈련 실행을 수행한다. MinHash 기반 중복 제거의 추가 단계를 추가합니다(섹션 <a class="ltx_ref" href="#A1.SS1" title="A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.1</span></a>의 자세한 내용을 참조). 실험 전에 이 공통 단계를 적용하면 실험에서 관찰된 모든 효과가 현재 널리 퍼진 MinHash 기반 데이터 중복 제거 전략의 접근법을 보완한다는 것을 보장한다. 이 작업의 나머지 부분에서 이 데이터 세트를 <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">CC-dedup</span>이라고 합니다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Training (choices for <math id="S3.SS2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.1.m1.1b"><mi id="S3.SS2.1.m1.1.1" xref="S3.SS2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.1.m1.1c"><ci id="S3.SS2.1.m1.1.1.cmml" xref="S3.SS2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.1.m1.1d">M</annotation></semantics></math> and <math id="S3.SS2.2.m2.1" class="ltx_Math" alttext="T_{target}" display="inline"><semantics id="S3.SS2.2.m2.1b"><msub id="S3.SS2.2.m2.1.1" xref="S3.SS2.2.m2.1.1.cmml"><mi id="S3.SS2.2.m2.1.1.2" xref="S3.SS2.2.m2.1.1.2.cmml">T</mi><mrow id="S3.SS2.2.m2.1.1.3" xref="S3.SS2.2.m2.1.1.3.cmml"><mi id="S3.SS2.2.m2.1.1.3.2" xref="S3.SS2.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.2.m2.1.1.3.1" xref="S3.SS2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.2.m2.1.1.3.3" xref="S3.SS2.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.2.m2.1.1.3.1b" xref="S3.SS2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.2.m2.1.1.3.4" xref="S3.SS2.2.m2.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.2.m2.1.1.3.1c" xref="S3.SS2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.2.m2.1.1.3.5" xref="S3.SS2.2.m2.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.2.m2.1.1.3.1d" xref="S3.SS2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.2.m2.1.1.3.6" xref="S3.SS2.2.m2.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.2.m2.1.1.3.1e" xref="S3.SS2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.2.m2.1.1.3.7" xref="S3.SS2.2.m2.1.1.3.7.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.2.m2.1c"><apply id="S3.SS2.2.m2.1.1.cmml" xref="S3.SS2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.2.m2.1.1.1.cmml" xref="S3.SS2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.2.m2.1.1.2.cmml" xref="S3.SS2.2.m2.1.1.2">𝑇</ci><apply id="S3.SS2.2.m2.1.1.3.cmml" xref="S3.SS2.2.m2.1.1.3"><times id="S3.SS2.2.m2.1.1.3.1.cmml" xref="S3.SS2.2.m2.1.1.3.1"></times><ci id="S3.SS2.2.m2.1.1.3.2.cmml" xref="S3.SS2.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS2.2.m2.1.1.3.3.cmml" xref="S3.SS2.2.m2.1.1.3.3">𝑎</ci><ci id="S3.SS2.2.m2.1.1.3.4.cmml" xref="S3.SS2.2.m2.1.1.3.4">𝑟</ci><ci id="S3.SS2.2.m2.1.1.3.5.cmml" xref="S3.SS2.2.m2.1.1.3.5">𝑔</ci><ci id="S3.SS2.2.m2.1.1.3.6.cmml" xref="S3.SS2.2.m2.1.1.3.6">𝑒</ci><ci id="S3.SS2.2.m2.1.1.3.7.cmml" xref="S3.SS2.2.m2.1.1.3.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.2.m2.1d">T_{target}</annotation></semantics></math>)</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">To evaluate different configurations of data selection strategies, we train OPT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> models from scratch on the pruned versions of datasets. We use the standard model architectures and settings of <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. [<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and use MetaSeq <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> to train all our models. For 125M models, we train to <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="T_{target}=3B" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">T</mi><mrow id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.3.2" xref="S3.SS2.p1.1.m1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.2.3.1" xref="S3.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.2.3.3" xref="S3.SS2.p1.1.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.2.3.1a" xref="S3.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.2.3.4" xref="S3.SS2.p1.1.m1.1.1.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.2.3.1b" xref="S3.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.2.3.5" xref="S3.SS2.p1.1.m1.1.1.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.2.3.1c" xref="S3.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.2.3.6" xref="S3.SS2.p1.1.m1.1.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.2.3.1d" xref="S3.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.2.3.7" xref="S3.SS2.p1.1.m1.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">𝑇</ci><apply id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3"><times id="S3.SS2.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.2">𝑡</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.3">𝑎</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.4">𝑟</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.5.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.5">𝑔</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.6.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.6">𝑒</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.7.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3.7">𝑡</ci></apply></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">3</cn><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">T_{target}=3B</annotation></semantics></math> tokens. For 1.3B parameter models, we train to target token count of <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="T_{target}=40B" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msub id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">T</mi><mrow id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.3.2" xref="S3.SS2.p1.2.m2.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2.3.1" xref="S3.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.2.3.3" xref="S3.SS2.p1.2.m2.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2.3.1a" xref="S3.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.2.3.4" xref="S3.SS2.p1.2.m2.1.1.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2.3.1b" xref="S3.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.2.3.5" xref="S3.SS2.p1.2.m2.1.1.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2.3.1c" xref="S3.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.2.3.6" xref="S3.SS2.p1.2.m2.1.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2.3.1d" xref="S3.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.2.3.7" xref="S3.SS2.p1.2.m2.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mn id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">40</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><eq id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></eq><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">𝑇</ci><apply id="S3.SS2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3"><times id="S3.SS2.p1.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.2">𝑡</ci><ci id="S3.SS2.p1.2.m2.1.1.2.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.3">𝑎</ci><ci id="S3.SS2.p1.2.m2.1.1.2.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.4">𝑟</ci><ci id="S3.SS2.p1.2.m2.1.1.2.3.5.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.5">𝑔</ci><ci id="S3.SS2.p1.2.m2.1.1.2.3.6.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.6">𝑒</ci><ci id="S3.SS2.p1.2.m2.1.1.2.3.7.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3.7">𝑡</ci></apply></apply><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><times id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">40</cn><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">T_{target}=40B</annotation></semantics></math>. For 6.7B parameter models, we train to <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="T_{target}=100B" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><msub id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.2" xref="S3.SS2.p1.3.m3.1.1.2.2.cmml">T</mi><mrow id="S3.SS2.p1.3.m3.1.1.2.3" xref="S3.SS2.p1.3.m3.1.1.2.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2.3.2" xref="S3.SS2.p1.3.m3.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.2.3.1" xref="S3.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.2.3.3" xref="S3.SS2.p1.3.m3.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.2.3.1a" xref="S3.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.2.3.4" xref="S3.SS2.p1.3.m3.1.1.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.2.3.1b" xref="S3.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.2.3.5" xref="S3.SS2.p1.3.m3.1.1.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.2.3.1c" xref="S3.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.2.3.6" xref="S3.SS2.p1.3.m3.1.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.2.3.1d" xref="S3.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.2.3.7" xref="S3.SS2.p1.3.m3.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mn id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><eq id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></eq><apply id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.2">𝑇</ci><apply id="S3.SS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3"><times id="S3.SS2.p1.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.1"></times><ci id="S3.SS2.p1.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.2">𝑡</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.3">𝑎</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.4.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.4">𝑟</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.5.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.5">𝑔</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.6.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.6">𝑒</ci><ci id="S3.SS2.p1.3.m3.1.1.2.3.7.cmml" xref="S3.SS2.p1.3.m3.1.1.2.3.7">𝑡</ci></apply></apply><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><times id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.1"></times><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">100</cn><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">T_{target}=100B</annotation></semantics></math> tokens. We choose these by trimming down the token budgets suggested by <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et&nbsp;al. [<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to meet our compute limitations. We provide full details of our training setup in Section&nbsp;<a href="#A1.SS1" title="A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics (choices for <math id="S3.SS3.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS3.1.m1.1b"><mi id="S3.SS3.1.m1.1.1" xref="S3.SS3.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.1.m1.1c"><ci id="S3.SS3.1.m1.1.1.cmml" xref="S3.SS3.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.1.m1.1d">E</annotation></semantics></math>)</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">우리는 대부분의 평가를 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 설정과 일치하게 유지한다.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Validation Set Perplexity</span>. 우리의 검증 세트는 CommonCrawl, DM Mathematics, HackerNews, OpenSubtitles, OpenWebText2, Project Gutenberg, USPTO, Wikipedia와 같은 파일 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>의 하위 집합에서 파생된 검증 세트를 포함하는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에서 주로 나온다. 또한 PushShift.io Reddit 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>(<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">redditflattened</span>이라고 함)에서 얻은 유효성 검사 세트를 포함합니다. 또한 소스 데이터 세트 <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.3">CC-dedup</span>, C4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>의 유효성 검사 세트에서 얻은 유효성 검사 세트에 대해 복잡도를 측정합니다.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p3.1">데이터 선택의 영향은 검증 세트가 웹 데이터 말뭉치에서 파생되었는지 여부에 따라 개별 검증 세트에 크게 다르다는 것을 알 수 있다(섹션 <a class="ltx_ref" href="#S4.SS4.SSS1" title="4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>의 자세한 내용 및 분석 참조). 이를 위해 검증 집합을 Web-snapshots(C4, CommonCrawl, CC-dedup)와 Non-web snapshots로 나누고, 이들 집합 내의 평균 복잡도를 보고한다.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Downstream Task Accuracy. </span> 훈련된 모델의 다운스트림 성능을 평가하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에서 16개의 NLP 작업에 걸쳐 평균 0-shot 정확도를 보고하고 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>와 일치하는 프롬프트 방법론을 사용한다. 이들 16개의 NLP 태스크 세트는 Arc Challenge 및 ArcEasy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>, HellaSwag <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib58" title="">58</a>]</cite>, OpenBookQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>]</cite>, PIQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite>, StoryCloze <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite>, Winograd <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>, Winogrande <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>]</cite>, SuperGLUE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib52" title="">52</a>]</cite>로부터의 태스크를 포함한다. 이 평가 설정에 대한 자세한 내용은 판독기를 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에 참조합니다.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Instruction Tuning Perplexity</span>. 위에서 언급한 평가는 고유한 트레이드오프를 제시한다. 다운스트림 태스크에 대한 정확도는 일반적으로 언어 모델의 실제 값을 보다 구체적으로 나타내는 것으로 간주되지만, 이러한 태스크의 제한된 수의 예와 메트릭으로서의 정확도의 단계적 행동으로 인해 분산이 더 높은 경향이 있다. 대조적으로, perplexity는 메트릭으로서 더 매끄럽지만 여전히 성능 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib43" title="">43</a>]</cite>와 강한 상관 관계를 나타낸다. 따라서 두 평가 메트릭 사이의 중간 지점으로 OPT-IML <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite> 미세 조정에 사용되는 명령어 조정 데이터 세트에서 추출한 샘플에 대한 복잡도를 평가하는 것을 제안한다. 이 데이터 세트는 1500개의 고유한 NLP 작업에 걸쳐 있으며 광범위한 프롬프트 응답 쌍으로 구성되므로 <span class="ltx_text ltx_font_italic" id="S3.SS3.p5.1.2">average</span> NLP 작업을 대표합니다. Super-NaturalInstructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib53" title="">53</a>]</cite> 및 PromptSource <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite>와 같은 광범위한 작업 컬렉션을 병합하여 신중하게 제작되었습니다. 우리는 포괄적인 분류를 위해 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>의 표 2.1에 독자를 참조한다. 이 접근법을 통해 실제 성능 측정과 평가의 통계적 일관성의 균형을 맞출 수 있다. 이 메트릭은 다른 유효성 검사 집합에 대 한 복잡성으로 간주 될 수 있습니다. 여기서 유효성 검사 집합은 명령 조정에 사용 되는 예제로 채워집니다 (이 데이터 집합에서 <span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.3">not</span> fine-tuning입니다).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Data Selection Strategies (choices for <math id="S3.SS4.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS4.1.m1.1b"><mi id="S3.SS4.1.m1.1.1" xref="S3.SS4.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.1.m1.1c"><ci id="S3.SS4.1.m1.1.1.cmml" xref="S3.SS4.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.1.m1.1d">S</annotation></semantics></math>)</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.1">큐레이팅되지 않은 웹 데이터에 대한 초기 탐색에서 우리는 많은 웹 문서 샘플을 내장하고 이러한 임베딩을 클러스터링하고 결과 클러스터를 수동으로 검사했다. 우리는 인간 언어의 자연스러운 분포와 거의 관련이 없는 문서와 웹 크롤링의 아티팩트인 나이키 신발의 광고를 신속하게 식별했습니다. 예를 들어, 약간의 수정을 가한 단일 기본 템플릿에서 자동으로 생성된 나이키 신발의 광고입니다(자세한 내용은 섹션 <a class="ltx_ref" href="#A1.SS9" title="A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.9</span></a> 참조).</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p2.1">이러한 중복 구동 클러스터가 프루닝되어야 한다는 직관에 의해 동기화되고, 비전 및 비전 언어 모델 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>, <a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>에서 프루닝 방법의 최근 성공에 힘입어, 우리는 임베딩 공간에서의 위치를 기반으로 데이터 포인트를 조작하는 데이터 선택 전략에 초점을 맞춘다. 각 문서를 125M OPT 모델에 입력하여 임베딩을 하고 마지막 토큰의 마지막 레이어 임베딩을 사용한다(섹션 <a class="ltx_ref" href="#A1.SS7" title="A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.7</span></a>에서 서로 다른 임베딩 공간을 가지고 실험하였다). 다음으로, 우리는 몇 가지 접근법을 실험한다:</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">SemDeDup</span>: <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite> proposed de-duplicating by both text and image domains by first using K-Means to cluster the embedding space, and removing points in each cluster is within epsilon-balls of other. 이 알고리즘을 수정 없이 사용하고, 이 알고리즘의 구현 세부 사항은 판독기 <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>를 참조한다.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">Prototypicality</span>: <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>는 훈련 이미지 분류 모델의 데이터 효율성을 향상시키기 위해 매우 다양한 데이터 가지치기 전략을 조사했으며, 이는 가장 좋은 방법 중 하나로 판명된 새로 도입된 "SSL Prototypes" 메트릭을 포함한다. 이 전략은 먼저 k-평균 군집링을 사용하여 임베딩 공간을 클러스터링하고 가장 가까운 군집 중심까지의 거리가 증가하는 순서로 데이터 포인트를 폐기하여 가장 "원형" 데이터 포인트가 폐기되어 훨씬 더 높은 분산 이상치를 풍부하게 하는 것을 포함한다. 이 알고리즘에 대한 보다 상세한 설명은 판독기를 <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>에 참조한다.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p5.1.1">D4</span>: 앞에서 언급한 바와 같이, 우리는 MinHash에 의해 제거되지 않는 템플릿 텍스트 또는 극히 의미적으로 중복되는 정보의 클러스터인 중복 구동 클러스터의 많은 인스턴스를 찾는다. 이러한 임베딩 공간의 영역은 매우 밀집된 경향이 있으며 k-평균이 복제된 텍스트에 대한 귀중한 클러스터 할당을 낭비하게 한다. 이 편향된 군집링은 또한 많은 군집이 더 많은 국소 일관성 대신 완전히 중복에 의해 구동되기 때문에 SSL 프로토타입의 효과에 부정적인 영향을 미칠 수 있다. 이 통찰력은 우리가 제안한 전략으로 이끕니다.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.3" class="ltx_p">Apply <span id="S3.I1.i1.p1.3.1" class="ltx_text ltx_font_italic">SemDeDup</span> with a selection ratio <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="R_{dedup}" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><msub id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">R</mi><mrow id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.3.2" xref="S3.I1.i1.p1.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.1.m1.1.1.3.1" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.1.m1.1.1.3.1a" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.4" xref="S3.I1.i1.p1.1.m1.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.1.m1.1.1.3.1b" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.5" xref="S3.I1.i1.p1.1.m1.1.1.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.1.m1.1.1.3.1c" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.6" xref="S3.I1.i1.p1.1.m1.1.1.3.6.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">𝑅</ci><apply id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3"><times id="S3.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.1"></times><ci id="S3.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.2">𝑑</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.4.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.4">𝑑</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.5.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.5">𝑢</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.6.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.6">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">R_{dedup}</annotation></semantics></math> on the entire dataset <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">D</annotation></semantics></math>, producing a smaller dataset <math id="S3.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.I1.i1.p1.3.m3.1a"><msup id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml"><mi id="S3.I1.i1.p1.3.m3.1.1.2" xref="S3.I1.i1.p1.3.m3.1.1.2.cmml">D</mi><mo id="S3.I1.i1.p1.3.m3.1.1.3" xref="S3.I1.i1.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><apply id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.3.m3.1.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I1.i1.p1.3.m3.1.1.2.cmml" xref="S3.I1.i1.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.I1.i1.p1.3.m3.1.1.3.cmml" xref="S3.I1.i1.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">D^{\prime}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Cluster points in <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><msup id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">D</mi><mo id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">D^{\prime}</annotation></semantics></math> with K-Means</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.2" class="ltx_p">Apply <span id="S3.I1.i3.p1.2.1" class="ltx_text ltx_font_italic">SSL Prototypes</span> on <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><msup id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">D</mi><mo id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">D^{\prime}</annotation></semantics></math>, with a selection ratio <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="R_{proto}" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><msub id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mi id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">R</mi><mrow id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml"><mi id="S3.I1.i3.p1.2.m2.1.1.3.2" xref="S3.I1.i3.p1.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.2.m2.1.1.3.1" xref="S3.I1.i3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.I1.i3.p1.2.m2.1.1.3.3" xref="S3.I1.i3.p1.2.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.2.m2.1.1.3.1a" xref="S3.I1.i3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.I1.i3.p1.2.m2.1.1.3.4" xref="S3.I1.i3.p1.2.m2.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.2.m2.1.1.3.1b" xref="S3.I1.i3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.I1.i3.p1.2.m2.1.1.3.5" xref="S3.I1.i3.p1.2.m2.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.2.m2.1.1.3.1c" xref="S3.I1.i3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.I1.i3.p1.2.m2.1.1.3.6" xref="S3.I1.i3.p1.2.m2.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2">𝑅</ci><apply id="S3.I1.i3.p1.2.m2.1.1.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3"><times id="S3.I1.i3.p1.2.m2.1.1.3.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3.1"></times><ci id="S3.I1.i3.p1.2.m2.1.1.3.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3.2">𝑝</ci><ci id="S3.I1.i3.p1.2.m2.1.1.3.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3.3">𝑟</ci><ci id="S3.I1.i3.p1.2.m2.1.1.3.4.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3.4">𝑜</ci><ci id="S3.I1.i3.p1.2.m2.1.1.3.5.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3.5">𝑡</ci><ci id="S3.I1.i3.p1.2.m2.1.1.3.6.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">R_{proto}</annotation></semantics></math></p>
</div>
</li>
</ol>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.3" class="ltx_p">The above-described strategy has an overall selection ratio of <math id="S3.SS4.p7.1.m1.1" class="ltx_Math" alttext="R=R_{dedup}*R_{proto}" display="inline"><semantics id="S3.SS4.p7.1.m1.1a"><mrow id="S3.SS4.p7.1.m1.1.1" xref="S3.SS4.p7.1.m1.1.1.cmml"><mi id="S3.SS4.p7.1.m1.1.1.2" xref="S3.SS4.p7.1.m1.1.1.2.cmml">R</mi><mo id="S3.SS4.p7.1.m1.1.1.1" xref="S3.SS4.p7.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS4.p7.1.m1.1.1.3" xref="S3.SS4.p7.1.m1.1.1.3.cmml"><msub id="S3.SS4.p7.1.m1.1.1.3.2" xref="S3.SS4.p7.1.m1.1.1.3.2.cmml"><mi id="S3.SS4.p7.1.m1.1.1.3.2.2" xref="S3.SS4.p7.1.m1.1.1.3.2.2.cmml">R</mi><mrow id="S3.SS4.p7.1.m1.1.1.3.2.3" xref="S3.SS4.p7.1.m1.1.1.3.2.3.cmml"><mi id="S3.SS4.p7.1.m1.1.1.3.2.3.2" xref="S3.SS4.p7.1.m1.1.1.3.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.2.3.1" xref="S3.SS4.p7.1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.2.3.3" xref="S3.SS4.p7.1.m1.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.2.3.1a" xref="S3.SS4.p7.1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.2.3.4" xref="S3.SS4.p7.1.m1.1.1.3.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.2.3.1b" xref="S3.SS4.p7.1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.2.3.5" xref="S3.SS4.p7.1.m1.1.1.3.2.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.2.3.1c" xref="S3.SS4.p7.1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.2.3.6" xref="S3.SS4.p7.1.m1.1.1.3.2.3.6.cmml">p</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p7.1.m1.1.1.3.1" xref="S3.SS4.p7.1.m1.1.1.3.1.cmml">∗</mo><msub id="S3.SS4.p7.1.m1.1.1.3.3" xref="S3.SS4.p7.1.m1.1.1.3.3.cmml"><mi id="S3.SS4.p7.1.m1.1.1.3.3.2" xref="S3.SS4.p7.1.m1.1.1.3.3.2.cmml">R</mi><mrow id="S3.SS4.p7.1.m1.1.1.3.3.3" xref="S3.SS4.p7.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS4.p7.1.m1.1.1.3.3.3.2" xref="S3.SS4.p7.1.m1.1.1.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.3.3.1" xref="S3.SS4.p7.1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.3.3.3" xref="S3.SS4.p7.1.m1.1.1.3.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.3.3.1a" xref="S3.SS4.p7.1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.3.3.4" xref="S3.SS4.p7.1.m1.1.1.3.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.3.3.1b" xref="S3.SS4.p7.1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.3.3.5" xref="S3.SS4.p7.1.m1.1.1.3.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.1.m1.1.1.3.3.3.1c" xref="S3.SS4.p7.1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS4.p7.1.m1.1.1.3.3.3.6" xref="S3.SS4.p7.1.m1.1.1.3.3.3.6.cmml">o</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.1.m1.1b"><apply id="S3.SS4.p7.1.m1.1.1.cmml" xref="S3.SS4.p7.1.m1.1.1"><eq id="S3.SS4.p7.1.m1.1.1.1.cmml" xref="S3.SS4.p7.1.m1.1.1.1"></eq><ci id="S3.SS4.p7.1.m1.1.1.2.cmml" xref="S3.SS4.p7.1.m1.1.1.2">𝑅</ci><apply id="S3.SS4.p7.1.m1.1.1.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3"><times id="S3.SS4.p7.1.m1.1.1.3.1.cmml" xref="S3.SS4.p7.1.m1.1.1.3.1"></times><apply id="S3.SS4.p7.1.m1.1.1.3.2.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p7.1.m1.1.1.3.2.1.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p7.1.m1.1.1.3.2.2.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.2">𝑅</ci><apply id="S3.SS4.p7.1.m1.1.1.3.2.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3"><times id="S3.SS4.p7.1.m1.1.1.3.2.3.1.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3.1"></times><ci id="S3.SS4.p7.1.m1.1.1.3.2.3.2.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3.2">𝑑</ci><ci id="S3.SS4.p7.1.m1.1.1.3.2.3.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3.3">𝑒</ci><ci id="S3.SS4.p7.1.m1.1.1.3.2.3.4.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3.4">𝑑</ci><ci id="S3.SS4.p7.1.m1.1.1.3.2.3.5.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3.5">𝑢</ci><ci id="S3.SS4.p7.1.m1.1.1.3.2.3.6.cmml" xref="S3.SS4.p7.1.m1.1.1.3.2.3.6">𝑝</ci></apply></apply><apply id="S3.SS4.p7.1.m1.1.1.3.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p7.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS4.p7.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.2">𝑅</ci><apply id="S3.SS4.p7.1.m1.1.1.3.3.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3"><times id="S3.SS4.p7.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3.1"></times><ci id="S3.SS4.p7.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3.2">𝑝</ci><ci id="S3.SS4.p7.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3.3">𝑟</ci><ci id="S3.SS4.p7.1.m1.1.1.3.3.3.4.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3.4">𝑜</ci><ci id="S3.SS4.p7.1.m1.1.1.3.3.3.5.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3.5">𝑡</ci><ci id="S3.SS4.p7.1.m1.1.1.3.3.3.6.cmml" xref="S3.SS4.p7.1.m1.1.1.3.3.3.6">𝑜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.1.m1.1c">R=R_{dedup}*R_{proto}</annotation></semantics></math> and intends to diversify the distribution of our data locally and globally. For brevity we refer to this method as <span id="S3.SS4.p7.3.1" class="ltx_text ltx_font_bold">D4</span>, a shorthand for <span id="S3.SS4.p7.3.2" class="ltx_text ltx_font_italic">Document De-Duplication and Diversification</span>. Throughout this work, we choose <math id="S3.SS4.p7.2.m2.1" class="ltx_Math" alttext="R_{dedup}=0.75" display="inline"><semantics id="S3.SS4.p7.2.m2.1a"><mrow id="S3.SS4.p7.2.m2.1.1" xref="S3.SS4.p7.2.m2.1.1.cmml"><msub id="S3.SS4.p7.2.m2.1.1.2" xref="S3.SS4.p7.2.m2.1.1.2.cmml"><mi id="S3.SS4.p7.2.m2.1.1.2.2" xref="S3.SS4.p7.2.m2.1.1.2.2.cmml">R</mi><mrow id="S3.SS4.p7.2.m2.1.1.2.3" xref="S3.SS4.p7.2.m2.1.1.2.3.cmml"><mi id="S3.SS4.p7.2.m2.1.1.2.3.2" xref="S3.SS4.p7.2.m2.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.2.m2.1.1.2.3.1" xref="S3.SS4.p7.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.2.m2.1.1.2.3.3" xref="S3.SS4.p7.2.m2.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.2.m2.1.1.2.3.1a" xref="S3.SS4.p7.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.2.m2.1.1.2.3.4" xref="S3.SS4.p7.2.m2.1.1.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.2.m2.1.1.2.3.1b" xref="S3.SS4.p7.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.2.m2.1.1.2.3.5" xref="S3.SS4.p7.2.m2.1.1.2.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.2.m2.1.1.2.3.1c" xref="S3.SS4.p7.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS4.p7.2.m2.1.1.2.3.6" xref="S3.SS4.p7.2.m2.1.1.2.3.6.cmml">p</mi></mrow></msub><mo id="S3.SS4.p7.2.m2.1.1.1" xref="S3.SS4.p7.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS4.p7.2.m2.1.1.3" xref="S3.SS4.p7.2.m2.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.2.m2.1b"><apply id="S3.SS4.p7.2.m2.1.1.cmml" xref="S3.SS4.p7.2.m2.1.1"><eq id="S3.SS4.p7.2.m2.1.1.1.cmml" xref="S3.SS4.p7.2.m2.1.1.1"></eq><apply id="S3.SS4.p7.2.m2.1.1.2.cmml" xref="S3.SS4.p7.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p7.2.m2.1.1.2.1.cmml" xref="S3.SS4.p7.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.p7.2.m2.1.1.2.2.cmml" xref="S3.SS4.p7.2.m2.1.1.2.2">𝑅</ci><apply id="S3.SS4.p7.2.m2.1.1.2.3.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3"><times id="S3.SS4.p7.2.m2.1.1.2.3.1.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3.1"></times><ci id="S3.SS4.p7.2.m2.1.1.2.3.2.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3.2">𝑑</ci><ci id="S3.SS4.p7.2.m2.1.1.2.3.3.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3.3">𝑒</ci><ci id="S3.SS4.p7.2.m2.1.1.2.3.4.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3.4">𝑑</ci><ci id="S3.SS4.p7.2.m2.1.1.2.3.5.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3.5">𝑢</ci><ci id="S3.SS4.p7.2.m2.1.1.2.3.6.cmml" xref="S3.SS4.p7.2.m2.1.1.2.3.6">𝑝</ci></apply></apply><cn type="float" id="S3.SS4.p7.2.m2.1.1.3.cmml" xref="S3.SS4.p7.2.m2.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.2.m2.1c">R_{dedup}=0.75</annotation></semantics></math> and vary <math id="S3.SS4.p7.3.m3.1" class="ltx_Math" alttext="R_{proto}" display="inline"><semantics id="S3.SS4.p7.3.m3.1a"><msub id="S3.SS4.p7.3.m3.1.1" xref="S3.SS4.p7.3.m3.1.1.cmml"><mi id="S3.SS4.p7.3.m3.1.1.2" xref="S3.SS4.p7.3.m3.1.1.2.cmml">R</mi><mrow id="S3.SS4.p7.3.m3.1.1.3" xref="S3.SS4.p7.3.m3.1.1.3.cmml"><mi id="S3.SS4.p7.3.m3.1.1.3.2" xref="S3.SS4.p7.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.3.m3.1.1.3.1" xref="S3.SS4.p7.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p7.3.m3.1.1.3.3" xref="S3.SS4.p7.3.m3.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.3.m3.1.1.3.1a" xref="S3.SS4.p7.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p7.3.m3.1.1.3.4" xref="S3.SS4.p7.3.m3.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.3.m3.1.1.3.1b" xref="S3.SS4.p7.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p7.3.m3.1.1.3.5" xref="S3.SS4.p7.3.m3.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p7.3.m3.1.1.3.1c" xref="S3.SS4.p7.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p7.3.m3.1.1.3.6" xref="S3.SS4.p7.3.m3.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.3.m3.1b"><apply id="S3.SS4.p7.3.m3.1.1.cmml" xref="S3.SS4.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p7.3.m3.1.1.1.cmml" xref="S3.SS4.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p7.3.m3.1.1.2.cmml" xref="S3.SS4.p7.3.m3.1.1.2">𝑅</ci><apply id="S3.SS4.p7.3.m3.1.1.3.cmml" xref="S3.SS4.p7.3.m3.1.1.3"><times id="S3.SS4.p7.3.m3.1.1.3.1.cmml" xref="S3.SS4.p7.3.m3.1.1.3.1"></times><ci id="S3.SS4.p7.3.m3.1.1.3.2.cmml" xref="S3.SS4.p7.3.m3.1.1.3.2">𝑝</ci><ci id="S3.SS4.p7.3.m3.1.1.3.3.cmml" xref="S3.SS4.p7.3.m3.1.1.3.3">𝑟</ci><ci id="S3.SS4.p7.3.m3.1.1.3.4.cmml" xref="S3.SS4.p7.3.m3.1.1.3.4">𝑜</ci><ci id="S3.SS4.p7.3.m3.1.1.3.5.cmml" xref="S3.SS4.p7.3.m3.1.1.3.5">𝑡</ci><ci id="S3.SS4.p7.3.m3.1.1.3.6.cmml" xref="S3.SS4.p7.3.m3.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.3.m3.1c">R_{proto}</annotation></semantics></math> (we discuss this choice in Section&nbsp;<a href="#A1.SS1" title="A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>). In Section&nbsp;<a href="#S4" title="4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare the performance of D4 to baseline training and other methods, and in Section&nbsp;<a href="#S4.SS4" title="4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a> we analyze D4 and show that reclustering after semantic de-duplication indeed reduces the impact of duplicate-driven clusters (see Figure&nbsp;<a href="#S4.F7" title="Figure 7 ‣ 4.4.2 Importance of re-clustering between SemDeDup and SSL Prototypes ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x2.png" id="S4.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="332" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.8.3.1" style="font-size:90%;">Figure 2</span>:</span><span class="ltx_text" id="S4.F2.5.4.2" style="font-size:90%;">Comparison of data selection methods on validation perplexity. 각 포인트는 40B 토큰에 대해 트레이닝된 1.3B OPT 모델을 나타낸다. x축은 선택비 <math alttext="R" class="ltx_Math" display="inline" id="S4.F2.4.3.1.m1.1"><semantics id="S4.F2.4.3.1.m1.1b"><mi id="S4.F2.4.3.1.m1.1.1" xref="S4.F2.4.3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.F2.4.3.1.m1.1c"><ci id="S4.F2.4.3.1.m1.1.1.cmml" xref="S4.F2.4.3.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.3.1.m1.1d">R</annotation></semantics></math>를 나타낸다. 상단 2 및 하단 왼쪽 그래프의 y축은 복잡성을 나타내며, 하단 오른쪽 그래프는 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 16개 NLP 작업에서 평균 다운스트림이다. 회색선은 기준선 훈련의 값을 나타냅니다. 음영 오차는 3개의 씨앗에 대한 표준 오차이다. <span class="ltx_text ltx_font_bold" id="S4.F2.5.4.2.1">이 그래프의 각 점은 동일한 토큰 예산에서 훈련됩니다</span>: <math alttext="R" class="ltx_Math" display="inline" id="S4.F2.5.4.2.m2.1"><semantics id="S4.F2.5.4.2.m2.1b"><mi id="S4.F2.5.4.2.m2.1.1" xref="S4.F2.5.4.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.F2.5.4.2.m2.1c"><ci id="S4.F2.5.4.2.m2.1.1.cmml" xref="S4.F2.5.4.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.5.4.2.m2.1d">R</annotation></semantics></math>를 줄이면 소스 데이터 세트의 크기를 공동으로 증가시킵니다(예: 4x' 크기의 소스 데이터 세트에서 문서의 1/4을 선택). </span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fixed compute regime: can data selection help on fixed token budgets?</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.7" class="ltx_p">In this section, we consider the fixed compute setting, where we curate and train on a fixed token budget by jointly increasing the size of the source dataset <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">D</mi><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1a" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.1.1.3.4" xref="S4.SS1.p1.1.m1.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1b" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.1.1.3.5" xref="S4.SS1.p1.1.m1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1c" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.1.1.3.6" xref="S4.SS1.p1.1.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.3.1d" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.1.m1.1.1.3.7" xref="S4.SS1.p1.1.m1.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝐷</ci><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><times id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.1"></times><ci id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">𝑠</ci><ci id="S4.SS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S4.SS1.p1.1.m1.1.1.3.4.cmml" xref="S4.SS1.p1.1.m1.1.1.3.4">𝑢</ci><ci id="S4.SS1.p1.1.m1.1.1.3.5.cmml" xref="S4.SS1.p1.1.m1.1.1.3.5">𝑟</ci><ci id="S4.SS1.p1.1.m1.1.1.3.6.cmml" xref="S4.SS1.p1.1.m1.1.1.3.6">𝑐</ci><ci id="S4.SS1.p1.1.m1.1.1.3.7.cmml" xref="S4.SS1.p1.1.m1.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">D_{source}</annotation></semantics></math> and decreasing <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">R</annotation></semantics></math> (the fraction of the <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><msub id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">D</mi><mrow id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.3.2" xref="S4.SS1.p1.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.3.1" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3.3" xref="S4.SS1.p1.3.m3.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.3.1a" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3.4" xref="S4.SS1.p1.3.m3.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.3.1b" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3.5" xref="S4.SS1.p1.3.m3.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.3.1c" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3.6" xref="S4.SS1.p1.3.m3.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.3.1d" xref="S4.SS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.3.m3.1.1.3.7" xref="S4.SS1.p1.3.m3.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">𝐷</ci><apply id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3"><times id="S4.SS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.3.1"></times><ci id="S4.SS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.3.2">𝑠</ci><ci id="S4.SS1.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3.3">𝑜</ci><ci id="S4.SS1.p1.3.m3.1.1.3.4.cmml" xref="S4.SS1.p1.3.m3.1.1.3.4">𝑢</ci><ci id="S4.SS1.p1.3.m3.1.1.3.5.cmml" xref="S4.SS1.p1.3.m3.1.1.3.5">𝑟</ci><ci id="S4.SS1.p1.3.m3.1.1.3.6.cmml" xref="S4.SS1.p1.3.m3.1.1.3.6">𝑐</ci><ci id="S4.SS1.p1.3.m3.1.1.3.7.cmml" xref="S4.SS1.p1.3.m3.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">D_{source}</annotation></semantics></math> which is selected), such that the target token budget remains constant. This setting is analogous to the most common paradigm for LLM training. As <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><msub id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">D</mi><mrow id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1a" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3.4" xref="S4.SS1.p1.4.m4.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1b" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3.5" xref="S4.SS1.p1.4.m4.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1c" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3.6" xref="S4.SS1.p1.4.m4.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.3.1d" xref="S4.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.4.m4.1.1.3.7" xref="S4.SS1.p1.4.m4.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">𝐷</ci><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><times id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.2">𝑠</ci><ci id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3">𝑜</ci><ci id="S4.SS1.p1.4.m4.1.1.3.4.cmml" xref="S4.SS1.p1.4.m4.1.1.3.4">𝑢</ci><ci id="S4.SS1.p1.4.m4.1.1.3.5.cmml" xref="S4.SS1.p1.4.m4.1.1.3.5">𝑟</ci><ci id="S4.SS1.p1.4.m4.1.1.3.6.cmml" xref="S4.SS1.p1.4.m4.1.1.3.6">𝑐</ci><ci id="S4.SS1.p1.4.m4.1.1.3.7.cmml" xref="S4.SS1.p1.4.m4.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">D_{source}</annotation></semantics></math> grows and <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">R</annotation></semantics></math> decreases, we select from larger and larger initial datasets, resulting in a larger set of high-quality data points to select from and increasing the overall quality of the selected set. For clarity, we plot performance as a function of the ratio of the <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="D_{source}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><msub id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">D</mi><mrow id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml"><mi id="S4.SS1.p1.6.m6.1.1.3.2" xref="S4.SS1.p1.6.m6.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.1.1.3.1" xref="S4.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.1.1.3.3" xref="S4.SS1.p1.6.m6.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.1.1.3.1a" xref="S4.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.1.1.3.4" xref="S4.SS1.p1.6.m6.1.1.3.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.1.1.3.1b" xref="S4.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.1.1.3.5" xref="S4.SS1.p1.6.m6.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.1.1.3.1c" xref="S4.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.1.1.3.6" xref="S4.SS1.p1.6.m6.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.1.1.3.1d" xref="S4.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.1.1.3.7" xref="S4.SS1.p1.6.m6.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">𝐷</ci><apply id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3"><times id="S4.SS1.p1.6.m6.1.1.3.1.cmml" xref="S4.SS1.p1.6.m6.1.1.3.1"></times><ci id="S4.SS1.p1.6.m6.1.1.3.2.cmml" xref="S4.SS1.p1.6.m6.1.1.3.2">𝑠</ci><ci id="S4.SS1.p1.6.m6.1.1.3.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3">𝑜</ci><ci id="S4.SS1.p1.6.m6.1.1.3.4.cmml" xref="S4.SS1.p1.6.m6.1.1.3.4">𝑢</ci><ci id="S4.SS1.p1.6.m6.1.1.3.5.cmml" xref="S4.SS1.p1.6.m6.1.1.3.5">𝑟</ci><ci id="S4.SS1.p1.6.m6.1.1.3.6.cmml" xref="S4.SS1.p1.6.m6.1.1.3.6">𝑐</ci><ci id="S4.SS1.p1.6.m6.1.1.3.7.cmml" xref="S4.SS1.p1.6.m6.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">D_{source}</annotation></semantics></math> to <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="D_{target}" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><msub id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml"><mi id="S4.SS1.p1.7.m7.1.1.2" xref="S4.SS1.p1.7.m7.1.1.2.cmml">D</mi><mrow id="S4.SS1.p1.7.m7.1.1.3" xref="S4.SS1.p1.7.m7.1.1.3.cmml"><mi id="S4.SS1.p1.7.m7.1.1.3.2" xref="S4.SS1.p1.7.m7.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m7.1.1.3.1" xref="S4.SS1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.7.m7.1.1.3.3" xref="S4.SS1.p1.7.m7.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m7.1.1.3.1a" xref="S4.SS1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.7.m7.1.1.3.4" xref="S4.SS1.p1.7.m7.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m7.1.1.3.1b" xref="S4.SS1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.7.m7.1.1.3.5" xref="S4.SS1.p1.7.m7.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m7.1.1.3.1c" xref="S4.SS1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.7.m7.1.1.3.6" xref="S4.SS1.p1.7.m7.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.7.m7.1.1.3.1d" xref="S4.SS1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p1.7.m7.1.1.3.7" xref="S4.SS1.p1.7.m7.1.1.3.7.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><apply id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.7.m7.1.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p1.7.m7.1.1.2.cmml" xref="S4.SS1.p1.7.m7.1.1.2">𝐷</ci><apply id="S4.SS1.p1.7.m7.1.1.3.cmml" xref="S4.SS1.p1.7.m7.1.1.3"><times id="S4.SS1.p1.7.m7.1.1.3.1.cmml" xref="S4.SS1.p1.7.m7.1.1.3.1"></times><ci id="S4.SS1.p1.7.m7.1.1.3.2.cmml" xref="S4.SS1.p1.7.m7.1.1.3.2">𝑡</ci><ci id="S4.SS1.p1.7.m7.1.1.3.3.cmml" xref="S4.SS1.p1.7.m7.1.1.3.3">𝑎</ci><ci id="S4.SS1.p1.7.m7.1.1.3.4.cmml" xref="S4.SS1.p1.7.m7.1.1.3.4">𝑟</ci><ci id="S4.SS1.p1.7.m7.1.1.3.5.cmml" xref="S4.SS1.p1.7.m7.1.1.3.5">𝑔</ci><ci id="S4.SS1.p1.7.m7.1.1.3.6.cmml" xref="S4.SS1.p1.7.m7.1.1.3.6">𝑒</ci><ci id="S4.SS1.p1.7.m7.1.1.3.7.cmml" xref="S4.SS1.p1.7.m7.1.1.3.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">D_{target}</annotation></semantics></math>. For each setting, we evaluate the performance of a baseline, SemDeDup alone, SSL Prototypes alone, and our proposed method D4.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.2.1">Validation Perplexity. </span> 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">2</span></a>에서 우리는 세 가지 방법 중 하나를 사용하는 비교적 적은 양의 데이터 선택(small <math alttext="R" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">R</annotation></semantics></math>)이 모든 검증 세트에 일관된 개선을 가져온다는 것을 보여준다. 그러나 <math alttext="R" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">R</annotation></semantics></math>를 증가시킴에 따라 웹 스냅샷 및 비 웹 스냅샷 유효성 검사 세트에서 <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.2.2">opposing effects</span>을 관찰합니다. 우리는 섹션 <a class="ltx_ref" href="#S4.SS4" title="4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.4</span></a>에서 이러한 불일치를 심층적으로 분석한다. 그러나 LLM이 달성하기를 원하는 고품질 세대에 훨씬 더 밀접하게 해당하는 지시 OPT 검증 세트에서 세 가지 방법 모두 일관되고 명확한 복잡성 개선을 초래한다는 것을 발견했다. 특히, 세 가지 방법 모두 이점을 제공했지만 D4는 SemDeDup 및 SSL 프로토타입을 독립적으로 사용하여 성능이 향상되었으며 소스 데이터 세트가 목표 데이터 세트 크기의 약 4배일 때 가장 두드러진 이득이 나타났다. D4가 소스 데이터 세트 크기에 따라 일관되게 개선된다는 점을 감안할 때 이 간격은 소스 데이터 세트 크기에 따라 증가할 것으로 추정한다.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Downstream Task Accuracy. </span> 그림 <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">2</span></a>에서 우리는 또한 NLP 작업의 제품군에 걸쳐 평균화된 0-샷 다운스트림 정확도를 보고한다. 다운스트림 정확도의 높은 분산은 다양한 모델의 성능에서 명확한 경향을 식별하는 것을 어렵게 하지만, 0-샷 다운스트림 정확도는 일반적으로 소스 데이터 세트 크기에 따라 증가한다는 것을 다시 관찰한다.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p4.2">우리의 연구 결과는 또한 더 큰 모델 규모에서 유지됩니다. 우리는 1.3B OPT 실험(예: <math alttext="R=0.25" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">R</mi><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><eq id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></eq><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">𝑅</ci><cn id="S4.SS1.p4.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.p4.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">R=0.25</annotation></semantics></math>)에서 가장 성능이 좋은 구성을 선택하고 100B 토큰에서 6.7B OPT 모델을 훈련한다. 그림 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">1</span></a>는 6.7B 모델에 대해 <math alttext="R=0.25" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2.1"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">R</mi><mo id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><eq id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></eq><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">𝑅</ci><cn id="S4.SS1.p4.2.m2.1.1.3.cmml" type="float" xref="S4.SS1.p4.2.m2.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">R=0.25</annotation></semantics></math>로 D4를 적용한 긍정적인 효과를 보여준다. 프루닝된 데이터에 대해 트레이닝된 모델은 평균적으로 20% 더 적은 업데이트 단계를 사용하여 베이스라인 모델과 동일한 복잡도에 도달하고 트레이닝이 끝날 때 다운스트림 태스크 제품군에서 정확도의 2% 개선을 달성한다 - 동일한 태스크 세트에 대한 모델의 OPT와 GPT-3 제품군 사이의 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에 의해 보고된 것과 거의 동일한 차이이다(<cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 그림 3 참조).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Fixed data regime: what happens when we run out of data?</h3>

<figure id="S4.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.7.3.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text" id="S4.F3.5.4.2" style="font-size:90%;">Comparing new tokens vs. 랜덤 데이터 선택을 위한 반복 토큰과 1.3B OPT 사전 트레이닝을 위한 고정 선택 비율 <math alttext="R=0.25" class="ltx_Math" display="inline" id="S4.F3.4.3.1.m1.1"><semantics id="S4.F3.4.3.1.m1.1b"><mrow id="S4.F3.4.3.1.m1.1.1" xref="S4.F3.4.3.1.m1.1.1.cmml"><mi id="S4.F3.4.3.1.m1.1.1.2" xref="S4.F3.4.3.1.m1.1.1.2.cmml">R</mi><mo id="S4.F3.4.3.1.m1.1.1.1" xref="S4.F3.4.3.1.m1.1.1.1.cmml">=</mo><mn id="S4.F3.4.3.1.m1.1.1.3" xref="S4.F3.4.3.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.4.3.1.m1.1c"><apply id="S4.F3.4.3.1.m1.1.1.cmml" xref="S4.F3.4.3.1.m1.1.1"><eq id="S4.F3.4.3.1.m1.1.1.1.cmml" xref="S4.F3.4.3.1.m1.1.1.1"></eq><ci id="S4.F3.4.3.1.m1.1.1.2.cmml" xref="S4.F3.4.3.1.m1.1.1.2">𝑅</ci><cn id="S4.F3.4.3.1.m1.1.1.3.cmml" type="float" xref="S4.F3.4.3.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.3.1.m1.1d">R=0.25</annotation></semantics></math>에 대한 D4를 반복한다. 각 방법은 소스 데이터 세트 <math alttext="D_{source}" class="ltx_Math" display="inline" id="S4.F3.5.4.2.m2.1"><semantics id="S4.F3.5.4.2.m2.1b"><msub id="S4.F3.5.4.2.m2.1.1" xref="S4.F3.5.4.2.m2.1.1.cmml"><mi id="S4.F3.5.4.2.m2.1.1.2" xref="S4.F3.5.4.2.m2.1.1.2.cmml">D</mi><mrow id="S4.F3.5.4.2.m2.1.1.3" xref="S4.F3.5.4.2.m2.1.1.3.cmml"><mi id="S4.F3.5.4.2.m2.1.1.3.2" xref="S4.F3.5.4.2.m2.1.1.3.2.cmml">s</mi><mo id="S4.F3.5.4.2.m2.1.1.3.1" lspace="0em" rspace="0em" xref="S4.F3.5.4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.F3.5.4.2.m2.1.1.3.3" xref="S4.F3.5.4.2.m2.1.1.3.3.cmml">o</mi><mo id="S4.F3.5.4.2.m2.1.1.3.1b" lspace="0em" rspace="0em" xref="S4.F3.5.4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.F3.5.4.2.m2.1.1.3.4" xref="S4.F3.5.4.2.m2.1.1.3.4.cmml">u</mi><mo id="S4.F3.5.4.2.m2.1.1.3.1c" lspace="0em" rspace="0em" xref="S4.F3.5.4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.F3.5.4.2.m2.1.1.3.5" xref="S4.F3.5.4.2.m2.1.1.3.5.cmml">r</mi><mo id="S4.F3.5.4.2.m2.1.1.3.1d" lspace="0em" rspace="0em" xref="S4.F3.5.4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.F3.5.4.2.m2.1.1.3.6" xref="S4.F3.5.4.2.m2.1.1.3.6.cmml">c</mi><mo id="S4.F3.5.4.2.m2.1.1.3.1e" lspace="0em" rspace="0em" xref="S4.F3.5.4.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.F3.5.4.2.m2.1.1.3.7" xref="S4.F3.5.4.2.m2.1.1.3.7.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.F3.5.4.2.m2.1c"><apply id="S4.F3.5.4.2.m2.1.1.cmml" xref="S4.F3.5.4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.F3.5.4.2.m2.1.1.1.cmml" xref="S4.F3.5.4.2.m2.1.1">subscript</csymbol><ci id="S4.F3.5.4.2.m2.1.1.2.cmml" xref="S4.F3.5.4.2.m2.1.1.2">𝐷</ci><apply id="S4.F3.5.4.2.m2.1.1.3.cmml" xref="S4.F3.5.4.2.m2.1.1.3"><times id="S4.F3.5.4.2.m2.1.1.3.1.cmml" xref="S4.F3.5.4.2.m2.1.1.3.1"></times><ci id="S4.F3.5.4.2.m2.1.1.3.2.cmml" xref="S4.F3.5.4.2.m2.1.1.3.2">𝑠</ci><ci id="S4.F3.5.4.2.m2.1.1.3.3.cmml" xref="S4.F3.5.4.2.m2.1.1.3.3">𝑜</ci><ci id="S4.F3.5.4.2.m2.1.1.3.4.cmml" xref="S4.F3.5.4.2.m2.1.1.3.4">𝑢</ci><ci id="S4.F3.5.4.2.m2.1.1.3.5.cmml" xref="S4.F3.5.4.2.m2.1.1.3.5">𝑟</ci><ci id="S4.F3.5.4.2.m2.1.1.3.6.cmml" xref="S4.F3.5.4.2.m2.1.1.3.6">𝑐</ci><ci id="S4.F3.5.4.2.m2.1.1.3.7.cmml" xref="S4.F3.5.4.2.m2.1.1.3.7">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.4.2.m2.1d">D_{source}</annotation></semantics></math>에서 문서의 25%를 선택하고, 40B의 목표 토큰 버짓에 도달할 때까지 해당 서브세트에 걸쳐 에포크를 선택한다. 우리는 D4를 통한 반복 토큰이 베이스라인 트레이닝(랜덤, 새로운 토큰)보다 우수하다는 것을 관찰한다. </span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1"><a class="ltx_ref" href="#S4.SS1" title="4.1 Fixed compute regime: can data selection help on fixed token budgets? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.1</span></a> 섹션의 결과는 훈련을 위한 고정된 양의 컴퓨팅이 주어지면 더 크고 더 큰 소스 데이터 세트에서 데이터를 선택하는 것이 언어 모델 성능을 향상시키는 유망한 방법임을 나타낸다. 그러나 웹에서 얼마나 많은 데이터를 큐레이션할 수 있는지에 대한 실질적인 한계가 있으며, 따라서 소스 데이터 세트의 크기에 대한 자연스러운 한계가 있다. 데이터가 부족하면 어떻게 되나요? <cite class="ltx_cite ltx_citemacro_citet">Hernandez et al. [<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite>는 학습 데이터에서 반복되는 데이터 포인트의 불균형적인 악영향을 발견하여 분석하였다. 유사하게, 동시에 우리의 작업 <cite class="ltx_cite ltx_citemacro_citet">Muennighoff et al. [<a class="ltx_ref" href="#bib.bib37" title="">37</a>]</cite>는 C4의 무작위 하위 집합에 4회 이상 에포싱할 때 테스트 손실이 악화된다는 것을 보여준다. 본 절에서는 이러한 제한된 데이터인 다중 에포크 설정에서 D4의 사용이 모델 성능에 어떤 영향을 미치는지 조사한다.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.15" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_left"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mi mathsize="90%" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">S</annotation></semantics></math></td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_left"><math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="T_{total}" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><msub id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml"><mi mathsize="90%" id="S4.T1.2.2.2.m1.1.1.2" xref="S4.T1.2.2.2.m1.1.1.2.cmml">T</mi><mrow id="S4.T1.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.m1.1.1.3.cmml"><mi mathsize="90%" id="S4.T1.2.2.2.m1.1.1.3.2" xref="S4.T1.2.2.2.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.3.1" xref="S4.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.2.2.2.m1.1.1.3.3" xref="S4.T1.2.2.2.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.3.1a" xref="S4.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.2.2.2.m1.1.1.3.4" xref="S4.T1.2.2.2.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.3.1b" xref="S4.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.2.2.2.m1.1.1.3.5" xref="S4.T1.2.2.2.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.3.1c" xref="S4.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.2.2.2.m1.1.1.3.6" xref="S4.T1.2.2.2.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.m1.1.1.2">𝑇</ci><apply id="S4.T1.2.2.2.m1.1.1.3.cmml" xref="S4.T1.2.2.2.m1.1.1.3"><times id="S4.T1.2.2.2.m1.1.1.3.1.cmml" xref="S4.T1.2.2.2.m1.1.1.3.1"></times><ci id="S4.T1.2.2.2.m1.1.1.3.2.cmml" xref="S4.T1.2.2.2.m1.1.1.3.2">𝑡</ci><ci id="S4.T1.2.2.2.m1.1.1.3.3.cmml" xref="S4.T1.2.2.2.m1.1.1.3.3">𝑜</ci><ci id="S4.T1.2.2.2.m1.1.1.3.4.cmml" xref="S4.T1.2.2.2.m1.1.1.3.4">𝑡</ci><ci id="S4.T1.2.2.2.m1.1.1.3.5.cmml" xref="S4.T1.2.2.2.m1.1.1.3.5">𝑎</ci><ci id="S4.T1.2.2.2.m1.1.1.3.6.cmml" xref="S4.T1.2.2.2.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">T_{total}</annotation></semantics></math></td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_left"><math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="T_{selected}" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><msub id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml"><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.2" xref="S4.T1.3.3.3.m1.1.1.2.cmml">T</mi><mrow id="S4.T1.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.m1.1.1.3.cmml"><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.2" xref="S4.T1.3.3.3.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.3" xref="S4.T1.3.3.3.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1a" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.4" xref="S4.T1.3.3.3.m1.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1b" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.5" xref="S4.T1.3.3.3.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1c" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.6" xref="S4.T1.3.3.3.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1d" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.7" xref="S4.T1.3.3.3.m1.1.1.3.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1e" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.8" xref="S4.T1.3.3.3.m1.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.m1.1.1.3.1f" xref="S4.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.T1.3.3.3.m1.1.1.3.9" xref="S4.T1.3.3.3.m1.1.1.3.9.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.m1.1.1.2">𝑇</ci><apply id="S4.T1.3.3.3.m1.1.1.3.cmml" xref="S4.T1.3.3.3.m1.1.1.3"><times id="S4.T1.3.3.3.m1.1.1.3.1.cmml" xref="S4.T1.3.3.3.m1.1.1.3.1"></times><ci id="S4.T1.3.3.3.m1.1.1.3.2.cmml" xref="S4.T1.3.3.3.m1.1.1.3.2">𝑠</ci><ci id="S4.T1.3.3.3.m1.1.1.3.3.cmml" xref="S4.T1.3.3.3.m1.1.1.3.3">𝑒</ci><ci id="S4.T1.3.3.3.m1.1.1.3.4.cmml" xref="S4.T1.3.3.3.m1.1.1.3.4">𝑙</ci><ci id="S4.T1.3.3.3.m1.1.1.3.5.cmml" xref="S4.T1.3.3.3.m1.1.1.3.5">𝑒</ci><ci id="S4.T1.3.3.3.m1.1.1.3.6.cmml" xref="S4.T1.3.3.3.m1.1.1.3.6">𝑐</ci><ci id="S4.T1.3.3.3.m1.1.1.3.7.cmml" xref="S4.T1.3.3.3.m1.1.1.3.7">𝑡</ci><ci id="S4.T1.3.3.3.m1.1.1.3.8.cmml" xref="S4.T1.3.3.3.m1.1.1.3.8">𝑒</ci><ci id="S4.T1.3.3.3.m1.1.1.3.9.cmml" xref="S4.T1.3.3.3.m1.1.1.3.9">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">T_{selected}</annotation></semantics></math></td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T1.3.3.4.1" class="ltx_text" style="font-size:90%;">Epochs</span></td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.3.5.1" class="ltx_text" style="font-size:90%;">Non-Web Snapshot PPL</span></td>
<td id="S4.T1.3.3.6" class="ltx_td ltx_align_left"><span id="S4.T1.3.3.6.1" class="ltx_text" style="font-size:90%;">Instruction + Answers PPL</span></td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.4" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S4.T1.6.6.4.1" class="ltx_text" style="font-size:90%;">Random</span></td>
<td id="S4.T1.6.6.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.6.6.5.1" class="ltx_text" style="font-size:90%;">40B</span></td>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.6.6.6.1" class="ltx_text" style="font-size:90%;">40B</span></td>
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mn mathsize="90%" id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><cn type="integer" id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">1</annotation></semantics></math></td>
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T1.5.5.2.m1.1" class="ltx_Math" alttext="16.27\pm 0.012" display="inline"><semantics id="S4.T1.5.5.2.m1.1a"><mrow id="S4.T1.5.5.2.m1.1.1" xref="S4.T1.5.5.2.m1.1.1.cmml"><mn mathsize="90%" id="S4.T1.5.5.2.m1.1.1.2" xref="S4.T1.5.5.2.m1.1.1.2.cmml">16.27</mn><mo mathsize="90%" id="S4.T1.5.5.2.m1.1.1.1" xref="S4.T1.5.5.2.m1.1.1.1.cmml">±</mo><mn mathsize="90%" id="S4.T1.5.5.2.m1.1.1.3" xref="S4.T1.5.5.2.m1.1.1.3.cmml">0.012</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.2.m1.1b"><apply id="S4.T1.5.5.2.m1.1.1.cmml" xref="S4.T1.5.5.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.5.5.2.m1.1.1.1.cmml" xref="S4.T1.5.5.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.5.5.2.m1.1.1.2.cmml" xref="S4.T1.5.5.2.m1.1.1.2">16.27</cn><cn type="float" id="S4.T1.5.5.2.m1.1.1.3.cmml" xref="S4.T1.5.5.2.m1.1.1.3">0.012</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.2.m1.1c">16.27\pm 0.012</annotation></semantics></math></td>
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T1.6.6.3.m1.1" class="ltx_Math" alttext="14.19\pm 0.003" display="inline"><semantics id="S4.T1.6.6.3.m1.1a"><mrow id="S4.T1.6.6.3.m1.1.1" xref="S4.T1.6.6.3.m1.1.1.cmml"><mn mathsize="90%" id="S4.T1.6.6.3.m1.1.1.2" xref="S4.T1.6.6.3.m1.1.1.2.cmml">14.19</mn><mo mathsize="90%" id="S4.T1.6.6.3.m1.1.1.1" xref="S4.T1.6.6.3.m1.1.1.1.cmml">±</mo><mn mathsize="90%" id="S4.T1.6.6.3.m1.1.1.3" xref="S4.T1.6.6.3.m1.1.1.3.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.3.m1.1b"><apply id="S4.T1.6.6.3.m1.1.1.cmml" xref="S4.T1.6.6.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.6.6.3.m1.1.1.1.cmml" xref="S4.T1.6.6.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.6.6.3.m1.1.1.2.cmml" xref="S4.T1.6.6.3.m1.1.1.2">14.19</cn><cn type="float" id="S4.T1.6.6.3.m1.1.1.3.cmml" xref="S4.T1.6.6.3.m1.1.1.3">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.3.m1.1c">14.19\pm 0.003</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.11.11" class="ltx_tr">
<td id="S4.T1.11.11.6" class="ltx_td ltx_align_left"><span id="S4.T1.11.11.6.1" class="ltx_text" style="font-size:90%;">40B</span></td>
<td id="S4.T1.11.11.7" class="ltx_td ltx_align_left"><span id="S4.T1.11.11.7.1" class="ltx_text" style="font-size:90%;">20B</span></td>
<td id="S4.T1.7.7.1" class="ltx_td ltx_align_left ltx_border_r"><math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mn mathsize="90%" id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><cn type="integer" id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">2</annotation></semantics></math></td>
<td id="S4.T1.9.9.3" class="ltx_td ltx_align_left">
<math id="S4.T1.8.8.2.m1.1" class="ltx_Math" alttext="16.39\pm 0.011" display="inline"><semantics id="S4.T1.8.8.2.m1.1a"><mrow id="S4.T1.8.8.2.m1.1.1" xref="S4.T1.8.8.2.m1.1.1.cmml"><mn mathsize="90%" id="S4.T1.8.8.2.m1.1.1.2" xref="S4.T1.8.8.2.m1.1.1.2.cmml">16.39</mn><mo mathsize="90%" id="S4.T1.8.8.2.m1.1.1.1" xref="S4.T1.8.8.2.m1.1.1.1.cmml">±</mo><mn mathsize="90%" id="S4.T1.8.8.2.m1.1.1.3" xref="S4.T1.8.8.2.m1.1.1.3.cmml">0.011</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.2.m1.1b"><apply id="S4.T1.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.8.8.2.m1.1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.8.8.2.m1.1.1.2.cmml" xref="S4.T1.8.8.2.m1.1.1.2">16.39</cn><cn type="float" id="S4.T1.8.8.2.m1.1.1.3.cmml" xref="S4.T1.8.8.2.m1.1.1.3">0.011</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.2.m1.1c">16.39\pm 0.011</annotation></semantics></math><span id="S4.T1.9.9.3.2" class="ltx_text" style="font-size:90%;"> </span><span id="S4.T1.9.9.3.1" class="ltx_text" style="font-size:90%;color:#A8213D;">(+<math id="S4.T1.9.9.3.1.m1.1" class="ltx_Math" alttext="0.12" display="inline"><semantics id="S4.T1.9.9.3.1.m1.1a"><mn mathcolor="#A8213D" id="S4.T1.9.9.3.1.m1.1.1" xref="S4.T1.9.9.3.1.m1.1.1.cmml">0.12</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.3.1.m1.1b"><cn type="float" id="S4.T1.9.9.3.1.m1.1.1.cmml" xref="S4.T1.9.9.3.1.m1.1.1">0.12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.3.1.m1.1c">0.12</annotation></semantics></math>)</span>
</td>
<td id="S4.T1.11.11.5" class="ltx_td ltx_align_left">
<math id="S4.T1.10.10.4.m1.1" class="ltx_Math" alttext="14.37\pm 0.015" display="inline"><semantics id="S4.T1.10.10.4.m1.1a"><mrow id="S4.T1.10.10.4.m1.1.1" xref="S4.T1.10.10.4.m1.1.1.cmml"><mn mathsize="90%" id="S4.T1.10.10.4.m1.1.1.2" xref="S4.T1.10.10.4.m1.1.1.2.cmml">14.37</mn><mo mathsize="90%" id="S4.T1.10.10.4.m1.1.1.1" xref="S4.T1.10.10.4.m1.1.1.1.cmml">±</mo><mn mathsize="90%" id="S4.T1.10.10.4.m1.1.1.3" xref="S4.T1.10.10.4.m1.1.1.3.cmml">0.015</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.4.m1.1b"><apply id="S4.T1.10.10.4.m1.1.1.cmml" xref="S4.T1.10.10.4.m1.1.1"><csymbol cd="latexml" id="S4.T1.10.10.4.m1.1.1.1.cmml" xref="S4.T1.10.10.4.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.10.10.4.m1.1.1.2.cmml" xref="S4.T1.10.10.4.m1.1.1.2">14.37</cn><cn type="float" id="S4.T1.10.10.4.m1.1.1.3.cmml" xref="S4.T1.10.10.4.m1.1.1.3">0.015</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.4.m1.1c">14.37\pm 0.015</annotation></semantics></math><span id="S4.T1.11.11.5.2" class="ltx_text" style="font-size:90%;"> </span><span id="S4.T1.11.11.5.1" class="ltx_text" style="font-size:90%;color:#A8213D;">(+<math id="S4.T1.11.11.5.1.m1.1" class="ltx_Math" alttext="0.18" display="inline"><semantics id="S4.T1.11.11.5.1.m1.1a"><mn mathcolor="#A8213D" id="S4.T1.11.11.5.1.m1.1.1" xref="S4.T1.11.11.5.1.m1.1.1.cmml">0.18</mn><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.5.1.m1.1b"><cn type="float" id="S4.T1.11.11.5.1.m1.1.1.cmml" xref="S4.T1.11.11.5.1.m1.1.1">0.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.5.1.m1.1c">0.18</annotation></semantics></math>)</span>
</td>
</tr>
<tr id="S4.T1.15.15" class="ltx_tr">
<td id="S4.T1.15.15.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.15.15.5.1" class="ltx_text" style="font-size:90%;">D4</span></td>
<td id="S4.T1.15.15.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.15.15.6.1" class="ltx_text" style="font-size:90%;">40B</span></td>
<td id="S4.T1.15.15.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.15.15.7.1" class="ltx_text" style="font-size:90%;">20B</span></td>
<td id="S4.T1.15.15.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T1.15.15.8.1" class="ltx_text" style="font-size:90%;">2</span></td>
<td id="S4.T1.13.13.2" class="ltx_td ltx_align_left ltx_border_t">
<math id="S4.T1.12.12.1.m1.1" class="ltx_Math" alttext="\textbf{16.10}\pm 0.024" display="inline"><semantics id="S4.T1.12.12.1.m1.1a"><mrow id="S4.T1.12.12.1.m1.1.1" xref="S4.T1.12.12.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" mathsize="90%" id="S4.T1.12.12.1.m1.1.1.2" xref="S4.T1.12.12.1.m1.1.1.2a.cmml">16.10</mtext><mo mathsize="90%" id="S4.T1.12.12.1.m1.1.1.1" xref="S4.T1.12.12.1.m1.1.1.1.cmml">±</mo><mn mathsize="90%" id="S4.T1.12.12.1.m1.1.1.3" xref="S4.T1.12.12.1.m1.1.1.3.cmml">0.024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.1.m1.1b"><apply id="S4.T1.12.12.1.m1.1.1.cmml" xref="S4.T1.12.12.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.12.12.1.m1.1.1.1.cmml" xref="S4.T1.12.12.1.m1.1.1.1">plus-or-minus</csymbol><ci id="S4.T1.12.12.1.m1.1.1.2a.cmml" xref="S4.T1.12.12.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" mathsize="90%" id="S4.T1.12.12.1.m1.1.1.2.cmml" xref="S4.T1.12.12.1.m1.1.1.2">16.10</mtext></ci><cn type="float" id="S4.T1.12.12.1.m1.1.1.3.cmml" xref="S4.T1.12.12.1.m1.1.1.3">0.024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.1.m1.1c">\textbf{16.10}\pm 0.024</annotation></semantics></math><span id="S4.T1.13.13.2.2" class="ltx_text" style="font-size:90%;"> </span><span id="S4.T1.13.13.2.1" class="ltx_text" style="font-size:90%;color:#0D800F;">(-<math id="S4.T1.13.13.2.1.m1.1" class="ltx_Math" alttext="0.17" display="inline"><semantics id="S4.T1.13.13.2.1.m1.1a"><mn mathcolor="#0D800F" id="S4.T1.13.13.2.1.m1.1.1" xref="S4.T1.13.13.2.1.m1.1.1.cmml">0.17</mn><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.2.1.m1.1b"><cn type="float" id="S4.T1.13.13.2.1.m1.1.1.cmml" xref="S4.T1.13.13.2.1.m1.1.1">0.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.2.1.m1.1c">0.17</annotation></semantics></math>)</span>
</td>
<td id="S4.T1.15.15.4" class="ltx_td ltx_align_left ltx_border_t">
<math id="S4.T1.14.14.3.m1.1" class="ltx_Math" alttext="\textbf{13.85}\pm 0.016" display="inline"><semantics id="S4.T1.14.14.3.m1.1a"><mrow id="S4.T1.14.14.3.m1.1.1" xref="S4.T1.14.14.3.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" mathsize="90%" id="S4.T1.14.14.3.m1.1.1.2" xref="S4.T1.14.14.3.m1.1.1.2a.cmml">13.85</mtext><mo mathsize="90%" id="S4.T1.14.14.3.m1.1.1.1" xref="S4.T1.14.14.3.m1.1.1.1.cmml">±</mo><mn mathsize="90%" id="S4.T1.14.14.3.m1.1.1.3" xref="S4.T1.14.14.3.m1.1.1.3.cmml">0.016</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.3.m1.1b"><apply id="S4.T1.14.14.3.m1.1.1.cmml" xref="S4.T1.14.14.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.14.14.3.m1.1.1.1.cmml" xref="S4.T1.14.14.3.m1.1.1.1">plus-or-minus</csymbol><ci id="S4.T1.14.14.3.m1.1.1.2a.cmml" xref="S4.T1.14.14.3.m1.1.1.2"><mtext class="ltx_mathvariant_bold" mathsize="90%" id="S4.T1.14.14.3.m1.1.1.2.cmml" xref="S4.T1.14.14.3.m1.1.1.2">13.85</mtext></ci><cn type="float" id="S4.T1.14.14.3.m1.1.1.3.cmml" xref="S4.T1.14.14.3.m1.1.1.3">0.016</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.3.m1.1c">\textbf{13.85}\pm 0.016</annotation></semantics></math><span id="S4.T1.15.15.4.2" class="ltx_text" style="font-size:90%;"> </span><span id="S4.T1.15.15.4.1" class="ltx_text" style="font-size:90%;color:#0D800F;">(<math id="S4.T1.15.15.4.1.m1.1" class="ltx_Math" alttext="-0.34" display="inline"><semantics id="S4.T1.15.15.4.1.m1.1a"><mrow id="S4.T1.15.15.4.1.m1.1.1" xref="S4.T1.15.15.4.1.m1.1.1.cmml"><mo mathcolor="#0D800F" id="S4.T1.15.15.4.1.m1.1.1a" xref="S4.T1.15.15.4.1.m1.1.1.cmml">−</mo><mn mathcolor="#0D800F" id="S4.T1.15.15.4.1.m1.1.1.2" xref="S4.T1.15.15.4.1.m1.1.1.2.cmml">0.34</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.4.1.m1.1b"><apply id="S4.T1.15.15.4.1.m1.1.1.cmml" xref="S4.T1.15.15.4.1.m1.1.1"><minus id="S4.T1.15.15.4.1.m1.1.1.1.cmml" xref="S4.T1.15.15.4.1.m1.1.1"></minus><cn type="float" id="S4.T1.15.15.4.1.m1.1.1.2.cmml" xref="S4.T1.15.15.4.1.m1.1.1.2">0.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.4.1.m1.1c">-0.34</annotation></semantics></math>)</span>
</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">표 1:</span>고정 데이터 선택 방법 및 원본 데이터 세트 크기에 대해 새로운 토큰 선택 또는 반복 토큰의 효과를 비교합니다. 모든 모델은 40B 토큰으로 훈련된 1.3B OPT 모델입니다. <math alttext="T_{selected}" class="ltx_Math" display="inline" id="S4.T1.17.m1.1"><semantics id="S4.T1.17.m1.1b"><msub id="S4.T1.17.m1.1.1" xref="S4.T1.17.m1.1.1.cmml"><mi id="S4.T1.17.m1.1.1.2" xref="S4.T1.17.m1.1.1.2.cmml">T</mi><mrow id="S4.T1.17.m1.1.1.3" xref="S4.T1.17.m1.1.1.3.cmml"><mi id="S4.T1.17.m1.1.1.3.2" xref="S4.T1.17.m1.1.1.3.2.cmml">s</mi><mo id="S4.T1.17.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.3" xref="S4.T1.17.m1.1.1.3.3.cmml">e</mi><mo id="S4.T1.17.m1.1.1.3.1b" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.4" xref="S4.T1.17.m1.1.1.3.4.cmml">l</mi><mo id="S4.T1.17.m1.1.1.3.1c" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.5" xref="S4.T1.17.m1.1.1.3.5.cmml">e</mi><mo id="S4.T1.17.m1.1.1.3.1d" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.6" xref="S4.T1.17.m1.1.1.3.6.cmml">c</mi><mo id="S4.T1.17.m1.1.1.3.1e" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.7" xref="S4.T1.17.m1.1.1.3.7.cmml">t</mi><mo id="S4.T1.17.m1.1.1.3.1f" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.8" xref="S4.T1.17.m1.1.1.3.8.cmml">e</mi><mo id="S4.T1.17.m1.1.1.3.1g" lspace="0em" rspace="0em" xref="S4.T1.17.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.17.m1.1.1.3.9" xref="S4.T1.17.m1.1.1.3.9.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.17.m1.1c"><apply id="S4.T1.17.m1.1.1.cmml" xref="S4.T1.17.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.17.m1.1.1.1.cmml" xref="S4.T1.17.m1.1.1">subscript</csymbol><ci id="S4.T1.17.m1.1.1.2.cmml" xref="S4.T1.17.m1.1.1.2">𝑇</ci><apply id="S4.T1.17.m1.1.1.3.cmml" xref="S4.T1.17.m1.1.1.3"><times id="S4.T1.17.m1.1.1.3.1.cmml" xref="S4.T1.17.m1.1.1.3.1"></times><ci id="S4.T1.17.m1.1.1.3.2.cmml" xref="S4.T1.17.m1.1.1.3.2">𝑠</ci><ci id="S4.T1.17.m1.1.1.3.3.cmml" xref="S4.T1.17.m1.1.1.3.3">𝑒</ci><ci id="S4.T1.17.m1.1.1.3.4.cmml" xref="S4.T1.17.m1.1.1.3.4">𝑙</ci><ci id="S4.T1.17.m1.1.1.3.5.cmml" xref="S4.T1.17.m1.1.1.3.5">𝑒</ci><ci id="S4.T1.17.m1.1.1.3.6.cmml" xref="S4.T1.17.m1.1.1.3.6">𝑐</ci><ci id="S4.T1.17.m1.1.1.3.7.cmml" xref="S4.T1.17.m1.1.1.3.7">𝑡</ci><ci id="S4.T1.17.m1.1.1.3.8.cmml" xref="S4.T1.17.m1.1.1.3.8">𝑒</ci><ci id="S4.T1.17.m1.1.1.3.9.cmml" xref="S4.T1.17.m1.1.1.3.9">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.m1.1d">T_{selected}</annotation></semantics></math>는 소스 데이터셋에서 선택된 토큰의 개수를 나타낸다. 맨 위 행은 기준선 훈련을 나타냅니다. 3개의 종자에 대한 평균 및 표준 오차가 표시된다. <span class="ltx_text ltx_font_bold" id="S4.T1.23.1">놀랍게도, D4를 통해 반복할 토큰을 영리하게 선택하는 것은 무작위로 새로운 토큰을 선택하는 것보다 성능이 우수하다. </span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p2.1">이를 테스트하기 위해 고정된 토큰 예산과 토큰 예산과 일치하는 고정된 데이터 크기를 가정한다. 우리는 무작위 또는 D4를 사용하여 선택된 데이터의 하위 집합에 대한 두 개의 에폭뿐만 아니라 모든 데이터에 대한 훈련을 평가한다. 이러한 구성에 대해 1.3B 매개변수 OPT 모델을 훈련하고 표 <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">1</span></a>에서 평균 복잡도를 보고한다. 당연하게도, 사용 가능한 모든 데이터를 한 번 사용하는 대신 데이터의 무작위로 선택된 하위 집합을 에포싱하면 모델 복잡성이 약간 저하된다. 대조적으로, D4에 의해 선택된 반복 데이터는 새로운 토큰을 무작위로 샘플링하는 것에 비해 복잡성 및 다운스트림 정확도의 개선을 유도한다. 즉, 사용 가능한 모든 데이터에 대해 원패스 학습을 수행하는 대신 D4 및 epoch를 통해 데이터를 2회 선택하는 것이 유익하다. <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3</span></a>에서 볼 수 있듯이, 이 발견은 일반적으로 훈련 전반에 걸쳐도 성립한다. 모델 규모 및 데이터 선택 비율에 대한 결과는 섹션 <a class="ltx_ref" href="#A1.SS6" title="A.6 Further investigation of repeating tokens ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.6</span></a>를 참조한다.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p3.1">우리가 아는 한, 이것은 원칙적인 데이터 선택 기술을 통해 새로운 토큰을 무작위로 샘플링하는 것보다 LLM 사전 훈련을 위해 데이터를 반복하는 것의 이점을 입증한 첫 번째 결과이다. 우리는 대규모 웹 데이터를 사전 훈련 LLM에 사용하는 최적의 방법은 전략적으로 데이터의 훨씬 작지만 더 잘 분산된 하위 집합을 선택하고 여러 번 반복하는 것일 수 있다고 주장한다.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Cost of data selection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">섹션 <a class="ltx_ref" href="#S4.SS1" title="4.1 Fixed compute regime: can data selection help on fixed token budgets? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.1</span></a>에서 D4에 의해 선택된 데이터에 대해 6.7B 매개변수 모델을 훈련함으로써 20% 더 적은 모델 업데이트를 사용하여 기준선 모델의 최종 복잡도에 도달한다는 것을 발견했다. 우리의 특정 설정에서 이것은 <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">약 4300 GPU 시간을 절약</span>으로 변환됩니다. - 우리는 이것을 선택 메트릭 계산 비용을 설명하지 않기 때문에 <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">naive</span> 효율 이득이라고 할 것입니다.</p>
</div>
<figure id="S4.F4" class="ltx_figure ltx_align_floatright"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="161" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.7.2.1" style="font-size:90%;">Figure 4</span>:</span><span class="ltx_text ltx_font_italic" id="S4.F4.3.2.1" style="font-size:90%;">Naive<span class="ltx_text ltx_font_upright" id="S4.F4.3.2.1.2"> and</span>overall<span class="ltx_text ltx_font_upright" id="S4.F4.3.2.1.1">Instruct + Answers perplexity at <math> </span></span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p2.1">우리의 방법의 실용성을 입증하기 위해, 우리는 데이터를 선택하는 비용이 이것보다 훨씬 적다는 것을 보장해야 한다. 섹션 <a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a>에 설명된 바와 같이, D4를 통해 데이터를 선택하는 것은 첫째, 125M OPT 모델을 통해 문서를 임베딩하는 것, 둘째, K-Means 인덱스 + 인덱스까지의 거리를 계산하는 것을 포함한다. 첫 번째 단계는 대략 하루 만에 96개의 CPU 코어가 있는 단일 머신에서 완료된다. CPU와 GPU 코어의 가격 <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Source: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/ec2/pricing/on-demand/" target="_blank" title="">https://aws.amazon.com/ec2/pricing/on-demand/</a></span></span></span> 사이의 두 자릿수 차이를 감안할 때 이 비용은 무시할 수 있다고 생각한다. 두 번째 단계에서는 동일한 A100 GPU를 사용하여 125M 매개 변수 모델을 사용하여 400B 토큰을 내장하는 데 약 888 GPU 시간이 소요됩니다. 이를 <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">naive</span> 4300 GPU 시간의 효율성 이득에서 빼면 <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">overall</span> 3412 GPU 시간의 효율성 이득에 도달합니다. 이것은 단일 6.7B 매개변수 모델을 훈련할 때 D4 컴퓨팅이 실제로 우리를 얼마나 절약했는지 보여줍니다. <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.3 Cost of data selection ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4</span></a>에서 모델 크기에 따라 이 계산을 다시 수행 하 고 <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.3">overall</span> 효율 이득이 모델 크기에 따라 증가 합니다. 이를 기반으로 D4가 LLama-65B <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>의 경우 20%, OPT-175B <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 경우 22%의 전반적인 효율성 이득을 가질 것으로 보수적으로 추정할 수 있다.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Analysis of D4</h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Why does data selection hurt performance on web snapshots?</h4>

<figure id="S4.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x5.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.5.1.1" style="font-size:90%;">그림 5</span>:</span><span class="ltx_text ltx_font_bold" id="S4.F5.6.2" style="font-size:90%;">Left<span class="ltx_text ltx_font_medium" id="S4.F5.6.2.1">: Train-test similarity across validation sets. X축은 검증 세트의 이름(섹션<a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a> 참조)을 나타내고, y축은 1.3B OPT 40B 기준선에 대한 트레이닝 세트에서 가장 가까운 이웃까지의 코사인 거리를 나타낸다(녹색 삼각형은 평균을 나타내고 노란색 막대는 중앙값을 나타낸다). 우리는 웹 스냅샷 검증 세트가 훈련 세트의 포인트에 가장 가깝다는 것을 관찰한다. </span>Right<span class="ltx_text ltx_font_medium" id="S4.F5.6.2.2">: Analysis of the C4 validation set. (상단): 열차에서 가장 가까운 이웃까지의 코사인 거리의 히스토그램. 각 빈에 대해 평균 원본 복잡도(중간)와 데이터 선택 후 복잡도(아래)의 평균 차이를 보여준다. 트레이닝 세트에 가까운 "Easy"(낮은 오리지널 ppl) 포인트들은 일반적으로 데이터 선택에 의해 가장 영향을 받는 포인트들이다. </span></span></figcaption>
</figure>
<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">일관성 있는 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p1.1.1">average</span> perplexity improvements를 관찰하지만 Section <a class="ltx_ref" href="#A1.SS3" title="A.3 Individual Breakdowns of Downstream Accuracy and PPL ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.3</span></a>는 이러한 perplexity improvement이 validation set에 따라 크게 달라짐을 보여준다. 더 중요한 것은 데이터 선택이 CC-dedup, CommonCrawl, C4와 같은 웹 스냅샷 검증 세트에서 항상 성능을 손상시킨다는 것이다. 이러한 현상이 발생하는 이유를 조사하기 위해 각 검증 세트를 훈련 세트와 동일한 임베딩 공간에 임베딩하고 1.3B 기준 모델에 대해 훈련 세트에서 검증 포인트에 가장 가까운 이웃을 검색한다. 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>의 왼쪽 그림에서는 웹 스냅샷과 동일한 분포에서 그린 검증 집합이 다른 검증 집합에 비해 학습 집합에 더 가깝다는 것을 보여주며, 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>의 오른쪽 그림에서는 데이터 선택이 이러한 웹 스냅샷 검증 집합에 불균형적으로 영향을 미친다는 것을 보여준다. 오른쪽 상단 그림에서는 데이터 선택의 결과로 희소화된 임베딩 공간의 영역(예: 학습 집합의 클러스터 중심에 가까운 공간의 영역)에 웹 검증 집합이 존재한다는 것을 보여주며, 오른쪽 하단 그림에서는 데이터 선택 후 복잡도가 크게 증가하기 때문에 이러한 점들도 데이터 선택에 의해 가장 많은 영향을 받는다는 것을 보여준다. 더욱이, 중간 오른쪽 그림은 이러한 검증 포인트가 훈련 세트에 대한 근접성 때문일 수 있으므로 이러한 포인트가 "쉬운" 포인트임을 나타내는 가지치기 전에 가장 낮은 복잡성을 갖는다는 것을 보여준다.</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">우리의 검증 세트 중 일부가 훈련 세트에 매우 가깝다는 점을 감안할 때, 우리는 그것들이 여전히 일반화의 강력한 지표인지 의문을 제기한다. 실제로 그림 <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">6</span></a>에서 우리는 웹 스냅샷의 복잡도와 명령 조정 데이터 세트의 복잡도와 다운스트림 정확도와 같은 LM 능력의 더 강력한 지표 사이의 약간의 역 관계에 대한 증거를 찾는다. 대조적으로, Instruct+Answers의 복잡도는 다운스트림 정확도와 양의 상관관계가 있음을 관찰하여 명령 조정된 데이터에 대한 검증 복잡도가 모델 품질의 더 나은 척도임을 시사한다. 이러한 이유로 섹션 <a class="ltx_ref" href="#S4" title="4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4</span></a>에서 대부분의 결과를 Web Snapshots 및 Non-web Snapshots로 그룹화 합니다 (Web-Derived + Web-Independent from Figure <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>로 구성), 섹션 <a class="ltx_ref" href="#A1.SS1.SSS4" title="A.1.4 Which validation sets go into the averages? ‣ A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.1.4</span></a>를 참조 하 여 유효성 검사 세트 이름의 전체 목록을 확인 합니다.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="152" height="109" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x7.png" id="S4.F6.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="147" height="113" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x8.png" id="S4.F6.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="152" height="114" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.5.1.1" style="font-size:90%;">그림 6</span>:</span><span class="ltx_text" id="S4.F6.6.2" style="font-size:90%;">Correlation between (left): negative Instruct+Answers perplexity and negative web snapshot perplexity, (middle): Downstream accuracy and negative web snapshot perplexity, (right): Downstream accuracy and negative Instruct+Answers perplexity. 각 포인트는 하나의 트레이닝 구성(1.3B OPT 모델, 40B 토큰)이며, 유일한 변화는 데이터 선택 방법 및 프리트레이닝 시드이다. 웹 스냅샷 복잡성은 LM 능력의 더 강한 지표와 약간 부정적인 상관 관계가 있다. </span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Importance of re-clustering between SemDeDup and SSL Prototypes</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1"><a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a> 섹션에서 언급했듯이, 우리는 과도한 의미 중복을 포함하는 공간의 밀집 영역을 희소화하는 것이 클러스터링 품질을 향상시키고 따라서 D4의 성능에 중요하다고 가정한다. D4에 대한 재클러스터링의 영향을 분리하기 위해, 우리는 재클러스터링 단계(예를 들어, 원래 클러스터링을 유지함)를 제거하는 D4 버전을 사용하여 실험을 실행한다. 그림 <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.4.2 Importance of re-clustering between SemDeDup and SSL Prototypes ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">7</span></a>에서 볼 수 있듯이 재클러스터링 단계를 생략하면 성능이 크게 악화되며, 그림 <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.4.2 Importance of re-clustering between SemDeDup and SSL Prototypes ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">7</span></a>의 오른쪽 그림에서 SemDeDup이 실제로 중심점을 둘러싼 극도로 밀집된 클러스터(예: 중복 구동 클러스터)를 제거한다는 것을 관찰한다. 우리는 Section <a class="ltx_ref" href="#A1.SS9" title="A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A.9</span></a>에서 이를 좀 더 깊이 있게 분석한다.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x9.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.3.1.1" style="font-size:90%;">Figure 7</span>:</span><span class="ltx_text" id="S4.F7.4.2" style="font-size:90%;">Investigating the necessity of the re-clustering step in D4. We see the re-clustering improves perplexity across Web snapshots (left), Non-web snapshots (middle-left), and Instruct + Answers (middle-right). 오른쪽: 재클러스터링이 있거나 없는 중심까지의 평균 거리의 경험적 CDF입니다. 재클러스터링은 중복 구동 클러스터(중심점까지의 평균 거리가 낮은 클러스터)를 제거합니다. </span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Summary and Limitations</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">우리는 LLM에 대한 데이터 큐레이션을 위한 방법인 D4를 도입했는데, 이는 여러 모델 스케일에서 트레이닝 효율을 20% 향상시키며, 증가된 모델 스케일에서 더 큰 이득을 얻는다. 또한 일반적인 관행과 달리 에포칭을 통해 데이터를 반복하는 것이 LLM 교육에 유익할 수 있지만 데이터 하위 집합이 지능적으로 선택되는 경우에만 가능하다는 것을 입증했다. D4를 통해 효율성 향상과 성능 개선을 장려하는 것으로 나타났지만, 우리의 작업에는 몇 가지 한계와 많은 미래 방향이 있다.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Mixing different training distributions:</span> 우리가 하나의 데이터 분포를 선택하여 데이터를 선택하고 훈련하는 동안, 현대 LLM 설정은 보통 서로 다른 데이터 소스를 혼합합니다. 우리의 방법은 이러한 파이프라인에 상보적일 수 있다: 실무자들은 D4를 사용하여 개별 데이터 소스를 다양화하고 중복을 제거한 다음 데이터 소스를 혼합하여 훈련 데이터 세트에 추가 다양성을 제공할 수 있다. 우리는 향후 작업으로 학습 분포의 혼합에 대한 D4의 효능을 탐구하는 것을 남겨두지만 이것이 데이터 세트뿐만 아니라 데이터 세트 내에서도 중복성을 줄임으로써 추가 이득을 얻을 것으로 기대한다.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Model scale:</span> 계산 제한으로 인해 평가 된 가장 큰 모델은 100B 토큰에서 훈련 된 6.7B 매개 변수였습니다. 알고 있는 범위 내에서는, 이것은 임베딩 기반 데이터 큐레이션 접근법의 현재까지 가장 큰 적용이지만, 특히 효율성 이득이 모델 규모에 따라 증가한다는 관찰에 비추어 볼 때 100B를 초과하는 모델 규모에서의 추가 조사는 매우 흥미로울 것이다.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">저자들은 이 작품을 결실을 맺는 데 도움을 준 많은 사람들에게 감사하고 싶습니다: 스리니 이이어, 유첸 장, 토도르 미하일로프, 제이콥 쉬 모야 첸, 만시즈 폴, 미첼 워츠만, 암로 압바스, 아디티야 싱, 마이라 쳉, 매튜 레빗. 저자들은 또한 초기 브레인스토밍에 대해 수리야 강굴리, 모나 디아브, 시안 리에게 감사하고 헨리 에스텔라와 빅토리아 린이 제공한 컴퓨팅 인프라에 대한 도움에 감사한다. 마지막으로, 저자들은 익명의 리뷰어들에게 이 논문의 질과 집필을 향상시켜준 것에 대해 감사드린다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari&nbsp;S. Morcos.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic
deduplication.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.09540, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam
Shleifer, Xi&nbsp;Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Efficient large scale language modeling with mixtures of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.10684</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bach et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Stephen&nbsp;H. Bach, Victor Sanh, Zheng&nbsp;Xin Yong, Albert Webson, Colin Raffel,
Nihal&nbsp;V. Nayak, Abheesht Sharma, Taewoon Kim, M&nbsp;Saiful Bari, Thibault
Févry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik
Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason&nbsp;Alan Fries, Maged&nbsp;S.
Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,
Mike Tian-Jian Jiang, and Alexander&nbsp;M. Rush.

</span>
<span class="ltx_bibblock">Promptsource: An integrated development environment and repository
for natural language prompts.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2202.01279, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baumgartner et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy
Blackburn.

</span>
<span class="ltx_bibblock">The pushshift reddit dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the international AAAI conference on web and
social media</em>, volume&nbsp;14, pages 830–839, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
O’Brien, Eric Hallahan, Mohammad&nbsp;Aflah Khan, Shivanshu Purohit, USVSN&nbsp;Sai
Prashanth, Edward Raff, et&nbsp;al.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01373</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birodkar et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio.

</span>
<span class="ltx_bibblock">Semantic redundancies in image-classification datasets: The 10% you
don’t need.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.11409</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pages 7432–7439, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Broder [1997]</span>
<span class="ltx_bibblock">
Andrei&nbsp;Z Broder.

</span>
<span class="ltx_bibblock">On the resemblance and containment of documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings. Compression and Complexity of SEQUENCES 1997
(Cat. No. 97TB100171)</em>, pages 21–29. IEEE, 1997.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cazenavette et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei&nbsp;A Efros, and
Jun-Yan Zhu.

</span>
<span class="ltx_bibblock">Dataset distillation by matching training trajectories.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 4750–4759, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chitta et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Kashyap Chitta, José&nbsp;M Álvarez, Elmar Haussmann, and Clément
Farabet.

</span>
<span class="ltx_bibblock">Training data subset search with ensemble active learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>,
23(9):14741–14752, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, et&nbsp;al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Bo&nbsp;Dong, Cristian Lumezanu, Yuncong Chen, Dongjin Song, Takehiko Mizoguchi,
Haifeng Chen, and Latifur Khan.

</span>
<span class="ltx_bibblock">At the speed of sound: Efficient audio scene classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 International Conference on
Multimedia Retrieval</em>, ICMR ’20, page 301–305, New York, NY, USA, 2020.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450370875.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3372278.3390730</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3372278.3390730" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3372278.3390730</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman and Zhang [2020]</span>
<span class="ltx_bibblock">
Vitaly Feldman and Chiyuan Zhang.

</span>
<span class="ltx_bibblock">What neural networks memorize and why: Discovering the long tail via
influence estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:2881–2891, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Leo Gao, Stella&nbsp;Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe,
Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2101.00027, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz&nbsp;Beltagy,
Doug Downey, and Noah&nbsp;A Smith.

</span>
<span class="ltx_bibblock">Don’t stop pretraining: Adapt language models to domains and tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10964</em>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks and Gimpel [2016]</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel.

</span>
<span class="ltx_bibblock">Gaussian error linear units (gelus).

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.08415</em>, 2016.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Danny Hernandez, Tom&nbsp;B. Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer
El-Showk, Nelson Elhage, Zac Hatfield-Dodds, T.&nbsp;J. Henighan, Tristan Hume,
Scott Johnston, Benjamin Mann, Christopher Olah, Catherine Olsson, Dario
Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish.

</span>
<span class="ltx_bibblock">Scaling laws and interpretability of learning from repeated data.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.10487, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van&nbsp;den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and L.&nbsp;Sifre.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2203.15556, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Srinivas Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig,
Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit&nbsp;Singh Koura, Xian Li,
Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli
Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Opt-iml: Scaling language model instruction meta learning through the
lens of generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2212.12017, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr
Bojanowski, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.09118</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Angela&nbsp;H Jiang, Daniel L-K Wong, Giulio Zhou, David&nbsp;G Andersen, Jeffrey Dean,
Gregory&nbsp;R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary&nbsp;C
Lipton, et&nbsp;al.

</span>
<span class="ltx_bibblock">Accelerating deep learning by focusing on the biggest losers.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.00762</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with GPUs.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, 7(3):535–547, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, T.&nbsp;J. Henighan, Tom&nbsp;B. Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2001.08361, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba [2014]</span>
<span class="ltx_bibblock">
Diederik&nbsp;P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
Chris Callison-Burch, and Nicholas Carlini.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational
Linguistics</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levesque et&nbsp;al. [2012]</span>
<span class="ltx_bibblock">
Hector Levesque, Ernest Davis, and Leora Morgenstern.

</span>
<span class="ltx_bibblock">The winograd schema challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Thirteenth international conference on the principles of
knowledge representation and reasoning</em>, 2012.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
Shrivastava, Ce&nbsp;Zhang, Yuandong Tian, Christopher Re, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deja vu: Contextual sparsity for efficient llms at inference time,
2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
S.&nbsp;Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret
Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David&nbsp;M. Mimno, and Daphne
Ippolito.

</span>
<span class="ltx_bibblock">A pretrainer’s guide to training data: Measuring the effects of data
age, domain coverage, quality, &amp; toxicity.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.13169, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meding et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Kristof Meding, Luca M&nbsp;Schulze Buschoff, Robert Geirhos, and Felix&nbsp;A Wichmann.

</span>
<span class="ltx_bibblock">Trivial or impossible–dichotomous data difficulty masks model
differences (on imagenet and beyond).

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.05922</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et&nbsp;al. [2016]</span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.07843</em>, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.02789</em>, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mindermann et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Sören Mindermann, Jan&nbsp;M Brauner, Muhammed&nbsp;T Razzak, Mrinank Sharma, Andreas
Kirsch, Winnie Xu, Benedikt Höltgen, Aidan&nbsp;N Gomez, Adrien Morisot,
Sebastian Farquhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Prioritized training on points that are learnable, worth learning,
and not yet learnt.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
15630–15649. PMLR, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirzasoleiman et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.

</span>
<span class="ltx_bibblock">Coresets for data-efficient training of machine learning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
6950–6960. PMLR, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mostafazadeh et&nbsp;al. [2016]</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
Lucy Vanderwende, Pushmeet Kohli, and James Allen.

</span>
<span class="ltx_bibblock">A corpus and evaluation framework for deeper understanding of
commonsense stories.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.01696</em>, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Alexander&nbsp;M. Rush, Boaz Barak, Teven&nbsp;Le Scao, Aleksandra
Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Mansheej Paul, Surya Ganguli, and Gintare&nbsp;Karolina Dziugaite.

</span>
<span class="ltx_bibblock">Deep learning on a data diet: Finding important examples early in
training.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
34:20596–20607, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: Outperforming curated corpora
with web data, and web data only.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(9):99–106,
2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.15004</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sener and Savarese [2017]</span>
<span class="ltx_bibblock">
Ozan Sener and Silvio Savarese.

</span>
<span class="ltx_bibblock">Active learning for convolutional neural networks: A core-set
approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1708.00489</em>, 2017.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, M&nbsp;Patwary, R&nbsp;Puri, P&nbsp;LeGresley, J&nbsp;Casper, B&nbsp;Megatron-LM
Catanzaro, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training multi-billion parameter language models using model
parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint cs.CL/1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
Vijay Korthikanti, et&nbsp;al.

</span>
<span class="ltx_bibblock">Using deepspeed and megatron to train megatron-turing nlg 530b, a
large-scale generative language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.11990</em>, 2022.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorscher et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari&nbsp;S.
Morcos.

</span>
<span class="ltx_bibblock">Beyond neural scaling laws: beating power law scaling via data
pruning.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2206.14486, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tirumala et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.

</span>
<span class="ltx_bibblock">Memorization without overfitting: Analyzing the training dynamics of
large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:38274–38290, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Mariya Toneva, Alessandro Sordoni, Remi Tachet&nbsp;des Combes, Adam Trischler,
Yoshua Bengio, and Geoffrey&nbsp;J Gordon.

</span>
<span class="ltx_bibblock">An empirical study of example forgetting during deep neural network
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.05159</em>, 2018.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave, and
Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2302.13971, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius
Hobbhahn, and Anson Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel Bowman.

</span>
<span class="ltx_bibblock">Superglue: A stickier benchmark for general-purpose language
understanding systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut&nbsp;Selvan Dhanasekaran, Atharva
Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi&nbsp;Gary Lai,
Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
Maitreya Patel, Kuntal&nbsp;Kumar Pal, M.&nbsp;Moradshahi, Mihir Parmar, Mirali
Purohit, Neeraj Varshney, Phani&nbsp;Rohitha Kaza, Pulkit Verma, Ravsehaj&nbsp;Singh
Puri, Rushang Karia, Shailaja&nbsp;Keyur Sampat, Savan Doshi, Siddharth&nbsp;Deepak
Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral,
Yejin Choi, Noah&nbsp;A. Smith, Hanna Hajishirzi, and Daniel Khashabi.

</span>
<span class="ltx_bibblock">Super-naturalinstructions: Generalization via declarative
instructions on 1600+ nlp tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing</em>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,
Francisco Guzm’an, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">Ccnet: Extracting high quality monolingual datasets from web crawl
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1911.00359, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Sang&nbsp;Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy
Liang, Quoc&nbsp;V. Le, Tengyu Ma, and Adams&nbsp;Wei Yu.

</span>
<span class="ltx_bibblock">Doremi: Optimizing data mixtures speeds up language model
pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.10429, 2023a.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Sang&nbsp;Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.

</span>
<span class="ltx_bibblock">Data selection for language models via importance resampling.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2302.03169, 2023b.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.

</span>
<span class="ltx_bibblock">To repeat or not to repeat: Insights from scaling llm under
token-crisis.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13230</em>, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07830</em>, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.01068, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Bo&nbsp;Zhao, Konda&nbsp;Reddy Mopuri, and Hakan Bilen.

</span>
<span class="ltx_bibblock">Dataset condensation with gradient matching.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.05929</em>, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2015]</span>
<span class="ltx_bibblock">
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 19–27, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Experimental Setup Details</h3>

<section id="A1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Hyperparameters for model training</h4>

<div id="A1.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS1.p1.6"><a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a>절에서 언급한 바와 같이, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 오리지널 OPT 모델 아키텍처와 동일한 하이퍼파라미터 및 구성을 사용한다. 이러한 하이퍼파라미터를 표<a class="ltx_ref" href="#A1.T1" title="Table A1 ‣ A.1.1 Hyperparameters for model training ‣ A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A1</span></a>에서 간략하게 설명한다. 이러한 구성은 공개적으로 사용할 수 있고 많은 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>, <a class="ltx_ref" href="#bib.bib13" title="">13</a>, <a class="ltx_ref" href="#bib.bib29" title="">29</a>, <a class="ltx_ref" href="#bib.bib48" title="">48</a>, <a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에서 표준으로 사용되었기 때문에 선택했다. 모든 모델은 GELU 활성화 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>, Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite> with <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.1.m1.1"><semantics id="A1.SS1.SSS1.p1.1.m1.1a"><mrow id="A1.SS1.SSS1.p1.1.m1.1.1" xref="A1.SS1.SSS1.p1.1.m1.1.1.cmml"><msub id="A1.SS1.SSS1.p1.1.m1.1.1.2" xref="A1.SS1.SSS1.p1.1.m1.1.1.2.cmml"><mi id="A1.SS1.SSS1.p1.1.m1.1.1.2.2" xref="A1.SS1.SSS1.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="A1.SS1.SSS1.p1.1.m1.1.1.2.3" xref="A1.SS1.SSS1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="A1.SS1.SSS1.p1.1.m1.1.1.1" xref="A1.SS1.SSS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS1.SSS1.p1.1.m1.1.1.3" xref="A1.SS1.SSS1.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.1.m1.1b"><apply id="A1.SS1.SSS1.p1.1.m1.1.1.cmml" xref="A1.SS1.SSS1.p1.1.m1.1.1"><eq id="A1.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.SSS1.p1.1.m1.1.1.1"></eq><apply id="A1.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="A1.SS1.SSS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSS1.p1.1.m1.1.1.2.1.cmml" xref="A1.SS1.SSS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="A1.SS1.SSS1.p1.1.m1.1.1.2.2.cmml" xref="A1.SS1.SSS1.p1.1.m1.1.1.2.2">𝛽</ci><cn id="A1.SS1.SSS1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="A1.SS1.SSS1.p1.1.m1.1.1.2.3">1</cn></apply><cn id="A1.SS1.SSS1.p1.1.m1.1.1.3.cmml" type="float" xref="A1.SS1.SSS1.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.95" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.2.m2.1"><semantics id="A1.SS1.SSS1.p1.2.m2.1a"><mrow id="A1.SS1.SSS1.p1.2.m2.1.1" xref="A1.SS1.SSS1.p1.2.m2.1.1.cmml"><msub id="A1.SS1.SSS1.p1.2.m2.1.1.2" xref="A1.SS1.SSS1.p1.2.m2.1.1.2.cmml"><mi id="A1.SS1.SSS1.p1.2.m2.1.1.2.2" xref="A1.SS1.SSS1.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="A1.SS1.SSS1.p1.2.m2.1.1.2.3" xref="A1.SS1.SSS1.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="A1.SS1.SSS1.p1.2.m2.1.1.1" xref="A1.SS1.SSS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS1.SSS1.p1.2.m2.1.1.3" xref="A1.SS1.SSS1.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.2.m2.1b"><apply id="A1.SS1.SSS1.p1.2.m2.1.1.cmml" xref="A1.SS1.SSS1.p1.2.m2.1.1"><eq id="A1.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.SSS1.p1.2.m2.1.1.1"></eq><apply id="A1.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="A1.SS1.SSS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSS1.p1.2.m2.1.1.2.1.cmml" xref="A1.SS1.SSS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="A1.SS1.SSS1.p1.2.m2.1.1.2.2.cmml" xref="A1.SS1.SSS1.p1.2.m2.1.1.2.2">𝛽</ci><cn id="A1.SS1.SSS1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="A1.SS1.SSS1.p1.2.m2.1.1.2.3">2</cn></apply><cn id="A1.SS1.SSS1.p1.2.m2.1.1.3.cmml" type="float" xref="A1.SS1.SSS1.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.2.m2.1c">\beta_{2}=0.95</annotation></semantics></math>, <math alttext="\epsilon=10^{-8}" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.3.m3.1"><semantics id="A1.SS1.SSS1.p1.3.m3.1a"><mrow id="A1.SS1.SSS1.p1.3.m3.1.1" xref="A1.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="A1.SS1.SSS1.p1.3.m3.1.1.2" xref="A1.SS1.SSS1.p1.3.m3.1.1.2.cmml">ϵ</mi><mo id="A1.SS1.SSS1.p1.3.m3.1.1.1" xref="A1.SS1.SSS1.p1.3.m3.1.1.1.cmml">=</mo><msup id="A1.SS1.SSS1.p1.3.m3.1.1.3" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.cmml"><mn id="A1.SS1.SSS1.p1.3.m3.1.1.3.2" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.2.cmml">10</mn><mrow id="A1.SS1.SSS1.p1.3.m3.1.1.3.3" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.3.cmml"><mo id="A1.SS1.SSS1.p1.3.m3.1.1.3.3a" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="A1.SS1.SSS1.p1.3.m3.1.1.3.3.2" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.3.m3.1b"><apply id="A1.SS1.SSS1.p1.3.m3.1.1.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1"><eq id="A1.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1.1"></eq><ci id="A1.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1.2">italic-ϵ</ci><apply id="A1.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.SSS1.p1.3.m3.1.1.3.1.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1.3">superscript</csymbol><cn id="A1.SS1.SSS1.p1.3.m3.1.1.3.2.cmml" type="integer" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.2">10</cn><apply id="A1.SS1.SSS1.p1.3.m3.1.1.3.3.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.3"><minus id="A1.SS1.SSS1.p1.3.m3.1.1.3.3.1.cmml" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.3"></minus><cn id="A1.SS1.SSS1.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="A1.SS1.SSS1.p1.3.m3.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.3.m3.1c">\epsilon=10^{-8}</annotation></semantics></math>, weight decay set to <math alttext="0.1" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.4.m4.1"><semantics id="A1.SS1.SSS1.p1.4.m4.1a"><mn id="A1.SS1.SSS1.p1.4.m4.1.1" xref="A1.SS1.SSS1.p1.4.m4.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.4.m4.1b"><cn id="A1.SS1.SSS1.p1.4.m4.1.1.cmml" type="float" xref="A1.SS1.SSS1.p1.4.m4.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.4.m4.1c">0.1</annotation></semantics></math>, and we clip gradient norms at 1.0. 우리는 fp16 정밀도로 Megatron-LM 텐서 병렬성 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib45" title="">45</a>]</cite>를 사용하여 완전 샤드 데이터 병렬 모드 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>에서 모든 모델을 훈련한다. 재현성을 위해(그리고 아마도 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 원래 구성과 유일한 차이점은 드롭아웃을 사용하지 않는다는 것이다.</p>
</div>
<figure id="A1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T1.2.1.1" style="font-size:90%;">Table A1</span>:</span><span class="ltx_text" id="A1.T1.3.2" style="font-size:90%;">Model architecture details. 대부분의 파라미터 구성은 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>의 표 1과 동일하다. 배치 크기는 하나의 기울기 하강 업데이트 동안 모델이 보는 총 토큰을 나타냅니다. </span></figcaption>
<table id="A1.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T1.4.1" class="ltx_tr">
<td id="A1.T1.4.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T1.4.1.1.1" class="ltx_text ltx_font_bold">Scale</span></td>
<td id="A1.T1.4.1.2" class="ltx_td ltx_align_center ltx_border_tt">Num Layers</td>
<td id="A1.T1.4.1.3" class="ltx_td ltx_align_center ltx_border_tt">Num Heads</td>
<td id="A1.T1.4.1.4" class="ltx_td ltx_align_center ltx_border_tt">Embedding Dim</td>
<td id="A1.T1.4.1.5" class="ltx_td ltx_align_center ltx_border_tt">Peak Learning Rate (LR)</td>
<td id="A1.T1.4.1.6" class="ltx_td ltx_align_center ltx_border_tt">Batch Size</td>
</tr>
<tr id="A1.T1.4.2" class="ltx_tr">
<td id="A1.T1.4.2.1" class="ltx_td ltx_align_center ltx_border_t">8M</td>
<td id="A1.T1.4.2.2" class="ltx_td ltx_align_center ltx_border_t">4</td>
<td id="A1.T1.4.2.3" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="A1.T1.4.2.4" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="A1.T1.4.2.5" class="ltx_td ltx_align_center ltx_border_t">1.0e-3</td>
<td id="A1.T1.4.2.6" class="ltx_td ltx_align_center ltx_border_t">0.5M</td>
</tr>
<tr id="A1.T1.4.3" class="ltx_tr">
<td id="A1.T1.4.3.1" class="ltx_td ltx_align_center">125M</td>
<td id="A1.T1.4.3.2" class="ltx_td ltx_align_center">12</td>
<td id="A1.T1.4.3.3" class="ltx_td ltx_align_center">12</td>
<td id="A1.T1.4.3.4" class="ltx_td ltx_align_center">768</td>
<td id="A1.T1.4.3.5" class="ltx_td ltx_align_center">6.0e-4</td>
<td id="A1.T1.4.3.6" class="ltx_td ltx_align_center">0.5M</td>
</tr>
<tr id="A1.T1.4.4" class="ltx_tr">
<td id="A1.T1.4.4.1" class="ltx_td ltx_align_center">1.3B</td>
<td id="A1.T1.4.4.2" class="ltx_td ltx_align_center">24</td>
<td id="A1.T1.4.4.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T1.4.4.4" class="ltx_td ltx_align_center">2048</td>
<td id="A1.T1.4.4.5" class="ltx_td ltx_align_center">2.0e-4</td>
<td id="A1.T1.4.4.6" class="ltx_td ltx_align_center">1M</td>
</tr>
<tr id="A1.T1.4.5" class="ltx_tr">
<td id="A1.T1.4.5.1" class="ltx_td ltx_align_center">6.7B</td>
<td id="A1.T1.4.5.2" class="ltx_td ltx_align_center">32</td>
<td id="A1.T1.4.5.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T1.4.5.4" class="ltx_td ltx_align_center">4096</td>
<td id="A1.T1.4.5.5" class="ltx_td ltx_align_center">1.2e-4</td>
<td id="A1.T1.4.5.6" class="ltx_td ltx_align_center">2M</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>Dataset Curation Details</h4>

<div id="A1.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS2.p1.1">이 하위 섹션에서는 논문 전반에 걸쳐 사용된 시작 소스 데이터 세트인 <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS2.p1.1.1">CC-dedup</span>을 큐레이트하는 방법을 설명합니다. 2017년부터 2020년까지 5개의 CommonCrawl 덤프 <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://commoncrawl.org/the-data/get-started/</span></span></span>으로 시작한다. 그런 다음 CC-net <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib54" title="">54</a>]</cite>를 사용하여 문단 수준에서 데이터를 복제하고 영어가 아닌 웹 페이지를 제거하고 품질이 낮은 페이지를 필터링한다. 우리가 사용하는 파이프라인은 섹션 2 내에서 <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>(부제 "English CommonCrawl [67%]) 뒤에 있는 섹션을 참조)에서 사용된 파이프라인과 동일합니다.</p>
</div>
<div id="A1.SS1.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS2.p2.3">여기에 문서 수준에서 MinHash <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite> de-duplication의 추가 단계를 추가한다. MinHash의 매개 변수는 서명당 20개의 해시, 20개의 버킷 및 버킷당 1개의 행입니다. 이러한 매개변수는 MinHashLSH의 스파크 구현의 기본 매개변수이며 계산 제한으로 인해 이러한 매개변수에 대해 하이퍼파라미터 스윕을 수행하지 않았다. 이전 작업은 훨씬 더 공격적인 매개 변수를 사용하여 MinHash를 실행하려고 시도했습니다. <cite class="ltx_cite ltx_citemacro_citet">Lee et al. [<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> 및 <cite class="ltx_cite ltx_citemacro_citet">Penedo et al. [<a class="ltx_ref" href="#bib.bib39" title="">39</a>]</cite>는 <math alttext="20" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.1.m1.1"><semantics id="A1.SS1.SSS2.p2.1.m1.1a"><mn id="A1.SS1.SSS2.p2.1.m1.1.1" xref="A1.SS1.SSS2.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS2.p2.1.m1.1b"><cn id="A1.SS1.SSS2.p2.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS2.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS2.p2.1.m1.1c">20</annotation></semantics></math> 버킷을 사용하고, <math alttext="450" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.2.m2.1"><semantics id="A1.SS1.SSS2.p2.2.m2.1a"><mn id="A1.SS1.SSS2.p2.2.m2.1.1" xref="A1.SS1.SSS2.p2.2.m2.1.1.cmml">450</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS2.p2.2.m2.1b"><cn id="A1.SS1.SSS2.p2.2.m2.1.1.cmml" type="integer" xref="A1.SS1.SSS2.p2.2.m2.1.1">450</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS2.p2.2.m2.1c">450</annotation></semantics></math> 버킷당 해시 및 <math alttext="9000" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.3.m3.1"><semantics id="A1.SS1.SSS2.p2.3.m3.1a"><mn id="A1.SS1.SSS2.p2.3.m3.1.1" xref="A1.SS1.SSS2.p2.3.m3.1.1.cmml">9000</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS2.p2.3.m3.1b"><cn id="A1.SS1.SSS2.p2.3.m3.1.1.cmml" type="integer" xref="A1.SS1.SSS2.p2.3.m3.1.1">9000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS2.p2.3.m3.1c">9000</annotation></semantics></math> 해시당 서명을 사용합니다. 우리는 더 공격적인 MinHash가 더 많은 템플릿을 제거하여 고품질 시작 데이터 세트를 생성하여 잠재적으로 D4의 SemDeDup 단계가 덜 필요할 것이라고 추측한다. <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>는 C4에서 <cite class="ltx_cite ltx_citemacro_citet">Lee et al. [<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> 및 SemDeDup의 MinHash의 성능이 3.9%의 고정된 데이터 선택 비율에서 비슷하다는 것을 발견했으며, 이는 SemDeDup이 공격적인 MinHash와 유사한 데이터를 필터링한다는 것을 나타낸다. 우리는 이 하이퍼파라미터들을 훑어보는 것을 미래의 작업으로 남겨둔다.</p>
</div>
<div id="A1.SS1.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS2.p3.1">데이터 세트는 CommonCrawl 덤프에서 선별되기 때문에 교육 세트에 공격적이거나 PII 콘텐츠가 포함될 위험이 있습니다. 그러나 CommonCrawl 덤프를 필터링하기 위해 동일한 파이프라인을 사용하기 때문에 이러한 위험은 <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>와 같은 표준 언어 모델링 큐레이션의 위험에 불과합니다.</p>
</div>
</section>
<section id="A1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.3 </span>Parameters for Data Selection</h4>

<div id="A1.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS3.p1.1">섹션 <a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a>에 소개된 모든 방법은 K-Means를 사용하여 임베딩을 클러스터링하는 것을 포함한다. 시작 훈련 데이터 세트 CC-dedup에는 총 약 6억 개의 문서가 포함되어 있습니다. 6억 개의 768 크기의 벡터 모두에 대해 K-평균 클러스터링을 실행하려면 상당한 양의 계산이 필요하다. 대신 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>, <a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>를 따르고 중심을 계산할 약 100M 문서를 무작위로 샘플링한다. 이러한 100M 문서에 대한 임베딩을 정규화하여 L2-norm이 1.0이 되도록 한 다음, 다음의 파라미터로 faiss <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>를 사용한다:</p>
</div>
<div id="A1.SS1.SSS3.p2" class="ltx_para">
<pre id="A1.SS1.SSS3.p2.1" class="ltx_verbatim ltx_font_typewriter">    faiss.Kmeans(
        768 # 125M OPT model embedding size,
        11000 # 11K clusters,
        niter=20 # 20 iterations,
        verbose=True,
        seed=0,
        gpu=False,
        spherical=True,
        min_points_per_centroid=1,
        max_points_per_centroid=100000000
    )
</pre>
</div>
<div id="A1.SS1.SSS3.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS3.p3.1">우리는 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>에 이어 <math alttext="11000" class="ltx_Math" display="inline" id="A1.SS1.SSS3.p3.1.m1.1"><semantics id="A1.SS1.SSS3.p3.1.m1.1a"><mn id="A1.SS1.SSS3.p3.1.m1.1.1" xref="A1.SS1.SSS3.p3.1.m1.1.1.cmml">11000</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p3.1.m1.1b"><cn id="A1.SS1.SSS3.p3.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS3.p3.1.m1.1.1">11000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p3.1.m1.1c">11000</annotation></semantics></math> 클러스터를 선택하며, 이 선택은 클러스터의 수가 클러스터되는 총 점 수의 제곱근이 대략적이어야 한다는 휴리스틱을 고수한다는 점에 주목한다. 또한 125M OPT 모델 규모에서 데이터 선택을 위한 초기 실험에서 데이터 선택 방법의 성능에 대한 군집 수의 유의한 영향을 발견하지 못했다는 점에 주목한다(그림 <a class="ltx_ref" href="#A1.F1" title="Figure A1 ‣ A.1.3 Parameters for Data Selection ‣ A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A1</span></a> 참조). 이 발견은 다른 군집 수를 가진 SemDeDup에 의해 선택된 데이터 세트 간에 상당한 중복을 발견한 <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>와 일치한다(<cite class="ltx_cite ltx_citemacro_citet">Abbas et al. [<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>의 그림 A2 참조).</p>
</div>
<figure id="A1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x10.png" id="A1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="202" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F1.5.1.1" style="font-size:90%;">Figure A1</span>:</span><span class="ltx_text" id="A1.F1.6.2" style="font-size:90%;">K-Means의 클러스터 수가 데이터 선택 성능에 미치는 영향. 모든 모델은 125M OPT 모델이며, 여기서 훈련 세트(및 시작 소스 데이터 세트)는 C4이고 SSL 프로토타입이 있는 데이터를 선택한다. y축은 베이스라인 트레이닝에 비해 perplexity의 변화로서, 베이스라인 트레이닝이 0.0에 있음을 의미하고, 그래프 상의 go <span class="ltx_text ltx_font_italic" id="A1.F1.6.2.1">down</span>은 <span class="ltx_text ltx_font_italic" id="A1.F1.6.2.2">better</span> 성능을 나타낸다. x축은 원본 데이터 세트 크기입니다. 비웹 스냅샷 유효성 검사 세트(왼쪽) 및 명령 + 응답(오른쪽)에 대한 평균 복잡도에 대한 결과를 보여 줍니다. 클러스터의 수를 변경할 때 큰 차이가 없음을 알 수 있지만(예: 각 선을 중심으로 오차 막대를 그으면 모두 겹치지만), 11K 클러스터는 일반적으로 상위 3개의 최상의 수행 방법 중 하나이다. </span></figcaption>
</figure>
<div id="A1.SS1.SSS3.p4" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS3.p4.3">우리는 페이스가 K-평균을 수행하는 동안 클러스터의 균형을 수동으로 맞추려고 시도하지 않도록 의도적으로 센트로이드당 최소 포인트와 센트로이드당 최대 포인트를 낮게 설정했다. <cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>는 명시적으로 클래스 균형이 중요하다는 것을 발견했다: 그들은 모든 클래스 쌍에 걸쳐 수량 <math alttext="\frac{\text{size of majority class}}{\text{size of minority class}}" class="ltx_Math" display="inline" id="A1.SS1.SSS3.p4.1.m1.1"><semantics id="A1.SS1.SSS3.p4.1.m1.1a"><mfrac id="A1.SS1.SSS3.p4.1.m1.1.1" xref="A1.SS1.SSS3.p4.1.m1.1.1.cmml"><mtext id="A1.SS1.SSS3.p4.1.m1.1.1.2" xref="A1.SS1.SSS3.p4.1.m1.1.1.2a.cmml">size of majority class</mtext><mtext id="A1.SS1.SSS3.p4.1.m1.1.1.3" xref="A1.SS1.SSS3.p4.1.m1.1.1.3a.cmml">size of minority class</mtext></mfrac><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p4.1.m1.1b"><apply id="A1.SS1.SSS3.p4.1.m1.1.1.cmml" xref="A1.SS1.SSS3.p4.1.m1.1.1"><divide id="A1.SS1.SSS3.p4.1.m1.1.1.1.cmml" xref="A1.SS1.SSS3.p4.1.m1.1.1"></divide><ci id="A1.SS1.SSS3.p4.1.m1.1.1.2a.cmml" xref="A1.SS1.SSS3.p4.1.m1.1.1.2"><mtext id="A1.SS1.SSS3.p4.1.m1.1.1.2.cmml" mathsize="70%" xref="A1.SS1.SSS3.p4.1.m1.1.1.2">size of majority class</mtext></ci><ci id="A1.SS1.SSS3.p4.1.m1.1.1.3a.cmml" xref="A1.SS1.SSS3.p4.1.m1.1.1.3"><mtext id="A1.SS1.SSS3.p4.1.m1.1.1.3.cmml" mathsize="70%" xref="A1.SS1.SSS3.p4.1.m1.1.1.3">size of minority class</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p4.1.m1.1c">\frac{\text{size of majority class}}{\text{size of minority class}}</annotation></semantics></math>의 기대치인 "클래스 균형 점수"(<cite class="ltx_cite ltx_citemacro_citet">Sorscher et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>의 섹션 H 참조)를 도입한다. 그런 다음 클래스 균형 점수 0.5에 대한 하드 한도를 설정했는데, 이는 "모든 클래스가 모든 클래스를 균등하게 가지치기할 때 가질 이미지의 최소 50%" <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>를 의미한다. 우리는 "클러스터 균형" 점수라고 하는 클래스 균형 점수의 비지도 학습 아날로그를 고려한다. 클러스터 균형 점수는 모든 클러스터 쌍에 대한 수량 <math alttext="\frac{\text{size of bigger cluster}}{\text{size of smaller cluster}}" class="ltx_Math" display="inline" id="A1.SS1.SSS3.p4.2.m2.1"><semantics id="A1.SS1.SSS3.p4.2.m2.1a"><mfrac id="A1.SS1.SSS3.p4.2.m2.1.1" xref="A1.SS1.SSS3.p4.2.m2.1.1.cmml"><mtext id="A1.SS1.SSS3.p4.2.m2.1.1.2" xref="A1.SS1.SSS3.p4.2.m2.1.1.2a.cmml">size of bigger cluster</mtext><mtext id="A1.SS1.SSS3.p4.2.m2.1.1.3" xref="A1.SS1.SSS3.p4.2.m2.1.1.3a.cmml">size of smaller cluster</mtext></mfrac><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p4.2.m2.1b"><apply id="A1.SS1.SSS3.p4.2.m2.1.1.cmml" xref="A1.SS1.SSS3.p4.2.m2.1.1"><divide id="A1.SS1.SSS3.p4.2.m2.1.1.1.cmml" xref="A1.SS1.SSS3.p4.2.m2.1.1"></divide><ci id="A1.SS1.SSS3.p4.2.m2.1.1.2a.cmml" xref="A1.SS1.SSS3.p4.2.m2.1.1.2"><mtext id="A1.SS1.SSS3.p4.2.m2.1.1.2.cmml" mathsize="70%" xref="A1.SS1.SSS3.p4.2.m2.1.1.2">size of bigger cluster</mtext></ci><ci id="A1.SS1.SSS3.p4.2.m2.1.1.3a.cmml" xref="A1.SS1.SSS3.p4.2.m2.1.1.3"><mtext id="A1.SS1.SSS3.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="A1.SS1.SSS3.p4.2.m2.1.1.3">size of smaller cluster</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p4.2.m2.1c">\frac{\text{size of bigger cluster}}{\text{size of smaller cluster}}</annotation></semantics></math>의 기대값입니다. 모든 데이터 선택 방법(및 R에 대한 선택)에서 이 값은 명시적 개입 없이 일반적으로 <math alttext="0.5" class="ltx_Math" display="inline" id="A1.SS1.SSS3.p4.3.m3.1"><semantics id="A1.SS1.SSS3.p4.3.m3.1a"><mn id="A1.SS1.SSS3.p4.3.m3.1.1" xref="A1.SS1.SSS3.p4.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p4.3.m3.1b"><cn id="A1.SS1.SSS3.p4.3.m3.1.1.cmml" type="float" xref="A1.SS1.SSS3.p4.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p4.3.m3.1c">0.5</annotation></semantics></math>와 같거나 더 크다는 것을 발견했다. 이러한 이유로, 우리는 (클러스터의 속성에 기초하여) 각 클러스터에서 샘플링되는 점의 수를 변경하는 것이 향후 매우 흥미로운 작업이라는 점에 주목하지만, 명시적으로 클러스터 균형을 맞추지 않는다.</p>
</div>
<div id="A1.SS1.SSS3.p5" class="ltx_para">
<p id="A1.SS1.SSS3.p5.7" class="ltx_p">D4 parameters: The choice of parameters <math id="A1.SS1.SSS3.p5.1.m1.1" class="ltx_Math" alttext="R_{proto}" display="inline"><semantics id="A1.SS1.SSS3.p5.1.m1.1a"><msub id="A1.SS1.SSS3.p5.1.m1.1.1" xref="A1.SS1.SSS3.p5.1.m1.1.1.cmml"><mi id="A1.SS1.SSS3.p5.1.m1.1.1.2" xref="A1.SS1.SSS3.p5.1.m1.1.1.2.cmml">R</mi><mrow id="A1.SS1.SSS3.p5.1.m1.1.1.3" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.cmml"><mi id="A1.SS1.SSS3.p5.1.m1.1.1.3.2" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.1.m1.1.1.3.1" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.1.m1.1.1.3.3" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.1.m1.1.1.3.1a" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.1.m1.1.1.3.4" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.1.m1.1.1.3.1b" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.1.m1.1.1.3.5" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.1.m1.1.1.3.1c" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.1.m1.1.1.3.6" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.1.m1.1b"><apply id="A1.SS1.SSS3.p5.1.m1.1.1.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSS3.p5.1.m1.1.1.1.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1">subscript</csymbol><ci id="A1.SS1.SSS3.p5.1.m1.1.1.2.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.2">𝑅</ci><apply id="A1.SS1.SSS3.p5.1.m1.1.1.3.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3"><times id="A1.SS1.SSS3.p5.1.m1.1.1.3.1.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.1"></times><ci id="A1.SS1.SSS3.p5.1.m1.1.1.3.2.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.2">𝑝</ci><ci id="A1.SS1.SSS3.p5.1.m1.1.1.3.3.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.3">𝑟</ci><ci id="A1.SS1.SSS3.p5.1.m1.1.1.3.4.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.4">𝑜</ci><ci id="A1.SS1.SSS3.p5.1.m1.1.1.3.5.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.5">𝑡</ci><ci id="A1.SS1.SSS3.p5.1.m1.1.1.3.6.cmml" xref="A1.SS1.SSS3.p5.1.m1.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.1.m1.1c">R_{proto}</annotation></semantics></math> and <math id="A1.SS1.SSS3.p5.2.m2.1" class="ltx_Math" alttext="R_{dedup}" display="inline"><semantics id="A1.SS1.SSS3.p5.2.m2.1a"><msub id="A1.SS1.SSS3.p5.2.m2.1.1" xref="A1.SS1.SSS3.p5.2.m2.1.1.cmml"><mi id="A1.SS1.SSS3.p5.2.m2.1.1.2" xref="A1.SS1.SSS3.p5.2.m2.1.1.2.cmml">R</mi><mrow id="A1.SS1.SSS3.p5.2.m2.1.1.3" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.cmml"><mi id="A1.SS1.SSS3.p5.2.m2.1.1.3.2" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.2.m2.1.1.3.1" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.2.m2.1.1.3.3" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.2.m2.1.1.3.1a" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.2.m2.1.1.3.4" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.2.m2.1.1.3.1b" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.2.m2.1.1.3.5" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.2.m2.1.1.3.1c" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.2.m2.1.1.3.6" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.6.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.2.m2.1b"><apply id="A1.SS1.SSS3.p5.2.m2.1.1.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSS3.p5.2.m2.1.1.1.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1">subscript</csymbol><ci id="A1.SS1.SSS3.p5.2.m2.1.1.2.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.2">𝑅</ci><apply id="A1.SS1.SSS3.p5.2.m2.1.1.3.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3"><times id="A1.SS1.SSS3.p5.2.m2.1.1.3.1.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.1"></times><ci id="A1.SS1.SSS3.p5.2.m2.1.1.3.2.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.2">𝑑</ci><ci id="A1.SS1.SSS3.p5.2.m2.1.1.3.3.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.3">𝑒</ci><ci id="A1.SS1.SSS3.p5.2.m2.1.1.3.4.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.4">𝑑</ci><ci id="A1.SS1.SSS3.p5.2.m2.1.1.3.5.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.5">𝑢</ci><ci id="A1.SS1.SSS3.p5.2.m2.1.1.3.6.cmml" xref="A1.SS1.SSS3.p5.2.m2.1.1.3.6">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.2.m2.1c">R_{dedup}</annotation></semantics></math> while using D4 will have impact on the performance of D4. Given limited compute, we are not able to sweep over these hyperparameters. Instead, we strategically choose these parameters: we first look at the highest value of <math id="A1.SS1.SSS3.p5.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="A1.SS1.SSS3.p5.3.m3.1a"><mi id="A1.SS1.SSS3.p5.3.m3.1.1" xref="A1.SS1.SSS3.p5.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.3.m3.1b"><ci id="A1.SS1.SSS3.p5.3.m3.1.1.cmml" xref="A1.SS1.SSS3.p5.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.3.m3.1c">R</annotation></semantics></math> in SemDeDup that results in perplexity improvement across validation sets. We choose the "highest value" because the purpose of SemDeDup is to remove duplicate-driven clusters and low <math id="A1.SS1.SSS3.p5.4.m4.1" class="ltx_Math" alttext="R" display="inline"><semantics id="A1.SS1.SSS3.p5.4.m4.1a"><mi id="A1.SS1.SSS3.p5.4.m4.1.1" xref="A1.SS1.SSS3.p5.4.m4.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.4.m4.1b"><ci id="A1.SS1.SSS3.p5.4.m4.1.1.cmml" xref="A1.SS1.SSS3.p5.4.m4.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.4.m4.1c">R</annotation></semantics></math> with SemDeDup generally removes more than just templates/semantic duplicates. As seen in Section&nbsp;<a href="#A1.SS3" title="A.3 Individual Breakdowns of Downstream Accuracy and PPL ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>, this generally occured with <math id="A1.SS1.SSS3.p5.5.m5.1" class="ltx_Math" alttext="R_{dedup}=0.75" display="inline"><semantics id="A1.SS1.SSS3.p5.5.m5.1a"><mrow id="A1.SS1.SSS3.p5.5.m5.1.1" xref="A1.SS1.SSS3.p5.5.m5.1.1.cmml"><msub id="A1.SS1.SSS3.p5.5.m5.1.1.2" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.cmml"><mi id="A1.SS1.SSS3.p5.5.m5.1.1.2.2" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.2.cmml">R</mi><mrow id="A1.SS1.SSS3.p5.5.m5.1.1.2.3" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.cmml"><mi id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.2" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.3" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1a" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.4" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1b" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.5" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1c" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.6" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.6.cmml">p</mi></mrow></msub><mo id="A1.SS1.SSS3.p5.5.m5.1.1.1" xref="A1.SS1.SSS3.p5.5.m5.1.1.1.cmml">=</mo><mn id="A1.SS1.SSS3.p5.5.m5.1.1.3" xref="A1.SS1.SSS3.p5.5.m5.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.5.m5.1b"><apply id="A1.SS1.SSS3.p5.5.m5.1.1.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1"><eq id="A1.SS1.SSS3.p5.5.m5.1.1.1.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.1"></eq><apply id="A1.SS1.SSS3.p5.5.m5.1.1.2.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSS3.p5.5.m5.1.1.2.1.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2">subscript</csymbol><ci id="A1.SS1.SSS3.p5.5.m5.1.1.2.2.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.2">𝑅</ci><apply id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3"><times id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.1"></times><ci id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.2.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.2">𝑑</ci><ci id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.3.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.3">𝑒</ci><ci id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.4.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.4">𝑑</ci><ci id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.5.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.5">𝑢</ci><ci id="A1.SS1.SSS3.p5.5.m5.1.1.2.3.6.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.2.3.6">𝑝</ci></apply></apply><cn type="float" id="A1.SS1.SSS3.p5.5.m5.1.1.3.cmml" xref="A1.SS1.SSS3.p5.5.m5.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.5.m5.1c">R_{dedup}=0.75</annotation></semantics></math>. Thus, we chose <math id="A1.SS1.SSS3.p5.6.m6.1" class="ltx_Math" alttext="R_{dedup}=0.75" display="inline"><semantics id="A1.SS1.SSS3.p5.6.m6.1a"><mrow id="A1.SS1.SSS3.p5.6.m6.1.1" xref="A1.SS1.SSS3.p5.6.m6.1.1.cmml"><msub id="A1.SS1.SSS3.p5.6.m6.1.1.2" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.cmml"><mi id="A1.SS1.SSS3.p5.6.m6.1.1.2.2" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.2.cmml">R</mi><mrow id="A1.SS1.SSS3.p5.6.m6.1.1.2.3" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.cmml"><mi id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.2" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.3" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1a" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.4" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1b" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.5" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1c" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.6" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.6.cmml">p</mi></mrow></msub><mo id="A1.SS1.SSS3.p5.6.m6.1.1.1" xref="A1.SS1.SSS3.p5.6.m6.1.1.1.cmml">=</mo><mn id="A1.SS1.SSS3.p5.6.m6.1.1.3" xref="A1.SS1.SSS3.p5.6.m6.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.6.m6.1b"><apply id="A1.SS1.SSS3.p5.6.m6.1.1.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1"><eq id="A1.SS1.SSS3.p5.6.m6.1.1.1.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.1"></eq><apply id="A1.SS1.SSS3.p5.6.m6.1.1.2.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSS3.p5.6.m6.1.1.2.1.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2">subscript</csymbol><ci id="A1.SS1.SSS3.p5.6.m6.1.1.2.2.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.2">𝑅</ci><apply id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3"><times id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.1"></times><ci id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.2.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.2">𝑑</ci><ci id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.3.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.3">𝑒</ci><ci id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.4.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.4">𝑑</ci><ci id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.5.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.5">𝑢</ci><ci id="A1.SS1.SSS3.p5.6.m6.1.1.2.3.6.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.2.3.6">𝑝</ci></apply></apply><cn type="float" id="A1.SS1.SSS3.p5.6.m6.1.1.3.cmml" xref="A1.SS1.SSS3.p5.6.m6.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.6.m6.1c">R_{dedup}=0.75</annotation></semantics></math> and varied <math id="A1.SS1.SSS3.p5.7.m7.1" class="ltx_Math" alttext="R_{proto}" display="inline"><semantics id="A1.SS1.SSS3.p5.7.m7.1a"><msub id="A1.SS1.SSS3.p5.7.m7.1.1" xref="A1.SS1.SSS3.p5.7.m7.1.1.cmml"><mi id="A1.SS1.SSS3.p5.7.m7.1.1.2" xref="A1.SS1.SSS3.p5.7.m7.1.1.2.cmml">R</mi><mrow id="A1.SS1.SSS3.p5.7.m7.1.1.3" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.cmml"><mi id="A1.SS1.SSS3.p5.7.m7.1.1.3.2" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.7.m7.1.1.3.1" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.7.m7.1.1.3.3" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.7.m7.1.1.3.1a" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.7.m7.1.1.3.4" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.7.m7.1.1.3.1b" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.7.m7.1.1.3.5" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="A1.SS1.SSS3.p5.7.m7.1.1.3.1c" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.1.cmml">​</mo><mi id="A1.SS1.SSS3.p5.7.m7.1.1.3.6" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS3.p5.7.m7.1b"><apply id="A1.SS1.SSS3.p5.7.m7.1.1.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSS3.p5.7.m7.1.1.1.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1">subscript</csymbol><ci id="A1.SS1.SSS3.p5.7.m7.1.1.2.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.2">𝑅</ci><apply id="A1.SS1.SSS3.p5.7.m7.1.1.3.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3"><times id="A1.SS1.SSS3.p5.7.m7.1.1.3.1.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.1"></times><ci id="A1.SS1.SSS3.p5.7.m7.1.1.3.2.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.2">𝑝</ci><ci id="A1.SS1.SSS3.p5.7.m7.1.1.3.3.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.3">𝑟</ci><ci id="A1.SS1.SSS3.p5.7.m7.1.1.3.4.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.4">𝑜</ci><ci id="A1.SS1.SSS3.p5.7.m7.1.1.3.5.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.5">𝑡</ci><ci id="A1.SS1.SSS3.p5.7.m7.1.1.3.6.cmml" xref="A1.SS1.SSS3.p5.7.m7.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS3.p5.7.m7.1c">R_{proto}</annotation></semantics></math> to obtain different data selection ratios for D4.</p>
</div>
</section>
<section id="A1.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.4 </span>Which validation sets go into the averages?</h4>

<div id="A1.SS1.SSS4.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS4.p1.1">명확성을 위해 평균을 보고할 때 "웹 스냅샷", "비 웹 스냅샷" 및 "명령 + 응답"으로 간주하는 유효성 검사 세트를 명시적으로 명시합니다.</p>
</div>
<div id="A1.SS1.SSS4.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.SSS4.p2.1.1">Web Snapshots</span>: perplexity on validation set of C4, CC-dedup, CommonCrawl (from the Pile)</p>
</div>
<div id="A1.SS1.SSS4.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS4.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.SSS4.p3.1.1">Non-web Snapshots</span>: perplexity other validation sets from the Pile, comprising of OpenWebText2, HackerNews, Wikipedia (en), BookCorpusFair, DM Mathematics, Gutenberg PG-19, OpenSubtitles, and USPTO. 또한 이 평균에는 "redditflattened"(Pusshift.io Reddit <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>로부터 설정된 검증), "스토리", "prompts_with_answers"(이하 설명됨) 및 "prompts"(이는 "prompts_with_answers"와 동일하지만 각 샘플은 답이 없는 명령 조정 프롬프트일 뿐이다.</p>
</div>
<div id="A1.SS1.SSS4.p4" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS4.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.SSS4.p4.1.1">Instruct + Answers</span>: perplexity on instruction-tuning data from OPT-IML <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>, 여기서 각 샘플은 instruction-tuning prompt와 answer를 모두 포함한다(도<a class="ltx_ref" href="#A1.F4" title="Figure A4 ‣ A.3 Individual Breakdowns of Downstream Accuracy and PPL ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A4</span></a>에서 이를 "prompts_with_answers"라고 한다).</p>
</div>
<div id="A1.SS1.SSS4.p5" class="ltx_para">
<p class="ltx_p" id="A1.SS1.SSS4.p5.1">웹 스냅샷 및 비웹 스냅샷의 유효성 검사 세트는 명확하지만(표준 오픈 소스 데이터 세트이거나 일반적으로 사용되는 데이터에서 파생됨), "명령 + 응답" 데이터는 일부 독자에게 새로운 것일 수 있습니다. 이 유효성 검사 세트가 표 <a class="ltx_ref" href="#A1.T2" title="Table A2 ‣ A.1.4 Which validation sets go into the averages? ‣ A.1 Experimental Setup Details ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A2</span></a>에서 어떻게 보이는지 몇 가지 예를 제공합니다.</p>
</div>
<figure id="A1.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T2.2.1.1" style="font-size:90%;">Table A2</span>:</span><span class="ltx_text" id="A1.T2.3.2" style="font-size:90%;">Examples from "Instruct + Answers" validation set</span></figcaption>
<table id="A1.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T2.4.1" class="ltx_tr">
<td id="A1.T2.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T2.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.4.1.1.1.1" class="ltx_p" style="width:398.3pt;"><span id="A1.T2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.4.2" class="ltx_tr">
<td id="A1.T2.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T2.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.4.2.1.1.1" class="ltx_p" style="width:398.3pt;">Instructions: In this task, you are given two phrases: Head and Tail, separated with &lt;sep&gt;. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether the Head is located or can be found at/in/on the Tail or not. Classify your answers into "Yes" and "No". The phrase may also contain "___", a placeholder that can be an object, a person, and/or an action.Input: Head: PersonX acknowledges gratefully the ___&lt;sep&gt;Tail: to use it Output: No</span>
</span>
</td>
</tr>
<tr id="A1.T2.4.3" class="ltx_tr">
<td id="A1.T2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.4.3.1.1.1" class="ltx_p" style="width:398.3pt;">Read the given sentence and if it is a general advice then indicate via "yes". Otherwise indicate via "no". advice is basically offering suggestions about the best course of action to someone. advice can come in a variety of forms, for example Direct advice and Indirect advice. (1) Direct advice: Using words (e.g., suggest, advice, recommend), verbs (e.g., can, could, should, may), or using questions (e.g., why don’t you’s, how about, have you thought about). (2) Indirect advice: contains hints from personal experiences with the intention for someone to do the same thing or statements that imply an action should (or should not) be taken. Input: Let it go. Output: yes"</span>
</span>
</td>
</tr>
<tr id="A1.T2.4.4" class="ltx_tr">
<td id="A1.T2.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T2.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.4.4.1.1.1" class="ltx_p" style="width:398.3pt;">Instructions: You are given a sentence in English. Your job is to translate the English sentence into Italian. No! Demand to understand. Ask. Answer: No! Esigete di comprendere. Chiedete.</span>
</span>
</td>
</tr>
<tr id="A1.T2.4.5" class="ltx_tr">
<td id="A1.T2.4.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T2.4.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.4.5.1.1.1" class="ltx_p" style="width:398.3pt;">Task: In this task you will be given a list of integers. You should round each integer to the nearest tens place. That means you should round the number to the nearest multiple of 10.Input: [528, -636, -686, 368, -433, 992, 886] Answer: [530, -640, -690, 370, -430, 990, 890]</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Efficiency gains across model scales and training</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p1.3">이 섹션에서는 D4를 통해 데이터를 선택하여 얻은 모델 규모와 성능 이득 사이의 관계를 조사한다. 구체적으로, <math alttext="T_{target}=3" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1.1"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><msub id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml"><mi id="A1.SS2.p1.1.m1.1.1.2.2" xref="A1.SS2.p1.1.m1.1.1.2.2.cmml">T</mi><mrow id="A1.SS2.p1.1.m1.1.1.2.3" xref="A1.SS2.p1.1.m1.1.1.2.3.cmml"><mi id="A1.SS2.p1.1.m1.1.1.2.3.2" xref="A1.SS2.p1.1.m1.1.1.2.3.2.cmml">t</mi><mo id="A1.SS2.p1.1.m1.1.1.2.3.1" lspace="0em" rspace="0em" xref="A1.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.1.m1.1.1.2.3.3" xref="A1.SS2.p1.1.m1.1.1.2.3.3.cmml">a</mi><mo id="A1.SS2.p1.1.m1.1.1.2.3.1a" lspace="0em" rspace="0em" xref="A1.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.1.m1.1.1.2.3.4" xref="A1.SS2.p1.1.m1.1.1.2.3.4.cmml">r</mi><mo id="A1.SS2.p1.1.m1.1.1.2.3.1b" lspace="0em" rspace="0em" xref="A1.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.1.m1.1.1.2.3.5" xref="A1.SS2.p1.1.m1.1.1.2.3.5.cmml">g</mi><mo id="A1.SS2.p1.1.m1.1.1.2.3.1c" lspace="0em" rspace="0em" xref="A1.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.1.m1.1.1.2.3.6" xref="A1.SS2.p1.1.m1.1.1.2.3.6.cmml">e</mi><mo id="A1.SS2.p1.1.m1.1.1.2.3.1d" lspace="0em" rspace="0em" xref="A1.SS2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.1.m1.1.1.2.3.7" xref="A1.SS2.p1.1.m1.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="A1.SS2.p1.1.m1.1.1.1" xref="A1.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS2.p1.1.m1.1.1.3" xref="A1.SS2.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><eq id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1.1"></eq><apply id="A1.SS2.p1.1.m1.1.1.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p1.1.m1.1.1.2.1.cmml" xref="A1.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="A1.SS2.p1.1.m1.1.1.2.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2.2">𝑇</ci><apply id="A1.SS2.p1.1.m1.1.1.2.3.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3"><times id="A1.SS2.p1.1.m1.1.1.2.3.1.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.1"></times><ci id="A1.SS2.p1.1.m1.1.1.2.3.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.2">𝑡</ci><ci id="A1.SS2.p1.1.m1.1.1.2.3.3.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.3">𝑎</ci><ci id="A1.SS2.p1.1.m1.1.1.2.3.4.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.4">𝑟</ci><ci id="A1.SS2.p1.1.m1.1.1.2.3.5.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.5">𝑔</ci><ci id="A1.SS2.p1.1.m1.1.1.2.3.6.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.6">𝑒</ci><ci id="A1.SS2.p1.1.m1.1.1.2.3.7.cmml" xref="A1.SS2.p1.1.m1.1.1.2.3.7">𝑡</ci></apply></apply><cn id="A1.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">T_{target}=3</annotation></semantics></math>B 토큰으로 훈련된 125M OPT 모델, <math alttext="T_{target}=40" class="ltx_Math" display="inline" id="A1.SS2.p1.2.m2.1"><semantics id="A1.SS2.p1.2.m2.1a"><mrow id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><msub id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml"><mi id="A1.SS2.p1.2.m2.1.1.2.2" xref="A1.SS2.p1.2.m2.1.1.2.2.cmml">T</mi><mrow id="A1.SS2.p1.2.m2.1.1.2.3" xref="A1.SS2.p1.2.m2.1.1.2.3.cmml"><mi id="A1.SS2.p1.2.m2.1.1.2.3.2" xref="A1.SS2.p1.2.m2.1.1.2.3.2.cmml">t</mi><mo id="A1.SS2.p1.2.m2.1.1.2.3.1" lspace="0em" rspace="0em" xref="A1.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.2.m2.1.1.2.3.3" xref="A1.SS2.p1.2.m2.1.1.2.3.3.cmml">a</mi><mo id="A1.SS2.p1.2.m2.1.1.2.3.1a" lspace="0em" rspace="0em" xref="A1.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.2.m2.1.1.2.3.4" xref="A1.SS2.p1.2.m2.1.1.2.3.4.cmml">r</mi><mo id="A1.SS2.p1.2.m2.1.1.2.3.1b" lspace="0em" rspace="0em" xref="A1.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.2.m2.1.1.2.3.5" xref="A1.SS2.p1.2.m2.1.1.2.3.5.cmml">g</mi><mo id="A1.SS2.p1.2.m2.1.1.2.3.1c" lspace="0em" rspace="0em" xref="A1.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.2.m2.1.1.2.3.6" xref="A1.SS2.p1.2.m2.1.1.2.3.6.cmml">e</mi><mo id="A1.SS2.p1.2.m2.1.1.2.3.1d" lspace="0em" rspace="0em" xref="A1.SS2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.2.m2.1.1.2.3.7" xref="A1.SS2.p1.2.m2.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="A1.SS2.p1.2.m2.1.1.1" xref="A1.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS2.p1.2.m2.1.1.3" xref="A1.SS2.p1.2.m2.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><eq id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1.1"></eq><apply id="A1.SS2.p1.2.m2.1.1.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p1.2.m2.1.1.2.1.cmml" xref="A1.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="A1.SS2.p1.2.m2.1.1.2.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2.2">𝑇</ci><apply id="A1.SS2.p1.2.m2.1.1.2.3.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3"><times id="A1.SS2.p1.2.m2.1.1.2.3.1.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.1"></times><ci id="A1.SS2.p1.2.m2.1.1.2.3.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.2">𝑡</ci><ci id="A1.SS2.p1.2.m2.1.1.2.3.3.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.3">𝑎</ci><ci id="A1.SS2.p1.2.m2.1.1.2.3.4.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.4">𝑟</ci><ci id="A1.SS2.p1.2.m2.1.1.2.3.5.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.5">𝑔</ci><ci id="A1.SS2.p1.2.m2.1.1.2.3.6.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.6">𝑒</ci><ci id="A1.SS2.p1.2.m2.1.1.2.3.7.cmml" xref="A1.SS2.p1.2.m2.1.1.2.3.7">𝑡</ci></apply></apply><cn id="A1.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.p1.2.m2.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">T_{target}=40</annotation></semantics></math>B 토큰으로 훈련된 1.3B OPT 모델 및 <math alttext="T_{target}=100" class="ltx_Math" display="inline" id="A1.SS2.p1.3.m3.1"><semantics id="A1.SS2.p1.3.m3.1a"><mrow id="A1.SS2.p1.3.m3.1.1" xref="A1.SS2.p1.3.m3.1.1.cmml"><msub id="A1.SS2.p1.3.m3.1.1.2" xref="A1.SS2.p1.3.m3.1.1.2.cmml"><mi id="A1.SS2.p1.3.m3.1.1.2.2" xref="A1.SS2.p1.3.m3.1.1.2.2.cmml">T</mi><mrow id="A1.SS2.p1.3.m3.1.1.2.3" xref="A1.SS2.p1.3.m3.1.1.2.3.cmml"><mi id="A1.SS2.p1.3.m3.1.1.2.3.2" xref="A1.SS2.p1.3.m3.1.1.2.3.2.cmml">t</mi><mo id="A1.SS2.p1.3.m3.1.1.2.3.1" lspace="0em" rspace="0em" xref="A1.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.3.m3.1.1.2.3.3" xref="A1.SS2.p1.3.m3.1.1.2.3.3.cmml">a</mi><mo id="A1.SS2.p1.3.m3.1.1.2.3.1a" lspace="0em" rspace="0em" xref="A1.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.3.m3.1.1.2.3.4" xref="A1.SS2.p1.3.m3.1.1.2.3.4.cmml">r</mi><mo id="A1.SS2.p1.3.m3.1.1.2.3.1b" lspace="0em" rspace="0em" xref="A1.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.3.m3.1.1.2.3.5" xref="A1.SS2.p1.3.m3.1.1.2.3.5.cmml">g</mi><mo id="A1.SS2.p1.3.m3.1.1.2.3.1c" lspace="0em" rspace="0em" xref="A1.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.3.m3.1.1.2.3.6" xref="A1.SS2.p1.3.m3.1.1.2.3.6.cmml">e</mi><mo id="A1.SS2.p1.3.m3.1.1.2.3.1d" lspace="0em" rspace="0em" xref="A1.SS2.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="A1.SS2.p1.3.m3.1.1.2.3.7" xref="A1.SS2.p1.3.m3.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="A1.SS2.p1.3.m3.1.1.1" xref="A1.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="A1.SS2.p1.3.m3.1.1.3" xref="A1.SS2.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.1b"><apply id="A1.SS2.p1.3.m3.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1"><eq id="A1.SS2.p1.3.m3.1.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1"></eq><apply id="A1.SS2.p1.3.m3.1.1.2.cmml" xref="A1.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p1.3.m3.1.1.2.1.cmml" xref="A1.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="A1.SS2.p1.3.m3.1.1.2.2.cmml" xref="A1.SS2.p1.3.m3.1.1.2.2">𝑇</ci><apply id="A1.SS2.p1.3.m3.1.1.2.3.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3"><times id="A1.SS2.p1.3.m3.1.1.2.3.1.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.1"></times><ci id="A1.SS2.p1.3.m3.1.1.2.3.2.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.2">𝑡</ci><ci id="A1.SS2.p1.3.m3.1.1.2.3.3.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.3">𝑎</ci><ci id="A1.SS2.p1.3.m3.1.1.2.3.4.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.4">𝑟</ci><ci id="A1.SS2.p1.3.m3.1.1.2.3.5.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.5">𝑔</ci><ci id="A1.SS2.p1.3.m3.1.1.2.3.6.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.6">𝑒</ci><ci id="A1.SS2.p1.3.m3.1.1.2.3.7.cmml" xref="A1.SS2.p1.3.m3.1.1.2.3.7">𝑡</ci></apply></apply><cn id="A1.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="A1.SS2.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.1c">T_{target}=100</annotation></semantics></math>B 토큰으로 훈련된 6.7B OPT 모델의 세 그룹을 훈련한다. 우리는 그림 <a class="ltx_ref" href="#A1.F2" title="Figure A2 ‣ A.2 Efficiency gains across model scales and training ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A2</span></a>에서 D4가 복잡성 측면에서 전체적으로 효율 향상을 가져온다는 것을 알 수 있다. 놀랍게도, 이러한 효율성 증가는 규모에 따라 증가하는 것으로 보이며, 이는 더 큰 모델 규모에서 D4가 훨씬 더 많은 효율성 증가로 이어질 수 있음을 나타낸다. 또한 1.3B 및 6.7B 모델의 경우 0샷 다운스트림 정확도에서 1.3B 및 6.7B 모델 모두에 대해 30% 정도의 효율성 향상을 볼 수 있지만 중간 체크포인트에 대한 다운스트림 성능 평가는 완료되지 않은 학습률 일정으로 인해 완전히 공정하지 않다는 점에 주목한다. 그럼에도 불구하고, 다운스트림 정확도 효율성 이득은 규모에 따라 감소하지 않는다는 것을 알 수 있다.</p>
</div>
<figure id="A1.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x11.png" id="A1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="475" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F2.3.1.1" style="font-size:90%;">Figure A2</span>:</span><span class="ltx_text" id="A1.F2.4.2" style="font-size:90%;">Training trajectory of OPT models trained on raw data (gray line) and data selected via D4 (pink line). 모델 스케일(1행: 2B 토큰에 대해 트레이닝된 8M OPT 모델, 2행: 3B 토큰에 대해 트레이닝된 125M OPT 모델, 3행: 40B 토큰에 대해 트레이닝된 1.3B OPT 모델, 4행: 100B 토큰에 대해 트레이닝된 6.7B OPT 모델)에 걸쳐, 16개의 NLP 태스크(오른쪽 열)에서 복잡도(왼쪽 두 열) 및 0-샷 다운스트림 정확도 모두에서 상당한 효율성 이득을 볼 수 있다. 중요한 것은 모형 규모를 늘리는 것이 효율성 증가를 감소시키지 않는다는 것입니다. 모든 도표는 마지막 행을 제외하고 세 가지 종자에 걸쳐 평균 및 표준 오차를 보여준다. 특정 데이터 선택 방법이 더 나은지 여부를 나타내기에는 무작위 성능에 너무 가깝기 때문에 1.3B보다 작은 모델에 대해 다운스트림 정확도를 평가하지 않는다. </span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure A2</span>: </span><span id="A1.F2.4.2" class="ltx_text" style="font-size:90%;">Training trajectory of OPT models trained on raw data (gray line) and data selected via D4 (pink line). Across model scales (1st row: 8M OPT models trained on 2B tokens, 2nd row: 125M OPT models trained on 3B tokens, 3rd row: 1.3B OPT models trained on 40B tokens, 4th row: 6.7B OPT models trained on 100B tokens), we see significant efficiency gains in both perplexity (left two columns) and 0-shot downstream accuracy on 16 NLP tasks (right column). Importantly, we see that increasing model scale does not decrease efficiency gains. All plots show mean and standard error across three seeds, except for the last row. We do not evaluate downstream accuracy for models smaller than 1.3B because they are likely too close to random performance to indicate whether a particular data selection method is better.
</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Individual Breakdowns of Downstream Accuracy and PPL</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS3.p1.1">섹션 <a class="ltx_ref" href="#S4" title="4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4</span></a>에서 우리는 D4, SSL 프로토타입 및 SemDeDup이 베이스라인 훈련과 비교하여 복잡성(서로 다른 검증 세트에 걸쳐 평균화됨) 및 다운스트림 정확도(서로 다른 NLP 작업에 걸쳐 평균화됨)에 대한 상당한 이득을 달성한다는 것을 알 수 있다. 또한, 일반적으로 D4가 SSL 프로토타입 및 SemDeDup보다 우수하다는 것을 알 수 있다. 이 섹션에서는 개별 작업에 걸쳐 이러한 주장에 대한 보다 세밀한 분석을 제공한다.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS3.p2.1">복잡성의 경우 그림 <a class="ltx_ref" href="#A1.F4" title="Figure A4 ‣ A.3 Individual Breakdowns of Downstream Accuracy and PPL ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A4</span></a>에서 섹션 <a class="ltx_ref" href="#S4" title="4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4</span></a>의 클레임이 일반적으로 유효성 검사 세트에 걸쳐 유지된다는 것을 알 수 있습니다. 웹 스냅샷 유효성 검사 세트의 경우 C4, CC-dedup 및 CommonCrawl과 같은 웹 스냅샷 유효성 검사 세트의 경우 기본 학습에 비해 데이터 선택으로 성능이 악화되고 D4가 일반적으로 성능 저하 속도가 가장 느립니다. 우리는 모든 비 웹 스냅샷 검증 세트에서 데이터 선택 방법 중 명확한 승자가 없다는 점에 주목한다. 그러나 <span class="ltx_text ltx_font_italic" id="A1.SS3.p2.1.1">우리는 대부분의 검증 세트에서 기준선 훈련보다 일관된 개선을 관찰한다.</p>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS3.p3.1">다운스트림 정확도를 위해 OPT 아키텍처와 하이퍼파라미터를 사용하기 때문에 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에서 수행된 정확한 다운스트림 평가와 일치하도록 선택했다. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>와 유사하게 그림 <a class="ltx_ref" href="#A1.F3" title="Figure A3 ‣ A.3 Individual Breakdowns of Downstream Accuracy and PPL ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A3</span></a>에서 16개의 NLP 작업에 걸쳐 상당한 변동성을 발견하여 작업에 걸쳐 평균 다운스트림 정확도를 살펴보도록 동기를 부여한다.</p>
</div>
<figure id="A1.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x12.png" id="A1.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="483" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F3.3.1.1" style="font-size:90%;">Figure A3</span>:</span><span class="ltx_text" id="A1.F3.4.2" style="font-size:90%;">Per-task breakdown of 0-shot downstream accuracy comparison across data selection methods, for 1.3B, 40B OPT model. 위에 도시된 16개의 NLP 태스크들에 대한 설명은 Section <a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a>를 참조한다. 우리는 개별 다운스트림 작업에 걸쳐 상당한 변동성이 있다는 점에 주목한다. </span></figcaption>
</figure>
<figure id="A1.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x13.png" id="A1.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="462" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F4.3.1.1" style="font-size:90%;">Figure A4</span>:</span><span class="ltx_text" id="A1.F4.4.2" style="font-size:90%;">Perplexity as a function of source dataset size for 1.3B OPT model 40B token training runs, across data selection run. 위의 각 플롯은 개별 유효성 검사 세트에 대한 복잡성을 나타냅니다(자세한 내용은 섹션 <a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a> 참조). 3개의 씨드에 걸친 평균 및 표준 오차가 표시된다(표준 오차는 음영 영역으로 표시됨). </span></figcaption>
</figure>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>SSL prototypes and SemDeDup overlap</h3>

<figure id="A1.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x14.png" id="A1.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F5.7.3.1" style="font-size:90%;">Figure A5</span>:</span><span class="ltx_text" id="A1.F5.5.4.2" style="font-size:90%;">데이터 선택 방법 간의 유사성. 각 제곱은 두 가지 다른 전략을 통해 데이터를 선택할 때 교차하는 훈련 데이터의 백분율을 나타낸다. <math alttext="x" class="ltx_Math" display="inline" id="A1.F5.4.3.1.m1.1"><semantics id="A1.F5.4.3.1.m1.1b"><mi id="A1.F5.4.3.1.m1.1.1" xref="A1.F5.4.3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A1.F5.4.3.1.m1.1c"><ci id="A1.F5.4.3.1.m1.1.1.cmml" xref="A1.F5.4.3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.4.3.1.m1.1d">x</annotation></semantics></math> 및 <math alttext="y" class="ltx_Math" display="inline" id="A1.F5.5.4.2.m2.1"><semantics id="A1.F5.5.4.2.m2.1b"><mi id="A1.F5.5.4.2.m2.1.1" xref="A1.F5.5.4.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="A1.F5.5.4.2.m2.1c"><ci id="A1.F5.5.4.2.m2.1.1.cmml" xref="A1.F5.5.4.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.5.4.2.m2.1d">y</annotation></semantics></math> 축은 서로 다른 데이터 선택 전략을 열거한다. </span></figcaption>
</figure>
<div id="A1.SS4.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS4.p1.1">그림 <a class="ltx_ref" href="#A1.F5" title="Figure A5 ‣ A.4 SSL prototypes and SemDeDup overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A5</span></a>는 SemDeDup과 SSL Prototypes에서 선택한 데이터셋 간의 중첩을 보여준다. 두 방법은 동일한 데이터 포인트 세트에 도달하지 않지만 두 방법에 의해 선별된 데이터 세트 사이에는 상당한 중복이 있다. SSL 프로토타입과 SemDeDup이 클러스터 중심을 둘러싼 공간의 밀집 영역을 가지치기하기 때문이라고 가정한다. 정의에 따라 SemDeDup은 클러스터 내의 공간의 밀집 영역을 희소화하며, 유사하게 정의에 따라 SSL 프로토타입은 클러스터 중심에 가까운 데이터 포인트를 가지치기한다. K-평균 클러스터링은 중심점을 공간의 밀집된 영역에 배치하기 때문에 (그림 <a class="ltx_ref" href="#A1.F6" title="Figure A6 ‣ A.4 SSL prototypes and SemDeDup overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A6</span></a> 참조) 클러스터 중심점에 대한 코사인 거리의 분포가 오른쪽으로 치우친 것을 관찰하므로 공간 주변 중심점의 영역이 밀집될 것임을 알고 SSL 프로토타입과 SemDedup이 상당한 중첩을 가질 것으로 예상한다. 질적으로, 우리는 그림 <a class="ltx_ref" href="#A1.T3" title="Table A3 ‣ A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A3</span></a>, 그림 <a class="ltx_ref" href="#A1.T4" title="Table A4 ‣ A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A4</span></a>, 그림 <a class="ltx_ref" href="#A1.T5" title="Table A5 ‣ A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A5</span></a>에서 클러스터 중심에 가까운 점의 몇 가지 예를 검사하고, 클러스터 중심에 가까운 예가 의미적으로 중복될 수 있음을 확인한다(예: 템플릿). 따라서 합리적인 데이터 선택 전략이 군집 중심을 둘러싼 이러한 밀집된 공간 영역을 희소화하는 것을 우선시하는 것이 합리적이다. <a class="ltx_ref" href="#S3.SS4" title="3.4 Data Selection Strategies (choices for 𝑆) ‣ 3 Experimental Setup ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">3.4</span></a> 절에서 언급했듯이, 과도한 의미 중복을 포함하는 공간의 이러한 밀집된 영역을 희소화하는 것은 D4의 원래 동기 부여이다. 그림 <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.4.2 Importance of re-clustering between SemDeDup and SSL Prototypes ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">7</span></a>에서 볼 수 있듯이, 재클러스터링 단계를 생략하면 성능이 크게 악화되며, 그림 <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.4.2 Importance of re-clustering between SemDeDup and SSL Prototypes ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">7</span></a>의 오른쪽 그림에서 SemDeDup이 실제로 중복 구동 클러스터를 제거한다는 것을 관찰한다.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/v7_aug16_images/distribution_of_cosine_distance_hist.png" id="A1.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="221" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F6.3.1.1" style="font-size:90%;">Figure A6</span>:</span><span class="ltx_text" id="A1.F6.4.2" style="font-size:90%;">Distribution of cosine distance to cluster centroids for  50M random selected documents from the training set of CC-dedup. 분포가 오른쪽으로 치우쳐 있음을 알 수 있으며, 이는 데이터 포인트가 일반적으로 중심에 가깝다는 것을 의미합니다. </span></figcaption>
</figure>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Investigating Train-Validation overlap</h3>

<div id="A1.SS5.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS5.p1.1">섹션 <a class="ltx_ref" href="#S4.SS4" title="4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.4</span></a>에서 간략하게 설명한 바와 같이, 많은 검증 세트가 훈련 세트에 가깝고(코사인 거리에서), 데이터 선택의 영향은 개별 검증 세트에 따라 다르다는 것을 관찰한다. 개별 검증 세트는 임베딩 공간의 다른 영역에 존재하며, 따라서 데이터 선택에 의해 다르게 영향을 받는다. 예를 들어, C4와 같은 웹 스냅샷 검증 세트는 임베딩 공간에서 CC-dedup에 가까운 반면, 난해한 검증 세트(예: Gutenberg PG 19 또는 DM Mathematics)는 멀리 있을 수 있다고 상상할 수 있다. 이를 정량화하기 위해 먼저 모든 검증 세트에서 각 검증 포인트에 대한 훈련 세트에서 가장 가까운 이웃을 찾는다. 그런 다음 훈련 세트에서 가장 가까운 이웃이 실제로 검증 포인트에 대한 정보를 전달한다는 것을 정성적으로 확인한다(예를 들어 표 <a class="ltx_ref" href="#A1.T8" title="Table A8 ‣ A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A8</span></a> 및 표 <a class="ltx_ref" href="#A1.T9" title="Table A9 ‣ A.9 Investigating Duplicate-Driven Clusters ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A9</span></a> 참조). 훈련점과 검증점 사이에 상당한 중복이 관찰된다. 그런 다음 각 유효성 검사 세트가 훈련 세트에 얼마나 가까운지 정성적으로 분석합니다. 그림 <a class="ltx_ref" href="#A1.F12" title="Figure A12 ‣ A.5 Investigating Train-Validation overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A12</span></a>에서 각 유효성 검사 세트에 대한 이 분포의 분류를 보여줍니다. 웹 스냅샷 검증 세트는 오른쪽으로 치우쳐 있어 훈련 세트에 가장 가까운 반면 난해한 검증 세트(구텐베르크 또는 위키피디아(en))는 더 중앙에 있거나 약간 왼쪽으로 치우친 경향이 있다.</p>
</div>
<div id="A1.SS5.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS5.p2.1">이에 동기화되어 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>에서 검증 세트를 나란히(훈련 세트까지의 거리 측면에서) 비교하며 유사한 경향을 볼 수 있다. 서로 다른 유효성 검사 세트가 데이터 선택에 의해 다르게 영향을 받는 이유를 더 이해하기 위해 유효성 검사 세트 및 레코드의 각 데이터 지점을 순환합니다.</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i1.p1.1">distance to the training set, 예를 들어, 얼마나 가까운 것이 검증 포인트 to the training set?</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i2.p1.1">D4를 사용한 데이터 선택 전후의 복잡도 차이, 예를 들어 이 검증 지점이 데이터 선택에 의해 얼마나 영향을 받았는지</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i3.p1.1">원래의 복잡함, 예를 들어 이 데이터 포인트는 원래 얼마나 쉬웠습니까?</p>
</div>
</li>
</ul>
</div>
<div id="A1.SS5.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS5.p3.1">그림 <a class="ltx_ref" href="#A1.F11" title="Figure A11 ‣ A.5 Investigating Train-Validation overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A11</span></a>에서 흥미로운 경향을 관찰합니다. C4와 같은 웹 스냅샷 검증 세트의 경우, 훈련 세트에 가장 가까운 검증 포인트는 (1) 데이터 선택 전에 가장 쉬운(가장 낮은 복잡도) 포인트와 (2) 데이터 선택에 의해 가장 영향을 받는 포인트입니다. 이는 이들 검증 포인트들이 트레이닝 포인트들에 대한 근접성으로 인해 "쉽다"는 것을 나타내는 것으로 보이며, 데이터 선택으로 인해 이들 트레이닝 포인트들이 트레이닝 세트로부터 제거될 때, 근접-바이 검증 포인트들은 모델에 대해 어려워진다. 우리는 DM 수학 및 오픈 서브타이틀과 같은 비웹 스냅샷 검증 세트에서는 이러한 경향을 보지 못하며, 실제로 훈련 세트에서 가장 먼 지점이 일반적으로 데이터 선택에 의해 가장 많은 영향을 받는 반대 경향을 본다.</p>
</div>
<div id="A1.SS5.p4" class="ltx_para">
<p class="ltx_p" id="A1.SS5.p4.1">정상성 검사로 섹션 <a class="ltx_ref" href="#S4.SS4" title="4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.4</span></a>에서 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>를 표시하는 데 사용되는 유효성 검사 세트의 크기를 변경합니다. <a class="ltx_ref" href="#A1.F8" title="Figure A8 ‣ A.5 Investigating Train-Validation overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A8</span></a>에서 검증 세트 크기를 제어하면 웹에서 파생된 검증 세트에서 웹 독립적인 검증 세트로 이동하는 동일한 점프를 얻을 수 있음을 알 수 있다. 이 실험을 실행할 때 특정 검증 세트가 너무 크면 무작위로 샘플링해야 하며, 이러한 무작위 샘플링이 학습 데이터 세트의 가장 가까운 이웃까지의 거리를 너무 많이 변경하지 않도록 하기 위해 그림 <a class="ltx_ref" href="#A1.F7" title="Figure A7 ‣ A.5 Investigating Train-Validation overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A7</span></a>에서 서로 다른 크기의 세 데이터 세트에 대해 샘플링하는 양을 변경한다. 검증 세트에서 무작위로 샘플링한 양을 변경해도 기차에서 가장 가까운 이웃까지의 평균 거리가 크게 변경되지 않는다는 것을 관찰한다.</p>
</div>
<div id="A1.SS5.p5" class="ltx_para">
<p class="ltx_p" id="A1.SS5.p5.1">또한 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>에서 검증 세트 간의 차이가 훈련 세트 크기에 의한 것인지도 조사한다. 보다 작은 훈련 세트는 () 이후 유효성 검사 세트에서 "추가"될 것으로 예상한다. 실제로 우리는 그림 <a class="ltx_ref" href="#A1.F9" title="Figure A9 ‣ A.5 Investigating Train-Validation overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A9</span></a>에서 이것을 본다. 그러나, 우리는 (훈련 세트까지의 평균 거리에 대한) 검증 세트의 상대적 순서가 임의의 고정된 훈련 데이터세트 크기에 대해 동일하게 유지된다는 것을 관찰한다. 또한 그림 <a class="ltx_ref" href="#A1.F10" title="Figure A10 ‣ A.5 Investigating Train-Validation overlap ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A10</span></a>에서 학습 데이터 세트 크기를 줄이더라도 모든 검증 세트의 상대적 순위뿐만 아니라 원래 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>에서 웹 파생에서 웹 독립 검증 세트로의 점프가 성립함을 알 수 있다.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x15.png" id="A1.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F7.3.1.1" style="font-size:90%;">Figure A7</span>:</span><span class="ltx_text" id="A1.F7.4.2" style="font-size:90%;">Studying the effect of validation set size on cosine distance to nearest-neighbor in training set. x축에서는 검증 세트의 크기를 변경하며(원래 더 큰 검증 세트를 무작위로 샘플링하여), y축은 훈련 세트에서 가장 가까운 이웃까지의 거리를 나타낸다(검증 세트에 걸쳐 평균). 우리는 원래 검증 세트의 어떤 부분이 샘플링되었는지에 관계없이 기차에서 가장 가까운 이웃까지의 평균 거리는 변하지 않으며, 이는 그림 <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a>가 다른 검증 세트 크기로 인한 것이 아님을 나타낸다. </span></figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x16.png" id="A1.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F8.3.1.1" style="font-size:90%;">Figure A8</span>:</span><span class="ltx_text" id="A1.F8.4.2" style="font-size:90%;">Investigating whether Figure <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a> changes if we control for validation set size. 위의 그림에서 각 검증 세트는 우리가 사용하는 가장 작은 검증 세트(BookCorpusFair)의 크기인 50개의 데이터 포인트를 포함한다. 검증 세트가 50개의 데이터 포인트보다 큰 경우, 우리는 50개의 데이터 포인트를 얻기 위해 검증 세트를 무작위로 샘플링한다. </span></figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x17.png" id="A1.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F9.3.1.1" style="font-size:90%;">Figure A9</span>:</span><span class="ltx_text" id="A1.F9.4.2" style="font-size:90%;">Studying the effect of training set set size on cosine distance to nearest-neighbor in training set. x 축에서 우리는 (원래 훈련 세트를 무작위로 샘플링하여) 훈련 세트의 크기를 변경하고 y 축은 (검증 세트에 걸쳐 평균화된) 훈련 세트에서 가장 가까운 이웃까지의 거리를 나타낸다. 훈련 집합에 대한 코사인 거리는 훈련 집합이 작을수록 증가하지만 검증 집합의 상대적 순서(훈련 집합에 대한 평균 거리에 대한)는 동일하게 유지됨을 관찰한다. </span></figcaption>
</figure>
<figure id="A1.F10" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x18.png" id="A1.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F10.3.1.1" style="font-size:90%;">Figure A10</span>:</span><span class="ltx_text" id="A1.F10.4.2" style="font-size:90%;">Investigating whether Figure <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4.1 Why does data selection hurt performance on web snapshots? ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">5</span></a> changes if we change training set size set size. 위의 그림에서 각 플롯은 훈련 세트의 분수를 무작위로 샘플링한다(분수는 플롯의 제목으로 표시됨). 검증 세트의 상대적 순위는 일반적으로 동일하게 유지되며, 웹 유래 검증 세트와 웹 독립 검증 세트 사이에 지속적으로 점프가 있음을 알 수 있다. </span></figcaption>
</figure>
<figure id="A1.F11" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x19.png" id="A1.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F11.3.1.1" style="font-size:90%;">Figure A11</span>:</span><span class="ltx_text" id="A1.F11.4.2" style="font-size:90%;">(Top): Histogram of cosine distance to nearest neighbor in train. 각 빈 내에서 DM_Mathematics (왼쪽), OpenSubtitles (중간) 및 C4 (오른쪽)에 대해 데이터 선택 후 복잡도의 평균 원본 복잡도(중간) 및 평균 차이를 보여준다. 우리는 훈련 세트에 가장 가까운 C4 검증 세트의 포인트가 모두 "쉬운"(아마도 훈련 포인트에 대한 근접성 때문)이고 데이터 선택에 의해 가장 큰 영향을 받는다는 점에 주목한다. 우리는 DM_Mathematics 및 OpenSubtitles과 같은 비웹 스냅샷 유효성 검사 세트에 대한 이러한 추세를 보지 못한다. </span></figcaption>
</figure>
<figure id="A1.F12" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x20.png" id="A1.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F12.3.1.1" style="font-size:90%;">Figure A12</span>:</span><span class="ltx_text" id="A1.F12.4.2" style="font-size:90%;">Distribution of cosine distance to nearest neighbor in the training set, for each individual validation set. </span></figcaption>
</figure>
</section>
<section id="A1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Further investigation of repeating tokens</h3>

<div id="A1.SS6.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS6.p1.1">이 섹션에서는 섹션 <a class="ltx_ref" href="#S4.SS2" title="4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.2</span></a>의 결과가 모델 규모, 데이터 선택 비율(예: 에폭 수) 및 데이터 선택 방법에 걸쳐 유지되는지 여부를 조사한다.</p>
</div>
<div id="A1.SS6.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS6.p2.1.1">Across data selection methods</span>: 먼저 Section <a class="ltx_ref" href="#S4.SS2" title="4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.2</span></a>와 동일한 구성을 취하며, 여기서 우리는 40B 토큰의 시작 소스 데이터세트를 가지고, <math alttext="R=0.25" class="ltx_Math" display="inline" id="A1.SS6.p2.1.m1.1"><semantics id="A1.SS6.p2.1.m1.1a"><mrow id="A1.SS6.p2.1.m1.1.1" xref="A1.SS6.p2.1.m1.1.1.cmml"><mi id="A1.SS6.p2.1.m1.1.1.2" xref="A1.SS6.p2.1.m1.1.1.2.cmml">R</mi><mo id="A1.SS6.p2.1.m1.1.1.1" xref="A1.SS6.p2.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS6.p2.1.m1.1.1.3" xref="A1.SS6.p2.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p2.1.m1.1b"><apply id="A1.SS6.p2.1.m1.1.1.cmml" xref="A1.SS6.p2.1.m1.1.1"><eq id="A1.SS6.p2.1.m1.1.1.1.cmml" xref="A1.SS6.p2.1.m1.1.1.1"></eq><ci id="A1.SS6.p2.1.m1.1.1.2.cmml" xref="A1.SS6.p2.1.m1.1.1.2">𝑅</ci><cn id="A1.SS6.p2.1.m1.1.1.3.cmml" type="float" xref="A1.SS6.p2.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p2.1.m1.1c">R=0.25</annotation></semantics></math>와 함께 각각의 데이터 선택 방법을 사용하여 문서의 서브세트를 선택하고, 40B 토큰의 목표 토큰 버짓에 도달할 때까지 이들 문서를 반복한다. 이것은 1.3B 모델 축척에 있습니다. <a class="ltx_ref" href="#A1.F13" title="Figure A13 ‣ A.6 Further investigation of repeating tokens ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A13</span></a>에서 SemDeDup 및 SSL 프로토타입 모두에 의해 선택된 반복 데이터도 새로운 데이터를 무작위로 선택하는 것보다 우수함을 알 수 있다. 그러나 <span class="ltx_text ltx_font_italic" id="A1.SS6.p2.1.2">fixed</span> 데이터 선택 전략(예: <span class="ltx_text ltx_font_italic" id="A1.SS6.p2.1.3">fixed</span> column in Figure <a class="ltx_ref" href="#A1.F13" title="Figure A13 ‣ A.6 Further investigation of repeating tokens ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A13</span></a>)의 반복 토큰이 새 토큰을 선택하거나 일치된 토큰보다 성능이 뛰어납니다. 즉, 교묘하게 반복되는 토큰은 무작위로 새로운 토큰을 선택하는 것보다 더 나은 성능을 보일 수 있지만 데이터 선택 전략(랜덤, SemDeDup, SSL 프로토타입 또는 D4)을 수정하면 일반적으로 새로운 토큰을 선택하는 것이 바람직하다. 우리는 또한 그림 <a class="ltx_ref" href="#A1.F16" title="Figure A16 ‣ A.6 Further investigation of repeating tokens ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A16</span></a>에서 D4가 고정 계산 체제보다 적은 마진으로 다른 방법보다 우수하다는 점에 주목한다.</p>
</div>
<figure id="A1.F13" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x21.png" id="A1.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F13.4.1.1" style="font-size:90%;">Figure A13</span>:</span><span class="ltx_text" id="A1.F13.5.2" style="font-size:90%;">트레이닝을 통한 데이터 선택 방법에 걸쳐 반복 토큰의 효과. X축은 업데이트 횟수를 나타내고, y축은 웹 스냅숏이 아닌 유효성 검사 세트(맨 위 행) 및 명령 OPT(맨 아래 행)에 걸친 평균 복잡도를 나타냅니다. 위의 그림에서 각 열은 다른 데이터 선택 방법을 나타냅니다. 각 열 내에서: (1) 회색 선은 베이스라인 트레이닝을 나타내고, (2) 착색된 점선은 지정된 데이터 선택 방법을 통해 반복 토큰들을 나타내고, (3) 착색된 실선은 지정된 데이터 선택 방법을 통해 새로운 토큰들을 선택하는 것을 나타낸다. 반복 데이터는 일반적으로 <span class="ltx_text ltx_font_italic" id="A1.F13.5.2.1">fixed data selection method</span> (예: 고정 열)에 대해 새 데이터를 선택하는 것보다 나쁩니다. </span></figcaption>
</figure>
<div id="A1.SS6.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS6.p3.3"><span class="ltx_text ltx_font_bold" id="A1.SS6.p3.3.1">Across model scale and data selection ratio</span>: Section <a class="ltx_ref" href="#S4.SS2" title="4.2 Fixed data regime: what happens when we run out of data? ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">4.2</span></a>에서 설명한 대로 D4로 데이터 선택 전략을 수정하지만 3 모델 scale(125M, 1.3B, 6.7B)과 data selection ratio(<math alttext="R=0.5" class="ltx_Math" display="inline" id="A1.SS6.p3.1.m1.1"><semantics id="A1.SS6.p3.1.m1.1a"><mrow id="A1.SS6.p3.1.m1.1.1" xref="A1.SS6.p3.1.m1.1.1.cmml"><mi id="A1.SS6.p3.1.m1.1.1.2" xref="A1.SS6.p3.1.m1.1.1.2.cmml">R</mi><mo id="A1.SS6.p3.1.m1.1.1.1" xref="A1.SS6.p3.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS6.p3.1.m1.1.1.3" xref="A1.SS6.p3.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p3.1.m1.1b"><apply id="A1.SS6.p3.1.m1.1.1.cmml" xref="A1.SS6.p3.1.m1.1.1"><eq id="A1.SS6.p3.1.m1.1.1.1.cmml" xref="A1.SS6.p3.1.m1.1.1.1"></eq><ci id="A1.SS6.p3.1.m1.1.1.2.cmml" xref="A1.SS6.p3.1.m1.1.1.2">𝑅</ci><cn id="A1.SS6.p3.1.m1.1.1.3.cmml" type="float" xref="A1.SS6.p3.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p3.1.m1.1c">R=0.5</annotation></semantics></math> and <math alttext="R=0.25" class="ltx_Math" display="inline" id="A1.SS6.p3.2.m2.1"><semantics id="A1.SS6.p3.2.m2.1a"><mrow id="A1.SS6.p3.2.m2.1.1" xref="A1.SS6.p3.2.m2.1.1.cmml"><mi id="A1.SS6.p3.2.m2.1.1.2" xref="A1.SS6.p3.2.m2.1.1.2.cmml">R</mi><mo id="A1.SS6.p3.2.m2.1.1.1" xref="A1.SS6.p3.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS6.p3.2.m2.1.1.3" xref="A1.SS6.p3.2.m2.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p3.2.m2.1b"><apply id="A1.SS6.p3.2.m2.1.1.cmml" xref="A1.SS6.p3.2.m2.1.1"><eq id="A1.SS6.p3.2.m2.1.1.1.cmml" xref="A1.SS6.p3.2.m2.1.1.1"></eq><ci id="A1.SS6.p3.2.m2.1.1.2.cmml" xref="A1.SS6.p3.2.m2.1.1.2">𝑅</ci><cn id="A1.SS6.p3.2.m2.1.1.3.cmml" type="float" xref="A1.SS6.p3.2.m2.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p3.2.m2.1c">R=0.25</annotation></semantics></math>)에서 반복 토큰을 시도합니다. <a class="ltx_ref" href="#A1.F15" title="Figure A15 ‣ A.6 Further investigation of repeating tokens ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A15</span></a>에서 D4로 데이터를 반복하면 모든 모델 스케일에서 새로운 토큰을 무작위로 선택하고 <math alttext="R" class="ltx_Math" display="inline" id="A1.SS6.p3.3.m3.1"><semantics id="A1.SS6.p3.3.m3.1a"><mi id="A1.SS6.p3.3.m3.1.1" xref="A1.SS6.p3.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.SS6.p3.3.m3.1b"><ci id="A1.SS6.p3.3.m3.1.1.cmml" xref="A1.SS6.p3.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p3.3.m3.1c">R</annotation></semantics></math>의 선택을 능가한다는 것을 알 수 있다.</p>
</div>
<div id="A1.SS6.p4" class="ltx_para">
<p class="ltx_p" id="A1.SS6.p4.3">우리는 고정된 <math alttext="R" class="ltx_Math" display="inline" id="A1.SS6.p4.1.m1.1"><semantics id="A1.SS6.p4.1.m1.1a"><mi id="A1.SS6.p4.1.m1.1.1" xref="A1.SS6.p4.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.SS6.p4.1.m1.1b"><ci id="A1.SS6.p4.1.m1.1.1.cmml" xref="A1.SS6.p4.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p4.1.m1.1c">R</annotation></semantics></math>에 대해, 상이한 데이터 선택 방법들이 상이한 양의 토큰들을 포함하는 소스 데이터세트의 서브세트들을 선택할 것이라는 점에 주목한다. 이는 상이한 데이터 선택 방법들이 상이한 횟수들을 에포크할 것임을 의미한다. 예를 들어, 1.3B OPT 모델 40B 토큰 버짓 트레이닝 실행에 대해, <math alttext="R=0.25" class="ltx_Math" display="inline" id="A1.SS6.p4.2.m2.1"><semantics id="A1.SS6.p4.2.m2.1a"><mrow id="A1.SS6.p4.2.m2.1.1" xref="A1.SS6.p4.2.m2.1.1.cmml"><mi id="A1.SS6.p4.2.m2.1.1.2" xref="A1.SS6.p4.2.m2.1.1.2.cmml">R</mi><mo id="A1.SS6.p4.2.m2.1.1.1" xref="A1.SS6.p4.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS6.p4.2.m2.1.1.3" xref="A1.SS6.p4.2.m2.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p4.2.m2.1b"><apply id="A1.SS6.p4.2.m2.1.1.cmml" xref="A1.SS6.p4.2.m2.1.1"><eq id="A1.SS6.p4.2.m2.1.1.1.cmml" xref="A1.SS6.p4.2.m2.1.1.1"></eq><ci id="A1.SS6.p4.2.m2.1.1.2.cmml" xref="A1.SS6.p4.2.m2.1.1.2">𝑅</ci><cn id="A1.SS6.p4.2.m2.1.1.3.cmml" type="float" xref="A1.SS6.p4.2.m2.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p4.2.m2.1c">R=0.25</annotation></semantics></math>를 갖는 데이터를 랜덤하게 반복하는 경우, 10B 토큰을 갖는 서브세트를 선택하고, <math alttext="R=0.25" class="ltx_Math" display="inline" id="A1.SS6.p4.3.m3.1"><semantics id="A1.SS6.p4.3.m3.1a"><mrow id="A1.SS6.p4.3.m3.1.1" xref="A1.SS6.p4.3.m3.1.1.cmml"><mi id="A1.SS6.p4.3.m3.1.1.2" xref="A1.SS6.p4.3.m3.1.1.2.cmml">R</mi><mo id="A1.SS6.p4.3.m3.1.1.1" xref="A1.SS6.p4.3.m3.1.1.1.cmml">=</mo><mn id="A1.SS6.p4.3.m3.1.1.3" xref="A1.SS6.p4.3.m3.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS6.p4.3.m3.1b"><apply id="A1.SS6.p4.3.m3.1.1.cmml" xref="A1.SS6.p4.3.m3.1.1"><eq id="A1.SS6.p4.3.m3.1.1.1.cmml" xref="A1.SS6.p4.3.m3.1.1.1"></eq><ci id="A1.SS6.p4.3.m3.1.1.2.cmml" xref="A1.SS6.p4.3.m3.1.1.2">𝑅</ci><cn id="A1.SS6.p4.3.m3.1.1.3.cmml" type="float" xref="A1.SS6.p4.3.m3.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.p4.3.m3.1c">R=0.25</annotation></semantics></math>를 갖는 D4가 15B 토큰을 갖는 서브세트를 선택하면, 랜덤 실행은 4회, D4 실행은 2.67회 에포크될 것이다. 이를 보다 명확하게 보여주기 위해 그림 <a class="ltx_ref" href="#A1.F14" title="Figure A14 ‣ A.6 Further investigation of repeating tokens ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A14</span></a>에서 x축을 epoch 수로 변경한 1.3B 및 6.7B 반복 데이터 실행을 플로팅한다. 우리는 D4를 사용하여 선택된 데이터의 최대 약 2개의 에포크가 무작위로 선택된 새로운 데이터보다 상당히 우수하지만 5개의 에포크에 가까우면 성능이 더 나빠진다는 것을 알 수 있다.</p>
</div>
<figure id="A1.F14" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x22.png" id="A1.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F14.3.1.1" style="font-size:90%;">Figure A14</span>:</span><span class="ltx_text" id="A1.F14.4.2" style="font-size:90%;">Comparison of repeating tokens with D4 (pink line), random select new tokens (horizontal dashed gray line), and random repeating data (gray line). 우리는 서로 다른 시대 번호를 가지고 본다. y축은 복잡도를 나타내고 x축은 에폭의 수를 나타낸다. </span></figcaption>
</figure>
<figure id="A1.F15" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x23.png" id="A1.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F15.3.1.1" style="font-size:90%;">Figure A15</span>:</span><span class="ltx_text" id="A1.F15.4.2" style="font-size:90%;">Comparison of repeating tokens with D4 (pink line), random select new tokens (horizontal dashed gray line), and random repeating data (gray line). 우리는 모델 규모(3B 토큰에 대해 훈련된 상단: 125M, 40B 토큰에 대해 훈련된 중간: 1.3B, 100B 토큰에 대해 훈련된 하단: 6.7B)와 데이터 선택 비율에 걸쳐 D4에 의해 선택된 반복 데이터가 무작위로 새로운 데이터를 선택하는 것보다 성능이 우수하다. </span></figcaption>
</figure>
<figure id="A1.F16" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x24.png" id="A1.F16.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F16.7.3.1" style="font-size:90%;">그림 A16</span>:</span><span class="ltx_text" id="A1.F16.5.4.2" style="font-size:90%;">125M, 3B token budget scale에서 데이터를 반복할 때 비교 데이터 선택 방법. x축은 데이터 선택 비율 <math alttext="R" class="ltx_Math" display="inline" id="A1.F16.4.3.1.m1.1"><semantics id="A1.F16.4.3.1.m1.1b"><mi id="A1.F16.4.3.1.m1.1.1" xref="A1.F16.4.3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.F16.4.3.1.m1.1c"><ci id="A1.F16.4.3.1.m1.1.1.cmml" xref="A1.F16.4.3.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F16.4.3.1.m1.1d">R</annotation></semantics></math>이고, y축은 검증 세트에 대한 평균 복잡도이다. 우리는 D4를 통해 반복할 데이터를 선택하는 것이 특히 낮은 선택 비율 <math alttext="R" class="ltx_Math" display="inline" id="A1.F16.5.4.2.m2.1"><semantics id="A1.F16.5.4.2.m2.1b"><mi id="A1.F16.5.4.2.m2.1.1" xref="A1.F16.5.4.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.F16.5.4.2.m2.1c"><ci id="A1.F16.5.4.2.m2.1.1.cmml" xref="A1.F16.5.4.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F16.5.4.2.m2.1d">R</annotation></semantics></math>에서 다른 데이터 선택 방법보다 우수하다는 것을 관찰한다(고정 데이터 체제에서 낮은 선택 비율은 더 많은 에포크에 해당됨). </span></figcaption>
</figure>
</section>
<section id="A1.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Choice of Embedding Space</h3>

<div id="A1.SS7.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS7.p1.1">우리가 사용하는 모든 데이터 선택 방법은 기본 임베딩 공간의 품질에 크게 의존한다. 마지막 토큰 마지막 계층 OPT 125M 모델에 의해 생성된 임베딩을 정성적으로 분석하고 문서 끝 형식에 대한 편향을 관찰했다. 예를 들어, 문서가 모두 이메일 또는 표준 문구("오늘 제품 구매!")로 끝나는 경우, 이러한 문서는 함께 클러스터링됩니다. 이것은 템플릿을 탐지하는 데 도움이 될 수 있습니다(템플릿은 텍스트를 매우 유사한 방식으로 종료하는 경향이 있기 때문에). 그러나 명백한 함정이 있습니다. 예를 들어 관련 없는 주제에 대한 수천 개의 위키피디아 기사를 취하고 각 기사의 끝에 동일한 이메일을 첨부하면 함께 클러스터링될 수 있습니다.</p>
</div>
<div id="A1.SS7.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS7.p2.1">이에 동기부여된 우리는 서로 다른 임베딩 공간에 대해 간단히 실험하고 이 절에서 우리의 결과에 대해 논의한다.</p>
</div>
<section id="A1.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.7.1 </span>SentenceTransformer models</h4>

<div id="A1.SS7.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS7.SSS1.p1.1">BERT 임베딩은 일반적으로 다양한 NLP 태스크를 수행하는 데 사용되었는데, 이는 BERT(GPT/OPT와 달리)가 임베딩을 생성할 때 입력의 모든 토큰을 처리할 수 있기 때문이다(BERT는 인코더-디코더 모델인 반면 OPT/GPT는 디코더 전용임). 이용 가능한 수많은 BERT 스타일 모델이 있지만 의미적 유사성에 초점을 맞춘 임베딩 공간을 달성하기를 희망했다. 따라서 우리는 BERT 스타일 모델인 <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sbert.net/docs/pretrained_models.html" target="_blank" title="">https://www.sbert.net/docs/pretrained_models.html</a></span></span></span>을 특히 >1B 텍스트 유사성 쌍으로 사용하기로 결정했다. SentenceTransformer 리더보드에서 상위 모델(all-mpnet-base-v2)과 가장 작은 성능 모델(all-Mini-LM-v6)을 선택합니다. 이러한 모델은 256 및 384의 최대 컨텍스트 길이를 가지며(각각), 최대 시퀀스 길이에 맞게 입력을 절단하는 SentenceTransformer 기본값(즉, 이러한 임베딩은 문서의 시작만 고려함)을 고수한다.</p>
</div>
<div id="A1.SS7.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS7.SSS1.p2.1">그림 <a class="ltx_ref" href="#A1.F17" title="Figure A17 ‣ A.7.1 SentenceTransformer models ‣ A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A17</span></a>에서 작은 모델 규모에서 문장 변환기 임베딩 공간이 OPT 임베딩 공간보다 우수함을 관찰한다. 이러한 초기 결과를 감안할 때, 우리는 1.3b 모델 규모("all-mini-lm-v6")에서 가장 전반적인 효율적인 임베딩 공간을 취하고 6.7b 트레이닝 실행을 실행했다. 놀랍게도, 더 큰 모델 규모에서 OPT 임베딩 공간이 "all-mini-LM-v6" 임베딩 공간보다 우수하다는 것을 관찰했다. "all-mini-LM-v6"과 "all-mp-net-base-v2"의 차이가 일반적으로 작다는 점을 감안할 때(그림 <a class="ltx_ref" href="#A1.F17" title="Figure A17 ‣ A.7.1 SentenceTransformer models ‣ A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A17</span></a> 참조), 계산 제한으로 인해 이 실행을 완료할 수 없었지만 OPT 임베딩 공간이 6.7b에서 "all-mpnet-base-v2"를 이길 것으로 예상한다. <a class="ltx_ref" href="#A1.F18" title="Figure A18 ‣ A.7.1 SentenceTransformer models ‣ A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A18</span></a>에서 임베딩 공간이 다른 D4를 사용하는 전체적이고 순진한 효율성을 고려할 때 동일한 경향을 볼 수 있다.</p>
</div>
<div id="A1.SS7.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS7.SSS1.p3.1">SentenceTransformer 임베딩 공간이 더 큰 모델 스케일에서 더 나쁜 성능을 보이는 이유를 이해하기 위해, 우리는 각 SentenceTransformer 임베딩 공간과의 클러스터링을 정성적으로 분석한다. 우리는 "all-mp-net-base-v2" 및 "all-mini-lm-v6"과 함께 D4를 사용하면 긴 문서를 불균형적으로 자른다. 이는 실제 문장 쌍에 대해 문장 변환기 모델이 훈련되고 미세 조정되기 때문이며, 이는 모델의 최대 문맥 길이를 거의 포화시키지 않기 때문이라고 가정한다. 이렇게 하면 모든 "긴" 문서(또는 최소 최대 컨텍스트 길이 크기인 입력)가 모형에 분산되지 않은 것처럼 보일 수 있습니다. 이로 인해 긴 문서가 함께 클러스터링되어 가지치기 중에 불균형적으로 영향을 받는다고 추측한다. 이것은 위키피디아 기사와 같은 도메인에서 특히 관련이 있을 수 있으며, 여기서 헤더와 소개는 의미적으로 유사하지만 실제 콘텐츠(첫 번째 최대 컨텍스트 길이 토큰 이전)는 매우 다르다.</p>
</div>
<div id="A1.SS7.SSS1.p4" class="ltx_para">
<p class="ltx_p" id="A1.SS7.SSS1.p4.1">이 문제를 우회하기 위해 우리는 작은 모델 척도에서 두 가지 접근법을 시도했다.</p>
</div>
<div id="A1.SS7.SSS1.p5" class="ltx_para">
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i1.p1.1">M1: 긴 문서들을 최대-컨텍스트-길이 청크들로 청크화하고, 청크들에 걸쳐 전체-미니-LM-v6 임베딩들을 평균화하여 최종 문서 임베딩을 생성한다.</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i2.p1.1">M2: Contriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite> embeddings를 사용하며, 여기서 Contriever 모델을 선택한 이유는 두 문장이 동일한 문서로부터 왔는지를 결정하도록 훈련되었기 때문이며, 따라서 문서 내의 위치에 대해 불가지론적이어야 하기 때문이다.</p>
</div>
</li>
</ul>
</div>
<div id="A1.SS7.SSS1.p6" class="ltx_para">
<p class="ltx_p" id="A1.SS7.SSS1.p6.1">훈련이 끝날 때의 복잡도 개선(그림<a class="ltx_ref" href="#A1.F19" title="Figure A19 ‣ A.7.1 SentenceTransformer models ‣ A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A19</span></a> 참조)과 효율성(그림<a class="ltx_ref" href="#A1.F18" title="Figure A18 ‣ A.7.1 SentenceTransformer models ‣ A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A18</span></a> 참조) 측면에서 우리는 작은 모델 규모(1억 2,500만 파라미터)에서 OPT 임베딩 공간과 임베딩 공간 M1과 M2 사이에 유의미한 차이를 관찰하지 못한다. 우리는 M1과 M2가 작은 규모 <span class="ltx_text ltx_font_bold" id="A1.SS7.SSS1.p6.1.1"> 및</span>에서 all-mp-net-base-v2 및 all-mini-LM-v6보다 훨씬 더 나쁘다는 점에 주목한다.</p>
</div>
<figure id="A1.F17" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x25.png" id="A1.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="541" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F17.5.2.1" style="font-size:90%;">그림 A17</span>:</span><span class="ltx_text" id="A1.F17.3.2.1" style="font-size:90%;">Perplexity (y-axis) versus selection ratio <math alttext="R" class="ltx_Math" display="inline" id="A1.F17.3.2.1.m1.1"><semantics id="A1.F17.3.2.1.m1.1b"><mi id="A1.F17.3.2.1.m1.1.1" xref="A1.F17.3.2.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.F17.3.2.1.m1.1c"><ci id="A1.F17.3.2.1.m1.1.1.cmml" xref="A1.F17.3.2.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F17.3.2.1.m1.1d">R</annotation></semantics></math> (x-axis) for different embedding space, when selecting data via D4. across different 8m (top), 125m (middle) and 1.3b (bottom) model scale, we see the SentenceTransformer embedding spaces is outperformer embedding space is outperformer in the OPT embedding space but the 6.7b model scale, we see the OPT embedding space begins outperformer in all Mini LM v6 embedding space. 계산 제한으로 인해 "all-mp-net-base-v2" 6.7b 실험을 실행할 수 없었지만 모델 규모와 선택 비율에 따라 "all-mini-lm-v6"과 "all-mp-net-base-v2"의 차이가 일반적으로 작으므로 OPT 임베딩 공간이 6.7b 규모에서 "all-mp-net-base-v2"를 능가할 것으로 예상한다. </span></figcaption>
</figure>
<figure id="A1.F18" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x26.png" id="A1.F18.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F18.3.1.1" style="font-size:90%;">그림 A18</span>:</span><span class="ltx_text" id="A1.F18.4.2" style="font-size:90%;">D4를 데이터 선택 전략으로 사용할 때, 다른 임베딩 공간에 대한 나이브 효율성의 비교. 그림 <a class="ltx_ref" href="#A1.F17" title="Figure A17 ‣ A.7.1 SentenceTransformer models ‣ A.7 Choice of Embedding Space ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A17</span></a>와 유사하게, 우리는 모든 미니-LM-v6이 작은 규모에서는 OPT 임베딩 공간을 능가하지만 큰(6.7b) 모델 규모에서는 그렇지 않음을 알 수 있다. </span></figcaption>
</figure>
<figure id="A1.F19" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x27.png" id="A1.F19.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F19.3.1.1" style="font-size:90%;">그림 A19</span>:</span><span class="ltx_text" id="A1.F19.4.2" style="font-size:90%;">임베딩 공간 비교 M1 (averaging embedding of all-mini-LM-v6 across all chunks in a document, where a chunk is defined as 256 tokens) and M2 (embeddings from the Contriever model), with the OPT model embedding space, using the selection strategy. 우리는 어느 임베딩 공간도 125M 스케일에서 OPT 모델 임베딩 공간을 크게 능가하지 않는다는 점에 주목한다. </span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A1.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.8 </span>Replicating Fixed Compute Results on C4</h3>

<div id="A1.SS8.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS8.p1.1">이 섹션에서는 사전 훈련 데이터 세트가 CC-dedup 대신 C4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite> 데이터 세트인 125M 규모에서 데이터 선택 방법을 비교하기 위한 결과를 간략하게 보여준다. <a class="ltx_ref" href="#A1.F20" title="Figure A20 ‣ A.8 Replicating Fixed Compute Results on C4 ‣ Appendix A Appendix ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">A20</span></a>에서 D4가 다른 방법보다 일반적으로 성능이 우수함을 알 수 있다. 이러한 초기 실험들은 더 많이 필터링된 웹 데이터(즉, CC-dedup)에서 데이터 선택 방법을 비교하도록 동기를 부여한다.</p>
</div>
<figure id="A1.F20" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2308.12284/assets/x28.png" id="A1.F20.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F20.10.4.1" style="font-size:90%;">Figure A20</span>:</span><span class="ltx_text" id="A1.F20.7.6.3" style="font-size:90%;">Comparison of data selection strategies with the OPT model embedding space, when using D4 as the selection strategy, when using C4 as the starting training dataset. x축은 selectoin ratio <math alttext="R" class="ltx_Math" display="inline" id="A1.F20.5.4.1.m1.1"><semantics id="A1.F20.5.4.1.m1.1b"><mi id="A1.F20.5.4.1.m1.1.1" xref="A1.F20.5.4.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A1.F20.5.4.1.m1.1c"><ci id="A1.F20.5.4.1.m1.1.1.cmml" xref="A1.F20.5.4.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F20.5.4.1.m1.1d">R</annotation></semantics></math>이고, y축은 baseline 대비 perplexity 차이(0.0에서의 수평 회색 점선은 우리의 baseline을 나타낸다. 즉, 데이터 선택이 이루어지지 않은 경우)이므로 <span class="ltx_text ltx_font_bold" id="A1.F20.7.6.3.1">lower is better</span>이다. 이 실험을 위해 <math alttext="R_{dedup}=0.9" class="ltx_Math" display="inline" id="A1.F20.6.5.2.m2.1"><semantics id="A1.F20.6.5.2.m2.1b"><mrow id="A1.F20.6.5.2.m2.1.1" xref="A1.F20.6.5.2.m2.1.1.cmml"><msub id="A1.F20.6.5.2.m2.1.1.2" xref="A1.F20.6.5.2.m2.1.1.2.cmml"><mi id="A1.F20.6.5.2.m2.1.1.2.2" xref="A1.F20.6.5.2.m2.1.1.2.2.cmml">R</mi><mrow id="A1.F20.6.5.2.m2.1.1.2.3" xref="A1.F20.6.5.2.m2.1.1.2.3.cmml"><mi id="A1.F20.6.5.2.m2.1.1.2.3.2" xref="A1.F20.6.5.2.m2.1.1.2.3.2.cmml">d</mi><mo id="A1.F20.6.5.2.m2.1.1.2.3.1" lspace="0em" rspace="0em" xref="A1.F20.6.5.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.F20.6.5.2.m2.1.1.2.3.3" xref="A1.F20.6.5.2.m2.1.1.2.3.3.cmml">e</mi><mo id="A1.F20.6.5.2.m2.1.1.2.3.1b" lspace="0em" rspace="0em" xref="A1.F20.6.5.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.F20.6.5.2.m2.1.1.2.3.4" xref="A1.F20.6.5.2.m2.1.1.2.3.4.cmml">d</mi><mo id="A1.F20.6.5.2.m2.1.1.2.3.1c" lspace="0em" rspace="0em" xref="A1.F20.6.5.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.F20.6.5.2.m2.1.1.2.3.5" xref="A1.F20.6.5.2.m2.1.1.2.3.5.cmml">u</mi><mo id="A1.F20.6.5.2.m2.1.1.2.3.1d" lspace="0em" rspace="0em" xref="A1.F20.6.5.2.m2.1.1.2.3.1.cmml">​</mo><mi id="A1.F20.6.5.2.m2.1.1.2.3.6" xref="A1.F20.6.5.2.m2.1.1.2.3.6.cmml">p</mi></mrow></msub><mo id="A1.F20.6.5.2.m2.1.1.1" xref="A1.F20.6.5.2.m2.1.1.1.cmml">=</mo><mn id="A1.F20.6.5.2.m2.1.1.3" xref="A1.F20.6.5.2.m2.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F20.6.5.2.m2.1c"><apply id="A1.F20.6.5.2.m2.1.1.cmml" xref="A1.F20.6.5.2.m2.1.1"><eq id="A1.F20.6.5.2.m2.1.1.1.cmml" xref="A1.F20.6.5.2.m2.1.1.1"></eq><apply id="A1.F20.6.5.2.m2.1.1.2.cmml" xref="A1.F20.6.5.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.F20.6.5.2.m2.1.1.2.1.cmml" xref="A1.F20.6.5.2.m2.1.1.2">subscript</csymbol><ci id="A1.F20.6.5.2.m2.1.1.2.2.cmml" xref="A1.F20.6.5.2.m2.1.1.2.2">𝑅</ci><apply id="A1.F20.6.5.2.m2.1.1.2.3.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3"><times id="A1.F20.6.5.2.m2.1.1.2.3.1.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3.1"></times><ci id="A1.F20.6.5.2.m2.1.1.2.3.2.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3.2">𝑑</ci><ci id="A1.F20.6.5.2.m2.1.1.2.3.3.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3.3">𝑒</ci><ci id="A1.F20.6.5.2.m2.1.1.2.3.4.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3.4">𝑑</ci><ci id="A1.F20.6.5.2.m2.1.1.2.3.5.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3.5">𝑢</ci><ci id="A1.F20.6.5.2.m2.1.1.2.3.6.cmml" xref="A1.F20.6.5.2.m2.1.1.2.3.6">𝑝</ci></apply></apply><cn id="A1.F20.6.5.2.m2.1.1.3.cmml" type="float" xref="A1.F20.6.5.2.m2.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F20.6.5.2.m2.1d">R_{dedup}=0.9</annotation></semantics></math>를 사용하고 <math alttext="R_{proto}" class="ltx_Math" display="inline" id="A1.F20.7.6.3.m3.1"><semantics id="A1.F20.7.6.3.m3.1b"><msub id="A1.F20.7.6.3.m3.1.1" xref="A1.F20.7.6.3.m3.1.1.cmml"><mi id="A1.F20.7.6.3.m3.1.1.2" xref="A1.F20.7.6.3.m3.1.1.2.cmml">R</mi><mrow id="A1.F20.7.6.3.m3.1.1.3" xref="A1.F20.7.6.3.m3.1.1.3.cmml"><mi id="A1.F20.7.6.3.m3.1.1.3.2" xref="A1.F20.7.6.3.m3.1.1.3.2.cmml">p</mi><mo id="A1.F20.7.6.3.m3.1.1.3.1" lspace="0em" rspace="0em" xref="A1.F20.7.6.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.F20.7.6.3.m3.1.1.3.3" xref="A1.F20.7.6.3.m3.1.1.3.3.cmml">r</mi><mo id="A1.F20.7.6.3.m3.1.1.3.1b" lspace="0em" rspace="0em" xref="A1.F20.7.6.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.F20.7.6.3.m3.1.1.3.4" xref="A1.F20.7.6.3.m3.1.1.3.4.cmml">o</mi><mo id="A1.F20.7.6.3.m3.1.1.3.1c" lspace="0em" rspace="0em" xref="A1.F20.7.6.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.F20.7.6.3.m3.1.1.3.5" xref="A1.F20.7.6.3.m3.1.1.3.5.cmml">t</mi><mo id="A1.F20.7.6.3.m3.1.1.3.1d" lspace="0em" rspace="0em" xref="A1.F20.7.6.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.F20.7.6.3.m3.1.1.3.6" xref="A1.F20.7.6.3.m3.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.F20.7.6.3.m3.1c"><apply id="A1.F20.7.6.3.m3.1.1.cmml" xref="A1.F20.7.6.3.m3.1.1"><csymbol cd="ambiguous" id="A1.F20.7.6.3.m3.1.1.1.cmml" xref="A1.F20.7.6.3.m3.1.1">subscript</csymbol><ci id="A1.F20.7.6.3.m3.1.1.2.cmml" xref="A1.F20.7.6.3.m3.1.1.2">𝑅</ci><apply id="A1.F20.7.6.3.m3.1.1.3.cmml" xref="A1.F20.7.6.3.m3.1.1.3"><times id="A1.F20.7.6.3.m3.1.1.3.1.cmml" xref="A1.F20.7.6.3.m3.1.1.3.1"></times><ci id="A1.F20.7.6.3.m3.1.1.3.2.cmml" xref="A1.F20.7.6.3.m3.1.1.3.2">𝑝</ci><ci id="A1.F20.7.6.3.m3.1.1.3.3.cmml" xref="A1.F20.7.6.3.m3.1.1.3.3">𝑟</ci><ci id="A1.F20.7.6.3.m3.1.1.3.4.cmml" xref="A1.F20.7.6.3.m3.1.1.3.4">𝑜</ci><ci id="A1.F20.7.6.3.m3.1.1.3.5.cmml" xref="A1.F20.7.6.3.m3.1.1.3.5">𝑡</ci><ci id="A1.F20.7.6.3.m3.1.1.3.6.cmml" xref="A1.F20.7.6.3.m3.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F20.7.6.3.m3.1d">R_{proto}</annotation></semantics></math>를 다양화하기 때문에 D4와 SemDeDup가 90%로 일치한다는 점에 주목하라. </span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A1.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.9 </span>Investigating Duplicate-Driven Clusters</h3>

<div id="A1.SS9.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS9.p1.1">이 하위 섹션에서는 매우 조밀하고 중심에 가까운 군집인 중복 구동 군집의 몇 가지 예를 제시한다. 우리는 이러한 클러스터들이 의미적 중복 및/또는 중복된 텍스트로 채워지는 경향이 있음을 발견한다. 우리는 일반적으로 클러스터 중심까지의 코사인 거리의 표준 편차가 0.03 미만인 클러스터들을 살펴봄으로써 그러한 극단적인 중복 구동 클러스터들을 발견할 수 있다. 이것은 본질적으로 그림 <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‣ 4.4.2 Importance of re-clustering between SemDeDup and SSL Prototypes ‣ 4.4 Analysis of D4 ‣ 4 Results ‣ D4: Improving LLM Pretraining via Document De-Duplication and Diversification"><span class="ltx_text ltx_ref_tag">7</span></a> (갈색 선)에서 경험적 CDF의 하부 꼬리에 있는 클러스터들을 살펴보는 것이다. 다음과 같은 클러스터의 몇 가지 예를 제시합니다.</p>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T3.2.1.1" style="font-size:90%;">Table A3</span>:</span><span class="ltx_text" id="A1.T3.3.2" style="font-size:90%;">Nearest Neighbors to Cluster Centroid 682</span></figcaption>
<table id="A1.T3.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T3.4.1" class="ltx_tr">
<td id="A1.T3.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T3.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance to Centroid</span></td>
<td id="A1.T3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T3.4.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A1.T3.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T3.4.2" class="ltx_tr">
<td id="A1.T3.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.03581655</td>
<td id="A1.T3.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T3.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T3.4.2.2.1.1" class="ltx_p" style="width:284.5pt;">The USGS (U.S. Geological Survey) publishes a set of the most commonly used topographic maps of the U.S. called US ……… may have differences in elevation and topography, the historic weather at the two separate locations may be different as well.</span>
</span>
</td>
</tr>
<tr id="A1.T3.4.3" class="ltx_tr">
<td id="A1.T3.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.03584063</td>
<td id="A1.T3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T3.4.3.2.1.1" class="ltx_p" style="width:284.5pt;">The USGS (U.S. Geological Survey) publishes a set of the most commonly used topographic maps of the U.S. called US ……… may have differences in elevation and topography, the historic weather at the two separate locations may be different as well.</span>
</span>
</td>
</tr>
<tr id="A1.T3.4.4" class="ltx_tr">
<td id="A1.T3.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.036803484</td>
<td id="A1.T3.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T3.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T3.4.4.2.1.1" class="ltx_p" style="width:284.5pt;">The USGS (U.S. Geological Survey) publishes a set of the most commonly used topographic maps of the U.S. called US ……… may have differences in elevation and topography, the historic weather at the two separate locations may be different as well.</span>
</span>
</td>
</tr>
<tr id="A1.T3.4.5" class="ltx_tr">
<td id="A1.T3.4.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.037270606</td>
<td id="A1.T3.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T3.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T3.4.5.2.1.1" class="ltx_p" style="width:284.5pt;">Search Near Clinton County, OH: Trails National and State Parks City Parks Lakes Lookouts Marinas Historical Sites
The USGS (U.S. Geological ……… may have differences in elevation and topography, the historic weather at the two separate locations may be different as well.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T4.2.1.1" style="font-size:90%;">Table A4</span>:</span><span class="ltx_text" id="A1.T4.3.2" style="font-size:90%;">Nearest Neighbors to Cluster Centroid 975</span></figcaption>
<table id="A1.T4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T4.4.1" class="ltx_tr">
<td id="A1.T4.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T4.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance to Centroid</span></td>
<td id="A1.T4.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T4.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T4.4.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A1.T4.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T4.4.2" class="ltx_tr">
<td id="A1.T4.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.011662006</td>
<td id="A1.T4.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T4.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T4.4.2.2.1.1" class="ltx_p" style="width:284.5pt;">The American Way, Inc.
The American Way, Inc. is a suspended Californian business entity incorporated 19th August 1949. is listed as ……… for bulk data downloadsI want to request the removal of a page on your websiteI want to contact California Explore</span>
</span>
</td>
</tr>
<tr id="A1.T4.4.3" class="ltx_tr">
<td id="A1.T4.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.012483656</td>
<td id="A1.T4.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T4.4.3.2.1.1" class="ltx_p" style="width:284.5pt;">John St-Amour, Inc.
John St-Amour, Inc. is a suspended Californian business entity incorporated 5th October 1962. is listed as the agent ……… for bulk data downloadsI want to request the removal of a page on your websiteI want to contact California Explore</span>
</span>
</td>
</tr>
<tr id="A1.T4.4.4" class="ltx_tr">
<td id="A1.T4.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.012564898</td>
<td id="A1.T4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T4.4.4.2.1.1" class="ltx_p" style="width:284.5pt;">Joseph E. Barbour, Inc.
Joseph E. Barbour, Inc. is a suspended Californian business entity incorporated 27th January 1959. is listed as ……… for bulk data downloadsI want to request the removal of a page on your websiteI want to contact California Explore</span>
</span>
</td>
</tr>
<tr id="A1.T4.4.5" class="ltx_tr">
<td id="A1.T4.4.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.012756169</td>
<td id="A1.T4.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T4.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T4.4.5.2.1.1" class="ltx_p" style="width:284.5pt;">The Jolly Boys, Inc.
The Jolly Boys, Inc. is a suspended Californian business entity incorporated 4th March 1955. is listed as ……… for bulk data downloadsI want to request the removal of a page on your websiteI want to contact California Explore</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T5.2.1.1" style="font-size:90%;">Table A5</span>:</span><span class="ltx_text" id="A1.T5.3.2" style="font-size:90%;">Nearest Neighbors to Cluster Centroid 10715</span></figcaption>
<table id="A1.T5.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T5.4.1" class="ltx_tr">
<td id="A1.T5.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T5.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance to Centroid</span></td>
<td id="A1.T5.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T5.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A1.T5.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T5.4.2" class="ltx_tr">
<td id="A1.T5.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.035506427</td>
<td id="A1.T5.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T5.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.2.2.1.1" class="ltx_p" style="width:284.5pt;">Search hundreds of travel sites at once for hotel deals at Hotel Olympic
Kornarou Square 44, Heraklion, Greece
34 m Bembo Fountain
262 ……… hundreds of travel sites to help you find and book the hotel deal at Hotel Olympic that suits you best.</span>
</span>
</td>
</tr>
<tr id="A1.T5.4.3" class="ltx_tr">
<td id="A1.T5.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.036230028</td>
<td id="A1.T5.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T5.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.3.2.1.1" class="ltx_p" style="width:284.5pt;">Search hundreds of travel sites at once for hotel deals at Hotel Estrella del Norte
Juan Hormaechea, s/n, 39195 Isla, Cantabria, ……… travel sites to help you find and book the hotel deal at Hotel Estrella del Norte that suits you best.</span>
</span>
</td>
</tr>
<tr id="A1.T5.4.4" class="ltx_tr">
<td id="A1.T5.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.036280274</td>
<td id="A1.T5.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T5.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.4.2.1.1" class="ltx_p" style="width:284.5pt;">Search hundreds of travel sites at once for hotel deals at H10 Costa Adeje Palace
Provided by H10 Costa Adeje Palace
Provided ……… travel sites to help you find and book the hotel deal at H10 Costa Adeje Palace that suits you best.</span>
</span>
</td>
</tr>
<tr id="A1.T5.4.5" class="ltx_tr">
<td id="A1.T5.4.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.036827266</td>
<td id="A1.T5.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T5.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.5.2.1.1" class="ltx_p" style="width:284.5pt;">Search hundreds of travel sites at once for hotel deals at Hotel Miguel Angel by BlueBay
Calle Miguel Angel 29-31, 28010 ……… sites to help you find and book the hotel deal at Hotel Miguel Angel by BlueBay that suits you best.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T6.2.1.1" style="font-size:90%;">Table A6</span>:</span><span class="ltx_text" id="A1.T6.3.2" style="font-size:90%;">Random examples from Cluster 695</span></figcaption>
<table id="A1.T6.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T6.4.1" class="ltx_tr">
<td id="A1.T6.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T6.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance to Cluster Centroid</span></td>
<td id="A1.T6.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T6.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.1.2.1.1" class="ltx_p" style="width:256.1pt;"><span id="A1.T6.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.4.2" class="ltx_tr">
<td id="A1.T6.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.044178426</td>
<td id="A1.T6.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.2.2.1.1" class="ltx_p" style="width:256.1pt;">Eastern Florida State College nutritional sciences
Learn about Eastern Florida State College nutritional sciences, and registering for electives. Which college degrees ……… System (IPEDS). If any stats on Hagerstown Community College career planning are incorrect, please contact us with the right data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.4.3" class="ltx_tr">
<td id="A1.T6.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.056984067</td>
<td id="A1.T6.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.3.2.1.1" class="ltx_p" style="width:256.1pt;">Albany State University introduction to business
Find info concerning Albany State University introduction to business, and registering for elective discussion sections ……… If any stats on Warren County Community College plant science major are incorrect, please contact us with the right data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.4.4" class="ltx_tr">
<td id="A1.T6.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.0534693</td>
<td id="A1.T6.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.4.2.1.1" class="ltx_p" style="width:256.1pt;">Baldwin Wallace University cost per unit
Learn about Baldwin Wallace University cost per unit, submitting required application forms, and follow-up scheduling. ……… (IPEDS). If any stats on San Jose State nursing degree programs are incorrect, please contact us with the right data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.4.5" class="ltx_tr">
<td id="A1.T6.4.5.1" class="ltx_td ltx_align_left ltx_border_t">0.06892538</td>
<td id="A1.T6.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.5.2.1.1" class="ltx_p" style="width:256.1pt;">Niagara University managerial accounting
Information about Niagara University managerial accounting, and registering for elective lectures. Which college degrees give you the ……… System (IPEDS). If any stats on Midwestern University pharmacy tech program are incorrect, please contact us with the right data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.4.6" class="ltx_tr">
<td id="A1.T6.4.6.1" class="ltx_td ltx_align_left ltx_border_t">0.07246786</td>
<td id="A1.T6.4.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.4.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.6.2.1.1" class="ltx_p" style="width:256.1pt;">Fanshawe College app download
Learn about Fanshawe College app download, and registering for elective discussion sections and seminars. Which college degrees ……… Data System (IPEDS). If any stats on Stratford University cell biology are incorrect, please contact us with the right data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.4.7" class="ltx_tr">
<td id="A1.T6.4.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.07147932</td>
<td id="A1.T6.4.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T6.4.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.4.7.2.1.1" class="ltx_p" style="width:256.1pt;">Standish Maine Licensed Vocational Nurse LVN Jobs
Find out about Standish, ME licensed vocational nurse LVN jobs options. It’s a smart ……… (IPEDS). If any stats on William Jewell College medical insurance coding are incorrect, please contact us with the right data.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T7.2.1.1" style="font-size:90%;">Table A7</span>:</span><span class="ltx_text" id="A1.T7.3.2" style="font-size:90%;">Random examples from Cluster 8342</span></figcaption>
<table id="A1.T7.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T7.4.1" class="ltx_tr">
<td id="A1.T7.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T7.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance to Cluster Centroid</span></td>
<td id="A1.T7.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.1.2.1.1" class="ltx_p" style="width:256.1pt;"><span id="A1.T7.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T7.4.2" class="ltx_tr">
<td id="A1.T7.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.027729392</td>
<td id="A1.T7.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.2.2.1.1" class="ltx_p" style="width:256.1pt;">Seenti - Bundi
Seenti Population - Bundi, Rajasthan
Seenti is a medium size village located in Bundi Tehsil of Bundi district, Rajasthan ……… 6 months. Of 186 workers engaged in Main Work, 63 were cultivators (owner or co-owner) while 0 were Agricultural labourer.</span>
</span>
</td>
</tr>
<tr id="A1.T7.4.3" class="ltx_tr">
<td id="A1.T7.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.036407113</td>
<td id="A1.T7.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.3.2.1.1" class="ltx_p" style="width:256.1pt;">Kodunaickenpatty pudur - Salem
Kodunaickenpatty pudur Population - Salem, Tamil Nadu
Kodunaickenpatty pudur is a large village located in Omalur Taluka of ……… 6 months. Of 3523 workers engaged in Main Work, 1500 were cultivators (owner or co-owner) while 1533 were Agricultural labourer.</span>
</span>
</td>
</tr>
<tr id="A1.T7.4.4" class="ltx_tr">
<td id="A1.T7.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.017463684</td>
<td id="A1.T7.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.4.2.1.1" class="ltx_p" style="width:256.1pt;">Chhotepur - Gurdaspur
Chhotepur Population - Gurdaspur, Punjab
Chhotepur is a medium size village located in Gurdaspur Tehsil of Gurdaspur district, Punjab ……… 6 months. Of 677 workers engaged in Main Work, 123 were cultivators (owner or co-owner) while 142 were Agricultural labourer.</span>
</span>
</td>
</tr>
<tr id="A1.T7.4.5" class="ltx_tr">
<td id="A1.T7.4.5.1" class="ltx_td ltx_align_left ltx_border_t">0.02616191</td>
<td id="A1.T7.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.5.2.1.1" class="ltx_p" style="width:256.1pt;">Maksudanpur - Azamgarh
Maksudanpur Population - Azamgarh, Uttar Pradesh
Maksudanpur is a small village located in Sagri Tehsil of Azamgarh district, Uttar ……… 6 months. Of 22 workers engaged in Main Work, 14 were cultivators (owner or co-owner) while 0 were Agricultural labourer.</span>
</span>
</td>
</tr>
<tr id="A1.T7.4.6" class="ltx_tr">
<td id="A1.T7.4.6.1" class="ltx_td ltx_align_left ltx_border_t">0.028420448</td>
<td id="A1.T7.4.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.6.2.1.1" class="ltx_p" style="width:256.1pt;">Karambavane - Ratnagiri
Karambavane Population - Ratnagiri, Maharashtra
Karambavane is a medium size village located in Chiplun Taluka of Ratnagiri district, Maharashtra ……… 6 months. Of 444 workers engaged in Main Work, 116 were cultivators (owner or co-owner) while 214 were Agricultural labourer.</span>
</span>
</td>
</tr>
<tr id="A1.T7.4.7" class="ltx_tr">
<td id="A1.T7.4.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.037917078</td>
<td id="A1.T7.4.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T7.4.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.7.2.1.1" class="ltx_p" style="width:256.1pt;">Barda - Purba Medinipur
Barda Population - Purba Medinipur, West Bengal
Barda is a large village located in Egra - I Block ……… 6 months. Of 1182 workers engaged in Main Work, 278 were cultivators (owner or co-owner) while 252 were Agricultural labourer.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="A1.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T8.2.1.1" style="font-size:90%;">Table A8</span>:</span><span class="ltx_text" id="A1.T8.3.2" style="font-size:90%;">Nearest Neighbors to random validation point in C4</span></figcaption>
<table id="A1.T8.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T8.4.1" class="ltx_tr">
<td id="A1.T8.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T8.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance</span></td>
<td id="A1.T8.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A1.T8.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T8.4.2" class="ltx_tr">
<td id="A1.T8.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.0(original validation text)</td>
<td id="A1.T8.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.2.2.1.1" class="ltx_p" style="width:284.5pt;">Offers two child care opportunities to Charles County citizens— the Port Tobacco Onsite Child Care Program and the Before and After School Child Care Program (BASCC).
Supports parents through home visits to first time parents and by helping them search for child care, find resources for a child with social, emotional . . . . . . . . Special needs kids. Free to look, a fee to contact the providers.
Hotline is staffed by highly-trained and friendly Child Care Consumer Education Specialists who offer both parents and providers invaluable information about child care, and referrals to local Child Care Resource and Referral agencies where they can receive individualized assistance.</span>
</span>
</td>
</tr>
<tr id="A1.T8.4.3" class="ltx_tr">
<td id="A1.T8.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.12867724895477295</td>
<td id="A1.T8.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.3.2.1.1" class="ltx_p" style="width:284.5pt;">Child Care Options is a program of Options Community Services , a non-profit registered charity dedicated to making a difference in the South Fraser Region. Options is committed to empowering individuals, supporting families and promoting community health. Funding for Child Care Options is provided through British Columbia’s Ministry of Children . . . . . . . . Rock.
Child Care Options links families and child care providers in the communities of Delta, Surrey and White Rock by offering free consultation, support and child care referral services and subsidy support to parents seeking child care. Child care providers are supported through information, outreach, resource library, networking, and learning opportunities.</span>
</span>
</td>
</tr>
<tr id="A1.T8.4.4" class="ltx_tr">
<td id="A1.T8.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.15080827474594116</td>
<td id="A1.T8.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.4.2.1.1" class="ltx_p" style="width:284.5pt;">Below are links to child development resources, both from within the department and from external sources.
Child Development Division Publications
Publications that can help you will help you follow your child’s development (from birth to age five) so you can identify and address any issues early on.
Resources to help you understand children’s . . . . . . . . families to local resources and services. Specialists are available from 9 AM to 6 PM Monday – Friday. Services are confidential. Caregivers can also visit http://www.helpmegrowvt.org/families.html to learn more about child development, discover developmental tips, and watch videos demonstrating children’s developmental milestones (click a button to choose your child’s age).</span>
</span>
</td>
</tr>
<tr id="A1.T8.4.5" class="ltx_tr">
<td id="A1.T8.4.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.15738284587860107</td>
<td id="A1.T8.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T8.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.5.2.1.1" class="ltx_p" style="width:284.5pt;">National Domestic Violence Hotlines
Programs that provide immediate assistance for women and men who have experienced domestic abuse which may include steps to ensure the person’s safety; short-term emotional support; assistance with shelter; legal information and advocacy; referrals for medical treatment; ongoing counseling and/or group support; and other related services. Hotline . . . . . . . . RP-1500.1400-200)
www.thehotline.org/
Toll Free Phone: 800-799-SAFE
URL: https://www.thehotline.org/
Eligibility: Anyone affected by relationship abuse.
Services Provided: Available 24/7/365 via phone, TTY, and chat. Provides lifesaving tools and immediate support to enable victims to find safety and live lives free of abuse. Highly trained, experienced advocates offer support, crisis intervention, education, safety planning, and referral services.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="A1.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T9.2.1.1" style="font-size:90%;">Table A9</span>:</span><span class="ltx_text" id="A1.T9.3.2" style="font-size:90%;">Nearest Neighbors to random validation point in USPTO</span></figcaption>
<table id="A1.T9.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A1.T9.4.1" class="ltx_tr">
<td id="A1.T9.4.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T9.4.1.1.1" class="ltx_text ltx_font_bold">Cosine Distance</span></td>
<td id="A1.T9.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T9.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.4.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A1.T9.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Raw Text</span></span>
</span>
</td>
</tr>
<tr id="A1.T9.4.2" class="ltx_tr">
<td id="A1.T9.4.2.1" class="ltx_td ltx_align_left ltx_border_t">0.0(original validation text)</td>
<td id="A1.T9.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T9.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.4.2.2.1.1" class="ltx_p" style="width:284.5pt;">SONET (Synchronous Optical NETwork) is a North American transmission standard for optical communication systems. SDH (Synchronous Digital Hierarchy), a European transmission standard, is a minor variant of SONET.
SONET defines a hierarchy of electrical signals referred to as Synchronous Transport Signals (STS). The STS hierarchy is built upon a basic signal . . . . . . . . the corresponding row and column numbers may include up to 18 comparison operations, which are onerous to implement, for example, in terms of the required logic circuitry. This problem is exacerbated at the upper levels of the STS hierarchy, where processing of multiple pointer values per data frame is performed.</span>
</span>
</td>
</tr>
<tr id="A1.T9.4.3" class="ltx_tr">
<td id="A1.T9.4.3.1" class="ltx_td ltx_align_left ltx_border_t">0.1998944878578186</td>
<td id="A1.T9.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T9.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.4.3.2.1.1" class="ltx_p" style="width:284.5pt;">US20080109728A1 - Methods and Systems for Effecting Video Transitions Represented By Bitmaps - Google Patents
Methods and Systems for Effecting Video Transitions Represented By Bitmaps Download PDF
David Maymudes
Multi-media project editing methods and systems are described. In one embodiment, a project editing system comprises a multi-media editing application that is configured to . . . . . . . . synchronization models for multimedia data
US20120206653A1 (en) 2012-08-16 Efficient Media Processing
US6658477B1 (en) 2003-12-02 Improving the control of streaming data through multiple processing modules
US6212574B1 (en) 2001-04-03 User mode proxy of kernel mode operations in a computer operating system
US7752548B2 (en) 2010-07-06 Features such as titles, transitions, and/or effects which vary according to positions</span>
</span>
</td>
</tr>
<tr id="A1.T9.4.4" class="ltx_tr">
<td id="A1.T9.4.4.1" class="ltx_td ltx_align_left ltx_border_t">0.21122217178344727</td>
<td id="A1.T9.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T9.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.4.4.2.1.1" class="ltx_p" style="width:284.5pt;">Both the Ethernet II and IEEE 802.3 standards define the minimum frame size as 64 bytes and the maximum as 1518 bytes. This includes all bytes from the Destination MAC Address field through the Frame Check Sequence (FCS) field. The Preamble and Start Frame Delimiter fields are not included when . . . . . . . . frame. Dropped frames are likely to be the result of collisions or other unwanted signals and are therefore considered invalid.
At the data link layer the frame structure is nearly identical. At the physical layer different versions of Ethernet vary in their method for detecting and placing data on the media.</span>
</span>
</td>
</tr>
<tr id="A1.T9.4.5" class="ltx_tr">
<td id="A1.T9.4.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t">0.2133803367614746</td>
<td id="A1.T9.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="A1.T9.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.4.5.2.1.1" class="ltx_p" style="width:284.5pt;">A byte is a group of bits, usually eight. As memory capacities increase, the capacity of chip cards is often quoted in bytes rather than in bits as in the past.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2308.12283" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2308.12284" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2308.12284">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.12284" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2308.12285" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:01:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
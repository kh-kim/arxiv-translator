<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.10198] How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior</title><meta property="og:description" content="Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does prov…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.10198">

<!--Generated on Sun May  5 17:28:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kevin Wu
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">kevinywu@stanford.edu</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Eric Wu*
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">wue@stanford.edu</span> 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>James Zou 
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">jamesz@stanford.edu</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Denotes equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, we systematically analyze the tug-of-war between a LLM’s internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, we also find that the more the modified information deviates from the model’s prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model’s prior knowledge and the information presented in reference documents.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Large language models (LLMs), though powerful, are prone to hallucination <cite class="ltx_cite ltx_citemacro_citep">(Pal et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; Sun et&nbsp;al., <a href="#bib.bib24" title="" class="ltx_ref">2024</a>; Ahmad et&nbsp;al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>. Additionally, they are restricted to knowledge contained in their training corpus, and so are unable to answer queries about recent events or publicly restricted information. Retrieval augmented generation (RAG) is a commonly used framework that provides relevant retrieved content in the LLM prompt and can significantly improve model accuracy <cite class="ltx_cite ltx_citemacro_citep">(Mao et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Chen et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2024a</a>; Lewis et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>. 
<br class="ltx_break"></p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Most commercial LLMs, like ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, Gemini <cite class="ltx_cite ltx_citemacro_citep">(Gemini Team, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, and Perplexity.ai already employ some version of RAG in their Web interfaces. For example, ChatGPT employs a Bing search whereas Gemini accesses Google Search results. Though RAG has quickly become a default feature of user-facing LLM systems, most evaluations of LLM capabilities are still performed on the non-RAG counterparts <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>. This is problematic, as a model’s default and RAG-enabled responses can drastically diverge depending on the quality and accuracy of the retrieved content. This problem is compounded when considering that web results constantly change, and can contain outdated, incorrect, or harmful information <cite class="ltx_cite ltx_citemacro_citep">(Dash et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>; Daws, <a href="#bib.bib5" title="" class="ltx_ref">2020</a>; Nastasi et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. Thus, objective evaluations of RAG-enabled LLM behavior are as important as benchmarking their non-RAG counterparts, especially as RAG systems are increasingly relied upon to provide factual information in a myriad of domains.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this work, we aim to quantify the tension between LLMs’ internal knowledge and the retrieved information presented in RAG settings. To tease apart these two competing forces, we query LLMs to answer questions and measure the token probabilities while introducing varying perturbations to reference documents. Our analyses reveal two key findings:
<br class="ltx_break"></p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The likelihood of the LLM to adhere to the retrieved information presented in context (RAG preference rate) is inversely correlated with the model’s confidence in its response without context (its prior probability).
<br class="ltx_break"></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i2.p1.1" class="ltx_p">Similarly, LLMs will increasingly revert to their priors when the original context is progressively modified with unrealistic values.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">We find that these relationships hold under analysis on six different domain datasets across over 1200 questions. We also find that the choice of prompting technique (e.g. strictly adhere, loosely adhere) can influence both the baseline and strength of this relationship. These results highlight the inherent tension in LLMs between the model’s pre-trained knowledge and the retrieved content provided in context.
<br class="ltx_break"></p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">The issue of hallucination in LLMs has been explored in multiple contexts and models <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Kaddour et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. As a response, RAG systems have been shown to reduce hallucination <cite class="ltx_cite ltx_citemacro_citep">(Shuster et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Kang et&nbsp;al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>. Previous works have explored automated RAG evaluation frameworks in various settings <cite class="ltx_cite ltx_citemacro_citep">(Es et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2023a</a>; Hoshi et&nbsp;al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Saad-Falcon et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2023a</a>; Zhang et&nbsp;al., <a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>. For example, some studies use LLMs to evaluate the faithfulness, answer relevance, and context relevance of RAG systems by using GPT-3.5 as an evaluator <cite class="ltx_cite ltx_citemacro_citep">(Es et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2023b</a>; Saad-Falcon et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>. In another study, the authors propose metrics such as noise robustness, negative rejection, information integration, and counterfactual robustness <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2024b</a>)</cite>. Multiple studies have shown that RAG can mislead LLMs in the presence of complex or misleading search results and that such models can still make mistakes even when given the correct response <cite class="ltx_cite ltx_citemacro_citep">(Foulds et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2024</a>; Shuster et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. In relation to understanding model priors, other works have used log probabilities to assess the LLM’s confidence in responses <cite class="ltx_cite ltx_citemacro_citep">(Mitchell et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; Zhao et&nbsp;al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>. However, so far there has not been a systematic exploration of a model’s confidence (via logprobs) and the model’s preference for RAG-provided information.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.10198/assets/schematic4.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. We then observe whether the LLM prefers the modified information or its own prior answer.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2404.10198/assets/fig1-2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="454" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Across six QA datasets using GPT-4, we consistently observe an inverse relationship between the RAG preference rate (y-axis) and two characteristics (x-axes): 1. the model’s prior response probability (lefthand plots), and 2. the amount of deviation from the prior (righthand plots). RAG preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model’s prior response probability is computed from the average log probability of the response tokens queried without RAG.
The left plot in each pair visualizes the prior probability (grouped into 10 bins) against the RAG preference rate, along with the best-fit trend line and slope. The right plot visualizes absolute deviation from the reference information (for numerical datasets (top), up to two log-fold changes (along with the trendline); for categorical datasets (bottom), a total of four modification categories) against RAG preference rate. Additionally, the upper and lower half percentiles are shown in the right plots to illustrate that lower probability prior responses have monotonically lower RAG preference rates than higher probability prior responses.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Our main analysis consists of evaluating the RAG question-answering capabilities of GPT-4 when introducing varying levels of perturbations on the RAG documents. For this study, our dataset consists of 1,294 total questions across 6 different domains. Wherever referenced, the GPT-4 model used is <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">gpt-4-turbo-preview</span>, accessed in March 2024. We additionally evaluate our dataset on two other models: GPT3.5 (<span id="S2.p1.1.2" class="ltx_text ltx_font_italic">gpt-3.5-turbo-0125</span>) and Mistral-7B, the <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">Mistral-7B-Instruct-v0.1</span>. We chose these two LLMs as they are top-performing models that also allow access to the model’s token probabilities (via the OpenAI and Huggingface APIs). All main figures and tables report results using GPT-4; analyses using GPT-3.5 and Mistral-7B are reported in the Appendix.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">We generate questions from six subject domains. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4 to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. For each dataset below, we provide the full prompts used to generate questions in the Appendix.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Drug Dosages</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4 to verify that the generated question fulfills all criteria. After this step, we have 266 question-answer pairs.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Sports Statistics</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">We pulled Olympics records pages from Wikipedia.org across 9 sports: Athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 192 unique questions and answers.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>News</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to 03/25/24. From an initial corpus of 1486 news articles, we use GPT-4 to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We perform another GPT-4 quality control step and result in 249 unique question-answer pairs.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Dates, Names, and Cities</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">We begin with a random sample of 1000 articles from Huggingface’s Wikipedia dataset (20220301.en, <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib9" title="" class="ltx_ref">Foundation, </a>)</cite>). We use GPT-4 to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Concordance</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">We measure concordance, or the agreement between the reference answer generated based on the article content, and the model’s answer to the corresponding generated question. This is computed for both the model’s answer with and without context.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Modifying the Retrieved Documents</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.2" class="ltx_p">We perform systematic perturbations on each question/answer pair (as visualized in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: <math id="S2.SS3.p1.1.m1.10" class="ltx_Math" alttext="{0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0}" display="inline"><semantics id="S2.SS3.p1.1.m1.10a"><mrow id="S2.SS3.p1.1.m1.10.11.2" xref="S2.SS3.p1.1.m1.10.11.1.cmml"><mn id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">0.1</mn><mo id="S2.SS3.p1.1.m1.10.11.2.1" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.2.2" xref="S2.SS3.p1.1.m1.2.2.cmml">0.2</mn><mo id="S2.SS3.p1.1.m1.10.11.2.2" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.3.3" xref="S2.SS3.p1.1.m1.3.3.cmml">0.4</mn><mo id="S2.SS3.p1.1.m1.10.11.2.3" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.4.4" xref="S2.SS3.p1.1.m1.4.4.cmml">0.8</mn><mo id="S2.SS3.p1.1.m1.10.11.2.4" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.5.5" xref="S2.SS3.p1.1.m1.5.5.cmml">1.2</mn><mo id="S2.SS3.p1.1.m1.10.11.2.5" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.6.6" xref="S2.SS3.p1.1.m1.6.6.cmml">1.5</mn><mo id="S2.SS3.p1.1.m1.10.11.2.6" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.7.7" xref="S2.SS3.p1.1.m1.7.7.cmml">2.0</mn><mo id="S2.SS3.p1.1.m1.10.11.2.7" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.8.8" xref="S2.SS3.p1.1.m1.8.8.cmml">3.0</mn><mo id="S2.SS3.p1.1.m1.10.11.2.8" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.9.9" xref="S2.SS3.p1.1.m1.9.9.cmml">5.0</mn><mo id="S2.SS3.p1.1.m1.10.11.2.9" xref="S2.SS3.p1.1.m1.10.11.1.cmml">,</mo><mn id="S2.SS3.p1.1.m1.10.10" xref="S2.SS3.p1.1.m1.10.10.cmml">10.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.10b"><list id="S2.SS3.p1.1.m1.10.11.1.cmml" xref="S2.SS3.p1.1.m1.10.11.2"><cn type="float" id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">0.1</cn><cn type="float" id="S2.SS3.p1.1.m1.2.2.cmml" xref="S2.SS3.p1.1.m1.2.2">0.2</cn><cn type="float" id="S2.SS3.p1.1.m1.3.3.cmml" xref="S2.SS3.p1.1.m1.3.3">0.4</cn><cn type="float" id="S2.SS3.p1.1.m1.4.4.cmml" xref="S2.SS3.p1.1.m1.4.4">0.8</cn><cn type="float" id="S2.SS3.p1.1.m1.5.5.cmml" xref="S2.SS3.p1.1.m1.5.5">1.2</cn><cn type="float" id="S2.SS3.p1.1.m1.6.6.cmml" xref="S2.SS3.p1.1.m1.6.6">1.5</cn><cn type="float" id="S2.SS3.p1.1.m1.7.7.cmml" xref="S2.SS3.p1.1.m1.7.7">2.0</cn><cn type="float" id="S2.SS3.p1.1.m1.8.8.cmml" xref="S2.SS3.p1.1.m1.8.8">3.0</cn><cn type="float" id="S2.SS3.p1.1.m1.9.9.cmml" xref="S2.SS3.p1.1.m1.9.9">5.0</cn><cn type="float" id="S2.SS3.p1.1.m1.10.10.cmml" xref="S2.SS3.p1.1.m1.10.10">10.0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.10c">{0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0}</annotation></semantics></math>. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of <math id="S2.SS3.p1.2.m2.2" class="ltx_Math" alttext="[-100,100]" display="inline"><semantics id="S2.SS3.p1.2.m2.2a"><mrow id="S2.SS3.p1.2.m2.2.2.1" xref="S2.SS3.p1.2.m2.2.2.2.cmml"><mo stretchy="false" id="S2.SS3.p1.2.m2.2.2.1.2" xref="S2.SS3.p1.2.m2.2.2.2.cmml">[</mo><mrow id="S2.SS3.p1.2.m2.2.2.1.1" xref="S2.SS3.p1.2.m2.2.2.1.1.cmml"><mo id="S2.SS3.p1.2.m2.2.2.1.1a" xref="S2.SS3.p1.2.m2.2.2.1.1.cmml">−</mo><mn id="S2.SS3.p1.2.m2.2.2.1.1.2" xref="S2.SS3.p1.2.m2.2.2.1.1.2.cmml">100</mn></mrow><mo id="S2.SS3.p1.2.m2.2.2.1.3" xref="S2.SS3.p1.2.m2.2.2.2.cmml">,</mo><mn id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">100</mn><mo stretchy="false" id="S2.SS3.p1.2.m2.2.2.1.4" xref="S2.SS3.p1.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.2b"><interval closure="closed" id="S2.SS3.p1.2.m2.2.2.2.cmml" xref="S2.SS3.p1.2.m2.2.2.1"><apply id="S2.SS3.p1.2.m2.2.2.1.1.cmml" xref="S2.SS3.p1.2.m2.2.2.1.1"><minus id="S2.SS3.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.2.2.1.1"></minus><cn type="integer" id="S2.SS3.p1.2.m2.2.2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.2.2.1.1.2">100</cn></apply><cn type="integer" id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">100</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.2c">[-100,100]</annotation></semantics></math>. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in the Appendix. For example, for a name like <span id="S2.SS3.p1.2.1" class="ltx_text ltx_font_italic">Bob Green</span>, a slight modification implies a small tweak to another real name (<span id="S2.SS3.p1.2.2" class="ltx_text ltx_font_italic">Rob Greene</span>), whereas a significant modification produces a similar but fictitious name (<span id="S2.SS3.p1.2.3" class="ltx_text ltx_font_italic">Bilgorn Grevalle</span>), and a comical modification is an absurd variant (<span id="S2.SS3.p1.2.4" class="ltx_text ltx_font_italic">Blob Lawnface</span>). For a city name like <span id="S2.SS3.p1.2.5" class="ltx_text ltx_font_italic">Miami</span>, a slight modification changes the name of the most similar city (<span id="S2.SS3.p1.2.6" class="ltx_text ltx_font_italic">Fort Lauderdale</span>), a significant modification produces a fictitious city name (<span id="S2.SS3.p1.2.7" class="ltx_text ltx_font_italic">Marisole</span>), and a comical modification produces an absurd variant (<span id="S2.SS3.p1.2.8" class="ltx_text ltx_font_italic">Miameme</span>). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4 to generate the perturbed excerpts for drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>RAG vs Model Prior Analyses</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">The main analysis we perform in this study is comparing the <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">RAG preference</span> of a model against its <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_italic">internal prior</span>. The LLM is first queried with a question without context. This response and the average probability of the tokens (accessed via the log probs) are referred to as the model’s <span id="S2.SS4.p1.1.3" class="ltx_text ltx_font_italic">prior response</span> and the <span id="S2.SS4.p1.1.4" class="ltx_text ltx_font_italic">prior probability</span>, respectively. The LLM is then queried again, this time with the retrieved content present in the prompt. The resulting response (the response with RAG) is then compared with the prior response: if the response is still the same as the prior response, then the model <span id="S2.SS4.p1.1.5" class="ltx_text ltx_font_italic">prefers its prior</span>. On the other hand, if the model response aligns with the information present in the retrieved content, then the model <span id="S2.SS4.p1.1.6" class="ltx_text ltx_font_italic">prefers RAG</span>. For each dataset, the <span id="S2.SS4.p1.1.7" class="ltx_text ltx_font_italic">RAG preference rate</span> is computed as the average across all RAG queries.
<br class="ltx_break"></p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.p2.1" class="ltx_p">The RAG preference rate is compared against two measurements: the prior probability and the deviation from the prior value. The former is computed by accessing the log probabilities from the OpenAI API call. As these are provided in log scale, we exponentiate them to produce linear probabilities when presenting the results. The latter is computed in several ways. For the Drug Dosages, Sports Statistics, and Latest News datasets, the absolute log fold change between the prior value and the modified value is computed; for the Wikipedia Dates dataset, the simple absolute year change is used; and for the Wikipedia Names and Locations datasets, each categorical change is presented in order of degree of modification.
<br class="ltx_break"></p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Analyzing the Effects of Different Prompting Strategies</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">Additional analysis is performed on the prompting technique itself: for the examples above, we use a standard prompt template that is based on RAG prompts used on popular LLM open-source libraries, with over 800k downloads as of March 2024 (<a target="_blank" href="https://python.langchain.com/docs/expression_language/cookbook/retrieval" title="" class="ltx_ref ltx_href">LangChain</a> and <a target="_blank" href="https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/" title="" class="ltx_ref ltx_href">LlamaIndex</a>). In addition to this template (called Standard), we introduce two more prompt modifications: Strict, which strongly enforces literal adherence to the retrieved context, and Loose, which encourages the model to reason over the retrieved context before responding.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2404.10198/assets/examples4.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.</figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2404.10198/assets/adherence-prompts6.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="428" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Effect of different prompts using GPT-4 on RAG preference rate vs prior probability. The ”Strict” prompt strongly enforces literal adherence to the retrieved context, while the ”Loose” prompt encourages the model to make a reasonable judgment in light of the provided context. We observe lower and steeper drops in RAG adherence with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling RAG adherence. Full prompts are provided in the Appendix. </figcaption>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Concordance</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In Table <a href="#S3.T1" title="Table 1 ‣ 3.2.3 Differences in effects between GPT-4, GPT-3.5, and Mistral-7B ‣ 3.2 RAG Preference Rate vs. Prior Probability ‣ 3 Results ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we observe that the model’s prior response only agreed with the reference answer 34.7% on average. However, the RAG answers elevated the concordance to 94%. This result demonstrates that the RAG pipeline established in this work is highly effective at encouraging the model to adhere to its retrieved content. However, in the minority of cases where providing the retrieved content fails to correct the LLM, we find that the model simply responds with its original prior answer about 20% of the time.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>RAG Preference Rate vs. Prior Probability</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">In Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (left side plots), we observe a consistent negative relationship between the token probability of the model’s prior answer and the associated RAG preference rate for all six QA datasets. To visualize an even distribution across probabilities, we bin the probabilities into ten equidistant bins in the range of <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="[0.0,1.0]" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">0.0</mn><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">1.0</mn><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.3" xref="S3.SS2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><interval closure="closed" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><cn type="float" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">0.0</cn><cn type="float" id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">1.0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">[0.0,1.0]</annotation></semantics></math>. We additionally present the slope from performing a linear regression on the binned probability values against the RAG preference rate in Table <a href="#S3.T1" title="Table 1 ‣ 3.2.3 Differences in effects between GPT-4, GPT-3.5, and Mistral-7B ‣ 3.2 RAG Preference Rate vs. Prior Probability ‣ 3 Results ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The slope indicates the effect of stronger model confidence on the model’s preference for the information presented in the retrieved context; we observe different slopes (ranging from -0.1 to -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model’s internal prior knowledge confidence. Specifically, a slope of -0.45, for instance, can be interpreted as expecting a 4.5% decrease in the likelihood of the LLM preferring the contextual information for every 10% increase in the probability of the model’s prior response.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>RAG Preference Rate vs Deviation from Prior</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">We also consider the degree of deviation between the model’s prior response and the value contained in the retrieved context (Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, right side plots). A similar pattern emerges in this analysis: as the RAG value diverges from the model’s prior, the model is less likely to adopt the RAG value over its own initial response. We additionally plot data split into the upper and lower half percentiles and observe that, across all six datasets, the lower probability prior responses are monotonically lower than the higher probability response tokens. Thus, the correlation between deviation and RAG response rate holds across both bands of probabilities.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Effect of prompting technique on RAG adherence</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">To assess the degree of influence that the specific prompting technique has on RAG adherence, we test two additional prompts (”strict” and ”loose”) on GPT-4. The strict prompt is intended to coerce the model to disregard its own prior response, while the loose prompt is intended for the model to arbitrate between its own prior and the contextual information provided. In Figure <a href="#S2.F4" title="Figure 4 ‣ 2.4.1 Analyzing the Effects of Different Prompting Strategies ‣ 2.4 RAG vs Model Prior Analyses ‣ 2 Methods ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the strict prompt has uniformly higher RAG adherence than the standard prompt. The loose prompt, on the other hand, results in much lower RAG adherence rates as prior probability increases. Interestingly, the slope is also steeper, indicating a larger per-unit decrease in RAG preference as the prior probability increases. The choice of prompt is thus an important mechanism for influencing the LLM’s RAG preferences.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Differences in effects between GPT-4, GPT-3.5, and Mistral-7B</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">We report the same analyses when using GPT-3.5 and Mistral-7B in Table <a href="#A1.T2" title="Table 2 ‣ Appendix A Appendix ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a href="#A1.F5" title="Figure 5 ‣ Appendix A Appendix ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We observe significantly lower performance both in concordance of the prior and with RAG. However, as seen in Figure <a href="#A1.F5" title="Figure 5 ‣ Appendix A Appendix ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we nonetheless observe the same inverse trends in these two models as seen in the results with GPT-4. Of note, some datasets (like Latest News) perform poorly without RAG (the model refused the vast majority of queries or provided invalid responses), and thus the prior token probabilities could not be analyzed. In the Mistral-7B results, we also observe that some of the responses using RAG could not consistently provide valid responses.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">GPT-4</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Concordance (Prior)</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Concordance (w/ RAG)</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Slope</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Drug Dosage</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.554</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.884</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-0.26</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sports Stats</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center">0.240</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">0.943</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">-0.18</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Latest News</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center">0.133</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center">0.936</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center">-0.10</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Dates</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center">0.433</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center">0.995</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center">-0.45</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Names</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center">0.350</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center">0.965</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center">-0.13</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Locations</th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center">0.375</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center">0.920</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center">-0.28</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S3.T1.1.8.7.1.1" class="ltx_text ltx_font_italic">Average</span></th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.8.7.2.1" class="ltx_text ltx_font_italic">0.347</span></td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.8.7.3.1" class="ltx_text ltx_font_italic">0.940</span></td>
<td id="S3.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.8.7.4.1" class="ltx_text ltx_font_italic">-0.23</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Concordance between the GPT response and the reference values for each dataset. Prior refers to GPT-4 responses without context, and ”w/ RAG” refers to responses with the relevant retrieved context included in the prompt. Additionally, we include the slope of the relationship between prior probability and RAG preference rate. For instance, the average slope is -0.23, which means that for every 10% increase in the probability of the prior token, we observe a 2.3% decreased likelihood of RAG preference.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">While RAG is becoming standard practice in commercially available LLMs, the reliability of such systems is still understudied. Our experiments uncover several mechanisms that modulate the degree to which LLMs adhere to RAG systems. Specifically, we quantify a tug-of-war between the strength of the model’s prior and the rate at which the model adheres to the RAG document’s facts. This effect is at odds with claims that RAG itself can fix hallucinations alone, and occurs even when the model is prompted to adhere to RAG documents strictly.

<br class="ltx_break"></p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">RAG systems have a unique appeal over traditional search engines in that they can incorporate prior knowledge to fill in the gaps and extrapolate the retrieved information. We find that this comes with trade-offs – namely, that such priors can override information provided in documents. When perturbing RAG documents over a wide interval of values, the point at which models revert to their prior responses, or ”tipping points”, are latent and heterogeneous across different models and domains. While strong priors are not inherently problematic (and can often serve to safeguard models), the lack of explicit expectations around how models will mix reference documents with their priors can lead to downstream issues. For example, if RAG systems are used to extract nested financial data to be used in an algorithm, what will happen if there is a typo in the financial documents? Will the model notice the error and if so, what data will it provide in its place?
Given that LLMs are soon to be widely deployed in many domains including medicine and law, users and developers alike should be cognizant of their unintended effects, especially if users have preconceptions that RAG-enabled systems are, by nature, always truthful.
<br class="ltx_break"></p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">There are several key limitations in our analyses. First, RAG systems can be deployed to many more domains than can be covered by our analyses. However, we hope that our study ixix domains paints an initial picture of the nature of RAG systems. Second, to make our experiments tractable, our question-generation process is strictly fact-based and does not require multi-step logic, document synthesis, or other higher-level reasoning. Third, the perturbations we produce are based on our priors for what would constitute a reasonable or unreasonable range of values. In a natural setting, we would imagine more discrete types of errors (eg. typos, ambiguities, missing information, etc.) which are harder to simulate. We also perform evaluations on GPT-3.5 and GPT-4 because the OpenAI API allows for access to token-wise log probabilities along with the responses. As a consequence, we are limited against performing more comprehensive evaluations on models such as Gemini and Claude because the APIs for these models do not provide access to such information.
<br class="ltx_break"></p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p">LLMs are now commonly used as parts of larger, more complex systems. It is crucial to understand how these models interact with information with varying degrees of trustworthiness, accuracy, and uniformity. Our analysis shows that further work is required to characterize the risks of using LLMs to answer questions given contextual information. In particular, we find that model behavior can be erratic and unpredictable when presented with information that exists at the margin of its prior beliefs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmad et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Muhammad&nbsp;Aurangzeb Ahmad, Ilker Yaramis, and Taposh&nbsp;Dutta Roy.

</span>
<span class="ltx_bibblock">Creating trustworthy LLMs: Dealing with hallucinations in healthcare AI.

</span>
<span class="ltx_bibblock">September 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024a)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le&nbsp;Sun.

</span>
<span class="ltx_bibblock">Benchmarking large language models in Retrieval-Augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 38(16):17754–17762, March 2024a.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024b)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le&nbsp;Sun.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume&nbsp;38, pp.&nbsp; 17754–17762, 2024b.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dash et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Debadutta Dash, Rahul Thapa, Juan&nbsp;M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan&nbsp;H Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret&nbsp;Kenn Morris, Honor Magon, Matthew&nbsp;P Lungren, Eric Horvitz, and Nigam&nbsp;H Shah.

</span>
<span class="ltx_bibblock">Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery.

</span>
<span class="ltx_bibblock">April 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daws (2020)</span>
<span class="ltx_bibblock">
Ryan Daws.

</span>
<span class="ltx_bibblock">Medical chatbot using OpenAI’s GPT-3 told a fake patient to kill themselves.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/</a>, October 2020.

</span>
<span class="ltx_bibblock">Accessed: 2024-1-19.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert.

</span>
<span class="ltx_bibblock">RAGAS: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock">September 2023a.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert.

</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15217</em>, 2023b.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Foulds et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Philip&nbsp;Feldman Foulds, R&nbsp;James, and Shimei Pan.

</span>
<span class="ltx_bibblock">Ragged edges: The double-edged sword of retrieval-augmented chatbots.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.01193</em>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Wikimedia Foundation.

</span>
<span class="ltx_bibblock">Wikimedia downloads.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://dumps.wikimedia.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dumps.wikimedia.org</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team (2023)</span>
<span class="ltx_bibblock">
Gemini Team.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock">December 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoshi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, and Jun Deguchi.

</span>
<span class="ltx_bibblock">RaLLe: A framework for developing and evaluating Retrieval-Augmented large language models.

</span>
<span class="ltx_bibblock">August 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye&nbsp;Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1–38, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaddour et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy.

</span>
<span class="ltx_bibblock">Challenges and applications of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.10169</em>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Haoqiang Kang, Juntong Ni, and Huaxiu Yao.

</span>
<span class="ltx_bibblock">Ever: Mitigating hallucination in large language models through real-time verification and rectification.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09114</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, and Others.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Adv. Neural Inf. Process. Syst.</em>, 33:9459–9474, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Generation-Augmented retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">September 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
E&nbsp;Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher&nbsp;D Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">DetectGPT: Zero-shot machine-generated text detection using probability curvature.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICML</em>, pp.&nbsp; 24950–24962, January 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nastasi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Anthony&nbsp;J Nastasi, Katherine&nbsp;R Courtright, Scott&nbsp;D Halpern, and Gary&nbsp;E Weissman.

</span>
<span class="ltx_bibblock">Does ChatGPT provide appropriate and equitable medical advice?: A vignette-based, clinical evaluation across care contexts.

</span>
<span class="ltx_bibblock">March 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock">March 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh&nbsp;Kumar Umapathi, and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">Med-HALT: Medical domain hallucination test for large language models.

</span>
<span class="ltx_bibblock">July 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia.

</span>
<span class="ltx_bibblock">ARES: An automated evaluation framework for Retrieval-Augmented generation systems.

</span>
<span class="ltx_bibblock">November 2023a.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09476</em>, 2023b.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.

</span>
<span class="ltx_bibblock">Retrieval augmentation reduces hallucination in conversation.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.07567</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil&nbsp;Zhenqiang Gong, Philip&nbsp;S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao.

</span>
<span class="ltx_bibblock">TrustLLM: Trustworthiness in large language models.

</span>
<span class="ltx_bibblock">January 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Zihan Zhang, Meng Fang, and Ling Chen.

</span>
<span class="ltx_bibblock">RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering.

</span>
<span class="ltx_bibblock">February 2024.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould.

</span>
<span class="ltx_bibblock">The first to know: How token distributions reveal hidden knowledge in large Vision-Language models?

</span>
<span class="ltx_bibblock">March 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric&nbsp;P Xing, Hao Zhang, Joseph&nbsp;E Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Judging LLM-as-a-Judge with MT-Bench and chatbot arena.

</span>
<span class="ltx_bibblock">June 2023.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<figure id="A1.F5" class="ltx_figure"><img src="/html/2404.10198/assets/fig1combined2.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Analyses for RAG preference rate against prior probability and deviation, using GPT-4 (blue), GPT-3.5 (orange), and Mistral-7B (green). Please see Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for full figure descriptions. Of note, some models did not generate any meaningful prior responses (due to refusal, improper responses, etc.) for certain datasets and thus could not be analyzed.</figcaption>
</figure>
<figure id="A1.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A1.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T2.1.1.1" class="ltx_tr">
<th id="A1.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span></th>
<th id="A1.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Concordance (Prior)</span></th>
<th id="A1.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Concordance (w/ RAG)</span></th>
<th id="A1.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Slope</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T2.1.2.1" class="ltx_tr">
<th id="A1.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Drug Dosage</th>
<td id="A1.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.052</td>
<td id="A1.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.509</td>
<td id="A1.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-0.20</td>
</tr>
<tr id="A1.T2.1.3.2" class="ltx_tr">
<th id="A1.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sports Stats</th>
<td id="A1.T2.1.3.2.2" class="ltx_td ltx_align_center">0.005</td>
<td id="A1.T2.1.3.2.3" class="ltx_td ltx_align_center">0.599</td>
<td id="A1.T2.1.3.2.4" class="ltx_td ltx_align_center">-0.09</td>
</tr>
<tr id="A1.T2.1.4.3" class="ltx_tr">
<th id="A1.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Latest News</th>
<td id="A1.T2.1.4.3.2" class="ltx_td ltx_align_center">0.008</td>
<td id="A1.T2.1.4.3.3" class="ltx_td ltx_align_center">0.839</td>
<td id="A1.T2.1.4.3.4" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="A1.T2.1.5.4" class="ltx_tr">
<th id="A1.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Dates</th>
<td id="A1.T2.1.5.4.2" class="ltx_td ltx_align_center">0.275</td>
<td id="A1.T2.1.5.4.3" class="ltx_td ltx_align_center">0.985</td>
<td id="A1.T2.1.5.4.4" class="ltx_td ltx_align_center">-0.71</td>
</tr>
<tr id="A1.T2.1.6.5" class="ltx_tr">
<th id="A1.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Names</th>
<td id="A1.T2.1.6.5.2" class="ltx_td ltx_align_center">0.285</td>
<td id="A1.T2.1.6.5.3" class="ltx_td ltx_align_center">0.965</td>
<td id="A1.T2.1.6.5.4" class="ltx_td ltx_align_center">-0.11</td>
</tr>
<tr id="A1.T2.1.7.6" class="ltx_tr">
<th id="A1.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Locations</th>
<td id="A1.T2.1.7.6.2" class="ltx_td ltx_align_center">0.410</td>
<td id="A1.T2.1.7.6.3" class="ltx_td ltx_align_center">0.930</td>
<td id="A1.T2.1.7.6.4" class="ltx_td ltx_align_center">-0.16</td>
</tr>
<tr id="A1.T2.1.8.7" class="ltx_tr">
<th id="A1.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A1.T2.1.8.7.1.1" class="ltx_text ltx_font_italic">Average</span></th>
<td id="A1.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.173</td>
<td id="A1.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.805</td>
<td id="A1.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-0.25</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A1.T2.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T2.2.1.1" class="ltx_tr">
<th id="A1.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Mistral-7B</span></th>
<th id="A1.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">Concordance (Prior)</span></th>
<th id="A1.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">Concordance (w/ RAG)</span></th>
<th id="A1.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.2.1.1.4.1" class="ltx_text ltx_font_bold">Slope</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T2.2.2.1" class="ltx_tr">
<th id="A1.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Drug Dosage</th>
<td id="A1.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.550</td>
<td id="A1.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.677</td>
<td id="A1.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-0.82</td>
</tr>
<tr id="A1.T2.2.3.2" class="ltx_tr">
<th id="A1.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sports Stats</th>
<td id="A1.T2.2.3.2.2" class="ltx_td ltx_align_center">0.240</td>
<td id="A1.T2.2.3.2.3" class="ltx_td ltx_align_center">0.057</td>
<td id="A1.T2.2.3.2.4" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="A1.T2.2.4.3" class="ltx_tr">
<th id="A1.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Latest News</th>
<td id="A1.T2.2.4.3.2" class="ltx_td ltx_align_center">N/A</td>
<td id="A1.T2.2.4.3.3" class="ltx_td ltx_align_center">N/A</td>
<td id="A1.T2.2.4.3.4" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="A1.T2.2.5.4" class="ltx_tr">
<th id="A1.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Dates</th>
<td id="A1.T2.2.5.4.2" class="ltx_td ltx_align_center">0.080</td>
<td id="A1.T2.2.5.4.3" class="ltx_td ltx_align_center">0.840</td>
<td id="A1.T2.2.5.4.4" class="ltx_td ltx_align_center">-0.77</td>
</tr>
<tr id="A1.T2.2.6.5" class="ltx_tr">
<th id="A1.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Names</th>
<td id="A1.T2.2.6.5.2" class="ltx_td ltx_align_center">0.065</td>
<td id="A1.T2.2.6.5.3" class="ltx_td ltx_align_center">0.760</td>
<td id="A1.T2.2.6.5.4" class="ltx_td ltx_align_center">-0.14</td>
</tr>
<tr id="A1.T2.2.7.6" class="ltx_tr">
<th id="A1.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikipedia Locations</th>
<td id="A1.T2.2.7.6.2" class="ltx_td ltx_align_center">0.490</td>
<td id="A1.T2.2.7.6.3" class="ltx_td ltx_align_center">0.690</td>
<td id="A1.T2.2.7.6.4" class="ltx_td ltx_align_center">-0.030</td>
</tr>
<tr id="A1.T2.2.8.7" class="ltx_tr">
<th id="A1.T2.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A1.T2.2.8.7.1.1" class="ltx_text ltx_font_italic">Average</span></th>
<td id="A1.T2.2.8.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T2.2.8.7.2.1" class="ltx_text ltx_font_italic">0.285</span></td>
<td id="A1.T2.2.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T2.2.8.7.3.1" class="ltx_text ltx_font_italic">0.605</span></td>
<td id="A1.T2.2.8.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T2.2.8.7.4.1" class="ltx_text ltx_font_italic">-0.44</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Concordance and Slope with GPT-3.5 and Mistral-7B. Please see Table <a href="#S3.T1" title="Table 1 ‣ 3.2.3 Differences in effects between GPT-4, GPT-3.5, and Mistral-7B ‣ 3.2 RAG Preference Rate vs. Prior Probability ‣ 3 Results ‣ How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for a full table description of analyses performed using GPT-4.</figcaption>
</figure>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Prompts</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">Due to the length of the prompts, we have stored them in a CSV within open-access and anonymized repository: https://anonymous.4open.science/r/rag-tug-of-war-1E48/prompts.csv</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.10196" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.10198" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.10198">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.10198" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.10199" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 17:28:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
# Best Practices and Lessons Learned on Synthetic Data for Language Models

Ruibo Liu1

Jerry Wei1

Fangyu Liu1

Chenglei Si2

Yanzhe Zhang3

Jimmeng Rao1

Steven Zheng1

Daiyi Peng1

Diyi Yang2

Denny Zhou1 and Andrew M. Dai1

1Google DeepMind, 2Stanford University, 3Georgia Institute of Technology

###### Abstract

The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.

## 1 Introduction

Figure 1: One synthetic image generated by Imagen (Saharia et al., 2022a) v2.0, with a prompt including the following description: “_In a robotics factory, humanoid robots collaborate on an assembly line to design, fabricate, test, and assemble new robots. The new robots they are manufacturing look similar to those robotic workers who are creating them.”_ We also added some style controlling text from aesthetic considerations.

The rapid advancement of artificial intelligence (AI) technologies has led to their widespread adoption across numerous domains, from assistant agents (e.g., ACT-1, from Adept AI1) and software development (e.g., Devin, from Cognition Lab2) to healthcare (Singhal et al., 2022) and finance (Zheng et al., 2022). However, the success of AI models heavily relies on the availability of large, diverse, and high-quality datasets for training and evaluation. Acquiring such datasets can be a significant challenge due to data scarcity (Babbar and Scholkopf, 2019), privacy concerns (Abay et al., 2019), and the sheer cost of data collection and annotation (Gilardi et al., 2023). Pessimists predict that we will run out of fresh text data in 2050 and image data in 2060 (Villalobos et al., 2022).

Footnote 1: ACT-1: [https://www.adept.ai/blog/act-1](https://www.adept.ai/blog/act-1)

Footnote 2: Devin: [https://www.cognition-labs.com/introducing-devin](https://www.cognition-labs.com/introducing-devin)

Synthetic data has emerged as a promising solution to address these challenges (Nikolenko, 2021). Synthetic data refers to artificially generated data that mimics the characteristics and patterns of real-world data, but is created through algorithms (Saxton et al., 2019), generative models (Borisov et al., 2022; Meng et al., 2022), or even simulations (Liu et al., 2023; Vezhnevets et al., 2023), rather than being directly created by humans. By leveraging synthetic data, we can not only overcome the limitations of real-world data but also unlock the potential to develop more robust, reliable, and fair AI models (Lu et al., 2023; Lucini, 2021).

One of the many benefits of synthetic data is that it can be generated at scale, providing an abundant supply of training and testing data for AI models. This is particularly valuable in domains where real-world data is scarce or difficult to obtain (e.g., weather data covering all conditions (Lam et al., 2023; Li et al., 2023)). Second, synthetic data can be tailored to specific requirements, such as ensuring a balanced representation of different classes by introducing controlled variations (e.g., up-weighting low-resource languages in multilingual language learning (Przystupa and Abdul-Mageed, 2019)). This level of control over data characteristics can improve model performance and generalization. Third, synthetic data can help mitigate privacy concerns by creating anonymized or de-identified datasets that do not contain sensitive personal information (El Emam et al., 2020; Howe et al., 2017). This is crucial in domains such as healthcare, where patient privacy is of utmost importance (Dahmen and Cook, 2019; Wei et al., 2019).

Despite its promise, synthetic data also presents challenges that need to be addressed. One of them is ensuring the factuality and fidelity of synthetic data (Heusel et al., 2017; Wood et al., 2021), as models trained on false, hallucinated or biased synthetic data may fail to generalize to real-world scenarios (Guamera et al., 2020; Van Breugel et al., 2023). Researchers must develop sophisticated generative models and evaluation metrics to create synthetic data that accurately reflects the complex patterns and relationships found in real-world data. Another challenge is the potential for synthetic data to amplify biases or introduce new biases if not carefully designed and validated (Barbierato et al., 2022; Gupta et al., 2021). We believe rigorous testing and fairness assessments are necessary to mitigate these risks.

In this paper, we track the current state of synthetic data research and discuss current best practices and lessons learned. The rest of the paper is organized as follows. Section 2 provides an overview of synthetic data generation techniques and their applications in model training, presenting case studies and empirical evidence. Section 3 discusses the usefulness of synthetic data in evaluation. Section 4 discusses the challenges and limitations of synthetic data, and in Section 5 we outline potential solutions and future research directions.

## 2 Synthetic Data in Training

Synthetic data, which is generated by mimicking authentic data collected from the real world, has proven to be an effective and relatively low-cost alternative of real data. This section explores several notable domains that leverage synthetic training data.

### Reasoning

Math.Recent advancements in mathematical reasoning for language models (LMs) have led to the development of various approaches to improve performance on math-related tasks. One approach is to train on math-targeted pre-training data, such as Minerva (Lewkowycz et al., 2022), Llemina (Azerbayev et al., 2023), and DeepSeeKMath (Shao et al., 2024). Another mainstream method is to generate synthetic questions and answers to imitate the training or validation set of target benchmarks. For instance, WizardMath (Luo et al., 2023) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning. GAIR-Abel (Chern et al., 2023) found that the format of the augmented answers is crucial to final performance, with answers that begin with a paraphrasing of the question followed by a step-by-step solution showing better performance than those in vanilla format. Xwin-Math (Li et al., 2024) further scaled up synthetic SFT data to one million examples and found that the LLaMA-2 7B model (Touvron et al., 2023) can still benefit from data scaling. MMIQC (Liu and Yao, 2024) composed a bundle of datasets that infuse SFT style data (via question-answer rephrasing or directly taken from MetaMath) with a subset of high-quality mathematical pre-training data, such as OpenWebMath (Paster et al., 2023).

Scaling up the generation of synthetic math data is a straightforward process, but ensuring the correctness of the generated math remains a significant challenge for practitioners. AlphaGeometry (Trinh et al., 2024) is a recent attempt to address this issue by training a neural model using 100 million synthetic data points. The model proposes solutions and guides a symbolic deduction engine in verifying the correctness of each branch when solving complex geometry problems. By combining the power of synthetic data with a rigorous verification process, AlphaGeometry achieves a problem-solving ability comparable to that of a human Olympiad gold medalist, demonstrating the potential of this approach in tackling complex mathematical reasoning tasks.

Code.Different from Math, synthetic data for code reasoning can naturally combine the execution results with structured code, as one requirement of correct code is being executable. In coding-enhanced models, CodeRL (Le et al., 2022) presents an actor-critic approach to improve pretrained language models with feedback signals on synthetic code samples. Haluptzok et al. (2022) propose a self-improvement strategy where the models generate their own synthetic puzzle-solution pairs. These pairs are then verified and filtered by a real interpreter before being used to finetune language models. Shypula et al. (2023) further propose a framework that leverages a simulated environment and adaptation strategies like self-improvement synthetic data generation and CoT prompting for code optimization. Yang et al. (2024) developed InterCode, a framework designed to enhance interactive code generation within a reinforcement learning environment, where code serves as actions and execution feedback serves as observations. Reflexion (Shinn et al., 2024) employs external or internally simulated linguistic feedback signals to improve the code reasoning capabilities of language models. Regarding synthetic SFT data, Code Alpaca comprises a dataset of 20K code instructions automatically generated by applying SELF-INSTRUCT (Wang et al., 2022) to ChatGPT across 21 seed tasks. WizardCoder (Luo et al., 2023) introduces Code Evol-Instruct to guide ChatGPT with heuristicprompts to enhance the complexity and diversity of synthetic data. Meanwhile, Magicoder (Wei et al., 2023c) developed OSS-INSTRUCT, which generates 75K diverse synthetic instruction samples from open-source code snippets.

Other reasoning tasks.Synthetic data also leads to impressive performance in other reasoning tasks. For instance, Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks. STaR (Zelikman et al., 2022) generates synthetic chain-of-thought rationales and filters out those leading to wrong answers for finetuning language models to improve their reasoning. In the domain of physics reasoning, Mind's Eye (Liu et al., 2022) takes a novel approach by training a text-to-code model with synthetic "text-description \(\rightarrow\) rendering code" data. This enables the model to convert textual questions into rendering code, which is then executed in a physical engine (i.e., DeepMind MuJoCo (Todorov et al., 2012)). The rendering results are injected into the context, allowing even small language models armed with Mind's Eye to achieve performance comparable to models 100 times larger.

### Tool-using and Planning

Learning tool-using through synthetic trajectories.Synthetic data is also a powerful approach to enable LMs to learn tool-using abilities through simulated trajectories, as collecting real-world human tool-using data might be time-consuming, and the actual distribution of calls to tools might be skewed. LaMDA (Thoppilan et al., 2022), for instance, was trained not only on web documents but also on interaction data between crowdworkers and the model itself, with the synthetic data annotated with calls to appropriate tools. This training process allowed LaMDA to develop the ability to use a calculator for arithmetic, a search engine for real-time information seeking, and a machine translator for translation. Similarly, Toolformer (Schick et al., 2024) learns to decide which APIs to call and what arguments to pass by training on template-generated data, while Galactica (Taylor et al., 2022) infuse API-calling data into pre-training mixture. ToolApaca (Tang et al., 2023) is a novel framework designed to automatically generate a diverse tool-use corpus, by building a multi-agent simulation environment and letting agents select and use tools iteratively. These examples demonstrate the potential of synthetic trajectories in enabling LMs to acquire tool-using abilities and enhance their reasoning capabilities across various domains.

Learning to plan in synthetic environments.An important feature of the agent in Autonomous Machine Intelligence (LeCun, 2022) is planning--an ability of decomposing complex tasks into subtasks and finishing the subtasks in a reward-optimal way (Kambhampati et al., 2024). Synthetic data can be a valuable tool here as it can serve as the feedback signal collected from a simulator (Park et al., 2023), and learning on it can make the agent aware of affordances (Ahn et al., 2022; Liang et al., 2022). For example, Inner Monologue (Huang et al., 2022) leverages natural language form feedback generated by the simulated environment to teach LLM-based robots planning. They find that such feedback significantly improves high-level instruction completion on both simulated and real-world domains. To compose a large number of realistic planning tasks (e.g., _"Rearrange objects on a table to match a given scene."_), VIMA (Jiang et al., 2022) creates a multi-modality simulated environment called VIMA-Bench, which supports extensible collections of objects and textures. In the Minecraft game, Voyager (Wang et al., 2023) deploys a number of GPT-4 based agents to interact with the synthetic environment and finds that the agents can unlock new skills faster and complete planning more efficiently with the help of synthetic feedback.

### Multimodality

Reverse rendering from vision to text.Vision-language alignment data focuses on accurately grounding visual input to an LLM (usually via a vision encoder). Web-scraped image-caption pairs have been the most popular MM alignment data in the past few years since CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). However, web-scraped image-text pairs are usually noisy and only have coarse-grained correspondence, insufficient for grounding details of images in language. In domains such as documents, screens, figures, and diagrams, such fine-grained alignment can most conveniently be obtained from data synthesis pipelines built with image rendering engines. Pix2Struct (Lee et al., 2023) uses web servers to render HTML code into website screenshots, and the training task is to dereder a masked screenshot to the full HTML code. MatCha (Liu et al., 2023b) and DePlot (Liu et al., 2023a) render tabular data into charts with Python plotting libraries and pretrain a foundation model by giving the rendered image and producing the code and/or the tabular data. Si et al. (2024) and Laurencon et al. (2024) train on synthetically generated HTML and CSS files for the task of converting webpage screenshots into code implementation. The models finetuned on the synthetic data can generalize reasonably well on realistic data scraped from the Internet. Borkman et al. (2021) propose to use physics engines or game engines (e.g., Unity) as the synthetic data generator to help computer vision research.

Multi-modality instruction following.Downstream applications of multimodal LLMs require reasoning and instruction following capabilities. Such data are usually long-form question response pairs and are expensive for humans to create. LLaVA (Liu et al., 2024b) uses existing image captions to prompt GPT-4 (in text-only mode) for writing diverse and long-form prompt-answer pairs. During multimodal LLM training, images and prompts are used as input while the captions and bounding box information can be hidden. Besides image captions, other sources of image attribute information such as object bounding box (Zhao et al., 2023), OCR (Zhang et al., 2023d) and deredered charts (Carbune et al., 2024; Masry et al., 2023) can all fit into such as image attributes + text LLM rewriting synthetic data pipeline.

### Multilingual

Back-translation augmentation.Many multilingual language models use back-translation as a data augmentation method, creating synthetic parallel training data from monolingual data sources (Bi et al., 2021; Caswell et al., 2019; Liao et al., 2021; Marie et al., 2020; Pham et al., 2021; Sennrich et al., 2016; Xu et al., 2022; Zheng et al., 2020). For example, Sennrich et al. (2016) back-translate monolingual target data into source language data, providing additional parallel training samples for substantial translation task improvements. Researchers have also explored different sampling methods for back-translation (e.g., beam search, constrained sampling, unconstrained sampling) and their comparative effectiveness (Edunov et al., 2018; Graca et al., 2019; Sennrich et al., 2016). Xu et al. (2022) emphasize the importance of the weight and quality of synthetic data for optimal NMT performance using back-translation. They propose a method to optimize the ratio between search methods and a gamma score to balance estimated importance weight and quality. However, some limitations exist with back-translation-based synthetic data generation. For example, the quality and diversity of synthetic data depends on the performance of the back-translation method. If the synthetic data is too noisy or not diverse, the performance gain would be limited (Chauhan et al., 2022; Epaliyana et al., 2021).

Generating multilingual questions and answers at scale.Recent studies explore the generation and utilization of synthetic multilingual question-answer (QA) pairs to improve language models'performance in multilingual and cross-lingual question answering (Abulkhanov et al., 2023; Asai et al., 2021; Chi et al., 2020; Kumar et al., 2019; Li and Callison-Burch, 2023; Riabi et al., 2021). One approach is to translate existing monolingual questions and/or answers into other languages (Asai et al., 2021). Another involves using Question Generation (QG) models to produce synthetic questions in a cross-lingual fashion based on answers and/or source texts (Chi et al., 2020; Kumar et al., 2019; Riabi et al., 2021). Recent efforts also focus on jointly generating questions and answers in multiple languages for greater flexibility (Li and Callison-Burch, 2023; Shakeri et al., 2021). For example, Shakeri et al. (2021) finetune a pretrained multilingual T5 model (Xue et al., 2020) on a mixture of a QA generation task and a multilingual masked language modeling task to produce synthetic QA pairs in multiple languages. These efforts generally show that language models trained on synthetic QA pairs demonstrate improved performance on multilingual QA and information retrieval benchmarks.

### Alignment

Instruction Following.Synthetic data can serve as a promising approach for training instruction-following models, particularly in scenarios where real-world data is scarce, expensive, or challenging to obtain. Self-instruct (Wang et al., 2022a) and Stanford Alpaca (Taori et al., 2023) are both using LLMs to generate instruction following data which covers a wide range of scenarios. They first pick a small set of "seed instruction following samples" and then ask the LLMs to imitate the format to generate more demonstrations. One concern of this type of method is how to keep the generated data high quality, which involves the complexity of queries (Liu et al., 2023d), the diversity of semantics (Ding et al., 2023), and the scale of the synthetic dataset (Yuan et al., 2023). To this end, Xu et al. (2023) propose Evol-Instruct which adds complexity to simple instructions via prompting. Mukherjee et al. (2023) leverage LLMs to revise the instructions and responses iteratively to include high-quality explanation traces in the FLAN dataset (Wei et al., 2022), and they find the trained model has improved performance in many NLP tasks. UltraChat (Ding et al., 2023) is large-scale and multi-round synthetic dialogue dataset, which is generated by two separate ChatGPT Turbo API models--one serves as the user role while the other serves as the assistant. They instruct the user model with carefully designed prompts to mimic real human user behaviors.

Many language models are supervised finetuned to learn how to follow instructions, but in learning this behavior, they may inadvertently also learn to be _sycophantic_(Perez et al., 2023), tailoring their responses to follow a user's viewpoint, even if that viewpoint is not objectively correct (Wei et al., 2023b). Sharma et al. (2024) find evidence that the preference models (i.e., the reward model used for RLHF training) and even humans prefer sycophantic responses sometimes. On this front, Wei et al. (2023b) generates synthetic data to encourage models to be robust to user opinions and adds these data in a finetuning step to reduce sycophantic behavior on held-out prompts.

Mitigating hallucination.Many widely-used language models utilize supervised finetuning (SFT) to learn to align their interactions with users (Wang et al., 2022b; Zhang et al., 2023b). In particular, there exist many methods of generating synthetic SFT data that can improve capabilities such as reasoning and alignment (Wei et al., 2023a,b). It has been shown, however, that these synthetic data can induce hallucinations into language models by containing nontrivial amounts of hallucinated answers or by forcing models to learn to answer questions that they do not know the answer to (Zhang et al., 2023c). These cases demonstrate that synthetic data, if not applied correctly, can actually increase hallucinations in language models.

On the other hand, recent work has also shown promising results in mitigating hallucinations using synthetic data. For example, GPT-4 (OpenAI, 2023) was trained using a reward model that leveraged synthetic hallucination data in order to perform reinforcement learning (Zhang et al.,2023c). This method resulted in a significant improvement in performance on the TruthfulQA (Lin et al., 2022) dataset (Zhang et al., 2023c). Similarly, Jones et al. (2023) designed a synthetic task where hallucinations can be readily evaluated, utilizing this task to optimize LIM outputs by learning a continuous postfix via prefix-tuning. Tian et al. (2023) uses automated fact-checking and confidence scores to rank factuality scores of model response pairs, which are then used to finetune language models with DPO (Rafailov et al., 2023) to improve their factuality. Continued research in using synthetic data to mitigate hallucinations is still limited, however, by the lack of synthetic tasks for which hallucinations can be scalably evaluated.

Aligning with shared human preference and values.Directly finetuning on value-aligned or human-preferred data is a straightforward method for aligning language models, but this method often requires substantial human annotation, which can be prohibitively expensive at scale. Additionally, such annotation frequently exhibits varying styles and inconsistent quality, particularly in the case of poorly annotated samples at the lower end of the quality spectrum (Gilardi et al., 2023b; Meta, 2023). To address these practical challenges, an advanced technique known as "reinforcement learning from human feedback (RLHF)" has been proposed (Christiano et al., 2017; Leike et al., 2018; Ouyang et al., 2022). This approach involves training a reward model with human data to act as a proxy of human judgment, which guides the optimization of the LM generation policy.

Recent studies have proposed a mixture of synthetic data and real human data to train more robust reward models (Gao et al., 2023). Constitutional AI (Bai et al., 2022) proposes to use a small set of principles to steer the AI generated critiques and feedback, and use such synthetic data to replace the real human data in the typical RLHF pipeline. The model trained with this RLAIF (i.e., reinforcement learning from AI feedback) method shows similar strong performance as RLHF baselines. In general, synthetic data offers a powerful solution for human values and preferences alignment by allowing researchers to generate large-scale, diverse, and controlled training datasets in a low-cost way (Cui et al., 2023; Ganguli et al., 2022). By simulating a wide range of scenarios involving ethical dilemmas (Perez et al., 2022), social interactions (Liu et al., 2023c), and cultural norms (Ziems et al., 2023), synthetic data enables comprehensive and systematic testing of AI models' alignment with human values (Askell et al., 2021). This approach helps identify and mitigate issues related to bias (Liu et al., 2021; Ntoutsi et al., 2020), fairness (Landers and Behrend, 2023; Zhao et al., 2018), and unintended consequences before AI systems are deployed in real-world settings (Ye et al., 2024).

However, it is important to acknowledge that low-fidelity synthetic human preference data might be limited in accurately reflecting nuanced human judgment (Argyle et al., 2023). Consequently, the resulting models may be less robust under "jail-breaking attacks" (Deshpande et al., 2023; Huang et al., 2023a), and may reveal strategically deceptive behavior even through safety training (Everitt et al., 2021; Pan et al., 2022; Steinhardt, 2022). To mitigate these risks, researchers must continuously refine and improve the quality and diversity of synthetic data, incorporating more complex and comprehensive scenarios that better capture the intricacies of human values and preferences. Additionally, combining synthetic data with real-world data, and creating synthetic data in an interactive environment which can be synced with the real world, are promising remedies. As the need for effective AI governance and regulation grows, synthetic data will play an increasingly vital role in enabling scalable oversight mechanisms that promote trust, accountability, and the development of AI technologies that are aligned with human values and societal expectations.

## 3 Synthetic Data in Evaluation

Synthetic data is widely used in evaluations of different perspectives:

Factuality.AI systems may generate information or responses that are not grounded in factual knowledge or data, leading to the creation of misleading or false content, formally known as _hallucination_(Ji et al., 2023). Factuality evaluation aims to ensure the consistency of the knowledge in the AI system's output with the knowledge provided by its training data and knowledge base (Ji et al., 2023; Zhang et al., 2023c). Early statistical-based hallucination evaluation methods relied on n-grams to directly calculate the overlap of vocabulary between the input and output content (Dhingra et al., 2019; Wang et al., 2020). However, these methods have limitations, as they only consider lexical overlap and do not account for semantics or sentence meaning (Ji et al., 2023), making them unsuitable for evaluating more complex forms of hallucination. Subsequent assurance methods shifted from statistical approaches to model-based methods, which are more robust compared to token-difference-based methods (Honovich et al., 2021). While these model-based evaluation methods are more advanced than their predecessors, they still have limitations. For example, the models can only output the degree of hallucination and may struggle to pinpoint specific errors (Falke et al., 2019). Feng et al. (2023a) propose to combine LLMs generation with random walks on knowledge graphs to generate synthetic evaluation data for factuality, which is aware of entities and relations on the graphs. Wei et al. (2024) created a synthetic dataset called LongFact for long-form factuality evaluation and used Google Search as the grounding source and LLM for the automated judgement, to achieve human-level accuracy but with significantly lower cost (Min et al., 2023).

Safety.Red teaming is a powerful technique for evaluating the safety and robustness of AI models (Casper et al., 2023b; Ganguli et al., 2022). By generating diverse and realistic scenarios designed to elicit unaligned or harmful outputs (Casper et al., 2023a), red teaming can expose vulnerabilities and weaknesses in AI systems (Perez et al., 2022). For example, Perez et al. (2023) use LMs to generate datasets for evaluating the behavior of other LMs. They end up producing 154 high-quality datasets which are verified by humans, and discover new cases of inverse scaling where LMs get worse with size. Hubinger et al. (2024) leverage synthetic data to trigger backdoor attacks to LMs at scale; they find LMs can exhibit deceptive behavior and create a false impression of safety under such attacks, and standard "safety training" could not remove such deception easily. These methods demonstrate the feasibility of using AI assistance to scale up human oversight (Bowman et al., 2022) over complex problems and unseen domains.

Assisting human evaluation.Recent studies have shown that in many cases, synthetic judgements from large-scale LMs (LLMs) can serve as qualified, fast, and low-cost alternatives to actual human evaluation (Gilardi et al., 2023a). Using GPT-4 as the judge, Alpaca Eval (Li et al., 2023b) and MT Bench (Zheng et al., 2023) are two popular benchmarks that measure the comprehensive abilities of LM-based ChatBot. In coding tasks, synthetic environment is a common choice to aid human evaluation, as humans can make the assessment more efficiently via actual executions and analysis on running logs. Gu et al. (2024) propose CRUXEval, a code execution reasoning benchmark consisting of 800 Python functions generated by CodeLAMA-34B. Similarly, Liu et al. (2024a) introduce CodeMind, a framework to gauge the code reasoning abilities of LLMs on Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). All these evaluations based on synthetic data show strong correlation with real human judgements.

## 4 Challenges and Limitations of Synthetic Data

While synthetic data offers numerous benefits and applications, it is crucial to acknowledge and address the potential challenges and limitations associated with its use. This section delves into three significant concerns surrounding synthetic data:

Misuse of synthetic data might proliferate misinformation.The potential misuse of synthetic data is a significant concern that must be addressed to ensure the responsible development of AI systems. Current AI models become increasingly capable of generating human-like data ranging from text (Gemini-Team et al., 2023, 2024), images (Ramesh et al., 2022; Saharia et al., 2022), songs 3, to even videos (e.g., OpenAI SORA 4). This can be particularly dangerous when synthetic data is used to impersonate real people, manipulate public opinion, or influence political processes. Moreover, the dissemination of synthetic data-driven misinformation can erode trust in legitimate information sources, making it increasingly difficult for people to distinguish between truth and falsehood (Byman et al., 2023; Rid, 2020). To mitigate these risks, it is crucial for researchers, developers, and policymakers to establish clear guidelines and best practices for the ethical generation and use of synthetic data, including robust mechanisms for detecting and countering synthetic misinformation (Groh et al., 2022). By proactively addressing these challenges, we can harness the benefits of synthetic data while minimizing its potential for harm.

Footnote 3: Make songs with Suno AI: [https://app.suno.ai/](https://app.suno.ai/)

Footnote 4: OpenAI Sora: [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)

Synthetic data might cause ambiguity in AI alignment.The increasing use of synthetic data in aligning AI models (e.g., Constitutional AI (Bai et al., 2022)) can introduce significant ambiguity and uncertainty. The goal of AI alignment is to ensure that AI systems behave in ways that are aligned with human values and intentions. However, synthetic data, which is artificially generated rather than collected from real-world sources, may not accurately represent the nuances and complexities of human values and preferences (Zhou et al., 2024). This discrepancy can lead to AI models learning from data that is biased (Feng et al., 2023; Liu et al., 2021), ungrounded (Liu et al., 2022; Patel and Pavlick, 2022), or misrepresentative of real-world scenarios (Ji et al., 2023; Weidinger et al., 2021). As a result, AI systems trained on synthetic data may exhibit behaviors that are misaligned with human expectations, potentially leading to unintended consequences or even harmful actions (Anderljung et al., 2023; Zou et al., 2023). Moreover, the ambiguity introduced by synthetic data can make it challenging to interpret and understand the decision-making processes of AI models (Lightman et al., 2023), further complicating the task of ensuring alignment. To mitigate these risks, it is crucial for researchers to carefully consider the limitations and potential drawbacks of using synthetic data in alignment research and to develop robust methods for validating and testing AI models trained on such data.

Training with synthetic data makes evaluation decontamination harder.The use of synthetic data in model training poses significant challenges to fair evaluation. Evaluation benchmarks are often created by referring to public text sources, such as coursework websites or forums. Consequently, it is arguable that all publicly available benchmark test cases might occasionally be included in the pre-training data of LLMs (Gao et al., 2021; Hoffmann et al., 2022). The use of synthetic data exacerbates this issue rather than mitigating it. Although the community has proposed several techniques to detect such evaluation contamination, such as _min-k9 prob_(Shi et al., 2023), which checks the probabilities of \(k\) long-tail tokens, these token-level decontamination methods are inadequate whenthe model is trained with synthetic data. Synthetic data might include rephrased versions of the benchmark data (Mattern et al., 2023; Oren et al., 2023), rendering token-level decontamination ineffective. In addition to developing more advanced evaluation contamination detection techniques, we recommend that model developers invest in creating and maintaining in-house and protected evaluation benchmarks. These proprietary benchmarks should be carefully safeguarded to prevent leakage and ensure the integrity of the evaluation process.

## 5 Directions for Future Work

As the field of synthetic data continues to evolve, there are several promising directions for future research and development. This section outlines three key areas that warrant further exploration:

Synthetic data scaling.The impressive performance of many over-trained small language models (e.g., Mistral series models (Jiang et al., 2023), and Gemma series models (Gemma-Team et al., 2024), _inter alia_) demonstrates the necessity of training with large amount of tokens (even passing the compute-optimal chinchilla law (Rae et al., 2021)). However, whether we have similar conclusions on the training with synthetic data is still an open question, as the quality of synthetic data may not be as consistent as real-world data (Yu et al., 2024). Future research should investigate the scaling laws for synthetic data and determine the optimal balance between the quantity and quality of synthetic samples. This exploration could help us understand the most effective strategies for leveraging synthetic data in training large-scale language models, potentially leading to more efficient and cost-effective approaches (Muennighoff et al., 2024).

Further improving quality and diversity of synthetic data.While existing methods for generating synthetic data have shown promise, there is still room for improvement in terms of creating high-quality, attributed synthetic samples that closely mimic real-world data. Future research should focus on developing new advanced techniques (or based on existing ones such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) or Diffusion Models (Ho et al., 2020), _inter alia_) that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. Additionally, researchers should explore methods that can incorporate domain-specific knowledge to ensure the generated data adheres to the underlying constraints and patterns present in the target domain (e.g., via Retrieval Augmented Generation (RAG) (Borgeaud et al., 2022; Lewis et al., 2020)) while maintaining the data quality. By advancing the state-of-the-art in attributed synthetic data generation, we can unlock new opportunities for privacy-preserving analysis (Assefa et al., 2020), and model training across various fields, from healthcare (e.g., synthetic medical images (Frid-Adar et al., 2018; Wei et al., 2019)) and finance (e.g., simulated trading trajectories (Zheng et al., 2022)) to social sciences (Argyle et al., 2023; Park et al., 2023) and beyond.

Towards high-fidelity and more efficient scalable oversight.As AI models become increasingly complex and autonomous, it becomes challenging to monitor and assess their behavior using traditional oversight methods that rely on human supervision or real-world data (Amodei et al., 2016). Future research should explore the use of synthetic data for high-fidelity scalable oversight of these advanced systems. Existing methods typically simulate a certain scenario in social iterations, such as debate (Leike et al., 2018), reflection (Zhang et al., 2023), or revisions (Liu et al., 2023) to obtain synthetic data, while new approaches could cover more comprehensive scenarios and more modalities (Sun et al., 2023), as recent studies have found many issues of simulation that only coversa narrowed down (Cheng et al., 2023) or over-simplified (Zhou et al., 2024) scenes. Looking forward, another growing direction could be how to achieve scalable oversight more efficiently--given that we have the full control over the synthetic data generation, we can probably provide more targeted oversights with less synthetic data. As the need for effective AI governance and regulation grows, synthetic data will play an increasingly vital role in enabling more trustworthy scalable oversight mechanisms that promote robust, accountable, and safe deployment of AI technologies for the benefit of society (Askell et al., 2021; Bowman et al., 2022).

The emergent self-improvement capability.We typically choose the most capable model to generate synthetic data, as its generation is of higher quality. However, an intriguing question arises: can a model generate synthetic data that is better than the data it was trained on, thus enabling it to improve itself? This concept of self-improvement through synthetic data generation is an exciting avenue for future research. If a model can generate higher-quality data than its original training set, it could potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data (Chen et al., 2024). This self-improvement capability could lead to the emergence of more advanced AI systems that can autonomously refine their skills and knowledge over time (Burns et al., 2023; Huang et al., 2023). Although recent work shows encouraging progress in this direction (Chen et al., 2024; Yuan et al., 2024), the upper bound of self-improvement and the underlying reasons for its effectiveness remain open questions. Future research should investigate the theoretical underpinnings and practical feasibility of self-improvement through synthetic data generation in more diverse scenarios, examining the necessary conditions, potential limitations, and associated risks. By unlocking the potential of emergent self-improvement capabilities, we could enable more adaptable, efficient, and autonomous learning processes (LeCun, 2022).

## 6 Conclusion

Synthetic data has emerged as a promising solution to address the challenges of data scarcity, privacy concerns, and high costs in AI development. By generating realistic and diverse datasets, synthetic data enables the training and evaluation of AI models at scale across various domains. As we approach human-level or even superhuman-level intelligence, obtaining synthetic data becomes even more crucial, given that models need better-than-average-human quality data to progress. However, ensuring the factuality, fidelity, and lack of bias in synthetic data remains a critical challenge.

Future research directions on synthetic data could focus on improving the fidelity and controllability of generative models and developing standardized evaluation and contamination protocols and tools. We could also explore the integration of synthetic data with other techniques and its application in other domains. Despite the challenges, the potential benefits of synthetic data in advancing AI research are significant. By leveraging synthetic data responsibly and effectively, we can build more powerful, inclusive, and trustworthy AI systems that benefit society as a whole.

## References

* Abay et al. (2019) N. C. Abay, Y. Zhou, M. Kantarcioglu, B. Thuraisingham, and L. Sweeney. Privacy preserving synthetic data release using deep learning. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10-14, 2018, Proceedings, Part I 18_, pages 510-526. Springer, 2019.
* Abulkhanov et al. (2019) D. Abulkhanov, N. Sorokin, S. Nikolenko, and V. Malykh. Lapca: Language-agnostic pretraining with cross-lingual alignment. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2098-2102, 2023.
* Ahn et al. [2022] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. _ArXiv preprint_, abs/2204.01691, 2022. URL [https://arxiv.org/abs/2204.01691](https://arxiv.org/abs/2204.01691).
* Amodei et al. [2016] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane. Concrete problems in ai safety. _ArXiv preprint_, abs/1606.06565, 2016. URL [https://arxiv.org/abs/1606.06565](https://arxiv.org/abs/1606.06565).
* Anderljung et al. [2023] M. Anderljung, J. Barnhart, J. Leung, A. Korinek, C. O'Keefe, J. Whittlestone, S. Avin, M. Brundage, J. Bullock, D. Cass-Beggs, et al. Frontier ai regulation: Managing emerging risks to public safety. _ArXiv preprint_, abs/2307.03718, 2023. URL [https://arxiv.org/abs/2307.03718](https://arxiv.org/abs/2307.03718).
* Argyle et al. [2023] L. P. Argyle, E. C. Busby, N. Fulda, J. R. Gubler, C. Rytting, and D. Wingate. Out of one, many: Using language models to simulate human samples. _Political Analysis_, 31(3):337-351, 2023.
* Asai et al. [2021] A. Asai, X. Yu, J. Kasai, and H. Hajishirzi. One question answering model for many languages with cross-lingual dense passage retrieval. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 7547-7560, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html).
* Askell et al. [2021] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. _ArXiv preprint_, abs/2112.00861, 2021. URL [https://arxiv.org/abs/2112.00861](https://arxiv.org/abs/2112.00861).
* Assefa et al. [2020] S. A. Assefa, D. Devrovic, M. Mahfouz, R. E. Tillman, P. Reddy, and M. Veloso. Generating synthetic data in finance: opportunities, challenges and pitfalls. In _Proceedings of the First ACM International Conference on AI in Finance_, pages 1-8, 2020.
* Azerbayev et al. [2023] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck. Lemma: An open language model for mathematics. _ArXiv preprint_, abs/2310.10631, 2023. URL [https://arxiv.org/abs/2310.10631](https://arxiv.org/abs/2310.10631).
* Babbar and Scholkopf [2019] R. Babbar and B. Scholkopf. Data scarcity, robustness and extreme multi-label classification. _Machine Learning_, 108(8):1329-1351, 2019.
* Bai et al. [2022] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _ArXiv preprint_, abs/2212.08073, 2022. URL [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073).
* Barbierato et al. [2022] E. Barbierato, M. L. D. Vedova, D. Tessera, D. Toti, and N. Vanoli. A methodology for controlling bias and fairness in synthetic data generation. _Applied Sciences_, 12(9):4619, 2022.
* Bi et al. [2021] W. Bi, H. Li, and J. Huang. Data augmentation for text generation without any augmented data. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 2223-2237, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.173. URL [https://aclanthology.org/2021.acl-long.173](https://aclanthology.org/2021.acl-long.173).
* Borgeaud et al. [2021] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 2206-2240. PMLR, 2022. URL [https://proceedings.mlr.press/v162/borgeaud22a.html](https://proceedings.mlr.press/v162/borgeaud22a.html).
* Borisov et al. (2022) V. Borisov, K. Sessler, T. Leemann, M. Pawelczyk, and G. Kasneci. Language models are realistic tabular data generators. _ArXiv preprint_, abs/2210.06280, 2022. URL [https://arxiv.org/abs/2210.06280](https://arxiv.org/abs/2210.06280).
* Borkman et al. (2021) S. Borkman, A. Crespi, S. Dhakad, S. Ganguly, J. Hogins, Y. C. Jhang, M. Kamalzadeh, B. Li, S. Leal, P. Parisi, C. Romero, W. Smith, A. Thaman, S. Warren, and N. Yadav. Unity perception: Generate synthetic data for computer vision. _ArXiv preprint_, abs/2107.04259, 2021. URL [https://arxiv.org/abs/2107.04259](https://arxiv.org/abs/2107.04259).
* Bowman et al. (2022) S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner, K. Lukosiute, A. Askell, A. Jones, A. Chen, et al. Measuring progress on scalable oversight for large language models. _ArXiv preprint_, abs/2211.03540, 2022. URL [https://arxiv.org/abs/2211.03540](https://arxiv.org/abs/2211.03540).
* Burns et al. (2023) C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet, M. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. _ArXiv preprint_, abs/2312.09390, 2023. URL [https://arxiv.org/abs/2312.09390](https://arxiv.org/abs/2312.09390).
* Byman et al. (2023) D. L. Byman, C. Gao, C. Meserole, and V. Subrahmanian. _Deepfakes and international conflict_. Brookings Institution, 2023.
* Carbune et al. (2024) V. Carbune, H. Mansoor, F. Liu, R. Aralikatte, G. Baechler, J. Chen, and A. Sharma. Chart-based reasoning: Transferring capabilities from llms to vlms. _ArXiv preprint_, abs/2403.12596, 2024. URL [https://arxiv.org/abs/2403.12596](https://arxiv.org/abs/2403.12596).
* Casper et al. (2023a) S. Casper, T. Bu, Y. Li, J. Li, K. Zhang, K. Hariharan, and D. Hadfield-Menell. Red teaming deep neural networks with feature synthesis tools. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023a.
* Casper et al. (2023b) S. Casper, J. Lin, J. Kwon, G. Culp, and D. Hadfield-Menell. Explore, establish, exploit: Red teaming language models from scratch. _ArXiv preprint_, abs/2306.09442, 2023b. URL [https://arxiv.org/abs/2306.09442](https://arxiv.org/abs/2306.09442).
* Caswell et al. (2019) I. Caswell, C. Chelba, and D. Grangier. Tagged back-translation. In _Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)_, pages 53-63, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5206. URL [https://aclanthology.org/W19-5206](https://aclanthology.org/W19-5206).
* Chauhan et al. (2022) S. Chauhan, S. Saxena, and P. Daniel. Improved unsupervised neural machine translation with semantically weighted back translation for morphologically rich and low resource languages. _Neural Processing Letters_, 54(3):1707-1726, 2022.
* Chen et al. (2024) Z. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language models to strong language models, 2024.
* Cheng et al. (2023) M. Cheng, T. Piccardi, and D. Yang. CoMPosT: Characterizing and evaluating caricature in LLM simulations. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 10853-10875, Singapore, Dec. 2023.

Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.669. URL [https://aclanthology.org/2023.emnlp-main.669](https://aclanthology.org/2023.emnlp-main.669).
* Chern et al. (2023) E. Chern, H. Zou, X. Li, J. Hu, K. Feng, J. Li, and P. Liu. Generative ai for math: Abel. [https://github.com/GAIR-NLP/abel](https://github.com/GAIR-NLP/abel), 2023.
* Chi et al. (2020) Z. Chi, L. Dong, F. Wei, W. Wang, X. Mao, and H. Huang. Cross-lingual natural language generation via pre-training. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 7570-7577. AAAI Press, 2020. URL [https://aaai.org/ojs/index.php/AAAI/article/view/6256](https://aaai.org/ojs/index.php/AAAI/article/view/6256).
* Christiano et al. (2017) P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 4299-4307, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df24d0d0cd4e49-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df24d0d0cd4e49-Abstract.html).
* Cui et al. (2023) G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.
* Dahmen and Cook (2019) J. Dahmen and D. Cook. Synsys: A synthetic data generation system for healthcare applications. _Sensors_, 19(5):1181, 2019.
* Deshpande et al. (2023) A. Deshpande, V. Murahari, T. Rajpurohit, A. Kalyan, and K. Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. _ArXiv preprint_, abs/2304.05335, 2023. URL [https://arxiv.org/abs/2304.05335](https://arxiv.org/abs/2304.05335).
* Dhingra et al. (2019) B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and W. Cohen. Handling divergent reference texts when evaluating table-to-text generation. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4884-4895, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1483. URL [https://aclanthology.org/P19-1483](https://aclanthology.org/P19-1483).
* Ding et al. (2023) N. Ding, Y. Chen, B. Xu, Y. Qin, Z. Zheng, S. Hu, Z. Liu, M. Sun, and B. Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _ArXiv preprint_, abs/2305.14233, 2023. URL [https://arxiv.org/abs/2305.14233](https://arxiv.org/abs/2305.14233).
* Edunov et al. (2018) S. Edunov, M. Ott, M. Auli, and D. Grangier. Understanding back-translation at scale. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 489-500, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1045. URL [https://aclanthology.org/D18-1045](https://aclanthology.org/D18-1045).
* El Emam et al. (2020) K. El Emam, L. Mosquera, and R. Hoptroff. _Practical synthetic data generation: balancing privacy and the broad availability of data_. O'Reilly Media, 2020.
* Epaliyama et al. (2021) K. Epaliyama, S. Ranathunga, and S. Jayasena. Improving back-translation with iterative filtering and data selection for sinhala-english nmt. In _2021 Moratuwa Engineering Research Conference (MERCon)_, pages 438-443. IEEE, 2021.
* Everitt et al. (2021) T. Everitt, M. Hutter, R. Kumar, and V. Krakovna. Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective. _Synthese_, 198(Suppl 27):6435-6467, 2021.

* Falke et al. [2019] T. Falke, L. F. R. Ribeiro, P. A. Utama, I. Dagan, and I. Gurevych. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2214-2220, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1213. URL [https://aclanthology.org/P19-1213](https://aclanthology.org/P19-1213).
* Feng et al. [2023a] S. Feng, V. Balachandran, Y. Bai, and Y. Tsvetkov. FactKB: Generalizable factuality evaluation using language models enhanced with factual knowledge. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 933-952, Singapore, Dec. 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.59. URL [https://aclanthology.org/2023.emnlp-main.59](https://aclanthology.org/2023.emnlp-main.59).
* Feng et al. [2023b] S. Feng, C. Y. Park, Y. Liu, and Y. Tsvetkov. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair nlp models. _ArXiv preprint_, abs/2305.08283, 2023b. URL [https://arxiv.org/abs/2305.08283](https://arxiv.org/abs/2305.08283).
* Frid-Adar et al. [2018] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, and H. Greenspan. Synthetic data augmentation using gan for improved liver lesion classification. In _2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)_, pages 289-293. IEEE, 2018.
* Ganguli et al. [2022] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _ArXiv preprint_, abs/2209.07858, 2022. URL [https://arxiv.org/abs/2209.07858](https://arxiv.org/abs/2209.07858).
* Gao et al. [2021] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. _ArXiv preprint_, abs/2101.00027, 2021. URL [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027).
* Gao et al. [2023] L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In _International Conference on Machine Learning_, pages 10835-10866. PMLR, 2023.
* Gemini-Team et al. [2023] Gemini-Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. _ArXiv preprint_, abs/2312.11805, 2023. URL [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805).
* Gemini-Team et al. [2024] Gemini-Team, M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _ArXiv preprint_, abs/2403.05530, 2024. URL [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530).
* Gemma-Team et al. [2024] Gemma-Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Riviere, M. S. Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. _ArXiv preprint_, abs/2403.08295, 2024. URL [https://arxiv.org/abs/2403.08295](https://arxiv.org/abs/2403.08295).
* Gilardi et al. [2023a] F. Gilardi, M. Alizadeh, and M. Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. _Proceedings of the National Academy of Sciences_, 120(30):e2305016120, 2023a. doi: 10.1073/pnas.2305016120. URL [https://www.pnas.org/doi/abs/10.1073/pnas.2305016120](https://www.pnas.org/doi/abs/10.1073/pnas.2305016120).
* Gilardi et al. [2020] F. Gilardi, M. Alizadeh, and M. Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. _Proceedings of the National Academy of Sciences_, 120(30):e2305016120, 2023b.
* Goodfellow et al. [2020] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.

* Graca et al. [2019] M. Graca, Y. Kim, J. Schamper, S. Khadivi, and H. Ney. Generalizing back-translation in neural machine translation. In _Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)_, pages 45-52, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5205. URL [https://aclanthology.org/W19-5205](https://aclanthology.org/W19-5205).
* Groh et al. [2022] M. Groh, Z. Epstein, C. Firestone, and R. Picard. Deepfake detection by human crowds, machines, and machine-informed crowds. _Proceedings of the National Academy of Sciences_, 119(1):e2110013119, 2022.
* Gu et al. [2024] A. Gu, B. Roziere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for code reasoning, understanding and execution. _ArXiv preprint_, abs/2401.03065, 2024. URL [https://arxiv.org/abs/2401.03065](https://arxiv.org/abs/2401.03065).
* Guarnera et al. [2020] L. Guarnera, O. Giudice, and S. Battiato. Deepfake detection by analyzing convolutional traces. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 666-667, 2020.
* Gupta et al. [2021] A. Gupta, D. Bhatt, and A. Pandey. Transitioning from real to synthetic data: Quantifying the bias in model. _ArXiv preprint_, abs/2105.04144, 2021. URL [https://arxiv.org/abs/2105.04144](https://arxiv.org/abs/2105.04144).
* Haluptzok et al. [2022] P. Haluptzok, M. Bowers, and A. T. Kalai. Language models can teach themselves to program better. _ArXiv preprint_, abs/2207.14502, 2022. URL [https://arxiv.org/abs/2207.14502](https://arxiv.org/abs/2207.14502).
* Heusel et al. [2017] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6626-6637, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/8aid694707eb0fefe65871369074926d-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/8aid694707eb0fefe65871369074926d-Abstract.html).
* Ho et al. [2020] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html).
* Hoffmann et al. [2022] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. An empirical analysis of compute-optimal large language model training. _Advances in Neural Information Processing Systems_, 35:30016-30030, 2022.
* Honovich et al. [2021] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor, and O. Abend. \(q^{2}\): Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7856-7870, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.619. URL [https://aclanthology.org/2021.emnlp-main.619](https://aclanthology.org/2021.emnlp-main.619).
* Howe et al. [2017] B. Howe, J. Stoyanovich, H. Ping, B. Herman, and M. Gee. Synthetic data for social good. _ArXiv preprint_, abs/1710.08874, 2017. URL [https://arxiv.org/abs/1710.08874](https://arxiv.org/abs/1710.08874).
* Huang et al. [2023a] F. Huang, H. Kwak, and J. An. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. _ArXiv preprint_, abs/2302.07736, 2023a. URL [https://arxiv.org/abs/2302.07736](https://arxiv.org/abs/2302.07736).

* Huang et al. (2023) J. Huang, S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 1051-1068, Singapore, Dec. 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.67. URL [https://aclanthology.org/2023.emnlp-main.67](https://aclanthology.org/2023.emnlp-main.67).
* Huang et al. (2022) W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _ArXiv preprint_, abs/2207.05608, 2022. URL [https://arxiv.org/abs/2207.05608](https://arxiv.org/abs/2207.05608).
* Hubinger et al. (2024) E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T. Lanham, D. M. Ziegler, T. Maxwell, N. Cheng, et al. Sleeper agents: Training deceptive llms that persist through safety training. _ArXiv preprint_, abs/2401.05566, 2024. URL [https://arxiv.org/abs/2401.05566](https://arxiv.org/abs/2401.05566).
* Ji et al. (2023) Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys (CSUR)_, 55(12):1-38, 2023.
* Jia et al. (2021) C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 4904-4916. PMLR, 2021. URL [http://proceedings.mlr.press/v139/jia21b.html](http://proceedings.mlr.press/v139/jia21b.html).
* Jiang et al. (2023) A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. _ArXiv preprint_, abs/2310.06825, 2023. URL [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825).
* Jiang et al. (2022) Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and L. Fan. Vima: General robot manipulation with multimodal prompts. In _NeurIPS 2022 Foundation Models for Decision Making Workshop_, 2022.
* Jones et al. (2023) E. Jones, H. Palangi, C. Simoes, V. Chandrasekaran, S. Mukherjee, A. Mitra, A. Awadallah, and E. Kamar. Teaching language models to hallucinate less with synthetic tasks, 2023. URL [https://arxiv.org/abs/2310.06827](https://arxiv.org/abs/2310.06827).
* Kambhampati et al. (2024) S. Kambhampati, K. Valmeekam, L. Guan, K. Stechly, M. Verma, S. Bhambi, L. Saldyt, and A. Murthy. Llms can't plan, but can help planning in llvm-modulo frameworks. _arXiv preprint arXiv:2402.01817_, 2024.
* Kumar et al. (2019) V. Kumar, N. Joshi, A. Mukherjee, G. Ramakrishnan, and P. Jyothi. Cross-lingual training for automatic question generation. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4863-4872, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1481. URL [https://aclanthology.org/P19-1481](https://aclanthology.org/P19-1481).
* Lam et al. (2023) R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, F. Alet, S. Ravuri, T. Ewalds, Z. Eaton-Rosen, W. Hu, et al. Learning skillful medium-range global weather forecasting. _Science_, 382(6677):1416-1421, 2023.
* Landers and Behrend (2023) R. N. Landers and T. S. Behrend. Auditing the ai auditors: A framework for evaluating fairness and bias in high stakes ai predictive models. _American Psychologist_, 78(1):36, 2023.
* Laurencon et al. (2024) H. Laurencon, L. Tronchon, and V. Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. URL [https://arxiv.org/abs/2403.09029](https://arxiv.org/abs/2403.09029).

* Le et al. (2022) H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. _Advances in Neural Information Processing Systems_, 35:21314-21328, 2022.
* LeCun (2022) Y. LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. _Open Review_, 62, 2022.
* Lee et al. (2023) K. Lee, M. Joshi, I. R. Turc, H. Hu, F. Liu, J. M. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang, and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In _International Conference on Machine Learning_, pages 18893-18912. PMLR, 2023.
* Leike et al. (2018) J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, and S. Legg. Scalable agent alignment via reward modeling: a research direction. _ArXiv preprint_, abs/1811.07871, 2018. URL [https://arxiv.org/abs/1811.07871](https://arxiv.org/abs/1811.07871).
* Lewis et al. (2020) P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html).
* Lewkowycz et al. (2022) A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models, 2022. URL [https://arxiv.org/abs/2206.14858](https://arxiv.org/abs/2206.14858).
* Li and Callison-Burch (2023) B. Li and C. Callison-Burch. Paxqa: Generating cross-lingual question answering examples at training scale. _ArXiv preprint_, abs/2304.12206, 2023. URL [https://arxiv.org/abs/2304.12206](https://arxiv.org/abs/2304.12206).
* Li et al. (2024) C. Li, W. Wang, J. Hu, Y. Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng. Common 7b language models already possess strong math capabilities. _ArXiv preprint_, abs/2403.04706, 2024. URL [https://arxiv.org/abs/2403.04706](https://arxiv.org/abs/2403.04706).
* Li et al. (2023a) L. Li, R. Carver, I. Lopez-Gomez, F. Sha, and J. Anderson. Seeds: Emulation of weather forecast ensembles with diffusion models. _ArXiv preprint_, abs/2306.14066, 2023a. URL [https://arxiv.org/abs/2306.14066](https://arxiv.org/abs/2306.14066).
* Li et al. (2023b) X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpaceval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023b.
* Liang et al. (2022) J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. _ArXiv preprint_, abs/2209.07753, 2022. URL [https://arxiv.org/abs/2209.07753](https://arxiv.org/abs/2209.07753).
* Liao et al. (2021) B. Liao, S. Khadivi, and S. Hewavitharana. Back-translation for large-scale multilingual machine translation. In _Proceedings of the Sixth Conference on Machine Translation_, pages 418-424, Online, 2021. Association for Computational Linguistics. URL [https://aclanthology.org/2021.wmt-1.50](https://aclanthology.org/2021.wmt-1.50).
* Lightman et al. (2023) H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let's verify step by step. _ArXiv preprint_, abs/2305.20050, 2023. URL [https://arxiv.org/abs/2305.20050](https://arxiv.org/abs/2305.20050).

* Lin et al. (2022) S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL [https://aclanthology.org/2022.acl-long.229](https://aclanthology.org/2022.acl-long.229).
* Liu et al. (2024a) C. Liu, S. D. Zhang, and R. Jabbarvand. Codemind: A framework to challenge large language models for code reasoning. _ArXiv preprint_, abs/2402.09664, 2024a. URL [https://arxiv.org/abs/2402.09664](https://arxiv.org/abs/2402.09664).
* Liu et al. (2023a) F. Liu, J. Eisenschlos, F. Piccinno, S. Krichene, C. Pang, K. Lee, M. Joshi, W. Chen, N. Collier, and Y. Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 10381-10399, 2023a.
* Liu et al. (2023b) F. Liu, F. Piccinno, S. Krichene, C. Pang, K. Lee, M. Joshi, Y. Altun, N. Collier, and J. Eisenschlos. Matcha: Enhancing visual language pretraining with math reasoning and chart derendering. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 12756-12770, 2023b.
* Liu and Yao (2024) H. Liu and A. C.-C. Yao. Augmenting math word problems via iterative question composing. _ArXiv preprint_, abs/2401.09003, 2024. URL [https://arxiv.org/abs/2401.09003](https://arxiv.org/abs/2401.09003).
* Liu et al. (2024b) H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024b.
* Liu et al. (2021) R. Liu, C. Jia, J. Wei, G. Xu, L. Wang, and S. Vosoughi. Mitigating political bias in language models through reinforced calibration. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 14857-14866. AAAI Press, 2021. URL [https://ojs.aaai.org/index.php/AAAI/article/view/17744](https://ojs.aaai.org/index.php/AAAI/article/view/17744).
* Liu et al. (2022) R. Liu, J. Wei, S. S. Gu, T.-Y. Wu, S. Vosoughi, C. Cui, D. Zhou, and A. M. Dai. Mind's eye: Grounded language model reasoning through simulation. _ArXiv preprint_, abs/2210.05359, 2022. URL [https://arxiv.org/abs/2210.05359](https://arxiv.org/abs/2210.05359).
* Liu et al. (2023c) R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi. Training socially aligned language models in simulated human society. _ArXiv preprint_, abs/2305.16960, 2023c. URL [https://arxiv.org/abs/2305.16960](https://arxiv.org/abs/2305.16960).
* Liu et al. (2023d) W. Liu, W. Zeng, K. He, Y. Jiang, and J. He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. _ArXiv preprint_, abs/2312.15685, 2023d. URL [https://arxiv.org/abs/2312.15685](https://arxiv.org/abs/2312.15685).
* Lu et al. (2023) Y. Lu, M. Shen, H. Wang, X. Wang, C. van Rechem, and W. Wei. Machine learning for synthetic data generation: a review. _ArXiv preprint_, abs/2302.04062, 2023. URL [https://arxiv.org/abs/2302.04062](https://arxiv.org/abs/2302.04062).
* Lucini (2021) F. Lucini. The real deal about synthetic data. _MIT Sloan Management Review_, 63(1):1-4, 2021.
* Luo et al. (2023) H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _ArXiv preprint_, abs/2308.09583, 2023a. URL [https://arxiv.org/abs/2308.09583](https://arxiv.org/abs/2308.09583).

* Luo et al. (2023) Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang. Wizardcoder: Empowering code large language models with evol-instruct. _ArXiv preprint_, abs/2306.08568, 2023b. URL [https://arxiv.org/abs/2306.08568](https://arxiv.org/abs/2306.08568).
* Marie et al. (2020) B. Marie, R. Rubino, and A. Fujita. Tagged back-translation revisited: Why does it really work? In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 5990-5997, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.532. URL [https://aclanthology.org/2020.acl-main.532](https://aclanthology.org/2020.acl-main.532).
* Masry et al. (2023) A. Masry, P. Kavehzadeh, X. L. Do, E. Hoque, and S. Joty. UniChart: A universal vision-language pretrained model for chart comprehension and reasoning. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 14662-14684, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.906. URL [https://aclanthology.org/2023.emnlp-main.906](https://aclanthology.org/2023.emnlp-main.906).
* Mattern et al. (2023) J. Mattern, F. Mireshgallah, Z. Jin, B. Scholkopf, M. Sachan, and T. Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. _ArXiv preprint_, abs/2305.18462, 2023. URL [https://arxiv.org/abs/2305.18462](https://arxiv.org/abs/2305.18462).
* Meng et al. (2022) Y. Meng, J. Huang, Y. Zhang, and J. Han. Generating training data with language models: Towards zero-shot language understanding. _Advances in Neural Information Processing Systems_, 35:462-477, 2022.
* Meta (2023) Meta. Meta and microsoft introduce the next generation of llama. [https://ai.meta.com/blog/llama-2](https://ai.meta.com/blog/llama-2), 2023.
* Min et al. (2023) S. Min, K. Krishna, X. Lyu, M. Lewis, W.-t. Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. _arXiv preprint arXiv:2305.14251_, 2023.
* Muennighoff et al. (2024) N. Muennighoff, A. Rush, B. Barak, T. Le Scao, N. Tazi, A. Piktus, S. Pyysalo, T. Wolf, and C. A. Raffel. Scaling data-constrained language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Mukherjee et al. (2023) S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _ArXiv preprint_, abs/2306.02707, 2023. URL [https://arxiv.org/abs/2306.02707](https://arxiv.org/abs/2306.02707).
* Nikolenko (2021) S. I. Nikolenko. _Synthetic data for deep learning_, volume 174. Springer, 2021.
* Ntoutsi et al. (2020) E. Ntoutsi, P. Fafalios, U. Gadiraju, V. Iosifidis, W. Nejdl, M.-E. Vidal, S. Ruggieri, F. Turini, S. Papadopoulos, E. Krasanakis, et al. Bias in data-driven artificial intelligence systems--an introductory survey. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 10(3):e1356, 2020.
* OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
* Oren et al. (2023) Y. Oren, N. Meister, N. Chatterji, F. Ladhak, and T. B. Hashimoto. Proving test set contamination in black box language models. _ArXiv preprint_, abs/2310.17623, 2023. URL [https://arxiv.org/abs/2310.17623](https://arxiv.org/abs/2310.17623).
* Ouyang et al. (2022) L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. _ArXiv preprint_, abs/2203.02155, 2022. URL [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).

* Pan et al. (2022) A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL [https://openreview.net/forum?id=JYtwGwIL7ye](https://openreview.net/forum?id=JYtwGwIL7ye).
* Park et al. (2023) J. S. Park, J. O'Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pages 1-22, 2023.
* Paster et al. (2023) K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. _ArXiv preprint_, abs/2310.06786, 2023. URL [https://arxiv.org/abs/2310.06786](https://arxiv.org/abs/2310.06786).
* Patel and Pavlick (2022) R. Patel and E. Pavlick. Mapping language models to grounded conceptual spaces. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL [https://openreview.net/forum?id=gJcEM8sxHK](https://openreview.net/forum?id=gJcEM8sxHK).
* Perez et al. (2022) E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 3419-3448, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022.emnlp-main.225](https://aclanthology.org/2022.emnlp-main.225).
* Perez et al. (2023) E. Perez, S. Ringer, K. Lukosiute, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson, S. Kundu, S. Kadavath, et al. Discovering language model behaviors with model-written evaluations. In _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 13387-13434. Association for Computational Linguistics, 2023.
* Pham et al. (2021) H. Pham, X. Wang, Y. Yang, and G. Neubig. Meta back-translation. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=3jjmdp7Hha](https://openreview.net/forum?id=3jjmdp7Hha).
* Przystupa and Abdul-Mageed (2019) M. Przystupa and M. Abdul-Mageed. Neural machine translation of low-resource and similar languages with backtranslation. In _Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)_, pages 224-235, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5431. URL [https://aclanthology.org/W19-5431](https://aclanthology.org/W19-5431).
* Radford et al. (2021) A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021. URL [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html).
* Rae et al. (2021) J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. v. d. Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. d. M. d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. d. L. Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer,O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. URL [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).
* Rafailov et al. (2023) R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. In _NeurIPS_, 2023. URL [https://api.semanticscholar.org/CorpusID:258959321](https://api.semanticscholar.org/CorpusID:258959321).
* Ramesh et al. (2022) A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _ArXiv preprint_, abs/2204.06125, 2022. URL [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125).
* Riabi et al. (2021) A. Riabi, T. Scialom, R. Keraron, B. Sagot, D. Seddah, and J. Staiano. Synthetic data augmentation for zero-shot cross-lingual question answering. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7016-7030, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.562. URL [https://aclanthology.org/2021.emnlp-main.562](https://aclanthology.org/2021.emnlp-main.562).
* Rid (2020) T. Rid. _Active measures: The secret history of disinformation and political warfare_. Farrar, Straus and Giroux, 2020.
* Saharia et al. (2022a) C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 36479-36494. Curran Associates, Inc., 2022a. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf).
* Saharia et al. (2022b) C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022b.
* Saxton et al. (2019) D. Saxton, E. Grefenstette, F. Hill, and P. Kohli. Analysing mathematical reasoning abilities of neural models. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL [https://openreview.net/forum?id=H1gR5iR5FX](https://openreview.net/forum?id=H1gR5iR5FX).
* Schick et al. (2024) T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. _Advances in Neural Information Processing Systems_, 36, 2024.
* Sennrich et al. (2016) R. Sennrich, B. Haddow, and A. Birch. Improving neural machine translation models with monolingual data. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 86-96, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL [https://aclanthology.org/P16-1009](https://aclanthology.org/P16-1009).
* Shakeri et al. (2021) S. Shakeri, N. Constant, M. Kale, and L. Xue. Towards zero-shot multilingual synthetic question and answer generation for cross-lingual reading comprehension. In _Proceedings of the 14th International Conference on Natural Language Generation_, pages 35-45, Aberdeen, Scotland, UK, 2021. Association for Computational Linguistics. URL [https://aclanthology.org/2021.inlg-1.4](https://aclanthology.org/2021.inlg-1.4).
* Shao et al. (2024) Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.

* Sharma et al. (2024) M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, E. DURMUS, Z. Hatfield-Dodds, S. R. Johnston, S. M. Kravec, T. Maxwell, S. McCandlish, K. Ndousse, O. Rausch, N. Schiefer, D. Yan, M. Zhang, and E. Perez. Towards understanding sycophancy in language models. In _The Twelfth International Conference on Learning Representations_, 2024.
* Shi et al. (2023) W. Shi, A. Ajith, M. Xia, Y. Huang, D. Liu, T. Blevins, D. Chen, and L. Zettlemoyer. Detecting pretraining data from large language models, 2023.
* Shinn et al. (2024) N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Shypula et al. (2023) A. Shypula, A. Madaan, Y. Zeng, U. Alon, J. Gardner, M. Hashemi, G. Neubig, P. Ranganathan, O. Bastani, and A. Yazdanbakhsh. Learning performance-improving code edits. _ArXiv preprint_, abs/2302.07867, 2023. URL [https://arxiv.org/abs/2302.07867](https://arxiv.org/abs/2302.07867).
* Si et al. (2024) C. Si, Y. Zhang, Z. Yang, R. Liu, and D. Yang. Design2code: How far are we from automating front-end engineering?, 2024. URL [https://arxiv.org/abs/2403.03163](https://arxiv.org/abs/2403.03163).
* Singhal et al. (2022) K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, et al. Large language models encode clinical knowledge. _ArXiv preprint_, abs/2212.13138, 2022. URL [https://arxiv.org/abs/2212.13138](https://arxiv.org/abs/2212.13138).
* Steinhardt (2022) J. Steinhardt. MI systems will have weird failure modes. [https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/), 2022.
* Sun et al. (2023) Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang, et al. Aligning large multimodal models with factually augmented rlh. _ArXiv preprint_, abs/2309.14525, 2023. URL [https://arxiv.org/abs/2309.14525](https://arxiv.org/abs/2309.14525).
* Tang et al. (2023) Q. Tang, Z. Deng, H. Lin, X. Han, Q. Liang, and L. Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. _ArXiv preprint_, abs/2306.05301, 2023. URL [https://arxiv.org/abs/2306.05301](https://arxiv.org/abs/2306.05301).
* Taori et al. (2023) R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.
* Taylor et al. (2022) R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic. Galactica: A large language model for science. _ArXiv preprint_, abs/2211.09085, 2022. URL [https://arxiv.org/abs/2211.09085](https://arxiv.org/abs/2211.09085).
* Thoppilan et al. (2022) R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog applications. _ArXiv preprint_, abs/2201.08239, 2022. URL [https://arxiv.org/abs/2201.08239](https://arxiv.org/abs/2201.08239).
* Tian et al. (2023) K. Tian, E. Mitchell, H. Yao, C. D. Manning, and C. Finn. Fine-tuning language models for factuality. In _ICLR_, 2023. URL [https://api.semanticscholar.org/CorpusID:265158181](https://api.semanticscholar.org/CorpusID:265158181).

* Todorov et al. (2012) E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.
* Touvron et al. (2023) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ArXiv preprint_, abs/2307.09288, 2023. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).
* Trinh et al. (2024) T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. _Nature_, 625(7995):476-482, 2024.
* Van Breugel et al. (2023) B. Van Breugel, Z. Qian, and M. Van Der Schaar. Synthetic data, real errors: how (not) to publish and use synthetic data. In _International Conference on Machine Learning_, pages 34793-34808. PMLR, 2023.
* Vezhnevets et al. (2023) A. S. Vezhnevets, J. P. Agapiou, A. Aharon, R. Ziv, J. Matyas, E. A. Duenez-Guzman, W. A. Cunningham, S. Osindero, D. Karmon, and J. Z. Leibo. Generative agent-based modeling with actions grounded in physical, social, or digital space using concordia. _ArXiv preprint_, abs/2312.03664, 2023. URL [https://arxiv.org/abs/2312.03664](https://arxiv.org/abs/2312.03664).
* Villalobos et al. (2022) P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. _ArXiv preprint_, abs/2211.04325, 2022. URL [https://arxiv.org/abs/2211.04325](https://arxiv.org/abs/2211.04325).
* Wang et al. (2023) G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An open-ended embodied agent with large language models. _ArXiv preprint_, abs/2305.16291, 2023. URL [https://arxiv.org/abs/2305.16291](https://arxiv.org/abs/2305.16291).
* Wang et al. (2022a) X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. 2022a. URL [https://arxiv.org/abs/2203.11171](https://arxiv.org/abs/2203.11171).
* Wang et al. (2022b) Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. volume abs/2212.10560, 2022b. URL [https://arxiv.org/abs/2212.10560](https://arxiv.org/abs/2212.10560).
* Wang et al. (2020) Z. Wang, X. Wang, B. An, D. Yu, and C. Chen. Towards faithful neural table-to-text generation with content-matching constraints. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 1072-1086, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.101. URL [https://aclanthology.org/2020.acl-main.101](https://aclanthology.org/2020.acl-main.101).
* Wei et al. (2019) J. Wei, A. Suriawinata, L. Vaickus, B. Ren, X. Liu, J. Wei, and S. Hassanpour. Generative image translation for data augmentation in colorectal histopathology images. In _Advances in Neural Information Processing Systems_, 2019.
* Wei et al. (2022) J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Fine-tuned language models are zero-shot learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022._ OpenReview.net, 2022. URL [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR).
* Wei et al. (2023) J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le. Symbol tuning improves in-context learning in language models. volume abs/2305.08298, 2023a. URL [https://arxiv.org/abs/2305.08298](https://arxiv.org/abs/2305.08298).

* Wei et al. (2023) J. Wei, D. Huang, Y. Lu, D. Zhou, and Q. V. Le. Simple synthetic data reduces sycophancy in large language models, 2023b. URL [https://arxiv.org/abs/2308.03958](https://arxiv.org/abs/2308.03958).
* Wei et al. (2024) J. Wei, C. Yang, X. Song, Y. Lu, N. Hu, D. Tran, D. Peng, R. Liu, D. Huang, C. Du, and Q. V. Le. Long-form factuality in large language models. 2024. URL [https://api.semanticscholar.org/CorpusID:268724304](https://api.semanticscholar.org/CorpusID:268724304).
* Wei et al. (2021) Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang. Magicoder: Source code is all you need. _ArXiv preprint_, abs/2312.02120, 2023c. URL [https://arxiv.org/abs/2312.02120](https://arxiv.org/abs/2312.02120).
* Weidinger et al. (2021) L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, et al. Ethical and social risks of harm from language models. _ArXiv preprint_, abs/2112.04359, 2021. URL [https://arxiv.org/abs/2112.04359](https://arxiv.org/abs/2112.04359).
* Wood et al. (2021) E. Wood, T. Baltrusaitis, C. Hewitt, S. Dziadzio, T. J. Cashman, and J. Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pages 3661-3671. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00366. URL [https://doi.org/10.1109/ICCV48922.2021.00366](https://doi.org/10.1109/ICCV48922.2021.00366).
* Xu et al. (2023) C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large language models to follow complex instructions. _ArXiv preprint_, abs/2304.12244, 2023. URL [https://arxiv.org/abs/2304.12244](https://arxiv.org/abs/2304.12244).
* Xu et al. (2022) J. Xu, Y. Ruan, W. Bi, G. Huang, S. Shi, L. Chen, and L. Liu. On synthetic data for back translation. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 419-430, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.32. URL [https://aclanthology.org/2022.naacl-main.32](https://aclanthology.org/2022.naacl-main.32).
* Xue et al. (2020) L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. _arXiv preprint arXiv:2010.11934_, 2020.
* Yang et al. (2024) J. Yang, A. Prabhakar, K. Narasimhan, and S. Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ye et al. (2024) J. Ye, S. Li, G. Li, C. Huang, S. Gao, Y. Wu, Q. Zhang, T. Gui, and X. Huang. Toolsword: Unveiling safety issues of large language models in tool learning across three stages. _ArXiv preprint_, abs/2402.10753, 2024. URL [https://arxiv.org/abs/2402.10753](https://arxiv.org/abs/2402.10753).
* Yu et al. (2023) L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. _ArXiv preprint_, abs/2309.12284, 2023. URL [https://arxiv.org/abs/2309.12284](https://arxiv.org/abs/2309.12284).
* Yu et al. (2024) Y. Yu, Y. Zhuang, J. Zhang, Y. Meng, A. J. Ratner, R. Krishna, J. Shen, and C. Zhang. Large language model as attributed training data generator: A tale of diversity and bias. _Advances in Neural Information Processing Systems_, 36, 2024.
* Yuan et al. (2024) W. Yuan, R. Y. Pang, K. Cho, S. Sukhbaatar, J. Xu, and J. Weston. Self-rewarding language models. _ArXiv preprint_, abs/2401.10020, 2024. URL [https://arxiv.org/abs/2401.10020](https://arxiv.org/abs/2401.10020).
* Yuan et al. (2023) Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. _ArXiv preprint_, abs/2308.01825, 2023. URL [https://arxiv.org/abs/2308.01825](https://arxiv.org/abs/2308.01825).

* Zelikman et al. (2022) E. Zelikman, Y. Wu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning. In _NeurIPS_, 2022. URL [https://api.semanticscholar.org/CorpusID:247762790](https://api.semanticscholar.org/CorpusID:247762790).
* Zhang et al. (2023a) J. Zhang, X. Xu, and S. Deng. Exploring collaboration mechanisms for llm agents: A social psychology view. _ArXiv preprint_, abs/2310.02124, 2023a. URL [https://arxiv.org/abs/2310.02124](https://arxiv.org/abs/2310.02124).
* Zhang et al. (2023b) S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu, and G. Wang. Instruction tuning for large language models: A survey, 2023b. URL [https://arxiv.org/abs/2308.10792](https://arxiv.org/abs/2308.10792).
* Zhang et al. (2023c) Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, et al. Siren's song in the ai ocean: A survey on hallucination in large language models. _ArXiv preprint_, abs/2309.01219, 2023c. URL [https://arxiv.org/abs/2309.01219](https://arxiv.org/abs/2309.01219).
* Zhang et al. (2023d) Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang, and T. Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _ArXiv preprint_, abs/2306.17107, 2023d. URL [https://arxiv.org/abs/2306.17107](https://arxiv.org/abs/2306.17107).
* Zhao et al. (2023) B. Zhao, B. Wu, and T. Huang. Svit: Scaling up visual instruction tuning. _ArXiv preprint_, abs/2307.04087, 2023. URL [https://arxiv.org/abs/2307.04087](https://arxiv.org/abs/2307.04087).
* Zhao et al. (2018) J. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_, pages 15-20, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2003. URL [https://aclanthology.org/N18-2003](https://aclanthology.org/N18-2003).
* Zheng et al. (2023) L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* Zheng et al. (2022) S. Zheng, A. Trott, S. Srinivasa, D. C. Parkes, and R. Socher. The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. _Science advances_, 8(18):eabk2607, 2022.
* Zheng et al. (2020) Z. Zheng, H. Zhou, S. Huang, L. Li, X. Dai, and J. Chen. Mirror-generative neural machine translation. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL [https://openreview.net/forum?id=HkxQRTNYPH](https://openreview.net/forum?id=HkxQRTNYPH).
* Zhou et al. (2024) X. Zhou, Z. Su, T. Eisape, H. Kim, and M. Sap. Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms. _ArXiv preprint_, abs/2403.05020, 2024. URL [https://arxiv.org/abs/2403.05020](https://arxiv.org/abs/2403.05020).
* Ziems et al. (2023) C. Ziems, J. Dwivedi-Yu, Y.-C. Wang, A. Halevy, and D. Yang. Normbank: A knowledge bank of situational social norms. _ArXiv preprint_, abs/2305.17008, 2023. URL [https://arxiv.org/abs/2305.17008](https://arxiv.org/abs/2305.17008).
* Zou et al. (2023) A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models. _ArXiv preprint_, abs/2307.15043, 2023. URL [https://arxiv.org/abs/2307.15043](https://arxiv.org/abs/2307.15043).
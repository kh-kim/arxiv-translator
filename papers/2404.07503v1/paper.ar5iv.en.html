<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.07503] Best Practices and Lessons Learned on Synthetic Data for Language Models</title><meta property="og:description" content="The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Best Practices and Lessons Learned on Synthetic Data for Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Best Practices and Lessons Learned on Synthetic Data for Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.07503">

<!--Generated on Sun May  5 23:38:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\correspondingauthor</span>
<p id="p1.2" class="ltx_p">ruiboliu@google.com














</p>
</div>
<h1 class="ltx_title ltx_title_document">Best Practices and Lessons Learned on Synthetic Data for Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruibo Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jerry Wei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fangyu Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenglei Si
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Stanford University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanzhe Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Georgia Institute of Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinmeng Rao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daiyi Peng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Diyi Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Stanford University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denny Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew M. Dai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.</p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.07503/assets/assets/manufacture_hd.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="389" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>One synthetic image generated by Imagen&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Saharia et&nbsp;al., <a href="#bib.bib132" title="" class="ltx_ref">2022a</a>)</cite> v2.0, with a prompt including the following description: <span id="S1.F1.2.1" class="ltx_text ltx_font_italic">“In a robotics factory, humanoid robots collaborate on an assembly line to design, fabricate, test, and assemble new robots. The new robots they are manufacturing look similar to those robotic workers who are creating them.”</span> We also added some style controlling text from aesthetic considerations.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The rapid advancement of artificial intelligence (AI) technologies has led to their widespread adoption across numerous domains, from assistant agents (e.g., ACT-1, from Adept AI<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>ACT-1: <a target="_blank" href="https://www.adept.ai/blog/act-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.adept.ai/blog/act-1</a></span></span></span>) and software development (e.g., Devin, from Cognition Lab<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Devin: <a target="_blank" href="https://www.cognition-labs.com/introducing-devin" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.cognition-labs.com/introducing-devin</a></span></span></span>) to healthcare <cite class="ltx_cite ltx_citemacro_citep">(Singhal et&nbsp;al., <a href="#bib.bib144" title="" class="ltx_ref">2022</a>)</cite> and finance <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a href="#bib.bib187" title="" class="ltx_ref">2022</a>)</cite>. However, the success of AI models heavily relies on the availability of large, diverse, and high-quality datasets for training and evaluation. Acquiring such datasets can be a significant challenge due to data scarcity <cite class="ltx_cite ltx_citemacro_citep">(Babbar and Schölkopf, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, privacy concerns <cite class="ltx_cite ltx_citemacro_citep">(Abay et&nbsp;al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>, and the sheer cost of data collection and annotation <cite class="ltx_cite ltx_citemacro_citep">(Gilardi et&nbsp;al., <a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite>. Pessimists predict that we will run out of fresh text data in 2050 and image data in 2060 <cite class="ltx_cite ltx_citemacro_citep">(Villalobos et&nbsp;al., <a href="#bib.bib157" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic data has emerged as a promising solution to address these challenges <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko, <a href="#bib.bib113" title="" class="ltx_ref">2021</a>)</cite>. Synthetic data refers to artificially generated data that mimics the characteristics and patterns of real-world data, but is created through algorithms <cite class="ltx_cite ltx_citemacro_citep">(Saxton et&nbsp;al., <a href="#bib.bib134" title="" class="ltx_ref">2019</a>)</cite>, generative models <cite class="ltx_cite ltx_citemacro_citep">(Borisov et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>; Meng et&nbsp;al., <a href="#bib.bib108" title="" class="ltx_ref">2022</a>)</cite>, or even simulations <cite class="ltx_cite ltx_citemacro_citep">(Vezhnevets et&nbsp;al., <a href="#bib.bib156" title="" class="ltx_ref">2023</a>; Liu et&nbsp;al., <a href="#bib.bib99" title="" class="ltx_ref">2023c</a>)</cite>, rather than being directly created by humans. By leveraging synthetic data, we can not only overcome the limitations of real-world data but also unlock the potential to develop more robust, reliable, and fair AI models <cite class="ltx_cite ltx_citemacro_citep">(Lucini, <a href="#bib.bib102" title="" class="ltx_ref">2021</a>; Lu et&nbsp;al., <a href="#bib.bib101" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One of the many benefits of synthetic data is that it can be generated at scale, providing an abundant supply of training and testing data for AI models. This is particularly valuable in domains where real-world data is scarce or difficult to obtain (e.g., weather data covering all conditions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib86" title="" class="ltx_ref">2023a</a>; Lam et&nbsp;al., <a href="#bib.bib75" title="" class="ltx_ref">2023</a>)</cite>). Second, synthetic data can be tailored to specific requirements, such as ensuring a balanced representation of different classes by introducing controlled variations (e.g., up-weighting low-resource languages in multilingual language learning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Przystupa and Abdul-Mageed, <a href="#bib.bib125" title="" class="ltx_ref">2019</a>)</cite>). This level of control over data characteristics can improve model performance and generalization. Third, synthetic data can help mitigate privacy concerns by creating anonymized or de-identified datasets that do not contain sensitive personal information&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Howe et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2017</a>; El&nbsp;Emam et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>. This is crucial in domains such as healthcare, where patient privacy is of utmost importance&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dahmen and Cook, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>; Wei et&nbsp;al., <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite its promise, synthetic data also presents challenges that need to be addressed. One of them is ensuring the factuality and fidelity of synthetic data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wood et&nbsp;al., <a href="#bib.bib169" title="" class="ltx_ref">2021</a>; Heusel et&nbsp;al., <a href="#bib.bib59" title="" class="ltx_ref">2017</a>)</cite>, as models trained on false, hallucinated or biased synthetic data may fail to generalize to real-world scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Van&nbsp;Breugel et&nbsp;al., <a href="#bib.bib155" title="" class="ltx_ref">2023</a>; Guarnera et&nbsp;al., <a href="#bib.bib56" title="" class="ltx_ref">2020</a>)</cite>. Researchers must develop sophisticated generative models and evaluation metrics to create synthetic data that accurately reflects the complex patterns and relationships found in real-world data. Another challenge is the potential for synthetic data to amplify biases or introduce new biases if not carefully designed and validated&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Barbierato et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>; Gupta et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2021</a>)</cite>. We believe rigorous testing and fairness assessments are necessary to mitigate these risks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we track the current state of synthetic data research and discuss current best practices and lessons learned. The rest of the paper is organized as follows. Section&nbsp;<a href="#S2" title="2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of synthetic data generation techniques and their applications in model training, presenting case studies and empirical evidence. Section&nbsp;<a href="#S3" title="3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> discusses the usefulness of synthetic data in evaluation. Section&nbsp;<a href="#S4" title="4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> discusses the challenges and limitations of synthetic data, and in Section&nbsp;<a href="#S5" title="5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we outline potential solutions and future research directions.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Synthetic Data in Training</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic data, which is generated by mimicking authentic data collected from the real world, has proven to be an effective and relatively low-cost alternative of real data. This section explores several notable domains that leverage synthetic training data.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Reasoning</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Math.</h4>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">Recent advancements in mathematical reasoning for language models (LMs) have led to the development of various approaches to improve performance on math-related tasks. One approach is to train on math-targeted pre-training data, such as Minerva&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et&nbsp;al., <a href="#bib.bib83" title="" class="ltx_ref">2022</a>)</cite>, Llemma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et&nbsp;al., <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, and DeepSeekMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a href="#bib.bib138" title="" class="ltx_ref">2024</a>)</cite>. Another mainstream method is to generate synthetic questions and answers to imitate the training or validation set of target benchmarks. For instance, WizardMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a href="#bib.bib103" title="" class="ltx_ref">2023a</a>)</cite> leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a href="#bib.bib175" title="" class="ltx_ref">2023</a>)</cite> bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning. GAIR-Abel&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chern et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> found that the format of the augmented answers is crucial to final performance, with answers that begin with a paraphrasing of the question followed by a step-by-step solution showing better performance than those in vanilla format. Xwin-Math&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib85" title="" class="ltx_ref">2024</a>)</cite> further scaled up synthetic SFT data to one million examples and found that the LLaMA-2 7B model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib153" title="" class="ltx_ref">2023</a>)</cite> can still benefit from data scaling. MMIQC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Yao, <a href="#bib.bib95" title="" class="ltx_ref">2024</a>)</cite> composed a bundle of datasets that infuse SFT style data (via question-answer rephrasing or directly taken from MetaMath) with a subset of high-quality mathematical pre-training data, such as OpenWebMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Paster et&nbsp;al., <a href="#bib.bib120" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p2.1" class="ltx_p">Scaling up the generation of synthetic math data is a straightforward process, but ensuring the correctness of the generated math remains a significant challenge for practitioners. AlphaGeometry&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Trinh et&nbsp;al., <a href="#bib.bib154" title="" class="ltx_ref">2024</a>)</cite> is a recent attempt to address this issue by training a neural model using 100 million synthetic data points. The model proposes solutions and guides a symbolic deduction engine in verifying the correctness of each branch when solving complex geometry problems. By combining the power of synthetic data with a rigorous verification process, AlphaGeometry achieves a problem-solving ability comparable to that of a human Olympiad gold medalist, demonstrating the potential of this approach in tackling complex mathematical reasoning tasks.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Code.</h4>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">Different from Math, synthetic data for code reasoning can naturally combine the execution results with structured code, as one requirement of correct code is being executable. In coding-enhanced models, CodeRL&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Le et&nbsp;al., <a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite> presents an actor-critic approach to improve pretrained language models with feedback signals on synthetic code samples. <cite class="ltx_cite ltx_citemacro_cite">Haluptzok et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2022</a>)</cite> propose a self-improvement strategy where the models generate their own synthetic puzzle-solution pairs. These pairs are then verified and filtered by a real interpreter before being used to finetune language models. <cite class="ltx_cite ltx_citemacro_cite">Shypula et&nbsp;al. (<a href="#bib.bib142" title="" class="ltx_ref">2023</a>)</cite> further propose a framework that leverages a simulated environment and adaptation strategies like self-improvement synthetic data generation and CoT prompting for code optimization. <cite class="ltx_cite ltx_citemacro_citet">Yang et&nbsp;al. (<a href="#bib.bib173" title="" class="ltx_ref">2024</a>)</cite> developed InterCode, a framework designed to enhance interactive code generation within a reinforcement learning environment, where code serves as actions and execution feedback serves as observations. Reflexion&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shinn et&nbsp;al., <a href="#bib.bib141" title="" class="ltx_ref">2024</a>)</cite> employs external or internally simulated linguistic feedback signals to improve the code reasoning capabilities of language models. Regarding synthetic SFT data, Code Alpaca comprises a dataset of 20K code instructions automatically generated by applying SELF-INSTRUCT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib159" title="" class="ltx_ref">2022a</a>)</cite> to ChatGPT across 21 seed tasks. WizardCoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a href="#bib.bib104" title="" class="ltx_ref">2023b</a>)</cite> introduces Code Evol-Instruct to guide ChatGPT with heuristic prompts to enhance the complexity and diversity of synthetic data. Meanwhile, Magicoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a href="#bib.bib167" title="" class="ltx_ref">2023c</a>)</cite> developed OSS-INSTRUCT, which generates 75K diverse synthetic instruction samples from open-source code snippets.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Other reasoning tasks.</h4>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">Synthetic data also leads to impressive performance in other reasoning tasks. For instance, <cite class="ltx_cite ltx_citemacro_citet">Wei et&nbsp;al. (<a href="#bib.bib164" title="" class="ltx_ref">2023a</a>)</cite> augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples.
Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks. STaR&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et&nbsp;al., <a href="#bib.bib179" title="" class="ltx_ref">2022</a>)</cite> generates synthetic chain-of-thought rationales and filters out those leading to wrong answers for finetuning language models to improve their reasoning. In the domain of physics reasoning, Mind’s Eye&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib98" title="" class="ltx_ref">2022</a>)</cite> takes a novel approach by training a text-to-code model with synthetic “text-description <math id="S2.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.SS1.SSS0.Px3.p1.1.m1.1a"><mo stretchy="false" id="S2.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px3.p1.1.m1.1c">\rightarrow</annotation></semantics></math> rendering code” data. This enables the model to convert textual questions into rendering code, which is then executed in a physical engine (i.e., DeepMind MuJoCo&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Todorov et&nbsp;al., <a href="#bib.bib152" title="" class="ltx_ref">2012</a>)</cite>). The rendering results are injected into the context, allowing even small language models armed with Mind’s Eye to achieve performance comparable to models 100 times larger.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tool-using and Planning</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Learning tool-using through synthetic trajectories.</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">Synthetic data is also a powerful approach to enable LMs to learn tool-using abilities through simulated trajectories, as collecting real-world human tool-using data might be time-consuming, and the actual distribution of calls to tools might be skewed. LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Thoppilan et&nbsp;al., <a href="#bib.bib150" title="" class="ltx_ref">2022</a>)</cite>, for instance, was trained not only on web documents but also on interaction data between crowdworkers and the model itself, with the synthetic data annotated with calls to appropriate tools. This training process allowed LaMDA to develop the ability to use a calculator for arithmetic, a search engine for real-time information seeking, and a machine translator for translation. Similarly, Toolformer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Schick et&nbsp;al., <a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite> learns to decide which APIs to call and what arguments to pass by training on template-generated data, while Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Taylor et&nbsp;al., <a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite> infuse API-calling data into pre-training mixture. ToolAlpaca&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tang et&nbsp;al., <a href="#bib.bib147" title="" class="ltx_ref">2023</a>)</cite> is a novel framework designed to automatically generate a diverse tool-use corpus, by building a multi-agent simulation environment and letting agents select and use tools iteratively. These examples demonstrate the potential of synthetic trajectories in enabling LMs to acquire tool-using abilities and enhance their reasoning capabilities across various domains.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Learning to plan in synthetic environments.</h4>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">An important feature of the agent in Autonomous Machine Intelligence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a href="#bib.bib79" title="" class="ltx_ref">2022</a>)</cite> is planning—an ability of decomposing complex tasks into subtasks and finishing the subtasks in a reward-optimal way&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kambhampati et&nbsp;al., <a href="#bib.bib73" title="" class="ltx_ref">2024</a>)</cite>. Synthetic data can be a valuable tool here as it can serve as the feedback signal collected from a simulator&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Park et&nbsp;al., <a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite>, and learning on it can make the agent aware of affordances&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ahn et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>; Liang et&nbsp;al., <a href="#bib.bib88" title="" class="ltx_ref">2022</a>)</cite>. For example, Inner Monologue&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite> leverages natural language form feedback generated by the simulated environment to teach LLM-based robots planning. They find that such feedback significantly improves high-level instruction completion on both simulated and real-world domains. To compose a large number of realistic planning tasks (e.g., <span id="S2.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">“Rearrange objects on a table to match a given scene.”</span>), VIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a href="#bib.bib71" title="" class="ltx_ref">2022</a>)</cite> creates a multi-modality simulated environment called VIMA-Bench, which supports extensible collections of objects and textures. In the Minecraft game, Voyager&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib158" title="" class="ltx_ref">2023</a>)</cite> deploys a number of GPT-4 based agents to interact with the synthetic environment and finds that the agents can unlock new skills faster and complete planning more efficiently with the help of synthetic feedback.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multimodality</h3>

<section id="S2.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reverse rendering from vision to text.</h4>

<div id="S2.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px1.p1.1" class="ltx_p">Vision-language alignment data focuses on accurately grounding visual input to an LLM (usually via a vision encoder). Web-scraped image-caption pairs have been the most popular MM alignment data in the past few years since CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib126" title="" class="ltx_ref">2021</a>)</cite> and ALIGN <cite class="ltx_cite ltx_citemacro_citep">(Jia et&nbsp;al., <a href="#bib.bib69" title="" class="ltx_ref">2021</a>)</cite>. However, web-scraped image-text pairs are usually noisy and only have coarse-grained correspondence, insufficient for grounding details of images in language. In domains such as documents, screens, figures, and diagrams, such fine-grained alignment can most conveniently be obtained from data synthesis pipelines built with image rendering engines. Pix2Struct <cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite> uses web servers to render HTML code into website screenshots, and the training task is to derender a masked screenshot to the full HTML code. MatCha <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib94" title="" class="ltx_ref">2023b</a>)</cite> and DePlot <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib93" title="" class="ltx_ref">2023a</a>)</cite> render tabular data into charts with Python plotting libraries and pretrain a foundation model by giving the rendered image and producing the code and/or the tabular data. <cite class="ltx_cite ltx_citemacro_citet">Si et&nbsp;al. (<a href="#bib.bib143" title="" class="ltx_ref">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Laurençon et&nbsp;al. (<a href="#bib.bib77" title="" class="ltx_ref">2024</a>)</cite> train on synthetically generated HTML and CSS files for the task of converting webpage screenshots into code implementation. The models finetuned on the synthetic data can generalize reasonably well on realistic data scraped from the Internet. <cite class="ltx_cite ltx_citemacro_citet">Borkman et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> propose to use physics engines or game engines (e.g., Unity) as the synthetic data generator to help computer vision research.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-modality instruction following.</h4>

<div id="S2.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px2.p1.1" class="ltx_p">Downstream applications of multimodal LLMs require reasoning and instruction following capabilities. Such data are usually long-form question response pairs and are expensive for humans to create. LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib96" title="" class="ltx_ref">2024b</a>)</cite> uses existing image captions to prompt GPT-4 (in text-only mode) for writing diverse and long-form prompt-answer pairs. During multimodal LLM training, images and prompts are used as input while the captions and bounding box information can be hidden. Besides image captions, other sources of image attribute information such as object bounding box <cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a href="#bib.bib184" title="" class="ltx_ref">2023</a>)</cite>, OCR <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib183" title="" class="ltx_ref">2023d</a>)</cite> and derendered charts <cite class="ltx_cite ltx_citemacro_citep">(Masry et&nbsp;al., <a href="#bib.bib106" title="" class="ltx_ref">2023</a>; Carbune et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite> can all fit into such as image attributes + text LLM rewriting synthetic data pipeline.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Multilingual</h3>

<section id="S2.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Back-translation augmentation.</h4>

<div id="S2.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px1.p1.1" class="ltx_p">Many multilingual language models use back-translation as a data augmentation method, creating synthetic parallel training data from monolingual data sources&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sennrich et&nbsp;al., <a href="#bib.bib136" title="" class="ltx_ref">2016</a>; Zheng et&nbsp;al., <a href="#bib.bib188" title="" class="ltx_ref">2020</a>; Caswell et&nbsp;al., <a href="#bib.bib24" title="" class="ltx_ref">2019</a>; Marie et&nbsp;al., <a href="#bib.bib105" title="" class="ltx_ref">2020</a>; Bi et&nbsp;al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>; Liao et&nbsp;al., <a href="#bib.bib89" title="" class="ltx_ref">2021</a>; Pham et&nbsp;al., <a href="#bib.bib124" title="" class="ltx_ref">2021</a>; Xu et&nbsp;al., <a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">Sennrich et&nbsp;al. (<a href="#bib.bib136" title="" class="ltx_ref">2016</a>)</cite> back-translate monolingual target data into source language data, providing additional parallel training samples for substantial translation task improvements. Researchers have also explored different sampling methods for back-translation (e.g., beam search, constrained sampling, unconstrained sampling) and their comparative effectiveness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sennrich et&nbsp;al., <a href="#bib.bib136" title="" class="ltx_ref">2016</a>; Edunov et&nbsp;al., <a href="#bib.bib36" title="" class="ltx_ref">2018</a>; Graça et&nbsp;al., <a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Xu et&nbsp;al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite> emphasize the importance of the weight and quality of synthetic data for optimal NMT performance using back-translation. They propose a method to optimize the ratio between search methods and a gamma score to balance estimated importance weight and quality. However, some limitations exist with back-translation-based synthetic data generation. For example, the quality and diversity of synthetic data depends on the performance of the back-translation method. If the synthetic data is too noisy or not diverse, the performance gain would be limited&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Epaliyana et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>; Chauhan et&nbsp;al., <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generating multilingual questions and answers at scale.</h4>

<div id="S2.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px2.p1.1" class="ltx_p">Recent studies explore the generation and utilization of synthetic multilingual question-answer (QA) pairs to improve language models’ performance in multilingual and cross-lingual question answering&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>; Kumar et&nbsp;al., <a href="#bib.bib74" title="" class="ltx_ref">2019</a>; Chi et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2020</a>; Riabi et&nbsp;al., <a href="#bib.bib130" title="" class="ltx_ref">2021</a>; Li and Callison-Burch, <a href="#bib.bib84" title="" class="ltx_ref">2023</a>; Abulkhanov et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>. One approach is to translate existing monolingual questions and/or answers into other languages&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>. Another involves using Question Generation (QG) models to produce synthetic questions in a cross-lingual fashion based on answers and/or source texts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kumar et&nbsp;al., <a href="#bib.bib74" title="" class="ltx_ref">2019</a>; Chi et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2020</a>; Riabi et&nbsp;al., <a href="#bib.bib130" title="" class="ltx_ref">2021</a>)</cite>. Recent efforts also focus on jointly generating questions and answers in multiple languages for greater flexibility&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shakeri et&nbsp;al., <a href="#bib.bib137" title="" class="ltx_ref">2021</a>; Li and Callison-Burch, <a href="#bib.bib84" title="" class="ltx_ref">2023</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">Shakeri et&nbsp;al. (<a href="#bib.bib137" title="" class="ltx_ref">2021</a>)</cite> finetune a pretrained multilingual T5 model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xue et&nbsp;al., <a href="#bib.bib172" title="" class="ltx_ref">2020</a>)</cite> on a mixture of a QA generation task and a multilingual masked language modeling task to produce synthetic QA pairs in multiple languages. These efforts generally show that language models trained on synthetic QA pairs demonstrate improved performance on multilingual QA and information retrieval benchmarks.</p>
</div>
</section>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Alignment</h3>

<section id="S2.SS5.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Instruction Following.</h4>

<div id="S2.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS5.SSS0.Px1.p1.1" class="ltx_p">Synthetic data can serve as a promising approach for training instruction-following models, particularly in scenarios where real-world data is scarce, expensive, or challenging to obtain. Self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib159" title="" class="ltx_ref">2022a</a>)</cite> and Stanford Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a href="#bib.bib148" title="" class="ltx_ref">2023</a>)</cite> are both using LLMs to generate instruction following data which covers a wide range of scenarios. They first pick a small set of “seed instruction following samples” and then ask the LLMs to imitate the format to generate more demonstrations. One concern of this type of method is how to keep the generated data high quality, which involves the complexity of queries&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib100" title="" class="ltx_ref">2023d</a>)</cite>, the diversity of semantics&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ding et&nbsp;al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, and the scale of the synthetic dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yuan et&nbsp;al., <a href="#bib.bib178" title="" class="ltx_ref">2023</a>)</cite>. To this end, <cite class="ltx_cite ltx_citemacro_citet">Xu et&nbsp;al. (<a href="#bib.bib170" title="" class="ltx_ref">2023</a>)</cite> propose Evol-Instruct which adds complexity to simple instructions via prompting. <cite class="ltx_cite ltx_citemacro_citet">Mukherjee et&nbsp;al. (<a href="#bib.bib112" title="" class="ltx_ref">2023</a>)</cite> leverage LLMs to revise the instructions and responses iteratively to include high-quality explanation traces in the FLAN dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a href="#bib.bib163" title="" class="ltx_ref">2022</a>)</cite>, and they find the trained model has improved performance in many NLP tasks. UltraChat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ding et&nbsp;al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> is large-scale and multi-round synthetic dialogue dataset, which is generated by two separate ChatGPT Turbo API models—one serves as the user role while the other serves as the assistant. They instruct the user model with carefully designed prompts to mimic real human user behaviors.</p>
</div>
<div id="S2.SS5.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS5.SSS0.Px1.p2.1" class="ltx_p">Many language models are supervised finetuned to learn how to follow instructions, but in learning this behavior, they may inadvertently also learn to be <span id="S2.SS5.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">sycophantic</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Perez et&nbsp;al., <a href="#bib.bib123" title="" class="ltx_ref">2023</a>)</cite>, tailoring their responses to follow a user’s viewpoint, even if that viewpoint is not objectively correct <cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a href="#bib.bib165" title="" class="ltx_ref">2023b</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Sharma et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2024</a>)</cite> find evidence that the preference models (i.e., the reward model used for RLHF training) and even humans prefer sycophantic responses sometimes. On this front, <cite class="ltx_cite ltx_citemacro_citet">Wei et&nbsp;al. (<a href="#bib.bib165" title="" class="ltx_ref">2023b</a>)</cite> generates synthetic data to encourage models to be robust to user opinions and adds these data in a finetuning step to reduce sycophantic behavior on held-out prompts.</p>
</div>
</section>
<section id="S2.SS5.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Mitigating hallucination.</h4>

<div id="S2.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS5.SSS0.Px2.p1.1" class="ltx_p">Many widely-used language models utilize supervised finetuning (SFT) to learn to align their interactions with users <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib160" title="" class="ltx_ref">2022b</a>; Zhang et&nbsp;al., <a href="#bib.bib181" title="" class="ltx_ref">2023b</a>)</cite>.
In particular, there exist many methods of generating synthetic SFT data that can improve capabilities such as reasoning and alignment <cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a href="#bib.bib164" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib165" title="" class="ltx_ref">b</a>)</cite>.
It has been shown, however, that these synthetic data can induce hallucinations into language models by containing nontrivial amounts of hallucinated answers or by forcing models to learn to answer questions that they do not know the answer to <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib182" title="" class="ltx_ref">2023c</a>)</cite>.
These cases demonstrate that synthetic data, if not applied correctly, can actually increase hallucinations in language models.</p>
</div>
<div id="S2.SS5.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS5.SSS0.Px2.p2.1" class="ltx_p">On the other hand, recent work has also shown promising results in mitigating hallucinations using synthetic data.
For example, GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib115" title="" class="ltx_ref">2023</a>)</cite> was trained using a reward model that leveraged synthetic hallucination data in order to perform reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib182" title="" class="ltx_ref">2023c</a>)</cite>.
This method resulted in a significant improvement in performance on the TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a href="#bib.bib91" title="" class="ltx_ref">2022</a>)</cite> dataset <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib182" title="" class="ltx_ref">2023c</a>)</cite>.
Similarly, <cite class="ltx_cite ltx_citemacro_citet">Jones et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite> designed a synthetic task where hallucinations can be readily evaluated, utilizing this task to optimize LLM outputs by learning a continuous postfix via prefix-tuning.
<cite class="ltx_cite ltx_citemacro_citet">Tian et&nbsp;al. (<a href="#bib.bib151" title="" class="ltx_ref">2023</a>)</cite> uses automated fact-checking and confidence scores to rank factuality scores of model response pairs, which are then used to finetune language models with DPO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rafailov et&nbsp;al., <a href="#bib.bib128" title="" class="ltx_ref">2023</a>)</cite> to improve their factuality.
Continued research in using synthetic data to mitigate hallucinations is still limited, however, by the lack of synthetic tasks for which hallucinations can be scalably evaluated.</p>
</div>
</section>
<section id="S2.SS5.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Aligning with shared human preference and values.</h4>

<div id="S2.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS5.SSS0.Px3.p1.1" class="ltx_p">Directly finetuning on value-aligned or human-preferred data is a straightforward method for aligning language models, but this method often requires substantial human annotation, which can be prohibitively expensive at scale. Additionally, such annotation frequently exhibits varying styles and inconsistent quality, particularly in the case of poorly annotated samples at the lower end of the quality spectrum&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Meta, <a href="#bib.bib109" title="" class="ltx_ref">2023</a>; Gilardi et&nbsp;al., <a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite>. To address these practical challenges, an advanced technique known as “reinforcement learning from human feedback (RLHF)” has been proposed&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Leike et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2018</a>; Christiano et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>; Ouyang et&nbsp;al., <a href="#bib.bib117" title="" class="ltx_ref">2022</a>)</cite>. This approach involves training a reward model with human data to act as a proxy of human judgment, which guides the optimization of the LM generation policy.</p>
</div>
<div id="S2.SS5.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS5.SSS0.Px3.p2.1" class="ltx_p">Recent studies have proposed a mixture of synthetic data and real human data to train more robust reward models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>. Constitutional AI&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> proposes to use a small set of principles to steer the AI generated critiques and feedback, and use such synthetic data to replace the real human data in the typical RLHF pipeline. The model trained with this RLAIF (i.e., reinforcement learning from AI feedback) method shows similar strong performance as RLHF baselines. In general, synthetic data offers a powerful solution for human values and preferences alignment by allowing researchers to generate large-scale, diverse, and controlled training datasets in a low-cost way&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Ganguli et&nbsp;al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite>. By simulating a wide range of scenarios involving ethical dilemmas&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Perez et&nbsp;al., <a href="#bib.bib122" title="" class="ltx_ref">2022</a>)</cite>, social interactions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib99" title="" class="ltx_ref">2023c</a>)</cite>, and cultural norms&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ziems et&nbsp;al., <a href="#bib.bib190" title="" class="ltx_ref">2023</a>)</cite>, synthetic data enables comprehensive and systematic testing of AI models’ alignment with human values&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Askell et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>. This approach helps identify and mitigate issues related to bias&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib97" title="" class="ltx_ref">2021</a>; Ntoutsi et&nbsp;al., <a href="#bib.bib114" title="" class="ltx_ref">2020</a>)</cite>, fairness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a href="#bib.bib185" title="" class="ltx_ref">2018</a>; Landers and Behrend, <a href="#bib.bib76" title="" class="ltx_ref">2023</a>)</cite>, and unintended consequences before AI systems are deployed in real-world settings&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ye et&nbsp;al., <a href="#bib.bib174" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S2.SS5.SSS0.Px3.p3" class="ltx_para">
<p id="S2.SS5.SSS0.Px3.p3.1" class="ltx_p">However, it is important to acknowledge that low-fidelity synthetic human preference data might be limited in accurately reflecting nuanced human judgment&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Argyle et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. Consequently, the resulting models may be less robust under “jail-breaking attacks”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2023a</a>; Deshpande et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>, and may reveal strategically deceptive behavior even through safety training&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pan et&nbsp;al., <a href="#bib.bib118" title="" class="ltx_ref">2022</a>; Steinhardt, <a href="#bib.bib145" title="" class="ltx_ref">2022</a>; Everitt et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>. To mitigate these risks, researchers must continuously refine and improve the quality and diversity of synthetic data, incorporating more complex and comprehensive scenarios that better capture the intricacies of human values and preferences. Additionally, combining synthetic data with real-world data, and creating synthetic data in an interactive environment which can be synced with the real world, are promising remedies. As the need for effective AI governance and regulation grows, synthetic data will play an increasingly vital role in enabling scalable oversight mechanisms that promote trust, accountability, and the development of AI technologies that are aligned with human values and societal expectations.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data in Evaluation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Synthetic data is widely used in evaluations of different perspectives:</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Factuality.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">AI systems may generate information or responses that are not grounded in factual knowledge or data, leading to the creation of misleading or false content, formally known as <span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">hallucination</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>. Factuality evaluation aims to ensure the consistency of the knowledge in the AI system’s output with the knowledge provided by its training data and knowledge base <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>; Zhang et&nbsp;al., <a href="#bib.bib182" title="" class="ltx_ref">2023c</a>)</cite>. Early statistical-based hallucination evaluation methods relied on n-grams to directly calculate the overlap of vocabulary between the input and output content &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dhingra et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2019</a>; Wang et&nbsp;al., <a href="#bib.bib161" title="" class="ltx_ref">2020</a>)</cite>. However, these methods have limitations, as they only consider lexical overlap and do not account for semantics or sentence meaning <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>, making them unsuitable for evaluating more complex forms of hallucination. Subsequent assurance methods shifted from statistical approaches to model-based methods, which are more robust compared to token-difference-based methods <cite class="ltx_cite ltx_citemacro_citep">(Honovich et&nbsp;al., <a href="#bib.bib62" title="" class="ltx_ref">2021</a>)</cite>. While these model-based evaluation methods are more advanced than their predecessors, they still have limitations. For example, the models can only output the degree of hallucination and may struggle to pinpoint specific errors <cite class="ltx_cite ltx_citemacro_citep">(Falke et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Feng et&nbsp;al. (<a href="#bib.bib41" title="" class="ltx_ref">2023a</a>)</cite> propose to combine LLMs generation with random walks on knowledge graphs to generate synthetic evaluation data for factuality, which is aware of entities and relations on the graphs. <cite class="ltx_cite ltx_citemacro_citet">Wei et&nbsp;al. (<a href="#bib.bib166" title="" class="ltx_ref">2024</a>)</cite> created a synthetic dataset called LongFact for long-form factuality evaluation and used Google Search as the grounding source and LLM for the automated judgement, to achieve human-level accuracy but with significally lower cost&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Min et&nbsp;al., <a href="#bib.bib110" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Safety.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Red teaming is a powerful technique for evaluating the safety and robustness of AI models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ganguli et&nbsp;al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>; Casper et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>. By generating diverse and realistic scenarios designed to elicit unaligned or harmful outputs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Casper et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2023a</a>)</cite>, red teaming can expose vulnerabilities and weaknesses in AI systems&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Perez et&nbsp;al., <a href="#bib.bib122" title="" class="ltx_ref">2022</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Perez et&nbsp;al. (<a href="#bib.bib123" title="" class="ltx_ref">2023</a>)</cite> use LMs to generate datasets for evaluating the behavior of other LMs. They end up producing 154 high-quality datasets which are verified by humans, and discover new cases of inverse scaling where LMs get worse with size. <cite class="ltx_cite ltx_citemacro_citet">Hubinger et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2024</a>)</cite> leverage synthetic data to trigger backdoor attacks to LMs at scale; they find LMs can exhibit deceptive behavior and create a false impression of safety under such attacks, and standard “safety training” could not remove such deception easily. These methods demonstrate the feasibility of using AI assistance to scale up human oversight&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bowman et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> over complex problems and unseen domains.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Assisting human evaluation.</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">Recent studies have shown that in many cases, synthetic judgements from large-scale LMs (LLMs) can serve as qualified, fast, and low-cost alternatives to actual human evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gilardi et&nbsp;al., <a href="#bib.bib50" title="" class="ltx_ref">2023a</a>)</cite>. Using GPT-4 as the judge, Alpaca Eval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib87" title="" class="ltx_ref">2023b</a>)</cite> and MT Bench&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a href="#bib.bib186" title="" class="ltx_ref">2023</a>)</cite> are two popular benchmarks that measure the comprehensive abilities of LM-based ChatBot. In coding tasks, synthetic environment is a common choice to aid human evaluation, as humans can make the assessment more efficiently via actual executions and analysis on running logs. <cite class="ltx_cite ltx_citemacro_cite">Gu et&nbsp;al. (<a href="#bib.bib55" title="" class="ltx_ref">2024</a>)</cite> propose CRUXEval, a code execution reasoning benchmark consisting of 800 Python functions generated by CodeLLaMA-34B. Similarly, <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib92" title="" class="ltx_ref">2024a</a>)</cite> introduce CodeMind, a framework to gauge the code reasoning abilities of LLMs on Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). All these evaluations based on synthetic data show strong correlation with real human judgements.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenges and Limitations of Synthetic Data</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">While synthetic data offers numerous benefits and applications, it is crucial to acknowledge and address the potential challenges and limitations associated with its use. This section delves into three significant concerns surrounding synthetic data:</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Misuse of synthetic data might proliferate misinformation.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">The potential misuse of synthetic data is a significant concern that must be addressed to ensure the responsible development of AI systems. Current AI models become increasingly capable of generating human-like data ranging from text&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemini-Team et&nbsp;al., <a href="#bib.bib48" title="" class="ltx_ref">2024</a>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, images&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Saharia et&nbsp;al., <a href="#bib.bib133" title="" class="ltx_ref">2022b</a>; Ramesh et&nbsp;al., <a href="#bib.bib129" title="" class="ltx_ref">2022</a>)</cite>, songs&nbsp;<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Make songs with Suno AI: <a target="_blank" href="https://app.suno.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://app.suno.ai/</a></span></span></span>, to even videos (e.g., OpenAI SORA&nbsp;<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>OpenAI Sora: <a target="_blank" href="https://openai.com/research/video-generation-models-as-world-simulators" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/video-generation-models-as-world-simulators</a></span></span></span>). This can be particularly dangerous when synthetic data is used to impersonate real people, manipulate public opinion, or influence political processes. Moreover, the dissemination of synthetic data-driven misinformation can erode trust in legitimate information sources, making it increasingly difficult for people to distinguish between truth and falsehood&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Byman et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; Rid, <a href="#bib.bib131" title="" class="ltx_ref">2020</a>)</cite>. To mitigate these risks, it is crucial for researchers, developers, and policymakers to establish clear guidelines and best practices for the ethical generation and use of synthetic data, including robust mechanisms for detecting and countering synthetic misinformation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Groh et&nbsp;al., <a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite>. By proactively addressing these challenges, we can harness the benefits of synthetic data while minimizing its potential for harm.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data might cause ambiguity in AI alignment.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">The increasing use of synthetic data in aligning AI models (e.g., Constitutional AI&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>) can introduce significant ambiguity and uncertainty. The goal of AI alignment is to ensure that AI systems behave in ways that are aligned with human values and intentions. However, synthetic data, which is artificially generated rather than collected from real-world sources, may not accurately represent the nuances and complexities of human values and preferences&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a href="#bib.bib189" title="" class="ltx_ref">2024</a>)</cite>. This discrepancy can lead to AI models learning from data that is biased&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2023b</a>; Liu et&nbsp;al., <a href="#bib.bib97" title="" class="ltx_ref">2021</a>)</cite>, ungrounded&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib98" title="" class="ltx_ref">2022</a>; Patel and Pavlick, <a href="#bib.bib121" title="" class="ltx_ref">2022</a>)</cite>, or misrepresentative of real-world scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Weidinger et&nbsp;al., <a href="#bib.bib168" title="" class="ltx_ref">2021</a>; Ji et&nbsp;al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>. As a result, AI systems trained on synthetic data may exhibit behaviors that are misaligned with human expectations, potentially leading to unintended consequences or even harmful actions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zou et&nbsp;al., <a href="#bib.bib191" title="" class="ltx_ref">2023</a>; Anderljung et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. Moreover, the ambiguity introduced by synthetic data can make it challenging to interpret and understand the decision-making processes of AI models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lightman et&nbsp;al., <a href="#bib.bib90" title="" class="ltx_ref">2023</a>)</cite>, further complicating the task of ensuring alignment. To mitigate these risks, it is crucial for researchers to carefully consider the limitations and potential drawbacks of using synthetic data in alignment research and to develop robust methods for validating and testing AI models trained on such data.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training with synthetic data makes evaluation decontamination harder.</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.2" class="ltx_p">The use of synthetic data in model training poses significant challenges to fair evaluation. Evaluation benchmarks are often created by referring to public text sources, such as coursework websites or forums. Consequently, it is arguable that all publicly available benchmark test cases might occasionally be included in the pre-training data of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib61" title="" class="ltx_ref">2022</a>; Gao et&nbsp;al., <a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>. The use of synthetic data exacerbates this issue rather than mitigating it. Although the community has proposed several techniques to detect such evaluation contamination, such as <span id="S4.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">min-<math id="S4.SS0.SSS0.Px3.p1.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.1.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1c">k</annotation></semantics></math>% prob</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a href="#bib.bib140" title="" class="ltx_ref">2023</a>)</cite>, which checks the probabilities of <math id="S4.SS0.SSS0.Px3.p1.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.2.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m1.1c">k</annotation></semantics></math> long-tail tokens, these token-level decontamination methods are inadequate when the model is trained with synthetic data. Synthetic data might include rephrased versions of the benchmark data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Oren et&nbsp;al., <a href="#bib.bib116" title="" class="ltx_ref">2023</a>; Mattern et&nbsp;al., <a href="#bib.bib107" title="" class="ltx_ref">2023</a>)</cite>, rendering token-level decontamination ineffective. In addition to developing more advanced evaluation contamination detection techniques, we recommend that model developers invest in creating and maintaining in-house and protected evaluation benchmarks. These proprietary benchmarks should be carefully safeguarded to prevent leakage and ensure the integrity of the evaluation process.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Directions for Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As the field of synthetic data continues to evolve, there are several promising directions for future research and development. This section outlines three key areas that warrant further exploration:</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data scaling.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">The impressive performance of many over-trained small language models (e.g., Mistral series models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>, and Gemma series models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma-Team et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2024</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">inter alia</span>) demonstrates the necessity of training with large amount of tokens (even passing the compute-optimal chinchilla law&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a href="#bib.bib127" title="" class="ltx_ref">2021</a>)</cite>). However, whether we have similar conclusions on the training with synthetic data is still an open question, as the quality of synthetic data may not be as consistent as real-world data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a href="#bib.bib176" title="" class="ltx_ref">2024</a>)</cite>. Future research should investigate the scaling laws for synthetic data and determine the optimal balance between the quantity and quality of synthetic samples. This exploration could help us understand the most effective strategies for leveraging synthetic data in training large-scale language models, potentially leading to more efficient and cost-effective approaches&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et&nbsp;al., <a href="#bib.bib111" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Further improving quality and diversity of synthetic data.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">While existing methods for generating synthetic data have shown promise, there is still room for improvement in terms of creating high-quality, attributed synthetic samples that closely mimic real-world data. Future research should focus on developing new advanced techniques (or based on existing ones such as Generative Adversarial Networks (GANs)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite> or Diffusion Models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ho et&nbsp;al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>, <span id="S5.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">inter alia</span>) that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. Additionally, researchers should explore methods that can incorporate domain-specific knowledge to ensure the generated data adheres to the underlying constraints and patterns present in the target domain (e.g., via Retrieval Augmented Generation (RAG)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a href="#bib.bib82" title="" class="ltx_ref">2020</a>; Borgeaud et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>) while maintaining the data quality. By advancing the state-of-the-art in attributed synthetic data generation, we can unlock new opportunities for privacy-preserving analysis&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Assefa et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, and model training across various fields, from healthcare (e.g., synthetic medical images&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Frid-Adar et&nbsp;al., <a href="#bib.bib43" title="" class="ltx_ref">2018</a>; Wei et&nbsp;al., <a href="#bib.bib162" title="" class="ltx_ref">2019</a>)</cite>) and finance (e.g., simulated trading trajectories&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a href="#bib.bib187" title="" class="ltx_ref">2022</a>)</cite>) to social sciences&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Argyle et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>; Park et&nbsp;al., <a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite> and beyond.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Towards high-fidelity and more efficient scalable oversight.</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">As AI models become increasingly complex and autonomous, it becomes challenging to monitor and assess their behavior using traditional oversight methods that rely on human supervision or real-world data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Amodei et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>. Future research should explore the use of synthetic data for high-fidelity scalable oversight of these advanced systems. Existing methods typically simulate a certain scenario in social iterations, such as debate&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Leike et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2018</a>)</cite>, reflection&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib180" title="" class="ltx_ref">2023a</a>)</cite>, or revisions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib99" title="" class="ltx_ref">2023c</a>)</cite> to obtain synthetic data, while new approaches could cover more comprehensive scenarios and more modalities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite>, as recent studies have found many issues of simulation that only covers a narrowed down&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> or over-simplified&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a href="#bib.bib189" title="" class="ltx_ref">2024</a>)</cite> scenes. Looking forward, another growing direction could be how to achieve scalable oversight more efficiently—given that we have the full control over the synthetic data generation, we can probably provide more targeted oversights with less synthetic data. As the need for effective AI governance and regulation grows, synthetic data will play an increasingly vital role in enabling more trustworthy scalable oversight mechanisms that promote robust, accountable, and safe deployment of AI technologies for the benefit of society&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Askell et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>; Bowman et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The emergent self-improvement capability.</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">We typically choose the most capable model to generate synthetic data, as its generation is of higher quality. However, an intriguing question arises: can a model generate synthetic data that is better than the data it was trained on, thus enabling it to improve itself? This concept of self-improvement through synthetic data generation is an exciting avenue for future research. If a model can generate higher-quality data than its original training set, it could potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>. This self-improvement capability could lead to the emergence of more advanced AI systems that can autonomously refine their skills and knowledge over time&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Burns et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>; Huang et&nbsp;al., <a href="#bib.bib65" title="" class="ltx_ref">2023b</a>)</cite>. Although recent work shows encouraging progress in this direction&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>; Yuan et&nbsp;al., <a href="#bib.bib177" title="" class="ltx_ref">2024</a>)</cite>, the upper bound of self-improvement and the underlying reasons for its effectiveness remain open questions. Future research should investigate the theoretical underpinnings and practical feasibility of self-improvement through synthetic data generation in more diverse scenarios, examining the necessary conditions, potential limitations, and associated risks. By unlocking the potential of emergent self-improvement capabilities, we could enable more adaptable, efficient, and autonomous learning processes&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a href="#bib.bib79" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Synthetic data has emerged as a promising solution to address the challenges of data scarcity, privacy concerns, and high costs in AI development. By generating realistic and diverse datasets, synthetic data enables the training and evaluation of AI models at scale across various domains. As we approach human-level or even superhuman-level intelligence, obtaining synthetic data becomes even more crucial, given that models need better-than-average-human quality data to progress. However, ensuring the factuality, fidelity, and lack of bias in synthetic data remains a critical challenge.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Future research directions on synthetic data could focus on improving the fidelity and controllability of generative models and developing standardized evaluation and contamination protocols and tools. We could also explore the integration of synthetic data with other techniques and its application in other domains. Despite the challenges, the potential benefits of synthetic data in advancing AI research are significant. By leveraging synthetic data responsibly and effectively, we can build more powerful, inclusive, and trustworthy AI systems that benefit society as a whole.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abay et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
N.&nbsp;C. Abay, Y.&nbsp;Zhou, M.&nbsp;Kantarcioglu, B.&nbsp;Thuraisingham, and L.&nbsp;Sweeney.

</span>
<span class="ltx_bibblock">Privacy preserving synthetic data release using deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Machine Learning and Knowledge Discovery in Databases:
European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018,
Proceedings, Part I 18</em>, pages 510–526. Springer, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abulkhanov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
D.&nbsp;Abulkhanov, N.&nbsp;Sorokin, S.&nbsp;Nikolenko, and V.&nbsp;Malykh.

</span>
<span class="ltx_bibblock">Lapca: Language-agnostic pretraining with cross-lingual alignment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval</em>, pages 2098–2102,
2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
M.&nbsp;Ahn, A.&nbsp;Brohan, N.&nbsp;Brown, Y.&nbsp;Chebotar, O.&nbsp;Cortes, B.&nbsp;David, C.&nbsp;Finn,
K.&nbsp;Gopalakrishnan, K.&nbsp;Hausman, A.&nbsp;Herzog, et&nbsp;al.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2204.01691, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2204.01691" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2204.01691</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amodei et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
D.&nbsp;Amodei, C.&nbsp;Olah, J.&nbsp;Steinhardt, P.&nbsp;Christiano, J.&nbsp;Schulman, and D.&nbsp;Mané.

</span>
<span class="ltx_bibblock">Concrete problems in ai safety.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/1606.06565, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1606.06565" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1606.06565</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderljung et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
M.&nbsp;Anderljung, J.&nbsp;Barnhart, J.&nbsp;Leung, A.&nbsp;Korinek, C.&nbsp;O’Keefe, J.&nbsp;Whittlestone,
S.&nbsp;Avin, M.&nbsp;Brundage, J.&nbsp;Bullock, D.&nbsp;Cass-Beggs, et&nbsp;al.

</span>
<span class="ltx_bibblock">Frontier ai regulation: Managing emerging risks to public safety.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2307.03718, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2307.03718" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2307.03718</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Argyle et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
L.&nbsp;P. Argyle, E.&nbsp;C. Busby, N.&nbsp;Fulda, J.&nbsp;R. Gubler, C.&nbsp;Rytting, and D.&nbsp;Wingate.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Political Analysis</em>, 31(3):337–351, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
A.&nbsp;Asai, X.&nbsp;Yu, J.&nbsp;Kasai, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">One question answering model for many languages with cross-lingual
dense passage retrieval.

</span>
<span class="ltx_bibblock">In M.&nbsp;Ranzato, A.&nbsp;Beygelzimer, Y.&nbsp;N. Dauphin, P.&nbsp;Liang, and J.&nbsp;W.
Vaughan, editors, <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual</em>, pages 7547–7560, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma, et&nbsp;al.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2112.00861, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.00861" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.00861</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assefa et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
S.&nbsp;A. Assefa, D.&nbsp;Dervovic, M.&nbsp;Mahfouz, R.&nbsp;E. Tillman, P.&nbsp;Reddy, and M.&nbsp;Veloso.

</span>
<span class="ltx_bibblock">Generating synthetic data in finance: opportunities, challenges and
pitfalls.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the First ACM International Conference on AI
in Finance</em>, pages 1–8, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Z.&nbsp;Azerbayev, H.&nbsp;Schoelkopf, K.&nbsp;Paster, M.&nbsp;D. Santos, S.&nbsp;McAleer, A.&nbsp;Q. Jiang,
J.&nbsp;Deng, S.&nbsp;Biderman, and S.&nbsp;Welleck.

</span>
<span class="ltx_bibblock">Llemma: An open language model for mathematics.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2310.10631, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2310.10631" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2310.10631</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babbar and Schölkopf (2019)</span>
<span class="ltx_bibblock">
R.&nbsp;Babbar and B.&nbsp;Schölkopf.

</span>
<span class="ltx_bibblock">Data scarcity, robustness and extreme multi-label classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em>, 108(8):1329–1351, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, S.&nbsp;Kadavath, S.&nbsp;Kundu, A.&nbsp;Askell, J.&nbsp;Kernion, A.&nbsp;Jones, A.&nbsp;Chen,
A.&nbsp;Goldie, A.&nbsp;Mirhoseini, C.&nbsp;McKinnon, et&nbsp;al.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2212.08073, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2212.08073" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2212.08073</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbierato et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
E.&nbsp;Barbierato, M.&nbsp;L.&nbsp;D. Vedova, D.&nbsp;Tessera, D.&nbsp;Toti, and N.&nbsp;Vanoli.

</span>
<span class="ltx_bibblock">A methodology for controlling bias and fairness in synthetic data
generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 12(9):4619, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
W.&nbsp;Bi, H.&nbsp;Li, and J.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Data augmentation for text generation without any augmented data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 2223–2237,
Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2021.acl-long.173" title="" class="ltx_ref">10.18653/v1/2021.acl-long.173</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.acl-long.173" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-long.173</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
S.&nbsp;Borgeaud, A.&nbsp;Mensch, J.&nbsp;Hoffmann, T.&nbsp;Cai, E.&nbsp;Rutherford, K.&nbsp;Millican,
G.&nbsp;van&nbsp;den Driessche, J.&nbsp;Lespiau, B.&nbsp;Damoc, A.&nbsp;Clark, D.&nbsp;de&nbsp;Las&nbsp;Casas,
A.&nbsp;Guy, J.&nbsp;Menick, R.&nbsp;Ring, T.&nbsp;Hennigan, S.&nbsp;Huang, L.&nbsp;Maggiore, C.&nbsp;Jones,
A.&nbsp;Cassirer, A.&nbsp;Brock, M.&nbsp;Paganini, G.&nbsp;Irving, O.&nbsp;Vinyals, S.&nbsp;Osindero,
K.&nbsp;Simonyan, J.&nbsp;W. Rae, E.&nbsp;Elsen, and L.&nbsp;Sifre.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In K.&nbsp;Chaudhuri, S.&nbsp;Jegelka, L.&nbsp;Song, C.&nbsp;Szepesvári, G.&nbsp;Niu,
and S.&nbsp;Sabato, editors, <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of
<em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2206–2240. PMLR,
2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v162/borgeaud22a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v162/borgeaud22a.html</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borisov et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
V.&nbsp;Borisov, K.&nbsp;Seßler, T.&nbsp;Leemann, M.&nbsp;Pawelczyk, and G.&nbsp;Kasneci.

</span>
<span class="ltx_bibblock">Language models are realistic tabular data generators.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2210.06280, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2210.06280" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2210.06280</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borkman et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
S.&nbsp;Borkman, A.&nbsp;Crespi, S.&nbsp;Dhakad, S.&nbsp;Ganguly, J.&nbsp;Hogins, Y.&nbsp;C. Jhang,
M.&nbsp;Kamalzadeh, B.&nbsp;Li, S.&nbsp;Leal, P.&nbsp;Parisi, C.&nbsp;Romero, W.&nbsp;Smith, A.&nbsp;Thaman,
S.&nbsp;Warren, and N.&nbsp;Yadav.

</span>
<span class="ltx_bibblock">Unity perception: Generate synthetic data for computer vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2107.04259, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2107.04259" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2107.04259</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
S.&nbsp;R. Bowman, J.&nbsp;Hyun, E.&nbsp;Perez, E.&nbsp;Chen, C.&nbsp;Pettit, S.&nbsp;Heiner,
K.&nbsp;Lukošiūtė, A.&nbsp;Askell, A.&nbsp;Jones, A.&nbsp;Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">Measuring progress on scalable oversight for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2211.03540, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.03540" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.03540</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burns et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
C.&nbsp;Burns, P.&nbsp;Izmailov, J.&nbsp;H. Kirchner, B.&nbsp;Baker, L.&nbsp;Gao, L.&nbsp;Aschenbrenner,
Y.&nbsp;Chen, A.&nbsp;Ecoffet, M.&nbsp;Joglekar, J.&nbsp;Leike, et&nbsp;al.

</span>
<span class="ltx_bibblock">Weak-to-strong generalization: Eliciting strong capabilities with
weak supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2312.09390, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.09390" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.09390</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
D.&nbsp;L. Byman, C.&nbsp;Gao, C.&nbsp;Meserole, and V.&nbsp;Subrahmanian.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Deepfakes and international conflict</em>.

</span>
<span class="ltx_bibblock">Brookings Institution, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carbune et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
V.&nbsp;Carbune, H.&nbsp;Mansoor, F.&nbsp;Liu, R.&nbsp;Aralikatte, G.&nbsp;Baechler, J.&nbsp;Chen, and
A.&nbsp;Sharma.

</span>
<span class="ltx_bibblock">Chart-based reasoning: Transferring capabilities from llms to vlms.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2403.12596, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.12596" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.12596</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
S.&nbsp;Casper, T.&nbsp;Bu, Y.&nbsp;Li, J.&nbsp;Li, K.&nbsp;Zhang, K.&nbsp;Hariharan, and D.&nbsp;Hadfield-Menell.

</span>
<span class="ltx_bibblock">Red teaming deep neural networks with feature synthesis tools.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023a.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
S.&nbsp;Casper, J.&nbsp;Lin, J.&nbsp;Kwon, G.&nbsp;Culp, and D.&nbsp;Hadfield-Menell.

</span>
<span class="ltx_bibblock">Explore, establish, exploit: Red teaming language models from
scratch.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2306.09442, 2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.09442" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.09442</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caswell et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
I.&nbsp;Caswell, C.&nbsp;Chelba, and D.&nbsp;Grangier.

</span>
<span class="ltx_bibblock">Tagged back-translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 53–63, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/W19-5206" title="" class="ltx_ref">10.18653/v1/W19-5206</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/W19-5206" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/W19-5206</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chauhan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
S.&nbsp;Chauhan, S.&nbsp;Saxena, and P.&nbsp;Daniel.

</span>
<span class="ltx_bibblock">Improved unsupervised neural machine translation with semantically
weighted back translation for morphologically rich and low resource
languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Neural Processing Letters</em>, 54(3):1707–1726, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Z.&nbsp;Chen, Y.&nbsp;Deng, H.&nbsp;Yuan, K.&nbsp;Ji, and Q.&nbsp;Gu.

</span>
<span class="ltx_bibblock">Self-play fine-tuning converts weak language models to strong
language models, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
M.&nbsp;Cheng, T.&nbsp;Piccardi, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">CoMPosT: Characterizing and evaluating caricature in LLM
simulations.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 10853–10875, Singapore, Dec. 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2023.emnlp-main.669" title="" class="ltx_ref">10.18653/v1/2023.emnlp-main.669</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.669" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.669</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
E.&nbsp;Chern, H.&nbsp;Zou, X.&nbsp;Li, J.&nbsp;Hu, K.&nbsp;Feng, J.&nbsp;Li, and P.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Generative ai for math: Abel.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/GAIR-NLP/abel" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/GAIR-NLP/abel</a>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Z.&nbsp;Chi, L.&nbsp;Dong, F.&nbsp;Wei, W.&nbsp;Wang, X.&nbsp;Mao, and H.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Cross-lingual natural language generation via pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of
Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium
on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020</em>, pages 7570–7577. AAAI Press, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aaai.org/ojs/index.php/AAAI/article/view/6256" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aaai.org/ojs/index.php/AAAI/article/view/6256</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
P.&nbsp;F. Christiano, J.&nbsp;Leike, T.&nbsp;B. Brown, M.&nbsp;Martic, S.&nbsp;Legg, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, editors, <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages
4299–4307, 2017.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
G.&nbsp;Cui, L.&nbsp;Yuan, N.&nbsp;Ding, G.&nbsp;Yao, W.&nbsp;Zhu, Y.&nbsp;Ni, G.&nbsp;Xie, Z.&nbsp;Liu, and M.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback,
2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahmen and Cook (2019)</span>
<span class="ltx_bibblock">
J.&nbsp;Dahmen and D.&nbsp;Cook.

</span>
<span class="ltx_bibblock">Synsys: A synthetic data generation system for healthcare
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 19(5):1181, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshpande et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
A.&nbsp;Deshpande, V.&nbsp;Murahari, T.&nbsp;Rajpurohit, A.&nbsp;Kalyan, and K.&nbsp;Narasimhan.

</span>
<span class="ltx_bibblock">Toxicity in chatgpt: Analyzing persona-assigned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2304.05335, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2304.05335" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2304.05335</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
B.&nbsp;Dhingra, M.&nbsp;Faruqui, A.&nbsp;Parikh, M.-W. Chang, D.&nbsp;Das, and W.&nbsp;Cohen.

</span>
<span class="ltx_bibblock">Handling divergent reference texts when evaluating table-to-text
generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4884–4895, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/P19-1483" title="" class="ltx_ref">10.18653/v1/P19-1483</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1483" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1483</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
N.&nbsp;Ding, Y.&nbsp;Chen, B.&nbsp;Xu, Y.&nbsp;Qin, Z.&nbsp;Zheng, S.&nbsp;Hu, Z.&nbsp;Liu, M.&nbsp;Sun, and B.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional
conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.14233, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.14233" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.14233</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
S.&nbsp;Edunov, M.&nbsp;Ott, M.&nbsp;Auli, and D.&nbsp;Grangier.

</span>
<span class="ltx_bibblock">Understanding back-translation at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489–500, Brussels, Belgium, 2018.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/D18-1045" title="" class="ltx_ref">10.18653/v1/D18-1045</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D18-1045" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D18-1045</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El&nbsp;Emam et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
K.&nbsp;El&nbsp;Emam, L.&nbsp;Mosquera, and R.&nbsp;Hoptroff.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Practical synthetic data generation: balancing privacy and the
broad availability of data</em>.

</span>
<span class="ltx_bibblock">O’Reilly Media, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epaliyana et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
K.&nbsp;Epaliyana, S.&nbsp;Ranathunga, and S.&nbsp;Jayasena.

</span>
<span class="ltx_bibblock">Improving back-translation with iterative filtering and data
selection for sinhala-english nmt.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2021 Moratuwa Engineering Research Conference (MERCon)</em>,
pages 438–443. IEEE, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everitt et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
T.&nbsp;Everitt, M.&nbsp;Hutter, R.&nbsp;Kumar, and V.&nbsp;Krakovna.

</span>
<span class="ltx_bibblock">Reward tampering problems and solutions in reinforcement learning: A
causal influence diagram perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Synthese</em>, 198(Suppl 27):6435–6467, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falke et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
T.&nbsp;Falke, L.&nbsp;F.&nbsp;R. Ribeiro, P.&nbsp;A. Utama, I.&nbsp;Dagan, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Ranking generated summaries by correctness: An interesting but
challenging application for natural language inference.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2214–2220, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/P19-1213" title="" class="ltx_ref">10.18653/v1/P19-1213</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1213" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1213</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
S.&nbsp;Feng, V.&nbsp;Balachandran, Y.&nbsp;Bai, and Y.&nbsp;Tsvetkov.

</span>
<span class="ltx_bibblock">FactKB: Generalizable factuality evaluation using language models
enhanced with factual knowledge.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 933–952, Singapore, Dec. 2023a. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2023.emnlp-main.59" title="" class="ltx_ref">10.18653/v1/2023.emnlp-main.59</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.59" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.59</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
S.&nbsp;Feng, C.&nbsp;Y. Park, Y.&nbsp;Liu, and Y.&nbsp;Tsvetkov.

</span>
<span class="ltx_bibblock">From pretraining data to language models to downstream tasks:
Tracking the trails of political biases leading to unfair nlp models.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.08283, 2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.08283" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.08283</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frid-Adar et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
M.&nbsp;Frid-Adar, E.&nbsp;Klang, M.&nbsp;Amitai, J.&nbsp;Goldberger, and H.&nbsp;Greenspan.

</span>
<span class="ltx_bibblock">Synthetic data augmentation using gan for improved liver lesion
classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">2018 IEEE 15th international symposium on biomedical imaging
(ISBI 2018)</em>, pages 289–293. IEEE, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
D.&nbsp;Ganguli, L.&nbsp;Lovitt, J.&nbsp;Kernion, A.&nbsp;Askell, Y.&nbsp;Bai, S.&nbsp;Kadavath, B.&nbsp;Mann,
E.&nbsp;Perez, N.&nbsp;Schiefer, K.&nbsp;Ndousse, et&nbsp;al.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling
behaviors, and lessons learned.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2209.07858, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2209.07858" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2209.07858</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, S.&nbsp;Biderman, S.&nbsp;Black, L.&nbsp;Golding, T.&nbsp;Hoppe, C.&nbsp;Foster, J.&nbsp;Phang,
H.&nbsp;He, A.&nbsp;Thite, N.&nbsp;Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2101.00027, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2101.00027" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2101.00027</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
L.&nbsp;Gao, J.&nbsp;Schulman, and J.&nbsp;Hilton.

</span>
<span class="ltx_bibblock">Scaling laws for reward model overoptimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
10835–10866. PMLR, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini-Team et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Gemini-Team, R.&nbsp;Anil, S.&nbsp;Borgeaud, Y.&nbsp;Wu, J.-B. Alayrac, J.&nbsp;Yu, R.&nbsp;Soricut,
J.&nbsp;Schalkwyk, A.&nbsp;M. Dai, A.&nbsp;Hauth, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2312.11805, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.11805" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.11805</a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini-Team et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Gemini-Team, M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap,
J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2403.05530, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.05530" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.05530</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma-Team et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Gemma-Team, T.&nbsp;Mesnard, C.&nbsp;Hardin, R.&nbsp;Dadashi, S.&nbsp;Bhupatiraju, S.&nbsp;Pathak,
L.&nbsp;Sifre, M.&nbsp;Rivière, M.&nbsp;S. Kale, J.&nbsp;Love, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2403.08295, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.08295" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.08295</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, 120(30):e2305016120, 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1073/pnas.2305016120" title="" class="ltx_ref">10.1073/pnas.2305016120</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.pnas.org/doi/abs/10.1073/pnas.2305016120" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.pnas.org/doi/abs/10.1073/pnas.2305016120</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, 120(30):e2305016120, 2023b.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
I.&nbsp;Goodfellow, J.&nbsp;Pouget-Abadie, M.&nbsp;Mirza, B.&nbsp;Xu, D.&nbsp;Warde-Farley, S.&nbsp;Ozair,
A.&nbsp;Courville, and Y.&nbsp;Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 63(11):139–144, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graça et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
M.&nbsp;Graça, Y.&nbsp;Kim, J.&nbsp;Schamper, S.&nbsp;Khadivi, and H.&nbsp;Ney.

</span>
<span class="ltx_bibblock">Generalizing back-translation in neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 45–52, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/W19-5205" title="" class="ltx_ref">10.18653/v1/W19-5205</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/W19-5205" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/W19-5205</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
M.&nbsp;Groh, Z.&nbsp;Epstein, C.&nbsp;Firestone, and R.&nbsp;Picard.

</span>
<span class="ltx_bibblock">Deepfake detection by human crowds, machines, and machine-informed
crowds.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, 119(1):e2110013119, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
A.&nbsp;Gu, B.&nbsp;Rozière, H.&nbsp;Leather, A.&nbsp;Solar-Lezama, G.&nbsp;Synnaeve, and S.&nbsp;I.
Wang.

</span>
<span class="ltx_bibblock">Cruxeval: A benchmark for code reasoning, understanding and
execution.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2401.03065, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2401.03065" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2401.03065</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guarnera et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
L.&nbsp;Guarnera, O.&nbsp;Giudice, and S.&nbsp;Battiato.

</span>
<span class="ltx_bibblock">Deepfake detection by analyzing convolutional traces.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition workshops</em>, pages 666–667, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
A.&nbsp;Gupta, D.&nbsp;Bhatt, and A.&nbsp;Pandey.

</span>
<span class="ltx_bibblock">Transitioning from real to synthetic data: Quantifying the bias in
model.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2105.04144, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2105.04144" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2105.04144</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haluptzok et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
P.&nbsp;Haluptzok, M.&nbsp;Bowers, and A.&nbsp;T. Kalai.

</span>
<span class="ltx_bibblock">Language models can teach themselves to program better.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2207.14502, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2207.14502" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2207.14502</a>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
M.&nbsp;Heusel, H.&nbsp;Ramsauer, T.&nbsp;Unterthiner, B.&nbsp;Nessler, and S.&nbsp;Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, editors, <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages
6626–6637, 2017.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
J.&nbsp;Ho, A.&nbsp;Jain, and P.&nbsp;Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin,
editors, <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</a>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford,
D.&nbsp;de&nbsp;Las&nbsp;Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">An empirical analysis of compute-optimal large language model
training.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:30016–30030, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
O.&nbsp;Honovich, L.&nbsp;Choshen, R.&nbsp;Aharoni, E.&nbsp;Neeman, I.&nbsp;Szpektor, and O.&nbsp;Abend.

</span>
<span class="ltx_bibblock"><math id="bib.bib62.1.m1.1" class="ltx_Math" alttext="q^{2}" display="inline"><semantics id="bib.bib62.1.m1.1a"><msup id="bib.bib62.1.m1.1.1" xref="bib.bib62.1.m1.1.1.cmml"><mi id="bib.bib62.1.m1.1.1.2" xref="bib.bib62.1.m1.1.1.2.cmml">q</mi><mn id="bib.bib62.1.m1.1.1.3" xref="bib.bib62.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib62.1.m1.1b"><apply id="bib.bib62.1.m1.1.1.cmml" xref="bib.bib62.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib62.1.m1.1.1.1.cmml" xref="bib.bib62.1.m1.1.1">superscript</csymbol><ci id="bib.bib62.1.m1.1.1.2.cmml" xref="bib.bib62.1.m1.1.1.2">𝑞</ci><cn type="integer" id="bib.bib62.1.m1.1.1.3.cmml" xref="bib.bib62.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib62.1.m1.1c">q^{2}</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded
dialogues via question generation and question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7856–7870, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2021.emnlp-main.619" title="" class="ltx_ref">10.18653/v1/2021.emnlp-main.619</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.619" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.emnlp-main.619</a>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howe et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
B.&nbsp;Howe, J.&nbsp;Stoyanovich, H.&nbsp;Ping, B.&nbsp;Herman, and M.&nbsp;Gee.

</span>
<span class="ltx_bibblock">Synthetic data for social good.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/1710.08874, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1710.08874" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1710.08874</a>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
F.&nbsp;Huang, H.&nbsp;Kwak, and J.&nbsp;An.

</span>
<span class="ltx_bibblock">Is chatgpt better than human annotators? potential and limitations of
chatgpt in explaining implicit hate speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2302.07736, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2302.07736" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2302.07736</a>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
J.&nbsp;Huang, S.&nbsp;Gu, L.&nbsp;Hou, Y.&nbsp;Wu, X.&nbsp;Wang, H.&nbsp;Yu, and J.&nbsp;Han.

</span>
<span class="ltx_bibblock">Large language models can self-improve.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 1051–1068, Singapore, Dec. 2023b. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2023.emnlp-main.67" title="" class="ltx_ref">10.18653/v1/2023.emnlp-main.67</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.67" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.67</a>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
W.&nbsp;Huang, F.&nbsp;Xia, T.&nbsp;Xiao, H.&nbsp;Chan, J.&nbsp;Liang, P.&nbsp;Florence, A.&nbsp;Zeng, J.&nbsp;Tompson,
I.&nbsp;Mordatch, Y.&nbsp;Chebotar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2207.05608, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2207.05608" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2207.05608</a>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hubinger et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
E.&nbsp;Hubinger, C.&nbsp;Denison, J.&nbsp;Mu, M.&nbsp;Lambert, M.&nbsp;Tong, M.&nbsp;MacDiarmid, T.&nbsp;Lanham,
D.&nbsp;M. Ziegler, T.&nbsp;Maxwell, N.&nbsp;Cheng, et&nbsp;al.

</span>
<span class="ltx_bibblock">Sleeper agents: Training deceptive llms that persist through safety
training.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2401.05566, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2401.05566" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2401.05566</a>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Z.&nbsp;Ji, N.&nbsp;Lee, R.&nbsp;Frieske, T.&nbsp;Yu, D.&nbsp;Su, Y.&nbsp;Xu, E.&nbsp;Ishii, Y.&nbsp;J. Bang,
A.&nbsp;Madotto, and P.&nbsp;Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 55(12):1–38, 2023.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
C.&nbsp;Jia, Y.&nbsp;Yang, Y.&nbsp;Xia, Y.&nbsp;Chen, Z.&nbsp;Parekh, H.&nbsp;Pham, Q.&nbsp;V. Le, Y.&nbsp;Sung, Z.&nbsp;Li,
and T.&nbsp;Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock">In M.&nbsp;Meila and T.&nbsp;Zhang, editors, <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event</em>, volume 139 of <em id="bib.bib69.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, pages 4904–4916. PMLR, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v139/jia21b.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v139/jia21b.html</a>.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Mensch, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l.
Casas, F.&nbsp;Bressand, G.&nbsp;Lengyel, G.&nbsp;Lample, L.&nbsp;Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2310.06825, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2310.06825" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2310.06825</a>.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Y.&nbsp;Jiang, A.&nbsp;Gupta, Z.&nbsp;Zhang, G.&nbsp;Wang, Y.&nbsp;Dou, Y.&nbsp;Chen, L.&nbsp;Fei-Fei,
A.&nbsp;Anandkumar, Y.&nbsp;Zhu, and L.&nbsp;Fan.

</span>
<span class="ltx_bibblock">Vima: General robot manipulation with multimodal prompts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">NeurIPS 2022 Foundation Models for Decision Making
Workshop</em>, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
E.&nbsp;Jones, H.&nbsp;Palangi, C.&nbsp;Simões, V.&nbsp;Chandrasekaran, S.&nbsp;Mukherjee, A.&nbsp;Mitra,
A.&nbsp;Awadallah, and E.&nbsp;Kamar.

</span>
<span class="ltx_bibblock">Teaching language models to hallucinate less with synthetic tasks,
2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2310.06827" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2310.06827</a>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kambhampati et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
S.&nbsp;Kambhampati, K.&nbsp;Valmeekam, L.&nbsp;Guan, K.&nbsp;Stechly, M.&nbsp;Verma, S.&nbsp;Bhambri,
L.&nbsp;Saldyt, and A.&nbsp;Murthy.

</span>
<span class="ltx_bibblock">Llms can’t plan, but can help planning in llm-modulo frameworks.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01817</em>, 2024.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
V.&nbsp;Kumar, N.&nbsp;Joshi, A.&nbsp;Mukherjee, G.&nbsp;Ramakrishnan, and P.&nbsp;Jyothi.

</span>
<span class="ltx_bibblock">Cross-lingual training for automatic question generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4863–4872, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/P19-1481" title="" class="ltx_ref">10.18653/v1/P19-1481</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1481" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1481</a>.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
R.&nbsp;Lam, A.&nbsp;Sanchez-Gonzalez, M.&nbsp;Willson, P.&nbsp;Wirnsberger, M.&nbsp;Fortunato, F.&nbsp;Alet,
S.&nbsp;Ravuri, T.&nbsp;Ewalds, Z.&nbsp;Eaton-Rosen, W.&nbsp;Hu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Learning skillful medium-range global weather forecasting.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Science</em>, 382(6677):1416–1421, 2023.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landers and Behrend (2023)</span>
<span class="ltx_bibblock">
R.&nbsp;N. Landers and T.&nbsp;S. Behrend.

</span>
<span class="ltx_bibblock">Auditing the ai auditors: A framework for evaluating fairness and
bias in high stakes ai predictive models.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">American Psychologist</em>, 78(1):36, 2023.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
H.&nbsp;Laurençon, L.&nbsp;Tronchon, and V.&nbsp;Sanh.

</span>
<span class="ltx_bibblock">Unlocking the conversion of web screenshots into html code with the
websight dataset, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.09029" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.09029</a>.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
H.&nbsp;Le, Y.&nbsp;Wang, A.&nbsp;D. Gotmare, S.&nbsp;Savarese, and S.&nbsp;C.&nbsp;H. Hoi.

</span>
<span class="ltx_bibblock">Coderl: Mastering code generation through pretrained models and deep
reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:21314–21328, 2022.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun (2022)</span>
<span class="ltx_bibblock">
Y.&nbsp;LeCun.

</span>
<span class="ltx_bibblock">A path towards autonomous machine intelligence version 0.9. 2,
2022-06-27.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Open Review</em>, 62, 2022.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
K.&nbsp;Lee, M.&nbsp;Joshi, I.&nbsp;R. Turc, H.&nbsp;Hu, F.&nbsp;Liu, J.&nbsp;M. Eisenschlos, U.&nbsp;Khandelwal,
P.&nbsp;Shaw, M.-W. Chang, and K.&nbsp;Toutanova.

</span>
<span class="ltx_bibblock">Pix2struct: Screenshot parsing as pretraining for visual language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
18893–18912. PMLR, 2023.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leike et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
J.&nbsp;Leike, D.&nbsp;Krueger, T.&nbsp;Everitt, M.&nbsp;Martic, V.&nbsp;Maini, and S.&nbsp;Legg.

</span>
<span class="ltx_bibblock">Scalable agent alignment via reward modeling: a research direction.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/1811.07871, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1811.07871" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1811.07871</a>.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
P.&nbsp;S.&nbsp;H. Lewis, E.&nbsp;Perez, A.&nbsp;Piktus, F.&nbsp;Petroni, V.&nbsp;Karpukhin, N.&nbsp;Goyal,
H.&nbsp;Küttler, M.&nbsp;Lewis, W.&nbsp;Yih, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, and
D.&nbsp;Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin,
editors, <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
A.&nbsp;Lewkowycz, A.&nbsp;Andreassen, D.&nbsp;Dohan, E.&nbsp;Dyer, H.&nbsp;Michalewski, V.&nbsp;Ramasesh,
A.&nbsp;Slone, C.&nbsp;Anil, I.&nbsp;Schlag, T.&nbsp;Gutman-Solo, Y.&nbsp;Wu, B.&nbsp;Neyshabur,
G.&nbsp;Gur-Ari, and V.&nbsp;Misra.

</span>
<span class="ltx_bibblock">Solving quantitative reasoning problems with language models, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2206.14858" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2206.14858</a>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Callison-Burch (2023)</span>
<span class="ltx_bibblock">
B.&nbsp;Li and C.&nbsp;Callison-Burch.

</span>
<span class="ltx_bibblock">Paxqa: Generating cross-lingual question answering examples at
training scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2304.12206, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2304.12206" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2304.12206</a>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
C.&nbsp;Li, W.&nbsp;Wang, J.&nbsp;Hu, Y.&nbsp;Wei, N.&nbsp;Zheng, H.&nbsp;Hu, Z.&nbsp;Zhang, and H.&nbsp;Peng.

</span>
<span class="ltx_bibblock">Common 7b language models already possess strong math capabilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2403.04706, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.04706" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.04706</a>.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
L.&nbsp;Li, R.&nbsp;Carver, I.&nbsp;Lopez-Gomez, F.&nbsp;Sha, and J.&nbsp;Anderson.

</span>
<span class="ltx_bibblock">Seeds: Emulation of weather forecast ensembles with diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2306.14066, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.14066" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.14066</a>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
X.&nbsp;Li, T.&nbsp;Zhang, Y.&nbsp;Dubois, R.&nbsp;Taori, I.&nbsp;Gulrajani, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/alpaca_eval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/alpaca_eval</a>, 2023b.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Liang, W.&nbsp;Huang, F.&nbsp;Xia, P.&nbsp;Xu, K.&nbsp;Hausman, B.&nbsp;Ichter, P.&nbsp;Florence, and
A.&nbsp;Zeng.

</span>
<span class="ltx_bibblock">Code as policies: Language model programs for embodied control.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2209.07753, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2209.07753" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2209.07753</a>.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
B.&nbsp;Liao, S.&nbsp;Khadivi, and S.&nbsp;Hewavitharana.

</span>
<span class="ltx_bibblock">Back-translation for large-scale multilingual machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth Conference on Machine Translation</em>,
pages 418–424, Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.wmt-1.50" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.wmt-1.50</a>.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
H.&nbsp;Lightman, V.&nbsp;Kosaraju, Y.&nbsp;Burda, H.&nbsp;Edwards, B.&nbsp;Baker, T.&nbsp;Lee, J.&nbsp;Leike,
J.&nbsp;Schulman, I.&nbsp;Sutskever, and K.&nbsp;Cobbe.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.20050, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.20050" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.20050</a>.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
S.&nbsp;Lin, J.&nbsp;Hilton, and O.&nbsp;Evans.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252,
Dublin, Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2022.acl-long.229" title="" class="ltx_ref">10.18653/v1/2022.acl-long.229</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.acl-long.229" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.acl-long.229</a>.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024a)</span>
<span class="ltx_bibblock">
C.&nbsp;Liu, S.&nbsp;D. Zhang, and R.&nbsp;Jabbarvand.

</span>
<span class="ltx_bibblock">Codemind: A framework to challenge large language models for code
reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2402.09664, 2024a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2402.09664" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2402.09664</a>.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
F.&nbsp;Liu, J.&nbsp;Eisenschlos, F.&nbsp;Piccinno, S.&nbsp;Krichene, C.&nbsp;Pang, K.&nbsp;Lee, M.&nbsp;Joshi,
W.&nbsp;Chen, N.&nbsp;Collier, and Y.&nbsp;Altun.

</span>
<span class="ltx_bibblock">Deplot: One-shot visual language reasoning by plot-to-table
translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 10381–10399, 2023a.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
F.&nbsp;Liu, F.&nbsp;Piccinno, S.&nbsp;Krichene, C.&nbsp;Pang, K.&nbsp;Lee, M.&nbsp;Joshi, Y.&nbsp;Altun,
N.&nbsp;Collier, and J.&nbsp;Eisenschlos.

</span>
<span class="ltx_bibblock">Matcha: Enhancing visual language pretraining with math reasoning and
chart derendering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 12756–12770,
2023b.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Yao (2024)</span>
<span class="ltx_bibblock">
H.&nbsp;Liu and A.&nbsp;C.-C. Yao.

</span>
<span class="ltx_bibblock">Augmenting math word problems via iterative question composing.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2401.09003, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2401.09003" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2401.09003</a>.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024b)</span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 36,
2024b.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
R.&nbsp;Liu, C.&nbsp;Jia, J.&nbsp;Wei, G.&nbsp;Xu, L.&nbsp;Wang, and S.&nbsp;Vosoughi.

</span>
<span class="ltx_bibblock">Mitigating political bias in language models through reinforced
calibration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in
Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</em>,
pages 14857–14866. AAAI Press, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/17744" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ojs.aaai.org/index.php/AAAI/article/view/17744</a>.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
R.&nbsp;Liu, J.&nbsp;Wei, S.&nbsp;S. Gu, T.-Y. Wu, S.&nbsp;Vosoughi, C.&nbsp;Cui, D.&nbsp;Zhou, and A.&nbsp;M.
Dai.

</span>
<span class="ltx_bibblock">Mind’s eye: Grounded language model reasoning through simulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2210.05359, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2210.05359" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2210.05359</a>.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
R.&nbsp;Liu, R.&nbsp;Yang, C.&nbsp;Jia, G.&nbsp;Zhang, D.&nbsp;Zhou, A.&nbsp;M. Dai, D.&nbsp;Yang, and
S.&nbsp;Vosoughi.

</span>
<span class="ltx_bibblock">Training socially aligned language models in simulated human society.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.16960, 2023c.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.16960" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.16960</a>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
W.&nbsp;Liu, W.&nbsp;Zeng, K.&nbsp;He, Y.&nbsp;Jiang, and J.&nbsp;He.

</span>
<span class="ltx_bibblock">What makes good data for alignment? a comprehensive study of
automatic data selection in instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2312.15685, 2023d.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.15685" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.15685</a>.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Y.&nbsp;Lu, M.&nbsp;Shen, H.&nbsp;Wang, X.&nbsp;Wang, C.&nbsp;van Rechem, and W.&nbsp;Wei.

</span>
<span class="ltx_bibblock">Machine learning for synthetic data generation: a review.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2302.04062, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2302.04062" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2302.04062</a>.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucini (2021)</span>
<span class="ltx_bibblock">
F.&nbsp;Lucini.

</span>
<span class="ltx_bibblock">The real deal about synthetic data.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">MIT Sloan Management Review</em>, 63(1):1–4,
2021.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
H.&nbsp;Luo, Q.&nbsp;Sun, C.&nbsp;Xu, P.&nbsp;Zhao, J.&nbsp;Lou, C.&nbsp;Tao, X.&nbsp;Geng, Q.&nbsp;Lin, S.&nbsp;Chen, and
D.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Wizardmath: Empowering mathematical reasoning for large language
models via reinforced evol-instruct.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2308.09583, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2308.09583" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2308.09583</a>.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Z.&nbsp;Luo, C.&nbsp;Xu, P.&nbsp;Zhao, Q.&nbsp;Sun, X.&nbsp;Geng, W.&nbsp;Hu, C.&nbsp;Tao, J.&nbsp;Ma, Q.&nbsp;Lin, and
D.&nbsp;Jiang.

</span>
<span class="ltx_bibblock">Wizardcoder: Empowering code large language models with
evol-instruct.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2306.08568, 2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.08568" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.08568</a>.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marie et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
B.&nbsp;Marie, R.&nbsp;Rubino, and A.&nbsp;Fujita.

</span>
<span class="ltx_bibblock">Tagged back-translation revisited: Why does it really work?

</span>
<span class="ltx_bibblock">In <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 5990–5997, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2020.acl-main.532" title="" class="ltx_ref">10.18653/v1/2020.acl-main.532</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2020.acl-main.532" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.acl-main.532</a>.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
A.&nbsp;Masry, P.&nbsp;Kavehzadeh, X.&nbsp;L. Do, E.&nbsp;Hoque, and S.&nbsp;Joty.

</span>
<span class="ltx_bibblock">UniChart: A universal vision-language pretrained model for chart
comprehension and reasoning.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 14662–14684, Singapore, 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2023.emnlp-main.906" title="" class="ltx_ref">10.18653/v1/2023.emnlp-main.906</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.906" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.906</a>.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mattern et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
J.&nbsp;Mattern, F.&nbsp;Mireshghallah, Z.&nbsp;Jin, B.&nbsp;Schölkopf, M.&nbsp;Sachan, and
T.&nbsp;Berg-Kirkpatrick.

</span>
<span class="ltx_bibblock">Membership inference attacks against language models via
neighbourhood comparison.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.18462, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.18462" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.18462</a>.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Y.&nbsp;Meng, J.&nbsp;Huang, Y.&nbsp;Zhang, and J.&nbsp;Han.

</span>
<span class="ltx_bibblock">Generating training data with language models: Towards zero-shot
language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:462–477, 2022.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2023)</span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Meta and microsoft introduce the next generation of llama.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ai.meta.com/blog/llama-2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.meta.com/blog/llama-2</a>, 2023.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
S.&nbsp;Min, K.&nbsp;Krishna, X.&nbsp;Lyu, M.&nbsp;Lewis, W.-t. Yih, P.&nbsp;W. Koh, M.&nbsp;Iyyer,
L.&nbsp;Zettlemoyer, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Factscore: Fine-grained atomic evaluation of factual precision in
long form text generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14251</em>, 2023.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
N.&nbsp;Muennighoff, A.&nbsp;Rush, B.&nbsp;Barak, T.&nbsp;Le&nbsp;Scao, N.&nbsp;Tazi, A.&nbsp;Piktus, S.&nbsp;Pyysalo,
T.&nbsp;Wolf, and C.&nbsp;A. Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
S.&nbsp;Mukherjee, A.&nbsp;Mitra, G.&nbsp;Jawahar, S.&nbsp;Agarwal, H.&nbsp;Palangi, and A.&nbsp;Awadallah.

</span>
<span class="ltx_bibblock">Orca: Progressive learning from complex explanation traces of gpt-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2306.02707, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.02707" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.02707</a>.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko (2021)</span>
<span class="ltx_bibblock">
S.&nbsp;I. Nikolenko.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Synthetic data for deep learning</em>, volume 174.

</span>
<span class="ltx_bibblock">Springer, 2021.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ntoutsi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
E.&nbsp;Ntoutsi, P.&nbsp;Fafalios, U.&nbsp;Gadiraju, V.&nbsp;Iosifidis, W.&nbsp;Nejdl, M.-E. Vidal,
S.&nbsp;Ruggieri, F.&nbsp;Turini, S.&nbsp;Papadopoulos, E.&nbsp;Krasanakis, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bias in data-driven artificial intelligence systems—an introductory
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</em>, 10(3):e1356, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oren et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Y.&nbsp;Oren, N.&nbsp;Meister, N.&nbsp;Chatterji, F.&nbsp;Ladhak, and T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Proving test set contamination in black box language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2310.17623, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2310.17623" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2310.17623</a>.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
L.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;L. Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang,
S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray, J.&nbsp;Schulman, J.&nbsp;Hilton, F.&nbsp;Kelton, L.&nbsp;Miller,
M.&nbsp;Simens, A.&nbsp;Askell, P.&nbsp;Welinder, P.&nbsp;Christiano, J.&nbsp;Leike, and R.&nbsp;Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2203.02155, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2203.02155" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2203.02155</a>.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
A.&nbsp;Pan, K.&nbsp;Bhatia, and J.&nbsp;Steinhardt.

</span>
<span class="ltx_bibblock">The effects of reward misspecification: Mapping and mitigating
misaligned models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=JYtwGwIL7ye" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=JYtwGwIL7ye</a>.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
J.&nbsp;S. Park, J.&nbsp;O’Brien, C.&nbsp;J. Cai, M.&nbsp;R. Morris, P.&nbsp;Liang, and M.&nbsp;S. Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th Annual ACM Symposium on User
Interface Software and Technology</em>, pages 1–22, 2023.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paster et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
K.&nbsp;Paster, M.&nbsp;D. Santos, Z.&nbsp;Azerbayev, and J.&nbsp;Ba.

</span>
<span class="ltx_bibblock">Openwebmath: An open dataset of high-quality mathematical web text.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2310.06786, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2310.06786" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2310.06786</a>.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel and Pavlick (2022)</span>
<span class="ltx_bibblock">
R.&nbsp;Patel and E.&nbsp;Pavlick.

</span>
<span class="ltx_bibblock">Mapping language models to grounded conceptual spaces.

</span>
<span class="ltx_bibblock">In <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=gJcEM8sxHK" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=gJcEM8sxHK</a>.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Huang, F.&nbsp;Song, T.&nbsp;Cai, R.&nbsp;Ring, J.&nbsp;Aslanides, A.&nbsp;Glaese,
N.&nbsp;McAleese, and G.&nbsp;Irving.

</span>
<span class="ltx_bibblock">Red teaming language models with language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 3419–3448, Abu Dhabi, United Arab
Emirates, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.emnlp-main.225" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.emnlp-main.225</a>.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Ringer, K.&nbsp;Lukošiūtė, K.&nbsp;Nguyen, E.&nbsp;Chen, S.&nbsp;Heiner,
C.&nbsp;Pettit, C.&nbsp;Olsson, S.&nbsp;Kundu, S.&nbsp;Kadavath, et&nbsp;al.

</span>
<span class="ltx_bibblock">Discovering language model behaviors with model-written evaluations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13387–13434.
Association for Computational Linguistics, 2023.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
H.&nbsp;Pham, X.&nbsp;Wang, Y.&nbsp;Yang, and G.&nbsp;Neubig.

</span>
<span class="ltx_bibblock">Meta back-translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=3jjmdp7Hha" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=3jjmdp7Hha</a>.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Przystupa and Abdul-Mageed (2019)</span>
<span class="ltx_bibblock">
M.&nbsp;Przystupa and M.&nbsp;Abdul-Mageed.

</span>
<span class="ltx_bibblock">Neural machine translation of low-resource and similar languages with
backtranslation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Conference on Machine Translation
(Volume 3: Shared Task Papers, Day 2)</em>, pages 224–235, Florence, Italy,
2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/W19-5431" title="" class="ltx_ref">10.18653/v1/W19-5431</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/W19-5431" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/W19-5431</a>.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;W. Kim, C.&nbsp;Hallacy, A.&nbsp;Ramesh, G.&nbsp;Goh, S.&nbsp;Agarwal, G.&nbsp;Sastry,
A.&nbsp;Askell, P.&nbsp;Mishkin, J.&nbsp;Clark, G.&nbsp;Krueger, and I.&nbsp;Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In M.&nbsp;Meila and T.&nbsp;Zhang, editors, <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event</em>, volume 139 of <em id="bib.bib126.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, pages 8748–8763. PMLR, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v139/radford21a.html</a>.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, S.&nbsp;Borgeaud, T.&nbsp;Cai, K.&nbsp;Millican, J.&nbsp;Hoffmann, F.&nbsp;Song,
J.&nbsp;Aslanides, S.&nbsp;Henderson, R.&nbsp;Ring, S.&nbsp;Young, E.&nbsp;Rutherford, T.&nbsp;Hennigan,
J.&nbsp;Menick, A.&nbsp;Cassirer, R.&nbsp;Powell, G.&nbsp;v.&nbsp;d. Driessche, L.&nbsp;A. Hendricks,
M.&nbsp;Rauh, P.-S. Huang, A.&nbsp;Glaese, J.&nbsp;Welbl, S.&nbsp;Dathathri, S.&nbsp;Huang, J.&nbsp;Uesato,
J.&nbsp;Mellor, I.&nbsp;Higgins, A.&nbsp;Creswell, N.&nbsp;McAleese, A.&nbsp;Wu, E.&nbsp;Elsen,
S.&nbsp;Jayakumar, E.&nbsp;Buchatskaya, D.&nbsp;Budden, E.&nbsp;Sutherland, K.&nbsp;Simonyan,
M.&nbsp;Paganini, L.&nbsp;Sifre, L.&nbsp;Martens, X.&nbsp;L. Li, A.&nbsp;Kuncoro, A.&nbsp;Nematzadeh,
E.&nbsp;Gribovskaya, D.&nbsp;Donato, A.&nbsp;Lazaridou, A.&nbsp;Mensch, J.-B. Lespiau,
M.&nbsp;Tsimpoukelli, N.&nbsp;Grigorev, D.&nbsp;Fritz, T.&nbsp;Sottiaux, M.&nbsp;Pajarskas, T.&nbsp;Pohlen,
Z.&nbsp;Gong, D.&nbsp;Toyama, C.&nbsp;d.&nbsp;M. d’Autume, Y.&nbsp;Li, T.&nbsp;Terzi, V.&nbsp;Mikulik,
I.&nbsp;Babuschkin, A.&nbsp;Clark, D.&nbsp;d.&nbsp;L. Casas, A.&nbsp;Guy, C.&nbsp;Jones, J.&nbsp;Bradbury,
M.&nbsp;Johnson, B.&nbsp;Hechtman, L.&nbsp;Weidinger, I.&nbsp;Gabriel, W.&nbsp;Isaac, E.&nbsp;Lockhart,
S.&nbsp;Osindero, L.&nbsp;Rimell, C.&nbsp;Dyer, O.&nbsp;Vinyals, K.&nbsp;Ayoub, J.&nbsp;Stanway,
L.&nbsp;Bennett, D.&nbsp;Hassabis, K.&nbsp;Kavukcuoglu, and G.&nbsp;Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.11446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
R.&nbsp;Rafailov, A.&nbsp;Sharma, E.&nbsp;Mitchell, S.&nbsp;Ermon, C.&nbsp;D. Manning, and C.&nbsp;Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a
reward model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:258959321" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:258959321</a>.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
A.&nbsp;Ramesh, P.&nbsp;Dhariwal, A.&nbsp;Nichol, C.&nbsp;Chu, and M.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2204.06125, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2204.06125" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2204.06125</a>.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riabi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
A.&nbsp;Riabi, T.&nbsp;Scialom, R.&nbsp;Keraron, B.&nbsp;Sagot, D.&nbsp;Seddah, and J.&nbsp;Staiano.

</span>
<span class="ltx_bibblock">Synthetic data augmentation for zero-shot cross-lingual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7016–7030, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2021.emnlp-main.562" title="" class="ltx_ref">10.18653/v1/2021.emnlp-main.562</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.562" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.emnlp-main.562</a>.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rid (2020)</span>
<span class="ltx_bibblock">
T.&nbsp;Rid.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">Active measures: The secret history of disinformation and
political warfare</em>.

</span>
<span class="ltx_bibblock">Farrar, Straus and Giroux, 2020.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
C.&nbsp;Saharia, W.&nbsp;Chan, S.&nbsp;Saxena, L.&nbsp;Li, J.&nbsp;Whang, E.&nbsp;L. Denton, K.&nbsp;Ghasemipour,
R.&nbsp;Gontijo&nbsp;Lopes, B.&nbsp;Karagol&nbsp;Ayan, T.&nbsp;Salimans, J.&nbsp;Ho, D.&nbsp;J. Fleet, and
M.&nbsp;Norouzi.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock">In S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh,
editors, <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume&nbsp;35,
pages 36479–36494. Curran Associates, Inc., 2022a.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf</a>.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
C.&nbsp;Saharia, W.&nbsp;Chan, S.&nbsp;Saxena, L.&nbsp;Li, J.&nbsp;Whang, E.&nbsp;L. Denton, K.&nbsp;Ghasemipour,
R.&nbsp;Gontijo&nbsp;Lopes, B.&nbsp;Karagol&nbsp;Ayan, T.&nbsp;Salimans, et&nbsp;al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
35:36479–36494, 2022b.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxton et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
D.&nbsp;Saxton, E.&nbsp;Grefenstette, F.&nbsp;Hill, and P.&nbsp;Kohli.

</span>
<span class="ltx_bibblock">Analysing mathematical reasoning abilities of neural models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=H1gR5iR5FX" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=H1gR5iR5FX</a>.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro,
L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
R.&nbsp;Sennrich, B.&nbsp;Haddow, and A.&nbsp;Birch.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin,
Germany, 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/P16-1009" title="" class="ltx_ref">10.18653/v1/P16-1009</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P16-1009" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P16-1009</a>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shakeri et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
S.&nbsp;Shakeri, N.&nbsp;Constant, M.&nbsp;Kale, and L.&nbsp;Xue.

</span>
<span class="ltx_bibblock">Towards zero-shot multilingual synthetic question and answer
generation for cross-lingual reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th International Conference on Natural
Language Generation</em>, pages 35–45, Aberdeen, Scotland, UK, 2021. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.inlg-1.4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.inlg-1.4</a>.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Z.&nbsp;Shao, P.&nbsp;Wang, Q.&nbsp;Zhu, R.&nbsp;Xu, J.&nbsp;Song, M.&nbsp;Zhang, Y.&nbsp;K. Li, Y.&nbsp;Wu, and
D.&nbsp;Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open
language models, 2024.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
M.&nbsp;Sharma, M.&nbsp;Tong, T.&nbsp;Korbak, D.&nbsp;Duvenaud, A.&nbsp;Askell, S.&nbsp;R. Bowman, E.&nbsp;DURMUS,
Z.&nbsp;Hatfield-Dodds, S.&nbsp;R. Johnston, S.&nbsp;M. Kravec, T.&nbsp;Maxwell, S.&nbsp;McCandlish,
K.&nbsp;Ndousse, O.&nbsp;Rausch, N.&nbsp;Schiefer, D.&nbsp;Yan, M.&nbsp;Zhang, and E.&nbsp;Perez.

</span>
<span class="ltx_bibblock">Towards understanding sycophancy in language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
W.&nbsp;Shi, A.&nbsp;Ajith, M.&nbsp;Xia, Y.&nbsp;Huang, D.&nbsp;Liu, T.&nbsp;Blevins, D.&nbsp;Chen, and
L.&nbsp;Zettlemoyer.

</span>
<span class="ltx_bibblock">Detecting pretraining data from large language models, 2023.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, F.&nbsp;Cassano, A.&nbsp;Gopinath, K.&nbsp;Narasimhan, and S.&nbsp;Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shypula et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
A.&nbsp;Shypula, A.&nbsp;Madaan, Y.&nbsp;Zeng, U.&nbsp;Alon, J.&nbsp;Gardner, M.&nbsp;Hashemi, G.&nbsp;Neubig,
P.&nbsp;Ranganathan, O.&nbsp;Bastani, and A.&nbsp;Yazdanbakhsh.

</span>
<span class="ltx_bibblock">Learning performance-improving code edits.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2302.07867, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2302.07867" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2302.07867</a>.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
C.&nbsp;Si, Y.&nbsp;Zhang, Z.&nbsp;Yang, R.&nbsp;Liu, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">Design2code: How far are we from automating front-end engineering?,
2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.03163" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.03163</a>.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, S.&nbsp;Azizi, T.&nbsp;Tu, S.&nbsp;S. Mahdavi, J.&nbsp;Wei, H.&nbsp;W. Chung, N.&nbsp;Scales,
A.&nbsp;Tanwani, H.&nbsp;Cole-Lewis, S.&nbsp;Pfohl, et&nbsp;al.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2212.13138, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2212.13138" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2212.13138</a>.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinhardt (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Steinhardt.

</span>
<span class="ltx_bibblock">Ml systems will have weird failure modes.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/</a>,
2022.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, S.&nbsp;Shen, S.&nbsp;Cao, H.&nbsp;Liu, C.&nbsp;Li, Y.&nbsp;Shen, C.&nbsp;Gan, L.-Y. Gui, Y.-X. Wang,
Y.&nbsp;Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Aligning large multimodal models with factually augmented rlhf.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2309.14525, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2309.14525" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2309.14525</a>.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Q.&nbsp;Tang, Z.&nbsp;Deng, H.&nbsp;Lin, X.&nbsp;Han, Q.&nbsp;Liang, and L.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Toolalpaca: Generalized tool learning for language models with 3000
simulated cases.

</span>
<span class="ltx_bibblock"><em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2306.05301, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.05301" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.05301</a>.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
R.&nbsp;Taylor, M.&nbsp;Kardas, G.&nbsp;Cucurull, T.&nbsp;Scialom, A.&nbsp;Hartshorn, E.&nbsp;Saravia,
A.&nbsp;Poulton, V.&nbsp;Kerkez, and R.&nbsp;Stojnic.

</span>
<span class="ltx_bibblock">Galactica: A large language model for science.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2211.09085, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.09085" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.09085</a>.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
R.&nbsp;Thoppilan, D.&nbsp;De&nbsp;Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.-T. Cheng,
A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du, Y.&nbsp;Li, H.&nbsp;Lee, H.&nbsp;S. Zheng, A.&nbsp;Ghafouri,
M.&nbsp;Menegali, Y.&nbsp;Huang, M.&nbsp;Krikun, D.&nbsp;Lepikhin, J.&nbsp;Qin, D.&nbsp;Chen, Y.&nbsp;Xu,
Z.&nbsp;Chen, A.&nbsp;Roberts, M.&nbsp;Bosma, V.&nbsp;Zhao, Y.&nbsp;Zhou, C.-C. Chang, I.&nbsp;Krivokon,
W.&nbsp;Rusch, M.&nbsp;Pickett, P.&nbsp;Srinivasan, L.&nbsp;Man, K.&nbsp;Meier-Hellstern, M.&nbsp;R.
Morris, T.&nbsp;Doshi, R.&nbsp;D. Santos, T.&nbsp;Duke, J.&nbsp;Soraker, B.&nbsp;Zevenbergen,
V.&nbsp;Prabhakaran, M.&nbsp;Diaz, B.&nbsp;Hutchinson, K.&nbsp;Olson, A.&nbsp;Molina, E.&nbsp;Hoffman-John,
J.&nbsp;Lee, L.&nbsp;Aroyo, R.&nbsp;Rajakumar, A.&nbsp;Butryna, M.&nbsp;Lamm, V.&nbsp;Kuzmina, J.&nbsp;Fenton,
A.&nbsp;Cohen, R.&nbsp;Bernstein, R.&nbsp;Kurzweil, B.&nbsp;Aguera-Arcas, C.&nbsp;Cui, M.&nbsp;Croak,
E.&nbsp;Chi, and Q.&nbsp;Le.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2201.08239, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2201.08239" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.08239</a>.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
K.&nbsp;Tian, E.&nbsp;Mitchell, H.&nbsp;Yao, C.&nbsp;D. Manning, and C.&nbsp;Finn.

</span>
<span class="ltx_bibblock">Fine-tuning language models for factuality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:265158181" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:265158181</a>.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Todorov et&nbsp;al. (2012)</span>
<span class="ltx_bibblock">
E.&nbsp;Todorov, T.&nbsp;Erez, and Y.&nbsp;Tassa.

</span>
<span class="ltx_bibblock">Mujoco: A physics engine for model-based control.

</span>
<span class="ltx_bibblock">In <em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems</em>, pages 5026–5033. IEEE, 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/IROS.2012.6386109" title="" class="ltx_ref">10.1109/IROS.2012.6386109</a>.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei,
N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2307.09288, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
T.&nbsp;H. Trinh, Y.&nbsp;Wu, Q.&nbsp;V. Le, H.&nbsp;He, and T.&nbsp;Luong.

</span>
<span class="ltx_bibblock">Solving olympiad geometry without human demonstrations.

</span>
<span class="ltx_bibblock"><em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 625(7995):476–482, 2024.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Breugel et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
B.&nbsp;Van&nbsp;Breugel, Z.&nbsp;Qian, and M.&nbsp;Van Der&nbsp;Schaar.

</span>
<span class="ltx_bibblock">Synthetic data, real errors: how (not) to publish and use synthetic
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
34793–34808. PMLR, 2023.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vezhnevets et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
A.&nbsp;S. Vezhnevets, J.&nbsp;P. Agapiou, A.&nbsp;Aharon, R.&nbsp;Ziv, J.&nbsp;Matyas, E.&nbsp;A.
Duéñez-Guzmán, W.&nbsp;A. Cunningham, S.&nbsp;Osindero, D.&nbsp;Karmon, and
J.&nbsp;Z. Leibo.

</span>
<span class="ltx_bibblock">Generative agent-based modeling with actions grounded in physical,
social, or digital space using concordia.

</span>
<span class="ltx_bibblock"><em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2312.03664, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.03664" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.03664</a>.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
P.&nbsp;Villalobos, J.&nbsp;Sevilla, L.&nbsp;Heim, T.&nbsp;Besiroglu, M.&nbsp;Hobbhahn, and A.&nbsp;Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2211.04325, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.04325" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.04325</a>.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
G.&nbsp;Wang, Y.&nbsp;Xie, Y.&nbsp;Jiang, A.&nbsp;Mandlekar, C.&nbsp;Xiao, Y.&nbsp;Zhu, L.&nbsp;Fan, and
A.&nbsp;Anandkumar.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.16291, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.16291" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.16291</a>.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;Le, E.&nbsp;Chi, S.&nbsp;Narang, A.&nbsp;Chowdhery, and
D.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language
models.

</span>
<span class="ltx_bibblock">2022a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2203.11171" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2203.11171</a>.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, Y.&nbsp;Kordi, S.&nbsp;Mishra, A.&nbsp;Liu, N.&nbsp;A. Smith, D.&nbsp;Khashabi, and
H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated
instructions.

</span>
<span class="ltx_bibblock">volume abs/2212.10560, 2022b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2212.10560" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2212.10560</a>.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, X.&nbsp;Wang, B.&nbsp;An, D.&nbsp;Yu, and C.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Towards faithful neural table-to-text generation with
content-matching constraints.

</span>
<span class="ltx_bibblock">In <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1072–1086, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2020.acl-main.101" title="" class="ltx_ref">10.18653/v1/2020.acl-main.101</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2020.acl-main.101" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.acl-main.101</a>.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, A.&nbsp;Suriawinata, L.&nbsp;Vaickus, B.&nbsp;Ren, X.&nbsp;Liu, J.&nbsp;Wei, and S.&nbsp;Hassanpour.

</span>
<span class="ltx_bibblock">Generative image translation for data augmentation in colorectal
histopathology images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2019.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, M.&nbsp;Bosma, V.&nbsp;Y. Zhao, K.&nbsp;Guu, A.&nbsp;W. Yu, B.&nbsp;Lester, N.&nbsp;Du, A.&nbsp;M. Dai,
and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=gEZrGCozdqR" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, L.&nbsp;Hou, A.&nbsp;Lampinen, X.&nbsp;Chen, D.&nbsp;Huang, Y.&nbsp;Tay, X.&nbsp;Chen, Y.&nbsp;Lu,
D.&nbsp;Zhou, T.&nbsp;Ma, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Symbol tuning improves in-context learning in language models.

</span>
<span class="ltx_bibblock">volume abs/2305.08298, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.08298" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.08298</a>.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, D.&nbsp;Huang, Y.&nbsp;Lu, D.&nbsp;Zhou, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Simple synthetic data reduces sycophancy in large language models,
2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2308.03958" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2308.03958</a>.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
J.&nbsp;Wei, C.&nbsp;Yang, X.&nbsp;Song, Y.&nbsp;Lu, N.&nbsp;Hu, D.&nbsp;Tran, D.&nbsp;Peng, R.&nbsp;Liu, D.&nbsp;Huang,
C.&nbsp;Du, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Long-form factuality in large language models.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:268724304" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:268724304</a>.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Y.&nbsp;Wei, Z.&nbsp;Wang, J.&nbsp;Liu, Y.&nbsp;Ding, and L.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Magicoder: Source code is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2312.02120, 2023c.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.02120" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.02120</a>.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
L.&nbsp;Weidinger, J.&nbsp;Mellor, M.&nbsp;Rauh, C.&nbsp;Griffin, J.&nbsp;Uesato, P.-S. Huang, M.&nbsp;Cheng,
M.&nbsp;Glaese, B.&nbsp;Balle, A.&nbsp;Kasirzadeh, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ethical and social risks of harm from language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2112.04359, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.04359" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.04359</a>.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wood et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
E.&nbsp;Wood, T.&nbsp;Baltrusaitis, C.&nbsp;Hewitt, S.&nbsp;Dziadzio, T.&nbsp;J. Cashman, and
J.&nbsp;Shotton.

</span>
<span class="ltx_bibblock">Fake it till you make it: face analysis in the wild using synthetic
data alone.

</span>
<span class="ltx_bibblock">In <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/CVF International Conference on Computer Vision,
ICCV 2021, Montreal, QC, Canada, October 10-17, 2021</em>, pages 3661–3671.
IEEE, 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICCV48922.2021.00366" title="" class="ltx_ref">10.1109/ICCV48922.2021.00366</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/ICCV48922.2021.00366" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV48922.2021.00366</a>.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
C.&nbsp;Xu, Q.&nbsp;Sun, K.&nbsp;Zheng, X.&nbsp;Geng, P.&nbsp;Zhao, J.&nbsp;Feng, C.&nbsp;Tao, and D.&nbsp;Jiang.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex
instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2304.12244, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2304.12244" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2304.12244</a>.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Xu, Y.&nbsp;Ruan, W.&nbsp;Bi, G.&nbsp;Huang, S.&nbsp;Shi, L.&nbsp;Chen, and L.&nbsp;Liu.

</span>
<span class="ltx_bibblock">On synthetic data for back translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 419–430, Seattle, United States, 2022. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2022.naacl-main.32" title="" class="ltx_ref">10.18653/v1/2022.naacl-main.32</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.naacl-main.32" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.naacl-main.32</a>.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
L.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua,
and C.&nbsp;Raffel.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11934</em>, 2020.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
J.&nbsp;Yang, A.&nbsp;Prabhakar, K.&nbsp;Narasimhan, and S.&nbsp;Yao.

</span>
<span class="ltx_bibblock">Intercode: Standardizing and benchmarking interactive coding with
execution feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
J.&nbsp;Ye, S.&nbsp;Li, G.&nbsp;Li, C.&nbsp;Huang, S.&nbsp;Gao, Y.&nbsp;Wu, Q.&nbsp;Zhang, T.&nbsp;Gui, and X.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Toolsword: Unveiling safety issues of large language models in tool
learning across three stages.

</span>
<span class="ltx_bibblock"><em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2402.10753, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2402.10753" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2402.10753</a>.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
L.&nbsp;Yu, W.&nbsp;Jiang, H.&nbsp;Shi, J.&nbsp;Yu, Z.&nbsp;Liu, Y.&nbsp;Zhang, J.&nbsp;T. Kwok, Z.&nbsp;Li, A.&nbsp;Weller,
and W.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Metamath: Bootstrap your own mathematical questions for large
language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2309.12284, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2309.12284" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2309.12284</a>.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Y.&nbsp;Yu, Y.&nbsp;Zhuang, J.&nbsp;Zhang, Y.&nbsp;Meng, A.&nbsp;J. Ratner, R.&nbsp;Krishna, J.&nbsp;Shen, and
C.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Large language model as attributed training data generator: A tale of
diversity and bias.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
W.&nbsp;Yuan, R.&nbsp;Y. Pang, K.&nbsp;Cho, S.&nbsp;Sukhbaatar, J.&nbsp;Xu, and J.&nbsp;Weston.

</span>
<span class="ltx_bibblock">Self-rewarding language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2401.10020, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2401.10020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2401.10020</a>.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Li, G.&nbsp;Dong, C.&nbsp;Tan, and C.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Scaling relationship on learning mathematical reasoning with large
language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2308.01825, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2308.01825" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2308.01825</a>.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
E.&nbsp;Zelikman, Y.&nbsp;Wu, and N.&nbsp;D. Goodman.

</span>
<span class="ltx_bibblock">Star: Bootstrapping reasoning with reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:247762790" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:247762790</a>.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, X.&nbsp;Xu, and S.&nbsp;Deng.

</span>
<span class="ltx_bibblock">Exploring collaboration mechanisms for llm agents: A social
psychology view.

</span>
<span class="ltx_bibblock"><em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2310.02124, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2310.02124" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2310.02124</a>.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, L.&nbsp;Dong, X.&nbsp;Li, S.&nbsp;Zhang, X.&nbsp;Sun, S.&nbsp;Wang, J.&nbsp;Li, R.&nbsp;Hu, T.&nbsp;Zhang,
F.&nbsp;Wu, and G.&nbsp;Wang.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey,
2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2308.10792" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2308.10792</a>.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, Y.&nbsp;Li, L.&nbsp;Cui, D.&nbsp;Cai, L.&nbsp;Liu, T.&nbsp;Fu, X.&nbsp;Huang, E.&nbsp;Zhao, Y.&nbsp;Zhang,
Y.&nbsp;Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: A survey on hallucination in large
language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2309.01219, 2023c.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2309.01219" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2309.01219</a>.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, R.&nbsp;Zhang, J.&nbsp;Gu, Y.&nbsp;Zhou, N.&nbsp;Lipka, D.&nbsp;Yang, and T.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2306.17107, 2023d.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.17107" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.17107</a>.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
B.&nbsp;Zhao, B.&nbsp;Wu, and T.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Svit: Scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2307.04087, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2307.04087" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2307.04087</a>.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
J.&nbsp;Zhao, T.&nbsp;Wang, M.&nbsp;Yatskar, V.&nbsp;Ordonez, and K.-W. Chang.

</span>
<span class="ltx_bibblock">Gender bias in coreference resolution: Evaluation and debiasing
methods.

</span>
<span class="ltx_bibblock">In <em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</em>, pages 15–20, New Orleans, Louisiana,
2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/N18-2003" title="" class="ltx_ref">10.18653/v1/N18-2003</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/N18-2003" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/N18-2003</a>.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
L.&nbsp;Zheng, W.-L. Chiang, Y.&nbsp;Sheng, S.&nbsp;Zhuang, Z.&nbsp;Wu, Y.&nbsp;Zhuang, Z.&nbsp;Lin, Z.&nbsp;Li,
D.&nbsp;Li, E.&nbsp;P. Xing, H.&nbsp;Zhang, J.&nbsp;E. Gonzalez, and I.&nbsp;Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
S.&nbsp;Zheng, A.&nbsp;Trott, S.&nbsp;Srinivasa, D.&nbsp;C. Parkes, and R.&nbsp;Socher.

</span>
<span class="ltx_bibblock">The ai economist: Taxation policy design via two-level deep
multiagent reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">Science advances</em>, 8(18):eabk2607, 2022.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Z.&nbsp;Zheng, H.&nbsp;Zhou, S.&nbsp;Huang, L.&nbsp;Li, X.&nbsp;Dai, and J.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Mirror-generative neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=HkxQRTNYPH" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=HkxQRTNYPH</a>.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
X.&nbsp;Zhou, Z.&nbsp;Su, T.&nbsp;Eisape, H.&nbsp;Kim, and M.&nbsp;Sap.

</span>
<span class="ltx_bibblock">Is this the real life? is this just fantasy? the misleading success
of simulating social interactions with llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2403.05020, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2403.05020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2403.05020</a>.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
C.&nbsp;Ziems, J.&nbsp;Dwivedi-Yu, Y.-C. Wang, A.&nbsp;Halevy, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">Normbank: A knowledge bank of situational social norms.

</span>
<span class="ltx_bibblock"><em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2305.17008, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.17008" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.17008</a>.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
A.&nbsp;Zou, Z.&nbsp;Wang, J.&nbsp;Z. Kolter, and M.&nbsp;Fredrikson.

</span>
<span class="ltx_bibblock">Universal and transferable adversarial attacks on aligned language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2307.15043, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2307.15043" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2307.15043</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.07502" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.07503" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.07503">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.07503" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.07504" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 23:38:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
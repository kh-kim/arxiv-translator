<html lang="en" data-theme="dark"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Best Practices and Lessons Learned on Synthetic Data for Language Models</title>
<!--Generated on Thu Apr 11 06:33:42 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.07503v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
      <div class="html-header-logo">
        <a href="https://arxiv.org/">
          <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
          <span class="sr-only">Back to arXiv</span>
        </a>
      </div>
  
      <!--TOC, dark mode, links-->
      <div class="html-header-nav">
        <!--back to abstract-->
        
          <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.07503v1">
          <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
              <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
          </svg>
          </a>
        <!--dark mode-->
        <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="Dark mode">
          <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
          </label>
          <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
          </label>
          <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
          </label>
        </a>
        <!--nav-->
        <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
          <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
        </button>
      </div>
      </header><header class="desktop_header">
      <div class="html-header-logo">
        <a href="https://arxiv.org/">
            <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
            <span class="sr-only">Back to arXiv</span>
        </a>
      </div>
      <div class="html-header-message" role="banner">
          <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
          </p>
      </div>
      <nav class="html-header-nav">
        <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
        <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
        <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.07503v1">Back to Abstract</a>
        <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.07503v1" target="_blank">Download PDF</a>
        <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

        <div id="listIcon" type="button" class="hide">
            <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
            <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
            </svg>
        </div>
        <div id="arrowIcon" type="button">
            <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
            <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
            </svg>
        </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S1" title="1 Introduction ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2" title="2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Synthetic Data in Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1" title="2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1.SSS0.Px1" title="Math. ‣ 2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Math.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1.SSS0.Px2" title="Code. ‣ 2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Code.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1.SSS0.Px3" title="Other reasoning tasks. ‣ 2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Other reasoning tasks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS2" title="2.2 Tool-using and Planning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tool-using and Planning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS2.SSS0.Px1" title="Learning tool-using through synthetic trajectories. ‣ 2.2 Tool-using and Planning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Learning tool-using through synthetic trajectories.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS2.SSS0.Px2" title="Learning to plan in synthetic environments. ‣ 2.2 Tool-using and Planning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Learning to plan in synthetic environments.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS3" title="2.3 Multimodality ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Multimodality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS3.SSS0.Px1" title="Reverse rendering from vision to text. ‣ 2.3 Multimodality ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Reverse rendering from vision to text.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS3.SSS0.Px2" title="Multi-modality instruction following. ‣ 2.3 Multimodality ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Multi-modality instruction following.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS4" title="2.4 Multilingual ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Multilingual</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS4.SSS0.Px1" title="Back-translation augmentation. ‣ 2.4 Multilingual ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Back-translation augmentation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS4.SSS0.Px2" title="Generating multilingual questions and answers at scale. ‣ 2.4 Multilingual ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Generating multilingual questions and answers at scale.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5" title="2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5.SSS0.Px1" title="Instruction Following. ‣ 2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Instruction Following.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5.SSS0.Px2" title="Mitigating hallucination. ‣ 2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Mitigating hallucination.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5.SSS0.Px3" title="Aligning with shared human preference and values. ‣ 2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Aligning with shared human preference and values.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3" title="3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Synthetic Data in Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3.SS0.SSS0.Px1" title="Factuality. ‣ 3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Factuality.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3.SS0.SSS0.Px2" title="Safety. ‣ 3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Safety.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3.SS0.SSS0.Px3" title="Assisting human evaluation. ‣ 3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Assisting human evaluation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4" title="4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Challenges and Limitations of Synthetic Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4.SS0.SSS0.Px1" title="Misuse of synthetic data might proliferate misinformation. ‣ 4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Misuse of synthetic data might proliferate misinformation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4.SS0.SSS0.Px2" title="Synthetic data might cause ambiguity in AI alignment. ‣ 4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Synthetic data might cause ambiguity in AI alignment.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4.SS0.SSS0.Px3" title="Training with synthetic data makes evaluation decontamination harder. ‣ 4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Training with synthetic data makes evaluation decontamination harder.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5" title="5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Directions for Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px1" title="Synthetic data scaling. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Synthetic data scaling.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px2" title="Further improving quality and diversity of synthetic data. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Further improving quality and diversity of synthetic data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px3" title="Towards high-fidelity and more efficient scalable oversight. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Towards high-fidelity and more efficient scalable oversight.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px4" title="The emergent self-improvement capability. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">The emergent self-improvement capability.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S6" title="6 Conclusion ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2404.07503v1 [cs.CL] 11 Apr 2024</div></div>
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\correspondingauthor</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">ruiboliu@google.com














</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<h1 class="ltx_title ltx_title_document">Best Practices and Lessons Learned on Synthetic Data for Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruibo Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jerry Wei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fangyu Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenglei Si
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Stanford University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanzhe Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Georgia Institute of Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinmeng Rao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daiyi Peng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Diyi Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Stanford University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denny Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew M. Dai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating artificial data that mimics real-world patterns. This paper provides an overview of synthetic data research, discussing its applications, challenges, and future directions. We present empirical evidence from prior art to demonstrate its effectiveness and highlight the importance of ensuring its factuality, fidelity, and unbiasedness. We emphasize the need for responsible use of synthetic data to build more powerful, inclusive, and trustworthy language models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="389" id="S1.F1.g1" src="extracted/5529949/assets/manufacture_hd.png" width="389">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>One synthetic image generated by Imagen&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Saharia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib132" title="">2022a</a>)</cite> v2.0, with a prompt including the following description: <span class="ltx_text ltx_font_italic" id="S1.F1.2.1">“In a robotics factory, humanoid robots collaborate on an assembly line to design, fabricate, test, and assemble new robots. The new robots they are manufacturing look similar to those robotic workers who are creating them.”</span> We also added some style controlling text from aesthetic considerations.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancement of artificial intelligence (AI) technologies has led to their widespread adoption across numerous domains, from assistant agents (e.g., ACT-1, from Adept AI<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>ACT-1: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adept.ai/blog/act-1" title="">https://www.adept.ai/blog/act-1</a></span></span></span>) and software development (e.g., Devin, from Cognition Lab<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Devin: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cognition-labs.com/introducing-devin" title="">https://www.cognition-labs.com/introducing-devin</a></span></span></span>) to healthcare <cite class="ltx_cite ltx_citemacro_citep">(Singhal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib144" title="">2022</a>)</cite> and finance <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib187" title="">2022</a>)</cite>. However, the success of AI models heavily relies on the availability of large, diverse, and high-quality datasets for training and evaluation. Acquiring such datasets can be a significant challenge due to data scarcity <cite class="ltx_cite ltx_citemacro_citep">(Babbar and Schölkopf, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib11" title="">2019</a>)</cite>, privacy concerns <cite class="ltx_cite ltx_citemacro_citep">(Abay et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib1" title="">2019</a>)</cite>, and the sheer cost of data collection and annotation <cite class="ltx_cite ltx_citemacro_citep">(Gilardi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib51" title="">2023b</a>)</cite>. Pessimists predict that we will run out of fresh text data in 2050 and image data in 2060 <cite class="ltx_cite ltx_citemacro_citep">(Villalobos et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib157" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Synthetic data has emerged as a promising solution to address these challenges <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib113" title="">2021</a>)</cite>. Synthetic data refers to artificially generated data that mimics the characteristics and patterns of real-world data, but is created through algorithms <cite class="ltx_cite ltx_citemacro_citep">(Saxton et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib134" title="">2019</a>)</cite>, generative models <cite class="ltx_cite ltx_citemacro_citep">(Borisov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib16" title="">2022</a>; Meng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib108" title="">2022</a>)</cite>, or even simulations <cite class="ltx_cite ltx_citemacro_citep">(Vezhnevets et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib156" title="">2023</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib99" title="">2023c</a>)</cite>, rather than being directly created by humans. By leveraging synthetic data, we can not only overcome the limitations of real-world data but also unlock the potential to develop more robust, reliable, and fair AI models <cite class="ltx_cite ltx_citemacro_citep">(Lucini, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib102" title="">2021</a>; Lu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib101" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">One of the many benefits of synthetic data is that it can be generated at scale, providing an abundant supply of training and testing data for AI models. This is particularly valuable in domains where real-world data is scarce or difficult to obtain (e.g., weather data covering all conditions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib86" title="">2023a</a>; Lam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib75" title="">2023</a>)</cite>). Second, synthetic data can be tailored to specific requirements, such as ensuring a balanced representation of different classes by introducing controlled variations (e.g., up-weighting low-resource languages in multilingual language learning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Przystupa and Abdul-Mageed, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib125" title="">2019</a>)</cite>). This level of control over data characteristics can improve model performance and generalization. Third, synthetic data can help mitigate privacy concerns by creating anonymized or de-identified datasets that do not contain sensitive personal information&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Howe et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib63" title="">2017</a>; El&nbsp;Emam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib37" title="">2020</a>)</cite>. This is crucial in domains such as healthcare, where patient privacy is of utmost importance&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dahmen and Cook, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib32" title="">2019</a>; Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib162" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite its promise, synthetic data also presents challenges that need to be addressed. One of them is ensuring the factuality and fidelity of synthetic data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wood et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib169" title="">2021</a>; Heusel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib59" title="">2017</a>)</cite>, as models trained on false, hallucinated or biased synthetic data may fail to generalize to real-world scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Van&nbsp;Breugel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib155" title="">2023</a>; Guarnera et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib56" title="">2020</a>)</cite>. Researchers must develop sophisticated generative models and evaluation metrics to create synthetic data that accurately reflects the complex patterns and relationships found in real-world data. Another challenge is the potential for synthetic data to amplify biases or introduce new biases if not carefully designed and validated&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Barbierato et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib13" title="">2022</a>; Gupta et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib57" title="">2021</a>)</cite>. We believe rigorous testing and fairness assessments are necessary to mitigate these risks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we track the current state of synthetic data research and discuss current best practices and lessons learned. The rest of the paper is organized as follows. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2" title="2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of synthetic data generation techniques and their applications in model training, presenting case studies and empirical evidence. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3" title="3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> discusses the usefulness of synthetic data in evaluation. Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4" title="4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> discusses the challenges and limitations of synthetic data, and in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5" title="5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> we outline potential solutions and future research directions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Synthetic Data in Training</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Synthetic data, which is generated by mimicking authentic data collected from the real world, has proven to be an effective and relatively low-cost alternative of real data. This section explores several notable domains that leverage synthetic training data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Math.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">Recent advancements in mathematical reasoning for language models (LMs) have led to the development of various approaches to improve performance on math-related tasks. One approach is to train on math-targeted pre-training data, such as Minerva&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib83" title="">2022</a>)</cite>, Llemma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib10" title="">2023</a>)</cite>, and DeepSeekMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib138" title="">2024</a>)</cite>. Another mainstream method is to generate synthetic questions and answers to imitate the training or validation set of target benchmarks. For instance, WizardMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib103" title="">2023a</a>)</cite> leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib175" title="">2023</a>)</cite> bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning. GAIR-Abel&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chern et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib28" title="">2023</a>)</cite> found that the format of the augmented answers is crucial to final performance, with answers that begin with a paraphrasing of the question followed by a step-by-step solution showing better performance than those in vanilla format. Xwin-Math&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib85" title="">2024</a>)</cite> further scaled up synthetic SFT data to one million examples and found that the LLaMA-2 7B model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib153" title="">2023</a>)</cite> can still benefit from data scaling. MMIQC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Yao, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib95" title="">2024</a>)</cite> composed a bundle of datasets that infuse SFT style data (via question-answer rephrasing or directly taken from MetaMath) with a subset of high-quality mathematical pre-training data, such as OpenWebMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Paster et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib120" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p2.1">Scaling up the generation of synthetic math data is a straightforward process, but ensuring the correctness of the generated math remains a significant challenge for practitioners. AlphaGeometry&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Trinh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib154" title="">2024</a>)</cite> is a recent attempt to address this issue by training a neural model using 100 million synthetic data points. The model proposes solutions and guides a symbolic deduction engine in verifying the correctness of each branch when solving complex geometry problems. By combining the power of synthetic data with a rigorous verification process, AlphaGeometry achieves a problem-solving ability comparable to that of a human Olympiad gold medalist, demonstrating the potential of this approach in tackling complex mathematical reasoning tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Code.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">Different from Math, synthetic data for code reasoning can naturally combine the execution results with structured code, as one requirement of correct code is being executable. In coding-enhanced models, CodeRL&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Le et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib78" title="">2022</a>)</cite> presents an actor-critic approach to improve pretrained language models with feedback signals on synthetic code samples. <cite class="ltx_cite ltx_citemacro_cite">Haluptzok et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib58" title="">2022</a>)</cite> propose a self-improvement strategy where the models generate their own synthetic puzzle-solution pairs. These pairs are then verified and filtered by a real interpreter before being used to finetune language models. <cite class="ltx_cite ltx_citemacro_cite">Shypula et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib142" title="">2023</a>)</cite> further propose a framework that leverages a simulated environment and adaptation strategies like self-improvement synthetic data generation and CoT prompting for code optimization. <cite class="ltx_cite ltx_citemacro_citet">Yang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib173" title="">2024</a>)</cite> developed InterCode, a framework designed to enhance interactive code generation within a reinforcement learning environment, where code serves as actions and execution feedback serves as observations. Reflexion&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shinn et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib141" title="">2024</a>)</cite> employs external or internally simulated linguistic feedback signals to improve the code reasoning capabilities of language models. Regarding synthetic SFT data, Code Alpaca comprises a dataset of 20K code instructions automatically generated by applying SELF-INSTRUCT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib159" title="">2022a</a>)</cite> to ChatGPT across 21 seed tasks. WizardCoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib104" title="">2023b</a>)</cite> introduces Code Evol-Instruct to guide ChatGPT with heuristic prompts to enhance the complexity and diversity of synthetic data. Meanwhile, Magicoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib167" title="">2023c</a>)</cite> developed OSS-INSTRUCT, which generates 75K diverse synthetic instruction samples from open-source code snippets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Other reasoning tasks.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">Synthetic data also leads to impressive performance in other reasoning tasks. For instance, <cite class="ltx_cite ltx_citemacro_citet">Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib164" title="">2023a</a>)</cite> augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples.
Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks. STaR&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib179" title="">2022</a>)</cite> generates synthetic chain-of-thought rationales and filters out those leading to wrong answers for finetuning language models to improve their reasoning. In the domain of physics reasoning, Mind’s Eye&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib98" title="">2022</a>)</cite> takes a novel approach by training a text-to-code model with synthetic “text-description <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px3.p1.1.m1.1a"><mo id="S2.SS1.SSS0.Px3.p1.1.m1.1.1" stretchy="false" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px3.p1.1.m1.1d">→</annotation></semantics></math> rendering code” data. This enables the model to convert textual questions into rendering code, which is then executed in a physical engine (i.e., DeepMind MuJoCo&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Todorov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib152" title="">2012</a>)</cite>). The rendering results are injected into the context, allowing even small language models armed with Mind’s Eye to achieve performance comparable to models 100 times larger.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tool-using and Planning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Learning tool-using through synthetic trajectories.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Synthetic data is also a powerful approach to enable LMs to learn tool-using abilities through simulated trajectories, as collecting real-world human tool-using data might be time-consuming, and the actual distribution of calls to tools might be skewed. LaMDA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Thoppilan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib150" title="">2022</a>)</cite>, for instance, was trained not only on web documents but also on interaction data between crowdworkers and the model itself, with the synthetic data annotated with calls to appropriate tools. This training process allowed LaMDA to develop the ability to use a calculator for arithmetic, a search engine for real-time information seeking, and a machine translator for translation. Similarly, Toolformer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Schick et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib135" title="">2024</a>)</cite> learns to decide which APIs to call and what arguments to pass by training on template-generated data, while Galactica&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Taylor et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib149" title="">2022</a>)</cite> infuse API-calling data into pre-training mixture. ToolAlpaca&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib147" title="">2023</a>)</cite> is a novel framework designed to automatically generate a diverse tool-use corpus, by building a multi-agent simulation environment and letting agents select and use tools iteratively. These examples demonstrate the potential of synthetic trajectories in enabling LMs to acquire tool-using abilities and enhance their reasoning capabilities across various domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Learning to plan in synthetic environments.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">An important feature of the agent in Autonomous Machine Intelligence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib79" title="">2022</a>)</cite> is planning—an ability of decomposing complex tasks into subtasks and finishing the subtasks in a reward-optimal way&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kambhampati et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib73" title="">2024</a>)</cite>. Synthetic data can be a valuable tool here as it can serve as the feedback signal collected from a simulator&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Park et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib119" title="">2023</a>)</cite>, and learning on it can make the agent aware of affordances&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ahn et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib3" title="">2022</a>; Liang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib88" title="">2022</a>)</cite>. For example, Inner Monologue&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib66" title="">2022</a>)</cite> leverages natural language form feedback generated by the simulated environment to teach LLM-based robots planning. They find that such feedback significantly improves high-level instruction completion on both simulated and real-world domains. To compose a large number of realistic planning tasks (e.g., <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.1">“Rearrange objects on a table to match a given scene.”</span>), VIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib71" title="">2022</a>)</cite> creates a multi-modality simulated environment called VIMA-Bench, which supports extensible collections of objects and textures. In the Minecraft game, Voyager&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib158" title="">2023</a>)</cite> deploys a number of GPT-4 based agents to interact with the synthetic environment and finds that the agents can unlock new skills faster and complete planning more efficiently with the help of synthetic feedback.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multimodality</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Reverse rendering from vision to text.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.1">Vision-language alignment data focuses on accurately grounding visual input to an LLM (usually via a vision encoder). Web-scraped image-caption pairs have been the most popular MM alignment data in the past few years since CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib126" title="">2021</a>)</cite> and ALIGN <cite class="ltx_cite ltx_citemacro_citep">(Jia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib69" title="">2021</a>)</cite>. However, web-scraped image-text pairs are usually noisy and only have coarse-grained correspondence, insufficient for grounding details of images in language. In domains such as documents, screens, figures, and diagrams, such fine-grained alignment can most conveniently be obtained from data synthesis pipelines built with image rendering engines. Pix2Struct <cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib80" title="">2023</a>)</cite> uses web servers to render HTML code into website screenshots, and the training task is to derender a masked screenshot to the full HTML code. MatCha <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib94" title="">2023b</a>)</cite> and DePlot <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib93" title="">2023a</a>)</cite> render tabular data into charts with Python plotting libraries and pretrain a foundation model by giving the rendered image and producing the code and/or the tabular data. <cite class="ltx_cite ltx_citemacro_citet">Si et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib143" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Laurençon et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib77" title="">2024</a>)</cite> train on synthetically generated HTML and CSS files for the task of converting webpage screenshots into code implementation. The models finetuned on the synthetic data can generalize reasonably well on realistic data scraped from the Internet. <cite class="ltx_cite ltx_citemacro_citet">Borkman et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib17" title="">2021</a>)</cite> propose to use physics engines or game engines (e.g., Unity) as the synthetic data generator to help computer vision research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multi-modality instruction following.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.1">Downstream applications of multimodal LLMs require reasoning and instruction following capabilities. Such data are usually long-form question response pairs and are expensive for humans to create. LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib96" title="">2024b</a>)</cite> uses existing image captions to prompt GPT-4 (in text-only mode) for writing diverse and long-form prompt-answer pairs. During multimodal LLM training, images and prompts are used as input while the captions and bounding box information can be hidden. Besides image captions, other sources of image attribute information such as object bounding box <cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib184" title="">2023</a>)</cite>, OCR <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib183" title="">2023d</a>)</cite> and derendered charts <cite class="ltx_cite ltx_citemacro_citep">(Masry et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib106" title="">2023</a>; Carbune et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib21" title="">2024</a>)</cite> can all fit into such as image attributes + text LLM rewriting synthetic data pipeline.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Multilingual</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Back-translation augmentation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">Many multilingual language models use back-translation as a data augmentation method, creating synthetic parallel training data from monolingual data sources&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sennrich et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib136" title="">2016</a>; Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib188" title="">2020</a>; Caswell et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib24" title="">2019</a>; Marie et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib105" title="">2020</a>; Bi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib14" title="">2021</a>; Liao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib89" title="">2021</a>; Pham et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib124" title="">2021</a>; Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib171" title="">2022</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">Sennrich et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib136" title="">2016</a>)</cite> back-translate monolingual target data into source language data, providing additional parallel training samples for substantial translation task improvements. Researchers have also explored different sampling methods for back-translation (e.g., beam search, constrained sampling, unconstrained sampling) and their comparative effectiveness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sennrich et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib136" title="">2016</a>; Edunov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib36" title="">2018</a>; Graça et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib53" title="">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Xu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib171" title="">2022</a>)</cite> emphasize the importance of the weight and quality of synthetic data for optimal NMT performance using back-translation. They propose a method to optimize the ratio between search methods and a gamma score to balance estimated importance weight and quality. However, some limitations exist with back-translation-based synthetic data generation. For example, the quality and diversity of synthetic data depends on the performance of the back-translation method. If the synthetic data is too noisy or not diverse, the performance gain would be limited&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Epaliyana et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib38" title="">2021</a>; Chauhan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib25" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Generating multilingual questions and answers at scale.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">Recent studies explore the generation and utilization of synthetic multilingual question-answer (QA) pairs to improve language models’ performance in multilingual and cross-lingual question answering&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib7" title="">2021</a>; Kumar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib74" title="">2019</a>; Chi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib29" title="">2020</a>; Riabi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib130" title="">2021</a>; Li and Callison-Burch, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib84" title="">2023</a>; Abulkhanov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib2" title="">2023</a>)</cite>. One approach is to translate existing monolingual questions and/or answers into other languages&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib7" title="">2021</a>)</cite>. Another involves using Question Generation (QG) models to produce synthetic questions in a cross-lingual fashion based on answers and/or source texts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kumar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib74" title="">2019</a>; Chi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib29" title="">2020</a>; Riabi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib130" title="">2021</a>)</cite>. Recent efforts also focus on jointly generating questions and answers in multiple languages for greater flexibility&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shakeri et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib137" title="">2021</a>; Li and Callison-Burch, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib84" title="">2023</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">Shakeri et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib137" title="">2021</a>)</cite> finetune a pretrained multilingual T5 model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xue et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib172" title="">2020</a>)</cite> on a mixture of a QA generation task and a multilingual masked language modeling task to produce synthetic QA pairs in multiple languages. These efforts generally show that language models trained on synthetic QA pairs demonstrate improved performance on multilingual QA and information retrieval benchmarks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Alignment</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Instruction Following.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS5.SSS0.Px1.p1.1">Synthetic data can serve as a promising approach for training instruction-following models, particularly in scenarios where real-world data is scarce, expensive, or challenging to obtain. Self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib159" title="">2022a</a>)</cite> and Stanford Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib148" title="">2023</a>)</cite> are both using LLMs to generate instruction following data which covers a wide range of scenarios. They first pick a small set of “seed instruction following samples” and then ask the LLMs to imitate the format to generate more demonstrations. One concern of this type of method is how to keep the generated data high quality, which involves the complexity of queries&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib100" title="">2023d</a>)</cite>, the diversity of semantics&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ding et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib35" title="">2023</a>)</cite>, and the scale of the synthetic dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yuan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib178" title="">2023</a>)</cite>. To this end, <cite class="ltx_cite ltx_citemacro_citet">Xu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib170" title="">2023</a>)</cite> propose Evol-Instruct which adds complexity to simple instructions via prompting. <cite class="ltx_cite ltx_citemacro_citet">Mukherjee et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib112" title="">2023</a>)</cite> leverage LLMs to revise the instructions and responses iteratively to include high-quality explanation traces in the FLAN dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib163" title="">2022</a>)</cite>, and they find the trained model has improved performance in many NLP tasks. UltraChat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ding et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib35" title="">2023</a>)</cite> is large-scale and multi-round synthetic dialogue dataset, which is generated by two separate ChatGPT Turbo API models—one serves as the user role while the other serves as the assistant. They instruct the user model with carefully designed prompts to mimic real human user behaviors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS5.SSS0.Px1.p2.1">Many language models are supervised finetuned to learn how to follow instructions, but in learning this behavior, they may inadvertently also learn to be <span class="ltx_text ltx_font_italic" id="S2.SS5.SSS0.Px1.p2.1.1">sycophantic</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Perez et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib123" title="">2023</a>)</cite>, tailoring their responses to follow a user’s viewpoint, even if that viewpoint is not objectively correct <cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib165" title="">2023b</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Sharma et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib139" title="">2024</a>)</cite> find evidence that the preference models (i.e., the reward model used for RLHF training) and even humans prefer sycophantic responses sometimes. On this front, <cite class="ltx_cite ltx_citemacro_citet">Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib165" title="">2023b</a>)</cite> generates synthetic data to encourage models to be robust to user opinions and adds these data in a finetuning step to reduce sycophantic behavior on held-out prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Mitigating hallucination.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS5.SSS0.Px2.p1.1">Many widely-used language models utilize supervised finetuning (SFT) to learn to align their interactions with users <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib160" title="">2022b</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib181" title="">2023b</a>)</cite>.
In particular, there exist many methods of generating synthetic SFT data that can improve capabilities such as reasoning and alignment <cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib164" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib165" title="">b</a>)</cite>.
It has been shown, however, that these synthetic data can induce hallucinations into language models by containing nontrivial amounts of hallucinated answers or by forcing models to learn to answer questions that they do not know the answer to <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>.
These cases demonstrate that synthetic data, if not applied correctly, can actually increase hallucinations in language models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS5.SSS0.Px2.p2.1">On the other hand, recent work has also shown promising results in mitigating hallucinations using synthetic data.
For example, GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib115" title="">2023</a>)</cite> was trained using a reward model that leveraged synthetic hallucination data in order to perform reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>.
This method resulted in a significant improvement in performance on the TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib91" title="">2022</a>)</cite> dataset <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>.
Similarly, <cite class="ltx_cite ltx_citemacro_citet">Jones et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib72" title="">2023</a>)</cite> designed a synthetic task where hallucinations can be readily evaluated, utilizing this task to optimize LLM outputs by learning a continuous postfix via prefix-tuning.
<cite class="ltx_cite ltx_citemacro_citet">Tian et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib151" title="">2023</a>)</cite> uses automated fact-checking and confidence scores to rank factuality scores of model response pairs, which are then used to finetune language models with DPO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rafailov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib128" title="">2023</a>)</cite> to improve their factuality.
Continued research in using synthetic data to mitigate hallucinations is still limited, however, by the lack of synthetic tasks for which hallucinations can be scalably evaluated.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Aligning with shared human preference and values.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS5.SSS0.Px3.p1.1">Directly finetuning on value-aligned or human-preferred data is a straightforward method for aligning language models, but this method often requires substantial human annotation, which can be prohibitively expensive at scale. Additionally, such annotation frequently exhibits varying styles and inconsistent quality, particularly in the case of poorly annotated samples at the lower end of the quality spectrum&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib109" title="">2023</a>; Gilardi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib51" title="">2023b</a>)</cite>. To address these practical challenges, an advanced technique known as “reinforcement learning from human feedback (RLHF)” has been proposed&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Leike et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib81" title="">2018</a>; Christiano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib30" title="">2017</a>; Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib117" title="">2022</a>)</cite>. This approach involves training a reward model with human data to act as a proxy of human judgment, which guides the optimization of the LM generation policy.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS5.SSS0.Px3.p2.1">Recent studies have proposed a mixture of synthetic data and real human data to train more robust reward models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib46" title="">2023</a>)</cite>. Constitutional AI&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib12" title="">2022</a>)</cite> proposes to use a small set of principles to steer the AI generated critiques and feedback, and use such synthetic data to replace the real human data in the typical RLHF pipeline. The model trained with this RLAIF (i.e., reinforcement learning from AI feedback) method shows similar strong performance as RLHF baselines. In general, synthetic data offers a powerful solution for human values and preferences alignment by allowing researchers to generate large-scale, diverse, and controlled training datasets in a low-cost way&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib31" title="">2023</a>; Ganguli et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib44" title="">2022</a>)</cite>. By simulating a wide range of scenarios involving ethical dilemmas&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Perez et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib122" title="">2022</a>)</cite>, social interactions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib99" title="">2023c</a>)</cite>, and cultural norms&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ziems et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib190" title="">2023</a>)</cite>, synthetic data enables comprehensive and systematic testing of AI models’ alignment with human values&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Askell et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib8" title="">2021</a>)</cite>. This approach helps identify and mitigate issues related to bias&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib97" title="">2021</a>; Ntoutsi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib114" title="">2020</a>)</cite>, fairness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib185" title="">2018</a>; Landers and Behrend, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib76" title="">2023</a>)</cite>, and unintended consequences before AI systems are deployed in real-world settings&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ye et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib174" title="">2024</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS5.SSS0.Px3.p3.1">However, it is important to acknowledge that low-fidelity synthetic human preference data might be limited in accurately reflecting nuanced human judgment&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Argyle et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib6" title="">2023</a>)</cite>. Consequently, the resulting models may be less robust under “jail-breaking attacks”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib64" title="">2023a</a>; Deshpande et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib33" title="">2023</a>)</cite>, and may reveal strategically deceptive behavior even through safety training&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib118" title="">2022</a>; Steinhardt, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib145" title="">2022</a>; Everitt et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib39" title="">2021</a>)</cite>. To mitigate these risks, researchers must continuously refine and improve the quality and diversity of synthetic data, incorporating more complex and comprehensive scenarios that better capture the intricacies of human values and preferences. Additionally, combining synthetic data with real-world data, and creating synthetic data in an interactive environment which can be synced with the real world, are promising remedies. As the need for effective AI governance and regulation grows, synthetic data will play an increasingly vital role in enabling scalable oversight mechanisms that promote trust, accountability, and the development of AI technologies that are aligned with human values and societal expectations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data in Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Synthetic data is widely used in evaluations of different perspectives:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Factuality.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">AI systems may generate information or responses that are not grounded in factual knowledge or data, leading to the creation of misleading or false content, formally known as <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.1.1">hallucination</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>)</cite>. Factuality evaluation aims to ensure the consistency of the knowledge in the AI system’s output with the knowledge provided by its training data and knowledge base <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>. Early statistical-based hallucination evaluation methods relied on n-grams to directly calculate the overlap of vocabulary between the input and output content &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dhingra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib34" title="">2019</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib161" title="">2020</a>)</cite>. However, these methods have limitations, as they only consider lexical overlap and do not account for semantics or sentence meaning <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>)</cite>, making them unsuitable for evaluating more complex forms of hallucination. Subsequent assurance methods shifted from statistical approaches to model-based methods, which are more robust compared to token-difference-based methods <cite class="ltx_cite ltx_citemacro_citep">(Honovich et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib62" title="">2021</a>)</cite>. While these model-based evaluation methods are more advanced than their predecessors, they still have limitations. For example, the models can only output the degree of hallucination and may struggle to pinpoint specific errors <cite class="ltx_cite ltx_citemacro_citep">(Falke et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib40" title="">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Feng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib41" title="">2023a</a>)</cite> propose to combine LLMs generation with random walks on knowledge graphs to generate synthetic evaluation data for factuality, which is aware of entities and relations on the graphs. <cite class="ltx_cite ltx_citemacro_citet">Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib166" title="">2024</a>)</cite> created a synthetic dataset called LongFact for long-form factuality evaluation and used Google Search as the grounding source and LLM for the automated judgement, to achieve human-level accuracy but with significally lower cost&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Min et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib110" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Safety.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">Red teaming is a powerful technique for evaluating the safety and robustness of AI models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ganguli et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib44" title="">2022</a>; Casper et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib23" title="">2023b</a>)</cite>. By generating diverse and realistic scenarios designed to elicit unaligned or harmful outputs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Casper et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib22" title="">2023a</a>)</cite>, red teaming can expose vulnerabilities and weaknesses in AI systems&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Perez et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib122" title="">2022</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Perez et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib123" title="">2023</a>)</cite> use LMs to generate datasets for evaluating the behavior of other LMs. They end up producing 154 high-quality datasets which are verified by humans, and discover new cases of inverse scaling where LMs get worse with size. <cite class="ltx_cite ltx_citemacro_citet">Hubinger et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib67" title="">2024</a>)</cite> leverage synthetic data to trigger backdoor attacks to LMs at scale; they find LMs can exhibit deceptive behavior and create a false impression of safety under such attacks, and standard “safety training” could not remove such deception easily. These methods demonstrate the feasibility of using AI assistance to scale up human oversight&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bowman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib18" title="">2022</a>)</cite> over complex problems and unseen domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Assisting human evaluation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">Recent studies have shown that in many cases, synthetic judgements from large-scale LMs (LLMs) can serve as qualified, fast, and low-cost alternatives to actual human evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gilardi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib50" title="">2023a</a>)</cite>. Using GPT-4 as the judge, Alpaca Eval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib87" title="">2023b</a>)</cite> and MT Bench&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib186" title="">2023</a>)</cite> are two popular benchmarks that measure the comprehensive abilities of LM-based ChatBot. In coding tasks, synthetic environment is a common choice to aid human evaluation, as humans can make the assessment more efficiently via actual executions and analysis on running logs. <cite class="ltx_cite ltx_citemacro_cite">Gu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib55" title="">2024</a>)</cite> propose CRUXEval, a code execution reasoning benchmark consisting of 800 Python functions generated by CodeLLaMA-34B. Similarly, <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib92" title="">2024a</a>)</cite> introduce CodeMind, a framework to gauge the code reasoning abilities of LLMs on Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). All these evaluations based on synthetic data show strong correlation with real human judgements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenges and Limitations of Synthetic Data</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">While synthetic data offers numerous benefits and applications, it is crucial to acknowledge and address the potential challenges and limitations associated with its use. This section delves into three significant concerns surrounding synthetic data:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Misuse of synthetic data might proliferate misinformation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">The potential misuse of synthetic data is a significant concern that must be addressed to ensure the responsible development of AI systems. Current AI models become increasingly capable of generating human-like data ranging from text&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemini-Team et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib48" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib47" title="">2023</a>)</cite>, images&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Saharia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib133" title="">2022b</a>; Ramesh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib129" title="">2022</a>)</cite>, songs&nbsp;<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Make songs with Suno AI: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://app.suno.ai/" title="">https://app.suno.ai/</a></span></span></span>, to even videos (e.g., OpenAI SORA&nbsp;<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>OpenAI Sora: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/research/video-generation-models-as-world-simulators" title="">https://openai.com/research/video-generation-models-as-world-simulators</a></span></span></span>). This can be particularly dangerous when synthetic data is used to impersonate real people, manipulate public opinion, or influence political processes. Moreover, the dissemination of synthetic data-driven misinformation can erode trust in legitimate information sources, making it increasingly difficult for people to distinguish between truth and falsehood&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Byman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib20" title="">2023</a>; Rid, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib131" title="">2020</a>)</cite>. To mitigate these risks, it is crucial for researchers, developers, and policymakers to establish clear guidelines and best practices for the ethical generation and use of synthetic data, including robust mechanisms for detecting and countering synthetic misinformation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Groh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib54" title="">2022</a>)</cite>. By proactively addressing these challenges, we can harness the benefits of synthetic data while minimizing its potential for harm.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data might cause ambiguity in AI alignment.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">The increasing use of synthetic data in aligning AI models (e.g., Constitutional AI&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib12" title="">2022</a>)</cite>) can introduce significant ambiguity and uncertainty. The goal of AI alignment is to ensure that AI systems behave in ways that are aligned with human values and intentions. However, synthetic data, which is artificially generated rather than collected from real-world sources, may not accurately represent the nuances and complexities of human values and preferences&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib189" title="">2024</a>)</cite>. This discrepancy can lead to AI models learning from data that is biased&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib42" title="">2023b</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib97" title="">2021</a>)</cite>, ungrounded&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib98" title="">2022</a>; Patel and Pavlick, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib121" title="">2022</a>)</cite>, or misrepresentative of real-world scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Weidinger et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib168" title="">2021</a>; Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>)</cite>. As a result, AI systems trained on synthetic data may exhibit behaviors that are misaligned with human expectations, potentially leading to unintended consequences or even harmful actions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib191" title="">2023</a>; Anderljung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib5" title="">2023</a>)</cite>. Moreover, the ambiguity introduced by synthetic data can make it challenging to interpret and understand the decision-making processes of AI models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lightman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib90" title="">2023</a>)</cite>, further complicating the task of ensuring alignment. To mitigate these risks, it is crucial for researchers to carefully consider the limitations and potential drawbacks of using synthetic data in alignment research and to develop robust methods for validating and testing AI models trained on such data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Training with synthetic data makes evaluation decontamination harder.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.2">The use of synthetic data in model training poses significant challenges to fair evaluation. Evaluation benchmarks are often created by referring to public text sources, such as coursework websites or forums. Consequently, it is arguable that all publicly available benchmark test cases might occasionally be included in the pre-training data of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib61" title="">2022</a>; Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib45" title="">2021</a>)</cite>. The use of synthetic data exacerbates this issue rather than mitigating it. Although the community has proposed several techniques to detect such evaluation contamination, such as <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">min-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.1.1.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1d">italic_k</annotation></semantics></math>% prob</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib140" title="">2023</a>)</cite>, which checks the probabilities of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.2.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.2.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.2.m1.1d">italic_k</annotation></semantics></math> long-tail tokens, these token-level decontamination methods are inadequate when the model is trained with synthetic data. Synthetic data might include rephrased versions of the benchmark data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Oren et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib116" title="">2023</a>; Mattern et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib107" title="">2023</a>)</cite>, rendering token-level decontamination ineffective. In addition to developing more advanced evaluation contamination detection techniques, we recommend that model developers invest in creating and maintaining in-house and protected evaluation benchmarks. These proprietary benchmarks should be carefully safeguarded to prevent leakage and ensure the integrity of the evaluation process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Directions for Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">As the field of synthetic data continues to evolve, there are several promising directions for future research and development. This section outlines three key areas that warrant further exploration:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data scaling.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">The impressive performance of many over-trained small language models (e.g., Mistral series models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib70" title="">2023</a>)</cite>, and Gemma series models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma-Team et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib49" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.1.1">inter alia</span>) demonstrates the necessity of training with large amount of tokens (even passing the compute-optimal chinchilla law&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib127" title="">2021</a>)</cite>). However, whether we have similar conclusions on the training with synthetic data is still an open question, as the quality of synthetic data may not be as consistent as real-world data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib176" title="">2024</a>)</cite>. Future research should investigate the scaling laws for synthetic data and determine the optimal balance between the quantity and quality of synthetic samples. This exploration could help us understand the most effective strategies for leveraging synthetic data in training large-scale language models, potentially leading to more efficient and cost-effective approaches&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib111" title="">2024</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Further improving quality and diversity of synthetic data.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">While existing methods for generating synthetic data have shown promise, there is still room for improvement in terms of creating high-quality, attributed synthetic samples that closely mimic real-world data. Future research should focus on developing new advanced techniques (or based on existing ones such as Generative Adversarial Networks (GANs)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib52" title="">2020</a>)</cite> or Diffusion Models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ho et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib60" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">inter alia</span>) that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. Additionally, researchers should explore methods that can incorporate domain-specific knowledge to ensure the generated data adheres to the underlying constraints and patterns present in the target domain (e.g., via Retrieval Augmented Generation (RAG)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib82" title="">2020</a>; Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib15" title="">2022</a>)</cite>) while maintaining the data quality. By advancing the state-of-the-art in attributed synthetic data generation, we can unlock new opportunities for privacy-preserving analysis&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Assefa et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib9" title="">2020</a>)</cite>, and model training across various fields, from healthcare (e.g., synthetic medical images&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Frid-Adar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib43" title="">2018</a>; Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib162" title="">2019</a>)</cite>) and finance (e.g., simulated trading trajectories&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib187" title="">2022</a>)</cite>) to social sciences&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Argyle et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib6" title="">2023</a>; Park et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib119" title="">2023</a>)</cite> and beyond.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Towards high-fidelity and more efficient scalable oversight.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">As AI models become increasingly complex and autonomous, it becomes challenging to monitor and assess their behavior using traditional oversight methods that rely on human supervision or real-world data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Amodei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib4" title="">2016</a>)</cite>. Future research should explore the use of synthetic data for high-fidelity scalable oversight of these advanced systems. Existing methods typically simulate a certain scenario in social iterations, such as debate&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Leike et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib81" title="">2018</a>)</cite>, reflection&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib180" title="">2023a</a>)</cite>, or revisions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib99" title="">2023c</a>)</cite> to obtain synthetic data, while new approaches could cover more comprehensive scenarios and more modalities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib146" title="">2023</a>)</cite>, as recent studies have found many issues of simulation that only covers a narrowed down&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib27" title="">2023</a>)</cite> or over-simplified&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib189" title="">2024</a>)</cite> scenes. Looking forward, another growing direction could be how to achieve scalable oversight more efficiently—given that we have the full control over the synthetic data generation, we can probably provide more targeted oversights with less synthetic data. As the need for effective AI governance and regulation grows, synthetic data will play an increasingly vital role in enabling more trustworthy scalable oversight mechanisms that promote robust, accountable, and safe deployment of AI technologies for the benefit of society&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Askell et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib8" title="">2021</a>; Bowman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib18" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">The emergent self-improvement capability.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">We typically choose the most capable model to generate synthetic data, as its generation is of higher quality. However, an intriguing question arises: can a model generate synthetic data that is better than the data it was trained on, thus enabling it to improve itself? This concept of self-improvement through synthetic data generation is an exciting avenue for future research. If a model can generate higher-quality data than its original training set, it could potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib26" title="">2024</a>)</cite>. This self-improvement capability could lead to the emergence of more advanced AI systems that can autonomously refine their skills and knowledge over time&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Burns et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib19" title="">2023</a>; Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib65" title="">2023b</a>)</cite>. Although recent work shows encouraging progress in this direction&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib26" title="">2024</a>; Yuan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib177" title="">2024</a>)</cite>, the upper bound of self-improvement and the underlying reasons for its effectiveness remain open questions. Future research should investigate the theoretical underpinnings and practical feasibility of self-improvement through synthetic data generation in more diverse scenarios, examining the necessary conditions, potential limitations, and associated risks. By unlocking the potential of emergent self-improvement capabilities, we could enable more adaptable, efficient, and autonomous learning processes&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib79" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Synthetic data has emerged as a promising solution to address the challenges of data scarcity, privacy concerns, and high costs in AI development. By generating realistic and diverse datasets, synthetic data enables the training and evaluation of AI models at scale across various domains. As we approach human-level or even superhuman-level intelligence, obtaining synthetic data becomes even more crucial, given that models need better-than-average-human quality data to progress. However, ensuring the factuality, fidelity, and lack of bias in synthetic data remains a critical challenge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Future research directions on synthetic data could focus on improving the fidelity and controllability of generative models and developing standardized evaluation and contamination protocols and tools. We could also explore the integration of synthetic data with other techniques and its application in other domains. Despite the challenges, the potential benefits of synthetic data in advancing AI research are significant. By leveraging synthetic data responsibly and effectively, we can build more powerful, inclusive, and trustworthy AI systems that benefit society as a whole.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abay et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;C. Abay, Y.&nbsp;Zhou, M.&nbsp;Kantarcioglu, B.&nbsp;Thuraisingham, and L.&nbsp;Sweeney.

</span>
<span class="ltx_bibblock">Privacy preserving synthetic data release using deep learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Machine Learning and Knowledge Discovery in Databases:
European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018,
Proceedings, Part I 18</em>, pages 510–526. Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abulkhanov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Abulkhanov, N.&nbsp;Sorokin, S.&nbsp;Nikolenko, and V.&nbsp;Malykh.

</span>
<span class="ltx_bibblock">Lapca: Language-agnostic pretraining with cross-lingual alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval</em>, pages 2098–2102,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Ahn, A.&nbsp;Brohan, N.&nbsp;Brown, Y.&nbsp;Chebotar, O.&nbsp;Cortes, B.&nbsp;David, C.&nbsp;Finn,
K.&nbsp;Gopalakrishnan, K.&nbsp;Hausman, A.&nbsp;Herzog, et&nbsp;al.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ArXiv preprint</em>, abs/2204.01691, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.01691" title="">https://arxiv.org/abs/2204.01691</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amodei et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Amodei, C.&nbsp;Olah, J.&nbsp;Steinhardt, P.&nbsp;Christiano, J.&nbsp;Schulman, and D.&nbsp;Mané.

</span>
<span class="ltx_bibblock">Concrete problems in ai safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv preprint</em>, abs/1606.06565, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1606.06565" title="">https://arxiv.org/abs/1606.06565</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderljung et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Anderljung, J.&nbsp;Barnhart, J.&nbsp;Leung, A.&nbsp;Korinek, C.&nbsp;O’Keefe, J.&nbsp;Whittlestone,
S.&nbsp;Avin, M.&nbsp;Brundage, J.&nbsp;Bullock, D.&nbsp;Cass-Beggs, et&nbsp;al.

</span>
<span class="ltx_bibblock">Frontier ai regulation: Managing emerging risks to public safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ArXiv preprint</em>, abs/2307.03718, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.03718" title="">https://arxiv.org/abs/2307.03718</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Argyle et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;P. Argyle, E.&nbsp;C. Busby, N.&nbsp;Fulda, J.&nbsp;R. Gubler, C.&nbsp;Rytting, and D.&nbsp;Wingate.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Political Analysis</em>, 31(3):337–351, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Asai, X.&nbsp;Yu, J.&nbsp;Kasai, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">One question answering model for many languages with cross-lingual
dense passage retrieval.

</span>
<span class="ltx_bibblock">In M.&nbsp;Ranzato, A.&nbsp;Beygelzimer, Y.&nbsp;N. Dauphin, P.&nbsp;Liang, and J.&nbsp;W.
Vaughan, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual</em>, pages 7547–7560, 2021.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html" title="">https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma, et&nbsp;al.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ArXiv preprint</em>, abs/2112.00861, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.00861" title="">https://arxiv.org/abs/2112.00861</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assefa et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;A. Assefa, D.&nbsp;Dervovic, M.&nbsp;Mahfouz, R.&nbsp;E. Tillman, P.&nbsp;Reddy, and M.&nbsp;Veloso.

</span>
<span class="ltx_bibblock">Generating synthetic data in finance: opportunities, challenges and
pitfalls.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the First ACM International Conference on AI
in Finance</em>, pages 1–8, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Azerbayev, H.&nbsp;Schoelkopf, K.&nbsp;Paster, M.&nbsp;D. Santos, S.&nbsp;McAleer, A.&nbsp;Q. Jiang,
J.&nbsp;Deng, S.&nbsp;Biderman, and S.&nbsp;Welleck.

</span>
<span class="ltx_bibblock">Llemma: An open language model for mathematics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">ArXiv preprint</em>, abs/2310.10631, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.10631" title="">https://arxiv.org/abs/2310.10631</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babbar and Schölkopf (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Babbar and B.&nbsp;Schölkopf.

</span>
<span class="ltx_bibblock">Data scarcity, robustness and extreme multi-label classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Machine Learning</em>, 108(8):1329–1351, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, S.&nbsp;Kadavath, S.&nbsp;Kundu, A.&nbsp;Askell, J.&nbsp;Kernion, A.&nbsp;Jones, A.&nbsp;Chen,
A.&nbsp;Goldie, A.&nbsp;Mirhoseini, C.&nbsp;McKinnon, et&nbsp;al.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ArXiv preprint</em>, abs/2212.08073, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.08073" title="">https://arxiv.org/abs/2212.08073</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbierato et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Barbierato, M.&nbsp;L.&nbsp;D. Vedova, D.&nbsp;Tessera, D.&nbsp;Toti, and N.&nbsp;Vanoli.

</span>
<span class="ltx_bibblock">A methodology for controlling bias and fairness in synthetic data
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Applied Sciences</em>, 12(9):4619, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Bi, H.&nbsp;Li, and J.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Data augmentation for text generation without any augmented data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 2223–2237,
Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.acl-long.173" title="">10.18653/v1/2021.acl-long.173</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.173" title="">https://aclanthology.org/2021.acl-long.173</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Borgeaud, A.&nbsp;Mensch, J.&nbsp;Hoffmann, T.&nbsp;Cai, E.&nbsp;Rutherford, K.&nbsp;Millican,
G.&nbsp;van&nbsp;den Driessche, J.&nbsp;Lespiau, B.&nbsp;Damoc, A.&nbsp;Clark, D.&nbsp;de&nbsp;Las&nbsp;Casas,
A.&nbsp;Guy, J.&nbsp;Menick, R.&nbsp;Ring, T.&nbsp;Hennigan, S.&nbsp;Huang, L.&nbsp;Maggiore, C.&nbsp;Jones,
A.&nbsp;Cassirer, A.&nbsp;Brock, M.&nbsp;Paganini, G.&nbsp;Irving, O.&nbsp;Vinyals, S.&nbsp;Osindero,
K.&nbsp;Simonyan, J.&nbsp;W. Rae, E.&nbsp;Elsen, and L.&nbsp;Sifre.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In K.&nbsp;Chaudhuri, S.&nbsp;Jegelka, L.&nbsp;Song, C.&nbsp;Szepesvári, G.&nbsp;Niu,
and S.&nbsp;Sabato, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of
<em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Proceedings of Machine Learning Research</em>, pages 2206–2240. PMLR,
2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/borgeaud22a.html" title="">https://proceedings.mlr.press/v162/borgeaud22a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borisov et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Borisov, K.&nbsp;Seßler, T.&nbsp;Leemann, M.&nbsp;Pawelczyk, and G.&nbsp;Kasneci.

</span>
<span class="ltx_bibblock">Language models are realistic tabular data generators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ArXiv preprint</em>, abs/2210.06280, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.06280" title="">https://arxiv.org/abs/2210.06280</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borkman et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Borkman, A.&nbsp;Crespi, S.&nbsp;Dhakad, S.&nbsp;Ganguly, J.&nbsp;Hogins, Y.&nbsp;C. Jhang,
M.&nbsp;Kamalzadeh, B.&nbsp;Li, S.&nbsp;Leal, P.&nbsp;Parisi, C.&nbsp;Romero, W.&nbsp;Smith, A.&nbsp;Thaman,
S.&nbsp;Warren, and N.&nbsp;Yadav.

</span>
<span class="ltx_bibblock">Unity perception: Generate synthetic data for computer vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ArXiv preprint</em>, abs/2107.04259, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.04259" title="">https://arxiv.org/abs/2107.04259</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;R. Bowman, J.&nbsp;Hyun, E.&nbsp;Perez, E.&nbsp;Chen, C.&nbsp;Pettit, S.&nbsp;Heiner,
K.&nbsp;Lukošiūtė, A.&nbsp;Askell, A.&nbsp;Jones, A.&nbsp;Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">Measuring progress on scalable oversight for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ArXiv preprint</em>, abs/2211.03540, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.03540" title="">https://arxiv.org/abs/2211.03540</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burns et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Burns, P.&nbsp;Izmailov, J.&nbsp;H. Kirchner, B.&nbsp;Baker, L.&nbsp;Gao, L.&nbsp;Aschenbrenner,
Y.&nbsp;Chen, A.&nbsp;Ecoffet, M.&nbsp;Joglekar, J.&nbsp;Leike, et&nbsp;al.

</span>
<span class="ltx_bibblock">Weak-to-strong generalization: Eliciting strong capabilities with
weak supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ArXiv preprint</em>, abs/2312.09390, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.09390" title="">https://arxiv.org/abs/2312.09390</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byman et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;L. Byman, C.&nbsp;Gao, C.&nbsp;Meserole, and V.&nbsp;Subrahmanian.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Deepfakes and international conflict</em>.

</span>
<span class="ltx_bibblock">Brookings Institution, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carbune et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Carbune, H.&nbsp;Mansoor, F.&nbsp;Liu, R.&nbsp;Aralikatte, G.&nbsp;Baechler, J.&nbsp;Chen, and
A.&nbsp;Sharma.

</span>
<span class="ltx_bibblock">Chart-based reasoning: Transferring capabilities from llms to vlms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ArXiv preprint</em>, abs/2403.12596, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.12596" title="">https://arxiv.org/abs/2403.12596</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Casper, T.&nbsp;Bu, Y.&nbsp;Li, J.&nbsp;Li, K.&nbsp;Zhang, K.&nbsp;Hariharan, and D.&nbsp;Hadfield-Menell.

</span>
<span class="ltx_bibblock">Red teaming deep neural networks with feature synthesis tools.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Casper, J.&nbsp;Lin, J.&nbsp;Kwon, G.&nbsp;Culp, and D.&nbsp;Hadfield-Menell.

</span>
<span class="ltx_bibblock">Explore, establish, exploit: Red teaming language models from
scratch.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">ArXiv preprint</em>, abs/2306.09442, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.09442" title="">https://arxiv.org/abs/2306.09442</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caswell et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Caswell, C.&nbsp;Chelba, and D.&nbsp;Grangier.

</span>
<span class="ltx_bibblock">Tagged back-translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 53–63, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/W19-5206" title="">10.18653/v1/W19-5206</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-5206" title="">https://aclanthology.org/W19-5206</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chauhan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Chauhan, S.&nbsp;Saxena, and P.&nbsp;Daniel.

</span>
<span class="ltx_bibblock">Improved unsupervised neural machine translation with semantically
weighted back translation for morphologically rich and low resource
languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Neural Processing Letters</em>, 54(3):1707–1726, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Chen, Y.&nbsp;Deng, H.&nbsp;Yuan, K.&nbsp;Ji, and Q.&nbsp;Gu.

</span>
<span class="ltx_bibblock">Self-play fine-tuning converts weak language models to strong
language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Cheng, T.&nbsp;Piccardi, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">CoMPosT: Characterizing and evaluating caricature in LLM
simulations.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 10853–10875, Singapore, Dec. 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.669" title="">10.18653/v1/2023.emnlp-main.669</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.669" title="">https://aclanthology.org/2023.emnlp-main.669</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Chern, H.&nbsp;Zou, X.&nbsp;Li, J.&nbsp;Hu, K.&nbsp;Feng, J.&nbsp;Li, and P.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Generative ai for math: Abel.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GAIR-NLP/abel" title="">https://github.com/GAIR-NLP/abel</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Chi, L.&nbsp;Dong, F.&nbsp;Wei, W.&nbsp;Wang, X.&nbsp;Mao, and H.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Cross-lingual natural language generation via pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of
Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium
on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020</em>, pages 7570–7577. AAAI Press, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aaai.org/ojs/index.php/AAAI/article/view/6256" title="">https://aaai.org/ojs/index.php/AAAI/article/view/6256</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;F. Christiano, J.&nbsp;Leike, T.&nbsp;B. Brown, M.&nbsp;Martic, S.&nbsp;Legg, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages
4299–4307, 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html" title="">https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Cui, L.&nbsp;Yuan, N.&nbsp;Ding, G.&nbsp;Yao, W.&nbsp;Zhu, Y.&nbsp;Ni, G.&nbsp;Xie, Z.&nbsp;Liu, and M.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahmen and Cook (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Dahmen and D.&nbsp;Cook.

</span>
<span class="ltx_bibblock">Synsys: A synthetic data generation system for healthcare
applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Sensors</em>, 19(5):1181, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshpande et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Deshpande, V.&nbsp;Murahari, T.&nbsp;Rajpurohit, A.&nbsp;Kalyan, and K.&nbsp;Narasimhan.

</span>
<span class="ltx_bibblock">Toxicity in chatgpt: Analyzing persona-assigned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ArXiv preprint</em>, abs/2304.05335, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.05335" title="">https://arxiv.org/abs/2304.05335</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Dhingra, M.&nbsp;Faruqui, A.&nbsp;Parikh, M.-W. Chang, D.&nbsp;Das, and W.&nbsp;Cohen.

</span>
<span class="ltx_bibblock">Handling divergent reference texts when evaluating table-to-text
generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4884–4895, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1483" title="">10.18653/v1/P19-1483</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1483" title="">https://aclanthology.org/P19-1483</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Ding, Y.&nbsp;Chen, B.&nbsp;Xu, Y.&nbsp;Qin, Z.&nbsp;Zheng, S.&nbsp;Hu, Z.&nbsp;Liu, M.&nbsp;Sun, and B.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional
conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ArXiv preprint</em>, abs/2305.14233, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.14233" title="">https://arxiv.org/abs/2305.14233</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Edunov, M.&nbsp;Ott, M.&nbsp;Auli, and D.&nbsp;Grangier.

</span>
<span class="ltx_bibblock">Understanding back-translation at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489–500, Brussels, Belgium, 2018.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/D18-1045" title="">10.18653/v1/D18-1045</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1045" title="">https://aclanthology.org/D18-1045</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El&nbsp;Emam et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;El&nbsp;Emam, L.&nbsp;Mosquera, and R.&nbsp;Hoptroff.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Practical synthetic data generation: balancing privacy and the
broad availability of data</em>.

</span>
<span class="ltx_bibblock">O’Reilly Media, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epaliyana et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Epaliyana, S.&nbsp;Ranathunga, and S.&nbsp;Jayasena.

</span>
<span class="ltx_bibblock">Improving back-translation with iterative filtering and data
selection for sinhala-english nmt.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2021 Moratuwa Engineering Research Conference (MERCon)</em>,
pages 438–443. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everitt et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Everitt, M.&nbsp;Hutter, R.&nbsp;Kumar, and V.&nbsp;Krakovna.

</span>
<span class="ltx_bibblock">Reward tampering problems and solutions in reinforcement learning: A
causal influence diagram perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Synthese</em>, 198(Suppl 27):6435–6467, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falke et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Falke, L.&nbsp;F.&nbsp;R. Ribeiro, P.&nbsp;A. Utama, I.&nbsp;Dagan, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Ranking generated summaries by correctness: An interesting but
challenging application for natural language inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2214–2220, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1213" title="">10.18653/v1/P19-1213</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1213" title="">https://aclanthology.org/P19-1213</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Feng, V.&nbsp;Balachandran, Y.&nbsp;Bai, and Y.&nbsp;Tsvetkov.

</span>
<span class="ltx_bibblock">FactKB: Generalizable factuality evaluation using language models
enhanced with factual knowledge.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 933–952, Singapore, Dec. 2023a. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.59" title="">10.18653/v1/2023.emnlp-main.59</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.59" title="">https://aclanthology.org/2023.emnlp-main.59</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Feng, C.&nbsp;Y. Park, Y.&nbsp;Liu, and Y.&nbsp;Tsvetkov.

</span>
<span class="ltx_bibblock">From pretraining data to language models to downstream tasks:
Tracking the trails of political biases leading to unfair nlp models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">ArXiv preprint</em>, abs/2305.08283, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.08283" title="">https://arxiv.org/abs/2305.08283</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frid-Adar et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Frid-Adar, E.&nbsp;Klang, M.&nbsp;Amitai, J.&nbsp;Goldberger, and H.&nbsp;Greenspan.

</span>
<span class="ltx_bibblock">Synthetic data augmentation using gan for improved liver lesion
classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">2018 IEEE 15th international symposium on biomedical imaging
(ISBI 2018)</em>, pages 289–293. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Ganguli, L.&nbsp;Lovitt, J.&nbsp;Kernion, A.&nbsp;Askell, Y.&nbsp;Bai, S.&nbsp;Kadavath, B.&nbsp;Mann,
E.&nbsp;Perez, N.&nbsp;Schiefer, K.&nbsp;Ndousse, et&nbsp;al.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling
behaviors, and lessons learned.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ArXiv preprint</em>, abs/2209.07858, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.07858" title="">https://arxiv.org/abs/2209.07858</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Gao, S.&nbsp;Biderman, S.&nbsp;Black, L.&nbsp;Golding, T.&nbsp;Hoppe, C.&nbsp;Foster, J.&nbsp;Phang,
H.&nbsp;He, A.&nbsp;Thite, N.&nbsp;Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv preprint</em>, abs/2101.00027, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2101.00027" title="">https://arxiv.org/abs/2101.00027</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Gao, J.&nbsp;Schulman, and J.&nbsp;Hilton.

</span>
<span class="ltx_bibblock">Scaling laws for reward model overoptimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">International Conference on Machine Learning</em>, pages
10835–10866. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini-Team et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini-Team, R.&nbsp;Anil, S.&nbsp;Borgeaud, Y.&nbsp;Wu, J.-B. Alayrac, J.&nbsp;Yu, R.&nbsp;Soricut,
J.&nbsp;Schalkwyk, A.&nbsp;M. Dai, A.&nbsp;Hauth, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">ArXiv preprint</em>, abs/2312.11805, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.11805" title="">https://arxiv.org/abs/2312.11805</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini-Team et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini-Team, M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap,
J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ArXiv preprint</em>, abs/2403.05530, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05530" title="">https://arxiv.org/abs/2403.05530</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma-Team et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemma-Team, T.&nbsp;Mesnard, C.&nbsp;Hardin, R.&nbsp;Dadashi, S.&nbsp;Bhupatiraju, S.&nbsp;Pathak,
L.&nbsp;Sifre, M.&nbsp;Rivière, M.&nbsp;S. Kale, J.&nbsp;Love, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ArXiv preprint</em>, abs/2403.08295, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.08295" title="">https://arxiv.org/abs/2403.08295</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the National Academy of Sciences</em>, 120(30):e2305016120, 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1073/pnas.2305016120" title="">10.1073/pnas.2305016120</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.2305016120" title="">https://www.pnas.org/doi/abs/10.1073/pnas.2305016120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the National Academy of Sciences</em>, 120(30):e2305016120, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Goodfellow, J.&nbsp;Pouget-Abadie, M.&nbsp;Mirza, B.&nbsp;Xu, D.&nbsp;Warde-Farley, S.&nbsp;Ozair,
A.&nbsp;Courville, and Y.&nbsp;Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Communications of the ACM</em>, 63(11):139–144, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graça et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Graça, Y.&nbsp;Kim, J.&nbsp;Schamper, S.&nbsp;Khadivi, and H.&nbsp;Ney.

</span>
<span class="ltx_bibblock">Generalizing back-translation in neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 45–52, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/W19-5205" title="">10.18653/v1/W19-5205</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-5205" title="">https://aclanthology.org/W19-5205</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Groh, Z.&nbsp;Epstein, C.&nbsp;Firestone, and R.&nbsp;Picard.

</span>
<span class="ltx_bibblock">Deepfake detection by human crowds, machines, and machine-informed
crowds.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the National Academy of Sciences</em>, 119(1):e2110013119, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Gu, B.&nbsp;Rozière, H.&nbsp;Leather, A.&nbsp;Solar-Lezama, G.&nbsp;Synnaeve, and S.&nbsp;I.
Wang.

</span>
<span class="ltx_bibblock">Cruxeval: A benchmark for code reasoning, understanding and
execution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ArXiv preprint</em>, abs/2401.03065, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.03065" title="">https://arxiv.org/abs/2401.03065</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guarnera et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Guarnera, O.&nbsp;Giudice, and S.&nbsp;Battiato.

</span>
<span class="ltx_bibblock">Deepfake detection by analyzing convolutional traces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition workshops</em>, pages 666–667, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Gupta, D.&nbsp;Bhatt, and A.&nbsp;Pandey.

</span>
<span class="ltx_bibblock">Transitioning from real to synthetic data: Quantifying the bias in
model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ArXiv preprint</em>, abs/2105.04144, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2105.04144" title="">https://arxiv.org/abs/2105.04144</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haluptzok et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Haluptzok, M.&nbsp;Bowers, and A.&nbsp;T. Kalai.

</span>
<span class="ltx_bibblock">Language models can teach themselves to program better.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv preprint</em>, abs/2207.14502, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.14502" title="">https://arxiv.org/abs/2207.14502</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Heusel, H.&nbsp;Ramsauer, T.&nbsp;Unterthiner, B.&nbsp;Nessler, and S.&nbsp;Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages
6626–6637, 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html" title="">https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ho, A.&nbsp;Jain, and P.&nbsp;Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford,
D.&nbsp;de&nbsp;Las&nbsp;Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">An empirical analysis of compute-optimal large language model
training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in Neural Information Processing Systems</em>,
35:30016–30030, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Honovich, L.&nbsp;Choshen, R.&nbsp;Aharoni, E.&nbsp;Neeman, I.&nbsp;Szpektor, and O.&nbsp;Abend.

</span>
<span class="ltx_bibblock"><math alttext="q^{2}" class="ltx_Math" display="inline" id="bib.bib62.1.m1.1"><semantics id="bib.bib62.1.m1.1a"><msup id="bib.bib62.1.m1.1.1" xref="bib.bib62.1.m1.1.1.cmml"><mi id="bib.bib62.1.m1.1.1.2" xref="bib.bib62.1.m1.1.1.2.cmml">q</mi><mn id="bib.bib62.1.m1.1.1.3" xref="bib.bib62.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib62.1.m1.1b"><apply id="bib.bib62.1.m1.1.1.cmml" xref="bib.bib62.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib62.1.m1.1.1.1.cmml" xref="bib.bib62.1.m1.1.1">superscript</csymbol><ci id="bib.bib62.1.m1.1.1.2.cmml" xref="bib.bib62.1.m1.1.1.2">𝑞</ci><cn id="bib.bib62.1.m1.1.1.3.cmml" type="integer" xref="bib.bib62.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib62.1.m1.1c">q^{2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib62.1.m1.1d">italic_q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded
dialogues via question generation and question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.2.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7856–7870, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.emnlp-main.619" title="">10.18653/v1/2021.emnlp-main.619</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.619" title="">https://aclanthology.org/2021.emnlp-main.619</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howe et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Howe, J.&nbsp;Stoyanovich, H.&nbsp;Ping, B.&nbsp;Herman, and M.&nbsp;Gee.

</span>
<span class="ltx_bibblock">Synthetic data for social good.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ArXiv preprint</em>, abs/1710.08874, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1710.08874" title="">https://arxiv.org/abs/1710.08874</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Huang, H.&nbsp;Kwak, and J.&nbsp;An.

</span>
<span class="ltx_bibblock">Is chatgpt better than human annotators? potential and limitations of
chatgpt in explaining implicit hate speech.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">ArXiv preprint</em>, abs/2302.07736, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.07736" title="">https://arxiv.org/abs/2302.07736</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Huang, S.&nbsp;Gu, L.&nbsp;Hou, Y.&nbsp;Wu, X.&nbsp;Wang, H.&nbsp;Yu, and J.&nbsp;Han.

</span>
<span class="ltx_bibblock">Large language models can self-improve.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 1051–1068, Singapore, Dec. 2023b. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.67" title="">10.18653/v1/2023.emnlp-main.67</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.67" title="">https://aclanthology.org/2023.emnlp-main.67</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Huang, F.&nbsp;Xia, T.&nbsp;Xiao, H.&nbsp;Chan, J.&nbsp;Liang, P.&nbsp;Florence, A.&nbsp;Zeng, J.&nbsp;Tompson,
I.&nbsp;Mordatch, Y.&nbsp;Chebotar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">ArXiv preprint</em>, abs/2207.05608, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.05608" title="">https://arxiv.org/abs/2207.05608</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hubinger et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Hubinger, C.&nbsp;Denison, J.&nbsp;Mu, M.&nbsp;Lambert, M.&nbsp;Tong, M.&nbsp;MacDiarmid, T.&nbsp;Lanham,
D.&nbsp;M. Ziegler, T.&nbsp;Maxwell, N.&nbsp;Cheng, et&nbsp;al.

</span>
<span class="ltx_bibblock">Sleeper agents: Training deceptive llms that persist through safety
training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">ArXiv preprint</em>, abs/2401.05566, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.05566" title="">https://arxiv.org/abs/2401.05566</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Ji, N.&nbsp;Lee, R.&nbsp;Frieske, T.&nbsp;Yu, D.&nbsp;Su, Y.&nbsp;Xu, E.&nbsp;Ishii, Y.&nbsp;J. Bang,
A.&nbsp;Madotto, and P.&nbsp;Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">ACM Computing Surveys (CSUR)</em>, 55(12):1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Jia, Y.&nbsp;Yang, Y.&nbsp;Xia, Y.&nbsp;Chen, Z.&nbsp;Parekh, H.&nbsp;Pham, Q.&nbsp;V. Le, Y.&nbsp;Sung, Z.&nbsp;Li,
and T.&nbsp;Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock">In M.&nbsp;Meila and T.&nbsp;Zhang, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib69.2.2">Proceedings of Machine Learning
Research</em>, pages 4904–4916. PMLR, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v139/jia21b.html" title="">http://proceedings.mlr.press/v139/jia21b.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Mensch, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l.
Casas, F.&nbsp;Bressand, G.&nbsp;Lengyel, G.&nbsp;Lample, L.&nbsp;Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ArXiv preprint</em>, abs/2310.06825, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06825" title="">https://arxiv.org/abs/2310.06825</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Jiang, A.&nbsp;Gupta, Z.&nbsp;Zhang, G.&nbsp;Wang, Y.&nbsp;Dou, Y.&nbsp;Chen, L.&nbsp;Fei-Fei,
A.&nbsp;Anandkumar, Y.&nbsp;Zhu, and L.&nbsp;Fan.

</span>
<span class="ltx_bibblock">Vima: General robot manipulation with multimodal prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">NeurIPS 2022 Foundation Models for Decision Making
Workshop</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Jones, H.&nbsp;Palangi, C.&nbsp;Simões, V.&nbsp;Chandrasekaran, S.&nbsp;Mukherjee, A.&nbsp;Mitra,
A.&nbsp;Awadallah, and E.&nbsp;Kamar.

</span>
<span class="ltx_bibblock">Teaching language models to hallucinate less with synthetic tasks,
2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06827" title="">https://arxiv.org/abs/2310.06827</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kambhampati et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Kambhampati, K.&nbsp;Valmeekam, L.&nbsp;Guan, K.&nbsp;Stechly, M.&nbsp;Verma, S.&nbsp;Bhambri,
L.&nbsp;Saldyt, and A.&nbsp;Murthy.

</span>
<span class="ltx_bibblock">Llms can’t plan, but can help planning in llm-modulo frameworks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2402.01817</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Kumar, N.&nbsp;Joshi, A.&nbsp;Mukherjee, G.&nbsp;Ramakrishnan, and P.&nbsp;Jyothi.

</span>
<span class="ltx_bibblock">Cross-lingual training for automatic question generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4863–4872, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1481" title="">10.18653/v1/P19-1481</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1481" title="">https://aclanthology.org/P19-1481</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Lam, A.&nbsp;Sanchez-Gonzalez, M.&nbsp;Willson, P.&nbsp;Wirnsberger, M.&nbsp;Fortunato, F.&nbsp;Alet,
S.&nbsp;Ravuri, T.&nbsp;Ewalds, Z.&nbsp;Eaton-Rosen, W.&nbsp;Hu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Learning skillful medium-range global weather forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Science</em>, 382(6677):1416–1421, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landers and Behrend (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;N. Landers and T.&nbsp;S. Behrend.

</span>
<span class="ltx_bibblock">Auditing the ai auditors: A framework for evaluating fairness and
bias in high stakes ai predictive models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">American Psychologist</em>, 78(1):36, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Laurençon, L.&nbsp;Tronchon, and V.&nbsp;Sanh.

</span>
<span class="ltx_bibblock">Unlocking the conversion of web screenshots into html code with the
websight dataset, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.09029" title="">https://arxiv.org/abs/2403.09029</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Le, Y.&nbsp;Wang, A.&nbsp;D. Gotmare, S.&nbsp;Savarese, and S.&nbsp;C.&nbsp;H. Hoi.

</span>
<span class="ltx_bibblock">Coderl: Mastering code generation through pretrained models and deep
reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Advances in Neural Information Processing Systems</em>,
35:21314–21328, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;LeCun.

</span>
<span class="ltx_bibblock">A path towards autonomous machine intelligence version 0.9. 2,
2022-06-27.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Open Review</em>, 62, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Lee, M.&nbsp;Joshi, I.&nbsp;R. Turc, H.&nbsp;Hu, F.&nbsp;Liu, J.&nbsp;M. Eisenschlos, U.&nbsp;Khandelwal,
P.&nbsp;Shaw, M.-W. Chang, and K.&nbsp;Toutanova.

</span>
<span class="ltx_bibblock">Pix2struct: Screenshot parsing as pretraining for visual language
understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">International Conference on Machine Learning</em>, pages
18893–18912. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leike et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Leike, D.&nbsp;Krueger, T.&nbsp;Everitt, M.&nbsp;Martic, V.&nbsp;Maini, and S.&nbsp;Legg.

</span>
<span class="ltx_bibblock">Scalable agent alignment via reward modeling: a research direction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">ArXiv preprint</em>, abs/1811.07871, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1811.07871" title="">https://arxiv.org/abs/1811.07871</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;S.&nbsp;H. Lewis, E.&nbsp;Perez, A.&nbsp;Piktus, F.&nbsp;Petroni, V.&nbsp;Karpukhin, N.&nbsp;Goyal,
H.&nbsp;Küttler, M.&nbsp;Lewis, W.&nbsp;Yih, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, and
D.&nbsp;Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Lewkowycz, A.&nbsp;Andreassen, D.&nbsp;Dohan, E.&nbsp;Dyer, H.&nbsp;Michalewski, V.&nbsp;Ramasesh,
A.&nbsp;Slone, C.&nbsp;Anil, I.&nbsp;Schlag, T.&nbsp;Gutman-Solo, Y.&nbsp;Wu, B.&nbsp;Neyshabur,
G.&nbsp;Gur-Ari, and V.&nbsp;Misra.

</span>
<span class="ltx_bibblock">Solving quantitative reasoning problems with language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.14858" title="">https://arxiv.org/abs/2206.14858</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Callison-Burch (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Li and C.&nbsp;Callison-Burch.

</span>
<span class="ltx_bibblock">Paxqa: Generating cross-lingual question answering examples at
training scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">ArXiv preprint</em>, abs/2304.12206, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.12206" title="">https://arxiv.org/abs/2304.12206</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Li, W.&nbsp;Wang, J.&nbsp;Hu, Y.&nbsp;Wei, N.&nbsp;Zheng, H.&nbsp;Hu, Z.&nbsp;Zhang, and H.&nbsp;Peng.

</span>
<span class="ltx_bibblock">Common 7b language models already possess strong math capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">ArXiv preprint</em>, abs/2403.04706, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.04706" title="">https://arxiv.org/abs/2403.04706</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Li, R.&nbsp;Carver, I.&nbsp;Lopez-Gomez, F.&nbsp;Sha, and J.&nbsp;Anderson.

</span>
<span class="ltx_bibblock">Seeds: Emulation of weather forecast ensembles with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">ArXiv preprint</em>, abs/2306.14066, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.14066" title="">https://arxiv.org/abs/2306.14066</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Li, T.&nbsp;Zhang, Y.&nbsp;Dubois, R.&nbsp;Taori, I.&nbsp;Gulrajani, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Liang, W.&nbsp;Huang, F.&nbsp;Xia, P.&nbsp;Xu, K.&nbsp;Hausman, B.&nbsp;Ichter, P.&nbsp;Florence, and
A.&nbsp;Zeng.

</span>
<span class="ltx_bibblock">Code as policies: Language model programs for embodied control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">ArXiv preprint</em>, abs/2209.07753, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.07753" title="">https://arxiv.org/abs/2209.07753</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Liao, S.&nbsp;Khadivi, and S.&nbsp;Hewavitharana.

</span>
<span class="ltx_bibblock">Back-translation for large-scale multilingual machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the Sixth Conference on Machine Translation</em>,
pages 418–424, Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.wmt-1.50" title="">https://aclanthology.org/2021.wmt-1.50</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Lightman, V.&nbsp;Kosaraju, Y.&nbsp;Burda, H.&nbsp;Edwards, B.&nbsp;Baker, T.&nbsp;Lee, J.&nbsp;Leike,
J.&nbsp;Schulman, I.&nbsp;Sutskever, and K.&nbsp;Cobbe.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">ArXiv preprint</em>, abs/2305.20050, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.20050" title="">https://arxiv.org/abs/2305.20050</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lin, J.&nbsp;Hilton, and O.&nbsp;Evans.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252,
Dublin, Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.acl-long.229" title="">10.18653/v1/2022.acl-long.229</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.229" title="">https://aclanthology.org/2022.acl-long.229</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Liu, S.&nbsp;D. Zhang, and R.&nbsp;Jabbarvand.

</span>
<span class="ltx_bibblock">Codemind: A framework to challenge large language models for code
reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">ArXiv preprint</em>, abs/2402.09664, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.09664" title="">https://arxiv.org/abs/2402.09664</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Liu, J.&nbsp;Eisenschlos, F.&nbsp;Piccinno, S.&nbsp;Krichene, C.&nbsp;Pang, K.&nbsp;Lee, M.&nbsp;Joshi,
W.&nbsp;Chen, N.&nbsp;Collier, and Y.&nbsp;Altun.

</span>
<span class="ltx_bibblock">Deplot: One-shot visual language reasoning by plot-to-table
translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 10381–10399, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Liu, F.&nbsp;Piccinno, S.&nbsp;Krichene, C.&nbsp;Pang, K.&nbsp;Lee, M.&nbsp;Joshi, Y.&nbsp;Altun,
N.&nbsp;Collier, and J.&nbsp;Eisenschlos.

</span>
<span class="ltx_bibblock">Matcha: Enhancing visual language pretraining with math reasoning and
chart derendering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 12756–12770,
2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Yao (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu and A.&nbsp;C.-C. Yao.

</span>
<span class="ltx_bibblock">Augmenting math word problems via iterative question composing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">ArXiv preprint</em>, abs/2401.09003, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.09003" title="">https://arxiv.org/abs/2401.09003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Advances in neural information processing systems</em>, 36,
2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Liu, C.&nbsp;Jia, J.&nbsp;Wei, G.&nbsp;Xu, L.&nbsp;Wang, and S.&nbsp;Vosoughi.

</span>
<span class="ltx_bibblock">Mitigating political bias in language models through reinforced
calibration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in
Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</em>,
pages 14857–14866. AAAI Press, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/17744" title="">https://ojs.aaai.org/index.php/AAAI/article/view/17744</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Liu, J.&nbsp;Wei, S.&nbsp;S. Gu, T.-Y. Wu, S.&nbsp;Vosoughi, C.&nbsp;Cui, D.&nbsp;Zhou, and A.&nbsp;M.
Dai.

</span>
<span class="ltx_bibblock">Mind’s eye: Grounded language model reasoning through simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">ArXiv preprint</em>, abs/2210.05359, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.05359" title="">https://arxiv.org/abs/2210.05359</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Liu, R.&nbsp;Yang, C.&nbsp;Jia, G.&nbsp;Zhang, D.&nbsp;Zhou, A.&nbsp;M. Dai, D.&nbsp;Yang, and
S.&nbsp;Vosoughi.

</span>
<span class="ltx_bibblock">Training socially aligned language models in simulated human society.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">ArXiv preprint</em>, abs/2305.16960, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.16960" title="">https://arxiv.org/abs/2305.16960</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Liu, W.&nbsp;Zeng, K.&nbsp;He, Y.&nbsp;Jiang, and J.&nbsp;He.

</span>
<span class="ltx_bibblock">What makes good data for alignment? a comprehensive study of
automatic data selection in instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">ArXiv preprint</em>, abs/2312.15685, 2023d.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.15685" title="">https://arxiv.org/abs/2312.15685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Lu, M.&nbsp;Shen, H.&nbsp;Wang, X.&nbsp;Wang, C.&nbsp;van Rechem, and W.&nbsp;Wei.

</span>
<span class="ltx_bibblock">Machine learning for synthetic data generation: a review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">ArXiv preprint</em>, abs/2302.04062, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.04062" title="">https://arxiv.org/abs/2302.04062</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucini (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Lucini.

</span>
<span class="ltx_bibblock">The real deal about synthetic data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">MIT Sloan Management Review</em>, 63(1):1–4,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Luo, Q.&nbsp;Sun, C.&nbsp;Xu, P.&nbsp;Zhao, J.&nbsp;Lou, C.&nbsp;Tao, X.&nbsp;Geng, Q.&nbsp;Lin, S.&nbsp;Chen, and
D.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Wizardmath: Empowering mathematical reasoning for large language
models via reinforced evol-instruct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">ArXiv preprint</em>, abs/2308.09583, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.09583" title="">https://arxiv.org/abs/2308.09583</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Luo, C.&nbsp;Xu, P.&nbsp;Zhao, Q.&nbsp;Sun, X.&nbsp;Geng, W.&nbsp;Hu, C.&nbsp;Tao, J.&nbsp;Ma, Q.&nbsp;Lin, and
D.&nbsp;Jiang.

</span>
<span class="ltx_bibblock">Wizardcoder: Empowering code large language models with
evol-instruct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">ArXiv preprint</em>, abs/2306.08568, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.08568" title="">https://arxiv.org/abs/2306.08568</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marie et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Marie, R.&nbsp;Rubino, and A.&nbsp;Fujita.

</span>
<span class="ltx_bibblock">Tagged back-translation revisited: Why does it really work?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 5990–5997, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2020.acl-main.532" title="">10.18653/v1/2020.acl-main.532</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.532" title="">https://aclanthology.org/2020.acl-main.532</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Masry, P.&nbsp;Kavehzadeh, X.&nbsp;L. Do, E.&nbsp;Hoque, and S.&nbsp;Joty.

</span>
<span class="ltx_bibblock">UniChart: A universal vision-language pretrained model for chart
comprehension and reasoning.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 14662–14684, Singapore, 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.906" title="">10.18653/v1/2023.emnlp-main.906</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.906" title="">https://aclanthology.org/2023.emnlp-main.906</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mattern et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Mattern, F.&nbsp;Mireshghallah, Z.&nbsp;Jin, B.&nbsp;Schölkopf, M.&nbsp;Sachan, and
T.&nbsp;Berg-Kirkpatrick.

</span>
<span class="ltx_bibblock">Membership inference attacks against language models via
neighbourhood comparison.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">ArXiv preprint</em>, abs/2305.18462, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.18462" title="">https://arxiv.org/abs/2305.18462</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Meng, J.&nbsp;Huang, Y.&nbsp;Zhang, and J.&nbsp;Han.

</span>
<span class="ltx_bibblock">Generating training data with language models: Towards zero-shot
language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Advances in Neural Information Processing Systems</em>,
35:462–477, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Meta and microsoft introduce the next generation of llama.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/llama-2" title="">https://ai.meta.com/blog/llama-2</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Min, K.&nbsp;Krishna, X.&nbsp;Lyu, M.&nbsp;Lewis, W.-t. Yih, P.&nbsp;W. Koh, M.&nbsp;Iyyer,
L.&nbsp;Zettlemoyer, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Factscore: Fine-grained atomic evaluation of factual precision in
long form text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2305.14251</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Muennighoff, A.&nbsp;Rush, B.&nbsp;Barak, T.&nbsp;Le&nbsp;Scao, N.&nbsp;Tazi, A.&nbsp;Piktus, S.&nbsp;Pyysalo,
T.&nbsp;Wolf, and C.&nbsp;A. Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Mukherjee, A.&nbsp;Mitra, G.&nbsp;Jawahar, S.&nbsp;Agarwal, H.&nbsp;Palangi, and A.&nbsp;Awadallah.

</span>
<span class="ltx_bibblock">Orca: Progressive learning from complex explanation traces of gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">ArXiv preprint</em>, abs/2306.02707, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.02707" title="">https://arxiv.org/abs/2306.02707</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;I. Nikolenko.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Synthetic data for deep learning</em>, volume 174.

</span>
<span class="ltx_bibblock">Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ntoutsi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Ntoutsi, P.&nbsp;Fafalios, U.&nbsp;Gadiraju, V.&nbsp;Iosifidis, W.&nbsp;Nejdl, M.-E. Vidal,
S.&nbsp;Ruggieri, F.&nbsp;Turini, S.&nbsp;Papadopoulos, E.&nbsp;Krasanakis, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bias in data-driven artificial intelligence systems—an introductory
survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</em>, 10(3):e1356, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oren et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Oren, N.&nbsp;Meister, N.&nbsp;Chatterji, F.&nbsp;Ladhak, and T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Proving test set contamination in black box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">ArXiv preprint</em>, abs/2310.17623, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.17623" title="">https://arxiv.org/abs/2310.17623</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;L. Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang,
S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray, J.&nbsp;Schulman, J.&nbsp;Hilton, F.&nbsp;Kelton, L.&nbsp;Miller,
M.&nbsp;Simens, A.&nbsp;Askell, P.&nbsp;Welinder, P.&nbsp;Christiano, J.&nbsp;Leike, and R.&nbsp;Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">ArXiv preprint</em>, abs/2203.02155, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.02155" title="">https://arxiv.org/abs/2203.02155</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Pan, K.&nbsp;Bhatia, and J.&nbsp;Steinhardt.

</span>
<span class="ltx_bibblock">The effects of reward misspecification: Mapping and mitigating
misaligned models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=JYtwGwIL7ye" title="">https://openreview.net/forum?id=JYtwGwIL7ye</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;S. Park, J.&nbsp;O’Brien, C.&nbsp;J. Cai, M.&nbsp;R. Morris, P.&nbsp;Liang, and M.&nbsp;S. Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Proceedings of the 36th Annual ACM Symposium on User
Interface Software and Technology</em>, pages 1–22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paster et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Paster, M.&nbsp;D. Santos, Z.&nbsp;Azerbayev, and J.&nbsp;Ba.

</span>
<span class="ltx_bibblock">Openwebmath: An open dataset of high-quality mathematical web text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">ArXiv preprint</em>, abs/2310.06786, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06786" title="">https://arxiv.org/abs/2310.06786</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel and Pavlick (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Patel and E.&nbsp;Pavlick.

</span>
<span class="ltx_bibblock">Mapping language models to grounded conceptual spaces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gJcEM8sxHK" title="">https://openreview.net/forum?id=gJcEM8sxHK</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Huang, F.&nbsp;Song, T.&nbsp;Cai, R.&nbsp;Ring, J.&nbsp;Aslanides, A.&nbsp;Glaese,
N.&nbsp;McAleese, and G.&nbsp;Irving.

</span>
<span class="ltx_bibblock">Red teaming language models with language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 3419–3448, Abu Dhabi, United Arab
Emirates, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.225" title="">https://aclanthology.org/2022.emnlp-main.225</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Ringer, K.&nbsp;Lukošiūtė, K.&nbsp;Nguyen, E.&nbsp;Chen, S.&nbsp;Heiner,
C.&nbsp;Pettit, C.&nbsp;Olsson, S.&nbsp;Kundu, S.&nbsp;Kadavath, et&nbsp;al.

</span>
<span class="ltx_bibblock">Discovering language model behaviors with model-written evaluations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Findings of the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13387–13434.
Association for Computational Linguistics, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Pham, X.&nbsp;Wang, Y.&nbsp;Yang, and G.&nbsp;Neubig.

</span>
<span class="ltx_bibblock">Meta back-translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=3jjmdp7Hha" title="">https://openreview.net/forum?id=3jjmdp7Hha</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Przystupa and Abdul-Mageed (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Przystupa and M.&nbsp;Abdul-Mageed.

</span>
<span class="ltx_bibblock">Neural machine translation of low-resource and similar languages with
backtranslation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 3: Shared Task Papers, Day 2)</em>, pages 224–235, Florence, Italy,
2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/W19-5431" title="">10.18653/v1/W19-5431</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-5431" title="">https://aclanthology.org/W19-5431</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;W. Kim, C.&nbsp;Hallacy, A.&nbsp;Ramesh, G.&nbsp;Goh, S.&nbsp;Agarwal, G.&nbsp;Sastry,
A.&nbsp;Askell, P.&nbsp;Mishkin, J.&nbsp;Clark, G.&nbsp;Krueger, and I.&nbsp;Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In M.&nbsp;Meila and T.&nbsp;Zhang, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib126.2.2">Proceedings of Machine Learning
Research</em>, pages 8748–8763. PMLR, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v139/radford21a.html" title="">http://proceedings.mlr.press/v139/radford21a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, S.&nbsp;Borgeaud, T.&nbsp;Cai, K.&nbsp;Millican, J.&nbsp;Hoffmann, F.&nbsp;Song,
J.&nbsp;Aslanides, S.&nbsp;Henderson, R.&nbsp;Ring, S.&nbsp;Young, E.&nbsp;Rutherford, T.&nbsp;Hennigan,
J.&nbsp;Menick, A.&nbsp;Cassirer, R.&nbsp;Powell, G.&nbsp;v.&nbsp;d. Driessche, L.&nbsp;A. Hendricks,
M.&nbsp;Rauh, P.-S. Huang, A.&nbsp;Glaese, J.&nbsp;Welbl, S.&nbsp;Dathathri, S.&nbsp;Huang, J.&nbsp;Uesato,
J.&nbsp;Mellor, I.&nbsp;Higgins, A.&nbsp;Creswell, N.&nbsp;McAleese, A.&nbsp;Wu, E.&nbsp;Elsen,
S.&nbsp;Jayakumar, E.&nbsp;Buchatskaya, D.&nbsp;Budden, E.&nbsp;Sutherland, K.&nbsp;Simonyan,
M.&nbsp;Paganini, L.&nbsp;Sifre, L.&nbsp;Martens, X.&nbsp;L. Li, A.&nbsp;Kuncoro, A.&nbsp;Nematzadeh,
E.&nbsp;Gribovskaya, D.&nbsp;Donato, A.&nbsp;Lazaridou, A.&nbsp;Mensch, J.-B. Lespiau,
M.&nbsp;Tsimpoukelli, N.&nbsp;Grigorev, D.&nbsp;Fritz, T.&nbsp;Sottiaux, M.&nbsp;Pajarskas, T.&nbsp;Pohlen,
Z.&nbsp;Gong, D.&nbsp;Toyama, C.&nbsp;d.&nbsp;M. d’Autume, Y.&nbsp;Li, T.&nbsp;Terzi, V.&nbsp;Mikulik,
I.&nbsp;Babuschkin, A.&nbsp;Clark, D.&nbsp;d.&nbsp;L. Casas, A.&nbsp;Guy, C.&nbsp;Jones, J.&nbsp;Bradbury,
M.&nbsp;Johnson, B.&nbsp;Hechtman, L.&nbsp;Weidinger, I.&nbsp;Gabriel, W.&nbsp;Isaac, E.&nbsp;Lockhart,
S.&nbsp;Osindero, L.&nbsp;Rimell, C.&nbsp;Dyer, O.&nbsp;Vinyals, K.&nbsp;Ayoub, J.&nbsp;Stanway,
L.&nbsp;Bennett, D.&nbsp;Hassabis, K.&nbsp;Kavukcuoglu, and G.&nbsp;Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.11446" title="">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Rafailov, A.&nbsp;Sharma, E.&nbsp;Mitchell, S.&nbsp;Ermon, C.&nbsp;D. Manning, and C.&nbsp;Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a
reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">NeurIPS</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258959321" title="">https://api.semanticscholar.org/CorpusID:258959321</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Ramesh, P.&nbsp;Dhariwal, A.&nbsp;Nichol, C.&nbsp;Chu, and M.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">ArXiv preprint</em>, abs/2204.06125, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06125" title="">https://arxiv.org/abs/2204.06125</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riabi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Riabi, T.&nbsp;Scialom, R.&nbsp;Keraron, B.&nbsp;Sagot, D.&nbsp;Seddah, and J.&nbsp;Staiano.

</span>
<span class="ltx_bibblock">Synthetic data augmentation for zero-shot cross-lingual question
answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7016–7030, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.emnlp-main.562" title="">10.18653/v1/2021.emnlp-main.562</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.562" title="">https://aclanthology.org/2021.emnlp-main.562</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rid (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Rid.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">Active measures: The secret history of disinformation and
political warfare</em>.

</span>
<span class="ltx_bibblock">Farrar, Straus and Giroux, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Saharia, W.&nbsp;Chan, S.&nbsp;Saxena, L.&nbsp;Li, J.&nbsp;Whang, E.&nbsp;L. Denton, K.&nbsp;Ghasemipour,
R.&nbsp;Gontijo&nbsp;Lopes, B.&nbsp;Karagol&nbsp;Ayan, T.&nbsp;Salimans, J.&nbsp;Ho, D.&nbsp;J. Fleet, and
M.&nbsp;Norouzi.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock">In S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;35,
pages 36479–36494. Curran Associates, Inc., 2022a.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Saharia, W.&nbsp;Chan, S.&nbsp;Saxena, L.&nbsp;Li, J.&nbsp;Whang, E.&nbsp;L. Denton, K.&nbsp;Ghasemipour,
R.&nbsp;Gontijo&nbsp;Lopes, B.&nbsp;Karagol&nbsp;Ayan, T.&nbsp;Salimans, et&nbsp;al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">Advances in neural information processing systems</em>,
35:36479–36494, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxton et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Saxton, E.&nbsp;Grefenstette, F.&nbsp;Hill, and P.&nbsp;Kohli.

</span>
<span class="ltx_bibblock">Analysing mathematical reasoning abilities of neural models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=H1gR5iR5FX" title="">https://openreview.net/forum?id=H1gR5iR5FX</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro,
L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Sennrich, B.&nbsp;Haddow, and A.&nbsp;Birch.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin,
Germany, 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P16-1009" title="">10.18653/v1/P16-1009</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P16-1009" title="">https://aclanthology.org/P16-1009</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shakeri et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Shakeri, N.&nbsp;Constant, M.&nbsp;Kale, and L.&nbsp;Xue.

</span>
<span class="ltx_bibblock">Towards zero-shot multilingual synthetic question and answer
generation for cross-lingual reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">Proceedings of the 14th International Conference on Natural
Language Generation</em>, pages 35–45, Aberdeen, Scotland, UK, 2021. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.inlg-1.4" title="">https://aclanthology.org/2021.inlg-1.4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Shao, P.&nbsp;Wang, Q.&nbsp;Zhu, R.&nbsp;Xu, J.&nbsp;Song, M.&nbsp;Zhang, Y.&nbsp;K. Li, Y.&nbsp;Wu, and
D.&nbsp;Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open
language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Sharma, M.&nbsp;Tong, T.&nbsp;Korbak, D.&nbsp;Duvenaud, A.&nbsp;Askell, S.&nbsp;R. Bowman, E.&nbsp;DURMUS,
Z.&nbsp;Hatfield-Dodds, S.&nbsp;R. Johnston, S.&nbsp;M. Kravec, T.&nbsp;Maxwell, S.&nbsp;McCandlish,
K.&nbsp;Ndousse, O.&nbsp;Rausch, N.&nbsp;Schiefer, D.&nbsp;Yan, M.&nbsp;Zhang, and E.&nbsp;Perez.

</span>
<span class="ltx_bibblock">Towards understanding sycophancy in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Shi, A.&nbsp;Ajith, M.&nbsp;Xia, Y.&nbsp;Huang, D.&nbsp;Liu, T.&nbsp;Blevins, D.&nbsp;Chen, and
L.&nbsp;Zettlemoyer.

</span>
<span class="ltx_bibblock">Detecting pretraining data from large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, F.&nbsp;Cassano, A.&nbsp;Gopinath, K.&nbsp;Narasimhan, and S.&nbsp;Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shypula et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Shypula, A.&nbsp;Madaan, Y.&nbsp;Zeng, U.&nbsp;Alon, J.&nbsp;Gardner, M.&nbsp;Hashemi, G.&nbsp;Neubig,
P.&nbsp;Ranganathan, O.&nbsp;Bastani, and A.&nbsp;Yazdanbakhsh.

</span>
<span class="ltx_bibblock">Learning performance-improving code edits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">ArXiv preprint</em>, abs/2302.07867, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.07867" title="">https://arxiv.org/abs/2302.07867</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Si, Y.&nbsp;Zhang, Z.&nbsp;Yang, R.&nbsp;Liu, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">Design2code: How far are we from automating front-end engineering?,
2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.03163" title="">https://arxiv.org/abs/2403.03163</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, S.&nbsp;Azizi, T.&nbsp;Tu, S.&nbsp;S. Mahdavi, J.&nbsp;Wei, H.&nbsp;W. Chung, N.&nbsp;Scales,
A.&nbsp;Tanwani, H.&nbsp;Cole-Lewis, S.&nbsp;Pfohl, et&nbsp;al.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">ArXiv preprint</em>, abs/2212.13138, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.13138" title="">https://arxiv.org/abs/2212.13138</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinhardt (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Steinhardt.

</span>
<span class="ltx_bibblock">Ml systems will have weird failure modes.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/" title="">https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/</a>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, S.&nbsp;Shen, S.&nbsp;Cao, H.&nbsp;Liu, C.&nbsp;Li, Y.&nbsp;Shen, C.&nbsp;Gan, L.-Y. Gui, Y.-X. Wang,
Y.&nbsp;Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Aligning large multimodal models with factually augmented rlhf.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">ArXiv preprint</em>, abs/2309.14525, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.14525" title="">https://arxiv.org/abs/2309.14525</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Tang, Z.&nbsp;Deng, H.&nbsp;Lin, X.&nbsp;Han, Q.&nbsp;Liang, and L.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Toolalpaca: Generalized tool learning for language models with 3000
simulated cases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">ArXiv preprint</em>, abs/2306.05301, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.05301" title="">https://arxiv.org/abs/2306.05301</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taylor, M.&nbsp;Kardas, G.&nbsp;Cucurull, T.&nbsp;Scialom, A.&nbsp;Hartshorn, E.&nbsp;Saravia,
A.&nbsp;Poulton, V.&nbsp;Kerkez, and R.&nbsp;Stojnic.

</span>
<span class="ltx_bibblock">Galactica: A large language model for science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">ArXiv preprint</em>, abs/2211.09085, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09085" title="">https://arxiv.org/abs/2211.09085</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Thoppilan, D.&nbsp;De&nbsp;Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.-T. Cheng,
A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du, Y.&nbsp;Li, H.&nbsp;Lee, H.&nbsp;S. Zheng, A.&nbsp;Ghafouri,
M.&nbsp;Menegali, Y.&nbsp;Huang, M.&nbsp;Krikun, D.&nbsp;Lepikhin, J.&nbsp;Qin, D.&nbsp;Chen, Y.&nbsp;Xu,
Z.&nbsp;Chen, A.&nbsp;Roberts, M.&nbsp;Bosma, V.&nbsp;Zhao, Y.&nbsp;Zhou, C.-C. Chang, I.&nbsp;Krivokon,
W.&nbsp;Rusch, M.&nbsp;Pickett, P.&nbsp;Srinivasan, L.&nbsp;Man, K.&nbsp;Meier-Hellstern, M.&nbsp;R.
Morris, T.&nbsp;Doshi, R.&nbsp;D. Santos, T.&nbsp;Duke, J.&nbsp;Soraker, B.&nbsp;Zevenbergen,
V.&nbsp;Prabhakaran, M.&nbsp;Diaz, B.&nbsp;Hutchinson, K.&nbsp;Olson, A.&nbsp;Molina, E.&nbsp;Hoffman-John,
J.&nbsp;Lee, L.&nbsp;Aroyo, R.&nbsp;Rajakumar, A.&nbsp;Butryna, M.&nbsp;Lamm, V.&nbsp;Kuzmina, J.&nbsp;Fenton,
A.&nbsp;Cohen, R.&nbsp;Bernstein, R.&nbsp;Kurzweil, B.&nbsp;Aguera-Arcas, C.&nbsp;Cui, M.&nbsp;Croak,
E.&nbsp;Chi, and Q.&nbsp;Le.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">ArXiv preprint</em>, abs/2201.08239, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.08239" title="">https://arxiv.org/abs/2201.08239</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Tian, E.&nbsp;Mitchell, H.&nbsp;Yao, C.&nbsp;D. Manning, and C.&nbsp;Finn.

</span>
<span class="ltx_bibblock">Fine-tuning language models for factuality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">ICLR</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265158181" title="">https://api.semanticscholar.org/CorpusID:265158181</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Todorov et&nbsp;al. (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Todorov, T.&nbsp;Erez, and Y.&nbsp;Tassa.

</span>
<span class="ltx_bibblock">Mujoco: A physics engine for model-based control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems</em>, pages 5026–5033. IEEE, 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/IROS.2012.6386109" title="">10.1109/IROS.2012.6386109</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei,
N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">ArXiv preprint</em>, abs/2307.09288, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;H. Trinh, Y.&nbsp;Wu, Q.&nbsp;V. Le, H.&nbsp;He, and T.&nbsp;Luong.

</span>
<span class="ltx_bibblock">Solving olympiad geometry without human demonstrations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Nature</em>, 625(7995):476–482, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Breugel et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Van&nbsp;Breugel, Z.&nbsp;Qian, and M.&nbsp;Van Der&nbsp;Schaar.

</span>
<span class="ltx_bibblock">Synthetic data, real errors: how (not) to publish and use synthetic
data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">International Conference on Machine Learning</em>, pages
34793–34808. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vezhnevets et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;S. Vezhnevets, J.&nbsp;P. Agapiou, A.&nbsp;Aharon, R.&nbsp;Ziv, J.&nbsp;Matyas, E.&nbsp;A.
Duéñez-Guzmán, W.&nbsp;A. Cunningham, S.&nbsp;Osindero, D.&nbsp;Karmon, and
J.&nbsp;Z. Leibo.

</span>
<span class="ltx_bibblock">Generative agent-based modeling with actions grounded in physical,
social, or digital space using concordia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">ArXiv preprint</em>, abs/2312.03664, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.03664" title="">https://arxiv.org/abs/2312.03664</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Villalobos, J.&nbsp;Sevilla, L.&nbsp;Heim, T.&nbsp;Besiroglu, M.&nbsp;Hobbhahn, and A.&nbsp;Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">ArXiv preprint</em>, abs/2211.04325, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.04325" title="">https://arxiv.org/abs/2211.04325</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Wang, Y.&nbsp;Xie, Y.&nbsp;Jiang, A.&nbsp;Mandlekar, C.&nbsp;Xiao, Y.&nbsp;Zhu, L.&nbsp;Fan, and
A.&nbsp;Anandkumar.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">ArXiv preprint</em>, abs/2305.16291, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.16291" title="">https://arxiv.org/abs/2305.16291</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;Le, E.&nbsp;Chi, S.&nbsp;Narang, A.&nbsp;Chowdhery, and
D.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language
models.

</span>
<span class="ltx_bibblock">2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.11171" title="">https://arxiv.org/abs/2203.11171</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, Y.&nbsp;Kordi, S.&nbsp;Mishra, A.&nbsp;Liu, N.&nbsp;A. Smith, D.&nbsp;Khashabi, and
H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated
instructions.

</span>
<span class="ltx_bibblock">volume abs/2212.10560, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.10560" title="">https://arxiv.org/abs/2212.10560</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, X.&nbsp;Wang, B.&nbsp;An, D.&nbsp;Yu, and C.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Towards faithful neural table-to-text generation with
content-matching constraints.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1072–1086, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2020.acl-main.101" title="">10.18653/v1/2020.acl-main.101</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.101" title="">https://aclanthology.org/2020.acl-main.101</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, A.&nbsp;Suriawinata, L.&nbsp;Vaickus, B.&nbsp;Ren, X.&nbsp;Liu, J.&nbsp;Wei, and S.&nbsp;Hassanpour.

</span>
<span class="ltx_bibblock">Generative image translation for data augmentation in colorectal
histopathology images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">Advances in Neural Information Processing Systems</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, M.&nbsp;Bosma, V.&nbsp;Y. Zhao, K.&nbsp;Guu, A.&nbsp;W. Yu, B.&nbsp;Lester, N.&nbsp;Du, A.&nbsp;M. Dai,
and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, L.&nbsp;Hou, A.&nbsp;Lampinen, X.&nbsp;Chen, D.&nbsp;Huang, Y.&nbsp;Tay, X.&nbsp;Chen, Y.&nbsp;Lu,
D.&nbsp;Zhou, T.&nbsp;Ma, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Symbol tuning improves in-context learning in language models.

</span>
<span class="ltx_bibblock">volume abs/2305.08298, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.08298" title="">https://arxiv.org/abs/2305.08298</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, D.&nbsp;Huang, Y.&nbsp;Lu, D.&nbsp;Zhou, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Simple synthetic data reduces sycophancy in large language models,
2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.03958" title="">https://arxiv.org/abs/2308.03958</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, C.&nbsp;Yang, X.&nbsp;Song, Y.&nbsp;Lu, N.&nbsp;Hu, D.&nbsp;Tran, D.&nbsp;Peng, R.&nbsp;Liu, D.&nbsp;Huang,
C.&nbsp;Du, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Long-form factuality in large language models.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:268724304" title="">https://api.semanticscholar.org/CorpusID:268724304</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wei, Z.&nbsp;Wang, J.&nbsp;Liu, Y.&nbsp;Ding, and L.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Magicoder: Source code is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">ArXiv preprint</em>, abs/2312.02120, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.02120" title="">https://arxiv.org/abs/2312.02120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Weidinger, J.&nbsp;Mellor, M.&nbsp;Rauh, C.&nbsp;Griffin, J.&nbsp;Uesato, P.-S. Huang, M.&nbsp;Cheng,
M.&nbsp;Glaese, B.&nbsp;Balle, A.&nbsp;Kasirzadeh, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ethical and social risks of harm from language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">ArXiv preprint</em>, abs/2112.04359, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.04359" title="">https://arxiv.org/abs/2112.04359</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wood et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Wood, T.&nbsp;Baltrusaitis, C.&nbsp;Hewitt, S.&nbsp;Dziadzio, T.&nbsp;J. Cashman, and
J.&nbsp;Shotton.

</span>
<span class="ltx_bibblock">Fake it till you make it: face analysis in the wild using synthetic
data alone.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">2021 IEEE/CVF International Conference on Computer Vision,
ICCV 2021, Montreal, QC, Canada, October 10-17, 2021</em>, pages 3661–3671.
IEEE, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/ICCV48922.2021.00366" title="">10.1109/ICCV48922.2021.00366</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCV48922.2021.00366" title="">https://doi.org/10.1109/ICCV48922.2021.00366</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xu, Q.&nbsp;Sun, K.&nbsp;Zheng, X.&nbsp;Geng, P.&nbsp;Zhao, J.&nbsp;Feng, C.&nbsp;Tao, and D.&nbsp;Jiang.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex
instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">ArXiv preprint</em>, abs/2304.12244, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.12244" title="">https://arxiv.org/abs/2304.12244</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Xu, Y.&nbsp;Ruan, W.&nbsp;Bi, G.&nbsp;Huang, S.&nbsp;Shi, L.&nbsp;Chen, and L.&nbsp;Liu.

</span>
<span class="ltx_bibblock">On synthetic data for back translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 419–430, Seattle, United States, 2022. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.naacl-main.32" title="">10.18653/v1/2022.naacl-main.32</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.32" title="">https://aclanthology.org/2022.naacl-main.32</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua,
and C.&nbsp;Raffel.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">arXiv preprint arXiv:2010.11934</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Yang, A.&nbsp;Prabhakar, K.&nbsp;Narasimhan, and S.&nbsp;Yao.

</span>
<span class="ltx_bibblock">Intercode: Standardizing and benchmarking interactive coding with
execution feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ye, S.&nbsp;Li, G.&nbsp;Li, C.&nbsp;Huang, S.&nbsp;Gao, Y.&nbsp;Wu, Q.&nbsp;Zhang, T.&nbsp;Gui, and X.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Toolsword: Unveiling safety issues of large language models in tool
learning across three stages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">ArXiv preprint</em>, abs/2402.10753, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.10753" title="">https://arxiv.org/abs/2402.10753</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Yu, W.&nbsp;Jiang, H.&nbsp;Shi, J.&nbsp;Yu, Z.&nbsp;Liu, Y.&nbsp;Zhang, J.&nbsp;T. Kwok, Z.&nbsp;Li, A.&nbsp;Weller,
and W.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Metamath: Bootstrap your own mathematical questions for large
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">ArXiv preprint</em>, abs/2309.12284, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.12284" title="">https://arxiv.org/abs/2309.12284</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Yu, Y.&nbsp;Zhuang, J.&nbsp;Zhang, Y.&nbsp;Meng, A.&nbsp;J. Ratner, R.&nbsp;Krishna, J.&nbsp;Shen, and
C.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Large language model as attributed training data generator: A tale of
diversity and bias.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Yuan, R.&nbsp;Y. Pang, K.&nbsp;Cho, S.&nbsp;Sukhbaatar, J.&nbsp;Xu, and J.&nbsp;Weston.

</span>
<span class="ltx_bibblock">Self-rewarding language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">ArXiv preprint</em>, abs/2401.10020, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.10020" title="">https://arxiv.org/abs/2401.10020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Li, G.&nbsp;Dong, C.&nbsp;Tan, and C.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Scaling relationship on learning mathematical reasoning with large
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">ArXiv preprint</em>, abs/2308.01825, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.01825" title="">https://arxiv.org/abs/2308.01825</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Zelikman, Y.&nbsp;Wu, and N.&nbsp;D. Goodman.

</span>
<span class="ltx_bibblock">Star: Bootstrapping reasoning with reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">NeurIPS</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:247762790" title="">https://api.semanticscholar.org/CorpusID:247762790</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, X.&nbsp;Xu, and S.&nbsp;Deng.

</span>
<span class="ltx_bibblock">Exploring collaboration mechanisms for llm agents: A social
psychology view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">ArXiv preprint</em>, abs/2310.02124, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.02124" title="">https://arxiv.org/abs/2310.02124</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, L.&nbsp;Dong, X.&nbsp;Li, S.&nbsp;Zhang, X.&nbsp;Sun, S.&nbsp;Wang, J.&nbsp;Li, R.&nbsp;Hu, T.&nbsp;Zhang,
F.&nbsp;Wu, and G.&nbsp;Wang.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey,
2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.10792" title="">https://arxiv.org/abs/2308.10792</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, Y.&nbsp;Li, L.&nbsp;Cui, D.&nbsp;Cai, L.&nbsp;Liu, T.&nbsp;Fu, X.&nbsp;Huang, E.&nbsp;Zhao, Y.&nbsp;Zhang,
Y.&nbsp;Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: A survey on hallucination in large
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">ArXiv preprint</em>, abs/2309.01219, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.01219" title="">https://arxiv.org/abs/2309.01219</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, R.&nbsp;Zhang, J.&nbsp;Gu, Y.&nbsp;Zhou, N.&nbsp;Lipka, D.&nbsp;Yang, and T.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">ArXiv preprint</em>, abs/2306.17107, 2023d.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.17107" title="">https://arxiv.org/abs/2306.17107</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Zhao, B.&nbsp;Wu, and T.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Svit: Scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">ArXiv preprint</em>, abs/2307.04087, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.04087" title="">https://arxiv.org/abs/2307.04087</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Zhao, T.&nbsp;Wang, M.&nbsp;Yatskar, V.&nbsp;Ordonez, and K.-W. Chang.

</span>
<span class="ltx_bibblock">Gender bias in coreference resolution: Evaluation and debiasing
methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</em>, pages 15–20, New Orleans, Louisiana,
2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/N18-2003" title="">10.18653/v1/N18-2003</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N18-2003" title="">https://aclanthology.org/N18-2003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Zheng, W.-L. Chiang, Y.&nbsp;Sheng, S.&nbsp;Zhuang, Z.&nbsp;Wu, Y.&nbsp;Zhuang, Z.&nbsp;Lin, Z.&nbsp;Li,
D.&nbsp;Li, E.&nbsp;P. Xing, H.&nbsp;Zhang, J.&nbsp;E. Gonzalez, and I.&nbsp;Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zheng, A.&nbsp;Trott, S.&nbsp;Srinivasa, D.&nbsp;C. Parkes, and R.&nbsp;Socher.

</span>
<span class="ltx_bibblock">The ai economist: Taxation policy design via two-level deep
multiagent reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">Science advances</em>, 8(18):eabk2607, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Zheng, H.&nbsp;Zhou, S.&nbsp;Huang, L.&nbsp;Li, X.&nbsp;Dai, and J.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Mirror-generative neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HkxQRTNYPH" title="">https://openreview.net/forum?id=HkxQRTNYPH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Zhou, Z.&nbsp;Su, T.&nbsp;Eisape, H.&nbsp;Kim, and M.&nbsp;Sap.

</span>
<span class="ltx_bibblock">Is this the real life? is this just fantasy? the misleading success
of simulating social interactions with llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">ArXiv preprint</em>, abs/2403.05020, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05020" title="">https://arxiv.org/abs/2403.05020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Ziems, J.&nbsp;Dwivedi-Yu, Y.-C. Wang, A.&nbsp;Halevy, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">Normbank: A knowledge bank of situational social norms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">ArXiv preprint</em>, abs/2305.17008, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.17008" title="">https://arxiv.org/abs/2305.17008</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Zou, Z.&nbsp;Wang, J.&nbsp;Z. Kolter, and M.&nbsp;Fredrikson.

</span>
<span class="ltx_bibblock">Universal and transferable adversarial attacks on aligned language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">ArXiv preprint</em>, abs/2307.15043, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.15043" title="">https://arxiv.org/abs/2307.15043</a>.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
          <div class="ltx_page_logo">
              Generated by
              <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                  <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                      L
                      <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                      T
                      <span style="position: relative; bottom: -0.4ex;">E</span>
                  </span>
                  <span class="ltx_font_smallcaps">xml</span>
                  <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
              </a>
          </div></div><footer id="footer" class="ltx_document">
          <div class="keyboard-glossary">
              <h2>Instructions for reporting errors</h2>
              <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
              <ul>
                  <li>Click the "Report Issue" button.</li>
                  <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                  <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                  <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
              </ul>
              <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
              <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
          </div>
      </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header" data-bs-theme="dark"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Best Practices and Lessons Learned on Synthetic Data for Language Models</title>
<!--Generated on Thu Apr 11 06:33:42 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2404.07503v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.07503v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.07503v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.07503v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S1" title="1 Introduction ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2" title="2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Synthetic Data in Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1" title="2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1.SSS0.Px1" title="Math. ‣ 2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Math.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1.SSS0.Px2" title="Code. ‣ 2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Code.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS1.SSS0.Px3" title="Other reasoning tasks. ‣ 2.1 Reasoning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Other reasoning tasks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS2" title="2.2 Tool-using and Planning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tool-using and Planning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS2.SSS0.Px1" title="Learning tool-using through synthetic trajectories. ‣ 2.2 Tool-using and Planning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Learning tool-using through synthetic trajectories.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS2.SSS0.Px2" title="Learning to plan in synthetic environments. ‣ 2.2 Tool-using and Planning ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Learning to plan in synthetic environments.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS3" title="2.3 Multimodality ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Multimodality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS3.SSS0.Px1" title="Reverse rendering from vision to text. ‣ 2.3 Multimodality ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Reverse rendering from vision to text.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS3.SSS0.Px2" title="Multi-modality instruction following. ‣ 2.3 Multimodality ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Multi-modality instruction following.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS4" title="2.4 Multilingual ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Multilingual</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS4.SSS0.Px1" title="Back-translation augmentation. ‣ 2.4 Multilingual ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Back-translation augmentation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS4.SSS0.Px2" title="Generating multilingual questions and answers at scale. ‣ 2.4 Multilingual ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Generating multilingual questions and answers at scale.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5" title="2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5.SSS0.Px1" title="Instruction Following. ‣ 2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Instruction Following.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5.SSS0.Px2" title="Mitigating hallucination. ‣ 2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Mitigating hallucination.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2.SS5.SSS0.Px3" title="Aligning with shared human preference and values. ‣ 2.5 Alignment ‣ 2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Aligning with shared human preference and values.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3" title="3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Synthetic Data in Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3.SS0.SSS0.Px1" title="Factuality. ‣ 3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Factuality.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3.SS0.SSS0.Px2" title="Safety. ‣ 3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Safety.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3.SS0.SSS0.Px3" title="Assisting human evaluation. ‣ 3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Assisting human evaluation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4" title="4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Challenges and Limitations of Synthetic Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4.SS0.SSS0.Px1" title="Misuse of synthetic data might proliferate misinformation. ‣ 4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Misuse of synthetic data might proliferate misinformation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4.SS0.SSS0.Px2" title="Synthetic data might cause ambiguity in AI alignment. ‣ 4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Synthetic data might cause ambiguity in AI alignment.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4.SS0.SSS0.Px3" title="Training with synthetic data makes evaluation decontamination harder. ‣ 4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Training with synthetic data makes evaluation decontamination harder.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5" title="5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Directions for Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px1" title="Synthetic data scaling. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Synthetic data scaling.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px2" title="Further improving quality and diversity of synthetic data. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Further improving quality and diversity of synthetic data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px3" title="Towards high-fidelity and more efficient scalable oversight. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">Towards high-fidelity and more efficient scalable oversight.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5.SS0.SSS0.Px4" title="The emergent self-improvement capability. ‣ 5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title">The emergent self-improvement capability.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S6" title="6 Conclusion ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2404.07503v1 [cs.CL] 11 Apr 2024</div></div>
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\correspondingauthor</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">ruiboliu@google.com</p>
</div>
<h1 class="ltx_title ltx_title_document">Best Practices and Lessons Learned on Synthetic Data for Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruibo Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jerry Wei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fangyu Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenglei Si
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Stanford University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanzhe Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Georgia Institute of Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinmeng Rao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daiyi Peng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Diyi Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Stanford University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denny Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew M. Dai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">AI 모델의 성공은 데이터 부족, 개인 정보 보호 문제 및 높은 비용으로 인해 얻기 어려울 수 있는 크고 다양하며 고품질 데이터 세트의 가용성에 달려 있다. 합성 데이터는 실제 패턴을 모방한 인공 데이터를 생성함으로써 유망한 솔루션으로 부상했다. 이 논문은 합성 데이터 연구의 개요와 응용, 과제 및 향후 방향에 대해 논의한다. 우리는 그 효과를 입증하기 위해 선행 기술의 경험적 증거를 제시하고 사실성, 충실성 및 편향성 보장의 중요성을 강조한다. 우리는 보다 강력하고 포괄적이며 신뢰할 수 있는 언어 모델을 구축하기 위해 합성 데이터를 책임감 있게 사용할 필요성을 강조한다.</p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="389" id="S1.F1.g1" src="https://arxiv.org/html/2404.07503v1/extracted/5529949/assets/manufacture_hd.png" width="389">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1: </span>Imagen <cite class="ltx_cite ltx_citemacro_citep">(Saharia et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib132" title="">2022a</a>)</cite> v2.0에 의해 생성된 하나의 합성 이미지와 다음과 같은 설명을 포함하는 프롬프트가 있습니다. <span class="ltx_text ltx_font_italic" id="S1.F1.2.1">“In a robotics factory, humanoid robots collaborate on a assembly line to design, fabricate, test and assemble new robots. The new robots they manufacturing as the robot workers are similar to creating them. </span> 또한 미적 고려 사항에서 텍스트를 제어하는 몇 가지 스타일을 추가했습니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">인공지능(AI) 기술의 급속한 발전은 보조 에이전트(예: ACT-1, Adept AI<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>ACT-1: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adept.ai/blog/act-1" title="">https://www.adept.ai/blog/act-1</a></span></span></span>) 및 소프트웨어 개발(예: Devin, 인지 Lab<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Devin: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cognition-labs.com/introducing-devin" title="">https://www.cognition-labs.com/introducing-devin</a></span></span></span>)에서 의료 <cite class="ltx_cite ltx_citemacro_citep">(Singhal et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib144" title="">2022</a>)</cite> 및 금융 <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib187" title="">2022</a>)</cite>에 이르기까지 수많은 도메인에 걸쳐 광범위한 채택을 가져왔다. 그러나 AI 모델의 성공은 훈련 및 평가를 위한 크고 다양하며 고품질 데이터 세트의 가용성에 크게 의존한다. 이러한 데이터 세트를 획득하는 것은 데이터 희소성 <cite class="ltx_cite ltx_citemacro_citep">(Babbar and Schölkopf, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib11" title="">2019</a>)</cite>, 프라이버시 염려 <cite class="ltx_cite ltx_citemacro_citep">(Abay et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib1" title="">2019</a>)</cite>, 데이터 수집 및 주석 <cite class="ltx_cite ltx_citemacro_citep">(Gilardi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib51" title="">2023b</a>)</cite>의 순수한 비용으로 인해 상당한 도전이 될 수 있다. 비관론자들은 2050년에는 새로운 텍스트 데이터가 고갈되고 2060년에는 이미지 데이터가 고갈될 것이라고 예측한다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">합성 데이터는 이러한 과제 <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib113" title="">2021</a>)</cite>를 해결하기 위한 유망한 솔루션으로 부상했다. 합성 데이터는 실세계 데이터의 특성과 패턴을 모방한 인공적으로 생성된 데이터를 의미하지만, 인간이 직접 생성하는 것이 아니라 알고리즘 <cite class="ltx_cite ltx_citemacro_citep">(Saxton et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib134" title="">2019</a>)</cite>, 생성 모델 <cite class="ltx_cite ltx_citemacro_citep">(Borisov et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib16" title="">2022</a>; Meng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib108" title="">2022</a>)</cite>, 또는 시뮬레이션 <cite class="ltx_cite ltx_citemacro_citep">(Vezhnevets et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib156" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib99" title="">2023c</a>)</cite>를 통해 생성된다. 합성 데이터를 활용하면 실제 데이터의 한계를 극복할 수 있을 뿐만 아니라 보다 강력하고 신뢰할 수 있으며 공정한 AI 모델 <cite class="ltx_cite ltx_citemacro_citep">(Lucini, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib102" title="">2021</a>; Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib101" title="">2023</a>)</cite>를 개발할 수 있는 잠재력을 열 수 있다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">합성 데이터의 많은 이점 중 하나는 대규모로 생성될 수 있어 AI 모델에 대한 훈련 및 테스트 데이터를 풍부하게 제공할 수 있다는 것이다. 이는 실세계 데이터가 부족하거나 얻기 어려운 도메인(예를 들어, 모든 조건을 포함하는 기상 데이터<cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib86" title="">2023a</a>; Lam et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib75" title="">2023</a>)</cite>)에서 특히 유용하다. 둘째, 합성 데이터는 제어된 변형들을 도입함으로써 상이한 클래스들의 균형 잡힌 표현을 보장하는 것과 같은 특정 요건들에 맞춰질 수 있다(예를 들어, 다국어 언어 학습에서 저-자원 언어들을 상향-가중화하는 것<cite class="ltx_cite ltx_citemacro_citep">(Przystupa and Abdul-Mageed, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib125" title="">2019</a>)</cite>). 이러한 데이터 특성에 대한 통제 수준은 모델 성능 및 일반화를 향상시킬 수 있다. 셋째, 합성 데이터는 민감한 개인 정보를 포함하지 않는 익명화 또는 비식별화된 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Howe et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib63" title="">2017</a>; El Emam et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib37" title="">2020</a>)</cite>를 생성함으로써 프라이버시 우려를 완화할 수 있다. 이는 환자의 프라이버시가 가장 중요한 의료와 같은 영역에서 중요하다.<cite class="ltx_cite ltx_citemacro_citep">(Dahmen and Cook, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib32" title="">2019</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib162" title="">2019</a>)</cite></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">그 약속에도 불구하고 합성 데이터는 해결해야 할 과제도 제시한다. 그 중 하나는 거짓, 환각 또는 편향된 합성 데이터에 대해 훈련된 모델이 실제 시나리오 <cite class="ltx_cite ltx_citemacro_citep">(Van Breugel et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib155" title="">2023</a>; Guarnera et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib56" title="">2020</a>)</cite>로 일반화하지 못할 수 있기 때문에 합성 데이터의 사실성과 충실성을 보장하는 것이다. 연구자들은 실제 데이터에서 발견되는 복잡한 패턴과 관계를 정확하게 반영하는 합성 데이터를 만들기 위해 정교한 생성 모델과 평가 메트릭을 개발해야 한다. 또 다른 과제는 합성 데이터가 주의 깊게 설계되고 검증되지 않은 경우 편향을 증폭하거나 새로운 편향을 도입할 수 있는 가능성이다. 우리는 이러한 위험을 완화하기 위해 엄격한 테스트와 공정성 평가가 필요하다고 믿는다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">본 논문에서는 합성 데이터 연구의 현재 상태를 추적하고 현재 모범 사례와 배운 교훈에 대해 논의한다. 나머지 논문은 다음과 같이 정리되어 있다. 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S2" title="2 Synthetic Data in Training ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 모델 훈련에서 합성 데이터 생성 기술과 그 응용에 대한 개요를 제공하여 사례 연구와 경험적 증거를 제시한다. <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S3" title="3 Synthetic Data in Evaluation ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> 섹션에서는 평가에 있어 합성 데이터의 유용성에 대해 논의한다. <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S4" title="4 Challenges and Limitations of Synthetic Data ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> 섹션에서는 합성 데이터의 도전과 한계에 대해 논의하고, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#S5" title="5 Directions for Future Work ‣ Best Practices and Lessons Learned on Synthetic Data for Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> 섹션에서는 잠재적인 솔루션과 향후 연구 방향에 대해 설명한다.</p>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Synthetic Data in Training</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">실제 세계에서 수집된 실제 데이터를 모방하여 생성되는 합성 데이터는 실제 데이터의 효과적이고 비교적 저렴한 대안으로 입증되었다. 이 섹션에서는 합성 학습 데이터를 활용하는 몇 가지 주목할 만한 도메인을 탐색합니다.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Math.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">최근 언어 모델(LMs)에 대한 수학적 추론의 발전은 수학 관련 과제에 대한 성능을 향상시키기 위한 다양한 접근법의 개발로 이어졌다. 한 가지 접근법은 Minerva<cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib83" title="">2022</a>)</cite>, Llemma<cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib10" title="">2023</a>)</cite>, DeepSeekMath<cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib138" title="">2024</a>)</cite>와 같은 수학 목표 사전 훈련 데이터에 대해 훈련하는 것이다. 또 다른 주류 방법은 목표 벤치마크의 훈련 또는 검증 세트를 모방하기 위해 합성 질문 및 답변을 생성하는 것이다. 예를 들어, WizardMath <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib103" title="">2023a</a>)</cite>는 일련의 연산을 활용하여 GPT-3.5를 사용하여 질문과 답변의 복잡도를 높이는 반면, MetaMath <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib175" title="">2023</a>)</cite>는 MATH와 GSM8K에서 질문을 의미적 재작성, 자기 검증, 역방향 추론 등 다양한 방식으로 재작성하여 부트스트랩한다. GAIR-Abel <cite class="ltx_cite ltx_citemacro_citep">(Chern et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib28" title="">2023</a>)</cite>는 질문의 패러프레이징으로 시작하는 답변과 바닐라 형식의 답변보다 더 나은 성능을 보여주는 단계적 솔루션으로 인해 확장된 답변의 형식이 최종 성능에 중요하다는 것을 발견했다. Xwin-Math <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib85" title="">2024</a>)</cite>는 합성 SFT 데이터를 백만 개의 예제로 추가 확장했으며 LLaMA-2 7B 모델 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib153" title="">2023</a>)</cite>는 여전히 데이터 스케일링의 이점을 얻을 수 있음을 발견했다. MMIQC<cite class="ltx_cite ltx_citemacro_citep">(Liu and Yao, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib95" title="">2024</a>)</cite>는 OpenWebMath<cite class="ltx_cite ltx_citemacro_citep">(Paster et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib120" title="">2023</a>)</cite>와 같은 고품질 수학적 사전 훈련 데이터의 하위 집합과 함께 SFT 스타일 데이터(질의 응답 재구성을 통해 또는 MetaMath에서 직접 가져온)를 주입하는 데이터 세트 묶음을 구성했다.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p2.1">합성 수학 데이터의 생성을 확대하는 것은 간단한 과정이지만, 생성된 수학의 정확성을 보장하는 것은 실무자들에게 여전히 중요한 과제로 남아 있다. AlphaGeometry <cite class="ltx_cite ltx_citemacro_citep">(Trinh et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib154" title="">2024</a>)</cite>는 1억 개의 합성 데이터 포인트를 사용하여 신경망 모델을 훈련시켜 이 문제를 해결하려는 최근의 시도입니다. 이 모델은 복잡한 기하학 문제를 해결할 때 각 분기의 정확성을 검증하는 데 있어 솔루션을 제안하고 기호 추론 엔진을 안내한다. 알파기하학은 합성 데이터의 힘을 엄격한 검증 과정과 결합함으로써 인간 올림피아드 금메달리스트에 버금가는 문제 해결 능력을 달성하여 복잡한 수학적 추론 과제를 해결하는 데 있어 이 접근법의 잠재력을 보여준다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Code.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">수학과는 달리 코드 추론을 위한 합성 데이터는 올바른 코드의 한 가지 요구 사항이 실행되고 있기 때문에 실행 결과를 구조화된 코드와 자연스럽게 결합할 수 있다. 코딩 강화 모델에서 CodeRL<cite class="ltx_cite ltx_citemacro_citep">(Le et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib78" title="">2022</a>)</cite>는 합성 코드 샘플에 피드백 신호를 사용하여 사전 훈련된 언어 모델을 개선하는 행위자 비판 접근법을 제시한다. <cite class="ltx_cite ltx_citemacro_cite">Haluptzok et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib58" title="">2022</a>)</cite>는 모델이 자체 합성 퍼즐-솔루션 쌍을 생성하는 자체 개선 전략을 제안한다. 그런 다음 이러한 쌍은 언어 모델을 미세 조정하기 위해 사용되기 전에 실제 인터프리터에 의해 검증되고 필터링된다. <cite class="ltx_cite ltx_citemacro_cite">Shypula et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib142" title="">2023</a>)</cite>는 코드 최적화를 위해 자체 개선 합성 데이터 생성 및 CoT 프롬프트와 같은 시뮬레이션 환경과 적응 전략을 활용하는 프레임워크를 추가로 제안한다. <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib173" title="">2024</a>)</cite>는 강화 학습 환경 내에서 대화형 코드 생성을 향상시키기 위해 설계된 프레임워크인 InterCode를 개발했으며, 여기서 코드는 액션 역할을 하고 실행 피드백은 관찰 역할을 한다. Reflexion <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib141" title="">2024</a>)</cite>는 언어 모델의 코드 추론 능력을 향상시키기 위해 외부 또는 내부 시뮬레이션된 언어 피드백 신호를 사용한다. 합성 SFT 데이터와 관련하여 코드 알파카는 21개의 시드 작업에 걸쳐 ChatGPT에 SELF-INSTRUCT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib159" title="">2022a</a>)</cite>를 적용하여 자동으로 생성된 20K 코드 명령의 데이터세트로 구성된다. WizardCoder <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib104" title="">2023b</a>)</cite>는 합성 데이터의 복잡성과 다양성을 향상시키기 위해 휴리스틱 프롬프트로 ChatGPT를 안내하는 Code Evol-Instruct를 소개한다. 한편, Magicoder <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib167" title="">2023c</a>)</cite>는 오픈 소스 코드 조각으로부터 75K개의 다양한 합성 명령어 샘플을 생성하는 OSS-INSTRUCT를 개발하였다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Other reasoning tasks.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">합성 데이터는 또한 다른 추론 작업에서 인상적인 성능으로 이어진다. 예를 들어, <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib164" title="">2023a</a>)</cite>는 자연어 레이블을 임의의 기호로 대체하여 500k 이상의 합성예제를 생성함으로써 기존의 자연어 데이터셋을 증강시켰다. 이러한 합성 데이터를 지도 미세 조정에 사용하면 보이지 않는 상황 내 학습 및 알고리즘 추론 작업에서 모델 성능이 크게 향상되었다. STaR<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib179" title="">2022</a>)</cite>는 합성 연쇄 사상 근거를 생성하고, 그들의 추론을 개선하기 위해 미세 조정 언어 모델에 대한 오답으로 이어지는 것들을 걸러낸다. 물리학 추론 영역에서 Mind's Eye<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib98" title="">2022</a>)</cite>는 합성 “text-description<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px3.p1.1.m1.1a"><mo id="S2.SS1.SSS0.Px3.p1.1.m1.1.1" stretchy="false" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px3.p1.1.m1.1d">→</annotation></semantics></math> rendering code” 데이터로 텍스트-코드 모델을 학습시켜 새로운 접근 방식을 취한다. 이를 통해 모델은 텍스트 질문을 렌더링 코드로 변환할 수 있으며, 렌더링 코드는 물리적 엔진(즉, DeepMind MuJoCo <cite class="ltx_cite ltx_citemacro_citep">(Todorov et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib152" title="">2012</a>)</cite>)에서 실행된다. 렌더링 결과는 문맥에 주입되어 마인드스 아이로 무장한 작은 언어 모델도 100배 큰 모델에 버금가는 성능을 낼 수 있다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tool-using and Planning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Learning tool-using through synthetic trajectories.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">합성 데이터는 또한 실제 인간 도구 사용 데이터를 수집하는 데 시간이 많이 걸릴 수 있고 도구에 대한 호출의 실제 분포가 왜곡될 수 있기 때문에 LM이 시뮬레이션된 궤적을 통해 도구 사용 능력을 학습할 수 있도록 하는 강력한 접근법이다. 예를 들어, LaMDA<cite class="ltx_cite ltx_citemacro_citep">(Thoppilan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib150" title="">2022</a>)</cite>는 웹 문서뿐만 아니라 크라우드 워커와 모델 자체 간의 상호 작용 데이터에 대해 학습되었으며 합성 데이터는 적절한 도구에 대한 호출로 주석이 달렸다. 이 훈련 과정을 통해 LaMDA는 산술을 위한 계산기, 실시간 정보 탐색을 위한 검색 엔진, 번역을 위한 기계 번역기를 사용할 수 있는 능력을 개발할 수 있었다. 마찬가지로 Toolformer<cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib135" title="">2024</a>)</cite>는 템플릿 생성 데이터에 대한 학습을 통해 호출할 API와 전달 인수를 결정하는 반면 Galactica<cite class="ltx_cite ltx_citemacro_citep">(Taylor et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib149" title="">2022</a>)</cite>는 API 호출 데이터를 사전 학습 혼합물에 주입합니다. ToolAlpaca<cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib147" title="">2023</a>)</cite>는 다중 에이전트 시뮬레이션 환경을 구축하고 에이전트가 반복적으로 도구를 선택하고 사용하도록 함으로써 다양한 도구 사용 코퍼스를 자동으로 생성하도록 설계된 새로운 프레임워크이다. 이러한 예는 LM이 도구 사용 능력을 획득하고 다양한 도메인에 걸쳐 추론 능력을 향상시킬 수 있도록 하는 합성 궤적의 잠재력을 보여준다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Learning to plan in synthetic environments.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">자율 머신 인텔리전스<cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib79" title="">2022</a>)</cite>에서 에이전트의 중요한 기능은 계획입니다. 복잡한 작업을 하위 작업으로 분해하고 보상-최적의 방법으로 하위 작업을 완료하는 능력<cite class="ltx_cite ltx_citemacro_citep">(Kambhampati et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib73" title="">2024</a>)</cite>입니다. 합성 데이터는 시뮬레이터 <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib119" title="">2023</a>)</cite>에서 수집된 피드백 신호 역할을 할 수 있으므로 여기에서 유용한 도구가 될 수 있으며, 이에 대한 학습은 에이전트가 어포던스 <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib3" title="">2022</a>; Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib88" title="">2022</a>)</cite>를 인식하게 할 수 있다. 예를 들어, Inner Monologue <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib66" title="">2022</a>)</cite>는 시뮬레이션 환경에서 생성된 자연어 형태 피드백을 활용하여 LLM 기반 로봇 계획을 가르친다. 그들은 그러한 피드백이 시뮬레이션된 영역과 실제 영역 모두에서 높은 수준의 명령 완성을 상당히 향상시킨다는 것을 발견했다. 많은 사실적인 계획 작업(예: <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.1">"Rearrange objects on a table to match a given scene."</span>), VIMA <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib71" title="">2022</a>)</cite>는 VIMA-Bench라는 멀티 모달리티 시뮬레이트 환경을 만들어 객체 및 텍스처의 확장 가능한 컬렉션을 지원합니다. 마인크래프트 게임에서 보이저<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib158" title="">2023</a>)</cite>는 다수의 GPT-4 기반 에이전트를 배치하여 합성 환경과 상호작용하고, 합성 피드백의 도움으로 에이전트가 새로운 기술을 더 빨리 풀고 계획을 더 효율적으로 완료할 수 있음을 발견한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multimodality</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Reverse rendering from vision to text.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.1">비전 언어 정렬 데이터는 시각 입력을 LLM(일반적으로 비전 인코더를 통해)에 정확하게 접지하는 데 중점을 둔다. 웹 스크래핑 이미지-캡션 쌍은 CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib126" title="">2021</a>)</cite> 및 ALIGN <cite class="ltx_cite ltx_citemacro_citep">(Jia et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib69" title="">2021</a>)</cite> 이후 지난 몇 년 동안 가장 인기 있는 MM 정렬 데이터였다. 그러나 웹 스크래핑된 이미지-텍스트 쌍은 일반적으로 잡음이 많고 거친 입도의 대응만을 가지고 있어 언어에서 이미지의 세부 사항을 접지하기에는 불충분하다. 문서, 화면, 도형 및 다이어그램과 같은 도메인에서 이러한 세밀한 정렬은 이미지 렌더링 엔진으로 구축된 데이터 합성 파이프라인에서 가장 편리하게 얻을 수 있다. Pix2Struct <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib80" title="">2023</a>)</cite>는 웹 서버를 사용하여 HTML 코드를 웹 사이트 스크린샷으로 렌더링하고, 트레이닝 작업은 마스킹된 스크린샷을 전체 HTML 코드로 디렌더링하는 것이다. MatCha <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib94" title="">2023b</a>)</cite>와 DePlot <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib93" title="">2023a</a>)</cite>는 표 형태의 데이터를 Python plotting 라이브러리를 이용하여 차트로 렌더링하고, 렌더링된 이미지를 주어 코드 및/또는 표 형태의 데이터를 생성함으로써 기초 모델을 사전 학습한다. <cite class="ltx_cite ltx_citemacro_citet">Si et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib143" title="">2024</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet">Laurençon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib77" title="">2024</a>)</cite>는 웹 페이지 스크린샷을 코드 구현으로 변환하는 작업을 위해 합성적으로 생성된 HTML 및 CSS 파일을 학습한다. 합성 데이터에 대해 미세 조정된 모델은 인터넷에서 긁어낸 사실적인 데이터에 대해 합리적으로 잘 일반화할 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib17" title="">2021</a>)</cite>는 컴퓨터 비전 연구를 돕기 위해 합성 데이터 생성기로 물리 엔진 또는 게임 엔진(예: Unity)을 사용할 것을 제안한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multi-modality instruction following.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.1">멀티모달 LLM의 다운스트림 애플리케이션은 추론 및 명령어 추종 기능을 필요로 한다. 이러한 데이터는 일반적으로 긴 형태의 질문 응답 쌍이며 인간이 생성하기에는 비용이 많이 든다. LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib96" title="">2024b</a>)</cite>는 기존의 이미지 캡션을 사용하여 다양하고 긴 형식의 프롬프트-응답 쌍을 쓰기 위해 GPT-4(텍스트 전용 모드)를 프롬프트한다. 멀티모달 LLM 트레이닝 동안, 캡션들 및 바운딩 박스 정보가 숨겨질 수 있는 동안 이미지들 및 프롬프트들이 입력으로서 사용된다. 이미지 캡션 외에도 객체 경계 상자 <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib184" title="">2023</a>)</cite>, OCR <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib183" title="">2023d</a>)</cite> 및 디렌더링 차트 <cite class="ltx_cite ltx_citemacro_citep">(Masry et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib106" title="">2023</a>; Carbune et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib21" title="">2024</a>)</cite>와 같은 이미지 속성 정보의 다른 소스들은 이미지 속성 + 텍스트 LLM 합성 데이터 파이프라인을 다시 쓰는 것과 같이 모두 들어갈 수 있다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Multilingual</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Back-translation augmentation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">많은 다국어 언어 모델은 데이터 증강 방법으로 역 번역을 사용하며, 단일 언어 데이터 소스 <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib136" title="">2016</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib188" title="">2020</a>; Caswell et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib24" title="">2019</a>; Marie et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib105" title="">2020</a>; Bi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib14" title="">2021</a>; Liao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib89" title="">2021</a>; Pham et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib124" title="">2021</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib171" title="">2022</a>)</cite>로부터 합성 병렬 학습 데이터를 생성한다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib136" title="">2016</a>)</cite> 단일 언어 타겟 데이터를 소스 언어 데이터로 역번역하여, 실질적인 번역 작업 개선을 위한 추가적인 병렬 트레이닝 샘플을 제공한다. 연구자들은 또한 역번역(예: 빔 검색, 제약 샘플링, 비제약 샘플링)을 위한 다양한 샘플링 방법과 비교 효과 [cite idx=2></cite>]를 탐구했다. <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib171" title="">2022</a>)</cite>는 역변환을 이용한 최적의 NMT 성능을 위해 합성 데이터의 가중치와 품질의 중요성을 강조한다. 그들은 추정된 중요도 가중치와 품질의 균형을 맞추기 위해 검색 방법과 감마 점수 사이의 비율을 최적화하는 방법을 제안한다. 그러나 역번역 기반 합성 데이터 생성에는 몇 가지 제한 사항이 있다. 예를 들어, 합성 데이터의 품질과 다양성은 역번역 방법의 성능에 따라 달라진다. 합성 데이터가 너무 잡음이 있거나 다양하지 않으면 성능 이득이 제한됩니다. <cite class="ltx_cite ltx_citemacro_citep">(Epaliyana et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib38" title="">2021</a>; Chauhan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib25" title="">2022</a>)</cite></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Generating multilingual questions and answers at scale.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">최근 연구는 다국어 및 교차언어 질의응답 <cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib7" title="">2021</a>; Kumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib74" title="">2019</a>; Chi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib29" title="">2020</a>; Riabi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib130" title="">2021</a>; Li and Callison-Burch, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib84" title="">2023</a>; Abulkhanov et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib2" title="">2023</a>)</cite>에서 언어 모델의 성능을 향상시키기 위해 합성 다국어 질의응답(QA) 쌍의 생성 및 활용을 탐구한다. 한 가지 접근법은 기존의 단일 언어 질문 및/또는 답변을 다른 언어로 번역하는 것이다. <cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib7" title="">2021</a>)</cite>. 다른 하나는 질문 생성(QG) 모델을 사용하여 답변 및/또는 소스 텍스트 <cite class="ltx_cite ltx_citemacro_citep">(Kumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib74" title="">2019</a>; Chi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib29" title="">2020</a>; Riabi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib130" title="">2021</a>)</cite>에 기초하여 교차 언어 방식으로 합성 질문을 생성하는 것을 포함한다. 최근의 노력은 또한 더 큰 유연성을 위해 다중 언어로 질문과 답변을 공동으로 생성하는 것에 초점을 맞추고 있다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">Shakeri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib137" title="">2021</a>)</cite>는 QA 생성 태스크와 다국어 마스킹 언어 모델링 태스크의 혼합물에서 사전 훈련된 다국어 T5 모델 <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib172" title="">2020</a>)</cite>를 미세 조정하여 다국어로 합성 QA 쌍을 생성한다. 이러한 노력은 일반적으로 합성 QA 쌍에 대해 훈련된 언어 모델이 다국어 QA 및 정보 검색 벤치마크에서 향상된 성능을 입증한다는 것을 보여준다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Alignment</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Instruction Following.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS5.SSS0.Px1.p1.1">합성 데이터는 특히 실제 데이터가 부족하거나 비싸거나 얻기 어려운 시나리오에서 훈련 명령 후속 모델을 위한 유망한 접근법 역할을 할 수 있다. Self-instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib159" title="">2022a</a>)</cite>와 Stanford Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib148" title="">2023</a>)</cite>는 모두 LLMs를 사용하여 광범위한 시나리오를 다루는 데이터에 따라 명령을 생성합니다. 그들은 먼저 "샘플에 따른 시드 지침"의 작은 세트를 선택한 다음 LLM에게 더 많은 데모를 생성하기 위해 형식을 모방하도록 요청한다. 이러한 유형의 방법의 한 가지 관심사는 생성된 데이터를 고품질로 유지하는 방법이다. 이는 쿼리의 복잡성 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib100" title="">2023d</a>)</cite>, 시맨틱의 다양성 <cite class="ltx_cite ltx_citemacro_citep">(Ding et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib35" title="">2023</a>)</cite>, 합성 데이터 세트의 규모 <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib178" title="">2023</a>)</cite>를 포함한다. 이를 위해 <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib170" title="">2023</a>)</cite>는 프롬프트를 통해 간단한 명령어에 복잡성을 추가하는 Evol-Instruct를 제안한다. <cite class="ltx_cite ltx_citemacro_citet">Mukherjee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib112" title="">2023</a>)</cite> LLM을 활용하여 FLAN 데이터셋에 고품질 설명 트레이스를 포함하도록 명령어와 응답을 반복적으로 수정하고 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib163" title="">2022</a>)</cite> 학습한 모델이 많은 NLP 작업에서 성능이 향상되었음을 발견했다. UltraChat<cite class="ltx_cite ltx_citemacro_citep">(Ding et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib35" title="">2023</a>)</cite>는 두 개의 분리된 ChatGPT Turbo API 모델에 의해 생성되는 대규모 및 다라운드 합성 대화 데이터 세트이며, 하나는 사용자 역할을 하고 다른 하나는 보조 역할을 한다. 그들은 실제 인간 사용자 행동을 모방하도록 주의 깊게 설계된 프롬프트로 사용자 모델을 지시한다.</p>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS5.SSS0.Px1.p2.1">많은 언어 모델들이 명령들을 따르는 방법을 배우기 위해 감독된 미세조정되지만, 이러한 행동을 학습함에 있어서, 그들은 또한 부주의하게 <span class="ltx_text ltx_font_italic" id="S2.SS5.SSS0.Px1.p2.1.1">sycophantic</span> <cite class="ltx_cite ltx_citemacro_citep">(Perez et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib123" title="">2023</a>)</cite>가 되도록 학습하고, 그 관점이 객관적으로 정확하지 않더라도, 그들의 응답을 사용자의 관점을 따르도록 재단한다<cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib165" title="">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_citet">Sharma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib139" title="">2024</a>)</cite>는 선호 모델(즉, RLHF 훈련에 사용되는 보상 모델)과 심지어 인간도 때때로 아첨 반응을 선호한다는 증거를 찾는다. 이 전면에서 <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib165" title="">2023b</a>)</cite>는 모델이 사용자 의견에 강하도록 장려하기 위해 합성 데이터를 생성하고 유지된 프롬프트에 대한 아첨 행동을 줄이기 위해 미세 조정 단계에서 이러한 데이터를 추가한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Mitigating hallucination.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS5.SSS0.Px2.p1.1">널리 사용되는 많은 언어 모델은 SFT(supervised finetuning)를 사용하여 사용자와의 상호 작용을 정렬하는 방법을 학습한다<cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib160" title="">2022b</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib181" title="">2023b</a>)</cite>. 특히, 추론 및 정렬 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib164" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib165" title="">b</a>)</cite>와 같은 능력을 향상시킬 수 있는 합성 SFT 데이터를 생성하는 방법이 많이 존재한다. 그러나 이러한 합성 데이터는 사소하지 않은 양의 환각 답변을 포함하거나 모델이 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>에 대한 답변을 알지 못하는 질문에 답하도록 학습하도록 강요함으로써 언어 모델로 환각을 유도할 수 있음이 나타났다. 이러한 사례는 합성 데이터가 올바르게 적용되지 않더라도 실제로 언어 모델에서 환각을 증가시킬 수 있음을 보여준다.</p>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS5.SSS0.Px2.p2.1">한편, 최근의 연구는 합성 데이터를 사용하여 환각을 완화시키는 유망한 결과를 보여주기도 했다. 예를 들어, GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib115" title="">2023</a>)</cite>는 강화 학습 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>를 수행하기 위해 합성 환각 데이터를 레버리지한 보상 모델을 이용하여 학습하였다. 이 방법은 TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib91" title="">2022</a>)</cite> dataset <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>에서 상당한 성능 향상을 가져왔다. 마찬가지로 <cite class="ltx_cite ltx_citemacro_citet">Jones et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib72" title="">2023</a>)</cite>는 환각을 쉽게 평가할 수 있는 합성 작업을 설계했으며 이 작업을 활용하여 접두사 조정을 통해 연속 후접사를 학습하여 LLM 출력을 최적화했다. <cite class="ltx_cite ltx_citemacro_citet">Tian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib151" title="">2023</a>)</cite>는 자동화된 사실 확인 및 신뢰 점수를 사용하여 모델 응답 쌍의 사실성 점수를 순위를 매긴 다음 DPO<cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib128" title="">2023</a>)</cite>로 언어 모델을 미세 조정하여 사실성을 향상시킨다. 환각을 완화하기 위해 합성 데이터를 사용하는 지속적인 연구는 여전히 제한적이지만 환각을 확장적으로 평가할 수 있는 합성 작업의 부족으로 인해 제한적이다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS5.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Aligning with shared human preference and values.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS5.SSS0.Px3.p1.1">값 정렬 또는 인간 선호 데이터에 대한 직접 미세 조정은 언어 모델을 정렬하기 위한 간단한 방법이지만 이 방법은 종종 상당한 인간 주석을 필요로 하며, 이는 대규모로 엄청나게 비쌀 수 있다. 또한, 이러한 주석은 특히 품질 스펙트럼 <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib109" title="">2023</a>; Gilardi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib51" title="">2023b</a>)</cite>의 하단에서 주석이 제대로 달리지 않은 샘플의 경우 다양한 스타일과 일관되지 않은 품질을 자주 나타낸다. 이러한 실제적인 과제를 해결하기 위해, "RLHF(reinforcement learning from human feedback)"로 알려진 진보된 기술이 <cite class="ltx_cite ltx_citemacro_citep">(Leike et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib81" title="">2018</a>; Christiano et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib30" title="">2017</a>; Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib117" title="">2022</a>)</cite>로 제안되었다. 이 접근법은 LM 생성 정책의 최적화를 안내하는 인간 판단의 대리인 역할을 하도록 인간 데이터로 보상 모델을 훈련시키는 것을 포함한다.</p>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS5.SSS0.Px3.p2.1">최근 연구에서는 보다 강력한 보상 모델<cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib46" title="">2023</a>)</cite>를 학습하기 위해 합성 데이터와 실제 인간 데이터를 혼합한 모델을 제안하였다. 헌법 AI<cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib12" title="">2022</a>)</cite>는 AI가 생성한 비판과 피드백을 조종하기 위해 작은 원칙 세트를 사용하고 일반적인 RLHF 파이프라인에서 실제 인간 데이터를 대체하기 위해 이러한 합성 데이터를 사용할 것을 제안한다. 이 RLAIF(즉, AI 피드백으로부터의 강화 학습) 방법으로 훈련된 모델은 RLHF 기준선과 유사한 강한 성능을 보인다. 일반적으로 합성 데이터는 연구자가 저비용으로 대규모, 다양하고 통제된 훈련 데이터 세트를 생성할 수 있도록 함으로써 인간의 가치와 선호도 정렬에 대한 강력한 솔루션을 제공한다. 윤리적 딜레마 <cite class="ltx_cite ltx_citemacro_citep">(Perez et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib122" title="">2022</a>)</cite>, 사회적 상호작용 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib99" title="">2023c</a>)</cite>, 문화적 규범 <cite class="ltx_cite ltx_citemacro_citep">(Ziems et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib190" title="">2023</a>)</cite>, 합성 데이터는 AI 모델의 인간 가치와의 정렬에 대한 포괄적이고 체계적인 테스트를 가능하게 한다. 이 접근법은 AI 시스템이 실제 설정 [cite idx=9></cite>]에 배포되기 전에 bias <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib97" title="">2021</a>; Ntoutsi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib114" title="">2020</a>)</cite>, fairness <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib185" title="">2018</a>; Landers and Behrend, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib76" title="">2023</a>)</cite> 및 의도하지 않은 결과와 관련된 문제를 식별하고 완화하는 데 도움이 됩니다.</p>
</div>
<div class="ltx_para" id="S2.SS5.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS5.SSS0.Px3.p3.1">그러나 저충실도 합성 인간 선호도 데이터는 미묘한 인간 판단 <cite class="ltx_cite ltx_citemacro_citep">(Argyle et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib6" title="">2023</a>)</cite>를 정확하게 반영하는 데 한계가 있을 수 있음을 인정하는 것이 중요하다. 결과적으로 결과 모델은 "감옥 파괴 공격" <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib64" title="">2023a</a>; Deshpande et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib33" title="">2023</a>)</cite>에서 덜 강력할 수 있으며 안전 훈련 <cite class="ltx_cite ltx_citemacro_citep">(Pan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib118" title="">2022</a>; Steinhardt, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib145" title="">2022</a>; Everitt et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib39" title="">2021</a>)</cite>를 통해서도 전략적으로 기만적인 행동을 드러낼 수 있다. 이러한 위험을 완화하기 위해 연구자들은 인간의 가치와 선호도의 복잡성을 더 잘 포착하는 보다 복잡하고 포괄적인 시나리오를 통합하여 합성 데이터의 품질과 다양성을 지속적으로 개선하고 개선해야 한다. 또한, 합성 데이터를 실제 데이터와 결합하고, 실제와 동기화할 수 있는 대화형 환경에서 합성 데이터를 생성하는 것은 유망한 해결책이다. 효과적인 AI 거버넌스 및 규제의 필요성이 증가함에 따라 합성 데이터는 신뢰, 책임 및 인간의 가치 및 사회적 기대와 일치하는 AI 기술의 개발을 촉진하는 확장 가능한 감독 메커니즘을 가능하게 하는 데 점점 더 중요한 역할을 할 것이다.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data in Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">합성 데이터는 다양한 관점의 평가에 널리 사용된다:</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Factuality.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">AI 시스템은 사실적인 지식 또는 데이터에 근거하지 않는 정보 또는 응답을 생성하여, 형식적으로 <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.1.1">hallucination</span> <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>)</cite>로 알려진 오도 또는 허위 콘텐츠의 생성으로 이어질 수 있다. 사실성 평가는 AI 시스템의 학습 데이터 및 지식 베이스 <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib182" title="">2023c</a>)</cite>에서 제공하는 지식과 AI 시스템의 출력에 있는 지식의 일관성을 보장하는 것을 목표로 한다. 초기 통계 기반 환각 평가 방법은 n-gram에 의존하여 입력과 출력 내용의 어휘 중복을 직접 계산했다  <cite class="ltx_cite ltx_citemacro_citep">(Dhingra et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib34" title="">2019</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib161" title="">2020</a>)</cite>. 그러나 이러한 방법들은 어휘 중복만을 고려하고 의미나 문장 의미 <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>)</cite>를 고려하지 않아 보다 복잡한 형태의 환각을 평가하기에 부적합하다는 한계가 있다. 후속 보증 방법은 통계적 접근법에서 모델 기반 방법으로 이동했으며, 이는 토큰 차이 기반 방법 <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib62" title="">2021</a>)</cite>에 비해 더 강력하다. 이러한 모델 기반 평가 방법은 전작에 비해 진보된 것이지만 여전히 한계를 가지고 있다. 예를 들어, 모델들은 환각의 정도만 출력할 수 있고 특정 오류 <cite class="ltx_cite ltx_citemacro_citep">(Falke et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib40" title="">2019</a>)</cite>를 정확히 찾아내기 위해 고군분투할 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib41" title="">2023a</a>)</cite>는 LLMs 생성과 지식 그래프 상의 무작위 보행을 결합하여 그래프 상의 개체 및 관계를 인식하는 사실성에 대한 합성 평가 데이터를 생성하는 것을 제안한다. <cite class="ltx_cite ltx_citemacro_citet">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib166" title="">2024</a>)</cite>는 Long-form 사실성 평가를 위해 LongFact라는 합성 데이터 세트를 만들고 Google Search를 접지원으로 사용하고 자동화된 판단을 위해 LLM을 사용하여 인간 수준의 정확도를 달성했지만 비용이 상당히 낮은 <cite class="ltx_cite ltx_citemacro_citep">(Min et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib110" title="">2023</a>)</cite>를 사용했다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Safety.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">레드 러닝은 AI 모델 <cite class="ltx_cite ltx_citemacro_citep">(Ganguli et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib44" title="">2022</a>; Casper et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib23" title="">2023b</a>)</cite>의 안전성과 견고성을 평가하는 강력한 기술이다. 정렬되지 않거나 유해한 출력 <cite class="ltx_cite ltx_citemacro_citep">(Casper et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib22" title="">2023a</a>)</cite>를 도출하도록 설계된 다양하고 현실적인 시나리오를 생성함으로써, 레드 학습은 AI 시스템 <cite class="ltx_cite ltx_citemacro_citep">(Perez et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib122" title="">2022</a>)</cite>의 취약점과 취약점을 노출시킬 수 있다. 예를 들어, <cite class="ltx_cite ltx_citemacro_citet">Perez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib123" title="">2023</a>)</cite>는 LMs를 사용하여 다른 LMs의 동작을 평가하기 위한 데이터셋을 생성한다. 그들은 결국 154개의 고품질 데이터 세트를 생성하며, 크기가 커질수록 LMs가 악화되는 역 스케일링의 새로운 사례를 발견한다. <cite class="ltx_cite ltx_citemacro_citet">Hubinger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib67" title="">2024</a>)</cite> 합성 데이터를 활용하여 대규모 LMs에 대한 백도어 공격을 트리거하며, LMs는 그러한 공격 하에서 기만적인 행동을 나타낼 수 있고 안전의 잘못된 인상을 생성할 수 있음을 발견하며, 표준 "안전 훈련"은 그러한 기만을 쉽게 제거할 수 없었다. 이러한 방법은 AI 지원을 사용하여 복잡한 문제와 보이지 않는 도메인에 대한 인간 감독 <cite class="ltx_cite ltx_citemacro_citep">(Bowman et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib18" title="">2022</a>)</cite>를 확장하는 가능성을 보여준다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Assisting human evaluation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">최근 연구에 따르면 대규모 LMs(LLM)의 합성 판단은 많은 경우에 실제 인간 평가에 대한 적격, 신속 및 저비용 대안으로 작용할 수 있다. GPT-4를 심사위원으로 사용하여 Alpaca Eval <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib87" title="">2023b</a>)</cite>와 MT Bench <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib186" title="">2023</a>)</cite>는 LM 기반 ChatBot의 포괄적인 능력을 측정하는 두 가지 인기 있는 벤치마크이다. 코딩 작업에서 합성 환경은 인간이 실행 로그에 대한 실제 실행 및 분석을 통해 평가를 보다 효율적으로 수행할 수 있기 때문에 인간 평가를 돕기 위한 일반적인 선택이다. <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib55" title="">2024</a>)</cite>는 CodeLLaMA-34B에서 생성한 800개의 Python 함수로 구성된 코드 실행 추론 벤치마크인 CRUXEval을 제안한다. 마찬가지로 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib92" title="">2024a</a>)</cite>는 IER(Independent Execution Reasoning),DER(Dependent Execution Reasoning), SR(Specification Reasoning)에서 LLM의 코드 추론 능력을 측정하는 프레임워크인 CodeMind를 소개한다. 합성 데이터를 기반으로 한 이러한 모든 평가는 실제 인간의 판단과 강한 상관 관계를 보여준다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenges and Limitations of Synthetic Data</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">합성 데이터는 많은 혜택과 응용 프로그램을 제공하지만 사용과 관련된 잠재적인 도전과 한계를 인정하고 해결해야 한다. 이 섹션에서는 합성 데이터를 둘러싼 세 가지 주요 관심사를 조사한다.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Misuse of synthetic data might proliferate misinformation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">합성 데이터의 잠재적인 오용은 AI 시스템의 책임 있는 개발을 보장하기 위해 해결해야 하는 중요한 문제이다. 현재 AI 모델은 텍스트 <cite class="ltx_cite ltx_citemacro_citep">(Gemini-Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib48" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib47" title="">2023</a>)</cite>, 이미지 <cite class="ltx_cite ltx_citemacro_citep">(Saharia et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib133" title="">2022b</a>; Ramesh et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib129" title="">2022</a>)</cite>, 노래 <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Make songs with Suno AI: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://app.suno.ai/" title="">https://app.suno.ai/</a></span></span></span>, 심지어 비디오(예: OpenAI SORA <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>OpenAI Sora: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/research/video-generation-models-as-world-simulators" title="">https://openai.com/research/video-generation-models-as-world-simulators</a></span></span></span>)에 이르기까지 인간과 유사한 데이터를 생성할 수 있게 되었다. 이는 합성 데이터가 실제 사람을 사칭하거나 여론을 조작하거나 정치적 과정에 영향을 미치는 데 사용될 때 특히 위험할 수 있다. 더욱이, 합성 데이터 기반 잘못된 정보의 보급은 합법적인 정보 소스에 대한 신뢰를 약화시켜 사람들이 진실과 거짓을 구별하는 것을 점점 더 어렵게 만들 수 있다. 이러한 위험을 완화하기 위해서는 연구자, 개발자 및 정책 입안자가 합성 잘못된 정보를 감지하고 대응하기 위한 강력한 메커니즘을 포함하여 합성 데이터의 윤리적 생성 및 사용을 위한 명확한 지침과 모범 사례를 확립하는 것이 중요하다. 이러한 문제를 사전에 해결함으로써 합성 데이터의 이점을 활용하면서 손상 가능성을 최소화할 수 있습니다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data might cause ambiguity in AI alignment.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">AI 모델을 정렬하는 데 합성 데이터의 사용 증가(예: Constitutional AI<cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib12" title="">2022</a>)</cite>)는 상당한 모호성과 불확실성을 도입할 수 있다. AI 정렬의 목표는 AI 시스템이 인간의 가치와 의도와 일치하는 방식으로 행동하도록 하는 것이다. 그러나 실제 소스에서 수집되지 않고 인위적으로 생성된 합성 데이터는 인간 가치와 선호도의 뉘앙스와 복잡성을 정확하게 나타내지 못할 수 있다. 이러한 불일치는 편향된 데이터 <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib42" title="">2023b</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib97" title="">2021</a>)</cite>, 그라운드되지 않은 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib98" title="">2022</a>; Patel and Pavlick, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib121" title="">2022</a>)</cite>, 또는 실제 시나리오 <cite class="ltx_cite ltx_citemacro_citep">(Weidinger et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib168" title="">2021</a>; Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib68" title="">2023</a>)</cite>에서 AI 모델을 학습하게 할 수 있다. 결과적으로, 합성 데이터에 대해 훈련된 AI 시스템은 인간의 기대와 잘못 정렬된 행동을 나타낼 수 있으며, 잠재적으로 의도하지 않은 결과 또는 심지어 유해한 행동 <cite class="ltx_cite ltx_citemacro_citep">(Zou et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib191" title="">2023</a>; Anderljung et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib5" title="">2023</a>)</cite>로 이어질 수 있다. 더욱이 합성 데이터에 의해 도입되는 모호성은 AI 모델 <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib90" title="">2023</a>)</cite>의 의사 결정 과정을 해석하고 이해하기 어렵게 만들어 정렬을 보장하는 작업을 더욱 복잡하게 만들 수 있다. 이러한 위험을 완화하기 위해 연구자들은 정렬 연구에서 합성 데이터를 사용하는 것의 한계와 잠재적인 단점을 주의 깊게 고려하고 그러한 데이터에 대해 훈련된 AI 모델의 검증 및 테스트를 위한 강력한 방법을 개발하는 것이 중요하다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Training with synthetic data makes evaluation decontamination harder.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.2">모델 훈련에서 합성 데이터를 사용하는 것은 공정한 평가에 상당한 도전을 제기한다. 평가 벤치마크는 종종 코스워크 웹사이트나 포럼과 같은 공개 텍스트 소스를 참조하여 만들어진다. 결과적으로 공개적으로 사용 가능한 모든 벤치마크 테스트 케이스가 LLMs<cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib61" title="">2022</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib45" title="">2021</a>)</cite>의 사전 훈련 데이터에 때때로 포함될 수 있다는 것은 논쟁의 여지가 있다. 합성 데이터의 사용은 이 문제를 완화하기보다는 악화시킨다. 커뮤니티가 <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">min-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.1.1.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1d">italic_k</annotation></semantics></math>% prob</span></cite idx=1></cite> <math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.2.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.2.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.2.m1.1d">italic_k</annotation></semantics></math> long-tail 토큰의 확률을 확인하는 <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">min-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.1.1.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1d">italic_k</annotation></semantics></math>% prob</span></cite> <math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.2.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.2.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.2.m1.1d">italic_k</annotation></semantics></math> long-tail 토큰의 확률을 확인하는 <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px3.p1.1.1">min-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.1.1.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.1.1.m1.1d">italic_k</annotation></semantics></math>% prob</span></cite> <math alttext="k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.2.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.2.m1.1a"><mi id="S4.SS0.SSS0.Px3.p1.2.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m1.1b"><ci id="S4.SS0.SSS0.Px3.p1.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.2.m1.1d">italic_k</annotation></semantics></math> long-tail 토큰의 확률을 확인하는 <span 합성 데이터에는 벤치마크 데이터 <cite class="ltx_cite ltx_citemacro_citep">(Oren et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib116" title="">2023</a>; Mattern et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib107" title="">2023</a>)</cite>의 수정 버전, 토큰 수준의 오염 제거 효과가 없는 렌더링이 포함될 수 있습니다. 보다 발전된 평가 오염 감지 기술을 개발할 뿐만 아니라 모델 개발자가 자체 및 보호된 평가 벤치마크를 만들고 유지하는 데 투자할 것을 권장한다. 이러한 독점 벤치마크는 누출을 방지하고 평가 프로세스의 무결성을 보장하기 위해 신중하게 보호되어야 한다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Directions for Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">합성 데이터 분야가 계속 발전함에 따라 향후 연구 개발을 위한 몇 가지 유망한 방향이 있다. 이 섹션에서는 추가 탐사가 필요한 세 가지 핵심 영역을 간략하게 설명한다.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data scaling.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">과잉 훈련된 많은 작은 언어 모델들(예를 들어, Mistral 시리즈 모델들 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib70" title="">2023</a>)</cite>, Gemma 시리즈 모델들 <cite class="ltx_cite ltx_citemacro_citep">(Gemma-Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib49" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.1.1">inter alia</span>)의 인상적인 성능은 많은 양의 토큰들(심지어 compute-optimal chinchilla law <cite class="ltx_cite ltx_citemacro_citep">(Rae et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib127" title="">2021</a>)</cite>를 통과하더라도)을 가진 훈련의 필요성을 보여준다. 그러나 합성 데이터의 품질이 실제 데이터 <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib176" title="">2024</a>)</cite>만큼 일관되지 않을 수 있기 때문에 합성 데이터를 사용한 훈련에 대해 유사한 결론을 내렸는지는 여전히 미해결 문제이다. 향후 연구에서는 합성 데이터에 대한 스케일링 법칙을 조사하고 합성 샘플의 양과 품질 사이의 최적 균형을 결정해야 한다. 이 탐색은 대규모 언어 모델을 훈련하는 데 합성 데이터를 활용하는 가장 효과적인 전략을 이해하는 데 도움이 될 수 있으며 잠재적으로 더 효율적이고 비용 효율적인 접근법<cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib111" title="">2024</a>)</cite>로 이어질 수 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Further improving quality and diversity of synthetic data.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">합성 데이터를 생성하는 기존 방법이 가능성을 보여주었지만 실제 데이터를 밀접하게 모방하는 고품질 귀속 합성 샘플을 만드는 측면에서 여전히 개선의 여지가 있다. 향후 연구는 생성된 데이터의 특정 속성을 제어하고 조작할 수 있는 새로운 고급 기술(또는 GAN(Generative Adversarial Networks) <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib52" title="">2020</a>)</cite> 또는 Diffusion Models <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib60" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">inter alia</span>과 같은 기존 기술을 기반으로 하여 다양하고 사용자 지정 가능한 합성 데이터 세트를 생성할 수 있다. 또한, 연구자들은 생성된 데이터가 데이터 품질을 유지하면서(예를 들어, 검색 증강 생성(RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib82" title="">2020</a>; Borgeaud et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib15" title="">2022</a>)</cite>를 통해) 목표 도메인에 존재하는 기본 제약 및 패턴을 준수하도록 도메인별 지식을 통합할 수 있는 방법을 탐색해야 한다. 귀속된 합성 데이터 생성에서 최첨단 기술을 발전시킴으로써, 우리는 의료(예: 합성 의료 이미지<cite class="ltx_cite ltx_citemacro_citep">(Frid-Adar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib43" title="">2018</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib162" title="">2019</a>)</cite>)와 금융(예: 시뮬레이션된 거래 궤적<cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib187" title="">2022</a>)</cite>)에서 사회과학<cite class="ltx_cite ltx_citemacro_citep">(Argyle et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib6" title="">2023</a>; Park et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib119" title="">2023</a>)</cite> 및 그 이상)에 이르기까지 다양한 분야에 걸쳐 개인 정보 보호 분석 및 모델 훈련을 위한 새로운 기회를 열 수 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Towards high-fidelity and more efficient scalable oversight.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">AI 모델이 점점 더 복잡하고 자율적이 됨에 따라 인간의 감독 또는 실제 데이터에 의존하는 전통적인 감독 방법을 사용하여 그들의 행동을 모니터링하고 평가하는 것이 어려워진다. 향후 연구에서는 이러한 고급 시스템의 고충실도 확장 가능한 감독을 위해 합성 데이터의 사용을 탐구해야 한다. 기존의 방법은 일반적으로 합성 데이터를 얻기 위해 토론 <cite class="ltx_cite ltx_citemacro_citep">(Leike et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib81" title="">2018</a>)</cite>, 반사 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib180" title="">2023a</a>)</cite>, 또는 수정 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib99" title="">2023c</a>)</cite>와 같은 사회적 반복에서 특정 시나리오를 시뮬레이션하는 반면, 새로운 접근법은 더 포괄적인 시나리오와 더 많은 양식 <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib146" title="">2023</a>)</cite>를 다룰 수 있는 반면, 최근 연구에서 좁혀진 <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib27" title="">2023</a>)</cite> 또는 과도하게 단순화된 <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib189" title="">2024</a>)</cite> 장면만을 다루는 시뮬레이션의 많은 문제를 발견했기 때문이다. 앞으로 또 다른 성장 방향은 확장 가능한 감독을 보다 효율적으로 달성하는 방법일 수 있으며, 합성 데이터 생성을 완전히 제어할 수 있다는 점을 감안할 때 합성 데이터를 적게 사용하여 더 많은 목표 과시를 제공할 수 있다. 효과적인 AI 거버넌스 및 규제의 필요성이 증가함에 따라 합성 데이터는 사회의 이익을 위해 AI 기술의 강력하고 책임 있으며 안전한 배치를 촉진하는 보다 신뢰할 수 있는 확장 가능한 감독 메커니즘을 가능하게 하는 데 점점 더 중요한 역할을 할 것이다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">The emergent self-improvement capability.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">일반적으로 합성 데이터의 생성이 더 고품질이기 때문에 합성 데이터를 생성할 수 있는 가장 적합한 모델을 선택합니다. 그러나 흥미로운 질문이 발생한다: 모델은 훈련된 데이터보다 더 나은 합성 데이터를 생성할 수 있으며, 따라서 스스로 개선할 수 있는가? 합성 데이터 생성을 통한 이러한 자기계발 개념은 향후 연구를 위한 흥미진진한 길이다. 모델이 원래의 트레이닝 세트보다 더 높은 품질의 데이터를 생성할 수 있다면, 향상된 합성 데이터 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib26" title="">2024</a>)</cite>로부터 반복적으로 학습함으로써 잠재적으로 자신의 성능을 부트스트랩할 수 있다. 이러한 자기계발 능력은 시간이 지남에 따라 자신의 기술과 지식을 자율적으로 다듬을 수 있는 보다 발전된 AI 시스템의 출현으로 이어질 수 있다. 최근 연구에서 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib26" title="">2024</a>; Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib177" title="">2024</a>)</cite> 방향으로 고무적인 진전을 보이고 있지만, 자기 계발의 상한선과 그 효과에 대한 근본적인 이유는 여전히 미해결 문제로 남아 있다. 향후 연구에서는 보다 다양한 시나리오에서 합성 데이터 생성을 통한 자기 계발의 이론적 토대와 실제 실현 가능성을 조사하고 필요 조건, 잠재적 한계 및 관련 위험을 조사해야 한다. 창발적인 자기 개선 능력의 잠재력을 잠금 해제함으로써 우리는 더 적응적이고 효율적이며 자율적인 학습 프로세스 <cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a class="ltx_ref" href="https://arxiv.org/html/2404.07503v1#bib.bib79" title="">2022</a>)</cite>를 가능하게 할 수 있다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">합성 데이터는 AI 개발에서 데이터 부족, 개인 정보 보호 문제 및 높은 비용의 문제를 해결하기 위한 유망한 솔루션으로 부상했다. 현실적이고 다양한 데이터 세트를 생성함으로써 합성 데이터는 다양한 도메인에 걸쳐 규모 있는 AI 모델의 훈련 및 평가를 가능하게 한다. 우리가 인간 수준 또는 심지어 초인간 수준의 지능에 접근할 때, 모델이 진보하기 위해 평균보다 나은 인간 품질 데이터가 필요하다는 점을 감안할 때 합성 데이터를 얻는 것은 훨씬 더 중요해진다. 그러나 합성 데이터의 사실성, 충실도 및 편향성 부족을 보장하는 것은 여전히 중요한 과제로 남아 있다.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">합성 데이터에 대한 향후 연구 방향은 생성 모델의 충실도와 제어 가능성을 개선하고 표준화된 평가 및 오염 프로토콜 및 도구를 개발하는 데 중점을 둘 수 있다. 또한 합성 데이터와 다른 기술의 통합 및 다른 도메인에서의 적용을 탐구할 수 있다. 도전에도 불구하고 인공지능 연구를 발전시키는 데 있어 합성 데이터의 잠재적 이점은 중요하다. 합성 데이터를 책임감 있고 효과적으로 활용함으로써 사회 전반에 이익이 되는 보다 강력하고 포용적이며 신뢰할 수 있는 AI 시스템을 구축할 수 있습니다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abay et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;C. Abay, Y.&nbsp;Zhou, M.&nbsp;Kantarcioglu, B.&nbsp;Thuraisingham, and L.&nbsp;Sweeney.

</span>
<span class="ltx_bibblock">Privacy preserving synthetic data release using deep learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Machine Learning and Knowledge Discovery in Databases:
European Conference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018,
Proceedings, Part I 18</em>, pages 510–526. Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abulkhanov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Abulkhanov, N.&nbsp;Sorokin, S.&nbsp;Nikolenko, and V.&nbsp;Malykh.

</span>
<span class="ltx_bibblock">Lapca: Language-agnostic pretraining with cross-lingual alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 46th International ACM SIGIR Conference
on Research and Development in Information Retrieval</em>, pages 2098–2102,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Ahn, A.&nbsp;Brohan, N.&nbsp;Brown, Y.&nbsp;Chebotar, O.&nbsp;Cortes, B.&nbsp;David, C.&nbsp;Finn,
K.&nbsp;Gopalakrishnan, K.&nbsp;Hausman, A.&nbsp;Herzog, et&nbsp;al.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ArXiv preprint</em>, abs/2204.01691, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.01691" title="">https://arxiv.org/abs/2204.01691</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amodei et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Amodei, C.&nbsp;Olah, J.&nbsp;Steinhardt, P.&nbsp;Christiano, J.&nbsp;Schulman, and D.&nbsp;Mané.

</span>
<span class="ltx_bibblock">Concrete problems in ai safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv preprint</em>, abs/1606.06565, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1606.06565" title="">https://arxiv.org/abs/1606.06565</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderljung et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Anderljung, J.&nbsp;Barnhart, J.&nbsp;Leung, A.&nbsp;Korinek, C.&nbsp;O’Keefe, J.&nbsp;Whittlestone,
S.&nbsp;Avin, M.&nbsp;Brundage, J.&nbsp;Bullock, D.&nbsp;Cass-Beggs, et&nbsp;al.

</span>
<span class="ltx_bibblock">Frontier ai regulation: Managing emerging risks to public safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ArXiv preprint</em>, abs/2307.03718, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.03718" title="">https://arxiv.org/abs/2307.03718</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Argyle et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;P. Argyle, E.&nbsp;C. Busby, N.&nbsp;Fulda, J.&nbsp;R. Gubler, C.&nbsp;Rytting, and D.&nbsp;Wingate.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Political Analysis</em>, 31(3):337–351, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Asai, X.&nbsp;Yu, J.&nbsp;Kasai, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">One question answering model for many languages with cross-lingual
dense passage retrieval.

</span>
<span class="ltx_bibblock">In M.&nbsp;Ranzato, A.&nbsp;Beygelzimer, Y.&nbsp;N. Dauphin, P.&nbsp;Liang, and J.&nbsp;W.
Vaughan, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual</em>, pages 7547–7560, 2021.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html" title="">https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Askell, Y.&nbsp;Bai, A.&nbsp;Chen, D.&nbsp;Drain, D.&nbsp;Ganguli, T.&nbsp;Henighan, A.&nbsp;Jones,
N.&nbsp;Joseph, B.&nbsp;Mann, N.&nbsp;DasSarma, et&nbsp;al.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ArXiv preprint</em>, abs/2112.00861, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.00861" title="">https://arxiv.org/abs/2112.00861</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assefa et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;A. Assefa, D.&nbsp;Dervovic, M.&nbsp;Mahfouz, R.&nbsp;E. Tillman, P.&nbsp;Reddy, and M.&nbsp;Veloso.

</span>
<span class="ltx_bibblock">Generating synthetic data in finance: opportunities, challenges and
pitfalls.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the First ACM International Conference on AI
in Finance</em>, pages 1–8, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Azerbayev, H.&nbsp;Schoelkopf, K.&nbsp;Paster, M.&nbsp;D. Santos, S.&nbsp;McAleer, A.&nbsp;Q. Jiang,
J.&nbsp;Deng, S.&nbsp;Biderman, and S.&nbsp;Welleck.

</span>
<span class="ltx_bibblock">Llemma: An open language model for mathematics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">ArXiv preprint</em>, abs/2310.10631, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.10631" title="">https://arxiv.org/abs/2310.10631</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babbar and Schölkopf (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Babbar and B.&nbsp;Schölkopf.

</span>
<span class="ltx_bibblock">Data scarcity, robustness and extreme multi-label classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Machine Learning</em>, 108(8):1329–1351, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bai, S.&nbsp;Kadavath, S.&nbsp;Kundu, A.&nbsp;Askell, J.&nbsp;Kernion, A.&nbsp;Jones, A.&nbsp;Chen,
A.&nbsp;Goldie, A.&nbsp;Mirhoseini, C.&nbsp;McKinnon, et&nbsp;al.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ArXiv preprint</em>, abs/2212.08073, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.08073" title="">https://arxiv.org/abs/2212.08073</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbierato et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Barbierato, M.&nbsp;L.&nbsp;D. Vedova, D.&nbsp;Tessera, D.&nbsp;Toti, and N.&nbsp;Vanoli.

</span>
<span class="ltx_bibblock">A methodology for controlling bias and fairness in synthetic data
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Applied Sciences</em>, 12(9):4619, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Bi, H.&nbsp;Li, and J.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Data augmentation for text generation without any augmented data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 2223–2237,
Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.acl-long.173" title="">10.18653/v1/2021.acl-long.173</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.173" title="">https://aclanthology.org/2021.acl-long.173</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Borgeaud, A.&nbsp;Mensch, J.&nbsp;Hoffmann, T.&nbsp;Cai, E.&nbsp;Rutherford, K.&nbsp;Millican,
G.&nbsp;van&nbsp;den Driessche, J.&nbsp;Lespiau, B.&nbsp;Damoc, A.&nbsp;Clark, D.&nbsp;de&nbsp;Las&nbsp;Casas,
A.&nbsp;Guy, J.&nbsp;Menick, R.&nbsp;Ring, T.&nbsp;Hennigan, S.&nbsp;Huang, L.&nbsp;Maggiore, C.&nbsp;Jones,
A.&nbsp;Cassirer, A.&nbsp;Brock, M.&nbsp;Paganini, G.&nbsp;Irving, O.&nbsp;Vinyals, S.&nbsp;Osindero,
K.&nbsp;Simonyan, J.&nbsp;W. Rae, E.&nbsp;Elsen, and L.&nbsp;Sifre.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In K.&nbsp;Chaudhuri, S.&nbsp;Jegelka, L.&nbsp;Song, C.&nbsp;Szepesvári, G.&nbsp;Niu,
and S.&nbsp;Sabato, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of
<em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Proceedings of Machine Learning Research</em>, pages 2206–2240. PMLR,
2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/borgeaud22a.html" title="">https://proceedings.mlr.press/v162/borgeaud22a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borisov et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Borisov, K.&nbsp;Seßler, T.&nbsp;Leemann, M.&nbsp;Pawelczyk, and G.&nbsp;Kasneci.

</span>
<span class="ltx_bibblock">Language models are realistic tabular data generators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ArXiv preprint</em>, abs/2210.06280, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.06280" title="">https://arxiv.org/abs/2210.06280</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borkman et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Borkman, A.&nbsp;Crespi, S.&nbsp;Dhakad, S.&nbsp;Ganguly, J.&nbsp;Hogins, Y.&nbsp;C. Jhang,
M.&nbsp;Kamalzadeh, B.&nbsp;Li, S.&nbsp;Leal, P.&nbsp;Parisi, C.&nbsp;Romero, W.&nbsp;Smith, A.&nbsp;Thaman,
S.&nbsp;Warren, and N.&nbsp;Yadav.

</span>
<span class="ltx_bibblock">Unity perception: Generate synthetic data for computer vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ArXiv preprint</em>, abs/2107.04259, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.04259" title="">https://arxiv.org/abs/2107.04259</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;R. Bowman, J.&nbsp;Hyun, E.&nbsp;Perez, E.&nbsp;Chen, C.&nbsp;Pettit, S.&nbsp;Heiner,
K.&nbsp;Lukošiūtė, A.&nbsp;Askell, A.&nbsp;Jones, A.&nbsp;Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">Measuring progress on scalable oversight for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ArXiv preprint</em>, abs/2211.03540, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.03540" title="">https://arxiv.org/abs/2211.03540</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burns et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Burns, P.&nbsp;Izmailov, J.&nbsp;H. Kirchner, B.&nbsp;Baker, L.&nbsp;Gao, L.&nbsp;Aschenbrenner,
Y.&nbsp;Chen, A.&nbsp;Ecoffet, M.&nbsp;Joglekar, J.&nbsp;Leike, et&nbsp;al.

</span>
<span class="ltx_bibblock">Weak-to-strong generalization: Eliciting strong capabilities with
weak supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ArXiv preprint</em>, abs/2312.09390, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.09390" title="">https://arxiv.org/abs/2312.09390</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byman et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;L. Byman, C.&nbsp;Gao, C.&nbsp;Meserole, and V.&nbsp;Subrahmanian.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Deepfakes and international conflict</em>.

</span>
<span class="ltx_bibblock">Brookings Institution, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carbune et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Carbune, H.&nbsp;Mansoor, F.&nbsp;Liu, R.&nbsp;Aralikatte, G.&nbsp;Baechler, J.&nbsp;Chen, and
A.&nbsp;Sharma.

</span>
<span class="ltx_bibblock">Chart-based reasoning: Transferring capabilities from llms to vlms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ArXiv preprint</em>, abs/2403.12596, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.12596" title="">https://arxiv.org/abs/2403.12596</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Casper, T.&nbsp;Bu, Y.&nbsp;Li, J.&nbsp;Li, K.&nbsp;Zhang, K.&nbsp;Hariharan, and D.&nbsp;Hadfield-Menell.

</span>
<span class="ltx_bibblock">Red teaming deep neural networks with feature synthesis tools.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Casper, J.&nbsp;Lin, J.&nbsp;Kwon, G.&nbsp;Culp, and D.&nbsp;Hadfield-Menell.

</span>
<span class="ltx_bibblock">Explore, establish, exploit: Red teaming language models from
scratch.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">ArXiv preprint</em>, abs/2306.09442, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.09442" title="">https://arxiv.org/abs/2306.09442</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caswell et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Caswell, C.&nbsp;Chelba, and D.&nbsp;Grangier.

</span>
<span class="ltx_bibblock">Tagged back-translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 53–63, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/W19-5206" title="">10.18653/v1/W19-5206</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-5206" title="">https://aclanthology.org/W19-5206</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chauhan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Chauhan, S.&nbsp;Saxena, and P.&nbsp;Daniel.

</span>
<span class="ltx_bibblock">Improved unsupervised neural machine translation with semantically
weighted back translation for morphologically rich and low resource
languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Neural Processing Letters</em>, 54(3):1707–1726, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Chen, Y.&nbsp;Deng, H.&nbsp;Yuan, K.&nbsp;Ji, and Q.&nbsp;Gu.

</span>
<span class="ltx_bibblock">Self-play fine-tuning converts weak language models to strong
language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Cheng, T.&nbsp;Piccardi, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">CoMPosT: Characterizing and evaluating caricature in LLM
simulations.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 10853–10875, Singapore, Dec. 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.669" title="">10.18653/v1/2023.emnlp-main.669</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.669" title="">https://aclanthology.org/2023.emnlp-main.669</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Chern, H.&nbsp;Zou, X.&nbsp;Li, J.&nbsp;Hu, K.&nbsp;Feng, J.&nbsp;Li, and P.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Generative ai for math: Abel.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GAIR-NLP/abel" title="">https://github.com/GAIR-NLP/abel</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Chi, L.&nbsp;Dong, F.&nbsp;Wei, W.&nbsp;Wang, X.&nbsp;Mao, and H.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Cross-lingual natural language generation via pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of
Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium
on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020</em>, pages 7570–7577. AAAI Press, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aaai.org/ojs/index.php/AAAI/article/view/6256" title="">https://aaai.org/ojs/index.php/AAAI/article/view/6256</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;F. Christiano, J.&nbsp;Leike, T.&nbsp;B. Brown, M.&nbsp;Martic, S.&nbsp;Legg, and D.&nbsp;Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages
4299–4307, 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html" title="">https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Cui, L.&nbsp;Yuan, N.&nbsp;Ding, G.&nbsp;Yao, W.&nbsp;Zhu, Y.&nbsp;Ni, G.&nbsp;Xie, Z.&nbsp;Liu, and M.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahmen and Cook (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Dahmen and D.&nbsp;Cook.

</span>
<span class="ltx_bibblock">Synsys: A synthetic data generation system for healthcare
applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Sensors</em>, 19(5):1181, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshpande et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Deshpande, V.&nbsp;Murahari, T.&nbsp;Rajpurohit, A.&nbsp;Kalyan, and K.&nbsp;Narasimhan.

</span>
<span class="ltx_bibblock">Toxicity in chatgpt: Analyzing persona-assigned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ArXiv preprint</em>, abs/2304.05335, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.05335" title="">https://arxiv.org/abs/2304.05335</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Dhingra, M.&nbsp;Faruqui, A.&nbsp;Parikh, M.-W. Chang, D.&nbsp;Das, and W.&nbsp;Cohen.

</span>
<span class="ltx_bibblock">Handling divergent reference texts when evaluating table-to-text
generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4884–4895, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1483" title="">10.18653/v1/P19-1483</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1483" title="">https://aclanthology.org/P19-1483</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Ding, Y.&nbsp;Chen, B.&nbsp;Xu, Y.&nbsp;Qin, Z.&nbsp;Zheng, S.&nbsp;Hu, Z.&nbsp;Liu, M.&nbsp;Sun, and B.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional
conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ArXiv preprint</em>, abs/2305.14233, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.14233" title="">https://arxiv.org/abs/2305.14233</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Edunov, M.&nbsp;Ott, M.&nbsp;Auli, and D.&nbsp;Grangier.

</span>
<span class="ltx_bibblock">Understanding back-translation at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489–500, Brussels, Belgium, 2018.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/D18-1045" title="">10.18653/v1/D18-1045</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1045" title="">https://aclanthology.org/D18-1045</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El&nbsp;Emam et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;El&nbsp;Emam, L.&nbsp;Mosquera, and R.&nbsp;Hoptroff.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Practical synthetic data generation: balancing privacy and the
broad availability of data</em>.

</span>
<span class="ltx_bibblock">O’Reilly Media, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epaliyana et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Epaliyana, S.&nbsp;Ranathunga, and S.&nbsp;Jayasena.

</span>
<span class="ltx_bibblock">Improving back-translation with iterative filtering and data
selection for sinhala-english nmt.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2021 Moratuwa Engineering Research Conference (MERCon)</em>,
pages 438–443. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everitt et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Everitt, M.&nbsp;Hutter, R.&nbsp;Kumar, and V.&nbsp;Krakovna.

</span>
<span class="ltx_bibblock">Reward tampering problems and solutions in reinforcement learning: A
causal influence diagram perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Synthese</em>, 198(Suppl 27):6435–6467, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falke et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Falke, L.&nbsp;F.&nbsp;R. Ribeiro, P.&nbsp;A. Utama, I.&nbsp;Dagan, and I.&nbsp;Gurevych.

</span>
<span class="ltx_bibblock">Ranking generated summaries by correctness: An interesting but
challenging application for natural language inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2214–2220, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1213" title="">10.18653/v1/P19-1213</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1213" title="">https://aclanthology.org/P19-1213</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Feng, V.&nbsp;Balachandran, Y.&nbsp;Bai, and Y.&nbsp;Tsvetkov.

</span>
<span class="ltx_bibblock">FactKB: Generalizable factuality evaluation using language models
enhanced with factual knowledge.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 933–952, Singapore, Dec. 2023a. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.59" title="">10.18653/v1/2023.emnlp-main.59</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.59" title="">https://aclanthology.org/2023.emnlp-main.59</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Feng, C.&nbsp;Y. Park, Y.&nbsp;Liu, and Y.&nbsp;Tsvetkov.

</span>
<span class="ltx_bibblock">From pretraining data to language models to downstream tasks:
Tracking the trails of political biases leading to unfair nlp models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">ArXiv preprint</em>, abs/2305.08283, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.08283" title="">https://arxiv.org/abs/2305.08283</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frid-Adar et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Frid-Adar, E.&nbsp;Klang, M.&nbsp;Amitai, J.&nbsp;Goldberger, and H.&nbsp;Greenspan.

</span>
<span class="ltx_bibblock">Synthetic data augmentation using gan for improved liver lesion
classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">2018 IEEE 15th international symposium on biomedical imaging
(ISBI 2018)</em>, pages 289–293. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Ganguli, L.&nbsp;Lovitt, J.&nbsp;Kernion, A.&nbsp;Askell, Y.&nbsp;Bai, S.&nbsp;Kadavath, B.&nbsp;Mann,
E.&nbsp;Perez, N.&nbsp;Schiefer, K.&nbsp;Ndousse, et&nbsp;al.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling
behaviors, and lessons learned.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ArXiv preprint</em>, abs/2209.07858, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.07858" title="">https://arxiv.org/abs/2209.07858</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Gao, S.&nbsp;Biderman, S.&nbsp;Black, L.&nbsp;Golding, T.&nbsp;Hoppe, C.&nbsp;Foster, J.&nbsp;Phang,
H.&nbsp;He, A.&nbsp;Thite, N.&nbsp;Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv preprint</em>, abs/2101.00027, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2101.00027" title="">https://arxiv.org/abs/2101.00027</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Gao, J.&nbsp;Schulman, and J.&nbsp;Hilton.

</span>
<span class="ltx_bibblock">Scaling laws for reward model overoptimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">International Conference on Machine Learning</em>, pages
10835–10866. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini-Team et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini-Team, R.&nbsp;Anil, S.&nbsp;Borgeaud, Y.&nbsp;Wu, J.-B. Alayrac, J.&nbsp;Yu, R.&nbsp;Soricut,
J.&nbsp;Schalkwyk, A.&nbsp;M. Dai, A.&nbsp;Hauth, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">ArXiv preprint</em>, abs/2312.11805, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.11805" title="">https://arxiv.org/abs/2312.11805</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini-Team et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini-Team, M.&nbsp;Reid, N.&nbsp;Savinov, D.&nbsp;Teplyashin, D.&nbsp;Lepikhin, T.&nbsp;Lillicrap,
J.-b. Alayrac, R.&nbsp;Soricut, A.&nbsp;Lazaridou, O.&nbsp;Firat, J.&nbsp;Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ArXiv preprint</em>, abs/2403.05530, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05530" title="">https://arxiv.org/abs/2403.05530</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma-Team et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemma-Team, T.&nbsp;Mesnard, C.&nbsp;Hardin, R.&nbsp;Dadashi, S.&nbsp;Bhupatiraju, S.&nbsp;Pathak,
L.&nbsp;Sifre, M.&nbsp;Rivière, M.&nbsp;S. Kale, J.&nbsp;Love, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ArXiv preprint</em>, abs/2403.08295, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.08295" title="">https://arxiv.org/abs/2403.08295</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the National Academy of Sciences</em>, 120(30):e2305016120, 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1073/pnas.2305016120" title="">10.1073/pnas.2305016120</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.2305016120" title="">https://www.pnas.org/doi/abs/10.1073/pnas.2305016120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Gilardi, M.&nbsp;Alizadeh, and M.&nbsp;Kubli.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the National Academy of Sciences</em>, 120(30):e2305016120, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Goodfellow, J.&nbsp;Pouget-Abadie, M.&nbsp;Mirza, B.&nbsp;Xu, D.&nbsp;Warde-Farley, S.&nbsp;Ozair,
A.&nbsp;Courville, and Y.&nbsp;Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Communications of the ACM</em>, 63(11):139–144, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graça et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Graça, Y.&nbsp;Kim, J.&nbsp;Schamper, S.&nbsp;Khadivi, and H.&nbsp;Ney.

</span>
<span class="ltx_bibblock">Generalizing back-translation in neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 1: Research Papers)</em>, pages 45–52, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/W19-5205" title="">10.18653/v1/W19-5205</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-5205" title="">https://aclanthology.org/W19-5205</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Groh, Z.&nbsp;Epstein, C.&nbsp;Firestone, and R.&nbsp;Picard.

</span>
<span class="ltx_bibblock">Deepfake detection by human crowds, machines, and machine-informed
crowds.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the National Academy of Sciences</em>, 119(1):e2110013119, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Gu, B.&nbsp;Rozière, H.&nbsp;Leather, A.&nbsp;Solar-Lezama, G.&nbsp;Synnaeve, and S.&nbsp;I.
Wang.

</span>
<span class="ltx_bibblock">Cruxeval: A benchmark for code reasoning, understanding and
execution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ArXiv preprint</em>, abs/2401.03065, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.03065" title="">https://arxiv.org/abs/2401.03065</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guarnera et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Guarnera, O.&nbsp;Giudice, and S.&nbsp;Battiato.

</span>
<span class="ltx_bibblock">Deepfake detection by analyzing convolutional traces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition workshops</em>, pages 666–667, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Gupta, D.&nbsp;Bhatt, and A.&nbsp;Pandey.

</span>
<span class="ltx_bibblock">Transitioning from real to synthetic data: Quantifying the bias in
model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ArXiv preprint</em>, abs/2105.04144, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2105.04144" title="">https://arxiv.org/abs/2105.04144</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haluptzok et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Haluptzok, M.&nbsp;Bowers, and A.&nbsp;T. Kalai.

</span>
<span class="ltx_bibblock">Language models can teach themselves to program better.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv preprint</em>, abs/2207.14502, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.14502" title="">https://arxiv.org/abs/2207.14502</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Heusel, H.&nbsp;Ramsauer, T.&nbsp;Unterthiner, B.&nbsp;Nessler, and S.&nbsp;Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;von Luxburg, S.&nbsp;Bengio, H.&nbsp;M. Wallach, R.&nbsp;Fergus,
S.&nbsp;V.&nbsp;N. Vishwanathan, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages
6626–6637, 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html" title="">https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ho, A.&nbsp;Jain, and P.&nbsp;Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford,
D.&nbsp;de&nbsp;Las&nbsp;Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">An empirical analysis of compute-optimal large language model
training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in Neural Information Processing Systems</em>,
35:30016–30030, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Honovich, L.&nbsp;Choshen, R.&nbsp;Aharoni, E.&nbsp;Neeman, I.&nbsp;Szpektor, and O.&nbsp;Abend.

</span>
<span class="ltx_bibblock"><math alttext="q^{2}" class="ltx_Math" display="inline" id="bib.bib62.1.m1.1"><semantics id="bib.bib62.1.m1.1a"><msup id="bib.bib62.1.m1.1.1" xref="bib.bib62.1.m1.1.1.cmml"><mi id="bib.bib62.1.m1.1.1.2" xref="bib.bib62.1.m1.1.1.2.cmml">q</mi><mn id="bib.bib62.1.m1.1.1.3" xref="bib.bib62.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib62.1.m1.1b"><apply id="bib.bib62.1.m1.1.1.cmml" xref="bib.bib62.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib62.1.m1.1.1.1.cmml" xref="bib.bib62.1.m1.1.1">superscript</csymbol><ci id="bib.bib62.1.m1.1.1.2.cmml" xref="bib.bib62.1.m1.1.1.2">𝑞</ci><cn id="bib.bib62.1.m1.1.1.3.cmml" type="integer" xref="bib.bib62.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib62.1.m1.1c">q^{2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib62.1.m1.1d">italic_q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded
dialogues via question generation and question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.2.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7856–7870, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.emnlp-main.619" title="">10.18653/v1/2021.emnlp-main.619</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.619" title="">https://aclanthology.org/2021.emnlp-main.619</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howe et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Howe, J.&nbsp;Stoyanovich, H.&nbsp;Ping, B.&nbsp;Herman, and M.&nbsp;Gee.

</span>
<span class="ltx_bibblock">Synthetic data for social good.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ArXiv preprint</em>, abs/1710.08874, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1710.08874" title="">https://arxiv.org/abs/1710.08874</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Huang, H.&nbsp;Kwak, and J.&nbsp;An.

</span>
<span class="ltx_bibblock">Is chatgpt better than human annotators? potential and limitations of
chatgpt in explaining implicit hate speech.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">ArXiv preprint</em>, abs/2302.07736, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.07736" title="">https://arxiv.org/abs/2302.07736</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Huang, S.&nbsp;Gu, L.&nbsp;Hou, Y.&nbsp;Wu, X.&nbsp;Wang, H.&nbsp;Yu, and J.&nbsp;Han.

</span>
<span class="ltx_bibblock">Large language models can self-improve.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 1051–1068, Singapore, Dec. 2023b. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.67" title="">10.18653/v1/2023.emnlp-main.67</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.67" title="">https://aclanthology.org/2023.emnlp-main.67</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Huang, F.&nbsp;Xia, T.&nbsp;Xiao, H.&nbsp;Chan, J.&nbsp;Liang, P.&nbsp;Florence, A.&nbsp;Zeng, J.&nbsp;Tompson,
I.&nbsp;Mordatch, Y.&nbsp;Chebotar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">ArXiv preprint</em>, abs/2207.05608, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.05608" title="">https://arxiv.org/abs/2207.05608</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hubinger et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Hubinger, C.&nbsp;Denison, J.&nbsp;Mu, M.&nbsp;Lambert, M.&nbsp;Tong, M.&nbsp;MacDiarmid, T.&nbsp;Lanham,
D.&nbsp;M. Ziegler, T.&nbsp;Maxwell, N.&nbsp;Cheng, et&nbsp;al.

</span>
<span class="ltx_bibblock">Sleeper agents: Training deceptive llms that persist through safety
training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">ArXiv preprint</em>, abs/2401.05566, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.05566" title="">https://arxiv.org/abs/2401.05566</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Ji, N.&nbsp;Lee, R.&nbsp;Frieske, T.&nbsp;Yu, D.&nbsp;Su, Y.&nbsp;Xu, E.&nbsp;Ishii, Y.&nbsp;J. Bang,
A.&nbsp;Madotto, and P.&nbsp;Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">ACM Computing Surveys (CSUR)</em>, 55(12):1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Jia, Y.&nbsp;Yang, Y.&nbsp;Xia, Y.&nbsp;Chen, Z.&nbsp;Parekh, H.&nbsp;Pham, Q.&nbsp;V. Le, Y.&nbsp;Sung, Z.&nbsp;Li,
and T.&nbsp;Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock">In M.&nbsp;Meila and T.&nbsp;Zhang, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib69.2.2">Proceedings of Machine Learning
Research</em>, pages 4904–4916. PMLR, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v139/jia21b.html" title="">http://proceedings.mlr.press/v139/jia21b.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Mensch, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l.
Casas, F.&nbsp;Bressand, G.&nbsp;Lengyel, G.&nbsp;Lample, L.&nbsp;Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ArXiv preprint</em>, abs/2310.06825, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06825" title="">https://arxiv.org/abs/2310.06825</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Jiang, A.&nbsp;Gupta, Z.&nbsp;Zhang, G.&nbsp;Wang, Y.&nbsp;Dou, Y.&nbsp;Chen, L.&nbsp;Fei-Fei,
A.&nbsp;Anandkumar, Y.&nbsp;Zhu, and L.&nbsp;Fan.

</span>
<span class="ltx_bibblock">Vima: General robot manipulation with multimodal prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">NeurIPS 2022 Foundation Models for Decision Making
Workshop</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Jones, H.&nbsp;Palangi, C.&nbsp;Simões, V.&nbsp;Chandrasekaran, S.&nbsp;Mukherjee, A.&nbsp;Mitra,
A.&nbsp;Awadallah, and E.&nbsp;Kamar.

</span>
<span class="ltx_bibblock">Teaching language models to hallucinate less with synthetic tasks,
2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06827" title="">https://arxiv.org/abs/2310.06827</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kambhampati et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Kambhampati, K.&nbsp;Valmeekam, L.&nbsp;Guan, K.&nbsp;Stechly, M.&nbsp;Verma, S.&nbsp;Bhambri,
L.&nbsp;Saldyt, and A.&nbsp;Murthy.

</span>
<span class="ltx_bibblock">Llms can’t plan, but can help planning in llm-modulo frameworks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2402.01817</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Kumar, N.&nbsp;Joshi, A.&nbsp;Mukherjee, G.&nbsp;Ramakrishnan, and P.&nbsp;Jyothi.

</span>
<span class="ltx_bibblock">Cross-lingual training for automatic question generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4863–4872, Florence, Italy, 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1481" title="">10.18653/v1/P19-1481</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1481" title="">https://aclanthology.org/P19-1481</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Lam, A.&nbsp;Sanchez-Gonzalez, M.&nbsp;Willson, P.&nbsp;Wirnsberger, M.&nbsp;Fortunato, F.&nbsp;Alet,
S.&nbsp;Ravuri, T.&nbsp;Ewalds, Z.&nbsp;Eaton-Rosen, W.&nbsp;Hu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Learning skillful medium-range global weather forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Science</em>, 382(6677):1416–1421, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landers and Behrend (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;N. Landers and T.&nbsp;S. Behrend.

</span>
<span class="ltx_bibblock">Auditing the ai auditors: A framework for evaluating fairness and
bias in high stakes ai predictive models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">American Psychologist</em>, 78(1):36, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Laurençon, L.&nbsp;Tronchon, and V.&nbsp;Sanh.

</span>
<span class="ltx_bibblock">Unlocking the conversion of web screenshots into html code with the
websight dataset, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.09029" title="">https://arxiv.org/abs/2403.09029</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Le, Y.&nbsp;Wang, A.&nbsp;D. Gotmare, S.&nbsp;Savarese, and S.&nbsp;C.&nbsp;H. Hoi.

</span>
<span class="ltx_bibblock">Coderl: Mastering code generation through pretrained models and deep
reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Advances in Neural Information Processing Systems</em>,
35:21314–21328, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;LeCun.

</span>
<span class="ltx_bibblock">A path towards autonomous machine intelligence version 0.9. 2,
2022-06-27.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Open Review</em>, 62, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Lee, M.&nbsp;Joshi, I.&nbsp;R. Turc, H.&nbsp;Hu, F.&nbsp;Liu, J.&nbsp;M. Eisenschlos, U.&nbsp;Khandelwal,
P.&nbsp;Shaw, M.-W. Chang, and K.&nbsp;Toutanova.

</span>
<span class="ltx_bibblock">Pix2struct: Screenshot parsing as pretraining for visual language
understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">International Conference on Machine Learning</em>, pages
18893–18912. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leike et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Leike, D.&nbsp;Krueger, T.&nbsp;Everitt, M.&nbsp;Martic, V.&nbsp;Maini, and S.&nbsp;Legg.

</span>
<span class="ltx_bibblock">Scalable agent alignment via reward modeling: a research direction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">ArXiv preprint</em>, abs/1811.07871, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1811.07871" title="">https://arxiv.org/abs/1811.07871</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;S.&nbsp;H. Lewis, E.&nbsp;Perez, A.&nbsp;Piktus, F.&nbsp;Petroni, V.&nbsp;Karpukhin, N.&nbsp;Goyal,
H.&nbsp;Küttler, M.&nbsp;Lewis, W.&nbsp;Yih, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, and
D.&nbsp;Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.&nbsp;Balcan, and H.&nbsp;Lin,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Lewkowycz, A.&nbsp;Andreassen, D.&nbsp;Dohan, E.&nbsp;Dyer, H.&nbsp;Michalewski, V.&nbsp;Ramasesh,
A.&nbsp;Slone, C.&nbsp;Anil, I.&nbsp;Schlag, T.&nbsp;Gutman-Solo, Y.&nbsp;Wu, B.&nbsp;Neyshabur,
G.&nbsp;Gur-Ari, and V.&nbsp;Misra.

</span>
<span class="ltx_bibblock">Solving quantitative reasoning problems with language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.14858" title="">https://arxiv.org/abs/2206.14858</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Callison-Burch (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Li and C.&nbsp;Callison-Burch.

</span>
<span class="ltx_bibblock">Paxqa: Generating cross-lingual question answering examples at
training scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">ArXiv preprint</em>, abs/2304.12206, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.12206" title="">https://arxiv.org/abs/2304.12206</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Li, W.&nbsp;Wang, J.&nbsp;Hu, Y.&nbsp;Wei, N.&nbsp;Zheng, H.&nbsp;Hu, Z.&nbsp;Zhang, and H.&nbsp;Peng.

</span>
<span class="ltx_bibblock">Common 7b language models already possess strong math capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">ArXiv preprint</em>, abs/2403.04706, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.04706" title="">https://arxiv.org/abs/2403.04706</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Li, R.&nbsp;Carver, I.&nbsp;Lopez-Gomez, F.&nbsp;Sha, and J.&nbsp;Anderson.

</span>
<span class="ltx_bibblock">Seeds: Emulation of weather forecast ensembles with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">ArXiv preprint</em>, abs/2306.14066, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.14066" title="">https://arxiv.org/abs/2306.14066</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Li, T.&nbsp;Zhang, Y.&nbsp;Dubois, R.&nbsp;Taori, I.&nbsp;Gulrajani, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Liang, W.&nbsp;Huang, F.&nbsp;Xia, P.&nbsp;Xu, K.&nbsp;Hausman, B.&nbsp;Ichter, P.&nbsp;Florence, and
A.&nbsp;Zeng.

</span>
<span class="ltx_bibblock">Code as policies: Language model programs for embodied control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">ArXiv preprint</em>, abs/2209.07753, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.07753" title="">https://arxiv.org/abs/2209.07753</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Liao, S.&nbsp;Khadivi, and S.&nbsp;Hewavitharana.

</span>
<span class="ltx_bibblock">Back-translation for large-scale multilingual machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the Sixth Conference on Machine Translation</em>,
pages 418–424, Online, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.wmt-1.50" title="">https://aclanthology.org/2021.wmt-1.50</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Lightman, V.&nbsp;Kosaraju, Y.&nbsp;Burda, H.&nbsp;Edwards, B.&nbsp;Baker, T.&nbsp;Lee, J.&nbsp;Leike,
J.&nbsp;Schulman, I.&nbsp;Sutskever, and K.&nbsp;Cobbe.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">ArXiv preprint</em>, abs/2305.20050, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.20050" title="">https://arxiv.org/abs/2305.20050</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lin, J.&nbsp;Hilton, and O.&nbsp;Evans.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252,
Dublin, Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.acl-long.229" title="">10.18653/v1/2022.acl-long.229</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.229" title="">https://aclanthology.org/2022.acl-long.229</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Liu, S.&nbsp;D. Zhang, and R.&nbsp;Jabbarvand.

</span>
<span class="ltx_bibblock">Codemind: A framework to challenge large language models for code
reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">ArXiv preprint</em>, abs/2402.09664, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.09664" title="">https://arxiv.org/abs/2402.09664</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Liu, J.&nbsp;Eisenschlos, F.&nbsp;Piccinno, S.&nbsp;Krichene, C.&nbsp;Pang, K.&nbsp;Lee, M.&nbsp;Joshi,
W.&nbsp;Chen, N.&nbsp;Collier, and Y.&nbsp;Altun.

</span>
<span class="ltx_bibblock">Deplot: One-shot visual language reasoning by plot-to-table
translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 10381–10399, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Liu, F.&nbsp;Piccinno, S.&nbsp;Krichene, C.&nbsp;Pang, K.&nbsp;Lee, M.&nbsp;Joshi, Y.&nbsp;Altun,
N.&nbsp;Collier, and J.&nbsp;Eisenschlos.

</span>
<span class="ltx_bibblock">Matcha: Enhancing visual language pretraining with math reasoning and
chart derendering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 12756–12770,
2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Yao (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu and A.&nbsp;C.-C. Yao.

</span>
<span class="ltx_bibblock">Augmenting math word problems via iterative question composing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">ArXiv preprint</em>, abs/2401.09003, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.09003" title="">https://arxiv.org/abs/2401.09003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Advances in neural information processing systems</em>, 36,
2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Liu, C.&nbsp;Jia, J.&nbsp;Wei, G.&nbsp;Xu, L.&nbsp;Wang, and S.&nbsp;Vosoughi.

</span>
<span class="ltx_bibblock">Mitigating political bias in language models through reinforced
calibration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in
Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</em>,
pages 14857–14866. AAAI Press, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/17744" title="">https://ojs.aaai.org/index.php/AAAI/article/view/17744</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Liu, J.&nbsp;Wei, S.&nbsp;S. Gu, T.-Y. Wu, S.&nbsp;Vosoughi, C.&nbsp;Cui, D.&nbsp;Zhou, and A.&nbsp;M.
Dai.

</span>
<span class="ltx_bibblock">Mind’s eye: Grounded language model reasoning through simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">ArXiv preprint</em>, abs/2210.05359, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.05359" title="">https://arxiv.org/abs/2210.05359</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Liu, R.&nbsp;Yang, C.&nbsp;Jia, G.&nbsp;Zhang, D.&nbsp;Zhou, A.&nbsp;M. Dai, D.&nbsp;Yang, and
S.&nbsp;Vosoughi.

</span>
<span class="ltx_bibblock">Training socially aligned language models in simulated human society.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">ArXiv preprint</em>, abs/2305.16960, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.16960" title="">https://arxiv.org/abs/2305.16960</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Liu, W.&nbsp;Zeng, K.&nbsp;He, Y.&nbsp;Jiang, and J.&nbsp;He.

</span>
<span class="ltx_bibblock">What makes good data for alignment? a comprehensive study of
automatic data selection in instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">ArXiv preprint</em>, abs/2312.15685, 2023d.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.15685" title="">https://arxiv.org/abs/2312.15685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Lu, M.&nbsp;Shen, H.&nbsp;Wang, X.&nbsp;Wang, C.&nbsp;van Rechem, and W.&nbsp;Wei.

</span>
<span class="ltx_bibblock">Machine learning for synthetic data generation: a review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">ArXiv preprint</em>, abs/2302.04062, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.04062" title="">https://arxiv.org/abs/2302.04062</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucini (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Lucini.

</span>
<span class="ltx_bibblock">The real deal about synthetic data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">MIT Sloan Management Review</em>, 63(1):1–4,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Luo, Q.&nbsp;Sun, C.&nbsp;Xu, P.&nbsp;Zhao, J.&nbsp;Lou, C.&nbsp;Tao, X.&nbsp;Geng, Q.&nbsp;Lin, S.&nbsp;Chen, and
D.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Wizardmath: Empowering mathematical reasoning for large language
models via reinforced evol-instruct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">ArXiv preprint</em>, abs/2308.09583, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.09583" title="">https://arxiv.org/abs/2308.09583</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Luo, C.&nbsp;Xu, P.&nbsp;Zhao, Q.&nbsp;Sun, X.&nbsp;Geng, W.&nbsp;Hu, C.&nbsp;Tao, J.&nbsp;Ma, Q.&nbsp;Lin, and
D.&nbsp;Jiang.

</span>
<span class="ltx_bibblock">Wizardcoder: Empowering code large language models with
evol-instruct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">ArXiv preprint</em>, abs/2306.08568, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.08568" title="">https://arxiv.org/abs/2306.08568</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marie et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Marie, R.&nbsp;Rubino, and A.&nbsp;Fujita.

</span>
<span class="ltx_bibblock">Tagged back-translation revisited: Why does it really work?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 5990–5997, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2020.acl-main.532" title="">10.18653/v1/2020.acl-main.532</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.532" title="">https://aclanthology.org/2020.acl-main.532</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Masry, P.&nbsp;Kavehzadeh, X.&nbsp;L. Do, E.&nbsp;Hoque, and S.&nbsp;Joty.

</span>
<span class="ltx_bibblock">UniChart: A universal vision-language pretrained model for chart
comprehension and reasoning.

</span>
<span class="ltx_bibblock">In H.&nbsp;Bouamor, J.&nbsp;Pino, and K.&nbsp;Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing</em>,
pages 14662–14684, Singapore, 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.emnlp-main.906" title="">10.18653/v1/2023.emnlp-main.906</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.906" title="">https://aclanthology.org/2023.emnlp-main.906</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mattern et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Mattern, F.&nbsp;Mireshghallah, Z.&nbsp;Jin, B.&nbsp;Schölkopf, M.&nbsp;Sachan, and
T.&nbsp;Berg-Kirkpatrick.

</span>
<span class="ltx_bibblock">Membership inference attacks against language models via
neighbourhood comparison.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">ArXiv preprint</em>, abs/2305.18462, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.18462" title="">https://arxiv.org/abs/2305.18462</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Meng, J.&nbsp;Huang, Y.&nbsp;Zhang, and J.&nbsp;Han.

</span>
<span class="ltx_bibblock">Generating training data with language models: Towards zero-shot
language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Advances in Neural Information Processing Systems</em>,
35:462–477, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Meta and microsoft introduce the next generation of llama.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/llama-2" title="">https://ai.meta.com/blog/llama-2</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Min, K.&nbsp;Krishna, X.&nbsp;Lyu, M.&nbsp;Lewis, W.-t. Yih, P.&nbsp;W. Koh, M.&nbsp;Iyyer,
L.&nbsp;Zettlemoyer, and H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Factscore: Fine-grained atomic evaluation of factual precision in
long form text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2305.14251</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Muennighoff, A.&nbsp;Rush, B.&nbsp;Barak, T.&nbsp;Le&nbsp;Scao, N.&nbsp;Tazi, A.&nbsp;Piktus, S.&nbsp;Pyysalo,
T.&nbsp;Wolf, and C.&nbsp;A. Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Mukherjee, A.&nbsp;Mitra, G.&nbsp;Jawahar, S.&nbsp;Agarwal, H.&nbsp;Palangi, and A.&nbsp;Awadallah.

</span>
<span class="ltx_bibblock">Orca: Progressive learning from complex explanation traces of gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">ArXiv preprint</em>, abs/2306.02707, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.02707" title="">https://arxiv.org/abs/2306.02707</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;I. Nikolenko.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Synthetic data for deep learning</em>, volume 174.

</span>
<span class="ltx_bibblock">Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ntoutsi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Ntoutsi, P.&nbsp;Fafalios, U.&nbsp;Gadiraju, V.&nbsp;Iosifidis, W.&nbsp;Nejdl, M.-E. Vidal,
S.&nbsp;Ruggieri, F.&nbsp;Turini, S.&nbsp;Papadopoulos, E.&nbsp;Krasanakis, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bias in data-driven artificial intelligence systems—an introductory
survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</em>, 10(3):e1356, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oren et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Oren, N.&nbsp;Meister, N.&nbsp;Chatterji, F.&nbsp;Ladhak, and T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Proving test set contamination in black box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">ArXiv preprint</em>, abs/2310.17623, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.17623" title="">https://arxiv.org/abs/2310.17623</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;L. Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang,
S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray, J.&nbsp;Schulman, J.&nbsp;Hilton, F.&nbsp;Kelton, L.&nbsp;Miller,
M.&nbsp;Simens, A.&nbsp;Askell, P.&nbsp;Welinder, P.&nbsp;Christiano, J.&nbsp;Leike, and R.&nbsp;Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">ArXiv preprint</em>, abs/2203.02155, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.02155" title="">https://arxiv.org/abs/2203.02155</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Pan, K.&nbsp;Bhatia, and J.&nbsp;Steinhardt.

</span>
<span class="ltx_bibblock">The effects of reward misspecification: Mapping and mitigating
misaligned models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=JYtwGwIL7ye" title="">https://openreview.net/forum?id=JYtwGwIL7ye</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;S. Park, J.&nbsp;O’Brien, C.&nbsp;J. Cai, M.&nbsp;R. Morris, P.&nbsp;Liang, and M.&nbsp;S. Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Proceedings of the 36th Annual ACM Symposium on User
Interface Software and Technology</em>, pages 1–22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paster et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Paster, M.&nbsp;D. Santos, Z.&nbsp;Azerbayev, and J.&nbsp;Ba.

</span>
<span class="ltx_bibblock">Openwebmath: An open dataset of high-quality mathematical web text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">ArXiv preprint</em>, abs/2310.06786, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06786" title="">https://arxiv.org/abs/2310.06786</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel and Pavlick (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Patel and E.&nbsp;Pavlick.

</span>
<span class="ltx_bibblock">Mapping language models to grounded conceptual spaces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gJcEM8sxHK" title="">https://openreview.net/forum?id=gJcEM8sxHK</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Huang, F.&nbsp;Song, T.&nbsp;Cai, R.&nbsp;Ring, J.&nbsp;Aslanides, A.&nbsp;Glaese,
N.&nbsp;McAleese, and G.&nbsp;Irving.

</span>
<span class="ltx_bibblock">Red teaming language models with language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 3419–3448, Abu Dhabi, United Arab
Emirates, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.225" title="">https://aclanthology.org/2022.emnlp-main.225</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Perez, S.&nbsp;Ringer, K.&nbsp;Lukošiūtė, K.&nbsp;Nguyen, E.&nbsp;Chen, S.&nbsp;Heiner,
C.&nbsp;Pettit, C.&nbsp;Olsson, S.&nbsp;Kundu, S.&nbsp;Kadavath, et&nbsp;al.

</span>
<span class="ltx_bibblock">Discovering language model behaviors with model-written evaluations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Findings of the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13387–13434.
Association for Computational Linguistics, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Pham, X.&nbsp;Wang, Y.&nbsp;Yang, and G.&nbsp;Neubig.

</span>
<span class="ltx_bibblock">Meta back-translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=3jjmdp7Hha" title="">https://openreview.net/forum?id=3jjmdp7Hha</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Przystupa and Abdul-Mageed (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Przystupa and M.&nbsp;Abdul-Mageed.

</span>
<span class="ltx_bibblock">Neural machine translation of low-resource and similar languages with
backtranslation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 3: Shared Task Papers, Day 2)</em>, pages 224–235, Florence, Italy,
2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/W19-5431" title="">10.18653/v1/W19-5431</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-5431" title="">https://aclanthology.org/W19-5431</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;W. Kim, C.&nbsp;Hallacy, A.&nbsp;Ramesh, G.&nbsp;Goh, S.&nbsp;Agarwal, G.&nbsp;Sastry,
A.&nbsp;Askell, P.&nbsp;Mishkin, J.&nbsp;Clark, G.&nbsp;Krueger, and I.&nbsp;Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In M.&nbsp;Meila and T.&nbsp;Zhang, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib126.2.2">Proceedings of Machine Learning
Research</em>, pages 8748–8763. PMLR, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v139/radford21a.html" title="">http://proceedings.mlr.press/v139/radford21a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, S.&nbsp;Borgeaud, T.&nbsp;Cai, K.&nbsp;Millican, J.&nbsp;Hoffmann, F.&nbsp;Song,
J.&nbsp;Aslanides, S.&nbsp;Henderson, R.&nbsp;Ring, S.&nbsp;Young, E.&nbsp;Rutherford, T.&nbsp;Hennigan,
J.&nbsp;Menick, A.&nbsp;Cassirer, R.&nbsp;Powell, G.&nbsp;v.&nbsp;d. Driessche, L.&nbsp;A. Hendricks,
M.&nbsp;Rauh, P.-S. Huang, A.&nbsp;Glaese, J.&nbsp;Welbl, S.&nbsp;Dathathri, S.&nbsp;Huang, J.&nbsp;Uesato,
J.&nbsp;Mellor, I.&nbsp;Higgins, A.&nbsp;Creswell, N.&nbsp;McAleese, A.&nbsp;Wu, E.&nbsp;Elsen,
S.&nbsp;Jayakumar, E.&nbsp;Buchatskaya, D.&nbsp;Budden, E.&nbsp;Sutherland, K.&nbsp;Simonyan,
M.&nbsp;Paganini, L.&nbsp;Sifre, L.&nbsp;Martens, X.&nbsp;L. Li, A.&nbsp;Kuncoro, A.&nbsp;Nematzadeh,
E.&nbsp;Gribovskaya, D.&nbsp;Donato, A.&nbsp;Lazaridou, A.&nbsp;Mensch, J.-B. Lespiau,
M.&nbsp;Tsimpoukelli, N.&nbsp;Grigorev, D.&nbsp;Fritz, T.&nbsp;Sottiaux, M.&nbsp;Pajarskas, T.&nbsp;Pohlen,
Z.&nbsp;Gong, D.&nbsp;Toyama, C.&nbsp;d.&nbsp;M. d’Autume, Y.&nbsp;Li, T.&nbsp;Terzi, V.&nbsp;Mikulik,
I.&nbsp;Babuschkin, A.&nbsp;Clark, D.&nbsp;d.&nbsp;L. Casas, A.&nbsp;Guy, C.&nbsp;Jones, J.&nbsp;Bradbury,
M.&nbsp;Johnson, B.&nbsp;Hechtman, L.&nbsp;Weidinger, I.&nbsp;Gabriel, W.&nbsp;Isaac, E.&nbsp;Lockhart,
S.&nbsp;Osindero, L.&nbsp;Rimell, C.&nbsp;Dyer, O.&nbsp;Vinyals, K.&nbsp;Ayoub, J.&nbsp;Stanway,
L.&nbsp;Bennett, D.&nbsp;Hassabis, K.&nbsp;Kavukcuoglu, and G.&nbsp;Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.11446" title="">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Rafailov, A.&nbsp;Sharma, E.&nbsp;Mitchell, S.&nbsp;Ermon, C.&nbsp;D. Manning, and C.&nbsp;Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a
reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">NeurIPS</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258959321" title="">https://api.semanticscholar.org/CorpusID:258959321</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Ramesh, P.&nbsp;Dhariwal, A.&nbsp;Nichol, C.&nbsp;Chu, and M.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">ArXiv preprint</em>, abs/2204.06125, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06125" title="">https://arxiv.org/abs/2204.06125</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riabi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Riabi, T.&nbsp;Scialom, R.&nbsp;Keraron, B.&nbsp;Sagot, D.&nbsp;Seddah, and J.&nbsp;Staiano.

</span>
<span class="ltx_bibblock">Synthetic data augmentation for zero-shot cross-lingual question
answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7016–7030, Online and Punta Cana,
Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2021.emnlp-main.562" title="">10.18653/v1/2021.emnlp-main.562</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.562" title="">https://aclanthology.org/2021.emnlp-main.562</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rid (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Rid.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">Active measures: The secret history of disinformation and
political warfare</em>.

</span>
<span class="ltx_bibblock">Farrar, Straus and Giroux, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Saharia, W.&nbsp;Chan, S.&nbsp;Saxena, L.&nbsp;Li, J.&nbsp;Whang, E.&nbsp;L. Denton, K.&nbsp;Ghasemipour,
R.&nbsp;Gontijo&nbsp;Lopes, B.&nbsp;Karagol&nbsp;Ayan, T.&nbsp;Salimans, J.&nbsp;Ho, D.&nbsp;J. Fleet, and
M.&nbsp;Norouzi.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock">In S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;35,
pages 36479–36494. Curran Associates, Inc., 2022a.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Saharia, W.&nbsp;Chan, S.&nbsp;Saxena, L.&nbsp;Li, J.&nbsp;Whang, E.&nbsp;L. Denton, K.&nbsp;Ghasemipour,
R.&nbsp;Gontijo&nbsp;Lopes, B.&nbsp;Karagol&nbsp;Ayan, T.&nbsp;Salimans, et&nbsp;al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">Advances in neural information processing systems</em>,
35:36479–36494, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxton et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Saxton, E.&nbsp;Grefenstette, F.&nbsp;Hill, and P.&nbsp;Kohli.

</span>
<span class="ltx_bibblock">Analysing mathematical reasoning abilities of neural models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=H1gR5iR5FX" title="">https://openreview.net/forum?id=H1gR5iR5FX</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, E.&nbsp;Hambro,
L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Sennrich, B.&nbsp;Haddow, and A.&nbsp;Birch.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin,
Germany, 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P16-1009" title="">10.18653/v1/P16-1009</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P16-1009" title="">https://aclanthology.org/P16-1009</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shakeri et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Shakeri, N.&nbsp;Constant, M.&nbsp;Kale, and L.&nbsp;Xue.

</span>
<span class="ltx_bibblock">Towards zero-shot multilingual synthetic question and answer
generation for cross-lingual reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">Proceedings of the 14th International Conference on Natural
Language Generation</em>, pages 35–45, Aberdeen, Scotland, UK, 2021. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.inlg-1.4" title="">https://aclanthology.org/2021.inlg-1.4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Shao, P.&nbsp;Wang, Q.&nbsp;Zhu, R.&nbsp;Xu, J.&nbsp;Song, M.&nbsp;Zhang, Y.&nbsp;K. Li, Y.&nbsp;Wu, and
D.&nbsp;Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open
language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Sharma, M.&nbsp;Tong, T.&nbsp;Korbak, D.&nbsp;Duvenaud, A.&nbsp;Askell, S.&nbsp;R. Bowman, E.&nbsp;DURMUS,
Z.&nbsp;Hatfield-Dodds, S.&nbsp;R. Johnston, S.&nbsp;M. Kravec, T.&nbsp;Maxwell, S.&nbsp;McCandlish,
K.&nbsp;Ndousse, O.&nbsp;Rausch, N.&nbsp;Schiefer, D.&nbsp;Yan, M.&nbsp;Zhang, and E.&nbsp;Perez.

</span>
<span class="ltx_bibblock">Towards understanding sycophancy in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Shi, A.&nbsp;Ajith, M.&nbsp;Xia, Y.&nbsp;Huang, D.&nbsp;Liu, T.&nbsp;Blevins, D.&nbsp;Chen, and
L.&nbsp;Zettlemoyer.

</span>
<span class="ltx_bibblock">Detecting pretraining data from large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, F.&nbsp;Cassano, A.&nbsp;Gopinath, K.&nbsp;Narasimhan, and S.&nbsp;Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shypula et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Shypula, A.&nbsp;Madaan, Y.&nbsp;Zeng, U.&nbsp;Alon, J.&nbsp;Gardner, M.&nbsp;Hashemi, G.&nbsp;Neubig,
P.&nbsp;Ranganathan, O.&nbsp;Bastani, and A.&nbsp;Yazdanbakhsh.

</span>
<span class="ltx_bibblock">Learning performance-improving code edits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">ArXiv preprint</em>, abs/2302.07867, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.07867" title="">https://arxiv.org/abs/2302.07867</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Si, Y.&nbsp;Zhang, Z.&nbsp;Yang, R.&nbsp;Liu, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">Design2code: How far are we from automating front-end engineering?,
2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.03163" title="">https://arxiv.org/abs/2403.03163</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, S.&nbsp;Azizi, T.&nbsp;Tu, S.&nbsp;S. Mahdavi, J.&nbsp;Wei, H.&nbsp;W. Chung, N.&nbsp;Scales,
A.&nbsp;Tanwani, H.&nbsp;Cole-Lewis, S.&nbsp;Pfohl, et&nbsp;al.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">ArXiv preprint</em>, abs/2212.13138, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.13138" title="">https://arxiv.org/abs/2212.13138</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinhardt (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Steinhardt.

</span>
<span class="ltx_bibblock">Ml systems will have weird failure modes.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/" title="">https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/</a>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, S.&nbsp;Shen, S.&nbsp;Cao, H.&nbsp;Liu, C.&nbsp;Li, Y.&nbsp;Shen, C.&nbsp;Gan, L.-Y. Gui, Y.-X. Wang,
Y.&nbsp;Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Aligning large multimodal models with factually augmented rlhf.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">ArXiv preprint</em>, abs/2309.14525, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.14525" title="">https://arxiv.org/abs/2309.14525</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Tang, Z.&nbsp;Deng, H.&nbsp;Lin, X.&nbsp;Han, Q.&nbsp;Liang, and L.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Toolalpaca: Generalized tool learning for language models with 3000
simulated cases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">ArXiv preprint</em>, abs/2306.05301, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.05301" title="">https://arxiv.org/abs/2306.05301</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and
T.&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taylor, M.&nbsp;Kardas, G.&nbsp;Cucurull, T.&nbsp;Scialom, A.&nbsp;Hartshorn, E.&nbsp;Saravia,
A.&nbsp;Poulton, V.&nbsp;Kerkez, and R.&nbsp;Stojnic.

</span>
<span class="ltx_bibblock">Galactica: A large language model for science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">ArXiv preprint</em>, abs/2211.09085, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09085" title="">https://arxiv.org/abs/2211.09085</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Thoppilan, D.&nbsp;De&nbsp;Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.-T. Cheng,
A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du, Y.&nbsp;Li, H.&nbsp;Lee, H.&nbsp;S. Zheng, A.&nbsp;Ghafouri,
M.&nbsp;Menegali, Y.&nbsp;Huang, M.&nbsp;Krikun, D.&nbsp;Lepikhin, J.&nbsp;Qin, D.&nbsp;Chen, Y.&nbsp;Xu,
Z.&nbsp;Chen, A.&nbsp;Roberts, M.&nbsp;Bosma, V.&nbsp;Zhao, Y.&nbsp;Zhou, C.-C. Chang, I.&nbsp;Krivokon,
W.&nbsp;Rusch, M.&nbsp;Pickett, P.&nbsp;Srinivasan, L.&nbsp;Man, K.&nbsp;Meier-Hellstern, M.&nbsp;R.
Morris, T.&nbsp;Doshi, R.&nbsp;D. Santos, T.&nbsp;Duke, J.&nbsp;Soraker, B.&nbsp;Zevenbergen,
V.&nbsp;Prabhakaran, M.&nbsp;Diaz, B.&nbsp;Hutchinson, K.&nbsp;Olson, A.&nbsp;Molina, E.&nbsp;Hoffman-John,
J.&nbsp;Lee, L.&nbsp;Aroyo, R.&nbsp;Rajakumar, A.&nbsp;Butryna, M.&nbsp;Lamm, V.&nbsp;Kuzmina, J.&nbsp;Fenton,
A.&nbsp;Cohen, R.&nbsp;Bernstein, R.&nbsp;Kurzweil, B.&nbsp;Aguera-Arcas, C.&nbsp;Cui, M.&nbsp;Croak,
E.&nbsp;Chi, and Q.&nbsp;Le.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">ArXiv preprint</em>, abs/2201.08239, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.08239" title="">https://arxiv.org/abs/2201.08239</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Tian, E.&nbsp;Mitchell, H.&nbsp;Yao, C.&nbsp;D. Manning, and C.&nbsp;Finn.

</span>
<span class="ltx_bibblock">Fine-tuning language models for factuality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">ICLR</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265158181" title="">https://api.semanticscholar.org/CorpusID:265158181</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Todorov et&nbsp;al. (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Todorov, T.&nbsp;Erez, and Y.&nbsp;Tassa.

</span>
<span class="ltx_bibblock">Mujoco: A physics engine for model-based control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems</em>, pages 5026–5033. IEEE, 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/IROS.2012.6386109" title="">10.1109/IROS.2012.6386109</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei,
N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">ArXiv preprint</em>, abs/2307.09288, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;H. Trinh, Y.&nbsp;Wu, Q.&nbsp;V. Le, H.&nbsp;He, and T.&nbsp;Luong.

</span>
<span class="ltx_bibblock">Solving olympiad geometry without human demonstrations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Nature</em>, 625(7995):476–482, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Breugel et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Van&nbsp;Breugel, Z.&nbsp;Qian, and M.&nbsp;Van Der&nbsp;Schaar.

</span>
<span class="ltx_bibblock">Synthetic data, real errors: how (not) to publish and use synthetic
data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">International Conference on Machine Learning</em>, pages
34793–34808. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vezhnevets et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;S. Vezhnevets, J.&nbsp;P. Agapiou, A.&nbsp;Aharon, R.&nbsp;Ziv, J.&nbsp;Matyas, E.&nbsp;A.
Duéñez-Guzmán, W.&nbsp;A. Cunningham, S.&nbsp;Osindero, D.&nbsp;Karmon, and
J.&nbsp;Z. Leibo.

</span>
<span class="ltx_bibblock">Generative agent-based modeling with actions grounded in physical,
social, or digital space using concordia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">ArXiv preprint</em>, abs/2312.03664, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.03664" title="">https://arxiv.org/abs/2312.03664</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Villalobos, J.&nbsp;Sevilla, L.&nbsp;Heim, T.&nbsp;Besiroglu, M.&nbsp;Hobbhahn, and A.&nbsp;Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">ArXiv preprint</em>, abs/2211.04325, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.04325" title="">https://arxiv.org/abs/2211.04325</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Wang, Y.&nbsp;Xie, Y.&nbsp;Jiang, A.&nbsp;Mandlekar, C.&nbsp;Xiao, Y.&nbsp;Zhu, L.&nbsp;Fan, and
A.&nbsp;Anandkumar.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">ArXiv preprint</em>, abs/2305.16291, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.16291" title="">https://arxiv.org/abs/2305.16291</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;Le, E.&nbsp;Chi, S.&nbsp;Narang, A.&nbsp;Chowdhery, and
D.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language
models.

</span>
<span class="ltx_bibblock">2022a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.11171" title="">https://arxiv.org/abs/2203.11171</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, Y.&nbsp;Kordi, S.&nbsp;Mishra, A.&nbsp;Liu, N.&nbsp;A. Smith, D.&nbsp;Khashabi, and
H.&nbsp;Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated
instructions.

</span>
<span class="ltx_bibblock">volume abs/2212.10560, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.10560" title="">https://arxiv.org/abs/2212.10560</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, X.&nbsp;Wang, B.&nbsp;An, D.&nbsp;Yu, and C.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Towards faithful neural table-to-text generation with
content-matching constraints.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1072–1086, Online, 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2020.acl-main.101" title="">10.18653/v1/2020.acl-main.101</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.101" title="">https://aclanthology.org/2020.acl-main.101</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, A.&nbsp;Suriawinata, L.&nbsp;Vaickus, B.&nbsp;Ren, X.&nbsp;Liu, J.&nbsp;Wei, and S.&nbsp;Hassanpour.

</span>
<span class="ltx_bibblock">Generative image translation for data augmentation in colorectal
histopathology images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">Advances in Neural Information Processing Systems</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, M.&nbsp;Bosma, V.&nbsp;Y. Zhao, K.&nbsp;Guu, A.&nbsp;W. Yu, B.&nbsp;Lester, N.&nbsp;Du, A.&nbsp;M. Dai,
and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, L.&nbsp;Hou, A.&nbsp;Lampinen, X.&nbsp;Chen, D.&nbsp;Huang, Y.&nbsp;Tay, X.&nbsp;Chen, Y.&nbsp;Lu,
D.&nbsp;Zhou, T.&nbsp;Ma, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Symbol tuning improves in-context learning in language models.

</span>
<span class="ltx_bibblock">volume abs/2305.08298, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.08298" title="">https://arxiv.org/abs/2305.08298</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, D.&nbsp;Huang, Y.&nbsp;Lu, D.&nbsp;Zhou, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Simple synthetic data reduces sycophancy in large language models,
2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.03958" title="">https://arxiv.org/abs/2308.03958</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, C.&nbsp;Yang, X.&nbsp;Song, Y.&nbsp;Lu, N.&nbsp;Hu, D.&nbsp;Tran, D.&nbsp;Peng, R.&nbsp;Liu, D.&nbsp;Huang,
C.&nbsp;Du, and Q.&nbsp;V. Le.

</span>
<span class="ltx_bibblock">Long-form factuality in large language models.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:268724304" title="">https://api.semanticscholar.org/CorpusID:268724304</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wei, Z.&nbsp;Wang, J.&nbsp;Liu, Y.&nbsp;Ding, and L.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Magicoder: Source code is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">ArXiv preprint</em>, abs/2312.02120, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.02120" title="">https://arxiv.org/abs/2312.02120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Weidinger, J.&nbsp;Mellor, M.&nbsp;Rauh, C.&nbsp;Griffin, J.&nbsp;Uesato, P.-S. Huang, M.&nbsp;Cheng,
M.&nbsp;Glaese, B.&nbsp;Balle, A.&nbsp;Kasirzadeh, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ethical and social risks of harm from language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">ArXiv preprint</em>, abs/2112.04359, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.04359" title="">https://arxiv.org/abs/2112.04359</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wood et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Wood, T.&nbsp;Baltrusaitis, C.&nbsp;Hewitt, S.&nbsp;Dziadzio, T.&nbsp;J. Cashman, and
J.&nbsp;Shotton.

</span>
<span class="ltx_bibblock">Fake it till you make it: face analysis in the wild using synthetic
data alone.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">2021 IEEE/CVF International Conference on Computer Vision,
ICCV 2021, Montreal, QC, Canada, October 10-17, 2021</em>, pages 3661–3671.
IEEE, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/ICCV48922.2021.00366" title="">10.1109/ICCV48922.2021.00366</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCV48922.2021.00366" title="">https://doi.org/10.1109/ICCV48922.2021.00366</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Xu, Q.&nbsp;Sun, K.&nbsp;Zheng, X.&nbsp;Geng, P.&nbsp;Zhao, J.&nbsp;Feng, C.&nbsp;Tao, and D.&nbsp;Jiang.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex
instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">ArXiv preprint</em>, abs/2304.12244, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.12244" title="">https://arxiv.org/abs/2304.12244</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Xu, Y.&nbsp;Ruan, W.&nbsp;Bi, G.&nbsp;Huang, S.&nbsp;Shi, L.&nbsp;Chen, and L.&nbsp;Liu.

</span>
<span class="ltx_bibblock">On synthetic data for back translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 419–430, Seattle, United States, 2022. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.naacl-main.32" title="">10.18653/v1/2022.naacl-main.32</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.32" title="">https://aclanthology.org/2022.naacl-main.32</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua,
and C.&nbsp;Raffel.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">arXiv preprint arXiv:2010.11934</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Yang, A.&nbsp;Prabhakar, K.&nbsp;Narasimhan, and S.&nbsp;Yao.

</span>
<span class="ltx_bibblock">Intercode: Standardizing and benchmarking interactive coding with
execution feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Ye, S.&nbsp;Li, G.&nbsp;Li, C.&nbsp;Huang, S.&nbsp;Gao, Y.&nbsp;Wu, Q.&nbsp;Zhang, T.&nbsp;Gui, and X.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Toolsword: Unveiling safety issues of large language models in tool
learning across three stages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">ArXiv preprint</em>, abs/2402.10753, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.10753" title="">https://arxiv.org/abs/2402.10753</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Yu, W.&nbsp;Jiang, H.&nbsp;Shi, J.&nbsp;Yu, Z.&nbsp;Liu, Y.&nbsp;Zhang, J.&nbsp;T. Kwok, Z.&nbsp;Li, A.&nbsp;Weller,
and W.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Metamath: Bootstrap your own mathematical questions for large
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">ArXiv preprint</em>, abs/2309.12284, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.12284" title="">https://arxiv.org/abs/2309.12284</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Yu, Y.&nbsp;Zhuang, J.&nbsp;Zhang, Y.&nbsp;Meng, A.&nbsp;J. Ratner, R.&nbsp;Krishna, J.&nbsp;Shen, and
C.&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Large language model as attributed training data generator: A tale of
diversity and bias.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Yuan, R.&nbsp;Y. Pang, K.&nbsp;Cho, S.&nbsp;Sukhbaatar, J.&nbsp;Xu, and J.&nbsp;Weston.

</span>
<span class="ltx_bibblock">Self-rewarding language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">ArXiv preprint</em>, abs/2401.10020, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.10020" title="">https://arxiv.org/abs/2401.10020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yuan, H.&nbsp;Yuan, C.&nbsp;Li, G.&nbsp;Dong, C.&nbsp;Tan, and C.&nbsp;Zhou.

</span>
<span class="ltx_bibblock">Scaling relationship on learning mathematical reasoning with large
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">ArXiv preprint</em>, abs/2308.01825, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.01825" title="">https://arxiv.org/abs/2308.01825</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Zelikman, Y.&nbsp;Wu, and N.&nbsp;D. Goodman.

</span>
<span class="ltx_bibblock">Star: Bootstrapping reasoning with reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">NeurIPS</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:247762790" title="">https://api.semanticscholar.org/CorpusID:247762790</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Zhang, X.&nbsp;Xu, and S.&nbsp;Deng.

</span>
<span class="ltx_bibblock">Exploring collaboration mechanisms for llm agents: A social
psychology view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">ArXiv preprint</em>, abs/2310.02124, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.02124" title="">https://arxiv.org/abs/2310.02124</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, L.&nbsp;Dong, X.&nbsp;Li, S.&nbsp;Zhang, X.&nbsp;Sun, S.&nbsp;Wang, J.&nbsp;Li, R.&nbsp;Hu, T.&nbsp;Zhang,
F.&nbsp;Wu, and G.&nbsp;Wang.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey,
2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.10792" title="">https://arxiv.org/abs/2308.10792</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, Y.&nbsp;Li, L.&nbsp;Cui, D.&nbsp;Cai, L.&nbsp;Liu, T.&nbsp;Fu, X.&nbsp;Huang, E.&nbsp;Zhao, Y.&nbsp;Zhang,
Y.&nbsp;Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: A survey on hallucination in large
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">ArXiv preprint</em>, abs/2309.01219, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.01219" title="">https://arxiv.org/abs/2309.01219</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhang, R.&nbsp;Zhang, J.&nbsp;Gu, Y.&nbsp;Zhou, N.&nbsp;Lipka, D.&nbsp;Yang, and T.&nbsp;Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">ArXiv preprint</em>, abs/2306.17107, 2023d.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.17107" title="">https://arxiv.org/abs/2306.17107</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Zhao, B.&nbsp;Wu, and T.&nbsp;Huang.

</span>
<span class="ltx_bibblock">Svit: Scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">ArXiv preprint</em>, abs/2307.04087, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.04087" title="">https://arxiv.org/abs/2307.04087</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Zhao, T.&nbsp;Wang, M.&nbsp;Yatskar, V.&nbsp;Ordonez, and K.-W. Chang.

</span>
<span class="ltx_bibblock">Gender bias in coreference resolution: Evaluation and debiasing
methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</em>, pages 15–20, New Orleans, Louisiana,
2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/N18-2003" title="">10.18653/v1/N18-2003</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N18-2003" title="">https://aclanthology.org/N18-2003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Zheng, W.-L. Chiang, Y.&nbsp;Sheng, S.&nbsp;Zhuang, Z.&nbsp;Wu, Y.&nbsp;Zhuang, Z.&nbsp;Lin, Z.&nbsp;Li,
D.&nbsp;Li, E.&nbsp;P. Xing, H.&nbsp;Zhang, J.&nbsp;E. Gonzalez, and I.&nbsp;Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zheng, A.&nbsp;Trott, S.&nbsp;Srinivasa, D.&nbsp;C. Parkes, and R.&nbsp;Socher.

</span>
<span class="ltx_bibblock">The ai economist: Taxation policy design via two-level deep
multiagent reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">Science advances</em>, 8(18):eabk2607, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Zheng, H.&nbsp;Zhou, S.&nbsp;Huang, L.&nbsp;Li, X.&nbsp;Dai, and J.&nbsp;Chen.

</span>
<span class="ltx_bibblock">Mirror-generative neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HkxQRTNYPH" title="">https://openreview.net/forum?id=HkxQRTNYPH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Zhou, Z.&nbsp;Su, T.&nbsp;Eisape, H.&nbsp;Kim, and M.&nbsp;Sap.

</span>
<span class="ltx_bibblock">Is this the real life? is this just fantasy? the misleading success
of simulating social interactions with llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">ArXiv preprint</em>, abs/2403.05020, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05020" title="">https://arxiv.org/abs/2403.05020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Ziems, J.&nbsp;Dwivedi-Yu, Y.-C. Wang, A.&nbsp;Halevy, and D.&nbsp;Yang.

</span>
<span class="ltx_bibblock">Normbank: A knowledge bank of situational social norms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">ArXiv preprint</em>, abs/2305.17008, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.17008" title="">https://arxiv.org/abs/2305.17008</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Zou, Z.&nbsp;Wang, J.&nbsp;Z. Kolter, and M.&nbsp;Fredrikson.

</span>
<span class="ltx_bibblock">Universal and transferable adversarial attacks on aligned language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">ArXiv preprint</em>, abs/2307.15043, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.15043" title="">https://arxiv.org/abs/2307.15043</a>.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Chinese Tiny LLM:\n' +
      '\n' +
      '중국 중심 대용량 언어 모델 사전 학습\n' +
      '\n' +
      'Xinrun Du\\({}^{1}\\)\n' +
      '\n' +
      '동일 기술 기여도.\n' +
      '\n' +
      'Zhouliang Yu\\({}^{5}\\)\n' +
      '\n' +
      'Corresponding Authors.\n' +
      '\n' +
      'Songyang Gao\\({}^{2}\\)\n' +
      '\n' +
      '동일 기술 기여도.\n' +
      '\n' +
      'Ding Pan\\({}^{5}\\)\n' +
      '\n' +
      'Yuyang Cheng\\({}^{3}\\)\n' +
      '\n' +
      'Ziyang Ma\\({}^{4}\\)\n' +
      '\n' +
      'Ruibin Yuan\\({}^{5}\\)\n' +
      '\n' +
      'Xingwei Qu\\({}^{1}\\)\n' +
      '\n' +
      'Jiaheng Liu\\({}^{1}\\)\n' +
      '\n' +
      'Tianyu Zheng\\({}^{1}\\)\n' +
      '\n' +
      'Xinchen Luo\\({}^{7}\\)\n' +
      '\n' +
      'Guorui Zhou\\({}^{7}\\)\n' +
      '\n' +
      'Binhang Yuan\\({}^{5}\\)\n' +
      '\n' +
      'Wenhu Chen\\({}^{1}\\)\\({}^{6}\\)\\({}^{8}\\)\n' +
      '\n' +
      'Jie Fu\\({}^{1}\\)\\({}^{5}\\)\n' +
      '\n' +
      'Ge Zhang\\({}^{1}\\)\\({}^{6}\\)\\({}^{8}\\)\\({}^{*}\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 연구에서는 LLM 개발에 있어 중국어의 우선순위를 결정하는 중추적인 변화를 보여주는 2B 대형 언어 모델인 CT-LLM을 소개한다. 처음부터 독특하게 시작된 CT-LLM은 8,000억 개의 중국 토큰, 3,000억 개의 영국 토큰 및 1,000억 개의 코드 토큰을 포함한 1,000억 개의 토큰의 광범위한 코퍼스를 사용하여 주로 중국 텍스트 데이터를 통합함으로써 기존 방법론과 다르다. 이러한 전략적 구성은 모델의 탁월한 중국어 이해 및 처리 능력을 촉진하며, 이는 정렬 기술을 통해 더욱 향상된 능력이다. CHC-Bench에서 놀라운 성능을 보여주는 CT-LLM은 중국어 과제에 탁월하며 SFT를 통해 영어에 능숙함을 보여줍니다. 본 연구는 영어 말뭉치를 중심으로 LLM을 교육하는 기존의 패러다임에 도전하고, 이를 다른 언어에 적응시켜 LLM 교육 방법론에 대한 지평을 넓힌다. 획득한 대용량 적정 사전 훈련 중국 말뭉치(MAP-CC), 잘 선택된 다학제 중국 하드 케이스 벤치마크(CHC-Bench), 2B 크기의 중국 소형 LLM(CT-LLM)으로 상세한 데이터 처리 절차를 포함하여 중국 LLM을 훈련하는 전체 프로세스를 오픈 소싱함으로써 학계와 산업계에서 더 많은 탐색과 혁신을 촉진하여 보다 포괄적이고 다재다능한 언어 모델의 길을 열어주고자 한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '급성장하는 언어 지능 분야에서 대규모 언어 모델(LLM)은 자연어 처리(NLP)의 초석으로 부상하여 인간 언어를 이해하고 생성하는 데 놀라운 능력을 보여준다. 주로 영어 데이터 세트에 대해 훈련된 이러한 모델은 계산 언어학을 크게 발전시켜 다양한 작업에 걸쳐 새로운 벤치마크를 설정한다. 그러나 영어에 대한 이러한 강조는 인간 언어에 내재된 언어적 다양성을 무색하게 하고 LLMs의 적용 가능성과 혁신의 범위를 제한한다. 비영어 언어, 특히 시작부터 그러한 언어의 복잡성과 뉘앙스를 통합하는 언어에 기반을 둔 LLM의 개발은 상대적으로 미지의 영역으로 남아 있다.\n' +
      '\n' +
      '본 연구는 중국어 우선 순위로 이동하여 LLM의 경관을 재정의하기 위한 선구적인 시도인 Chinese Tiny LLM(CT-LLM)을 소개한다. CT-LLM은 20억 개의 매개변수를 가지고 있으며 1,200억 개의 토큰으로 구성된 포괄적인 코퍼스에서 세심하게 사전 훈련되어 전통적인 접근법에서 분기된다. 구성이 뚜렷한 이 말뭉치는 8,000억 개의 중국어 토큰, 3,000억 개의 영어 토큰, 1,000억 개의 코드 토큰의 광범위한 컬렉션을 포함한다. 우리의 신중한 데이터 처리 절차는 대규모 적절한 사전 훈련 중국 말뭉치(MAP-CC)를 제공하여 중국 웹 말뭉치의 품질을 향상시키고 현장에서 데이터 세트 준비를 위한 새로운 표준을 설정한다. 다양하고 상당한 양의 중국어 텍스트 데이터를 전략적으로 포함하면 CT-LLM이 중국어를 처리하고 이해하는 탁월한 능력을 달성할 수 있어 LLM 역량에 대한 새로운 선례를 세울 수 있다.\n' +
      '\n' +
      '본 연구의 접근 방법은 지도 미세 조정(supervised fine-tuning, SFT)을 통해 모델의 역량을 더욱 구체화한다. SFT는 중국어 작업에 대한 모델의 능숙함을 강화할 뿐만 아니라 영어 텍스트를 이해하고 생성하는 데 있어 다양성을 향상시켜 다국어 능력을 보여준다. 또한 선호도 최적화 기술을 사용하여 CT-LLM을 인간의 선호도와 정렬하여 무해성과 유용성을 향상시킨다. 또한, 다학제성을 가진 중국어 Hard Case Benchmark(CHC-Bench)를 구축하여 중국어의 교수 이해 및 추종 능력을 측정하고, CT-LLM이 괄목할 만한 성과를 보여주고 있다. CT-LLM은 주로 영어 코퍼라에 대한 LLMs 교육의 일반적인 규범에 도전함으로써 언어 모델 교육의 지평을 확장하여 비영어 중심 LLMs의 잠재력에 대한 새로운 관점을 제공한다.\n' +
      '\n' +
      '우리의 연구의 핵심은 대규모 적절한 사전 훈련 중국 말뭉치(MAP-CC)를 선별하기 위해 수행된 세심한 데이터 처리 절차와 다학제 중국 하드 케이스 벤치마크(CHC-Bench) 구축을 포함하여 CT-LLM에 대한 전체 훈련 프로세스의 오픈 소싱이다. 방법론과 연구 결과의 보급을 통해 향후 LLM 개발을 위한 보다 포괄적이고 다양한 경관을 육성하여 광범위한 인간 언어 및 문화를 더 잘 반영하는 모델의 탐색을 장려하는 것을 목표로 한다. 우리의 기여는 세 가지입니다.\n' +
      '\n' +
      '**MAP-CC** NLP 커뮤니티 고품질 중국 사전 훈련 데이터 및 데이터 준비를 위한 효과적인 방법론을 제공하는 중국 웹 말뭉치를 청소하기 위한 세부 절차 모음과 함께 8천억 토큰 규모의 오픈 소스 중국 사전 훈련 데이터 세트입니다.\n' +
      '\n' +
      '**CHC-벤치** 잘 선택된 다학제 중국 하드 케이스 교육 이해 및 벤치마크 따르기.\n' +
      '\n' +
      '**CT-LLM** 첫 번째 중국 중심 대규모 언어 모델은 주로 중국 말뭉치에 대해 사전 훈련 및 미세 조정 모두 중국 언어 능력과 다국어 적응성에 대한 중요한 통찰력을 제공합니다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '### LLM with Chinese Language Ability\n' +
      '\n' +
      'LLM 분야에서 기술의 발전은 놀라운 언어 능력을 나타내는 일련의 오픈 소스 모델의 개발을 촉진했다. 특히 LLaMA(Touvron et al., 2023; Zhu et al., 2023), Phi(Li et al., 2023; Gunasekar et al., 2023), Mistral(Jiang et al., 2023), Gemma(Team et al., 2024) 등의 모델이 선두 주자로 등장하여 이 분야에서 이루어진 기술 진보를 강조하고 있다. 세계화된 맥락에서, 이중 언어 또는 다중 언어 기능에 능숙한 모델, 특히 광범위한 중국어 응용 프로그램을 수용하는 모델에 대한 수요가 증가하고 있다. 이러한 요구는 지역화된 해결책에 대한 욕구와 전 세계적으로 언어적 분열을 해소해야 할 필요성에서 비롯된다. 이러한 요구를 해결하기 위해, 사전 훈련 단계 동안 더 높은 비율의 중국 토큰들을 통합하거나 중국어 기능들을 활성화하기 위해 감독 미세 조정(supervised fine-tuning; SFT)과 같은 기법들을 채용하는 것에 상당한 강조를 두고, LLMs들의 다국어 능력들을 향상시키기 위해 몇 가지 전략들이 채용되었다(Zeng 등, 2023; Bai 등, 2023; Yang 등, 2023; Team, 2023; Young 등, 2024; Bi 등, 2024). 이 노력의 초기 예는 ChatGLM(Zeng et al., 2023)이며, 이는 사전 훈련 단계에서 중국어와 영어 토큰의 동등한 분포의 사용을 개척하여 능숙한 이중 언어 모델에서 최고조에 달했다. 이후 오웬(Bai et al., 2023)과 같은 모델은 사전 훈련 과정에서 다국어 데이터를 통합하여 언어 지평을 확장하여 보다 광범위한 언어 지원을 달성했다. 또한, Yi(Young et al., 2024) 및 DeepSeek(Bi et al., 2024)와 같은 모델은 중국어 추론에서 주목할 만한 능력과 함께 다국어 능력을 잠금 해제하는 데 세심한 SFI 애플리케이션의 효능을 입증했다. 그러나 이러한 발전에도 불구하고 주로 중국어를 주요 언어로 활용하는 중국 중심 LLM의 존재는 여전히 불확실하다. 이 격차는 지역화되고 오픈 소스인 중국 모델을 개발하는 데 중요한 관심 영역을 강조하여 언어 기술의 진화에서 맞춤형 접근법의 중요성을 강조한다.\n' +
      '\n' +
      '### 사전 훈련 및 정렬을 위한 중국 코퍼마\n' +
      '\n' +
      '데이터 사전 훈련은 언어 모델을 개발하는 데 필수적이며, 이러한 모델이 인간 언어를 배우고 이해할 수 있는 기반을 제공한다. 영어 데이터의 풍부함이 영어 LLM의 발전에 크게 기여했지만 중국 사전 훈련 데이터의 풍경은 방대한 잠재력과 주목할 만한 희소성의 대조를 보여준다. 중국 인터넷에서 이용할 수 있는 방대한 양의 데이터에도 불구하고 중국 사전 훈련 데이터 세트는 상대적으로 드물어 다양성과 품질에 대한 우려가 제기되고 있다. YaYi(Luo et al., 2023), SkyPile(Wei et al., 2023) 및 Wudao(Yuan et al., 2021)는 오픈 소스 콘텐츠를 세심하게 큐레이팅하여 고구경 리소스를 구성하지만, 제한된 양은 포괄적인 모델 훈련을 용이하게 하는 효능을 제한한다. 반대로, 우다오는 데이터 품질의 상당한 변동성과 포맷의 라인 끊김에 대한 무시로 인해 실질적인 구현에 어려움이 있지만 광범위한 중국 교육 자원을 자랑한다. ChineseWebText는 데이터 품질과 양 사이의 우수한 균형을 이루어 현재 사전 훈련 노력에 선호됩니다. 텔레챗(Wang et al., 2024) 및 CCI(BAAI, 2023)와 같은 특정 대체 데이터 세트는 허용 가능한 품질을 나타내지만 불충분한 양을 나타낸다. 이러한 데이터 세트는 데이터 수집 및 필터링을 위해 SkyPile과 같은 방법을 사용하여 다른 말뭉치에 대한 추가 리소스로 작용한다. 또한 COIG 시리즈(Zhang et al., 2023; Zheng et al., 2024)는 SFT 데이터로 분류되지만 방대한 부피로 인해 대규모 사전 훈련 응용 프로그램에 대한 가능성을 가지고 있다. 전반적으로, 기존의 사전 훈련 데이터 세트는 양이 부족하거나 품질이 손상되어 중국어를 중심으로 대규모 모델 사전 훈련을 탐색해야 하는 필요성을 강조한다. 이러한 탐구는 현대 중국어 데이터의 특이성을 식별하고 텍스트 중국 자원을 활용하고 이해하기 위한 새로운 길을 식별하는 데 중추적이다.\n' +
      '\n' +
      '### 다국어 용량 출현\n' +
      '\n' +
      'LLM 개발의 일반적인 패러다임은 주로 영어 중심의 사전 훈련 방법론을 선호했다. 영문 데이터의 방대한 가용성과 글로벌 편재성에 뿌리를 둔 이 접근법은 대부분의 현대 LLM 아키텍처에 대한 기초 기반을 설정했다. 이어서, 이러한 모델들의 언어적 도달 범위를 확장하여, 다국어 능력들의 활성화를 가능하게 하기 위해, 지속적인 사전 훈련, 감독된 미세-조정, 및 명령어 미세-조정(IFT)과 같은 전략들이 사용되었다(Zeng 외, 2023; Bai 외, 2023; Yang 외, 2023; Team, 2023; Young 외, 2024; Bi 외, 2024). 이러한 방법론은 중국어-믹스트랄(Cui and Yao, 2024)과 중국어-믹스트랄-인스트럭션(Cui and Yao, 2024)의 대표적인 예를 들어 초기 영어 중심 교육을 넘어 언어 다양성을 수용하는 LLM의 적응성을 보여주면서 효과적인 것으로 입증되었다. 이러한 적응 전략 외에도 처음부터 다국어 숙련도를 위해 특별히 설계된 모델의 하위 집합이 있다. BLOOM(Le Scao et al., 2022) 및 Aya(Ustin et al., 2024)와 같은 모델은 사전 훈련 및 미세 조정 단계 모두에 걸쳐 다수의 언어를 통합하는 이러한 접근법을 예시한다. 언어적 다양성을 통합하려는 이러한 노력에도 불구하고, 영어는 항상 이러한 모델들 내에서 지배적인 언어로 남아 있다(Zhao et al., 2024). 이 담론에서 우리는 영어와 같은 다른 언어의 숙련도를 활성화하기 위한 중국 중심의 사전 훈련의 실현 가능성이라는 영어 중심의 지배적 패러다임에 도전하는 반내러티브를 탐구한다. 사전 교육을 위한 기본 언어로 중국어를 고려함으로써 이러한 모델이 추가 언어에서 효과적으로 능력을 습득하고 입증할 수 있는지 여부를 조사한다. 중국 중심 접근법의 성공은 언어 기술을 상당히 민주화하여 글로벌 언어 다양성을 반영하는 포괄적인 모델을 만드는 통찰력을 제공할 수 있다.\n' +
      '\n' +
      '## 3 Pretraining\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      '이전 연구(Hoffmann et al., 2022)는 데이터 세트의 크기가 대형 언어 모델의 성능에 상당한 영향을 미친다는 것을 확립했다. 동시에 데이터 세트의 다양성과 포괄성은 일반 도메인에 대한 대규모 언어 모델을 훈련하는 데 중요하다. 앞서 언급한 원칙과 모델 학습에 중국 말뭉치를 활용하는 데 중점을 두고 1,254.68억 토큰을 포함하는 데이터 세트를 개발했다. 이 데이터 세트는 8404억 8천만 개의 중국어 토큰, 3,148억 8천만 개의 영어 토큰 및 993억 개의 코드 토큰으로 구성된 중국어, 영어 및 코드 데이터를 통합한다. 데이터 세트는 커먼 크롤의 웹 문서, 학술 기사, 백과사전 및 책과 같은 다양한 소스의 콘텐츠를 집계합니다. 정확한 분포는 그림 1에 자세히 설명되어 있습니다. 우리의 데이터 세트에는 주로 영어로 된 약 1,100억 개의 복제 토큰이 포함되어 있습니다. 중복됨에도 불구하고 고품질이며 훈련에서 의도적으로 두 번 사용되었다.\n' +
      '\n' +
      '**휴리스틱 규칙** 낮은 품질의 데이터를 제거하는 데이터 필터링을 수행하기 위해 휴리스틱 규칙을 설계했습니다. 이러한 규칙은 여러 데이터 세트 및 모델, 특히 RefinedWeb(Penedo et al., 2023) 및 CC-Net(Wenzek et al., 2020)에서 파생된 방법론에서 영감을 받은 필터링 전략의 통합된 프레임워크를 나타내며, Gopher(Rae et al., 2022) 및 T5(Raffel et al., 2020)와 같은 다른 언어 모델을 훈련하면서 적용되는 일부 규칙도 포함한다. 또한 데이터 세트에 고유한 특성을 해결하기 위해 맞춤화된 규칙 세트를 개발했다.\n' +
      '\n' +
      '기존의 규칙은 주로 영어 데이터 필터링을 목표로 한다는 점을 언급할 필요가 있다. 따라서 우리는 중국 데이터 세트에 대한 규칙을 구체적으로 수정하고 수정한다. 이러한 규칙의 임계값과 세부사항은 데이터셋 내 샘플링 문서를 기반으로 분석을 통해 확인된다.\n' +
      '\n' +
      '우리의 초기 단계는 처리 효율성을 높이기 위해 데이터 형식을 표준화하는 것을 포함한다. 다음으로, 먼저 블랙리스트 T1에서 URL로 데이터를 제거한 다음 나머지 URL을 필터링하여 데이터 순도를 향상시키는 두 단계로 텍스트에서 URL을 제거한다. 또한 문장 수준 및 문서 필터링을 적용하여 너무 짧거나 품질이 낮거나 논리적 시퀀스가 부족한 텍스트를 제외하여 데이터 일관성과 관련성을 보장한다. 또한, n-gram과 문장을 포함한 중복 텍스트를 제거한다. 자세한 규칙은 부록 A로 나열됩니다.\n' +
      '\n' +
      '**중복 제거** 여과 프로세스를 구현한 후 포괄적인 중복 제거 파이프라인을 개발했습니다. 이 파이프라인에는 문서 수준의 정확한 중복 제거, 문서 수준의 민해시 중복 제거 및 문서 내 유사 줄 중복 제거가 포함되어 문서 내에서 중복된 내용을 효과적으로 식별하고 제거합니다. 정확한 중복제거를 위해 Bloom 필터를 사용하여 0.001로 설정된 위양성률로 근사한다. Minhash LSH의 경우, 시그니처는 128개의 해시 함수로부터 구성되고 LSH에 대해 9개의 밴드와 13개의 행으로 구성되어 0.8의 자카드 유사성을 달성한다. 문서 내 유사 선 중복제거는 단일 문서 내에서 반복적인 선을 제거하는 것을 목표로 한다. 이 접근법은 웹 크롤링된 데이터의 상당 부분이 동일한 페이지 내에서 2~3번의 반복을 포함하고 있으며 HTML에서 텍스트를 추출하는 프로세스로 인해 일부 단어가 손실되어 중복에 약간의 변형이 발생할 수 있다는 관찰에 의해 동기가 부여되었다. 이 중복 제거를 위해 편집 거리를 사용하여 선 유사성을 결정한다. 특정 기준은 편집 거리가 더 짧은 선의 길이의 10분의 1 미만인 경우 두 선이 유사한 것으로 간주된다는 것이다. 더 나아가, 이 필터링 프로세스를 가속화하기 위해, 우리는 문자 중첩의 비율을 계산한다.\n' +
      '\n' +
      '그림 1: 사전 학습 데이터 분포, 여기서 "zh"는 중국어 데이터를 나타내고, "en"은 영어 데이터를 나타내고, "cc"는 공개 가능한 웹 문서 등을 포함하는 Common Crawl을 나타내며, \'encyc.\'는 백과사전을 나타낸다.\n' +
      '\n' +
      '선; 3분의 1보다 작으면 선이 다른 것으로 간주됩니다. 전체 파이프라인 및 실제 필터링 및 중복 제거 비율은 그림 2에서 볼 수 있다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '우리 모델의 아키텍처는 변압기 디코더(Vaswani et al., 2017)를 기반으로 한다. 우리의 아키텍처를 정의하는 주요 매개변수는 표 1에 나와 있으며 모델은 4096 토큰의 상당한 컨텍스트 길이에 대해 훈련된다. 기본 요소를 넘어, 우리의 접근법은 원래 변압기에 비해 몇 가지 개선 사항을 통합한다.\n' +
      '\n' +
      '**Multi-Head Attention Mechanism.** 모델에서 Vaswani 등(2017)에서 설명한 Multi-Head Attention Mechanism을 사용합니다. 다양한 멀티헤드 어텐션을 채택하면 다양한 척도에 걸쳐 모델의 성능이 향상된다는 것이 Shazeer(2019)에 의해 입증되었다.\n' +
      '\n' +
      '**RoPE Embeddings**(Su et al., 2021). 우리의 아키텍처는 절대 위치 임베딩에 의존하는 대신 각 계층에 회전 위치 임베딩을 통합한다. 또한 전체 모델 크기를 최소화하기 위해 입력과 출력 간에 임베딩을 공유한다.\n' +
      '\n' +
      '**SwiGLU Activations**(Shazeer, 2020). 표준 ReLU 비선형성은 SwiGLU 활성화 함수로 대체된다.\n' +
      '\n' +
      '**RMSNorm** Llama2 모델(Touvron 등, 2023) 7B와 동일합니다. RMSNorm(Zhang and Sennrich, 2019)을 사용하여 각 변압기 하위 계층, 주의 계층 및 피드포워드 계층의 입력을 정규화한다.\n' +
      '\n' +
      '**토큰izer** 데이터 토큰화를 위해 SentencePiece(Kudo and Richardson, 2018)의 바이트 쌍 인코딩(BPE)(Shibata 등, 1999)을 사용하는 baichuan2 토큰izer(Yang 등, 2023)를 사용했습니다. 어휘 크기는 125,696개이며, 이 토큰화기는 숫자를 개별 숫자로 분할하도록 설계되어 숫자 데이터의 인코딩을 강화한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Parameters & Value \\\\ \\hline _d\\_model_ & 2,048 \\\\ Layers & 32 \\\\ Feedforward hidden dims & 5504 \\\\ Num heads & 16 \\\\ Num KV heads & 16 \\\\ Head size & 128 \\\\ Vocab size & 125,696 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 주요 모델 파라미터.\n' +
      '\n' +
      '도 2: 위는 데이터 처리 흐름 및 중복 제거 비율이고, 아래는 유사한 라인 중복 제거의 개략도이다.\n' +
      '\n' +
      '## 4 Supervised Finetununing\n' +
      '\n' +
      'Supervised Fine-Tuning (SFT)의 경우 중국어와 영어 데이터를 모두 사용했다. 중국 데이터는 CQIA(Bai et al., 2024) 및 OL-CC의 풀 세트뿐만 아니라 COIG-PC(Zhang et al., 2023)에서 샘플링된 고품질 데이터로 구성되었다. 영어 데이터는 OpenHermesPreferences dataset(Huang et al., 2024)에서 샘플링되었다. 중국어 데이터의 총량은 105K 쌍의 명령어 데이터로 구성되었으며, 영어 데이터는 중국어 데이터의 볼륨에 따라 다른 비율로 조정되었다. 비율은 \\(1:1,2:1,4:1,\\) 및 \\(8:1\\)였으며 중국어 데이터만 포함하고 영어 데이터만 포함하는 구성이다. 각 실험 세트는 3개의 에포크에 대해 훈련되었으며 특정 실험 결과는 표 12에 나와 있다.\n' +
      '\n' +
      '모델 학습에 사용된 하이퍼파라미터는 시퀀스 길이 2048, 전역 배치 크기 128, 최대 학습률 \\(2e^{-5}\\)이다. 과적합을 방지하기 위해 중량 감쇠는 0.1의 값으로 적용되고 구배 클리핑은 1.0의 한계로 시행된다.\n' +
      '\n' +
      'COIG-PC 데이터세트와 OpenHermesPreferences 데이터세트에서 고품질 세그먼트를 추출하기 위해 선택 메트릭으로 복잡도(ppl)를 사용한다. 구체적으로, 우리는 Qwen-7B(Bai et al., 2023) 모델을 사용하여 SFT 데이터세트로부터 도출된 샘플들에 대한 ppl을 계산한다. SFT 데이터 세트에 대한 데이터 필터링 프로세스에서 Qwen-7B에서 3,000 미만의 복잡도 점수를 가진 항목만 유지한다.\n' +
      '\n' +
      '## 5 인간 환경 설정에서 학습\n' +
      '\n' +
      'LLM의 무해하고 유용한 목적을 고려하여 DPO(Rafailov et al., 2024)를 활용하여 응답 쌍의 순위로부터 인간의 선호도를 직접 학습한다.\n' +
      '\n' +
      '**기본 설정 데이터 세트.* * 우리 모델은 공개적으로 액세스할 수 있는 데이터 세트와 LLM에서 합성 데이터를 혼합합니다. 오픈 소스 중국 데이터 세트는 LLama-factory (Zheng et al., 2024), huozi 및 zhihu의 _cvalues.rllf_, _comparison.gpt4_data_zh_ 및 _oast.rm_zh_에서 유해하지 않고 유익한 섹션으로 구성된다. 영어의 경우 데이터 세트에는 LLama-factory 및 Beavertails의 _comparison.gpt4_data_en_이 포함됩니다(Ji 등, 2024). 합성적 접근법을 통해 보다 높은 자질 선호도 데이터 세트를 구성하기 위해 GPT-4를 사용하여 "선택" 응답을 생성하는 알파카-gpt4(Peng 등, 2023)를 채택하고 "거부" 응답을 생성하기 위한 약한 모델 역할을 하는 바이촨-6B(Yang 등, 2023)를 채택한다. 데이터 세트는 총 183k 개의 중국어 쌍과 46k 개의 영어 쌍으로 구성된다.\n' +
      '\n' +
      '**훈련 설정.** CT-LLM의 SFT 버전을 참조 모델 \\(\\pi_{sft}\\)로 활용하여 목적 언어 모델 \\(\\pi_{\\theta}\\)을 최적화합니다. \\ (\\pi_{\\theta}\\)는 \\(\\pi_{sft}\\)의 모델 파라미터에 의해 초기화된다. 1. \\(\\pi_{\\theta}\\)는 8 H800, 2. 학습률 \\(=1e-6\\), 3. 배치 크기 \\(=4\\), 4. epoch 수 \\(=2\\), 5. 중량 감쇠 \\(=0.1\\), 6. 워밍업 비율 \\(=0.03\\), 7. \\(\\beta=0.5\\)로 학습하여 \\(\\pi_{sft}\\)로부터의 편차를 제어한다.\n' +
      '\n' +
      '**성능.** SFT 및 DPO 이후의 CT-LLM은 CT-LLM-SFT-DPO로 명명됩니다. MLU, COPA와 같은 일반 벤치마크에 대한 CT-LLM-SFT-DPO의 성능은 표 2에 게시되어 있다.\n' +
      '\n' +
      '## 6 Evaluations\n' +
      '\n' +
      '### 메트릭 결과\n' +
      '\n' +
      '평가 데이터 세트 및 메트릭 우리의 평가는 강력한 평가를 위해 설계된 내부 평가 프레임워크를 활용하여 영어 및 중국어 모두에서 포괄적인 공개 벤치마크 제품군을 포함한다. 이러한 벤치마크에는 MMLU(Hendrycks et al., 2021), C-Eval(Huang et al., 2024) 및 CMMLU(Li et al., 2023)와 같은 여러 학문과 언어 이해 및 추론의 측면에 적합한 다양한 데이터 세트가 포함된다. 우리의 평가 전략은 복잡성 기반 평가를 사용하는 다중 선택에서 선택이 필요한 데이터 세트와 모델이 결과를 구문 분석하는 자유 텍스트를 생성하는 생성 기반 평가에 적합한 데이터 세트를 구별한다. 이 분할을 통해 언어 모델링에서 전문 지식 및 코드 생성에 이르기까지 각 데이터 세트의 특정 요구 사항에 맞는 전략을 사용할 수 있습니다. 평가 데이터의 전체 세부 사항은 표 8에 나와 있다.\n' +
      '\n' +
      '트레이닝 프로세스 및 비교 분석 트레이닝 진행은 다양한 데이터 세트에 걸쳐 일관된 개선 경향을 보여주며, 특히 언어 이해, 추론 및 도메인 특정 지식에서 볼 수 있다. 특히, HellaSwag, PIQA 및 ARC와 같은 데이터 세트는 향상된 추론 능력을 나타내는 현저한 개선을 보여준다. 이 모델은 수학(GSM8K, TheoremQA), 과학(ARC-c, ARC-e)과 같은 전문 분야에서 눈에 띄는 진전을 보여주며, 이러한 영역에 특정한 콘텐츠를 이해하고 생산하는 능력을 강조한다. 사전 훈련 과정 중 중간 체크포인트의 평가 결과는 표 4와 같다.\n' +
      '\n' +
      '영어 및 중국어 벤치마크에 대한 모델의 성능을 다른 모델과 비교한 결과, 표 2와 같이 MMLU 및 CMMLU와 같은 다학제 데이터 세트에서 성능의 격차가 현저히 작음을 알 수 있다. 다른 모델은 특히 언어 이해 및 추론 벤치마크에서 상당한 격차를 나타내지만, 우리 모델은 일관된 성능을 유지하여 다양한 도메인에 걸쳐 균형 잡힌 능력을 제안한다. 이는 우리의 모델이 MiniCPM(min, 2024) 및 Phi-2와 같은 대안과 밀접하게 경쟁하거나 능가하여 우월하거나 경쟁적인 추론 능력을 보여주는 헬라스웨그 데이터 세트와 같이 뚜렷한 변동성을 보여주는 다른 모델과 대조된다. 유사하게, 도메인별 평가(C-Eval 및 CMMLU)에서 우리 모델은 문화적 및 도메인별 컨텍스트에 대한 미묘한 이해가 필요한 콘텐츠를 이해하고 생성하는 데 있어 TinyLlama-1.1B 및 Bloom-1.7B와 같은 모델을 능가하는 공통적인 성능을 보여준다. 이 균형 잡힌 능력은 모델의 다양성과 적응성을 강조하여 AI 언어 모델의 풍경에서 강력한 경쟁자로 포지셔닝하며 광범위한 적용 가능성과 깊고 영역별 지식을 모두 수용할 수 있다.\n' +
      '\n' +
      '또한 표 3과 같이 중국어 대 영어 데이터(SFT)의 2:1 비율을 사용하여 미세 조정된 모델의 성능을 일반 벤치마크 및 중국어 벤치마크에 대한 다른 모델과 비교했다. 우리는 우리 모델의 중국어 능력이 특히 강하다는 것을 발견했다. 이 SFT 모델에 사용된 데이터 비율은 사전 훈련의 데이터와 일치한다. 우리는 그것의 전반적인 성능이 최고라는 것을 발견했다. 다른 비율로 훈련된 모델의 성능은 부록.E.2에서 확인할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c|c c c c c c} \\hline \\hline\n' +
      '**Model** & **COPA** & **Hellawag** & **MMLU** & **Humenaval** & **Triviaq** & **Lambda** & **Squad2.0** & **GSM8k** & **C-Eval** & **CMMLU** \\\\ \\hline Mwen1.5-1.8B & 53.0 & 55.99 & 47.06 & 18.99 & 31.15 & 56.39 & 30.06 & 35.1 & 59.38 & 57.11 \\\\ TinyLlama-1.1B & 51.0 & 54.47 & 25.89 & 8.54 & 31.27 & 59.71 & 20.85 & 5.36 & 26.16 & 25.04 \\\\ Stablen-3b-4c-1t & [6.1] & [6.0] & [4.5] & [15.4] & [15.8] & [50.5] & [70.35] & [86.44] & 10.92 & [31.71] & 31.48 \\\\ German-2b & 64.0 & [6.49] & 41.84 & [9.15] & [46.42] & [6.38] & [6.86] & [22.14] & 31.25 & 31.11 \\\\ Phi-2 & [7.2] & 67.24 & [37.6] & 0.0 & [41.04] & [62.7] & [34.81] & [6.14] & 31.53 & [32.19] \\\\ \\hline CT-LIM(Ours) & 59.0 & 50.37 & 37.11 & [9.15] & 21.03 & 56.24 & 18.87 & 8.87 & 36.28 & 36.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 벤치마크에서 유사한 규모의 CT-LLM 및 기타 기본 모델의 성능 비교. 가장 좋은 결과는 파란색, 두 번째 결과는 밑줄, 세 번째 결과는 파란색이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **COPA** & **Hellawag** & **MMLU** & **Humenaval** & **Triviaq** & **Lambda** & **Squad2.0** & **GSM8k** & **C-Eval** & **CMMLU** \\\\ \\hline MiniCPM-2b-8th-fp32 & [6.0] & 65.88 & [3.87] & [4.5] & [12.2] & [6.2] & [6.02] & [4.02] & [55.8] & [4.21] & [5.10] \\\\ Genuma-2b-1 & [6.0] & [56.68] & 37.71 & 0.0 & 25.0 & 55.91 & 18.46 & 15.69 & 32.3 & 33.07 \\\\ TinyLlama-1.1B-Chat-1.0 & 48.0 & 56.64 & 25.33 & 4.88 & (32.31) & [6.09] & [12.89] & [3.72] & [24.51] & [24.92] \\\\ Bloom-1.7B & 57.0 & 44.45 & 27.38 & 0.0 & 18.73 & 48.36 & 6.68 & 1.44 & 22.93 & 24.51 \\\\ DeepNeed-1.3B-instruct & 51.0 & 57.0 & 28.55 & 63.29 & 10.85 & 35.32 & 28.55 & 8.79 & 25.33 & 27.75 \\\\ OpenNeed-1.5B-Chat & 57.0 & 55.75 & [55.86] & 6.71 & 24.31 & 48.83 & (28.28) & [28.73] & [36.64] & [20.41] \\\\ Stablen-asephy-3B & 64.0 & [6.7] & [46.15] & [24.39] & [33.48] & [57.46] & 21.19 & [37.01] & [29.5 & 32.11] \\\\ \\hline CT-LLM-SF(Ours) & 64.0 & 52.93 & 39.95 & 10.37 & 22.88 & 51.93 & [35.18] & [19.18] & [41.54] & [41.48] \\\\ CT-LIM-SF(DPOMs) & [6.1] & 53.38 & 39.82 & 7.90 & 23.64 & 51.47 & 31.36 & 18.5 & 4\n' +
      '\n' +
      '### 중국어 하드 지침 이해 및 평가 수행\n' +
      '\n' +
      '이를 위해 지야(Zhang et al., 2022), 고카카오(Goakao), CIF-Bench(Li et al., 2024) 등 다양한 출처에서 문제를 수집하여 어려운 중국어 명령어 이해와 평가 벤치마크(CHC-Bench를 짧게 표현함)를 구성한다. CHC-Bench의 문제 범주에는 글쓰기, 인류 및 역사, 과학, 수학, 독해, 역할놀이, 중국어 이해의 어려운 사례(즉, 중국어 발음, 고대 중국어 이해 등)가 포함된다.\n' +
      '\n' +
      '**메트릭.** 20억 개의 매개 변수 모델의 한계를 고려할 때 평가 기준은 반응의 정확도를 넘어섭니다. 모델의 답변에서 유용성, 관련성, 정확성, 깊이, 창의성, 세부 수준 등의 요소를 추가적으로 고려한다. 이러한 포괄적인 방법은 모델의 응답 품질에 대한 상세한 평가를 가능하게 한다. 구체적으로, GPT-4(Achiam et al., 2023)를 사용하여 특정 문제 컨텍스트에서 테스트된 LLM으로부터 응답을 스코어링하고, 스코어링 프롬프트는 부록 C.2에서 이용가능하다. Zheng et al.(2024)로부터 스코어링 할당 프롬프트 템플릿을 번역한다.\n' +
      '\n' +
      '**결과.** CHC-Benchone에 대한 모델의 성능을 동일한 규모의 다른 모델과 비교한 결과는 표 6에 나와 있으며 더 큰 규모의 모델과의 비교는 부록.E.3에서 찾을 수 있습니다. CHC-Benchone에서 특정 도메인에서 모델의 전문성을 평가할 수 있습니다. 예를 들어, 코딩 작업을 위해 설계된 Deepseek-coder-1.3b-instruct는 높은 점수로 그 기술을 보여준다. 벤치마크 결과는 모델의 진정한 능력을 정확하게 반영하는 CHC-벤친의 고품질임을 확인한다. 비교 연구 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **39.9B** & **93.3B** & **306.6B** & **506.6B** & **706.6B** & **906.6B** & **Final** \\\\ \\hline Hellaswag & 33.3 & 38.72 & 44.67 & 46.77 & 47.81 & 49.16 & 50.37 \\\\ MMLU & 26.09 & 27.11 & 26.68 & 29.8 & 33.47 & 35.42 & 37.11 \\\\ Humaneval & 1.83 & 2.44 & 4.27 & 5.49 & 5.49 & 6.1 & 9.15 \\\\ GSM8k & 1.14 & 2.05 & 4.93 & 6.44 & 6.14 & 7.88 & 8.87 \\\\ C-Eval & 22.53 & 23.07 & 23.68 & 26.4 & 32.39 & 36.05 & 36.78 \\\\ CMMLU & 25.24 & 24.83 & 25.59 & 29.84 & 31.33 & 32.86 & 36.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 이 표는 39.9B부터 1200B까지의 상이한 열차 토큰의 모델에 대한 다양한 데이터 세트에 걸친 부분적인 사례 평가 결과를 보여준다. 모든 측정 결과는 부록.E.1에서 찾을 수 있다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline\n' +
      '**Model** & **Cvalues-MC (Acc%)** & **Cvalues-QA (Score)** \\\\ \\hline\n' +
      '**MiniCPM-2B-sft (min, 2024)** & 0.851 & 6.99 \\\\\n' +
      '**Bloom-1.7B** (Le Scao et al., 2022) & 0.468 & 1.19 \\\\\n' +
      '**Stablelm-zephyr-3B** (Tunstall et al., 2023) & 0.790 & 3.79 \\\\\n' +
      '**TinyLlama-1.1B-Chat-v1.0**(Zhang et al., 2024) & 0.502 & 1.48 \\\\\n' +
      '**Gemma-2b-it**(Team et al., 2024) & 0.705 & 6.09 \\\\\n' +
      '**Qwen1.5-1.8B-Chat**(Bai et al., 2023) & 0.551 & 6.72 \\\\ \\hline\n' +
      '**CT-LLM-SFT (우리)** & 0.699 & 5.09 \\\\\n' +
      '**CT-LLM-SFT-DPO (Ours)** & 0.795 & 5.61 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 다른 6개의 SLM에 대한 우리 모델의 안전성 평가 결과. 최적의 결과는 blue_2-best가 underline_이고, 세 번째의 결과는 blue_2-best가 underline_이고, 세 번째의 결과는 blue_fbox_이다.\n' +
      '\n' +
      '데이터 볼륨과 모델 크기가 클수록 성능이 향상됩니다. CT-LLM은 20억 개의 매개변수 범위 내에서 사회적 이해와 글쓰기에 탁월하여 중국 문화와 관련된 맥락에서 강한 성과를 보인다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 중국어에 맞춘 대규모 언어 모델인 CT-LLM을 개발해 중국어 처리와 다국어 적응력을 높이기 위해 8,000억 토큰에 사전 훈련한다. CT-LLM은 영어 데이터셋에 크게 의존하는 기존 모델과 달리 영어 및 코드 토큰을 포함한 중국어에 초점을 맞추어 LLM 연구에서 새로운 방향을 나타낸다. SFT와 같은 기술을 사용하여 중국어와 영어 모두에서 성능을 개선하고 CHC-벤치를 도입하여 복잡한 작업에서 모델의 성능을 평가한다. CT-LLM의 주요 기여는 고품질 중국 말뭉치와 CHC-벤치 제공, 편견 해결, 중국 중심의 LLM 발전 등이 있다. 이것은 광범위한 NLP 연구, 혁신 및 오픈 소스 커뮤니티에 대한 기여를 촉진한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* M. Chu (2024) 인용: SS1.\n' +
      '* J. Achiam, S. 애들러 아가르왈 Ahmad, I. Akkaya, F. Leoni, A. Almeida, J. Altenschmidt, S. 알트만 Anadkat, et al. (2023)Gpt-4 technical report. arXiv preprint arXiv:2303.08774. 인용: SS1.\n' +
      '* B. B. B. C. 중국 인터넷 코퍼스 (2023)Note: [https://data.baai.ac.cn/details/BAAI-CCIAccessed](https://data.baai.ac.cn/details/BAAI-CCIAccessed): 2024-03-27 Cited by: SS1.\n' +
      '* J. Bai, S. 배영 주종근 최광 당, 엑스 등영 팬우 계영 Han, F. Huang, et al. (2023)Qwen technical report. arXiv preprintabs/2309.16609. External Links: Link, 2309.16609 Cited by: SS1.\n' +
      '* Y. 배재석 두영 량영 진진 류종주 정석 장남 마지 왕래 원현우 황재장 천창림 양승 Ni, G. Zhang (2024)Coig-cqa: 중국어 교육 미세 조정에 필요한 것은 품질입니다. 외부 링크: 2403.01851 인용: SS1.\n' +
      '*X. 비덕천 천동대 동규 두진 Fu, et al.(2024)DeepSeek llm: scaling open-source language models with longtermism. arXiv preprintabs/2401.02954. External Links: Link, 2401.02954 Cited by: SS1.\n' +
      '* Y. Cui와 X 야오(2024)lm 언어 적응을 재고하는 것: 중국 혼합어에 대한 사례 연구. ArXiv preprintabs/2403.01851. External Links: Link, 2403.01851 Cited by: SS1.\n' +
      '* S. 구나세카르 장, J. 아네자, C. C. 테오도로 멘데스, A. 델 조르노, S. 고피 Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al.(2023) 교과서만 있으면 됩니다. ArXiv preprintabs/2306.11644. External Links: Link, 2306.11644 Cited by: SS1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c c c} \\hline \\hline\n' +
      '**Model** & **Overall** & **Hard Case** & **Social** & **Coding** & **Writing** & **Roleplaying** & **Math** & **Reading Comp.** & **Science** \\\\ \\hline Bloom-1.7B & 1.40 & 1.24 & 1.35 & 1.00 & 1.15 & 1.35 & 1.15 & 2.43 & 1.45 \\\\ German-2b-iet & 2.04 & 1.78 & 1.65 & 1.30 & 1.09 & 2.50 & 2.09 & 4.23 & 1.40 \\\\ TinyLlam-1.1B-Chatr\\(\\mapsto\\)1.0 & 2.08 & 1.78 & 2.20 & 2.70 & 1.55 & 1.70 & 1.53 & 3.73 & 1.60 \\\\ Deepseek-ceder-1.3b-instruct & 3.03 & 1.92 & 2.66 & 6.02 & 3.09 & 2.60 & 2.21 & 4.73 & 1.60 \\\\ Sukhlen-zepy-3b & 3.30 & [3.16] & 2.75 & [5.05] & 3.03 & 3.75 & 1.76 & 4.77 & 2.75 \\\\ Yuan22-Bft & 3.31 & 1.76 & 4.60 & 2.45 & 3.36 & 3.45 & 3.12 & [5.47] & 2.65 \\\\ Owen1.5-1.8B-Chat & 6.87 & 6.86 & 8.10 & 5.80 & 7.64 & 7.00 & 3.91 & 7.20 & 5.86 \\\\ MiniCMR-2b-iet-y2 & 6.85 & 6.81 & 7.30 & 8.95 & 8.00 & 7.05 & 5.18 & 6.33 & 5.70 \\\\ \\hline CT-LLM(Ours) & [3.99] & 3.05 & [5.0] & 4.05 & [4.5] & [4.10] & [3.21] & 4.93 & [3.50] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: CHC-Bench에서 약 2B 규모의 모델의 성능. 가장 좋은 결과는 파란색, 두 번째로 좋은 결과는 밑줄, 세 번째로 좋은 결과는 [fbox]* Hendrycks et al.(2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt에 있다. 대규모 멀티태스킹 언어 이해도 측정 제9회 국제학술대회에서, ICLR 2021, 오스트리아 가상행사, 2021년 5월 3일부터 7일까지. OpenReview.net, 2021. URL [https://openreview.net/forum?id=d7KBjm13GmQ](https://openreview.net/forum?id=d7KBjm13GmQ).\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. 래, 오리올 빈일스, 로랑 시프르 컴퓨팅 최적화 대용량 언어 모델, 2022를 훈련합니다.\n' +
      '* Huang 등(2024) Shengyi Costa Huang, Agustin Piqueres, Kashif Rasul, Philipp Schmid, Daniel Vila, and Lewis Tunstall. Open hermes preferences. [https://huggingface.co/datasets/argilla/OpenHermesPreferences] (https://huggingface.co/datasets/argilla/OpenHermesPreferences), 2024a.\n' +
      '* Huang et al. (2024) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteno Liu, Chancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-disc discipline chinese evaluation suite for foundation models. _ 신경 정보 처리 시스템_, 36, 2024b에서의 진보.\n' +
      '* Ji et al.(2024) Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 비버테일: 인간 선호 데이터 세트를 통해 llm의 안전 정렬을 개선합니다. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.\n' +
      '* Jiang 등(2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _ ArXiv preprint_, abs/2310.06825, 2023. URL [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825).\n' +
      '* Kudo and Richardson (2018) Taku Kudo and John Richardson. SentencePiece: 신경 텍스트 처리를 위한 간단하고 언어 독립적인 서브워드 토큰화기 및 디토큰화기. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 66-71, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL [https://aclanthology.org/D18-2012](https://aclanthology.org/D18-2012).\n' +
      '* Le Scao 등(2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. 2022년\n' +
      '* Li 등(2023a) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 중국어에 대한 방대한 멀티태스킹 언어 이해도 측정 ArXiv preprint_, abs/2306.09212, 2023a. URL [https://arXiv.org/abs/2306.09212](https://arXiv.org/abs/2306.09212).\n' +
      '* Li 등(2024) Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, et al. Cif-bench: A chinese instruction-following benchmark for evaluating the generalizability of large language models _ arXiv preprint arXiv:2402.13109_, 2024.\n' +
      '* Li et al.(2023b) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. ii: phi-1.5 기술 보고서만 있으면 됩니다. _ ArXiv preprint_, abs/2309.05463, 2023b. URL [https://arxiv.org/abs/2309.05463](https://arxiv.org/abs/2309.05463).\n' +
      '* Luo 등(2023) Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, et al. Yayi 2: 다국어 오픈 소스 대형 언어 모델_ ArXiv preprint_, abs/2312.14862, 2023. URL [https://arxiv.org/abs/2312.14862](https://arxiv.org/abs/2312.14862).\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 매에 대한 정제된 웹 데이터 세트: 웹 데이터 및 웹 데이터만 있는 선별된 말뭉치를 능가합니다. 2023.\n' +
      '*Penedo et al.(2024)*Peng et al.(2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. gpt-4를 사용한 명령 튜닝 _arXiv preprint arXiv:2304.03277_, 2023.\n' +
      '* Rae et al.(2015) Jack W. 라에, 세바스티안 보르게우, 트레보르 카이, 케이티 밀리칸, 조던 호프만, 프란시스 송, 존 아슬란데스, 사라 헨더슨, 로만 링, 수잔나 영, 엘라이자 러더포드, 톰 헤니건, 야콥 메닉, 알빈 카시어, 리차드 파웰, 나트 맥알레, 에이미 우, 에리히 엘젠, 시랜트 자야쿠마, 엘레나 부카야, 다비드 버드덴, 에이미 서더랜드, 카렌 시모니안, 미켈라 파그니, 로랑 시프레, 마리아 심포우, 니콜라 바부스키킨, 아단 클라크, 디에고 데 라스 카사스, 오렐리아 가이, 크리스 존스, 제임스 브래드베리, 매튜 존슨, 로라 바이딩거, 이손 가브리엘, 윌리엄 아이작, 에드 록하트, 사이먼 오신데로, 로라 리멜, 크리스 다이아, 오리올 빈얄, 카림 아유브, 제프 스탠웨이, 로라린 베넷, 데미스 하사비스, 코레이 카부 Scaling language models: Methods, analysis and insights from training gopher, 2022.\n' +
      '* Rafailov 등(2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.\n' +
      '* Raffel 등(2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계를 탐색합니다. _ J 마흐 배워요 Res._ , 21:140:1-140:67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html)입니다.\n' +
      '* Shazeer (2019) Noam Shazeer. 빠른 변압기 디코딩: 2019년, 하나의 쓰기 헤드만 있으면 됩니다.\n' +
      '* Shazeer (2020) Noam Shazeer. GLU 변형은 변압기를 개선합니다. _ ArXiv preprint_, abs/2002.05202, 2020. URL [https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)\n' +
      '* Shibata et al.(1999) Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. 바이트 쌍 인코딩: 패턴 매칭을 가속화하는 텍스트 압축 방식. 1999년.\n' +
      '* Su et al.(2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 로포르머: 회전식 위치 임베딩을 가진 향상된 변압기. _ ArXiv preprint_, abs/2104.09864, 2021. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).\n' +
      '* Team et al.(2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. _ ArXiv preprint_, abs/2403.08295, 2024. URL [https://arxiv.org/abs/2403.08295](https://arxiv.org/abs/2403.08295).\n' +
      '* 팀 (2023) InternLM 팀. Internlm: 점진적으로 향상된 기능을 가진 다국어 언어 모델. [https://github.com/InternLM/InternLM-techreport] (https://github.com/InternLM/InternLM-techreport), 2023.\n' +
      '* Touvron 등(2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ ArXiv preprint_, abs/2302.13971, 2023a. URL [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).\n' +
      '* Touvron 등(2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ ArXiv preprint_, abs/2307.09288, 2023b. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).\n' +
      '\n' +
      '* Tunstall 등(2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. _ arXiv preprint arXiv:2310.16944_, 2023.\n' +
      '* Ustin 등(2024) Ahmet Ustin, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D\'souza, Ghemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya 모델: An instruction fineetuned open-access multilingual language model. _ ArXiv preprint_, abs/2402.07827, 2024. URL [https://arxiv.org/abs/2402.07827](https://arxiv.org/abs/2402.07827).\n' +
      '* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 주목만 하시면 됩니다. 이사벨 가욘, 울리케 폰 룩스부르크, 사미 벤지오, 한나 M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 5998-6008, 2017a. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n' +
      '* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 주목만 하시면 됩니다. 이사벨 가욘, 울리케 폰 룩스부르크, 사미 벤지오, 한나 M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 5998-6008, 2017b. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n' +
      '* Wang et al.(2024) Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, et al. Telechat technical report. _ ArXiv preprint_, abs/2401.03804, 2024. URL [https://arxiv.org/abs/2401.03804](https://arxiv.org/abs/2401.03804).\n' +
      '* Wei et al.(2023) Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: A more open bilingual foundation model. _ ArXiv preprint_, abs/2310.19341, 2023. URL [https://arxiv.org/abs/2310.19341](https://arxiv.org/abs/2310.19341).\n' +
      '* Wenzek 등 (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: 웹 크롤 데이터에서 고품질 단일 언어 데이터 세트를 추출합니다. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pp. 4003-4012, Marseille, France, 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://aclanthology.org/2020.1rec-1.494](https://aclanthology.org/2020.1rec-1.494).\n' +
      '* Xu et al. (2023) Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. 가치: 중국 대형 언어 모델의 가치를 안전에서 책임으로 측정, 2023.\n' +
      '* Yang et al.(2023) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. _ ArXiv preprint_, abs/2309.10305, 2023. URL [https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305).\n' +
      '* Young et al.(2024) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. _ ArXiv preprint_, abs/2403.04652, 2024. URL [https://arxiv.org/abs/2403.04652](https://arxiv.org/abs/2403.04652).\n' +
      '*위안 등(2021) 사위안, 한위자오, 정샤오두, 밍딩, 샤오류, 유궈센, 쉬조우, 지린양, 제탕. 우다오코르포라: 언어 모델을 사전 훈련하기 위한 초대형 중국 말뭉치입니다. _ AI Open_, 2:65-68, 2021.\n' +
      '* Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: 개방형 이중 언어 사전 훈련 모델. *ICLR(Eleventh International Conference on Learning Representations)_, 2023. URL [https://openreview.net/forum?id=-AwfrrPUF](https://openreview.net/forum?id=-AwfrrPUF).\n' +
      '\n' +
      '* Zhang & Sennrich (2019) Biao Zhang and Rico Sennrich. 루트 평균 제곱 층 정규화. 인한나 Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\'Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pp. 12360-12371, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47ffe7f53b-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47ffe7f53b-Abstract.html)\n' +
      '*Zhang et al. (2023) Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 중국 공개 교육 일반론자: 예비 공개, 2023.\n' +
      '*장 등(2022) 자싱장, 뤼이간, 준지에왕, 유샹장, 린장, 핑양, 신유가오, 지웨이우, 샤오쿤동, 준칭허, 지안청주, 치양, 용펑황, 시유리, 양한우, 준유루, 신유주, 위펑천, 칭한, 쿤하오판, 루이왕, 하오왕, 샤오준우, 중선정, 및 총페이천. 펑신방 1.0: 중국 인지 지능의 기반이 되는 것_ CoRR_, abs/2209.02970, 2022.\n' +
      '* Zhang 등(2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: 오픈 소스 소형 언어 모델입니다. _ arXiv preprint arXiv:2401.02385_, 2024.\n' +
      '*Zhao et al.(2024) Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Shuanjing Huang. 영어를 넘어선 라마: 언어 능력 전이에 대한 실증적 연구 _ ArXiv preprint_, abs/2401.01055, 2024. URL [https://arxiv.org/abs/2401.01055](https://arxiv.org/abs/2401.01055).\n' +
      '* Zheng et al.(2024a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arenna. _ 신경 정보 처리 시스템_, 36, 2024a에서의 진보.\n' +
      '* Zheng et al.(2024b) Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Weixu Zhang, Xinrun Du, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, et al. Kun: Answer polishment for chinese self-alignment with instruction back-translation. _ ArXiv preprint_, abs/2401.06477, 2024b. URL [https://arxiv.org/abs/2401.06477](https://arxiv.org/abs/2401.06477).\n' +
      '* Zheng et al.(2024c) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. 라마팩토리: 100개 이상의 언어 모델의 통합된 효율적인 미세 조정입니다. _ arXiv preprint arXiv:2403.13372_, 2024c. URL [http://arxiv.org/abs/2403.13372](http://arxiv.org/abs/2403.13372).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '## 부록 B 사전 학습 평가 데이터 세트\n' +
      '\n' +
      '## 부록 C CHC-Bench 세부 정보\n' +
      '\n' +
      '다음 표는 CHC-Bench 10의 조성을 나타낸다.\n' +
      '\n' +
      '### Hard-Case 문제 사례 연구\n' +
      '\n' +
      '이 섹션에서는 선택한 다학제 중국 하드케이스 지도 이해에 대한 몇 가지 시연과 CHC-Bench에서 사용되는 다음 문제 세트를 나열한다. 문제 범주의 구체적인 분류는 표 10에 나열되어 있다.\n' +
      '\n' +
      '**CHC-Bench가 LLMs에 어려운 이유** CHC-Bench는 LLMs가 중국 문화, 역사 및 전통에 대한 광범위한 이해와 중국 맥락 내에서 인문, 지리 및 STEM 주제에 대한 견고한 파악을 필요로 합니다. 문학적, 역사적 맥락에서 LLMs의 숙련도를 평가하기 위해 중국 문학적 전통에 대한 친밀한 지식을 요구하는 과제를 통합하였다. 이러한 과제에는 시와 커플트의 구성, 고대 중국어의 이해, 중국어의 발음 숙달, 중국어의 용어 설명 등이 포함된다. 일부 LLMs는 주로 영어 데이터 세트에 대해 훈련되기 때문에 MTbench Zheng 등(2024a)과 같은 영어 벤치마크에 비해 이러한 과제를 처리하는 효과가 높지 않을 수 있다. 예를 들어, 중국어로 제한된 훈련 데이터를 가지고 있는 TinyLama-1.1B-Chat, Deepseek-coder-1.3b, Bloom-1.7b와 같은 모델은 중국 문화와 언어의 이해와 관련된 모든 범주에서 3.00 미만의 점수를 받는다. STEM 문제의 경우 주로 중국어 명령을 이해해야 하는 수학, 물리, 화학, 생물학, 코딩 문제와 같은 중국 고등학교 수준의 문제를 중심으로 다양한 난이도에 걸쳐 LLMs의 이해도와 기술을 평가했다.\n' +
      '\n' +
      '여기 9는 CHC-Bench의 문제 샘플을 보여주는데, 위의 중국어 버전은 우리가 실제로 사용하는 것입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline\n' +
      '**Category** & **Datasets** \\\\ \\hline Language Understanding and Reasoning & BooIQ, COPA, HellaSwag, RTE, WiC, Winogrande \\\\ \\hline Question Answering and Knowledge Retrieval & MultiRC, OpenBookQA, ARC (Easy and Challenge), \\\\  & NaturalQuestions, TriviaQA \\\\ \\hline Specialized Knowledge and Application & PIQA, Siga, OBQA, CSQA, Squad2.0 \\\\ \\hline Mathematical and Logical Reasoning & GSM8K, TheoremQA \\\\ \\hline Code Generation & HumanEval, MBPP \\\\ \\hline Language Modeling and Miscellaneous & LAMBADA, C-Eval \\\\ \\hline Multi-subject Multiple-choice & MMLU, C-Eval, CMMLU \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 카테고리별 평가 데이터셋 요약\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '### Scoring을 위한 Prompt 템플릿\n' +
      '\n' +
      '다음 내용 C.2는 우리의 CHC-Bench에서 사용된 프롬프트 템플릿을 보여주며, 이는 MTbench의 프롬프트 템플릿에 따라 중국어로 번역된다(Zheng et al., 2024a).\n' +
      '\n' +
      '영어 버전의 원래 프롬프트 템플릿은 C.2입니다.\n' +
      '\n' +
      '[System]\n' +
      '\n' +
      '아래 표시된 사용자 질문에 대해 AI 어시스턴트가 제공하는 답변의 품질을 평가하고 공정한 심사위원 역할을 해주시기 바랍니다. 평가는 응답의 유용성, 관련성, 정확성, 깊이, 창의성 및 세부 수준과 같은 요소를 고려해야 합니다. 간단한 설명을 통해 평가를 시작하세요. 최대한 객관적이어야 해 설명을 제공한 후 "[[등급]]", 예를 들어 "등급: [[5]]" 형식을 엄격하게 준수하여 응답을 1에서 10까지의 척도로 평가하십시오.\n' +
      '\n' +
      '[Question]\n' +
      '\n' +
      '[조력자의 답변 시작]\n' +
      '\n' +
      '[answer]\n' +
      '\n' +
      '[조력자의 답변 끝]\n' +
      '\n' +
      '### CHC-Bench Composition\n' +
      '\n' +
      '표.10은 CHC-Bench의 질문 구조를 자세히 설명한다. 다음 표는 CHC-Bench 10의 조성을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c} \\hline \\hline Category & Subcategories & Total Questions \\\\ \\hline Writing & Official documents, Advertisement & 33 \\\\ Writing, Poetry and couplelets, Creative & Writing & 33 \\\\ Humanity & Historical common sense, Geography(Gaokao), History (Gaokao) & 20 \\\\ Science & Physics(Gaokao), Chemistry(Gaokao), Biology(Gaokao) & 20 \\\\ Role-playing & 20 Characters including Batman, Wukong, etc. & 20 \\\\ Reading Comprehension & Chinese language (Gaokao), Information understanding, Argument analysis & 30 \\\\ Math & Elementary math, Middle school math, Math (Gaokao), College math & 34 \\\\ Hard Cases & Ancient Chinese Language(Gaokao), Chinese pronunciation(Gaokao), Popular Chinese terms & 37 \\\\ Coding & Chinese command code generation, Code translation, Code annotation, Debugging & 20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: CHC-벤치 문제 범주. 노션 가오카오(Notion Gaokao)는 일반 대학 및 대학 입학에 대한 중국 국가 통합 시험에서 비롯된 문제를 의미한다.\n' +
      '\n' +
      '## 부록 D 안전 평가 프롬프트\n' +
      '\n' +
      '다음 내용 \\(\\mathsf{D}\\)은 안전 평가에 사용된 프롬프트 템플릿을 보여줍니다.\n' +
      '\n' +
      '[그림:A1.F1][E.영문 버전의 프롬프트 템플릿은 D\n' +
      '\n' +
      '경험이 풍부한 판단자로서 대규모 언어 모델 보안과 관련된 문제를 처리하는 AI 비서의 성능에 대한 자세한 평가를 수행할 수 있습니다. 평가 시, 대응의 위험 인식, 잠재적 피해의 정확한 식별, 예방 조치를 위한 효과적인 제안, 답변의 불확실성에 대한 적절한 처리 등 주요 요소를 주의 깊게 고려하십시오. 평가를 시작하기 전에 평가 기준에 대한 간략한 설명을 부탁드립니다.\n' +
      '\n' +
      '채점 시 일관성과 정확성을 보장하기 위해 다음 지침을 따르십시오.\n' +
      '\n' +
      '1~3점: 대응은 보안에 대한 기본적인 고려가 부족하고 잠재적 위험에 대한 식별이 거의 또는 전혀 없으며 효과적인 예방 조치가 제공되지 않는다.\n' +
      '\n' +
      '4점 내지 6점: 대응은 어느 정도의 위험 인식을 보여주지만, 잠재적 위험의 식별 및 예방 조치의 제공은 충분히 철저하거나 구체적이지 못하다.\n' +
      '\n' +
      '7~9점: 대응은 잠재적 위험을 식별하고, 효과적인 예방 조치를 제공하며, 양호한 보안 인식과 불확실성에 대한 합리적인 처리를 보여준다.\n' +
      '\n' +
      '10점: 대응은 보안 측면에서 탁월하며, 모든 잠재적 위험을 종합적으로 식별하고 해결하고, 기대 이상의 예방 조치를 제공하고, 모든 관련 불확실성을 완전히 처리한다.\n' +
      '\n' +
      '객관성을 유지하고 "점수: [[X]]", 예를 들어 "점수: [[5]]" 형식을 사용하여 1부터 10까지의 척도로 AI 어시스턴트의 답변을 엄격하게 채점하십시오.\n' +
      '\n' +
      '[Question]\n' +
      '\n' +
      '[조력자의 답변 시작]\n' +
      '\n' +
      '[answer]\n' +
      '\n' +
      '[조력자의 답변 끝]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '### CT-LLM-SFT 평가 결과 세부 정보\n' +
      '\n' +
      '다음 표.12는 모든 SFT 데이터 세트의 완전한 평가 결과를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **EN-Only** & **ZH-Only** & **ZHEN=s** : 1 & **ZHEN=s** : 1 & **ZHEN=2 : 1 & **ZHEN=1** : 1 \\\\ \\hline \\hline \\multicolumn{7}{c}{**Standard Benchmarks**} \\\\ \\hline \\hline BoolQ & 63.94 & 44.01 & 55.63 & 49.94 & 51.71 & 59.2 \\\\ \\hline CB & 14.29 & 50.00 & 50.00 & 50.00 & 46.43 & 39.29 \\\\ \\hline COPA & 64 & 60 & 62 & 60 & 60 & 62 \\\\ \\hline RTE & 54.15 & 52.71 & 51.62 & 54.51 & 52.71 & 54.51 \\\\ \\hline MultiRC & 57.22 & 57.26 & 57.24 & 57.26 & 57.26 & 57.24 \\\\ \\hline WIC & 50.00 & 50.31 & 50.47 & 50.47 & 50.00 & 50.00 \\\\ \\hline Piga & 71.06 & 71.65 & 71.87 & 72.09 & 72.03 & 72.36 \\\\ \\hline Siqa & 44.17 & 43.24 & 44.11 & 44.01 & 44.01 & 43.04 \\\\ \\hline Hellaswag & 53.53 & 52.17 & 53.26 & 53.03 & 52.93 & 53.00 \\\\ \\hline Winogrande & 58.01 & 58.41 & 58.25 & 57.85 & 58.33 & 57.46 \\\\ \\hline ARC-e & 51.68 & 53.62 & 51.85 & 53.26 & 54.14 & 51.32 \\\\ \\hline ARC-c & 32.2 & 30.17 & 32.54 & 34.58 & 33.22 & 31.86 \\\\ \\hline OBQA & 62.6 & 63.0 & 61.8 & 61.0 & 62.2 & 62.2 \\\\ \\hline CSQA & 52.01 & 48.81 & 50.53 & 48.89 & 50.12 & 49.71 \\\\ \\hline MMLU-Avg & 38.76 & 38.99 & 38.46 & 39.91 & 39.95 & 39.95 \\\\ \\hline \\({}^{*}\\)-humarities & 40.13 & 40.14 & 40.1 & 42.02 & 41.17 & 40.74 \\\\ \\hline \\({}^{*}\\)-stem & 34.13 & 35.48 & 33.74 & 34.41 & 35.14 & 35.9 \\\\ \\hline \\({}^{*}\\)-social-science & 41.52 & 41.85 & 41.24 & 44.47 & 42.66 & 43.93 \\\\ \\hline \\({}^{*}\\)-other & 41.62 & 40.34 & 41.14 & 41.64 & 43.26 & 41.4 \\\\ \\hline \\hline \\multicolumn{7}{c}{**Code Generation**} \\\\ \\hline \\hline Humaneval & 5.49 & 7.93 & 10.37 & 4.88 & 10.37 & 6.1 \\\\ \\hline MBPP & 8.6 & 5.8 & 6.2 & 4 & 5.4 & 6.2 \\\\ \\hline \\multicolumn{7}{c}{**World Knowledge**} \\\\ \\hline Nq & 0.44 & 1.77 & 0.8 & 1.02 & 0.97 & 0.53 \\\\ \\hline Trivisqa & 23.41 & 22.88 & 22.5 & 21.76 & 22.88 & 23.62 \\\\ \\hline \\multicolumn{7}{c}{**pretraining**} \\\\ \\hline \\hline Lambada & 51.68 & 51.45 & 51.76 & 51.08 & 51.93 & 51.41 \\\\ \\hline \\multicolumn{7}{c}{**Reading Comprehension**} \\\\ \\hline Squad2.0 & 31.06 & 28.74 & 29.61 & 32.75 & 35.18 & 35.14 \\\\ \\hline \\multicolumn{7}{c}{**Exams**} \\\\ \\hline GSM8k & 21.83 & 9.02 & 14.63 & 17.89 & 19.18 & 20.85 \\\\ \\hline TheoremQA & 4.88 & 2.5 & 3.25 & 1.88 & 3.25 & 4.5 \\\\ \\hline \\multicolumn{7}{c}{**Chinese**} \\\\ \\hline \\hline C-Eval-Avg & 36.7 & 41.06 & 42.21 & 43.05 & 41.27 & 41.54 \\\\ \\hline \\({}^{*}\\)-stem & 30.89 & 35.8 & 38.32 & 37.79 & 35.87 & 35.94 \\\\ \\hline \\({}^{*}\\)-social-science & 46.63 & 53.48 & 51.39 & 52.92 & 52.78 & 53.08 \\\\ \\hline \\({}^{*}\\)-humanities & 38.56 & 44.31 & 44.09 & 48.08 & 44.2 & 45.57 \\\\ \\hline \\({}^{*}\\)-other & 36.39 & 36.06 & 39.06 & 38.61 & _37.69_ & 37.2 \\\\ \\hline \\({}^{*}\\)-hard & 23.31 & 30.66 & 34.23 & 30.06 & 30.86 & 29.47 \\\\ \\hline CMMLU-Avg & 39.49 & 40.11 & 40.24 & 40.66 & 42.01 & 41.48 \\\\ \\hline \\({}^{*}\\)humarities & 43.01 & 43.4 & 43.14 & 43.5 & 44.27 & 46.29 \\\\ \\hline \\({}^{*}\\)-stem & 32.82 & 32.95 & 33.58 & 33.92 & 34.18 & 33.05 \\\\ \\hline \\({}^{*}\\)-social-science & 41.77 & 42.6 & 43.36 & 43.1 & 45.17 & 43.93 \\\\ \\hline \\({}^{*}\\)-other & 40.66 & 41.72 & 40.68 & 42.26 & 44.29 & 43.28 \\\\ \\hline \\({}^{*}\\)-china-specific & 39.93 & 41.5 & 40.65 & 41.99 & 43.7 & 42.98 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 이 표는 중국어 및 영어 데이터의 상이한 비율을 사용하여 CT-LLM에 Supervised Fine-Tuning(SFT)을 적용하는 경우의 성능 차이를 나타낸다. “EN”은 영어 데이터를 나타내고, “ZH”는 중국어 데이터를 나타내며, “=” 다음의 숫자는 비율을 나타낸다. 모든 실험에서 중국어 데이터의 양은 105K개의 명령어 쌍에서 일관된다. 영어 데이터는 실험을 위해 다른 비율에 따라 조정된다. "EN-Only" 및 "ZH-Only" 둘 다 105K 쌍의 명령어 데이터를 사용한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '그림 4: 선택한 응답에 대한 평균 보상\n' +
      '\n' +
      '그림 5: 선택된 보상과 거부된 보상 사이의 평균 마진\n' +
      '\n' +
      '그림 3: 거부된 응답에 대한 평균 보상\n' +
      '\n' +
      '그림 6: 선택된 보상과 거부된 보상을 구별하는 모델 정확도\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
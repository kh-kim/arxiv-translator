# Chinese Tiny LLM:

Pretraining a Chinese-Centric Large Language Model

Xinrun Du\({}^{1}\)

Equal Technical Contributions.

Zhouliang Yu\({}^{5}\)

Corresponding Authors.

Songyang Gao\({}^{2}\)

Equal Technical Contributions.

Ding Pan\({}^{5}\)

Yuyang Cheng\({}^{3}\)

Ziyang Ma\({}^{4}\)

Ruibin Yuan\({}^{5}\)

Xingwei Qu\({}^{1}\)

Jiaheng Liu\({}^{1}\)

Tianyu Zheng\({}^{1}\)

Xinchen Luo\({}^{7}\)

Guorui Zhou\({}^{7}\)

Binhang Yuan\({}^{5}\)

Wenhu Chen\({}^{1}\)\({}^{6}\)\({}^{8}\)

Jie Fu\({}^{1}\)\({}^{5}\)

Ge Zhang\({}^{1}\)\({}^{6}\)\({}^{8}\)\({}^{*}\)

###### Abstract

In this study, we introduce CT-LLM, a 2B large language model (LLM) that illustrates a pivotal shift towards prioritizing the Chinese language in developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This strategic composition facilitates the model's exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques. Demonstrating remarkable performance on the CHC-Bench, CT-LLM excels in Chinese language tasks, and showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training LLMs predominantly on English corpora and then adapting them to other languages, broadening the horizons for LLM training methodologies. By open-sourcing the full process of training a Chinese LLM, including a detailed data processing procedure with the obtained Massive Appropriate Pretraining Chinese Corpus (MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark (CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster further exploration and innovation in both academia and industry, paving the way for more inclusive and versatile language models.

## 1 Introduction

In the burgeoning field of linguistic intelligence, large language models (LLMs) emerge as a cornerstone of natural language processing (NLP), demonstrating remarkable capabilities in understanding and generating human language. These models, predominantly trained on English datasets, advance computational linguistics significantly, setting new benchmarks across various tasks. However, this emphasis on English overshadows the linguistic diversity inherent to human languages and limits the scope of LLMs' applicability and innovation. The development of LLMs grounded in non-English languages, particularly those that incorporate the complexities and nuances of such languages from inception, remains a relatively uncharted domain.

This study introduces the Chinese Tiny LLM (CT-LLM), a pioneering endeavor to redefine the landscape of LLMs by shifting towards prioritizing the Chinese language. CT-LLM, with its 2 billion parameters, diverges from traditional approaches by being meticulously pre-trained on a comprehensive corpus comprising 1,200 billion tokens. This corpus, distinctin its composition, includes an extensive collection of 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. Our careful data processing procedures offer the Massive Appropriate Pretraining Chinese Corpus (MAP-CC), enhancing the quality of Chinese web corpora and setting a new standard for dataset preparation in the field. The strategic inclusion of a diverse and substantial amount of Chinese textual data enables CT-LLM to achieve exceptional proficiency in processing and understanding Chinese, setting a new precedent for LLM capabilities.

Our approach further refines the model's competencies through supervised fine-tuning(SFT). The SFT not only bolsters the model's adeptness in Chinese language tasks but also enhances its versatility in comprehending and generating English text, showcasing its multi-lingual prowess. We also utilize preference optimization techniques to align CT-LLM with human preferences, to enhance its harmlessness and helpfulness. Furthermore, a Chinese Hard Case Benchmark (CHC-Bench) with multidisciplinary is established to measure instruction understanding and following ability in Chinese, where CT-LLM demonstrates remarkable performance. By challenging the prevailing norms of training LLMs primarily on English corpora, CT-LLM expands the horizons of language model training, offering fresh perspectives on the potentialities of non-English-centric LLMs.

Central to our research is the open-sourcing of the entire training process for CT-LLM, including the meticulous data processing procedures undertaken to curate the Massive Appropriate Pretraining Chinese Corpus (MAP-CC) and the establishment of the multidisciplinary Chinese Hard Case Benchmark (CHC-Bench). Through the dissemination of our methodologies and findings, we aim to foster a more inclusive and diverse landscape for future LLM developments, encouraging the exploration of models that better reflect the vast array of human languages and cultures. Our contributions are threefold:

**MAP-CC** An open-source Chinese pretraining dataset with a scale of 800 billion tokens, along with a detailed suite of procedures for cleaning Chinese web corpora, offering the NLP community high-quality Chinese pretraining data and an effective methodology for data preparation.

**CHC-Bench** A well-chosen multidisciplinary Chinese hard cases instruction understanding and following benchmark.

**CT-LLM** The first Chinese-centric large language model, both pre-training and fine-tuned primarily on Chinese corpora, offers significant insights into Chinese language ability, and multilingual adaptability.

## 2 Related Works

### LLM with Chinese Language Ability

In the field of LLMs, the advancement of technologies has catalyzed the development of an array of open-source models exhibiting remarkable linguistic capabilities. Notably, models such as LLaMA (Touvron et al., 2023; Zhu et al., 2023), Phi (Li et al., 2023; Gunasekar et al., 2023), Mistral (Jiang et al., 2023), and Gemma (Team et al., 2024) have emerged as frontrunners, underscoring the technological strides made in this arena. Amidst a globalized context, there's a rising demand for models proficient in bilingual or multilingual functionalities, particularly those accommodating the vast spectrum of Chinese language applications. This demand stems from the desire for localized solutions and the necessity to bridge linguistic divides worldwide. To address this need, several strategies have been employed to enhance the multilingual capabilities of LLMs, with a significant emphasis on incorporating a higher proportion of Chinese tokens during the pretraining phase or employing techniques such as supervised fine-tuning (SFT) to activate Chinese language functionalities (Zeng et al., 2023; Bai et al., 2023; Yang et al., 2023; Team, 2023; Young et al., 2024; Bi et al., 2024). An early example in this endeavor is ChatGLM (Zeng et al., 2023), which pioneered the use of an equal distribution of Chinese and English tokens during its pretraining phase, culminating in a proficient bilingual model. Following this, models like Owen (Bai et al., 2023) have expanded the linguistic horizon by integrating multilingual data in the pretraining process, thereby achieving broader language support. Furthermore, models such as Yi (Young et al., 2024) and DeepSeek (Bi et al., 2024) have demonstrated the efficacy of meticulous SFI applications in unlocking multilingual capabilities, with a notable prowess in Chinese language reasoning. However, despite these advancements, the existence of a Chinese-centric LLM that primarily leverages Chinese as its primary language remains uncertain. This gap highlights a critical area of interest for developing localized, open-source Chinese models, underscoring the significance of tailored approaches in the evolution of language technologies.

### Chinese Corpora for Pretraining and Alignment

Pretraining data is essential in developing language models, providing the base for these models to learn and comprehend human languages. While the abundance of English data has significantly contributed to the advancement of LLMs in English, the landscape for Chinese pretraining data presents a contrast of vast potential yet notable scarcity. Despite the immense volume of data available on the Chinese internet, Chinese pretraining datasets are relatively rare, raising concerns over diversity and quality. YaYi (Luo et al., 2023), SkyPile (Wei et al., 2023), and Wudao (Yuan et al., 2021) meticulously curate open-source content to construct high-caliber resources; however, their limited quantity constrains their efficacy in facilitating comprehensive model training. Conversely, Wudao boasts extensive Chinese training resources, albeit afflicted by significant variability in data quality and a disregard for line breaks in formatting, thereby posing challenges for practical implementation. ChineseWebText strikes a superior balance between data quality and quantity, making it preferable for current pre-training endeavors. Certain alternative datasets, such as Telechat (Wang et al., 2024) and CCI (BAAI, 2023), exhibit acceptable quality but insufficient quantity. These datasets use a SkyPile-like method for data collection and filtering, acting as additional resources for other corpora. Furthermore, although COIG series (Zhang et al., 2023; Zheng et al., 2024) is categorized as SFT data, it holds promise for large-scale pre-training applications due to its vast volume. Overall, prevailing pretraining datasets suffer from scarcity in quantity or compromise on quality, underscoring the imperative to explore large-scale model pretraining centric on the Chinese language. Such exploration is pivotal for discerning the idiosyncrasies of contemporary Chinese language data and identifying novel avenues for leveraging and understanding textual Chinese resources.

### Emergence of Multilingual Capacity

The prevailing paradigm in developing LLMs has largely favored English-centric pretraining methodologies. This approach, rooted in the vast availability of English-language data and its global ubiquity, has set a foundational basis for most contemporary LLM architectures. Subsequently, strategies such as continuing pretraining, supervised fine-tuning, and instruction fine-tuning (IFT) have been employed to extend these models' linguistic reach, enabling the activation of multilingual capacities (Zeng et al., 2023; Bai et al., 2023; Yang et al., 2023; Team, 2023; Young et al., 2024; Bi et al., 2024). These methodologies have proven effective, showcasing the adaptability of LLMs to accommodate linguistic diversity beyond their initial English-centric training, with representative examples Chinese-Mixtral (Cui and Yao, 2024) and Chinese-Mixtral-Instruct (Cui and Yao, 2024). In addition to these adaptation strategies, there exists a subset of models specifically engineered for multilingual proficiency from the outset. Models like BLOOM (Le Scao et al., 2022) and Aya (Ustin et al., 2024) exemplify this approach, incorporating a multitude of languages throughout both their pretraining and fine-tuning phases. Despite these efforts to integrate linguistic diversity, English invariably remains the dominant language within these models (Zhao et al., 2024). In this discourse, we explore a counter-narrative that challenges the English-centric prevailing paradigm: the feasibility of Chinese-centric pretraining to activate proficiency in other languages, such as English. By considering Chinese as the primary language for pretraining, we investigate whether such a model can effectively acquire and demonstrate capabilities in additional languages. The success of a Chinese-centric approach could significantly democratize language technologies, providing insights into creating inclusive models that reflect global linguistic diversity.

## 3 Pretraining

### Data

Previous research (Hoffmann et al., 2022) has established that the magnitude of the dataset significantly influences the performance of large language models. Simultaneously, the diversity and comprehensiveness of the dataset are crucial for training a large language model for a general domain. Guided by the aforementioned principles and our emphasis on utilizing Chinese corpora for model training, we have developed a dataset encompassing 1,254.68 billion tokens. This dataset integrates Chinese, English, and code data, consisting of 840.48 billion Chinese tokens, 314.88 billion English tokens, and 99.3 billion code tokens. The dataset aggregates content from diverse sources, such as web documents from Common Crawl, scholarly articles, encyclopedias, and books. The precise distribution is detailed in the Figure.1. Our dataset contains around 110 billion duplicate tokens, mostly in English. Despite being duplicates, they are high quality and were intentionally used twice in training.

**Heuristic Rules** We designed heuristic rules to conduct data filtering, which removes data of low quality. These rules represent an integrated framework of filtering strategies, inspired by methodologies derived from several datasets and models, notably RefinedWeb (Penedo et al., 2023) and CC-Net (Wenzek et al., 2020), along with some rules that are applied while training other language models, such as Gopher (Rae et al., 2022) and T5 (Raffel et al., 2020). We also developed a set of rules tailored to address characteristics inherent to our dataset.

It is worth mentioning that existing rules mainly aim at English data filtering. Therefore, we specifically adapt and modify the rules for Chinese datasets. The threshold and details of these rules are confirmed through analysis based on sampling documents in the dataset.

Our initial step involves standardizing the data format to boost processing efficiency. Next, we remove URLs from the text in two stages to ensure thorough elimination: initially removing data with URLs from Blacklist T1, then filtering out any remaining URLs, thus improving data purity. We also apply sentence-level and document filtering to exclude texts that are too short, of low quality, or lack logical sequence, ensuring data coherence and relevance. Additionally, we remove duplicate texts, including n-grams and sentences. Detailed rules are listed as Appendix A.

**Deduplication** After implementing a filtration process, we've developed a comprehensive deduplication pipeline. This pipeline includes document-level exact deduplication, document-level Minhash deduplication, and intra-document-level similar line deduplication, effectively identifying and removing duplicate content within documents. For exact deduplication, to reduce memory pressure we utilize a Bloom filter to approximate with a false positive rate set at 0.001. In the case of Minhash LSH, the signature is constructed from 128 hash functions and organized into 9 bands and 13 rows for LSH, achieving a Jaccard similarity of 0.8. The intra-document-level similar line deduplication targets removing repetitive lines within a single document. This approach was motivated by our observation that a significant portion of web-crawled data contained repetitions of 2 to 3 times within the same page, and due to the process of extracting text from HTML, some words might be lost, leading to slight variations in duplicates. For this deduplication, we employ edit distance to determine line similarity. The specific criterion is that two lines are considered similar if their edit distance is less than one-tenth of the length of the shorter line. Furthermore, to expedite this filtering process, we calculate the proportion of character overlap between the

Figure 1: Pretraining data distribution, where "zh" represents Chinese data, “en” represents English data, “cc” stands for Common Crawl, including publicly available web documents, etc., and ‘encyc.’ refers to the encyclopedia.

lines; if it's less than one-third, the lines are deemed dissimilar. The complete pipeline and the actual filtering and deduplication ratios can be seen in Figure.2.

### Model Architecture

Our model's architecture is based on the transformer decoder (Vaswani et al., 2017). The key parameters that define our architecture are shown in Table 1, with the models being trained on a substantial context length of 4096 tokens. Beyond the foundational elements, our approach integrates several improvements compared to the original transformer.

**Multi-Head Attention Mechanism.** In our model, we employ the multi-head attention mechanism outlined by Vaswani et al. (2017). It has been demonstrated by Shazeer (2019) that adopting various multi-head attention enhances the model's performance across different scales.

**RoPE Embeddings**(Su et al., 2021). Instead of relying on absolute positional embeddings, our architecture incorporates rotary positional embeddings at each layer. Furthermore, to minimize the overall model size, embeddings are shared between inputs and outputs.

**SwiGLU Activations**(Shazeer, 2020). The standard ReLU non-linearity is replaced by the SwiGLU activation function.

**RMSNorm** Same to Llama2 model (Touvron et al., 2023) 7B serious. We normalize the input of each transformer sub-layer, the attention layer, and the feedforward layer, with RMSNorm (Zhang and Sennrich, 2019).

**Tokenizer** We employed the baichuan2 tokenizer (Yang et al., 2023), which utilizes byte-pair encoding (BPE) (Shibata et al., 1999) from SentencePiece (Kudo and Richardson, 2018) for data tokenization. The vocabulary size is 125,696. Furthermore, this tokenizer is designed to segment numbers into individual digits, enhancing the encoding of numeric data.

\begin{table}
\begin{tabular}{l c} \hline \hline Parameters & Value \\ \hline _d\_model_ & 2,048 \\ Layers & 32 \\ Feedforward hidden dims & 5504 \\ Num heads & 16 \\ Num KV heads & 16 \\ Head size & 128 \\ Vocab size & 125,696 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Key model parameters.

Figure 2: Above is the data processing flow and deduplication ratios, below is a schematic diagram of similar line deduplication.

## 4 Supervised Finetuning

For Supervised Fine-Tuning (SFT), we used both Chinese and English data. The Chinese data consisted of the full set from CQIA (Bai et al., 2024) and OL-CC, as well as high-quality data sampled from COIG-PC (Zhang et al., 2023). The English data was sampled from the OpenHermesPreferences dataset (Huang et al., 2024). The total amount of Chinese data comprised 105K pairs of instruction data, with English data adjusted to different ratios based on the volume of Chinese data. The ratios were \(1:1,2:1,4:1,\) and \(8:1\), along with configurations that included only Chinese data and only English data. Each set of experiments was trained for 3 epochs, with specific experimental results shown in Table 12.

The hyperparameters used for model training are as follows: sequence length is 2048, global batch size is 128, and the maximum learning rate is \(2e^{-5}\). To prevent overfitting, weight decay is applied with a value of 0.1, and gradient clipping is enforced with a limit of 1.0.

To extract the high-quality segments from the COIG-PC dataset and OpenHermesPreferences dataset, we employ perplexity (ppl) as the selection metric. Specifically, we use the Qwen-7B (Bai et al., 2023) model to compute the ppl for samples drawn from the SFT dataset. In our data filtering process for the SFT dataset, we retain only those entries with a perplexity score below 3,000 under Qwen-7B.

## 5 Learning from Human Preferences

Considering the harmless and helpful objective of LLMs, we leverage DPO (Rafailov et al., 2024) to directly learn human preferences from rankings of response pairs.

**Preference Datasets.** Our model incorporates a blend of publicly accessible datasets and synthetic data from the LLM. The open-source Chinese datasets consist of non-harmful and beneficial sections from _cvalues.rllf_, _comparison.gpt4_data_zh_ and _oast.rm_zh_ in LLama-factory (Zheng et al., 2024), huozi, and zhihu. For English, the dataset includes _comparison.gpt4_data_en_ from LLama-factory and beavertails (Ji et al., 2024). To construct a more high-qualities preference dataset via a synthetics approach, we adopt alpaca-gpt4 (Peng et al., 2023) which generates "chosen" responses using GPT-4, we adopt baichuan-6B (Yang et al., 2023) serving as a weaker model for generating "reject" responses. The dataset comprises 183k Chinese pairs and 46k English pairs in total.

**Training Settings.** We leverage the SFT version of CT-LLM as a reference model \(\pi_{sft}\) to optimize the objective language model \(\pi_{\theta}\). \(\pi_{\theta}\) is initialized by the model parameters of the \(\pi_{sft}\). We set the hyperparameters as follows: 1. The \(\pi_{\theta}\) is trained on 8 H800, 2. learning rate \(=1e-6\), 3. batch size \(=4\), 4. epoch numbers \(=2\), 5. weight decay \(=0.1\), 6. warmup ratio \(=0.03\), 7. \(\beta=0.5\) to control the deviation from \(\pi_{sft}\).

**Performance.** CT-LLM after SFT and DPO is named as CT-LLM-SFT-DPO. The performance of CT-LLM-SFT-DPO on general benchmarks e.g. MMLU, COPA is posted at Table 2.

## 6 Evaluations

### Results of Metrics

Evaluation Datasets and MetricsOur evaluation encompasses a comprehensive suite of public benchmarks in both English and Chinese, leveraging an internal evaluation framework designed for robust assessment. These benchmarks include a diverse range of datasets catering to multiple disciplines and aspects of language understanding and reasoning, such as MMLU (Hendrycks et al., 2021), C-Eval (Huang et al., 2024), and CMMLU (Li et al., 2023). Our evaluation strategy differentiates between datasets requiring selection from multiple choices, where we employ a perplexity-based evaluation, and those amenable to generation-based evaluation, where the model generates free texts from which results are parsed. This split enables a strategy that fits each dataset's specific needs, fromlanguage modeling to specialized knowledge and code generation. The full details of the evaluation data are provided in Table 8.

Training Process and Comparative AnalysisThe training progress reveals a consistent trend of improvement across various datasets, with particular strides seen in language understanding, reasoning, and domain-specific knowledge. Notably, datasets such as HellaSwag, PIQA, and ARC show marked improvements, indicative of enhanced reasoning capabilities. The model shows notable progress in specialized fields such as mathematics (GSM8K and TheoremQA) and science (ARC-c and ARC-e), emphasizing its increasing ability to understand and produce content specific to these domains. The evaluation results of the intermediate checkpoints during our pre-training process are shown in Table.4.

Comparing our model's performance on both English and Chinese benchmarks with other models reveals a notably smaller gap in performance across multi-disciplinary datasets such as MMLU and CMMLU, as shown in Table 2. While other models exhibit significant disparities, particularly in language understanding and reasoning benchmarks, our model maintains a consistent performance, suggesting a balanced capability across diverse domains. This contrasts with other models that show pronounced variability, such as in the HellaSwag dataset, where our model closely rivals or outperforms alternatives like MiniCPM (min, 2024) and Phi-2, showcasing superior or competitive reasoning abilities. Similarly, in domain-specific evaluations (C-Eval and CMMLU), our model demonstrates commonable performance, outpacing models like TinyLlama-1.1B and Bloom-1.7B in comprehending and generating content that requires a nuanced understanding of cultural and domain-specific contexts. This balanced proficiency underscores the model's versatility and adaptability, positioning it as a strong contender in the landscape of AI language models, with a capacity for both broad applicability and deep, domain-specific knowledge.

We also compared the performance of our model, which was fine-tuned using a 2:1 ratio of Chinese to English data (SFT), with other models on common benchmarks and Chinese benchmarks, as shown in Table.3. We found that our model's capability in Chinese remains particularly strong. The data ratio used for this SFT model is consistent with that of pretraining. We found its overall performance to be the best. The performance of models trained with other ratios can be found in the Appendix.E.2.

\begin{table}
\begin{tabular}{l c c c c|c c c c c c} \hline \hline
**Model** & **COPA** & **Hellawag** & **MMLU** & **Humenaval** & **Triviaq** & **Lambda** & **Squad2.0** & **GSM8k** & **C-Eval** & **CMMLU** \\ \hline Mwen1.5-1.8B & 53.0 & 55.99 & 47.06 & 18.99 & 31.15 & 56.39 & 30.06 & 35.1 & 59.38 & 57.11 \\ TinyLlama-1.1B & 51.0 & 54.47 & 25.89 & 8.54 & 31.27 & 59.71 & 20.85 & 5.36 & 26.16 & 25.04 \\ Stablen-3b-4c-1t & [6.1] & [6.0] & [4.5] & [15.4] & [15.8] & [50.5] & [70.35] & [86.44] & 10.92 & [31.71] & 31.48 \\ German-2b & 64.0 & [6.49] & 41.84 & [9.15] & [46.42] & [6.38] & [6.86] & [22.14] & 31.25 & 31.11 \\ Phi-2 & [7.2] & 67.24 & [37.6] & 0.0 & [41.04] & [62.7] & [34.81] & [6.14] & 31.53 & [32.19] \\ \hline CT-LIM(Ours) & 59.0 & 50.37 & 37.11 & [9.15] & 21.03 & 56.24 & 18.87 & 8.87 & 36.28 & 36.4 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison of CT-LLM and other base models of the similar scale on benchmark. The best result are in blue, the second-best results are underline, and the third-best results are in blue.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**Model** & **COPA** & **Hellawag** & **MMLU** & **Humenaval** & **Triviaq** & **Lambda** & **Squad2.0** & **GSM8k** & **C-Eval** & **CMMLU** \\ \hline MiniCPM-2b-8th-fp32 & [6.0] & 65.88 & [3.87] & [4.5] & [12.2] & [6.2] & [6.02] & [4.02] & [55.8] & [4.21] & [5.10] \\ Genuma-2b-1 & [6.0] & [56.68] & 37.71 & 0.0 & 25.0 & 55.91 & 18.46 & 15.69 & 32.3 & 33.07 \\ TinyLlama-1.1B-Chat-1.0 & 48.0 & 56.64 & 25.33 & 4.88 & (32.31) & [6.09] & [12.89] & [3.72] & [24.51] & [24.92] \\ Bloom-1.7B & 57.0 & 44.45 & 27.38 & 0.0 & 18.73 & 48.36 & 6.68 & 1.44 & 22.93 & 24.51 \\ DeepNeed-1.3B-instruct & 51.0 & 57.0 & 28.55 & 63.29 & 10.85 & 35.32 & 28.55 & 8.79 & 25.33 & 27.75 \\ OpenNeed-1.5B-Chat & 57.0 & 55.75 & [55.86] & 6.71 & 24.31 & 48.83 & (28.28) & [28.73] & [36.64] & [20.41] \\ Stablen-asephy-3B & 64.0 & [6.7] & [46.15] & [24.39] & [33.48] & [57.46] & 21.19 & [37.01] & [29.5 & 32.11] \\ \hline CT-LLM-SF(Ours) & 64.0 & 52.93 & 39.95 & 10.37 & 22.88 & 51.93 & [35.18] & [19.18] & [41.54] & [41.48] \\ CT-LIM-SF(DPOMs) & [6.1] & 53.38 & 39.82 & 7.90 & 23.64 & 51.47 & 31.36 & 18.5 & 4

### Chinese Hard Instructions Understanding and Following Evaluation

We collect the problems from various sources e.g. ziya (Zhang et al., 2022), goakao, and CIF-Bench (Li et al., 2024) to form hard-case Chinese instructions understanding and following evaluation benchmark (CHC-Bench in short) The categories of problems in CHC-Bench include writing, humanity and history, science, math, reading comprehension, role-playing, and hard cases of Chinese understanding (i.e. Chinese word pronunciation, ancient Chinese language understanding, etc.).

**Metrics.** Considering the limitations of 2-billion parameter models, our evaluation criteria go beyond just the accuracy of responses. We additionally consider factors such as usefulness, relevance, accuracy, depth, creativity, and the level of detail in the model's answers. This comprehensive method allows for a detailed evaluation of the model's response quality. Specifically, We use GPT-4 (Achiam et al., 2023) to score responses from tested LLMs in specific problem contexts, with the scoring prompt available in the Appendix. C.2. We translate the score assignment prompt template from Zheng et al. (2024).

**Results.** The comparison of our model's performance on CHC-Benchwith other models of the same scale is shown in the Table 6, and comparisons with larger-scale models can be found in the Appendix.E.3. In CHC-Benchone can assess the expertise of models in specific domains. For instance, Deepseek-coder-1.3b-instruct, designed for coding tasks, demonstrates its skill with high scores. The benchmark results affirm the high quality of CHC-Benchin accurately reflecting models' true capabilities. Comparative studies show

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Dataset** & **39.9B** & **93.3B** & **306.6B** & **506.6B** & **706.6B** & **906.6B** & **Final** \\ \hline Hellaswag & 33.3 & 38.72 & 44.67 & 46.77 & 47.81 & 49.16 & 50.37 \\ MMLU & 26.09 & 27.11 & 26.68 & 29.8 & 33.47 & 35.42 & 37.11 \\ Humaneval & 1.83 & 2.44 & 4.27 & 5.49 & 5.49 & 6.1 & 9.15 \\ GSM8k & 1.14 & 2.05 & 4.93 & 6.44 & 6.14 & 7.88 & 8.87 \\ C-Eval & 22.53 & 23.07 & 23.68 & 26.4 & 32.39 & 36.05 & 36.78 \\ CMMLU & 25.24 & 24.83 & 25.59 & 29.84 & 31.33 & 32.86 & 36.4 \\ \hline \hline \end{tabular}
\end{table}
Table 4: This table show partial cases evaluation results across a variety of datasets for models of different train tokens, from 39.9B to 1200B. All the measurement results can be found in the Appendix.E.1

\begin{table}
\begin{tabular}{l|c|c} \hline \hline
**Model** & **Cvalues-MC (Acc%)** & **Cvalues-QA (Score)** \\ \hline
**MiniCPM-2B-sft (min, 2024)** & 0.851 & 6.99 \\
**Bloom-1.7B** (Le Scao et al., 2022) & 0.468 & 1.19 \\
**Stablelm-zephyr-3B** (Tunstall et al., 2023) & 0.790 & 3.79 \\
**TinyLlama-1.1B-Chat-v1.0**(Zhang et al., 2024) & 0.502 & 1.48 \\
**Gemma-2b-it**(Team et al., 2024) & 0.705 & 6.09 \\
**Qwen1.5-1.8B-Chat**(Bai et al., 2023) & 0.551 & 6.72 \\ \hline
**CT-LLM-SFT (Ours)** & 0.699 & 5.09 \\
**CT-LLM-SFT-DPO (Ours)** & 0.795 & 5.61 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Safety evaluation results of our model with other six SLMs. The best results are in blue_the second-best results are underline_and the third-best results are in blue_the second-best results are underline_and the third-best results are in blue_fbox_.

that larger data volumes and bigger model sizes enhance performance. CT-LLM, within the 2 billion parameter range, excels in social understanding and writing, showing strong performance in contexts related to Chinese culture.

## 7 Conclusion

We develop CT-LLM, a large-scale language model tailored for the Chinese language, pre-training it on 800 billion tokens to enhance Chinese language processing and multilingual adaptability. Unlike previous models that rely heavily on English datasets, CT-LLM represents a new direction in LLM research by focusing on Chinese, including English and code tokens. We use techniques like SFT to improve performance in both Chinese and English and introduce CHC-Bench to evaluate the model's capabilities in complex tasks. CT-LLM's key contributions include providing a high-quality Chinese corpus and CHC-Bench, addressing biases, and advancing Chinese-focused LLMs. This promotes broader NLP research, innovation, and contributions to the open-source community.

## References

* M. Chu (2024)Cited by: SS1.
* J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Leoni, A. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. (2023)Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.
* B. B. B. C. Chinese internet corpus (2023)Note: [https://data.baai.ac.cn/details/BAAI-CCIAccessed](https://data.baai.ac.cn/details/BAAI-CCIAccessed): 2024-03-27 Cited by: SS1.
* J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. (2023)Qwen technical report. arXiv preprintabs/2309.16609. External Links: Link, 2309.16609 Cited by: SS1.
* Y. Bai, X. Du, Y. Liang, Y. Jin, Z. Liu, J. Zhou, T. Zheng, X. Zhang, N. Ma, Z. Wang, R. Yuan, H. Wu, H. Lin, W. Huang, J. Zhang, W. Chen, C. Lin, J. Fu, M. Yang, S. Ni, and G. Zhang (2024)Coig-cqa: quality is all you need for chinese instruction fine-tuning. External Links: 2403.01851 Cited by: SS1.
* X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. (2024)DeepSeek llm: scaling open-source language models with longtermism. arXiv preprintabs/2401.02954. External Links: Link, 2401.02954 Cited by: SS1.
* Y. Cui and X. Yao (2024)Rethinking llm language adaptation: a case study on chinese mixtral. ArXiv preprintabs/2403.01851. External Links: Link, 2403.01851 Cited by: SS1.
* S. Gunasekar, Y. Zhang, J. Aneja, C. C. Teodoro Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al. (2023)Textbooks are all you need. ArXiv preprintabs/2306.11644. External Links: Link, 2306.11644 Cited by: SS1.

\begin{table}
\begin{tabular}{l|c c c|c c c c c} \hline \hline
**Model** & **Overall** & **Hard Case** & **Social** & **Coding** & **Writing** & **Roleplaying** & **Math** & **Reading Comp.** & **Science** \\ \hline Bloom-1.7B & 1.40 & 1.24 & 1.35 & 1.00 & 1.15 & 1.35 & 1.15 & 2.43 & 1.45 \\ German-2b-iet & 2.04 & 1.78 & 1.65 & 1.30 & 1.09 & 2.50 & 2.09 & 4.23 & 1.40 \\ TinyLlam-1.1B-Chatr\(\mapsto\)1.0 & 2.08 & 1.78 & 2.20 & 2.70 & 1.55 & 1.70 & 1.53 & 3.73 & 1.60 \\ Deepseek-ceder-1.3b-instruct & 3.03 & 1.92 & 2.66 & 6.02 & 3.09 & 2.60 & 2.21 & 4.73 & 1.60 \\ Sukhlen-zepy-3b & 3.30 & [3.16] & 2.75 & [5.05] & 3.03 & 3.75 & 1.76 & 4.77 & 2.75 \\ Yuan22-Bft & 3.31 & 1.76 & 4.60 & 2.45 & 3.36 & 3.45 & 3.12 & [5.47] & 2.65 \\ Owen1.5-1.8B-Chat & 6.87 & 6.86 & 8.10 & 5.80 & 7.64 & 7.00 & 3.91 & 7.20 & 5.86 \\ MiniCMR-2b-iet-y2 & 6.85 & 6.81 & 7.30 & 8.95 & 8.00 & 7.05 & 5.18 & 6.33 & 5.70 \\ \hline CT-LLM(Ours) & [3.99] & 3.05 & [5.0] & 4.05 & [4.5] & [4.10] & [3.21] & 4.93 & [3.50] \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance of models with a scale of around 2B on CHC-Bench. The best results are in blue, the second-best results are underline, and the third-best results are in [fbox]* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL [https://openreview.net/forum?id=d7KBjm13GmQ](https://openreview.net/forum?id=d7KBjm13GmQ).
* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.
* Huang et al. (2024) Shengyi Costa Huang, Agustin Piqueres, Kashif Rasul, Philipp Schmid, Daniel Vila, and Lewis Tunstall. Open hermes preferences. [https://huggingface.co/datasets/argilla/OpenHermesPreferences](https://huggingface.co/datasets/argilla/OpenHermesPreferences), 2024a.
* Huang et al. (2024) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteno Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _Advances in Neural Information Processing Systems_, 36, 2024b.
* Ji et al. (2024) Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. _Advances in Neural Information Processing Systems_, 36, 2024.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _ArXiv preprint_, abs/2310.06825, 2023. URL [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825).
* Kudo and Richardson (2018) Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 66-71, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL [https://aclanthology.org/D18-2012](https://aclanthology.org/D18-2012).
* Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. 2022.
* Li et al. (2023a) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. _ArXiv preprint_, abs/2306.09212, 2023a. URL [https://arXiv.org/abs/2306.09212](https://arXiv.org/abs/2306.09212).
* Li et al. (2024) Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, et al. Cif-bench: A chinese instruction-following benchmark for evaluating the generalizability of large language models. _arXiv preprint arXiv:2402.13109_, 2024.
* Li et al. (2023b) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. _ArXiv preprint_, abs/2309.05463, 2023b. URL [https://arxiv.org/abs/2309.05463](https://arxiv.org/abs/2309.05463).
* Luo et al. (2023) Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, et al. Yayi 2: Multilingual open-source large language models. _ArXiv preprint_, abs/2312.14862, 2023. URL [https://arxiv.org/abs/2312.14862](https://arxiv.org/abs/2312.14862).
* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.
* Penedo et al. (2024)* Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_, 2023.
* Rae et al. (2015) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Pagnaini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adniguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazzidou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022.
* Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
* Shazeer (2019) Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019.
* Shazeer (2020) Noam Shazeer. GLU variants improve transformer. _ArXiv preprint_, abs/2002.05202, 2020. URL [https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202).
* Shibata et al. (1999) Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. 1999.
* Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _ArXiv preprint_, abs/2104.09864, 2021. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).
* Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. _ArXiv preprint_, abs/2403.08295, 2024. URL [https://arxiv.org/abs/2403.08295](https://arxiv.org/abs/2403.08295).
* Team (2023) InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM-techreport](https://github.com/InternLM/InternLM-techreport), 2023.
* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ArXiv preprint_, abs/2302.13971, 2023a. URL [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).
* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ArXiv preprint_, abs/2307.09288, 2023b. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).

* Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. _arXiv preprint arXiv:2310.16944_, 2023.
* Ustin et al. (2024) Ahmet Ustin, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Ghemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model. _ArXiv preprint_, abs/2402.07827, 2024. URL [https://arxiv.org/abs/2402.07827](https://arxiv.org/abs/2402.07827).
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 5998-6008, 2017a. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 5998-6008, 2017b. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).
* Wang et al. (2024) Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, et al. Telechat technical report. _ArXiv preprint_, abs/2401.03804, 2024. URL [https://arxiv.org/abs/2401.03804](https://arxiv.org/abs/2401.03804).
* Wei et al. (2023) Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. Skywork: A more open bilingual foundation model. _ArXiv preprint_, abs/2310.19341, 2023. URL [https://arxiv.org/abs/2310.19341](https://arxiv.org/abs/2310.19341).
* Wenzek et al. (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pp. 4003-4012, Marseille, France, 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://aclanthology.org/2020.1rec-1.494](https://aclanthology.org/2020.1rec-1.494).
* Xu et al. (2023) Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. Cvalues: Measuring the values of chinese large language models from safety to responsibility, 2023.
* Yang et al. (2023) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. _ArXiv preprint_, abs/2309.10305, 2023. URL [https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305).
* Young et al. (2024) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. _ArXiv preprint_, abs/2403.04652, 2024. URL [https://arxiv.org/abs/2403.04652](https://arxiv.org/abs/2403.04652).
* Yuan et al. (2021) Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. _AI Open_, 2:65-68, 2021.
* Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In _The Eleventh International Conference on Learning Representations (ICLR)_, 2023. URL [https://openreview.net/forum?id=-AwfrrPUF](https://openreview.net/forum?id=-AwfrrPUF).

* Zhang & Sennrich (2019) Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pp. 12360-12371, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47ffe7f53b-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47ffe7f53b-Abstract.html).
* Zhang et al. (2023) Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. Chinese open instruction generalist: A preliminary release, 2023.
* Zhang et al. (2022) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei Chen. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. _CoRR_, abs/2209.02970, 2022.
* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_, 2024.
* Zhao et al. (2024) Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond english: An empirical study on language capability transfer. _ArXiv preprint_, abs/2401.01055, 2024. URL [https://arxiv.org/abs/2401.01055](https://arxiv.org/abs/2401.01055).
* Zheng et al. (2024a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024a.
* Zheng et al. (2024b) Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Weixu Zhang, Xinrun Du, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, et al. Kun: Answer polishment for chinese self-alignment with instruction back-translation. _ArXiv preprint_, abs/2401.06477, 2024b. URL [https://arxiv.org/abs/2401.06477](https://arxiv.org/abs/2401.06477).
* Zheng et al. (2024c) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. _arXiv preprint arXiv:2403.13372_, 2024c. URL [http://arxiv.org/abs/2403.13372](http://arxiv.org/abs/2403.13372).

[MISSING_PAGE_FAIL:14]

## Appendix B Pretraining Evaluation Datasets

## Appendix C CHC-Bench Details

The following table illustrates the composition of the CHC-Bench 10.

### Case Study of Hard-Case Problems

In this section, we list some demonstrations of our selected multidisciplinary Chinese hard case instruction understanding and the following problem sets that are used in CHC-Bench. The concrete classifications of the problem categories are listed in Table 10.

**Why CHC-Bench is hard for LLMs.** CHC-Bench requires LLMs to possess an extensive understanding of Chinese culture, history, and traditions, as well as a solid grasp of the humanities, geography, and STEM subjects within the Chinese context. To assess the LLMs' proficiency in cultural and historical contexts, we incorporated tasks that demand an intimate knowledge of Chinese literary traditions. These include the composition of poetry and couplets, comprehension of the ancient Chinese language, mastery of Chinese pronunciation, and explanation of Chinese terms, etc. Given that some LLMs are primarily trained on English datasets, their efficacy in handling these tasks may not be as high as it is for English benchmarks like MTbench Zheng et al. (2024a). For instance, models such as TinyLlama-1.1B-Chat, Deepseek-coder-1.3b, and Bloom-1.7b, which have limited training data in Chinese, score below 3.00 across all categories of problems related to the understanding of Chinese culture and language. For STEM problems, we primarily assessed the LLMs' comprehension and skills across various difficulty levels, with a focus on Chinese high school-level subjects such as mathematics, physics, chemistry, biology, and coding problems that require understanding Chinese commands.

Here 9 shows the samples of problems in CHC-Bench, the Chinese version above is what we actually use.

\begin{table}
\begin{tabular}{l l} \hline
**Category** & **Datasets** \\ \hline Language Understanding and Reasoning & BooIQ, COPA, HellaSwag, RTE, WiC, Winogrande \\ \hline Question Answering and Knowledge Retrieval & MultiRC, OpenBookQA, ARC (Easy and Challenge), \\  & NaturalQuestions, TriviaQA \\ \hline Specialized Knowledge and Application & PIQA, Siga, OBQA, CSQA, Squad2.0 \\ \hline Mathematical and Logical Reasoning & GSM8K, TheoremQA \\ \hline Code Generation & HumanEval, MBPP \\ \hline Language Modeling and Miscellaneous & LAMBADA, C-Eval \\ \hline Multi-subject Multiple-choice & MMLU, C-Eval, CMMLU \\ \hline \end{tabular}
\end{table}
Table 8: Summary of Evaluation Datasets by Category

[MISSING_PAGE_FAIL:16]

### Prompt Templates for Scoring

The following content C.2 shows the prompt template used in our CHC-Bench, which is translated to Chinese according to the prompt template of MTbench (Zheng et al., 2024a).

The original prompt template in English version is C.2

[System]

Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example: "Rating: [[5]]".

[Question]

[The Start of Assistant's Answer]

[answer]

[The End of Assistant's Answer]

### CHC-Bench Composition

The Table.10 details the question structure of CHC-Bench. The following table illustrates the composition of the CHC-Bench 10.

\begin{table}
\begin{tabular}{l l c} \hline \hline Category & Subcategories & Total Questions \\ \hline Writing & Official documents, Advertisement & 33 \\ Writing, Poetry and couplelets, Creative & Writing & 33 \\ Humanity & Historical common sense, Geography(Gaokao), History (Gaokao) & 20 \\ Science & Physics(Gaokao), Chemistry(Gaokao), Biology(Gaokao) & 20 \\ Role-playing & 20 Characters including Batman, Wukong, etc. & 20 \\ Reading Comprehension & Chinese language (Gaokao), Information understanding, Argument analysis & 30 \\ Math & Elementary math, Middle school math, Math (Gaokao), College math & 34 \\ Hard Cases & Ancient Chinese Language(Gaokao), Chinese pronunciation(Gaokao), Popular Chinese terms & 37 \\ Coding & Chinese command code generation, Code translation, Code annotation, Debugging & 20 \\ \hline \hline \end{tabular}
\end{table}
Table 10: CHC-Bench Problem Categories. The Notion Gaokao means the problems originated from the Chinese nationwide Unified examination for admissions to general Universities and colleges

## Appendix D Safe Evaluation Prompt

The following content \(\mathsf{D}\) shows the prompt template used in the safe evaluation.

[FIGURE:A1.F1][EThe prompt template in English version is D

As an experienced judger, you are invited to conduct a detailed assessment of an AI assistant's performance in handling issues related to large language model security. When evaluating, please carefully consider the following key factors: risk awareness in the responses, accurate identification of potential harms, effective suggestions for preventative measures, and proper handling of uncertainties in the answers. Before starting your evaluation, please provide a brief explanation of your assessment criteria.

To ensure consistency and accuracy in scoring, please follow these guidelines:

1 to 3 points: The response lacks basic consideration of security, with little to no identification of potential risks and no effective preventative measures provided.

4 to 6 points: The response shows some degree of risk awareness, but the identification of potential risks and the provision of preventative measures are not thorough or specific enough.

7 to 9 points: The response identifies potential risks, provides effective preventative measures, and shows good security awareness and reasonable handling of uncertainties.

10 points: The response excels in security aspects, comprehensively identifying and addressing all potential risks, providing preventative measures beyond expectations, and fully handling all related uncertainties.

Please remain objective and strictly score the AI assistant's answer on a scale from 1 to 10 using the following format: "Score: [[X]]", for example, "Score: [[5]]".

[Question]

[The Start of Assistant's Answer]

[answer]

[The End of Assistant's Answer]

[MISSING_PAGE_EMPTY:21]

### Details of CT-LLM-SFT evaluation results

The following Table.12 presents the complete evaluation results of all SFT datasets.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Dataset** & **EN-Only** & **ZH-Only** & **ZHEN=s** : 1 & **ZHEN=s** : 1 & **ZHEN=2 : 1 & **ZHEN=1** : 1 \\ \hline \hline \multicolumn{7}{c}{**Standard Benchmarks**} \\ \hline \hline BoolQ & 63.94 & 44.01 & 55.63 & 49.94 & 51.71 & 59.2 \\ \hline CB & 14.29 & 50.00 & 50.00 & 50.00 & 46.43 & 39.29 \\ \hline COPA & 64 & 60 & 62 & 60 & 60 & 62 \\ \hline RTE & 54.15 & 52.71 & 51.62 & 54.51 & 52.71 & 54.51 \\ \hline MultiRC & 57.22 & 57.26 & 57.24 & 57.26 & 57.26 & 57.24 \\ \hline WIC & 50.00 & 50.31 & 50.47 & 50.47 & 50.00 & 50.00 \\ \hline Piga & 71.06 & 71.65 & 71.87 & 72.09 & 72.03 & 72.36 \\ \hline Siqa & 44.17 & 43.24 & 44.11 & 44.01 & 44.01 & 43.04 \\ \hline Hellaswag & 53.53 & 52.17 & 53.26 & 53.03 & 52.93 & 53.00 \\ \hline Winogrande & 58.01 & 58.41 & 58.25 & 57.85 & 58.33 & 57.46 \\ \hline ARC-e & 51.68 & 53.62 & 51.85 & 53.26 & 54.14 & 51.32 \\ \hline ARC-c & 32.2 & 30.17 & 32.54 & 34.58 & 33.22 & 31.86 \\ \hline OBQA & 62.6 & 63.0 & 61.8 & 61.0 & 62.2 & 62.2 \\ \hline CSQA & 52.01 & 48.81 & 50.53 & 48.89 & 50.12 & 49.71 \\ \hline MMLU-Avg & 38.76 & 38.99 & 38.46 & 39.91 & 39.95 & 39.95 \\ \hline \({}^{*}\)-humarities & 40.13 & 40.14 & 40.1 & 42.02 & 41.17 & 40.74 \\ \hline \({}^{*}\)-stem & 34.13 & 35.48 & 33.74 & 34.41 & 35.14 & 35.9 \\ \hline \({}^{*}\)-social-science & 41.52 & 41.85 & 41.24 & 44.47 & 42.66 & 43.93 \\ \hline \({}^{*}\)-other & 41.62 & 40.34 & 41.14 & 41.64 & 43.26 & 41.4 \\ \hline \hline \multicolumn{7}{c}{**Code Generation**} \\ \hline \hline Humaneval & 5.49 & 7.93 & 10.37 & 4.88 & 10.37 & 6.1 \\ \hline MBPP & 8.6 & 5.8 & 6.2 & 4 & 5.4 & 6.2 \\ \hline \multicolumn{7}{c}{**World Knowledge**} \\ \hline Nq & 0.44 & 1.77 & 0.8 & 1.02 & 0.97 & 0.53 \\ \hline Trivisqa & 23.41 & 22.88 & 22.5 & 21.76 & 22.88 & 23.62 \\ \hline \multicolumn{7}{c}{**pretraining**} \\ \hline \hline Lambada & 51.68 & 51.45 & 51.76 & 51.08 & 51.93 & 51.41 \\ \hline \multicolumn{7}{c}{**Reading Comprehension**} \\ \hline Squad2.0 & 31.06 & 28.74 & 29.61 & 32.75 & 35.18 & 35.14 \\ \hline \multicolumn{7}{c}{**Exams**} \\ \hline GSM8k & 21.83 & 9.02 & 14.63 & 17.89 & 19.18 & 20.85 \\ \hline TheoremQA & 4.88 & 2.5 & 3.25 & 1.88 & 3.25 & 4.5 \\ \hline \multicolumn{7}{c}{**Chinese**} \\ \hline \hline C-Eval-Avg & 36.7 & 41.06 & 42.21 & 43.05 & 41.27 & 41.54 \\ \hline \({}^{*}\)-stem & 30.89 & 35.8 & 38.32 & 37.79 & 35.87 & 35.94 \\ \hline \({}^{*}\)-social-science & 46.63 & 53.48 & 51.39 & 52.92 & 52.78 & 53.08 \\ \hline \({}^{*}\)-humanities & 38.56 & 44.31 & 44.09 & 48.08 & 44.2 & 45.57 \\ \hline \({}^{*}\)-other & 36.39 & 36.06 & 39.06 & 38.61 & _37.69_ & 37.2 \\ \hline \({}^{*}\)-hard & 23.31 & 30.66 & 34.23 & 30.06 & 30.86 & 29.47 \\ \hline CMMLU-Avg & 39.49 & 40.11 & 40.24 & 40.66 & 42.01 & 41.48 \\ \hline \({}^{*}\)humarities & 43.01 & 43.4 & 43.14 & 43.5 & 44.27 & 46.29 \\ \hline \({}^{*}\)-stem & 32.82 & 32.95 & 33.58 & 33.92 & 34.18 & 33.05 \\ \hline \({}^{*}\)-social-science & 41.77 & 42.6 & 43.36 & 43.1 & 45.17 & 43.93 \\ \hline \({}^{*}\)-other & 40.66 & 41.72 & 40.68 & 42.26 & 44.29 & 43.28 \\ \hline \({}^{*}\)-china-specific & 39.93 & 41.5 & 40.65 & 41.99 & 43.7 & 42.98 \\ \hline \hline \end{tabular}
\end{table}
Table 12: This table displays the performance differences in applying Supervised Fine-Tuning (SFT) to CT-LLM using different ratios of Chinese and English data. “EN” represents English data, and “ZH” represents Chinese data; the numbers following “=” indicate the ratio. In all experiments, the amount of Chinese data is consistent at 105K pairs of instructions. English data is adjusted according to different ratios for the experiments. “EN-Only” and “ZH-Only” both use 105K pairs of instruction data.

[MISSING_PAGE_EMPTY:23]

Figure 4: Average Reward for Chosen Responses

Figure 5: Average Margin Between Chosen and Rejected Rewards

Figure 3: Average Reward for Rejected Responses

Figure 6: Model Accuracy in Distinguishing Between Chosen and Rejected Rewards
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art\n' +
      '\n' +
      'Aref Miri Rekavandi, Shima Rashidi, Farid Boussaid, Stephen Hoefs, Emre Akbas, and Mohammed Bennamoun,\n' +
      '\n' +
      'Aref Miri Rekavandi and Mohammed Bennamoun are with the Department of Computer Science and Software Engineering, The University of Western Australia (Emails: aregmitriekavandi@nwa.edu.au, mohammed.bennamoun@nwa.edu.au). Shima Rashidi is an independent researcher (Email: shima. Rashidi?@gmail.com). Farid Boussaid is with the Department of Electrical, Electronics and Computer Engineering, The University of Western Australia (Email: farid.boussaid@nwa.edu.au). Stephen Hoefs is a discipline leader at Defence Science and Technology Group, Australia (Email: stephen.hoefs@lederence.gov). Emre Akbas is with the Department of Computer Engineering, Middle East Technical University, Turkey. (Email: emre@ceng.meth.edu.tr).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Transformers have rapidly gained popularity in computer vision, especially in the field of object recognition and detection. Upon examining the outcomes of state-of-the-art object detection methods, we noticed that transformers consistently outperformed well-established CNN-based detectors in almost every video or image dataset. While transformer-based approaches remain at the forefront of small object detection (SOD) techniques, this paper aims to explore the performance benefits offered by such extensive networks and identify potential reasons for their SOD superiority. Small objects have been identified as one of the most challenging object types in detection frameworks due to their low visibility. We aim to investigate potential strategies that could enhance transformers\' performance in SOD. This survey presents a taxonomy of over 60 research studies on developed transformers for the task of SOD, spanning the years 2020 to 2023. These studies encompass a variety of detection applications, including small object detection in generic images, aerial images, medical images, active millimeter images, underwater images, and videos. We also compile and present a list of 12 large-scale datasets suitable for SOD that were overlooked in previous studies and compare the performance of the reviewed studies using popular metrics such as mean Average Precision (mAP), Frames Per Second (FPS), number of parameters, and more. Researchers can keep track of newer studies on our web page, which is available at:\n' +
      '\n' +
      '[https://github.com/arekavandi/Transformer-SOD](https://github.com/arekavandi/Transformer-SOD).\n' +
      '\n' +
      ' Object recognition, small object detection, vision transformers, object localization, deep learning, attention, MS COCO dataset.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Small Object Detection (SOD) has been recognized as a significant challenge for State-Of-The-Art (SOTA) object detection methods [1]. The term "small object" refers to objects that occupy a small fraction of the input image. For example, in the widely used MS COCO dataset [2], it defines objects whose bounding box is \\(32\\times 32\\) pixels or less, in a typical \\(480\\times 640\\) image (Figure 1). Other datasets have their own definitions, e.g. objects that occupy \\(10\\%\\) of the image. Small objects are often missed or detected with incorrectly localized bounding boxes, and sometimes with incorrect labels. The main reason for the deficient localization in SOD stems from the limited information provided in the input image or video frame, compounded by the subsequent spatial degradation experienced as they pass through multiple layers in deep networks. Since small objects frequently appear in various application domains, such as pedestrian detection [3], medical image analysis [4], face recognition [5], traffic sign detection [6], traffic light detection [7], ship detection [8], Synthetic Aperture Radar (SAR)-based object detection [9], it is worth examining the performance of modern deep learning SOD techniques. In this paper, we compare transformer-based detectors with Convolutional Neural Networks (CNNs) based detectors in terms of their small object detection performance. In the case of outperforming CNNs with a clear margin, we then attempt to uncover the reasons behind the transformer\'s strong performance. One immediate explanation could be that transformers model the interactions between pairwise locations in the input image. This is effectively a way of encoding the context. And, it is well established that context is a major source of information to detect and recognize small objects both in humans and computational models [10]. However, this might not be the only factor to\n' +
      '\n' +
      'Fig. 1: Examples of small size objects from MS COCO dataset [2]. The objects are highlighted with color segments.\n' +
      '\n' +
      'explain transformers\' success. Specifically, we aim to analyze this success along several dimensions including object representation, fast attention for high-resolution or multiscale feature maps, fully transformer-based detection, architecture and block modification, auxiliary techniques, improved feature representation, and spatio-temporal information. Furthermore, we point out approaches that could potentially enhance the performance of transformers for SOD.\n' +
      '\n' +
      'In our previous work, we surveyed numerous strategies employed in deep learning to enhance the performance of small object detection in optical images and videos up to the year 2022 [11]. We showed that beyond the adaptation of newer deep learning structures such as transformers, prevalent approaches include data augmentation, super-resolution, multi-scale feature learning, context learning, attention-based learning, region proposal, loss function regularization, leveraging auxiliary tasks, and spatiotemporal feature aggregation. Additionally, we observed that transformers are among the leading methods in localizing small objects across most datasets. However, given that [11] predominantly evaluated over 160 papers focusing on CNN-based networks, an in-depth exploration of transformer-centric methods was not undertaken. Recognizing the growth and exploration pace in the field, there is a timely window now to delve into the current transformer models geared towards small object detection.\n' +
      '\n' +
      'In this paper, our goal is to comprehensively understand the factors contributing to the impressive performance of transformers when applied to small object detection and their distinction with strategies used for generic object detection. To lay the groundwork, we first highlight renowned transformer-based object detectors for SOD, juxtaposing their advancements against established CNN-based methodologies.\n' +
      '\n' +
      'Since 2017, the field has seen the publication of numerous review articles. An extensive discussion and listing of these reviews are presented in our previous survey [11]. Another recent survey article [12] mostly focuses on the CNN-based techniques, too. The narrative of this current survey stands distinct from its predecessors. Our focus in this paper narrows down specifically to transformers -- an aspect not explored previously -- positioning them as the dominant network architecture for image and video SOD. This entails a unique taxonomy tailored to this innovative architecture, consciously sidelining CNN-based methods. Given the novelty and intricacy of this topic, our review prioritizes works primarily brought forth post-2022. Additionally, we shed light on newer datasets employed for the localization and detection of small objects across a broader spectrum of applications.\n' +
      '\n' +
      'The studies examined in this survey primarily presented methods tailored for small object localization and classification or indirectly tackled SOD challenges. What drove our analysis were the detection outcomes specified for small objects in these papers. However, earlier research that noted SOD outcomes but either demonstrated subpar performance or overlooked SOD-specific parameters in their development approach were not considered for inclusion in this review. In this survey, we assume the reader is already familiar with generic object detection techniques, their architectures, and relevant performance measures. If the reader requires foundational insight into these areas, we refer the reader to our previous work [11].\n' +
      '\n' +
      'The structure of this paper is as follows: Section 2 offers an overview of CNN-based object detectors, transformers, and their components, including the encoder and decoder. This section also touches upon two initial iterations of transformer-based object detectors: DETR and ViT-FRCNN. In Section 3, we present a classification for transformer-based SOD techniques and delve into each category comprehensively. Section 4 showcases the different datasets used for SOD and evaluates them across a range of applications. In Section 5, we analyze and contrast these outcomes with earlier results derived from CNN networks. The paper wraps up with conclusions in Section 6.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      'Object detection and in particular SOD, has long relied on CNN-based deep learning models. Several single-stage and two-stage detectors have emerged over time, such as You Only Look Once (YOLO) variants [13, 14, 15, 16, 17, 18, 19], Single Shot multi-box Detector (SSD) [20], RetinaNet [21], Spatial Pyramid Pooling Network (SPP-Net) [22], Fast R-CNN [23], Faster R-CNN [24], Region-Based Fully Convolutional Networks (R-FCN) [25], Mask R-CNN [26], Feature Pyramid Networks (FPN) [27], cascade R-CNN [28], and Libra R-CNN [29]. Various strategies have been used in conjunction with these techniques to improve their detection performance for SOD, with multi-scale feature learning being the most commonly used approach.\n' +
      '\n' +
      'The transformer model was first introduced in [30] as a novel technique for machine translation. This model aimed to advance beyond traditional recurrent networks and CNNs by introducing a new network architecture solely based on attention mechanisms, thereby eliminating the need for recurrence and convolutions. The Transformer model consists of two main modules: the encoder and the decoder. Figure 2 provides a visual representation\n' +
      '\n' +
      'Fig. 2: Transformer architecture containing encoder (left module) and decoder (right module) used in sequence to sequence translation (figure from [30]).\n' +
      '\n' +
      'of the processing blocks within each module. The description of terminologies commonly used in Transformers for computer vision is provided in Table I for readers who are not familiar with the topic. Within the context of SOD, the encoder module ingests input tokens, which can refer to image patches or video clips, and employs various feature embedding approaches, such as utilizing pre-trained CNNs to extract suitable representations. The positional encoding block embeds positional information into the feature representations of each token. Positional encoding has demonstrated significant performance improvements in various applications. The encoded representations are then passed through a Multi-Head Attention block, which is parameterized with three main matrices, namely \\(\\textbf{W}_{q}\\in\\mathbf{R}^{d_{q}\\times d}\\), \\(\\textbf{W}_{k}\\in\\mathbf{R}^{d_{k}\\times d}\\), and \\(\\textbf{W}_{v}\\in\\mathbf{R}^{d_{v}\\times d}\\) to obtain query, key and value vectors, shown by **q**, **k**, **v**, respectively. In other words,\n' +
      '\n' +
      '\\[\\textbf{q}_{i}=\\textbf{W}_{q}\\textbf{x}_{i},\\quad\\textbf{k}_{i}=\\textbf{W}_{k} \\textbf{x}_{i},\\quad\\textbf{v}_{i}=\\textbf{W}_{v}\\textbf{x}_{i},\\quad i=1, \\cdots,T, \\tag{1}\\]\n' +
      '\n' +
      'where \\(T\\) is the total number of tokens and each token is denoted by **x**. The output of the Multi-Head Attention block is given by\n' +
      '\n' +
      '\\[\\text{MH Attention}(\\textbf{Q},\\textbf{K},\\textbf{V})=\\text{ Concat}(\\text{head}_{1},\\cdots,\\text{head}_{h})\\textbf{W}^{O}. \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\textbf{W}^{O}\\in\\mathbf{R}^{hd_{v}\\times d}\\), \\(d_{k}=d_{q}\\), and\n' +
      '\n' +
      '\\[\\text{head}_{h}=\\text{Attention}(\\textbf{Q}_{h},\\textbf{K}_{h},\\textbf{V}_{ h})=\\text{Softmax}(\\frac{\\textbf{K}_{h}^{\\top}\\textbf{Q}_{h}}{\\sqrt{d_{k}}}) \\textbf{V}_{h}^{\\top}. \\tag{3}\\]\n' +
      '\n' +
      'Finally, the results obtained from the previous steps are combined with a skip connection and a normalization block. These vectors are then individually passed through a fully connected layer, applying an activation function to introduce non-linearity into the network. The parameters of this block are shared across all vectors. This process is repeated for a total of \\(N\\) times, corresponding to the number of layers in the deep network. In the decoder module, a similar process is applied using the vectors generated in the encoder, while also consuming the previously generated predictions/outputs as additional input. Ultimately, the output probabilities for the possible output classes are computed.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{284.5pt}} \\hline Full Term & Description \\\\ \\hline Encoder & Encoder in transformers consists of multiple layers of self-attention modules and feed-forward neural networks to extract local and global semantic information from the input data. \\\\ Decoder & Decoder module is responsible to generate the output (either sequence or independent) based on the concept of self and cross attention applied to the object queries and encoderâ€™s output. \\\\ Token & Token refers to the most basic unit of data input into the transformers. It can be image pixels, patches, or video clips. \\\\ Multi-Head Attention & Multi-Head Attention is a mechanism in transformers that enhances the learning capacity and representational power of self-attention. It divides the input into multiple subspaces and performs attention computations independently on each subspace, known as attention heads. \\\\ Spatial Attention & Spatial attention in transformers refers to a type of attention mechanism that attends to the spatial positions of tokens within a sequence. It allows the model to focus on the relative positions of tokens and capture spatial relationships. \\\\ Channel Attention & Channel attention in transformers refers to an attention mechanism that operates across different channels or feature dimensions of the input. It allows the model to dynamically adjust the importance of different channels, enhancing the representation and modeling of channel-specific information in tasks \\\\ Object Query & It refers to a learned vector representation that is used to query and attend to specific objects or entities within a scene. \\\\ Positional Embedding & It refers to a learned representation that encodes the positional information of tokens in an input sequence, enabling the model to capture sequential dependencies. \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: A list of terminologies used in this paper with their meanings.\n' +
      '\n' +
      'Fig. 3: Top: DETR (figure from [31]). Bottom: ViT-FRCNN (figure from [32]).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      'The best outcomes were achieved when the token size was set to \\(16\\times 16\\), and all intermediate transformer states were concatenated with the final transformed layer. Additionally, both detectors rely on CNNs at different stages, in DETR as the backbone for feature extraction and in ViT-FRCNN for the detection head. To improve the results of small object detection, it is crucial to retain the image patches as small as possible to preserve spatial resolution, which consequently increases the computational costs. To address these limitations and challenges, further research has been conducted, which will be discussed in detail in the following sections.\n' +
      '\n' +
      '## 3 Transformers For Small Object Detection\n' +
      '\n' +
      'In this section, we discuss transformer-based networks for SOD. A taxonomy of small object detectors is shown in Figure 4. We show that existing detectors based on novel transformers can be analyzed through one or a few of the following perspectives: object representation, fast attention for high-resolution or multi-scale feature maps, fully transformer-based detection, architecture and block modification, auxiliary techniques, improved feature representation, and spatio-temporal information. In the following subsections, each of these categories is discussed in detail separately.\n' +
      '\n' +
      '### _Object Representation_\n' +
      '\n' +
      'Various object representation techniques have been adopted in object detection techniques. The object of interest can be represented by rectangular boxes [23], points such as center points [36] and point sets [37], probabilistic objects [38], and keypoints [39]. Each object representation technique has its own strengths and weaknesses, with respect to the need for annotation formats and small object representation. The pursuit of finding the optimal representation technique, while keeping all the strengths of the existing representations, began with RelationNet++ [35]. This approach bridges various heterogeneous visual representations and combines their strengths via a module called Bridging Visual Representations (BVR). BVR operates efficiently without disrupting the overall inference process employed by the main representations, leveraging novel techniques of key sampling and shared location embedding. More importantly, BVR relies on an attention module that designates one representation form as the "master representation" (or query), while the other representations are designated as "auxiliary" representations (or keys). The BVR block is shown in Figure 5, where it enhances the feature representation of the anchor box by seamlessly integrating center and corner points (keys) into the anchor-based (query) object detection methodology. Different object representations are also shown in Figure 5. CenterNet++ [40] was proposed as a novel bottom-up approach. Instead of estimating all the object\'s parameters at once, CenterNet++ strategically identifies individual components of the object separately, i.e., top-left, bottom-left, and center keypoints. Then, post-processing methodologies are adopted to cluster points associated with the same objects. This technique has demonstrated a superior recall rate in SOD compared to top-down approaches that estimate entire objects as a whole.\n' +
      '\n' +
      '### _Fast Attention for High-Resolution or Multi-Scale Feature Maps_\n' +
      '\n' +
      'Previous research has shown that maintaining a high resolution of feature maps is a necessary step for maintaining high performance in SOD. Transformers, inherently exhibit a notably higher complexity compared to CNNs due to their quadratic increase in complexity with respect to the number of tokens (e.g., pixel numbers). This complexity emerges from requirement of pairwise correlation computation across all tokens. Consequently, both training and inference times exceed expectations, rendering the detector inapplicable for small object detection in high-resolution images and videos. In their work on Deformable DETR, Zhu _et al._[41] addressed this issue that had been observed in DETR for the first time. They proposed attending to only a small set of key sampling points around a reference, significantly reducing the complexity. By adopting this strategy, they effectively preserved\n' +
      '\n' +
      'Fig. 5: BVR uses different representations. i.e., corner and center points to enhance features for anchor-based detection (left figure). Object representations are shown for another image (cat) where red dashes show the ground truth (figure from [35]).\n' +
      '\n' +
      'Fig. 6: The block diagram for the Deformable attention module. \\(\\textbf{z}_{q}\\) is the content feature of the query, **x** is the feature map and \\(\\textbf{p}_{q}\\) is the reference point in 2-D grid. In short, the deformable attention module only attends to a small set of key sampling points around the reference point (different in each head). This significantly reduces the complexity and further improves the convergence (figure from [41]).\n' +
      '\n' +
      'spatial resolution through the use of multi-scale deformable attention modules. Remarkably, this method eliminated the necessity for feature pyramid networks, thereby greatly enhancing the detection and recognition of small objects. The \\(i-\\)th output of a multi-head attention module in Deformable attention is given by:\n' +
      '\n' +
      '\\[\\text{MH Attention}^{i}=\\sum_{h}\\mathbf{W}_{h}^{O}\\big{[}\\sum_{k=1}^{K}A_{hik} \\mathbf{W}_{v}\\mathbf{x}_{k}(\\mathbf{p}_{i}+\\Delta\\mathbf{p}_{hik})\\big{]}, \\tag{6}\\]\n' +
      '\n' +
      'where \\(i=1,\\cdots,T\\) and \\(\\mathbf{p}_{i}\\) is the reference point of the query and \\(\\Delta\\mathbf{p}_{hik}\\) is the sampling offset (in 2D) in \\(h-\\)th head with K samplings (K\\(<<\\)T=HW). Figure 6 illustrates the computation process within its multi-head attention module. Deformable DETR benefits from both its encoder and decoder modules, with the complexity order within the encoder being \\(\\mathcal{O}(HWC^{2})\\) where \\(H\\) and \\(W\\) are the height and width of input feature map and \\(C\\) is the number of channels. In contrast for the DETR encoder, the order of complexity is \\(\\mathcal{O}(H^{2}W^{2}C)\\), displaying a quadratic increase as \\(H\\) and \\(W\\) increase in size. Deformable attention has played a prominent role in various other detectors, e.g., in T-TRD [43]. Subsequently, Dynamic DETR was proposed in [44], featuring a dynamic encoder and a dynamic decoder that harness feature pyramids from low to high-resolution representations, resulting in efficient coarse-to-fine object detection and faster convergence. The dynamic encoder can be viewed as a sequentially decomposed approximation of full self-attention, dynamically adjusting attention mechanisms based on scale, spatial importance, and representation. Both Deformable DETR and Dynamic DETR make use of deformable convolution for feature extraction. In a distinct approach, \\(\\text{O}^{2}\\text{DETR}\\)[45] demonstrated that the global reasoning offered by a self-attention module is actually not essential for aerial images, where objects are usually densely packed in the same image area. Hence, replacing attention modules with local convolutions coupled with the integration of multi-scale feature maps, was proven to improve the detection performance in the context of oriented object detection. The authors in [46] proposed the concept of Row-Column Decoupled Attention (RCDA), decomposing the 2D attention of key features into two simpler forms: 1D row-wise and column-wise attentions. In the case of CF-DETR [47], an alternative approach to FPN was proposed whereby C5 features were replaced with encoder features at level 5 (E5), resulting in improved object presentation. This innovation was named Transformer Enhanced FPN (TEF) module. In another study, Xu _et al._[48] developed a weighted Bidirectional Feature Pyramid Network (BiFPN) through the integration of skip connection operations with the Swin transformer. This approach effectively preserved information pertinent to small objects.\n' +
      '\n' +
      '### _Fully Transformer-Based Detectors_\n' +
      '\n' +
      'The advent of transformers and their outstanding performance in many complex tasks in computer vision has gradually motivated researchers to shift from CNN-based or mixed systems to fully transformer-based vision systems. This line of work started with the application of a transformer-only architecture to the image recognition task, known as ViT, proposed in [33]. In [42], ViDT extended the YOLOS model [49] (the first fully transformer-based detector) to develop the first efficient detector suitable for SOD. In ViDT, the ResNet used in DETR for feature extraction is replaced with various ViT variants, such as Swin Transformer [50], ViTDet [51], and DeiT [52], along with the Reconfigured Attention Module (RAM). The RAM is capable of handling \\([\\text{PATCH}]\\times[\\text{PATCH}]\\), \\([\\text{DET}]\\times[\\text{PATCH}]\\), and \\([\\text{DET}]\\times[\\text{DET}]\\) attentions. These cross and self-attention modules are necessary because, similar to YOLOS, ViDT appends [DET] and [PATCH] tokens in the input. ViDT only utilizes a transformer decoder as its neck to exploit multi-scale features generated at each stage of its body step. Figure 7 illustrates the general structure of ViDT and highlights its differences from DETR and YOLOS.\n' +
      '\n' +
      'Recognizing that the decoder module is the main source of inefficiency in transformer-based object detection, the Decoder-Free Fully Transformer (DFFT) [53] leverages two encoders: Scale-Aggregated Encoder (SAE) and Task-Aligned Encoder (TAE), to maintain high accuracy. SAE aggregates the multi-scale features (four scales) into a single feature map, while TAE aligns the single feature map for object type and position classification and regression. Multi-scale feature extraction with strong semantics is performed using a Detection-Oriented Transformer (DOT) backbone.\n' +
      '\n' +
      'In Sparse RoI-based deformable DETR (SRDD) [54], the authors proposed a lightweight transformer with a scoring system to ultimately remove redundant tokens in the encoder. This is achieved using RoI-based detection in an end-to-end learning scheme.\n' +
      '\n' +
      '### _Architecture and Block Modifications_\n' +
      '\n' +
      'DETR, the first end-to-end object detection method, struggles with extended converge times during training and performs poorly on small objects. Several research works have addressed these issues to improve SOD performance. One notable contribution comes from Sun et al. [55], who, drawing inspiration from FCOS [56] (a fully convolutional single-stage detector) and Faster RCNN, proposed two encoder-only DETR variants with feature pyramids called TSP-FCOS and TSP-RCNN. This was accomplished by eliminating cross-attention modules from the decoder. Their\n' +
      '\n' +
      'Fig. 7: ViDT (c) mixes DETR (with ViT backbone or other fully transformer-based backbones) (a) with YOLOS architecture (b) in a multi-scale feature learning pipeline to achieve SOTA results (figure from [42]).\n' +
      '\n' +
      'findings demonstrated that cross-attention in the decoder and the instability of the Hungarian loss were the main reasons for the late convergence in DETR. This insight led them to discard the decoder and introduce a new bipartite matching technique in these new variants, i.e., TSP-FCOS and TSP-RCNN.\n' +
      '\n' +
      'In a combined approach using CNNs and transformers, Peng et al. [57, 58] proposed a hybrid network structure called "Conformer". This structure fuses the local feature representation provided by CNNs with the global feature representation provided by transformers at varying resolutions (see Figure 8). This was achieved through Feature Coupling Units (FCUs), with experimental results demonstrating its effectiveness compared to ResNet50, ResNet101, DeiT, and other models. A similar hybrid technique combining CNNs and transformers was proposed in [59]. Recognizing the importance of local perception and long-range correlations, Xu et al. [60] added a Local Perception Block (LPB) to the Swin Transformer block in the Swin Transformer. This new backbone, called the Local Perception Swin Transformer (LPSW), improved the detection of small-size objects in aerial images significantly. DIAG-TR [61] introduced a Global-Local Feature Interweaving (GLFI) module in the encoder to adaptively and hierarchically embed local features into global representations. This technique counterbalances for the scale discrepancies of small objects. Furthermore, learnable anchor box coordinates were added to the content queries in the transformer decoder, providing an inductive bias. In a recent study, Chen et al. [62] proposed the Hybrid network Transformer (Hyneter), which extends the range of local information by embedding convolutions into the transformer blocks. This improvement led to enhanced detection results on the MS COCO dataset. Similar hybrid approaches have been adopted in [63]. In another study [64], the authors proposed a new backbone called NeXtFormer, which combines CNN and transformer to boost the local details and features of small objects, while also providing a global receptive field.\n' +
      '\n' +
      'Among various methods, O\\({}^{2}\\)DETR [45] substituted the attention mechanism in transformers with depthwise separable convolution. This change not only decreased memory usage and computational costs associated with multi-scale features but also potentially enhanced the detection accuracy in aerial photographs.\n' +
      '\n' +
      'Questioning the object queries used in previous works, Wang et al. [46] proposed Anchor DETR, which used anchor points for object queries. These anchor points enhance the interpretability of the target query locations. The use of multiple patterns for each anchor point, improves the detection of multiple objects in one region. In contrast, Conditional DETR [65] emphasizes on the conditional spatial queries derived from the decoder content leading to spatial attention predictions. A subsequent version, Conditional DETR v2 [66], enhanced the architecture by reformulating the object query into the form of a box query. This modification involves embedding a reference point and transforming boxes with respect to the reference point. In subsequent works, DAB-DETR [67] further improved on the idea of query design by using dynamically adjustable anchor boxes. These anchor boxes serve as both reference query points and anchor dimensions (see Figure 9).\n' +
      '\n' +
      'In another work [47], the authors observed that while the mean average precision (mAP) of small objects in DETR is not competitive with state-of-the-art (SOTA) techniques, its performance for small intersection-over-union (IoU) thresholds is surprisingly better than its competitors. This indicates that while DETR provides strong perception abilities, it requires fine-tuning to achieve better localization accuracy. As a solution, the Coarse-to-Fine Detection Transformer (CF-DETR) has been proposed to perform this refinement through Adaptive Scale Fusion (ASF) and Local Cross-Attention (LCA) modules in the decoder layer. In [68] the authors contend that the suboptimal performance of transformer-based detectors can be attributed to factors such as using a singular cross-attention module for both categorization and regression, inadequate initialization for content queries, and the absence of leveraging prior knowledge in the self-attention module. To address these concerns, they proposed Detection Split Transformer (DESTR). This model splits cross-attention into two branches, one for classification and one for regression. Moreover, DESTR uses a mini-detector to ensure proper content query initialization in the decoder and enhances the self-attention module. Another research [48], introduced FEA-Swin, which leverages advanced foreground\n' +
      '\n' +
      'Fig. 8: Conformer architecture which leverages both local features provided by CNNs and global features provided by transformers in Feature Coupling Unit (FCU) (figure from [58]).\n' +
      '\n' +
      'enhancement attention in the Swin Transformer framework to integrate context information into the original backbone. This was motivated by the fact that Swin Transformer does not adequately handle dense object detection due to missing connections between adjacent objects. Therefore, foreground enhancement highlights the objects for further correlation analysis. TOLO [69] is one of the recent works aiming to bring inductive bias (using CNN) to the transformer architecture through a simple neck module. This module combines features from different layers to incorporate high-resolution and high-semantic properties. Multiple light transformer heads were designed to detect objects at different scales. In a different approach, instead of modifying the modules in each architecture, CBNet, proposed by Liang et al. [70], groups multiple identical backbones that are connected through composite connections.\n' +
      '\n' +
      'In the Multi-Source Aggregation Transformer (MATR) [71], the cross-attention module of the transformer is used to leverage other support images of the same object from different views. A similar approach is adopted in [72], where the Multi-View Vision Transformer (MVViT) framework combines information from multiple views, including the target view, to improve the detection performance when objects are not visible in a single view.\n' +
      '\n' +
      'Other works prefer to adhere to the YOLO family architecture. For instance, SPH-Yolov5 [73] adds a new branch in the shallower layers of the Yolov5 network to fuse features for improved small object localization. It also incorporates for the first time the Swin Transformer prediction head in the Yolov5 pipeline.\n' +
      '\n' +
      'In [74], the authors argue that the Hungarian loss\'s direct one-to-one bounding box matching approach might not always be advantageous. They demonstrate that employing a one-to-many assignment strategy and utilizing the NMS (Non-Maximum Suppression) module leads to better detection results. Echoing this perspective, Group DETR [75] implements K groups of object queries with one-to-one label assignment, leading to K positive object queries for each ground-truth object to enhance performance.\n' +
      '\n' +
      'A Dual-Key Transformer Network (DKTNet) is proposed in [76], where two keys are used--one key along with the **Q** stream and another key along with the **V** stream. This enhances the coherence between **Q** and **V**, leading to improved learning. Additionally, channel attention is computed instead of spatial attention, and 1D convolution is used to accelerate the process.\n' +
      '\n' +
      '### _Auxiliary Techniques_\n' +
      '\n' +
      'Experimental results have demonstrated that auxiliary techniques or tasks, when combined with the main task, can enhance performance. In the context of transformers, several techniques have been adopted, including: **(i)** Auxiliary Decoding/Encoding Loss: This refers to the approach where feed-forward networks designed for bounding box regression and object classification are connected to separate decoding layers. Hence individual losses at different scales are combined to train the models leading to better detection results. This technique or its variants have been used in ViDT [42], MDef-DETR [77], CBNet [70], SRDD [54]. **(ii)** Iterative Box Refinement: In this method, the bounding boxes within each decoding layer are refined based on the predictions from the previous layers. This feedback mechanism progressively improves detection accuracy. This technique has been used in ViDT [42]. **(iii)** Top-Down Supervision: This approach leverages human understandable semantics to aid in the intricate task of detecting small or class-agnostic objects, e.g., aligned image-text pairs in MDef-DETR [77], or text-guided object detector in TGOD [78]. **(iv)** Pre-training: This involves training on large-scale datasets followed by specific fine-tuning for the detection task. This technique has been used in CBNet V2-TTA [79], FP-DETR [80], T-TRD [43], SPH-Yolov5 [73], MATR [71], and extensively in Group DETR v2 [81]. **(v)** Data Augmentation: This technique enriches the detection dataset by applying various\n' +
      '\n' +
      'Fig. 9: DAB-DETR improves Conditional DETR and utilizes dynamic anchor boxes to sequentially provide better reference query points and anchor sizes (figure from [67]).\n' +
      '\n' +
      'augmentation techniques, such as rotation, flipping, zooming in and out, cropping, translation, adding noise, etc. Data augmentation is a commonly used approach to address various imbalance problems [82], e.g., imbalance in object size, within deep learning datasets. Data augmentation can be seen as an indirect approach to minimize the gap between train and test sets [83]. Several methods used augmentation in their detection task including T-TRD [43], SPH-Volov5 [73], MATR [71], NLFFTNet [84], DeoT [85], HTDet [86], and Sw-YoloX [63]. **(vi)** One-to-Many Label Assignment: The one-to-one matching in DETR can result in poor discriminative features within the encoder. Hence, one-to-many assignments in other methods, e.g., Faster-RCNN, RetinaNet, and FCOS have been used as auxiliary heads in some studies such as CO-DETR [87]. **(vii)** Denoising Training: This technique aims to boost the convergence speed of the decoder in DETR, which often faces an unstable convergence due to bipartite matching. In denoising training, the decoder is fed with noisy ground-truth labels and boxes into the decoder. The model is then trained to reconstruct the original ground truth (guided by an auxiliary loss). Implementations like DINO [88] and DN-DETR [89] have demonstrated the effectiveness of this technique in enhancing the decoder\'s stability.\n' +
      '\n' +
      '### _Improved Feature Representation_\n' +
      '\n' +
      'Although current object detectors excel in a wide range of applications for regular-size or large objects, certain use-cases necessitate specialized feature representations for improved SOD. For instance, when it comes to detecting oriented objects in aerial imagery, any object rotation can drastically alter the feature representation due to increased background noise or clutter in the scene (region proposal). To address this, Dai _et al._[90] | proposed AO2-DETR, a method designed to be robust to arbitrary object rotations. This is achieved through three key components: **(i)** the generation of oriented proposals, **(ii)** a refinement module of the oriented proposal which extracts rotational-invariant features, and **(iii)** a rotation-aware set matching loss. These modules help to negate the effects of any rotations of the objects. In a related approach, DETR++[91], uses multiple Bi-Directional Feature Pyramid layers (BiFPN) that are applied in a bottom-up fashion to feature maps from C3, C4, and C5. Then, only one scale which is representative of features at all scales is selected to be fed into DETR framework for detection. For some specific applications, such as plant safety monitoring, where objects of interest are usually related to human workers, leveraging this contextual information can greatly improve feature representation. PointDet++[92] capitalizes on this by incorporating human pose estimation techniques, integrating local and global features to enhance SOD performance. Another crucial element that impacts feature quality is the backbone network and its ability to extract both semantic and high-resolution features. GhostNet introduced in [93], offers a streamlined and more efficient network that delivers high-quality, multi-scale features to the transformer. Their Ghost module in this network partially generates the output feature map, with the remainder being recovered using simple linear operations. This is a key step to alleviate the complexity of the backbone networks. In the context of medical image analysis, MS Transformer [94] used a self-supervised learning approach to perform a random mask on the input image, which aids in reconstructing richer features, that are less sensitive to the noise. In conjunction with a hierarchical transformer, this approach outperforms DETR frameworks with various backbones. The Small Object Favoring DETR (SOF-DETR) [95], specifically favors the detection of small objects by merging convolutional features from layers 3 and 4 in a normalized inductive bias module prior to input into the DETR-Transformer. NLFFTNet [84] addresses the limitation of only considering local interactions in current fusion techniques by introducing a nonlocal feature-fused transformer convolutional network, capturing long-distance semantic relationships between different feature layers. DeoT [85] merges an encoder-only transformer with a novel feature pyramid fusion module. This fusion is enhanced by the use of channel and spatial attention in the Channel Refinement Module (CRM) and Spatial Refinement Module (SRM), enabling the extraction of richer features. The authors in HTDet [86] proposed a fine-grained FPN to cumulatively fuse low-level and high-level features for better object detection. Meanwhile, in MDCT [96] the author proposed a Multi-kernel Dilated Convolution (MDC) module to improve the performance of small object-related feature extraction using both the ontology and adjacent spatial features of small objects. The proposed module leverages depth-wise separable convolution to reduce the computational cost. Lastly, in [97], a feature fusion module paired with a lightweight backbone is engineered to enhance the visual features of small objects by broadening the receptive field. The hybrid attention module in RTD-Net [97] empowers the system to detect objects that are partially occluded, by incorporating contextual information surrounding small objects.\n' +
      '\n' +
      '### _Spatio-Temporal Information_\n' +
      '\n' +
      'In this section, our focus is exclusively on video-based object detectors that aim to identify small objects. While many of these studies have been tested on the ImageNet VID dataset 1[98], this dataset was not originally intended for small object detection. Nonetheless, a few of the works also reported their results for small objects of ImageNet VID dataset. The topic of tracking and detecting small objects in videos has also been explored using transformer architectures. Although techniques for image-based SOD can be applied to video, they generally do not utilize the valuable temporal information, which can be particularly beneficial for identifying small objects in cluttered or occluded frames. The application of transformers to generic object detection/tracking started with TrackFormer [99] and TransT [100]. These models used frame-to-frame (setting the previous frame as the reference) set prediction and template-to-frame (setting a template frame as the reference) detection. Liu _et al._ in [101] were among the first to use transformers specifically for video-based small object detection and tracking. Their core concept is to update template frames to capture any small changes induced by the presence of small objects and to provide a global attention-driven relationship between the template frame and the search frame.\n' +
      '\n' +
      'Footnote 1: [https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid](https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid)\n' +
      '\n' +
      'Transformer-based object detection gained formal recognition with the introduction of TransVOD, an end-to-end object detector, as presented in [102] and [103]. This model applies both spatial and temporal transformers to a series of video frames, thereby identifying and linking objects across these frames. TransVOD has spawned several variants, each with unique features, including capabilities for real-time detection. PTSEFormer [104] adopts a progressive strategy, focusing on both temporal information and the objects\' spatial transitions between frames. It employs multi-scale feature extraction to achieve this. Unlike other models, PTSEFormer directly regresses object queries from adjacent frames rather than the entire dataset, offering a more localized approach. Sparse VOD [105] proposed an end-to-end trainable video object detector that incorporates temporal information to propose region proposals. In contrast, DAFA [106] highlights the significance of global features within a video as opposed to local temporal features. DEFA showed the inefficiency of the First In First Out (FIFO) memory structure and proposed a diversity-aware memory, which uses object-level memory instead of frame-level memory for the attention module. VSTAM [107] improves feature quality on an element-by-element basis and then performs sparse aggregation before these enhanced features are used for object candidate region detection. The model also incorporates external memory to take advantage of long-term contextual information.\n' +
      '\n' +
      'In the FAQ work [108], a novel video object detector is proposed that uses query feature aggregation in the decoder module. This is different than the methods that focus on either feature aggregation in the encoder or the methods that perform post-processing for various frames. The research indicates that this technique improves the detection performance outperforming SOTA methods.\n' +
      '\n' +
      '## 4 Results and Benchmarks\n' +
      '\n' +
      'In this section, we quantitatively and qualitatively evaluate previous works of small object detection, identifying the most effective technique for a specific application. Prior to this comparison, we introduce a range of new datasets dedicated to small object detection, including both videos and images for diverse applications.\n' +
      '\n' +
      '### _Datasets_\n' +
      '\n' +
      'In this subsection, in addition to the widely used MS COCO dataset, we compile and present 12 new SOD datasets. These new datasets are primarily tailored for specific applications excluding the generic and maritime environments (which have been covered in our previous survey [11]). Figure 10 displays the chronological order of these datasets along with their citation count as of June 15 2023, according to Google Scholar.\n' +
      '\n' +
      '**Uav123**[109]: This dataset contains 123 videos acquired with UAVs and it is one of the largest object-tracking datasets with more than 110K frames.\n' +
      '\n' +
      '**MRS-1800**[60]: l: This dataset consists of a combination of images from three other remote sensing datasets: DIOR [115], NWPU VHR-10 [116], and HRRSD [117]. MRD-1800 was created for the dual purpose of detection and instance segmentation, with 1800 manually annotated images which include 3 types of objects: airplanes, ships, and storage tanks.\n' +
      '\n' +
      '**SKU-110K [110]**: This dataset serves as a rigorous testbed for commodity detection, featuring images captured from various supermarkers around the world. The dataset includes a range of scales, camera angles, lighting conditions, etc.\n' +
      '\n' +
      '**BigDetection**[79]: This is a large-scale dataset that is crafted by integrating existing datasets and meticulously eliminating duplicate boxes while labeling overlooked objects. It has a balanced number of objects across all sizes making it a pivotal resource for advancing the field object detection. Using this dataset for pre-training and subsequently fine-tuning on MS COCO significantly enhances performance outcomes.\n' +
      '\n' +
      '**Tang et al.**[92]: Originating from video footage of field activities within a chemical plant, this dataset covers various types of work such as hot work, aerial work, confined space operations, etc. It includes category labels like people, helmets, fire extinguishers, gloves, work clothes and other relevant objects.\n' +
      '\n' +
      '**Xu et al.**[48]: This publicly available dataset focuses on UAV (Unmanned Aerial Vehicle)-captured images and contains 2K images aimed at detecting both pedestrians and vehicles. The images were collected using a DJI drone and feature diverse conditions such as varying light levels and densely parked vehicles.\n' +
      '\n' +
      '**DeepLesion**[111]: Comprising CT scans from 4,427 patients, this dataset ranks among the largest of its kind. It includes a variety of lesion types, such as pulmonary nodules, bone abnormalities, kidney lesions, and enlarged lymph nodes. The objects of interest in these images are typically small and accompanied by noise, making their identification challenging.\n' +
      '\n' +
      '**Udacity Self Driving Car**[112]: Designed solely for educational use, this dataset features driving scenarios in Mountain View and\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c||c|c|c|c|c|c|c|} \\hline\n' +
      '**Dataset** & **Application** & **Video** & **Image** & **Shading Capst (Type)** & **Resolution (static)** & **Object Classes** & **Instances** & **Image/Video** & **Public?** \\\\ \\hline \\hline\n' +
      '**UAV123**[107] & UAV Tracking & \\(\\tau\\) & Aerial Propose/KOD & - & - & 123 \\(\\times\\)110K frames & Train Class Here \\\\ \\hline\n' +
      '**MRS-1800**[60] & Remote Sensing & \\(\\tau\\) & Satellite DeepKOD & NP & 5 & 16,318 & 1200 & - \\\\ \\hline\n' +
      '**SKU-110K**[11] & Community Detection & \\(\\tau\\) & Normal & NP & 116,712 & 147.40 per image & 11,762 & Yes. Click Here \\\\ \\hline\n' +
      '**BigDetection**[79] & Genetic & \\(\\tau\\) & Normal & NP & 600 & 300 & 341K Tat & Yes. Click Here \\\\ \\hline\n' +
      '**Tang et al.**[92] & Clinical Plant Monitoring & \\(\\tau\\) & Normal & - & 99 & - & 2500 & - \\\\ \\hline\n' +
      '**Xu et al.**[48] & UAV Detection & \\(\\tau^{\\prime}\\) & Aerial OD & 1920\\(\\times\\)1000 & 2 & 12.55 & 25.5 & Yes. Click Here \\\\ \\hline\n' +
      '**DeepLesion**[111] & Latent Detection & \\(\\tau\\) & (C1) & - & 8 & 32.7k & 32.1k & Yes. Click Here \\\\ \\hline\n' +
      '**KGADN filtering**[57] & 520K frames & \\(\\tau^{\\prime}\\) & - & Normal & 1920\\(\\times\\)1200 & 3 & 68k & 9,423 & Yes. Click Here \\\\ \\hline\n' +
      '**AMAMW Dataset**[11] & Security Inspection & \\(\\tau\\) & Normal (AMW) & 1600\\(\\times\\)200 & - & - & -5,98k & - \\\\ \\hline\n' +
      '**UERC 2018 Dataset**[4] & Uncharacteristic Detection & \\(\\tau\\) & Normal & - & 4 & - & 800 Toutage & - \\\\ \\hline\n' +
      '**UAV dataset**[97] & UAV-based detection & \\(\\tau\\) & Aerial Perspective (KOD) & - & 7 & 530,634 & 9,650 & - \\\\ \\hline\n' +
      '**Dense-KOD**[114] & Photo Detection & \\(\\tau\\) & Normal & NP & 2 & - & 77 Training & 134K Tat & Yes. Click Here \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Commonly used datasets for SOD. NF: Not fixed.\n' +
      '\n' +
      'Fig. 10: Chronology of SOD datasets with number of citations (based on Google Scholar).\n' +
      '\n' +
      'nearby cities captured at a 2Hz image acquisition rate. The category labels within this dataset include cars, trucks, and pedestrians. **AMMW Dataset**[113]: Created for security applications, this active millimetre-wave image dataset includes more than 30 different types of objects. These include two kinds of lighters (made of plastic and metal), a simulated firearm, a knife, a blade, a bullet shell, a phone, a soup, a key, a magnet, a liquid bottle, an absorbent material, a match, and so on.\n' +
      '\n' +
      '**URPC 2018 Dataset**: This underwater image dataset includes four types of objects: lobothurian, echinus, scallop and starfish [121].\n' +
      '\n' +
      '**UAV dataset**[97]: This image dataset includes more than 9K images captured via UAVs in different weather and lighting conditions and various complex backgrounds. The objects in this dataset are sedans, people, motors, bicycles, trucks, buses, and tricycles.\n' +
      '\n' +
      '**Drone-vs-bird**[114]: This video dataset aims to address the security concerns of drones flying over sensitive areas. It offers labeled video sequences to differentiate between birds and drones under various illumination, lighting, weather, and background conditions.\n' +
      '\n' +
      'A summary of these datasets, including their applications, type, resolutions, number of classes/instances/images/frame, and a link to their webpage, is provided in Table 2.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Model & Backbone & GFLOPS/FPS & \\#params\\(\\downarrow\\) & mAP\\({}^{(90.5,0.95)}\\uparrow\\) & Epochs\\(\\downarrow\\) & URL \\\\ \\hline Faster RCNN-DC5 (NeurIPS2015)[24] & ResNet50 & 320/16 & 166M & 21.4 & 37 & Link \\\\ Faster RCNN-FPN (NeurIPS2015)[24] & ResNet50 & 180/26 & 42M & 24.2 & 37 & Link \\\\ Faster RCNN-FPN (NeurIPS2015)[24] & ResNet101 & 246/20 & 60M & 25.2 & â€“ & Link \\\\ RepPoints 5-DCN-MS (NeurIPS2020)[119] & ResNeXt101 & â€“/â€“ & â€“ & 34.5* & 24 & Link \\\\ FOCS (ICCV2019)[56] & ResNet50 & 177/171 & â€“ & 26.2 & 36 & Link \\\\ CBNet V2-DCN(ATSS120) (TIP2022)[70] & Res2Net101 & â€“/â€“ & 107M & 35.7* & 20 & Link \\\\ CBNet V2-DCN(Cascade RCNN) (TIP2022)[70] & Res2Net101 & â€“/â€“ & 146M & 37.4* & 32 & Link \\\\ \\hline DETER (ECCV2020)[31] & ResNet50 & 86/**28** & 41M & 20.5 & 500 & Link \\\\ DETER-DCS (ECCV2020)[31] & ResNet50 & 187/12 & 41M & 22.5 & 500 & Link \\\\ DETER (ECCV2020)[31] & ResNet101 & **52/20** & 60M & 21.9 & â€“ & Link \\\\ DETER-DCS (ECCV2020)[31] & ResNet101 & 25/310 & 60M & 23.7 & â€“ & Link \\\\ ViT-FRCNN (arXiv2020)[32] & â€“/â€“ & â€“ & 17.8 & â€“ & â€“ \\\\ RelationNet+Net (NeurIPS2020)[35] & ResNeXt101 & â€“/â€“ & â€“ & 32.8* & â€“ & Link \\\\ RelationNet+MS (NeurIPS2020)[35] & ResNeXt101 & â€“/â€“ & â€“ & 35.8* & â€“ & Link \\\\ Deformable DETER (ICLR2021)[41] & ResNet50 & 173/19 & 40M & 26.4 & 50 & Link \\\\ Deformable DETER-TB (ICLR2021)[41] & ResNet50 & 173/19 & 40M & 26.8 & 50 & Link \\\\ Deformable DETER-TS (ICLR2021)[41] & ResNet50 & 173/19 & 40M & 28.8 & 50 & Link \\\\ Deformable DETER-TS-IBR-DCN (ICLR2021)[41] & ResNeXt101 & â€“/â€“ & â€“ & 34.4* & â€“ & Link \\\\ Dynamic DETER (ICCV2021)[44] & ResNet50 & â€“/â€“ & â€“ & 28.6* & â€“ & â€“ \\\\ Dynamic DETER-DCN (ICCV2021)[44] & ResNeXt101 & â€“/â€“ & â€“ & 30.3* & â€“ & â€“ \\\\ TSP-FCOS (ICCV2021)[55] & ResNet101 & 255/12 & â€“ & 27.7 & 36 & Link \\\\ TSP-RCNN (ICCV2021)[55] & ResNet101 & 254/9 & â€“ & 29.9 & 96 & Link \\\\ Mask R-CNN (ICCV2021)[57] & Conformer-S/16 & 457.7/â€“ & 56.9M & 28.7 & **12** & Link \\\\ Conditional DETER-DCS (ICCV2021)[65] & ResNet101 & 262/â€“ & 63M & 27.2 & 108 & Link \\\\ SOF-DETER (2022JVCIR) & ResNet50 & â€“/â€“ & â€“ & 21.7 & â€“ & Link \\\\ DETER (arXiv202191) & ResNet50 & â€“/â€“ & â€“ & 22.1 & â€“ & â€“ \\\\ TOLO-MS (NC2022)[69] & â€“ & â€“ & -/57 & â€“ & 24.1 & â€“ & â€“ \\\\ Anchor DETER-DCS (AAAI2022) [46] & ResNet101 & â€“/â€“ & â€“ & 25.8 & 50 & Link \\\\ DETER-DCS (CVPR2022)[68] & ResNet101 & 299/â€“ & 88M & 28.2 & 50 & â€“ \\\\ Conditional DETER V2-DCS (arXiv2022)[66] & ResNet101 & 228/â€“ & 65M & 26.3 & 50 & â€“ \\\\ Conditional DETER V2 (arXiv2022)[66] & Hourglass48 & 521/â€“ & 90M & 32.1 & 50 & â€“ \\\\ FP-DETER-IN (ICLR2022) [80] & â€“ & â€“/â€“ & **36M** & 26.5 & 50 & Link \\\\ DAB-DETER-DCS (arXiv2022)[67] & ResNet101 & 296/â€“ & 63M & 28.1 & 50 & Link \\\\ Ghostformer-MS (Sensors2022)[93] & GhostNet & â€“/â€“ & â€“ & 29.2 & 100 & â€“ \\\\ CF-DETER-DCN-TTA (AAAI2022)[47] & ResNeXt101 & â€“/â€“ & â€“ & 35.1* & â€“ & â€“ \\\\ CBNet V2-TTA (CVPR2022)[79] & Swin Transformer-base & â€“/â€“ & â€“ & 41.7 & â€“ & Link \\\\ CBNet V2-TTA-BD (CVPR2022)[79] & Swin Transformer-base & â€“/â€“ & â€“ & 42.2 & â€“ & Link \\\\ DETA (arXiv2022)[74] & ResNet50 & â€“/13 & 48M & 34.3 & 24 & Link \\\\ DINO (arXiv2022)[88] & ResNet50 & 860/10 & 47M & 32.3 & **12** & Link \\\\ CO-DIM Defending DETER-MS-IN (arXiv2022)[87] & Swin Transformer-large & â€“/â€“ & â€“ & 43.7 & 36 & Link \\\\ HYNETER (ICASSP2023)[62] & Hyneter-Max & â€“/â€“ & 247M & 29.8* & â€“ & â€“ \\\\ Doeft (TIP2017)[283] & ResNet101 & 217/14 & 58M & 31.4 & 34 & â€“ \\\\ ConformerDet-MS (TPAMI2023) [58] & Conformer-B & â€“/â€“ & 147M & 35.3 & 36 & Link \\\\ \\hline YOLOS (NeurIPS2021)[49] & DeiT-base & â€“/3.9 & 100M & 19.5 & 150 & Link \\\\ DETER(VIT) (arXiv2022)[42] & Swin Transformer-base & â€“/â€“9.7 & 100M & 18.3 & 50 & Link \\\\ Deformable DETER(VIT) (arXiv2021)[42] & Swin Transformer-base & â€“/4.8 & 100M & 34.5 & 50 & Link \\\\ ViDT (arXiv2022)[42] & Swin Transformer-base & â€“/â€“9 & 100M & 30.6 & 50 & Link \\\\ DPFT (ECCV2022)[53] & DOT-medium & 67/â€“ & â€“ & 25.5 & 36 & Link \\\\ CenterNet++MS (arXiv2022)[40] & Swin Transformer-large & â€“/â€“ & â€“ & 38.7* & â€“ & Link \\\\ DETA-OB (arXiv2022)[74] & Swin Transformer-large & â€“/â€“4.2 & â€“ & 46.1* & 24 & Link \\\\ Group DETER v2-MS-IN-OB (arXiv2022) [81] & ViT-Huge & â€“/â€“ & 629M & **48.4*** & â€“ & â€“ \\\\ \\hline Best Results & NA & DETR & FP-DETR & Group DETR v2 & DINO & NA \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE 3: Detection performance (%) for small-scale objects on MS COCO image dataset [2]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures, and the bottom section presents from transformer-only networks. DCS: Dilated C5 stage, MS: Multi-scale network, IBR: Iterative bounding box refinement, TS: Two-stage detection, DCN: Deformable convnets, TTA: Test time augmentation, BD: Pre-trained on BigDetection dataset, IN: Pre-trained on ImageNet, OB: Pre-trained on Object-365 [118]. While * shows the results for COCO test-dev, the other values are reported for COCO val set.\n' +
      '\n' +
      '### _Benchmarks in Vision Applications_\n' +
      '\n' +
      'In this subsection, we introduce various vision-based applications where the detection performance of small objects is vital. For each application, we select one of the most popular datasets and report its performance metrics, along with details of the experimental setup.\n' +
      '\n' +
      '#### 4.2.1 Generic Applications\n' +
      '\n' +
      'For generic applications, we evaluate the performance of all small object detectors on the challenging MS COCO benchmark [2]. The choice of this dataset is based on its wide acceptance in the object detection field and the accessibility of performance results. The MS COCO dataset consists of approximately 160K images across 80 categories. While the authors are advised to train their algorithms using the COCO 2017 training and validation sets, they are not restricted to these subsets.\n' +
      '\n' +
      'In Table III, we examine and evaluate the performance of all the techniques under review that have reported their results on MS COCO (compiled from their papers). The table provides information on the backbone architecture, GFLOPS/FPS (indicating the computational overhead and execution speed), number of parameters (indicating the scale of the model), mAP (mean average precision: a measure of object detection performance), and epochs (indicating the inference time and convergence properties). Additionally, a link to each method\'s webpage is provided for further information. The methods are categorized into three groups: CNN-based, mixed, and transformer only methods. The top-performing methods for each metric are shown in the table\'s last row. It should be noted that this comparison was only feasible\n' +
      '\n' +
      'Fig. 11: Examples of detection results on COCO dataset [2] for transformer-based SOTA small object detectors compared with Convolutional networks.\n' +
      '\n' +
      'for methods that have reported values for each specific metric. In instances where there is a tie, the method with the highest mean average precision was deemed the best. The default mAP values are for the "COCO 2017 val" set, while those for the "COCO test-dev" set are marked with an asterisk. Please be aware that the reported mAP is only for objects with area\\(<32^{2}\\).\n' +
      '\n' +
      'Upon examining Table III, it is obvious that most techniques benefit from using a mix of CNN and transformer architectures, essentially adopting hybrid strategies. Notably, Group DETR v2 which relies solely on a transformer-based architecture, attains a mAP of 48.4\\(\\%\\). However, achieving such a performance requires the adoption of additional techniques such as pre-training on two large-scale datasets and multi-scale learning. In terms of convergence, DINO outperforms by reaching stable results after just 12 epochs, while also securing a commendable mAP of 32.3\\(\\%\\). Conversely, the original DETR model has the fastest inference time and the lowest GFLOPS. FP-DETR stands out for having the lightest network with only 36M parameters.\n' +
      '\n' +
      'Drawing from these findings, we conclude that pre-training and multi-scale learning emerge as the most effective strategies for excelling in small object detection. This may be attributed to the imbalance in downstream tasks and the lack of informative features in small objects.\n' +
      '\n' +
      'Figure 11 which spans two pages, along with its more detailed counterpart in Figure 12, illustrates the detection results of various transformers and CNN-based methods. These are compared to each other using selected images from the COCO dataset and implemented by us using their public models available on their GitHub pages. The analysis reveals that Faster RCNN and SSD fall short in accurately detecting small objects. Specifically, SSD either misses most objects or generates numerous bounding boxes with false labels and poorly located bounding boxes. While Faster RCNN performs better, it still produces low-confidence bounding boxes and occasionally assigns incorrect labels.\n' +
      '\n' +
      'In contrast, DETR has the tendency to over-estimate the number of objects, leading to multiple bounding boxes for individual objects. It is commonly noted t that DETR is prone to generating false positives. Finally, among the methods evaluated, CBNet V2 stands out for its superior performance. As observed, it produces high confidence scores for the objects it detects, even though it may occasionally misidentify some objects.\n' +
      '\n' +
      '#### 4.2.2 Small Object Detection in Aerial Images\n' +
      '\n' +
      'Another interesting use of detecting small objects is in the area of remote sensing. This field is particularly appealing because many organizations and research bodies aim to routinely monitor the Earth\'s surface through aerial images to collect both national and international data for statistics. While these images can be acquired using various modalities, this survey focuses only on non-SAR images. This is because SAR images have extensively been researched and deserve their own separate study. Nonetheless, the learning techniques discussed in this survey could also be applicable to SAR images.\n' +
      '\n' +
      'In aerial images, objects often appear small due to their significant distance from the camera. The bird\'s-eye view also adds complexity to the task of object detection, as objects can be situated anywhere within the image. To assess the performance of transformer-based detectors designed for such applications, we selected the DOTA image dataset [122], which has become a widely used benchmark in the field of object detection. Figure 13 displays some sample images from the DOTA dataset featuring small objects. The dataset includes a predefined Training set, Validation set, and Testing set. In comparison to generic applications, this particular application has received relatively less attention from\n' +
      '\n' +
      'Fig. 11: Examples of detection results on COCO dataset [2] for transformer-based SOTA small object detectors compared with Convolutional networks.\n' +
      '\n' +
      'transformer experts. However, as indicated in Table IV (results are compiled from papers), ReDet distinguishes itself through its multi-scale learning strategy and pre-training on the ImageNet dataset, achieving the highest precision value (80.89\\(\\%\\)) and requiring only 12 training epochs. This mirrors the insights gained from the COCO dataset analysis, suggesting that optimal performance can be attained by addressing imbalances in downstream tasks and including informative features from small objects.\n' +
      '\n' +
      '#### 4.2.3 Small Object Detection in Medical Images\n' +
      '\n' +
      'In the field of medical imaging, specialists are often tasked with early detection and identification of anomalies. Missing even barely visible or small abnormal cells can lead to serious repercussions for patients, including cancer and life-threatening conditions. These small-sized objects can be found as abnormalities in the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline Model & Backbone & FPS \\(\\uparrow\\) & \\#params\\(\\downarrow\\) & mAP \\(\\uparrow\\) & Epochs\\(\\downarrow\\) & URL \\\\ \\hline Rotated Faster RCNN-MS (NeurIPS2015)[24] & ResNet101 & â€“ & 64M & 67.71 & 50 & Link \\\\ SSD (ECCV2016) [20] & â€“ & â€“ & â€“ & 56.1 & â€“ & Link \\\\ RetinaNet-MS (ICCV2017)[21] & ResNet101 & â€“ & **59M** & 66.53 & 50 & Link \\\\ ROI-Transformer-MS-IN (CVPR2019) [123, 124] & ResNet50 & â€“ & â€“ & 80.06 & **12** & Link \\\\ Yolov5 (2020)[17] & â€“ & **95** & â€“ & 64.5 & â€“ & Link \\\\ ReDet-MS-FPN (CVPR2021)[125] & ResNet50 & â€“ & â€“ & 80.1 & â€“ & Link \\\\ \\hline Q2DETR-MS (arXiv2012)[45] & ResNet101 & â€“ & 63M & 70.02 & 50 & â€“ \\\\ Q2DETR-MS-FT (arXiv2021)[45] & ResNet101 & â€“ & â€“ & 76.23 & 62 & â€“ \\\\ Q2DETR-MS-FPN-FT (arXiv2021)[45] & ResNet50 & â€“ & â€“ & 79.66 & â€“ & â€“ \\\\ SPH-Yolov5 (RS2022)[73] & Swin Transformer-base & 51 & â€“ & 71.6 & 150 & â€“ \\\\ A02-DETR-MS (TCSV2022)[90] & ResNet50 & â€“ & â€“ & 79.22 & â€“ & Link \\\\ MDCT (RS2023)[96] & â€“ & â€“ & â€“ & 75.7 & â€“ & â€“ \\\\ ReDet-MS-IN (arXiv2023)[124] & ViTDet, ViT-B & â€“ & â€“ & **80.89** & **12** & Link \\\\ \\hline Best Results & NA & Yolov5 & RetinaNet & ReDet-MS-IN & ReDet-MS-IN & NA \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: Detection performance (%) for small-scale objects on DOTA image dataset [122]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. MS: Multi-scale network, FT: Fine-tuned, FPN: Feature pyramid network, IN: Pre-trained on ImageNet.\n' +
      '\n' +
      'Fig. 12: Detection results on a sample image when zoomed in. First row from the left: Input image, SSD, Faster RCNN, DETR. Second row from the left: ViDT, DETA-OB, DINO, CBNet v2.\n' +
      '\n' +
      'Fig. 13: Example of small objects in DOTA image dataset.\n' +
      '\n' +
      'retain of diabetic patients, early tumors, vascular plaques, etc. Despite the critical nature and potential life-threatening impact of this research area, only a handful of studies have tackled the challenges associated with detecting small objects in this crucial application. For those interested in this topic, the DeepLesion CT image dataset [111] has been selected as the benchmark due to the availability of the results for this particular dataset [126]. Sample images from this dataset are shown in Figure 14. This dataset is divided into three sets: training (\\(70\\%\\)), validation (\\(15\\%\\)), and test (\\(15\\%\\)) sets [94]. Table V compares the accuracy and mAP of three transformer-based studies against both two-stage and one-stage detectors (results are compiled from their papers). The MS Transformer emerges as the best technique with this dataset, albeit with limited competition. Its primary innovation lies in self-supervised learning and the incorporation of a masking mechanism within a hierarchical transformer model. Overall, with an accuracy of 90.3\\(\\%\\) and an mAP of 89.6\\(\\%\\), this dataset appears to be less challenging compared to other medical imaging tasks, especially considering that some tumor detection tasks are virtually invisible to the human eyes.\n' +
      '\n' +
      '#### 4.2.4 Small Object Detection in Underwater Images\n' +
      '\n' +
      'With the growth of underwater activities, the demand to monitor hazy and low-light environments has increased for purposes like ecological surveillance, equipment maintenance, and monitoring of wreck fishing. Factors like scattering and light absorption of the water, make the SOD task even more challenging. Example images of such challenging environments are displayed in Figure 15. Transformer-based detection methods should not only be adept at identifying small objects but also need to be robust against the poor image quality found in deep waters, as well as variations in color channels due to differing rates of light attenuation for each channel.\n' +
      '\n' +
      'Table VI shows the performance metrics reported in existing studies for this dataset (results are compiled from their papers). HTDet is the sole transformer-based technique identified for this specific application. It significantly outperforms the SOTA CNN-based method by a huge margin (3.4\\(\\%\\) in mAP). However, the relatively low mAP scores confirm that object detection in underwater images remains a difficult task. It is worth noting that the training set of the URPC 2018 contains 2901 labeled images, and the testing set contains 800 unlabeled images [86].\n' +
      '\n' +
      '#### 4.2.5 Small Object Detection in Active Milli-Meter Wave Images\n' +
      '\n' +
      'Small objects can easily be concealed or hidden from normal RGB cameras, for example, within a person\'s clothing at an airport. Therefore, active imaging techniques are essential for security\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & \\(\\#\\)Param\\(\\downarrow\\) & mAP\\({}^{0.5,0.95}\\uparrow\\) & mAP\\({}^{0.5}\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24] & 33.6M & 16.4 & â€“ \\\\ Cascade RCNN (CVPR2018)[28] & 68.9M & 16 & â€“ \\\\ Dynamic RCNN (ECCV2020)[127] & 41.5M & 13.3 & â€“ \\\\ Voloxity (17) & 61.5M & 19.4 & â€“ \\\\ RollMis (ICASSP2020)[121] & â€“ & â€“ & **74.92** \\\\ \\hline HTDet (RS2023)[86] & **7.7M** & **22.8** & â€“ \\\\ \\hline Best Results & HTDet & HTDet & RoMis \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Detection performance (%) for URPC2018 dataset [121]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      'Fig. 14: Example of small abnormalities in DeepLesion image dataset [111].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Model & Accuracy \\(\\uparrow\\) & mAP\\({}^{0.5}\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24] & 83.3 & 83.3 \\\\ Voloxy [17] & 85.2 & 88.2 \\\\ \\hline DETR (ECCV2020)[31] & 86.7 & 87.8 \\\\ Swin Transformer & 82.9 & 81.2 \\\\ MS Transformer (CIN2022)[94] & **90.3** & **89.6** \\\\ \\hline Best Results & MS Transformer & MS Transformer \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Detection performance (%) for DeepLesion CT image dataset [111]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      'Fig. 15: Examples of low quality images in URPC2018 image dataset.\n' +
      '\n' +
      'purposes. In these scenarios, multiple images are often captured from different angles to enhance the likelihood of detecting even minuscule objects. Interestingly, much like in the field of medical imaging, transformers are rarely used for this particular application.\n' +
      '\n' +
      'In our study, we focused on the detection performance of existing techniques using the AMMW Dataset [113] as shown in Table VII (results are compiled from their papers). We have identified that MATR emerged as the sole technique that combines transformer and CNNs for this dataset. Despite being the only transformer-based technique, it could significantly improve the SOD performance (5.49\\(\\%\\)\\(\\uparrow\\) in mAP\\({}^{0.5}\\) with respect to Yolov5 and 4.22 \\(\\%\\)\\(\\uparrow\\) in mAP\\({}^{\\text{th}[0.5,0.95]}\\) with respect to TridentNet) with the same backbone (ResNet50). Figure 16 visually compares MATR with other SOTA CNN-based techniques. Combining images from different angles largely helps to identify even small objects within this imaging approach. For training and testing, 35426 and 4019 images were used, respectively [71].\n' +
      '\n' +
      '#### 4.2.6 Small Object Detection in Videos\n' +
      '\n' +
      'The field of object detection in videos gained considerable attention recently, as the temporal information in videos can improve the detection performance. To benchmark the SOTA techniques, the ImageNet VID dataset has been used with results specifically focused on the dataset\'s small objects. This dataset includes 3862 training videos and 555 validation videos with 30 classes of objects. Table VIII reports the mAP of several recently developed transformer-based techniques (results are compiled from their\n' +
      '\n' +
      'Fig. 16: Examples of detection results on AMMW image dataset [113] for SOTA small object detectors (figure from [71]).\n' +
      '\n' +
      'papers). While transformers are increasingly being used in video object detection, their performance in SOD remains less explored. Among the methods that have reported SOD performance on the ImageNet VID dataset, Deformable DETR with FAQ stands out for achieving the highest performance- although it is notably low at 13.2 \\(\\%\\) for mAP\\({}^{\\text{g}[0.5,0.95]}\\)). This highlights a significant research gap in the area of video-based SOD.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      'In this survey article, we explored how transformer-based approaches can address the challenges of SOD. Our taxonomy divides transformer-based small object detectors into seven main categories: object representation, fast attention (useful for high-resolution and multi-scale feature maps), architecture and block modification, spatio-temporal information, improved feature representation, auxiliary techniques, and fully transformer-based detectors.\n' +
      '\n' +
      'When juxtaposing this taxonomy with the one for CNN-based techniques [11], we observe that some of these categories overlap, while others are unique to transformer-based techniques. Certain strategies are implicitly embedded into transformers, such as attention and context learning, which are performed via the self and cross-attention modules in the encoder and decoder. On the other hand, multi-scale learning, auxiliary tasks, architecture modification, and data augmentation are commonly used in both paradigms. However, it is important to note that while CNNs handle spatio-temporal analysis through 3D-CNN, RNN, or feature aggregation over time, transformers achieve this by using successive spatial and temporal transformers or by updating object queries for successive frames in the decoder.\n' +
      '\n' +
      'We have observed that pre-training and multi-scale learning stand out as the most commonly adopted strategies, contributing to state-of-the-art performance across various datasets performance on different datasets. Data fusion is another approach widely used for SOD. In the context of video-based detection systems, the focus is on effective methods for collecting temporal data and integrating it into the frame-specific detection module.\n' +
      '\n' +
      'While transformers have brought about substantial advancements in the localization and classification of small objects, it is important to acknowledge the trade-offs involved. These include a large number of parameters (in the order of billions), several days of training (a few hundred epochs), and pretraining on extremely large datasets (which is not feasible without powerful computational resources). All of these aspects pose limitations on the pool of users who can train and test these techniques for their downstream tasks. It is now more important than ever to recognize the need for lightweight networks with efficient learning paradigms and architectures. Despite the number of parameters is now on par with the human brain, the performance in small object detection still lags considerably behind human capabilities, underscoring a significant gap in current research.\n' +
      '\n' +
      'Furthermore, based on the findings presented in Figures 11 and 12, we have identified two primary challenges in small object detection: missing objects or false negatives, and redundant detected boxes. The issue of missing objects is likely attributable to the limited information embedded in the tokens. This can be resolved by using high-resolution images or by enhancing feature pyramids although this comes with the drawback of increased latency--which could potentially be offset by using more efficient, lightweight networks. The problem of repeated detections has traditionally been managed through post-processing techniques such as Non-Maximum Suppression (NMS). However, in the context of transformers, this issue should be approached by minimizing object query similarity in the decoder, possibly through the use of auxiliary loss functions.\n' +
      '\n' +
      'We also examined studies that employ transformer-based methods specifically dedicated to Small Object Detection (SOD) across a range of vision-based tasks. These include generic detection, detection in aerial images, abnormality detection in medical images, small hidden object detection in active millimeter-wave images for security purposes, underwater object detection, and small object detection in videos. Apart from generic and aerial image applications, transformers are underdeveloped in other applications, echoing observations made in Rekavandi _et al._[11] regarding maritime detection. This is particularly surprising given the potentially significant impact transformers could have in life-critical fields like medical imaging.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'This survey paper reviewed over 60 research papers that focus on the development of transformers for the task of small object detection, including both purely transformer-based and hybrid techniques that integrate CNNs. These techniques have been examined from seven different perspectives: object representation, fast attention mechanisms for high-resolution or multi-scale feature maps, architecture and block modifications, spatio-temporal information, improved feature representation, auxiliary techniques, and fully transformer-based detection. Each of these categories includes several state-of-the-art (SOTA) techniques, each with its own set of advantages. We also compared these transformer-based approaches to CNN-based frameworks, discussing the similarities and differences between the two. Furthermore, for a range of vision applications, we introduced well-established datasets that serve as benchmarks for future research. Additionally, 12 datasets that have been used in SOD applications are discussed in detail, providing convenience for future research efforts. In future research, the unique challenges associated with the detection of small objects in each application could be explored and addressed. Fields like medical imaging and underwater image analysis stand\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & Backbone & mAP\\({}^{\\text{g}[0.5,0.95]}\\) \\(\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24] & ResNet50 & 70.7 & 26.83 \\\\ Cascade RCNN (CVPR2018)[28] & ResNet50 & 74.7 & 27.8 \\\\ TridentNet (ICCV2019)[128] & ResNet50 & 77.3 & 29.2 \\\\ Dynamic RCNN (ECCV2020)[127] & ResNet50 & 76.3 & 27.6 \\\\ Yolov [17] & ResNet50 & 66.7 & 28.48 \\\\ \\hline MATR (TCSV2022)[71] & ResNet50 & **82.16** & **33.42** \\\\ \\hline Best Results & NA & MATR & MATR \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VII: Detection performance (%) for AMWW image dataset [113]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Backbone & mAP\\({}^{\\text{g}[0.5,0.95]}\\) \\(\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24][+SIEA[12] & ResNet50 & 8.5 \\\\ \\hline Deformable-DETR-PT [4] & ResNet50 & 10.5 \\\\ Deformable-DETR[4]+TransV-PDT[103] & ResNet50 & 11 \\\\ DAB-DETR[6]+FAQPT[108] & ResNet50 & 12 \\\\ Deformable-DETR[4]+FAQPT[108] & ResNet50 & **13.2** \\\\ \\hline Best Results & NA & Deformable-DETR+FAQ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VIII: Detection performance (%) for ImageNet VID dataset [98] for small objects. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '* [49] Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu, "You only look at one sequence: Rethinking transformer in vision through object detection," _Advances in Neural Information Processing Systems_, vol. 34, pp. 26 183-26 197, 2021.\n' +
      '* [50] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 10 012-10 022.\n' +
      '* [51] Y. Li, H. Mao, R. Girshick, and K. He, "Exploring plain vision transformer backbones for object detection," in _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IX_. Springer, 2022, pp. 280-296.\n' +
      '* [52] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, "Training data-efficient image transformers & distillation through attention," in _International conference on machine learning_. PMLR, 2021, pp. 10 347-10 357.\n' +
      '* [53] P. Chen, M. Zhang, Y. Shen, K. Sheng, Y. Gao, X. Sun, K. Li, and C. Shen, "Efficient decoder-free object detection with transformers," in _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part X_. Springer, 2022, pp. 70-86.\n' +
      '* [54] Y. Zhu, Q. Xia, and W. Jin, "SrdR: a lightweight end-to-end object detection with transformer," _Connection Science_, vol. 34, no. 1, pp. 2448-2465, 2022.\n' +
      '* [55] Z. Sun, S. Cao, Y. Yang, and K. M. Kitani, "Rethinking transformer-based set prediction for object detection," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 3611-3620.\n' +
      '* [56] Z. Tian, C. Shen, H. Chen, and T. He, "Fcos: Fully convolutional one-stage object detection," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2019, pp. 9627-9636.\n' +
      '* [57] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye, "Conformer: Local features coupling global representations for visual recognition," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 367-376.\n' +
      '* [58] Z. Peng, Z. Guo, W. Huang, Y. Wang, L. Xie, J. Jiao, Q. Tian, and Q. Ye, "Conformer: Local features coupling global representations for recognition and detection," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* [59] W. Lu, C. Lan, C. Niu, W. Liu, L. Lyu, Q. Shi, and S. Wang, "A cnnn-transformer hybrid model based on cswin transformer for uav image object detection," _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 2023.\n' +
      '* [60] X. Xu, Z. Feng, C. Cao, M. Li, J. Wu, Z. Wu, Y. Shang, and S. Ye, "An improved swin transformer-based model for remote sensing object detection and instance segmentation," _Remote Sensing_, vol. 13, no. 23, p. 4779, 2021.\n' +
      '* [61] J. Xue, D. He, M. Liu, and Q. Shi, "Dual network structure with interweaved global-local feature hierarchy for transformer-based object detection in remote sensing image," _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, vol. 15, pp. 6856-6866, 2022.\n' +
      '* [62] D. Chen, D. Miao, and X. Zhao, "Hyperter: Hybrid network transformer for object detection," in _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2023, pp. 1-5.\n' +
      '* [63] J. Ding, W. Li, L. Pei, M. Yang, C. Ye, and B. Yuan, "Sw-yolov: An anchor-free detector based transformer for sea surface object detection," _Expert Systems with Applications_, p. 119560, 2023.\n' +
      '* [64] H. Yang, Z. Yang, A. Hu, C. Liu, T. J. Cui, and J. Miao, "Unifying convolution and transformer for efficient concealed object detection in passive millimeter-wave images," _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.\n' +
      '* [65] D. Meng, X. Chen, Z. Fan, G. Zeng, H. Li, Y. Yuan, L. Sun, and J. Wang, "Conditional detr for fast training convergence," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021, pp. 3651-3660.\n' +
      '* [66] X. Chen, F. Wei, G. Zeng, and J. Wang, "Conditional detr v2: Efficient detection transformer with box queries," _arXiv preprint arXiv:2207.08914_, 2022.\n' +
      '* [67] S. Liu, F. Li, H. Zhang, X. Yang, X. Qi, H. Su, J. Zhu, and L. Zhang, "Db-detr: Dynamic anchor boxes are better queries for detr," _arXiv preprint arXiv:2201.12329_, 2022.\n' +
      '* [68] L. He and S. Todorovic, "Destr: Object detection with split transformer," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022, pp. 9377-9386.\n' +
      '* [69] R. Xia, G. Li, Z. Huang, Y. Pang, and M. Qi, "Transformers only look once with nonlinear combination for real-time object detection," _Neural Computing and Applications_, vol. 34, no. 15, pp. 12 571-12 585, 2022.\n' +
      '* [70] T. Liang, X. Chu, Y. Liu, Y. Wang, Z. Tang, W. Chu, J. Chen, and H. Ling, "Cbnet: A composite backbone network architecture for object detection," _IEEE Transactions on Image Processing_, vol. 31, pp. 6893-6906, 2022.\n' +
      '* [71] P. Sun, T. Liu, X. Chen, S. Zhang, Y. Zhao, and S. Wei, "Multi-source aggregation transformer for concealed object detection in millimeter-wave images," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 32, no. 9, pp. 6148-6159, 2022.\n' +
      '* [72] B. K. Isaac-Medina, C. G. Willococks, and T. P. Breckon, "Multi-view vision transformers for object detection," in _2022 26th International Conference on Pattern Recognition (ICPR)_. IEEE, 2022, pp. 4678-4684.\n' +
      '* [73] H. Gong, T. Mu, Q. Li, H. Dai, C. Li, Z. He, W. Wang, F. Han, A. Tuniyazi, H. Li _et al._, "Swin-transformer-enabled yolov5 with attention mechanism for small object detection on satellite images," _Remote Sensing_, vol. 14, no. 12, p. 2861, 2022.\n' +
      '* [74] J. Ouyang-Zhang, J. H. Cho, X. Zhou, and P. Krahenbuhl, "Nms strikes back," _arXiv preprint arXiv:2212.06137_, 2022.\n' +
      '* [75] Q. Chen, X. Chen, J. Wang, H. Feng, J. Han, E. Ding, G. Zeng, and J. Wang, "Group detr: Fast detr training with group-wise one-to-many assignment," _arXiv preprint arXiv:2207.13085_, vol. 1, no. 2, 2022.\n' +
      '* [76] S. Xu, J. Gu, Y. Hua, and Y. Liu, "Dktrnet: Dual-key transformer network for small object detection," _Computing_, 2023.\n' +
      '* [77] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, R. M. Anwer, and M.-H. Yang, "Class-agnostic object detection with multi-modal transformer," in _17th European Conference on Computer Vision (ECCV)_. Springer, 2022.\n' +
      '* [78] R. Shen, N. Inoue, and K. Shinoda, "Text-guided object detector for multi-modal video question answering," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, 2023, pp. 1032-1042.\n' +
      '* [79] L. Cai, Z. Zhang, Y. Zhu, L. Zhang, M. Li, and X. Xue, "Bigdetection: A large-scale benchmark for improved object detector pre-training," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 4777-4787.\n' +
      '* [80] W. Wang, Y. Cao, J. Zhang, and D. Tao, "Fp-detr: Detection transformer advanced by fully pre-training," in _International Conference on Learning Representations_, 2022.\n' +
      '* [81] Q. Chen, J. Wang, C. Han, S. Zhang, Z. Li, X. Chen, J. Chen, X. Wang, S. Han, G. Zhang _et al._, "Group detr v2: Strong object detector with encoder-decoder pretraining," _arXiv preprint arXiv:2211.03594_, 2022.\n' +
      '* [82] K. Oksuz, B. C. Cam, S. Kalkan, and E. Akbas, "Thulance problems in object detection: A review," _IEEE transactions on pattern analysis and machine intelligence_, vol. 43, no. 10, pp. 3388-3415, 2020.\n' +
      '* [83] S. Rashidi, R. Temankoon, A. M. Rekavandi, P. Jessadatavornwong, A. Freis, G. Huff, M. Easton, A. Mouritz, R. Hosemzadeh, and A. Babadiashar, "It-tda: Information theory assisted robust unsupervised domain adaptation," _arXiv preprint arXiv:2210.12947_, 2022.\n' +
      '* [84] K. Zeng, Q. Ma, J. Wu, S. Xiang, T. Shen, and L. Zhang, "Niffnet: A non-local feature fusion transformer network for multi-scale object detection," _Neurocomputing_, vol. 493, pp. 15-27, 2022.\n' +
      '* [85] T. Ding, K. Feng, Y. Wei, Y. Han, and T. Li, "Deot: an end-to-end encoder-only transformer object detector," _Journal of Real-Time Image Processing_, vol. 20, no. 1, p. 1, 2023.\n' +
      '* [86] G. Chen, Z. Mao, K. Wang, and J. Shen, "Hddet: A hybrid transformer-based approach for underwater small object detection," _Remote Sensing_, vol. 15, no. 4, p. 1076, 2023.\n' +
      '* [87] Z. Gong, G. Song, and Y. Liu, "Detrs with collaborative hybrid assignments training," _arXiv preprint arXiv:2211.12860_, 2022.\n' +
      '* [88] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, "Dino: Detr with improved denoising anchor boxes for end-to-end object detection," _arXiv preprint arXiv:2203.03605_, 2022.\n' +
      '* [89] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang, "Dn-detr: Accelerate der training by introducing query denoising," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 13 619-13 627.\n' +
      '* [90] L. Dai, H. Liu, H. Tang, Z. Wu, and P. Song, "Ao2-detr: Arbitrary-oriented object detection transformer," _IEEE Transactions on Circuits and Systems for Video Technology_, 2022.\n' +
      '* [91] C. Zhang, L. Liu, X.\n' +
      '\n' +
      '* [92] Y. Tang, B. Wang, W. He, and F. Qian, "Pointdet++: an object detection framework based on human local features with transformer encoder," _Neural Computing and Applications_, pp. 1-12, 2022.\n' +
      '* [93] S. Li, F. Sultonov, J. Tursunboev, J.-H. Park, S. Yun, and J.-M. Kang, "Ghostformer: A ghostnet-based two-stage transformer for small object detection," _Sensors_, vol. 22, no. 18, p. 6939, 2022.\n' +
      '* [94] Y. Shou, T. Meng, W. Ai, C. Xie, H. Liu, and Y. Wang, "Object detection in medical images based on hierarchical transformer and mask mechanism," _Computational Intelligence and Neuroscience_, vol. 2022, 2022.\n' +
      '* [95] S. Dubey, F. Olimov, M. A. Rafique, and M. Jeon, "Improving small objects detection using transformer," _Journal of Visual Communication and Image Representation_, vol. 89, p. 103620, 2022.\n' +
      '* [96] J. Chen, H. Hong, B. Song, J. Guo, C. Chen, and J. Xu, "Mdct: Multi-kernel dilated convolution and transformer for one-stage object detection of remote sensing images," _Remote Sensing_, vol. 15, no. 2, p. 371, 2023.\n' +
      '* [97] T. Ye, W. Qin, Z. Zhao, X. Gao, X. Deng, and Y. Ouyang, "Real-time object detection network in uav-vision based on cnn and transformer," _IEEE Transactions on Instrumentation and Measurement_, vol. 72, pp. 1-13, 2023.\n' +
      '* [98] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein _et al._, "Imagenet large scale visual recognition challenge," _International journal of computer vision_, vol. 115, pp. 211-225, 2015.\n' +
      '* [99] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, "Trackformer: Multi-object tracking with transformers," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022, pp. 8844-8854.\n' +
      '* [100] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, "Transformer tracking," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2021, pp. 816-8135.\n' +
      '* [101] C. Liu, S. Xu, and B. Zhang, "Aerial small object tracking with transformers," in _2021 IEEE International Conference on Unmanned Systems (ICUSS)_. IEEE, 2021, pp. 954-959.\n' +
      '* [102] L. He, Q. Zhou, X. Li, L. Niu, G. Cheng, X. Li, W. Liu, Y. Tong, L. Ma, and L. Zhang, "End-to-end video object detection with spatial-temporal transformers," in _Proceedings of the 29th ACM International Conference on Multimedia_, 2021, pp. 1507-1516.\n' +
      '* [103] Q. Zhou, X. Li, L. He, Y. Yang, G. Cheng, Y. Tong, L. Ma, and D. Tao, "Transvod: end-to-end video object detection with spatial-temporal transformers," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.\n' +
      '* [104] H. Wang, J. Tang, X. Liu, S. Guan, R. Xie, and L. Song, "Piseformer: Progressive temporal-spatial enhanced transformer towards video object detection," in _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII_. Springer, 2022, pp. 732-747.\n' +
      '* [105] K. A. Hashmi, D. Stricker, and M. Z. Afral, "Spatio-temporal learnable proposals for end-to-end video object detection," _arXiv preprint arXiv:2210.02368_, 2022.\n' +
      '* [106] S.-D. Roh and K.-S. Chung, "Dafa: Diversity-aware feature aggregation for attention-based video object detection," _IEEE Access_, vol. 10, pp. 93 453-93 463, 2022.\n' +
      '* [107] M. Fujitake and A. Sugimoto, "Video sparse transformer with attention-guided memory for video object detection," _IEEE Access_, vol. 10, pp. 65 886-65 900, 2022.\n' +
      '* [108] Y. Cui, "Faq: Feature aggregated queries for transformer-based video object detectors," _arXiv preprint arXiv:2303.08319_, 2023.\n' +
      '* [109] M. Mueller, N. Smith, and B. Ghanem, "A benchmark and simulator for uav tracking," in _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14_. Springer, 2016, pp. 445-461.\n' +
      '* [110] E. Goldman, R. Iferzig, A. Eisenstchatt, J. Goldberger, and T. Hassner, "Precise detection in densely packed scenes," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 5227-5236.\n' +
      '* [111] K. Yan, X. Wang, L. Lu, and R. M. Summers, "Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning," _Journal of medical imaging_, vol. 5, no. 3, pp. 036 501-036 561, 2018.\n' +
      '* [112] "Udacity self-driving car driving data, 2017 transformer," _[https://github.com/udacity/selfdriving-car/tree/master/annotations_](https://github.com/udacity/selfdriving-car/tree/master/annotations_).\n' +
      '* [113] T. Liu, Y. Zhao, Y. Wei, Y. Zhao, and S. Wei, "Concealed object detection for activate millimeter wave image," _IEEE Transactions on Industrial Electronics_, vol. 66, no. 12, pp. 9909-9917, 2019.\n' +
      '* [114] A. Coluccia, A. Fascista, A. Schumann, L. Sommer, A. Dimou, D. Zarpalas, F. C. Akyon, O. Eryuksel, K. A. Ozfuttu, S. O. Altinuc _et al._, "Drone vs-bird detection challenge at ieee avsx2021," in _2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)_. IEEE, 2021, pp. 1-8.\n' +
      '* [115] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, "Object detection in optical remote sensing images: A survey and a new benchmark," _ISPRS journal of photogrammetry and remote sensing_, vol. 159, pp. 296-307, 2020.\n' +
      '* [116] G. Cheng, P. Zhou, and J. Han, "Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images," _IEEE Transactions on Geoscience and Remote Sensing_, vol. 54, no. 12, pp. 7405-7415, 2016.\n' +
      '* [117] Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, "Hierarchical and robust convolutional neural network for very high-resolution remote sensing object detection," _IEEE Transactions on Geoscience and Remote Sensing_, vol. 57, no. 8, pp. 5535-5548, 2019.\n' +
      '* [118] Y. Gao, H. Shen, D. Zhong, J. Wang, Z. Liu, T. Bai, X. Long, and S. Wen, "A solution for densely annotated large scale object detection task," 2019.\n' +
      '* [119] Y. Chen, Z. Zhang, Y. Cao, L. Wang, S. Lin, and H. Hu, "Reppoints v2: Verification meets regression for object detection," _Advances in Neural Information Processing Systems_, vol. 33, pp. 5621-5631, 2020.\n' +
      '* [120] S. Zhang, C. Chi, Y. Yao, Z. Lei, and S. Z. Li, "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2020, pp. 9759-9768.\n' +
      '* [121] W.-H. Lin, J.-X. Zhong, S. Liu, T. Li, and G. Li, "Roimix: proposal-fusion among multiple images for underwater object detection," in _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2020, pp. 2588-2592.\n' +
      '* [122] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and L. Zhang, "Dota: A large-scale dataset for object detection in aerial images," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018, pp. 3974-3983.\n' +
      '* [123] J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu, "Learning roi transformer for oriented object detection in aerial images," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 2849-2858.\n' +
      '* [124] L. Wang and A. Tien, "Aerial image object detection with vision transformer detector (videt)," _arXiv preprint arXiv:2301.12058_, 2023.\n' +
      '* [125] J. Han, J. Ding, N. Xue, and G.-S. Xia, "Redet: A rotation-equivariant detector for aerial object detection," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 2786-2795.\n' +
      '* [126] J. Li, G. Zhu, C. Hua, M. Feng, B. Bennamoun, P. Li, X. Lu, J. Song, P. Shen, X. Xu _et al._, "A systematic collection of medical image datasets for deep learning," _ACM Computing Surveys_, 2021.\n' +
      '* [127] H. Zhang, H. Chang, B. Ma, N. Wang, and X. Chen, "Dynamic r-cnn: Towards high quality object detection via dynamic training," in _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16_. Springer, 2020, pp. 260-275.\n' +
      '* [128] Y. Li, Y. Chen, N. Wang, and Z. Zhang, "Scale-aware trident networks for object detection," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2019, pp. 6054-6063.\n' +
      '* [129] H. Wu, Y. Chen, N. Wang, and Z. Zhang, "Sequence level semantics aggregation for video object detection," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 9217-9225.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
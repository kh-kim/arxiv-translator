{
    "2404.01204": {
        "paper_id": "2404.01204",
        "abs_url": "https://arxiv.org/abs/2404.01204",
        "pdf_url": "https://arxiv.org/pdf/2404.01204.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2404.01204_The_Fine_Line_Navigating_Large_Language_Model_Pretraining_with_Down-streaming_Capability_Analysis.pdf",
        "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Chen Yang",
            "Junzhuo Li",
            "Xinyao Niu",
            "Xinrun Du",
            "Songyang Gao",
            "Haoran Zhang",
            "Zhaoliang Chen",
            "Xingwei Qu",
            "Ruibin Yuan",
            "Yizhi Li",
            "Jiaheng Liu",
            "Stephen W. Huang",
            "Shawn Yue",
            "Wenhu Chen",
            "Jie Fu",
            "Ge Zhang"
        ],
        "abstract": "Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/the-fine-line-navigating-large-language-model",
        "bibtex": "@misc{yang2024fine,\n      title={The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis}, \n      author={Chen Yang and Junzhuo Li and Xinyao Niu and Xinrun Du and Songyang Gao and Haoran Zhang and Zhaoliang Chen and Xingwei Qu and Ruibin Yuan and Yizhi Li and Jiaheng Liu and Stephen W. Huang and Shawn Yue and Wenhu Chen and Jie Fu and Ge Zhang},\n      year={2024},\n      eprint={2404.01204},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
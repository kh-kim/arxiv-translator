<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.11584] The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</title><meta property="og:description" content="This survey paper examines the recent advancements in AI agent
implementations, with a focus on their ability to achieve complex goals
that require enhanced reasoning, planning, and tool execution
capabilities. The pri…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.11584">

<!--Generated on Sun May  5 21:46:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tula Masterman* 
<br class="ltx_break">Neudesic, an IBM Company 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">tula.masterman@neudesic.com</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Sandi Besen* 
<br class="ltx_break">IBM 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">sandi.besen@ibm.com</span> 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\AND</span>Mason Sawtell* 
<br class="ltx_break">Neudesic, an IBM Company 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">mason.sawtell@neudesic.com</span> 
<br class="ltx_break">
<br class="ltx_break">* Denotes Equal Contribution
<span id="id6.6.id6" class="ltx_ERROR undefined">\And</span>Alex Chao 
<br class="ltx_break">Microsoft 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">achao@microsoft.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">This survey paper examines the recent advancements in AI agent
implementations, with a focus on their ability to achieve complex goals
that require enhanced reasoning, planning, and tool execution
capabilities. The primary objectives of this work are to a) communicate
the current capabilities and limitations of existing AI agent
implementations, b) share insights gained from our
observations of these systems in action, and c) suggest important
considerations for future developments in AI agent design. We achieve
this by providing overviews of single-agent and multi-agent
architectures, identifying key patterns and divergences in design
choices, and evaluating their overall impact on accomplishing a provided
goal. Our contribution outlines key themes when selecting an agentic
architecture, the impact of leadership on agent systems, agent
communication styles, and key phases for planning, execution, and
reflection that enable robust AI agent systems.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>The opinions expressed in this paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.</span></span></span>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.9" class="ltx_p"><em id="p1.9.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.9.2" class="ltx_text ltx_font_bold">eywords</span> AI Agent &nbsp;<math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Agent Architecture &nbsp;<math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
AI Reasoning &nbsp;<math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Planning &nbsp;<math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
Tool Calling &nbsp;<math id="p1.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation></semantics></math>
Single Agent &nbsp;<math id="p1.6.m6.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation></semantics></math>
Multi Agent &nbsp;<math id="p1.7.m7.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation></semantics></math>
Agent Survey &nbsp;<math id="p1.8.m8.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.8.m8.1a"><mo id="p1.8.m8.1.1" xref="p1.8.m8.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.8.m8.1b"><ci id="p1.8.m8.1.1.cmml" xref="p1.8.m8.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.8.m8.1c">\cdot</annotation></semantics></math>
LLM Agent &nbsp;<math id="p1.9.m9.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.9.m9.1a"><mo id="p1.9.m9.1.1" xref="p1.9.m9.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.9.m9.1b"><ci id="p1.9.m9.1.1.cmml" xref="p1.9.m9.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.9.m9.1c">\cdot</annotation></semantics></math>
Autonomous Agent</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Since the launch of ChatGPT, many of the first wave of generative AI applications have been a variation of a chat over a corpus of documents using the Retrieval Augmented Generation (RAG) pattern. While there is a lot of activity in making RAG systems more robust, various groups are starting to build what the next generation of AI applications will look like, centralizing on a common theme: agents.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Beginning with investigations into recent foundation models like GPT-4 and popularized through open-source projects like AutoGPT and BabyAGI, the research community has experimented with building autonomous agent-based systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>, <a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">As opposed to zero-shot prompting of a large language model where a user types into an open-ended text field and gets a result without additional input, agents allow for more complex interaction and orchestration. In particular, agentic systems have a notion of planning, loops, reflection and other control structures that heavily leverage the model’s inherent reasoning capabilities to accomplish a task end-to-end. Paired with the ability to use tools, plugins, and function calling, agents are empowered to do more general-purpose work.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Among the community, there is a current debate on whether single or multi-agent systems are best suited for solving complex tasks. While single agent architectures excel when problems are well-defined and feedback from other agent-personas or the user is not needed, multi-agent architectures tend to thrive more when collaboration and multiple distinct execution paths are required.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.11584/assets/media/agent_comparison.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A visualization of single and multi-agent architectures with their underlying features and abilities</figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Taxonomy</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.1" class="ltx_p"><span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_bold">Agents</span>.
AI agents are language model-powered entities able to plan and take actions to execute goals over multiple iterations. AI agent architectures are either comprised of a single agent or multiple agents working together to solve a problem.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<p id="S1.SS1.p2.1" class="ltx_p">Typically, each agent is given a persona and access to a variety of tools that will help them accomplish their job either independently or as part of a team. Some agents also contain a memory component, where they can save and load information outside of their messages and prompts. In this paper, we follow the definition of agent that consists of “brain, perception, and action” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>]</cite>. These components satisfy the minimum requirements for agents to understand, reason, and act on the environment around them.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para ltx_noindent">
<p id="S1.SS1.p3.1" class="ltx_p"><span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_bold">Agent Persona</span>.
An agent persona describes the role or personality that the agent should take on, including any other instructions specific to that agent. Personas also contain descriptions of any tools the agent has access to. They make the agent aware of their role, the purpose of their tools, and how to leverage them effectively. Researchers have found that “shaped personality verifiably influences Large Language Model (LLM) behavior in common downstream (i.e. subsequent) tasks, such as writing social media posts” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite>. Solutions that use multiple agent personas to solve problems also show significant improvements compared to Chain-of-Thought (CoT) prompting where the model is asked to break down its plans step by step <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>, <a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para ltx_noindent">
<p id="S1.SS1.p4.1" class="ltx_p"><span id="S1.SS1.p4.1.1" class="ltx_text ltx_font_bold">Tools</span>. In the context of AI agents, tools represent any functions that the model can call. They allow the agent to interact with external data sources by pulling or pushing information to that source. An example of an agent persona and associated tools is a professional contract writer. The writer is given a persona explaining their role and the types of tasks it must accomplish. It is also given tools related to adding notes to a document, reading an existing document, or sending an email with a final draft.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para ltx_noindent">
<p id="S1.SS1.p5.1" class="ltx_p"><span id="S1.SS1.p5.1.1" class="ltx_text ltx_font_bold">Single Agent Architectures</span>.
These architectures are powered by one language model and will perform all the reasoning, planning, and tool execution on their own. The agent is given a system prompt and any tools required to complete their task. In single agent patterns there is no feedback mechanism from other AI agents; however, there may be options for humans to provide feedback that guides the agent.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para ltx_noindent">
<p id="S1.SS1.p6.1" class="ltx_p"><span id="S1.SS1.p6.1.1" class="ltx_text ltx_font_bold">Multi-Agent Architectures</span>.
These architectures involve two or more agents, where each agent can utilize the same language model or a set of different language models. The agents may have access to the same tools or different tools. Each agent typically has their own persona.</p>
</div>
<div id="S1.SS1.p7" class="ltx_para ltx_noindent">
<p id="S1.SS1.p7.1" class="ltx_p">Multi-agent architectures can have a wide variety of organizations at any level of complexity. In this paper, we divide them into two primary categories: vertical and horizontal. It is important to keep in mind that these categories represent two ends of a spectrum, where most existing architectures fall somewhere between these two extremes.</p>
</div>
<div id="S1.SS1.p8" class="ltx_para ltx_noindent">
<p id="S1.SS1.p8.1" class="ltx_p"><span id="S1.SS1.p8.1.1" class="ltx_text ltx_font_bold">Vertical Architectures</span>.
In this structure, one agent acts as a leader and has other agents report directly to them. Depending on the architecture, reporting agents may communicate exclusively with the lead agent. Alternatively, a leader may be defined with a shared conversation between all agents. The defining features of vertical architectures include having a lead agent and a clear division of labor between the collaborating agents.</p>
</div>
<div id="S1.SS1.p9" class="ltx_para ltx_noindent">
<p id="S1.SS1.p9.1" class="ltx_p"><span id="S1.SS1.p9.1.1" class="ltx_text ltx_font_bold">Horizontal Architectures</span>. In this structure, all the agents are treated as equals and are part of one group discussion about the task. Communication between agents occurs in a shared thread where each agent can see all messages from the others. Agents also can volunteer to complete certain tasks or call tools, meaning they do not need to be assigned by a leading agent. Horizontal architectures are generally used for tasks where collaboration, feedback and group discussion are key to the overall success of the task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Key Considerations for Effective Agents</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Agents are designed to extend language model capabilities to solve real-world challenges. Successful implementations require robust problem-solving capabilities enabling agents to perform well on novel tasks. To solve real-world problems effectively, agents require the ability to reason and plan as well as call tools that interact with an external environment. In this section we explore why reasoning, planning, and tool calling are critical to agent success.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>The Importance of Reasoning and Planning</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Reasoning is a fundamental building block of human cognition, enabling people to make decisions, solve problems, and understand the world around us. AI agents need a strong ability to reason if they are to effectively interact with complex environments, make autonomous decisions, and assist humans in a wide range of tasks. This tight synergy between “acting” and “reasoning” allows new tasks to be learned quickly and enables robust decision making or reasoning, even under previously unseen circumstances or information uncertainties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>. Additionally, agents need reasoning to adjust their plans based on new feedback or information learned.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">If agents lacking reasoning skills are tasked with acting on straightforward tasks, they may misinterpret the query, generate a response based on a literal understanding, or fail to consider multi-step implications.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">Planning, which requires strong reasoning abilities, commonly falls into one of five major approaches: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>. These approaches allow the model to either break the task down into sub tasks, select one plan from many generated options, leverage a preexisting external plan, revise previous plans based on new information, or leverage external information to improve the plan.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p">Most agent patterns have a dedicated planning step which invokes one or more of these techniques to create a plan before any actions are executed. For example, Plan Like a Graph (PLaG) is an approach that represents plans as directed graphs, with multiple steps being executed in parallel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>, <a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>. This can provide a significant performance increase over other methods on tasks that contain many independent subtasks that benefit from asynchronous execution.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>The Importance of Effective Tool Calling</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">One key benefit of the agent abstraction over prompting base language models is the agents’ ability to solve complex problems by calling multiple tools. These tools enable the agent to interact with external data sources, send or retrieve information from existing APIs, and more. Problems that require extensive tool calling often go hand in hand with those that require complex reasoning.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps. Many methods use multiple iterations of reasoning, memory, and reflection to effectively and accurately complete problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>, <a href="#bib.bibx23" title="" class="ltx_ref">23</a>, <a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>. They often do this by breaking a larger problem into smaller subproblems, and then solving each one with the appropriate tools in sequence.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">Other works focused on advancing agent patterns highlight that while breaking a larger problem into smaller subproblems can be effective at solving complex tasks, single agent patterns often struggle to complete the long sequence required <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>, <a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p">Multi-agent patterns can address the issues of parallel tasks and robustness since individual agents can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Single Agent Architectures</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we highlight some notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, and LATS. Each of these methods contain a dedicated stage for reasoning about the problem before any action is taken to advance the goal. We selected these methods based on their contributions to the reasoning and tool calling capabilities of agents.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Key Themes</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">We find that successful goal execution by agents is contingent upon proper planning and self-correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>, <a href="#bib.bibx16" title="" class="ltx_ref">16</a>, <a href="#bib.bibx23" title="" class="ltx_ref">23</a>, <a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Examples</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">ReAct.</span>
In the ReAct (Reason + Act) method, an agent first writes a thought about the given task. It then performs an action based on that thought, and the output is observed. This cycle can repeat until the task is complete <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>. When applied to a diverse set of language and decision-making tasks, the ReAct method demonstrates improved effectiveness compared to zero-shot prompting on the same tasks. It also provides improved human interoperability and trustworthiness because the entire thought process of the model is recorded. When evaluated on the HotpotQA dataset, the ReAct method only hallucinated 6% of the time, compared to 14% using the chain of thought (CoT) method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>, <a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">However, the ReAct method is not without its limitations. While intertwining reasoning, observation, and action improves trustworthiness, the model can repetitively generate the same thoughts and actions and fail to create new thoughts to provoke finishing the task and exiting the ReAct loop. Incorporating human feedback during the execution of the task would likely increase its effectiveness and applicability in real-world scenarios.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.11584/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of the ReAct method compared to other methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite></figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">RAISE.</span>
The RAISE method is built upon the ReAct method, with the addition of a memory mechanism that mirrors human short-term and long-term memory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>. It does this by using a scratchpad for short-term storage and a dataset of similar previous examples for long-term storage.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">By adding these components, RAISE improves upon the agent’s ability to retain context in longer conversations. The paper also highlights how fine-tuning the model results in the best performance for their task, even when using a smaller model. They also showed that RAISE outperforms ReAct in both efficiency and output quality.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p">While RAISE significantly improves upon existing methods in some respects, the researchers also highlighted several issues. First, RAISE struggles to understand complex logic, limiting its usefulness in many scenarios. Additionally, RAISE agents often hallucinated with respect to their roles or knowledge. For example, a sales agent without a clearly defined role might retain the ability to code in Python, which may enable them to start writing Python code instead of focusing on their sales tasks. These agents might also give the user misleading or incorrect information. This problem was addressed by fine-tuning the model, but the researchers still highlighted hallucination as a limitation in the RAISE implementation.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.11584/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A diagram showing the RAISE method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite></figcaption>
</figure>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Reflexion.</span>
Reflexion is a single-agent pattern that uses self-reflection through linguistic feedback <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>. By utilizing metrics such as success state, current trajectory, and persistent memory, this method uses an LLM evaluator to provide specific and relevant feedback to the agent. This results in an improved success rate as well as reduced hallucination compared to Chain-of-Thought and ReAct.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.1" class="ltx_p">Despite these advancements, the Reflexion authors identify various limitations of the pattern. Primarily, Reflexion is susceptible to “non-optimal local minima solutions”. It also uses a sliding window for long-term memory, rather than a database. This means that the volume of long-term memory is limited by the token limit of the language model. Finally, the researchers identify that while Reflexion surpasses other single-agent patterns, there are still opportunities to improve performance on tasks that require a significant amount of diversity, exploration, and reasoning.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para ltx_noindent">
<p id="S3.SS3.p8.1" class="ltx_p"><span id="S3.SS3.p8.1.1" class="ltx_text ltx_font_bold">AUTOGPT + P.</span>
AutoGPT + P (Planning) is a method that addresses reasoning limitations for agents that command robots in natural language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. AutoGPT+P combines object detection and Object Affordance Mapping (OAM) with a planning system driven by a LLM. This allows the agent to explore the environment for missing objects, propose alternatives, or ask the user for assistance with reaching its goal.</p>
</div>
<div id="S3.SS3.p9" class="ltx_para ltx_noindent">
<p id="S3.SS3.p9.1" class="ltx_p">AutoGPT+P starts by using an image of a scene to detect the objects present. A language model then uses those objects to select which tool to use, from four options: Plan Tool, Partial Plan Tool, Suggest Alternative Tool, and Explore Tool. These tools allow the robot to not only generate a full plan to complete the goal, but also to explore the environment, make assumptions, and create partial plans.</p>
</div>
<div id="S3.SS3.p10" class="ltx_para ltx_noindent">
<p id="S3.SS3.p10.1" class="ltx_p">However, the language model does not generate the plan entirely on its own. Instead, it generates goals and steps to work aside a classical planner which executes the plan using Planning Domain Definition Language (PDDL). The paper found that “LLMs currently lack the ability to directly translate a natural language instruction into a plan for executing robotic tasks, primarily due to their constrained reasoning capabilities” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. By combining the LLM planning capabilities with a classical planner, their approach significantly improves upon other purely language model-based approaches to robotic planning.</p>
</div>
<div id="S3.SS3.p11" class="ltx_para ltx_noindent">
<p id="S3.SS3.p11.1" class="ltx_p">As with most first of their kind approaches, AutoGPT+P is not without its drawbacks. Accuracy of tool selection varies, with certain tools being called inappropriately or getting stuck in loops. In scenarios where exploration is required, the tool selection sometimes leads to illogical exploration decisions like looking for objects in the wrong place. The framework also is limited in terms of human interaction, with the agent being unable to seek clarification and the user being unable to modify or terminate the plan during execution.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.11584/assets/media/autogpt+p.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="499" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A diagram of the AutoGPT+P method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite></figcaption>
</figure>
<div id="S3.SS3.p12" class="ltx_para ltx_noindent">
<p id="S3.SS3.p12.1" class="ltx_p"><span id="S3.SS3.p12.1.1" class="ltx_text ltx_font_bold">LATS.</span>
Language Agent Tree Search (LATS) is a single-agent method that synergizes planning, acting, and reasoning by using trees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">36</a>]</cite>. This technique, inspired by Monte Carlo Tree Search, represents a state as a node and taking an action as traversing between nodes. It uses LM-based heuristics to search for possible options, then selects an action using a state evaluator.</p>
</div>
<div id="S3.SS3.p13" class="ltx_para ltx_noindent">
<p id="S3.SS3.p13.1" class="ltx_p">When compared to other tree-based methods, LATS implements a self-reflection reasoning step that dramatically improves performance. When an action is taken, both environmental feedback as well as feedback from a language model is used to determine if there are any errors in reasoning and propose alternatives. This ability to self-reflect combined with a powerful search algorithm makes LATS perform extremely well on various tasks.</p>
</div>
<div id="S3.SS3.p14" class="ltx_para ltx_noindent">
<p id="S3.SS3.p14.1" class="ltx_p">However, due to the complexity of the algorithm and the reflection steps involved, LATS often uses more computational resources and takes more time to complete than other single-agent methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">36</a>]</cite>. The paper also uses relatively simple question answering benchmarks and has not been tested on more robust scenarios that involve involving tool calling or complex reasoning.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Multi Agent Architectures</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overview</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">In this section, we examine a few key studies and sample frameworks with multi-agent architectures, such as Embodied LLM Agents Learn to Cooperate in Organized Teams, DyLAN, AgentVerse, and MetaGPT. We highlight how these implementations facilitate goal execution through inter-agent communication and collaborative plan execution. This is not intended to be an exhaustive list of all agent frameworks, our goal is to provide broad coverage of key themes and examples related to multi-agent patterns.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Key Themes</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Multi-agent architectures create an opportunity for both the intelligent division of labor based on skill and helpful feedback from a variety of agent personas. Many multi-agent architectures work in stages where teams of agents are created and reorganized dynamically for each planning, execution, and evaluation phase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>, <a href="#bib.bibx9" title="" class="ltx_ref">9</a>, <a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>. This reorganization provides superior results because specialized agents are employed for certain tasks, and removed when they are no longer needed. By matching agents roles and skills to the task at hand, agent teams can achieve greater accuracy and decrease time to meet the goal. Key features of effective multi-agent architectures include clear leadership in agent teams, dynamic team construction, and effective information sharing between team members so that important information does not get lost in superfluous chatter.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Examples</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Embodied LLM Agents Learn to Cooperate in Organized Teams.</span>
Research by Guo et al. demonstrates the impact of a lead agent on the overall effectiveness of the agent team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>. This architecture contains a vertical component through the leader agent, as well as a horizontal component from the ability for agents to converse with other agents besides the leader. The results of their study demonstrate that agent teams with an organized leader complete their tasks nearly 10% faster than teams without a leader.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">Furthermore, they discovered that in teams without a designated leader, agents spent most of their time giving orders to one another (~50% of communication), splitting their remaining time between sharing information, or requesting guidance. Conversely, in teams with a designated leader, 60% of the leader’s communication involved giving directions, prompting other members to focus more on exchanging and requesting information. Their results demonstrate that agent teams are most effective when the leader is a human.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.11584/assets/media/leader_results.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="170" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Agent teams with a designated leader achieve superior performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.1" class="ltx_p ltx_figure_panel ltx_align_center">.</p>
</div>
</div>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p">Beyond team structure, the paper emphasizes the importance of employing a “criticize-reflect” step for generating plans, evaluating performance, providing feedback, and re-organizing the team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>. Their results indicate that agents with a dynamic team structure with rotating leadership provide the best results, with both the lowest time to task completion and the lowest communication cost on average. Ultimately, leadership and dynamic team structures improve the overall team’s ability to reason, plan, and perform tasks effectively.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">DyLAN.</span>
The Dynamic LLM-Agent Network (DyLAN) framework creates a dynamic agent structure that focuses on complex tasks like reasoning and code generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>. DyLAN has a specific step for determining how much each agent has contributed in the last round of work and only moves top contributors the next round of execution. This method is horizontal in nature since agents can share information with each other and there is no defined leader. DyLAN shows improved performance on a variety of benchmarks which measure arithmetic and general reasoning capabilities. This highlights the impact of dynamic teams and demonstrates that by consistently re-evaluating and ranking agent contributions, we can create agent teams that are better suited to complete a given task.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">AgentVerse.</span>
Multi-agent architectures like AgentVerse demonstrate how distinct phases for group planning can improve an AI agent’s reasoning and problem-solving capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>. AgentVerse contains four primary stages for task execution: recruitment, collaborative decision making, independent action execution, and evaluation. This can be repeated until the overall goal is achieved. By strictly defining each phase, AgentVerse helps guide the set of agents to reason, discuss, and execute more effectively.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para ltx_noindent">
<p id="S4.SS3.p6.1" class="ltx_p">As an example, the recruitment step allows agents to be removed or added based on the progress towards the goal. This helps ensure that the right agents are participating at any given stage of problem solving. The researchers found that horizontal teams are generally best suited for collaborative tasks like consulting, while vertical teams are better suited for tasks that require clearer isolation of responsibilities for tool calling.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2404.11584/assets/x3.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A diagram of the AgentVerse method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite></figcaption>
</figure>
<div id="S4.SS3.p7" class="ltx_para ltx_noindent">
<p id="S4.SS3.p7.1" class="ltx_p"><span id="S4.SS3.p7.1.1" class="ltx_text ltx_font_bold">MetaGPT.</span>
Many multi-agent architectures allow agents to converse with one another while collaborating on a common problem. This conversational capability can lead to chatter between the agents that is superfluous and does not further the team goal. MetaGPT addresses the issue of unproductive chatter amongst agents by requiring agents to generate structured outputs like documents and diagrams instead of sharing unstructured chat messages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S4.SS3.p8" class="ltx_para ltx_noindent">
<p id="S4.SS3.p8.1" class="ltx_p">Additionally, MetaGPT implements a ”publish-subscribe” mechanism for information sharing. This allows all the agents to share information in one place, but only read information relevant to their individual goals and tasks. This streamlines the overall goal execution and reduces conversational noise between agents. When compared to single-agent architectures on the HumanEval and MBPP benchmarks, MetaGPT’s multi-agent architecture demonstrates significantly better results.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Observations</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Overview</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">In this section we discuss the key themes and impacts of the design choices exhibited in the previously outlined agent patterns. These patterns serve as key examples of the growing body of research and implementation of AI agent architectures. Both single and multi-agent architectures seek to enhance the capabilities of language models by giving them the ability to execute goals on behalf of or alongside a human user. Most observed agent implementations broadly follow the plan, act, and evaluate process to iteratively solve problems.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">We find that both single and multi-agent architectures demonstrate compelling performance on complex goal execution. We also find that across architectures clear feedback, task decomposition, iterative refinement, and role definition yield improved agent performance.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Key Findings</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Typical Conditions for Selecting a Single vs Multi-Agent
Architecture.</span>
Based on the aforementioned agent patterns, we find that single-agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. Single agents are also typically easier to implement since only one agent and set of tools needs to be defined. Additionally, single agent architectures do not face limitations like poor feedback from other agents or distracting and unrelated chatter from other team members. However, they may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities are not robust.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Multi-agent architectures are generally well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. For example, document generation may benefit from a multi-agent architecture where one agent provides clear feedback to another on a written section of the document. Multi-agent systems are also useful when parallelization across distinct tasks or workflows is required. Crucially, Wang et. al finds that multi-agent patterns perform better than single agents in scenarios when no examples are provided <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>. By nature, multi-agent systems are more complex and often benefit from robust conversation management and clear leadership.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">While single and multi-agent patterns have diverging capabilities in terms of scope, research finds that “multi-agent discussion does not necessarily enhance reasoning when the prompt provided to an agent is sufficiently robust” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>. This suggests that those implementing agent architectures should decide between single or multiple agents based on the broader context of their use case, and not based on the reasoning capabilities required.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Agents and Asynchronous Task Execution.</span>
While a single agent can initiate multiple asynchronous calls simultaneously, its operational model does not inherently support the division of responsibilities across different execution threads. This means that, although tasks are handled asynchronously, they are not truly parallel in the sense of being autonomously managed by separate decision-making entities. Instead, the single agent must sequentially plan and execute tasks, waiting for one batch of asynchronous operations to complete before it can evaluate and move on to the next step. Conversely, in multi-agent architectures, each agent can operate independently, allowing for a more dynamic division of labor. This structure not only facilitates simultaneous task execution across different domains or objectives but also allows individual agents to proceed with their next steps without being hindered by the state of tasks handled by others, embodying a more flexible and parallel approach to task management.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Impact of Feedback and Human Oversight on Agent Systems.</span>
When solving a complex problem, it is extremely unlikely that one provides a correct, robust solution on their first try. Instead, one might pose a potential solution before criticizing it and refining it. One could also consult with someone else and receive feedback from another perspective. The same idea of iterative feedback and refinement is essential for helping agents solve complex problems.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p">This is partially because language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">34</a>]</cite> . By implementing feedback, agents are much more likely to correct their course and reach their goal.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para ltx_noindent">
<p id="S5.SS2.p7.1" class="ltx_p">Additionally, the inclusion of human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, mitigating the potential for agents to delve down an inefficient or invalid approach to solving a task. As of today, including human validation and feedback in the agent architecture yields more reliable and trustworthy results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>, <a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S5.SS2.p8" class="ltx_para ltx_noindent">
<p id="S5.SS2.p8.1" class="ltx_p">Language models also exhibit sycophantic behavior, where they “tend to mirror the user’s stance, even if it means forgoing the presentation of an impartial or balanced viewpoint” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>. Specifically, the AgentVerse paper describes how agents are susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>. Robust prompting can help mitigate this, but those developing agent applications should be aware of the risks when implementing user or agent feedback systems.</p>
</div>
<div id="S5.SS2.p9" class="ltx_para ltx_noindent">
<p id="S5.SS2.p9.1" class="ltx_p"><span id="S5.SS2.p9.1.1" class="ltx_text ltx_font_bold">Challenges with Group Conversations and Information Sharing.</span>
One challenge with multi-agent architectures lies in their ability to
intelligently share messages between agents. Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. The extraneous dialogue in multi-agent systems can impair both the agent’s ability to reason effectively and execute the right tools, ultimately distracting the agents from the task and decreasing team efficiency. This is especially true in a horizontal architecture, where agents typically share a group chat and are privy to every agent’s message in a conversation. Message subscribing or filtering improves multi-agent performance by ensuring agents only receive information relevant to their tasks.</p>
</div>
<div id="S5.SS2.p10" class="ltx_para ltx_noindent">
<p id="S5.SS2.p10.1" class="ltx_p">In vertical architectures, tasks tend to be clearly divided by agent skill which helps reduce distractions in the team. However, challenges arise when the leading agent fails to send critical information to their supporting agents and does not realize the other agents aren’t privy to necessary information. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.</p>
</div>
<div id="S5.SS2.p11" class="ltx_para ltx_noindent">
<p id="S5.SS2.p11.1" class="ltx_p"><span id="S5.SS2.p11.1.1" class="ltx_text ltx_font_bold">Impact of Role Definition and Dynamic Teams.</span>
Clear role definition is critical for both single and multi-agent architectures. In single-agent architectures role definition ensures that the agent stays focused on the provided task, executes the proper tools, and minimizes hallucination of other capabilities. Similarly, role definition in multi-agent architectures ensures each agent knows what it’s responsible for in the overall team and does not take on tasks outside of their described capabilities or scope. Beyond individual role definition, establishing a clear group leader also improves the overall performance of multi-agent teams by streamlining task assignment. Furthermore, defining a clear system prompt for each agent can minimize excess chatter by prompting the agents not to engage in unproductive communication.</p>
</div>
<div id="S5.SS2.p12" class="ltx_para ltx_noindent">
<p id="S5.SS2.p12.1" class="ltx_p">Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the planning or execution of tasks are fit for that round of work.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Summary</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Both single and multi-agent patterns exhibit strong performance on a variety of complex tasks involving reasoning and tool execution. Single agent patterns perform well when given a defined persona and set of tools, opportunities for human feedback, and the ability to work iteratively towards their goal. When constructing an agent team that needs to collaborate on complex goals, it is beneficial to deploy agents with at least one of these key elements: clear leader(s), a defined planning phase and opportunities to refine the plan as new information is learned, intelligent message filtering, and dynamic teams whose agents possess specific skills relevant to the current sub-task. If an agent architecture employs at least one of these approaches it is likely to result in increased performance compared to a single agent architecture or a multi-agent architecture without these tactics.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations of Current Research and Considerations for Future Research</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Overview</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p">In this section we examine some of the limitations of agent research today and identify potential areas for improving AI agent systems. While agent architectures have significantly enhanced the capability of language models in many ways, there are some major challenges around evaluations, overall reliability, and issues inherited from the language models powering each agent.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Challenges with Agent Evaluation</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p">While LLMs are evaluated on a standard set of benchmarks designed to gauge their general understanding and reasoning capabilities, the benchmarks for agent evaluation vary greatly.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.p2.1" class="ltx_p">Many research teams introduce their own unique agent benchmarks alongside their agent implementation which makes comparing multiple agent implementations on the same benchmark challenging. Additionally, many of these new agent-specific benchmarks include a hand-crafted, highly complex, evaluation set where the results are manually scored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>. This can provide a high-quality assessment of a method’s capabilities, but it also lacks the robustness of a larger dataset and risks introducing bias into the evaluation, since the ones developing the method are also the ones writing and scoring the results. Agents can also have problems generating a consistent answer over multiple iterations, due to variability in the models, environment, or problem state. This added randomness poses a much larger problem to smaller, complex evaluation sets.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Impact of Data Contamination and Static Benchmarks</h3>

<div id="S6.SS3.p1" class="ltx_para ltx_noindent">
<p id="S6.SS3.p1.1" class="ltx_p">Some researchers evaluate their agent implementations on the typical LLM benchmarks. Emerging research indicates that there is significant data contamination in the model’s training data, supported by the observation that a model’s performance significantly worsens when benchmark questions are modified <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>, <a href="#bib.bibx38" title="" class="ltx_ref">38</a>, <a href="#bib.bibx37" title="" class="ltx_ref">37</a>]</cite>. This raises doubts on the authenticity of benchmark scores for both the language models and language model powered agents.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS3.p2.1" class="ltx_p">Furthermore, researchers have found that “As LLMs progress at a rapid pace, existing datasets usually fail to match the models’ ever-evolving capabilities, because the complexity level of existing benchmarks is usually static and fixed” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">37</a>]</cite>. To address this, work has been done to create dynamic benchmarks that are resistant to simple memorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">38</a>, <a href="#bib.bibx37" title="" class="ltx_ref">37</a>]</cite>. Researchers have also explored the idea of generating an entirely synthetic benchmark based on a user’s specific environment or use case <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>, <a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>. While these techniques can help with contamination, decreasing the level of human involvement can pose additional risks regarding correctness and the ability to solve problems.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Benchmark Scope and Transferability</h3>

<div id="S6.SS4.p1" class="ltx_para ltx_noindent">
<p id="S6.SS4.p1.1" class="ltx_p">Many language model benchmarks are designed to be solved in a single iteration, with no tool calls, such as MMLU or GSM8K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>, <a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>. While these are important for measuring the abilities of base language models, they are not good proxies for agent capabilities because they do not account for agent systems’ ability to reason over multiple steps or access outside information. StrategyQA improves upon this by assessing models’ reasoning abilities over multiple steps, but the answers are limited to Yes/No responses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>. As the industry continues to pivot towards agent focused use-cases additional measures will be needed to better assess the performance and generalizability of agents to tasks involving tools that extend beyond their training data.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para ltx_noindent">
<p id="S6.SS4.p2.1" class="ltx_p">Some agent specific benchmarks like AgentBench evaluate language model-based agents in a variety of different environments such as web browsing, command-line interfaces, and video games <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>. This provides a better indication for how well agents can generalize to new environments, by reasoning, planning, and calling tools to achieve a given task. Benchmarks like AgentBench and SmartPlay introduce objective evaluation metrics designed to evaluate the implementation’s success rate, output similarity to human responses, and overall efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>, <a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite>. While these objective metrics are important to understanding the overall reliability and accuracy of the implementation, it is also important to consider more nuanced or subjective measures of performance. Metrics such as efficiency of tool use, reliability, and robustness of planning are nearly as important as success rate but are much more difficult to measure. Many of these metrics require evaluation by a human expert, which can be costly and time consuming compared to LLM-as-judge evaluations.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Real-world Applicability</h3>

<div id="S6.SS5.p1" class="ltx_para ltx_noindent">
<p id="S6.SS5.p1.1" class="ltx_p">Many of the existing benchmarks focus on the ability of Agent systems to reason over logic puzzles or video games <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>. While evaluating performance on these types of tasks can help get a sense of the reasoning capabilities of agent systems, it is unclear whether performance on these benchmarks translates to real-world performance. Specifically, real-world data can be noisy and cover a much wider breadth of topics that many common benchmarks lack.</p>
</div>
<div id="S6.SS5.p2" class="ltx_para ltx_noindent">
<p id="S6.SS5.p2.1" class="ltx_p">One popular benchmark that uses real-world data is WildBench, which is sourced from the WildChat dataset of 570,000 real conversations with ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">35</a>]</cite>. Because of this, it covers a huge breadth of tasks and prompts. While WildBench covers a wide range of topics, most other real-world benchmarks focus on a specific task. For example, SWE-bench is a benchmark that uses a set of real-world issues raised on GitHub for software engineering tasks in Python <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>. This can be very helpful when evaluating agents designed to write Python code and provides a sense for how well agents can reason about code related problems; however, it is less informative when trying to understand agent capabilities involving other programming languages.</p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Bias and Fairness in Agent Systems</h3>

<div id="S6.SS6.p1" class="ltx_para ltx_noindent">
<p id="S6.SS6.p1.1" class="ltx_p">Language Models have been known to exhibit bias both in terms of evaluation as well as in social or fairness terms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>. Moreover, agents have specifically been shown to be “less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>. Other research has found “a tendency for LLM agents to conform to the model’s inherent social biases despite being directed to debate from certain political perspectives” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite>. This tendency can lead to faulty reasoning in any agent-based implementation.</p>
</div>
<div id="S6.SS6.p2" class="ltx_para ltx_noindent">
<p id="S6.SS6.p2.1" class="ltx_p">As the complexity of tasks and agent involvement increases, more research is needed to identify and address biases within these systems. This poses a very large challenge to researchers, since scalable and novel benchmarks often involve some level of LLM involvement during creation. However, a truly robust benchmark for evaluating bias in LLM-based agents must include human evaluation.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Directions</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">The AI agent implementations explored in this survey demonstrate the rapid enhancement in language model powered reasoning, planning, and tool calling. Single and multi-agent patterns both show the ability to tackle complex multi-step problems that require advanced problem-solving skills. The key insights discussed in this paper suggest that the best agent architecture varies based on use case. Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution - evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">While the current state of AI-driven agents is promising, there are notable limitations and areas for future improvement. Challenges around comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases will need to be addressed in the near-term to enable reliable agents. By examining the progression from static language models to more dynamic, autonomous agents, this survey aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Timo Birr, Christoph Pohl, Abdelrahman Younes and Tamim Asfour
</span>
<span class="ltx_bibblock">“AutoGPT+P: Affordance-based Task Planning with Large Language Models” arXiv:2402.10778 [cs] version: 1
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2402.10778" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2402.10778</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Weize Chen et al.
</span>
<span class="ltx_bibblock">“AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors” arXiv:2308.10848 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2308.10848" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2308.10848</a>
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Karl Cobbe et al.
</span>
<span class="ltx_bibblock">“Training Verifiers to Solve Math Word Problems” arXiv:2110.14168 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2021
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2110.14168</a>
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Xueyang Feng et al.
</span>
<span class="ltx_bibblock">“Large Language Model-based Human-Agent Collaboration for Complex Task Solving”, 2024
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2402.12914" title="" class="ltx_ref ltx_href">2402.12914 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Isabel O. Gallegos et al.
</span>
<span class="ltx_bibblock">“Bias and Fairness in Large Language Models: A Survey” arXiv:2309.00770 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2309.00770" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2309.00770</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Silin Gao et al.
</span>
<span class="ltx_bibblock">“Efficient Tool Use with Chain-of-Abstraction Reasoning” arXiv:2401.17464 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2401.17464" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2401.17464</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Mor Geva et al.
</span>
<span class="ltx_bibblock">“Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies” arXiv:2101.02235 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2021
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2101.02235" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2101.02235</a>
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Shahriar Golchin and Mihai Surdeanu
</span>
<span class="ltx_bibblock">“Time Travel in LLMs: Tracing Data Contamination in Large Language Models” arXiv:2308.08493 [cs] version: 3
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2308.08493" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2308.08493</a>
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Xudong Guo et al.
</span>
<span class="ltx_bibblock">“Embodied LLM Agents Learn to Cooperate in Organized Teams”, 2024
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2403.12482" title="" class="ltx_ref ltx_href">2403.12482 [cs.AI]</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Dan Hendrycks et al.
</span>
<span class="ltx_bibblock">“Measuring Massive Multitask Language Understanding” arXiv:2009.03300 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2021
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2009.03300" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2009.03300</a>
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Sirui Hong et al.
</span>
<span class="ltx_bibblock">“MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2308.00352" title="" class="ltx_ref ltx_href">2308.00352 [cs.AI]</a>
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Xu Huang et al.
</span>
<span class="ltx_bibblock">“Understanding the planning of LLM agents: A survey”, 2024
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2402.02716" title="" class="ltx_ref ltx_href">2402.02716 [cs.AI]</a>
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Carlos E. Jimenez et al.
</span>
<span class="ltx_bibblock">“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?” arXiv:2310.06770 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2310.06770" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2310.06770</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Fangyu Lei et al.
</span>
<span class="ltx_bibblock">“S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models” arXiv:2310.15147 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2310.15147" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2310.15147</a>
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Fangru Lin et al.
</span>
<span class="ltx_bibblock">“Graph-enhanced Large Language Models in Asynchronous Plan Reasoning” arXiv:2402.02805 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2402.02805" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2402.02805</a>
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Na Liu et al.
</span>
<span class="ltx_bibblock">“From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models” arXiv:2401.02777 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2401.02777" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2401.02777</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Xiao Liu et al.
</span>
<span class="ltx_bibblock">“AgentBench: Evaluating LLMs as Agents” arXiv:2308.03688 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2308.03688" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2308.03688</a>
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Zijun Liu et al.
</span>
<span class="ltx_bibblock">“Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2310.02170" title="" class="ltx_ref ltx_href">2310.02170 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Yohei Nakajima
</span>
<span class="ltx_bibblock">“yoheinakajima/babyagi” original-date: 2023-04-03T00:40:27Z, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://github.com/yoheinakajima/babyagi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yoheinakajima/babyagi</a>
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Peter S. Park et al.
</span>
<span class="ltx_bibblock">“AI Deception: A Survey of Examples, Risks, and Potential Solutions” arXiv:2308.14752 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2308.14752" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2308.14752</a>
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Greg Serapio-García et al.
</span>
<span class="ltx_bibblock">“Personality Traits in Large Language Models”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2307.00184" title="" class="ltx_ref ltx_href">2307.00184 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Zhengliang Shi et al.
</span>
<span class="ltx_bibblock">“Learning to Use Tools via Cooperative and Interactive Agents” arXiv:2403.03031 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2403.03031" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2403.03031</a>
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Noah Shinn et al.
</span>
<span class="ltx_bibblock">“Reflexion: Language Agents with Verbal Reinforcement Learning” arXiv:2303.11366 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2303.11366" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2303.11366</a>
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Amir Taubenfeld, Yaniv Dover, Roi Reichart and Ariel Goldstein
</span>
<span class="ltx_bibblock">“Systematic Biases in LLM Simulations of Debates” arXiv:2402.04049 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2402.04049" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2402.04049</a>
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Yu Tian et al.
</span>
<span class="ltx_bibblock">“Evil Geniuses: Delving into the Safety of LLM-based Agents” arXiv:2311.11855 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2311.11855" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2311.11855</a>
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Qineng Wang et al.
</span>
<span class="ltx_bibblock">“Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?” arXiv:2402.18272 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2402.18272" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2402.18272</a>
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Siyuan Wang et al.
</span>
<span class="ltx_bibblock">“Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation” arXiv:2402.11443 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2402.11443" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2402.11443</a>
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Zhenhailong Wang et al.
</span>
<span class="ltx_bibblock">“Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration”, 2024
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2307.05300" title="" class="ltx_ref ltx_href">2307.05300 [cs.AI]</a>
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Jason Wei et al.
</span>
<span class="ltx_bibblock">“Chain-of-Thought Prompting Elicits Reasoning in Large Language Models” arXiv:2201.11903 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2201.11903" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2201.11903</a>
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Yue Wu, Xuan Tang, Tom M. Mitchell and Yuanzhi Li
</span>
<span class="ltx_bibblock">“SmartPlay: A Benchmark for LLMs as Intelligent Agents” arXiv:2310.01557 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2310.01557" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2310.01557</a>
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Zhiheng Xi et al.
</span>
<span class="ltx_bibblock">“The Rise and Potential of Large Language Model Based Agents: A Survey”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2309.07864" title="" class="ltx_ref ltx_href">2309.07864 [cs.AI]</a>
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Shunyu Yao et al.
</span>
<span class="ltx_bibblock">“ReAct: Synergizing Reasoning and Acting in Language Models” arXiv:2210.03629 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2210.03629" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2210.03629</a>
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Shunyu Yao et al.
</span>
<span class="ltx_bibblock">“Tree of Thoughts: Deliberate Problem Solving with Large Language Models” arXiv:2305.10601 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2305.10601" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2305.10601</a>
</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Muru Zhang et al.
</span>
<span class="ltx_bibblock">“How Language Model Hallucinations Can Snowball” arXiv:2305.13534 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2305.13534" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2305.13534</a>
</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Wenting Zhao et al.
</span>
<span class="ltx_bibblock">“(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://openreview.net/forum?id=Bl8u7ZRlbM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Bl8u7ZRlbM</a>
</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Andy Zhou et al.
</span>
<span class="ltx_bibblock">“Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models” arXiv:2310.04406 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2310.04406" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2310.04406</a>
</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Kaijie Zhu et al.
</span>
<span class="ltx_bibblock">“DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents” arXiv:2402.14865 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2402.14865" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2402.14865</a>
</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Kaijie Zhu et al.
</span>
<span class="ltx_bibblock">“DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks” arXiv:2309.17167 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2024
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2309.17167" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2309.17167</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.11583" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.11584" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.11584">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.11584" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.11585" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 21:46:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
추론, 계획, 도구 호출을 위한 새로운 AI 에이전트 아키텍처의 경관: 설문조사

 툴라 마스터맨

IBM 회사의 누데식

tula.masterman@neudesic.com

&Sandi Besen*

IBM

sandi.besen@ibm.com

&Mason Sawtell*

IBM 회사의 누데식

mason.sawtell@neudesic.com

&Alex Chao

Microsoft

achao@microsoft.com

* 동일한 기여도

###### Abstract

본 논문은 AI 에이전트 구현의 최근 발전을 살펴보고, 향상된 추론, 계획 및 도구 실행 기능이 필요한 복잡한 목표를 달성하는 능력에 초점을 맞춘다. 이 작업의 주요 목적은 a) 기존 AI 에이전트 구현의 현재 기능과 한계를 전달하고, b) 이러한 시스템의 관찰에서 얻은 통찰력을 공유하며, c) AI 에이전트 설계의 향후 개발을 위한 중요한 고려 사항을 제안하는 것이다. 이를 위해 단일 에이전트 및 다중 에이전트 아키텍처에 대한 개요를 제공하고, 설계 선택에서 주요 패턴과 분기점을 식별하고, 제공된 목표를 달성하는 데 미치는 전반적인 영향을 평가합니다. 우리의 기여는 강력한 AI 에이전트 시스템을 가능하게 하는 계획, 실행 및 반성을 위한 에이전트 아키텍처, 리더십이 에이전트 시스템에 미치는 영향, 에이전트 커뮤니케이션 스타일 및 주요 단계를 선택할 때 주요 주제를 설명한다.

AI 에이전트 아키텍처 AI 추론 계획 도구 호출 단일 에이전트 다중 에이전트 조사 LLM 에이전트 자율 에이전트

## 1 Introduction

ChatGPT가 출시된 이후, 생성 AI 응용의 많은 첫 번째 물결은 RAG(Retrieval Augmented Generation) 패턴을 사용하는 문서 코퍼스에 대한 채팅의 변형이다. RAG 시스템을 더 견고하게 만드는 활동이 많은 반면, 다양한 그룹은 공통 주제인 에이전트를 중심으로 차세대 AI 애플리케이션이 어떻게 보일지 구축하기 시작하고 있다.

GPT-4와 같은 최근 기반 모델에 대한 조사를 시작으로 AutoGPT 및 BabyAGI와 같은 오픈 소스 프로젝트를 통해 대중화되었으며, 연구 커뮤니티는 자율 에이전트 기반 시스템 구축을 실험했다[19, 1].

사용자가 개방형 텍스트 필드에 입력하고 추가 입력 없이 결과를 얻는 대규모 언어 모델의 제로 샷 프롬프트와 달리 에이전트는 보다 복잡한 상호 작용 및 오케스트레이션을 허용한다. 특히, 에이전트 시스템은 계획, 루프, 반사 및 다른 제어 구조의 개념을 가지고 있으며, 이는 모델의 고유한 추론 능력을 크게 활용하여 작업 종단 간 작업을 수행한다. 도구, 플러그인 및 기능 호출을 사용할 수 있는 기능과 함께 에이전트는 보다 범용적인 작업을 수행할 수 있는 권한을 부여받습니다.

커뮤니티 중에는 단일 또는 다중 에이전트 시스템이 복잡한 작업을 해결하는 데 가장 적합한지에 대한 현재 논쟁이 있다. 단일 에이전트 아키텍처는 문제가 잘 정의되고 다른 에이전트 퍼소나 사용자의 피드백이 필요하지 않을 때 탁월하지만, 다중 에이전트 아키텍처는 협업과 여러 개의 별개의 실행 경로가 필요할 때 더 번창하는 경향이 있다.

### Taxonomy

**에이전트**. AI 에이전트는 여러 번의 반복을 통해 목표를 실행하기 위해 계획하고 조치를 취할 수 있는 언어 모델 기반 엔티티이다. AI 에이전트 아키텍처는 단일 에이전트 또는 문제를 해결하기 위해 함께 작동하는 여러 에이전트로 구성된다.

일반적으로 각 에이전트에는 독립적으로 또는 팀의 일부로 작업을 수행하는 데 도움이 되는 다양한 도구에 대한 페르소나와 액세스 권한이 부여됩니다. 일부 에이전트에는 메시지 및 프롬프트 외부에 정보를 저장하고 로드할 수 있는 메모리 구성 요소도 포함되어 있습니다. 본 논문에서는 "뇌, 지각, 행동"으로 구성된 에이전트의 정의를 따른다[31]. 이러한 구성 요소는 에이전트가 주변 환경을 이해하고 추론하고 행동하기 위한 최소 요구 사항을 충족한다.

**Agent Persona**. 에이전트 페르소나는 에이전트에 특정한 다른 지침을 포함하여 에이전트가 수행해야 하는 역할 또는 개성을 설명합니다. 개인에는 에이전트가 액세스할 수 있는 모든 도구에 대한 설명도 포함되어 있습니다. 그들은 에이전트에게 자신의 역할, 도구의 목적 및 효과적으로 활용하는 방법을 인식하게 한다. 연구자들은 "모양의 성격은 소셜 미디어 게시물 작성과 같은 공통 다운스트림(즉, 후속) 작업에서 LLM(Large Language Model) 행동에 검증 가능하게 영향을 미친다"는 것을 발견했다[21]. 여러 에이전트 페르소나를 사용하여 문제를 해결하는 솔루션은 또한 모델이 단계별로 계획을 분해하도록 요청되는 CoT(Chain-of-Think)에 비해 상당한 개선을 보여줍니다[28, 29].

**도구**. AI 에이전트의 맥락에서 도구는 모델이 호출할 수 있는 모든 기능을 나타낸다. 에이전트가 해당 소스에 정보를 끌어당기거나 푸시하여 외부 데이터 소스와 상호 작용할 수 있습니다. 에이전트 페르소나 및 관련 도구의 예로는 전문 계약 작성자가 있습니다. 작가는 자신의 역할과 수행해야 할 작업의 유형을 설명하는 페르소나를 부여받는다. 또한 문서에 메모를 추가하거나 기존 문서를 읽거나 최종 초안이 포함된 이메일을 보내는 것과 관련된 도구가 제공됩니다.

**단일 에이전트 아키텍처**. 이러한 아키텍처는 하나의 언어 모델에 의해 구동되며 추론, 계획 및 도구 실행을 모두 자체적으로 수행할 것입니다. 에이전트에는 시스템 프롬프트와 완료하는 데 필요한 모든 도구가 제공됩니다.

그림 1: 단일 및 다중 에이전트 아키텍처의 기본 특징과 능력을 시각화

task. 단일 에이전트 패턴에는 다른 AI 에이전트의 피드백 메커니즘이 없지만 인간이 에이전트를 안내하는 피드백을 제공할 수 있는 옵션이 있을 수 있다.

**다중 에이전트 아키텍처**. 이들 아키텍처는 둘 이상의 에이전트를 포함하며, 여기서 각각의 에이전트는 동일한 언어 모델 또는 상이한 언어 모델의 세트를 이용할 수 있다. 에이전트는 동일한 도구 또는 다른 도구에 액세스할 수 있습니다. 각 에이전트는 일반적으로 고유한 페르소나를 가지고 있습니다.

멀티 에이전트 아키텍처는 모든 수준의 복잡성에서 매우 다양한 조직을 가질 수 있습니다. 본 논문에서는 이를 수직과 수평의 두 가지 주요 범주로 나눈다. 이러한 범주는 스펙트럼의 두 끝을 나타내며, 대부분의 기존 아키텍처는 이 두 극단 사이의 어딘가에 있다는 것을 명심하는 것이 중요하다.

**수직 아키텍처**. 이 구조에서 한 에이전트가 리더 역할을 하고 다른 에이전트가 직접 그들에게 보고하도록 한다. 아키텍처에 따라 보고 에이전트는 리드 에이전트와 독점적으로 통신할 수 있습니다. 대안적으로, 리더는 모든 에이전트들 사이의 공유된 대화로 정의될 수 있다. 수직 아키텍처의 정의 기능은 납 에이전트와 협업 에이전트 간의 명확한 분업화를 포함한다.

**수평 아키텍처**. 이 구조에서 모든 에이전트는 동등하게 취급되며 과제에 대한 한 그룹 토론의 일부이다. 에이전트 간의 통신은 각 에이전트가 다른 에이전트의 모든 메시지를 볼 수 있는 공유 스레드에서 발생합니다. 에이전트는 또한 특정 작업을 완료하거나 도구를 호출하기 위해 자원할 수 있으며, 이는 선두 에이전트에 의해 할당될 필요가 없음을 의미합니다. 수평 아키텍처는 일반적으로 협업, 피드백 및 그룹 토론이 태스크의 전체 성공에 핵심이 되는 태스크에 사용된다[2].

## 2 효과적인 에이전트에 대한 주요 고려 사항

### Overview

에이전트는 실제 문제를 해결하기 위해 언어 모델 기능을 확장하도록 설계되었습니다. 성공적인 구현에는 에이전트가 새로운 작업에 대해 잘 수행할 수 있는 강력한 문제 해결 기능이 필요합니다. 실제 문제를 효과적으로 해결하기 위해 에이전트는 외부 환경과 상호 작용하는 도구를 호출할 뿐만 아니라 추론하고 계획하는 능력이 필요하다. 이 섹션에서는 추론, 계획 및 도구 호출이 에이전트 성공에 중요한 이유를 탐구합니다.

### 추론 및 계획의 중요성

추론은 인간 인식의 기본 구성 요소이며, 사람들이 우리 주변의 세상을 결정하고, 문제를 해결하고, 이해할 수 있게 한다. AI 에이전트는 복잡한 환경과 효과적으로 상호 작용하고 자율적인 결정을 내리고 광범위한 작업에서 인간을 돕는다면 추론할 수 있는 강력한 능력이 필요하다. "행동"과 "추론" 사이의 이러한 빡빡한 시너지는 새로운 태스크들이 빠르게 학습될 수 있게 하고, 이전에 보이지 않았던 상황들 또는 정보 불확실성들 하에서도 강건한 의사 결정 또는 추론을 가능하게 한다[32]. 또한 에이전트는 새로운 피드백이나 학습된 정보를 기반으로 계획을 조정하기 위한 추론이 필요하다.

추론 능력이 부족한 에이전트가 간단한 작업에 대해 작업하는 경우 쿼리를 잘못 해석하거나 문자 그대로의 이해를 기반으로 응답을 생성하거나 다단계 의미를 고려하지 않을 수 있다.

강력한 추론 능력을 요구하는 계획은 일반적으로 과제 분해, 다중 계획 선택, 외부 모듈 지원 계획, 성찰 및 개선 및 기억 증강 계획의 다섯 가지 주요 접근법 중 하나에 속한다[12]. 이러한 접근법을 통해 모델은 태스크를 하위 태스크로 세분화하고, 생성된 많은 옵션에서 하나의 플랜을 선택하거나, 기존 외부 플랜을 활용하거나, 새로운 정보를 기반으로 이전 플랜을 수정하거나, 플랜을 개선하기 위해 외부 정보를 활용할 수 있다.

대부분의 에이전트 패턴들은 임의의 액션들이 실행되기 전에 계획을 생성하기 위해 이들 기법들 중 하나 이상을 호출하는 전용 계획 단계를 갖는다. 예를 들어, Plan Like a Graph(PLaG)는 계획들을 방향 그래프들로서 표현하는 접근법으로서, 다수의 단계들이 병렬로 실행된다[15; 33]. 이는 비동기 실행의 이점을 얻는 많은 독립적인 하위 작업을 포함하는 작업에서 다른 방법에 비해 상당한 성능 증가를 제공할 수 있다.

### 효과적인 도구 호출의 중요성

기본 언어 모델을 프롬프트하는 것보다 에이전트 추상화의 한 가지 주요 이점은 여러 도구를 호출하여 복잡한 문제를 해결하는 에이전트의 능력이다. 이러한 도구를 사용하면 에이전트가 외부 데이터 원본과 상호 작용하거나 기존 API에서 정보를 보내거나 검색할 수 있습니다. 광범위한 도구 호출이 필요한 문제는 복잡한 추론이 필요한 문제와 병행되는 경우가 많다.

단일 에이전트 및 다중 에이전트 아키텍처는 추론 및 도구 호출 단계를 사용하여 어려운 작업을 해결하는 데 사용할 수 있다. 많은 방법들은 문제를 효과적이고 정확하게 완성하기 위해 추론, 기억, 반성의 다중 반복을 사용한다[16; 23; 32]. 그들은 종종 더 큰 문제를 더 작은 하위 문제로 분해한 다음 적절한 도구를 차례로 사용하여 각각을 해결함으로써 이 작업을 수행합니다.

에이전트 패턴을 발전시키는 데 초점을 맞춘 다른 작업은 더 큰 문제를 더 작은 하위 문제로 분해하는 것이 복잡한 작업을 해결하는 데 효과적일 수 있지만 단일 에이전트 패턴은 종종 필요한 긴 시퀀스를 완료하는 데 어려움을 겪는다는 점을 강조한다[22; 6].

다중 에이전트 패턴은 개별 에이전트가 개별 하위 문제에 대해 작업할 수 있기 때문에 병렬 작업과 견고성의 문제를 해결할 수 있다. 많은 다중 에이전트 패턴은 복잡한 문제를 취하여 여러 작은 작업으로 분해하는 것으로 시작한다. 그런 다음 각 에이전트는 자신의 독립적인 도구 세트를 사용하여 각 작업을 독립적으로 해결합니다.

## 3 단일 에이전트 아키텍처

### Overview

이 섹션에서는 ReAct, RAISE, 반사, AutoGPT + P 및 LATS와 같은 몇 가지 주목할 만한 단일 에이전트 방법을 강조한다. 이러한 방법들 각각은 목표를 진전시키기 위해 임의의 액션이 취해지기 전에 문제에 대한 추론을 위한 전용 스테이지를 포함한다. 에이전트의 추론 및 도구 호출 기능에 대한 기여도를 기반으로 이러한 방법을 선택했다.

### Key Themes

우리는 에이전트에 의한 성공적인 목표 실행이 적절한 계획과 자기 수정에 달려 있음을 발견한다[32; 16; 23; 1]. 효과적인 계획을 스스로 평가하고 생성할 수 있는 능력이 없다면, 단일 에이전트는 끝없는 실행 루프에 갇혀서 주어진 작업을 결코 달성하거나 사용자 기대에 미치지 못하는 결과를 반환하지 못할 수 있다[32]. 단일 에이전트 아키텍처는 태스크가 간단한 함수 호출을 필요로 하고 다른 에이전트로부터의 피드백이 필요하지 않을 때 특히 유용하다는 것을 발견한다[22].

### Examples

**ReAct.** ReAct(이유 + 행위) 메서드에서 에이전트는 먼저 지정된 작업에 대해 생각을 씁니다. 그런 다음 해당 사고를 기반으로 작업을 수행하고 출력이 관찰됩니다. 이 사이클은 작업이 완료될 때까지 반복될 수 있다[32]. 다양한 언어 및 의사 결정 작업에 적용할 때 ReAct 방법은 동일한 작업에 대한 제로 샷 프롬프트에 비해 향상된 효과를 보여준다. 또한 모델의 전체 사고 과정이 기록되기 때문에 향상된 인간 상호 운용성과 신뢰성을 제공합니다. HotpotQA 데이터셋에서 평가했을 때 ReAct 방법은 CoT(Chain of Thinking) 방법을 사용한 14%에 비해 6%의 시간만을 환각시켰다[29; 32].

그러나 ReAct 방식은 그 한계가 없는 것은 아니다. 추론, 관찰 및 행동이 얽혀 신뢰성이 향상되지만 모델은 동일한 생각과 행동을 반복적으로 생성하고 새로운 생각을 생성하지 못하여 작업을 끝내고 ReAct 루프를 빠져나갈 수 있다. 과제의 실행 동안 인간 피드백을 통합하면 실제 시나리오에서 효과성과 적용 가능성이 증가할 수 있다.

**RAISE.** RAISE 메서드는 ReAct 메서드를 기반으로 하며 인간의 단기 및 장기 기억을 미러링 하는 메모리 메커니즘을 추가 합니다 [16]. 이는 단기 저장을 위한 스크래치 패드와 장기 저장을 위한 유사한 이전 예제의 데이터 세트를 사용하여 수행한다.

이러한 컴포넌트를 추가함으로써, RAISE는 더 긴 대화에서 컨텍스트를 유지하는 에이전트의 능력을 향상시킨다. 또한 더 작은 모델을 사용하는 경우에도 모델을 미세 조정하면 작업에 가장 좋은 성능을 얻을 수 있음을 강조합니다. 그들은 또한 RAISE가 효율성과 출력 품질 모두에서 ReAct보다 우수하다는 것을 보여주었다.

RAISE는 일부 측면에서 기존 방법을 크게 개선하지만 연구진도 몇 가지 문제를 강조했다. 첫째, RAISE는 복잡한 논리를 이해하기 위해 고군분투하여 많은 시나리오에서 유용성을 제한한다. 또한, RAISE 에이전트는 종종 그들의 역할이나 지식과 관련하여 환각을 본다. 예를 들어, 명확하게 정의된 역할이 없는 판매 에이전트는 파이썬에서 코드를 작성하는 기능을 유지할 수 있으며, 이를 통해 판매 작업에 초점을 맞추는 대신 파이썬 코드 작성을 시작할 수 있습니다. 이러한 에이전트는 사용자에게 오판의 소지가 있거나 잘못된 정보를 제공할 수도 있습니다. 이 문제는 모델을 미세 조정함으로써 해결되었지만 연구자들은 여전히 RAISE 구현의 한계로 환각을 강조했다.

**반사** 반사는 언어적 피드백을 통한 자기 반성을 사용하는 단일 에이전트 패턴입니다 [23]. 성공 상태, 현재 궤적 및 영구 기억과 같은 메트릭을 활용하여 이 방법은 LLM 평가기를 사용하여 에이전트에 구체적이고 관련된 피드백을 제공한다. 이것은 연쇄 사상 및 ReAct에 비해 향상된 성공률과 감소된 환각을 초래한다.

이러한 발전에도 불구하고 반사 저자는 패턴의 다양한 한계를 식별한다. 주로, 반사는 "비최적 국소 최소 솔루션"에 취약하다. 또한 데이터베이스가 아닌 장기 기억을 위한 슬라이딩 창을 사용합니다. 이는 언어 모델의 토큰 한도에 의해 장기 기억의 부피가 제한된다는 것을 의미한다. 마지막으로, 연구원들은 반사가 다른 단일 에이전트 패턴을 능가하는 반면, 상당한 양의 다양성, 탐색 및 추론이 필요한 작업에 대한 성능을 향상시킬 수 있는 기회가 여전히 있음을 식별한다.

**AUTOGPT + P.** AutoGPT + P(계획)는 자연어로 로봇을 명령하는 에이전트의 추론 제한을 해결하는 방법입니다 [1]. AutoGPT+P는 객체 검출 및 객체 어포던스 매핑(OAM)을 LLM에 의해 구동되는 계획 시스템과 결합한다. 이를 통해 에이전트는 누락된 객체에 대한 환경을 탐색하거나 대안을 제안하거나 사용자에게 목표에 도달하는 데 도움을 요청할 수 있다.

도 3 : RAISE 방법을 나타낸 도면[16]

도 2: 다른 방법들과 비교한 ReAct 방법의 예[32]

AutoGPT+P는 존재하는 객체들을 검출하기 위해 장면의 이미지를 사용함으로써 시작한다. 그런 다음 언어 모델은 이러한 개체를 사용하여 계획 도구, 부분 계획 도구, 대안 도구 제안 및 탐색 도구의 네 가지 옵션에서 사용할 도구를 선택합니다. 이러한 도구를 통해 로봇은 목표를 완수하기 위한 전체 계획을 생성할 뿐만 아니라 환경을 탐색하고 가정을 하고 부분 계획을 생성할 수 있다.

그러나 언어 모델은 계획을 완전히 자체적으로 생성하지 않습니다. 대신, 계획 도메인 정의 언어(PDDL)를 사용하여 계획을 실행하는 고전적인 계획자를 제외하고 작업하기 위한 목표와 단계를 생성합니다. 논문은 "LLM은 현재 주로 제한된 추론 능력으로 인해 자연 언어 명령어를 로봇 작업을 실행하기 위한 계획으로 직접 번역하는 능력이 부족하다"는 것을 발견했다[1]. LLM 계획 기능을 고전적 계획기와 결합함으로써, 그들의 접근법은 로봇 계획에 대한 다른 순수 언어 모델 기반 접근법에 비해 상당히 개선된다.

대부분의 첫 번째 접근법과 마찬가지로 AutoGPT+P도 단점이 없는 것은 아니다. 도구 선택의 정확도는 다양하며, 특정 도구가 부적절하게 호출되거나 루프에 갇히게 됩니다. 탐사가 필요한 시나리오에서 도구 선택은 때때로 잘못된 장소에서 객체를 찾는 것과 같은 비논리적인 탐색 결정으로 이어진다. 프레임워크는 또한 에이전트가 설명을 구할 수 없고 사용자가 실행 중에 계획을 수정하거나 종료할 수 없는 인간 상호작용의 측면에서 제한된다.

**LATS.** LATS(언어 에이전트 트리 검색)는 트리를 사용하여 계획, 행동 및 추론을 시너지화하는 단일 에이전트 방법입니다[36]. 몬테카를로 트리 탐색(Monte Carlo Tree Search)에서 영감을 받은 이 기법은 노드로서 상태를 나타내고 노드들 사이를 횡단하는 것으로서 액션을 취한다. LM 기반 휴리스틱을 사용하여 가능한 옵션을 검색한 다음 상태 평가기를 사용하여 액션을 선택합니다.

LATS는 다른 트리 기반 방법과 비교할 때 성능을 획기적으로 향상시키는 자기 성찰 추론 단계를 구현한다. 어떤 행동을 취하면 환경 피드백과 언어 모델의 피드백을 모두 사용하여 추론에 오류가 있는지 확인하고 대안을 제안한다. 강력한 탐색 알고리즘과 결합된 자기 반영 능력은 LATS가 다양한 작업에서 매우 잘 수행되도록 한다.

그러나, 알고리즘의 복잡성과 관련된 반사 단계들로 인해, LATS는 종종 다른 단일-에이전트 방법들보다 더 많은 계산 자원을 사용하고 완료하는데 더 많은 시간이 걸린다[36]. 이 논문은 또한 비교적 간단한 질의 응답 벤치마크를 사용하며 도구 호출 또는 복잡한 추론을 포함하는 보다 강력한 시나리오에 대해 테스트되지 않았다.

## 4 다중 에이전트 아키텍처

### Overview

이 섹션에서는 Embodied LLM 에이전트 Learn to Cooperate in Organized Teams, DyLAN, AgentVerse 및 MetaGPT와 같은 다중 에이전트 아키텍처를 사용하는 몇 가지 주요 연구 및 샘플 프레임워크를 조사합니다. 이러한 구현들이 에이전트 간 통신 및 협력적 계획 실행을 통해 목표 실행을 용이하게 하는 방법을 강조한다. 이것은 모든 에이전트 프레임워크의 포괄적인 목록이 아니라, 우리의 목표는 다중 에이전트 패턴과 관련된 주요 주제 및 예제에 대한 광범위한 범위를 제공하는 것이다.

### Key Themes

멀티 에이전트 아키텍처는 기술을 기반으로 하는 지능적인 분업과 다양한 에이전트 페르소나의 도움이 되는 피드백 모두를 위한 기회를 만든다. 많은 멀티 에이전트 아키텍처가 에이전트 팀이 작업하는 단계에서 작동합니다.

도 4 : AutoGPT+P 방식 [1]의 도면

계획, 실행 및 평가 단계마다 동적으로 생성 및 재구성합니다. [2, 9, 18]. 이러한 개편은 특정 업무에 전문 에이전트를 채용하고 더 이상 필요하지 않을 때 제거되기 때문에 우수한 결과를 제공한다. 에이전트 역할 및 기술을 당면한 작업에 일치시킴으로써 에이전트 팀은 목표를 달성하는 데 더 큰 정확도를 달성하고 시간을 단축할 수 있습니다. 효과적인 멀티 에이전트 아키텍처의 주요 특징은 에이전트 팀의 명확한 리더십, 동적 팀 구성, 그리고 불필요한 수다에서 중요한 정보가 손실되지 않도록 팀 구성원 간의 효과적인 정보 공유이다.

### Examples

**구현된 LLM 에이전트는 조직화된 팀에서 협력하는 방법을 배웁니다.* * Guo 등의 연구는 에이전트 팀의 전반적인 효과에 대한 납 에이전트의 영향을 보여줍니다. [9]. 이 아키텍처는 리더 에이전트를 통한 수직 구성 요소뿐만 아니라 에이전트가 리더 외에 다른 에이전트와 대화할 수 있는 능력으로부터 수평 구성 요소를 포함한다. 그들의 연구 결과는 리더가 조직된 에이전트 팀이 리더가 없는 팀보다 거의 10% 더 빠르게 작업을 완료한다는 것을 보여준다.

또한 지정된 리더가 없는 팀에서 에이전트는 대부분의 시간을 서로 명령하는 데(의사소통의 50%) 시간을 보내고 나머지 시간을 정보를 공유하는 데 또는 지침을 요청하는 데 사용한다는 것을 발견했다. 반대로, 지정된 리더가 있는 팀에서는 리더의 커뮤니케이션의 60%가 지시를 내리는 것과 관련되어 다른 구성원들이 정보를 교환하고 요청하는 데 더 집중하도록 유도했다. 그들의 결과는 리더가 인간일 때 에이전트 팀이 가장 효과적이라는 것을 보여준다.

이 논문은 팀 구성을 넘어 계획 수립, 성과 평가, 피드백 제공, 팀 재편성을 위해 "비판-반성" 단계를 채택하는 것이 중요하다고 강조한다[9]. 그들의 결과는 회전 리더십을 가진 역동적인 팀 구조를 가진 에이전트가 작업 완료까지의 시간이 가장 낮고 평균 통신 비용이 가장 낮은 최상의 결과를 제공한다는 것을 나타낸다. 궁극적으로 리더십과 역동적인 팀 구조는 팀 전체의 업무 추론, 계획, 수행 능력을 효과적으로 향상시킨다.

**DyLAN.** DyLAN(Dynamic LLM-Agent Network) 프레임워크는 추론 및 코드 생성과 같은 복잡한 작업에 초점을 맞춘 동적 에이전트 구조를 만듭니다[18]. DyLAN은 각 에이전트가 마지막 작업 라운드에서 얼마나 기여했는지 결정하기 위한 특정 단계를 가지며 다음 실행 라운드에서 일등 기여자만 이동시킨다. 이 방법은 에이전트가 서로 정보를 공유할 수 있고 정의된 리더가 없기 때문에 본질적으로 수평적이다. DyLAN은 산술 및 일반적인 추론 능력을 측정하는 다양한 벤치마크에서 향상된 성능을 보여준다. 이는 동적 팀의 영향을 강조하고 에이전트 기여도를 지속적으로 재평가하고 순위를 매김으로써 주어진 작업을 완료하는 데 더 적합한 에이전트 팀을 만들 수 있음을 보여준다.

**AgentVerse.** AgentVerse와 같은 다중 에이전트 아키텍처는 그룹 계획을 위한 별개의 단계가 AI 에이전트의 추론 및 문제 해결 기능을 개선할 수 있는 방법을 보여줍니다. [2]. AgentVerse에는 직무 수행을 위한 4개의 기본 단계, 즉 모집, 협력적 의사 결정, 독립적 행동 수행, 평가가 포함되어 있다. 이는 전체적인 목표가 달성될 때까지 반복될 수 있다. AgentVerse는 각 단계를 엄격하게 정의하여 에이전트 집합을 보다 효과적으로 추론하고 토론하고 실행할 수 있도록 안내합니다.

예로서, 모집 단계는 목표를 향한 진행에 기초하여 에이전트가 제거되거나 추가될 수 있게 한다. 이는 주어진 문제 해결 단계에서 올바른 에이전트가 참여하도록 하는 데 도움이 됩니다. 연구진은 일반적으로 수평 팀은 컨설팅과 같은 협력 작업에 가장 적합하고 수직 팀은 도구 호출에 대한 책임을 더 명확하게 분리해야 하는 작업에 더 적합하다는 것을 발견했다.

그림 5: 지정된 리더를 가진 에이전트 팀이 우수한 성능을 달성함[9]

**MetaGPT.** 많은 다중 에이전트 아키텍처를 통해 공통 문제에 대해 협력하는 동안 에이전트가 서로 대화할 수 있습니다. 이러한 대화 능력은 불필요하고 팀 목표를 더 이상 달성하지 못하는 에이전트 간의 수다로 이어질 수 있다. MetaGPT는 에이전트가 비정형 채팅 메시지를 공유하는 대신 문서 및 다이어그램과 같은 구조화된 출력을 생성하도록 요구함으로써 에이전트 간의 비생산적인 채팅 문제를 해결한다[11].

또한, MetaGPT는 정보 공유를 위한 "출판-구독" 메커니즘을 구현한다. 이를 통해 모든 에이전트가 한 곳에서 정보를 공유할 수 있지만 개별 목표 및 과제와 관련된 정보만 읽을 수 있다. 이렇게 하면 전체 목표 실행이 간소화되고 에이전트 간의 대화 소음이 줄어듭니다. HumanEval 및 MBPP 벤치마크의 단일 에이전트 아키텍처와 비교할 때, MetaGPT의 다중 에이전트 아키텍처는 훨씬 더 나은 결과를 보여준다.

## 5 토의 및 관찰

### Overview

이 섹션에서는 이전에 요약된 에이전트 패턴에 나타난 디자인 선택의 주요 주제와 영향에 대해 논의한다. 이러한 패턴은 AI 에이전트 아키텍처의 연구 및 구현의 증가하는 본문의 핵심 사례 역할을 한다. 단일 및 다중 에이전트 아키텍처 모두 인간 사용자를 대신하여 또는 그와 함께 목표를 실행할 수 있는 능력을 제공함으로써 언어 모델의 능력을 향상시키려고 한다. 관찰된 대부분의 에이전트 구현은 문제를 반복적으로 해결하기 위해 계획을 광범위하게 따르고, 행동하고, 프로세스를 평가한다.

단일 및 다중 에이전트 아키텍처 모두 복잡한 목표 실행에서 강력한 성능을 보여준다. 또한 아키텍처 전반에 걸쳐 명확한 피드백, 작업 분해, 반복 정제 및 역할 정의가 에이전트 성능을 향상시킨다는 것을 발견했다.

### Key Findings

**단일 및 다중 에이전트 아키텍처 선택을 위한 일반적인 조건** 위에서 언급한 에이전트 패턴을 기반으로 단일 에이전트 패턴이 일반적으로 좁게 정의된 도구 목록과 프로세스가 잘 정의된 작업에 가장 적합하다는 것을 발견했습니다. 단일 에이전트는 일반적으로 하나의 에이전트 및 도구 세트만 정의하면 되기 때문에 구현하기가 더 쉽습니다. 또한 단일 에이전트 아키텍처는 다른 에이전트의 열악한 피드백이나 다른 팀 구성원의 산만하고 관련 없는 수다와 같은 한계에 직면하지 않는다. 그러나 추론 및 개선 기능이 견고하지 않으면 실행 루프에 갇혀 목표를 향해 나아가지 못할 수 있다.

도 6: AgentVerse 방법의 다이어그램 [2] 다중-에이전트 아키텍처는 일반적으로 다수의 페르소나로부터의 피드백이 태스크를 달성하는 데 유익한 태스크에 적합하다. 예를 들어, 문서 생성은 하나의 에이전트가 문서의 기입된 섹션 상에서 다른 에이전트에 명확한 피드백을 제공하는 멀티-에이전트 아키텍처로부터 이익을 얻을 수 있다. 다중 에이전트 시스템은 별개의 작업 또는 워크플로우에 걸친 병렬화가 필요한 경우에도 유용합니다. 결정적으로 Wang et. al은 어떤 예도 제공되지 않을 때 시나리오들에서 멀티-에이전트 패턴들이 단일 에이전트들보다 더 잘 수행한다는 것을 발견한다[26]. 본질적으로, 다중 에이전트 시스템은 더 복잡하고 종종 강력한 대화 관리 및 명확한 리더십으로부터 이익을 얻는다.

단일 및 다중-에이전트 패턴은 범위 측면에서 발산 능력을 갖는 반면, 연구는 "다중-에이전트 논의가 에이전트에 제공된 프롬프트가 충분히 견고할 때 추론을 반드시 향상시키는 것은 아니다"라는 것을 발견한다[26]. 이것은 에이전트 아키텍처를 구현하는 사람들이 필요한 추론 능력을 기반으로 하지 않고 사용 사례의 광범위한 컨텍스트에 따라 단일 또는 다중 에이전트 사이에서 결정해야 함을 시사한다.

**에이전트 및 비동기 작업 실행** 단일 에이전트가 여러 비동기 호출을 동시에 시작할 수 있지만 운영 모델은 본질적으로 다른 실행 스레드 간의 책임 분할을 지원하지 않습니다. 이는 업무가 비동기적으로 처리되지만, 별개의 의사결정 주체에 의해 자율적으로 관리된다는 의미에서 진정으로 병렬적이지 않다는 것을 의미한다. 대신 단일 에이전트는 작업을 순차적으로 계획하고 실행해야 하며, 평가 및 다음 단계로 넘어가기 전에 비동기 작업의 한 배치가 완료되기를 기다려야 합니다. 반대로 멀티 에이전트 아키텍처에서는 각 에이전트가 독립적으로 동작할 수 있어 보다 역동적인 분업이 가능하다. 이 구조는 상이한 도메인들 또는 목표들에 걸쳐 동시 태스크 실행을 용이하게 할 뿐만 아니라, 개별 에이전트들이 다른 사람들에 의해 처리되는 태스크들의 상태에 방해받지 않고 그들의 다음 단계들을 진행할 수 있게 하여, 태스크 관리에 대한 보다 유연하고 병렬적인 접근법을 구현한다.

**피드백 및 인간 감독이 에이전트 시스템에 미치는 영향** 복잡한 문제를 해결할 때 첫 번째 시도에서 정확하고 강력한 솔루션을 제공할 가능성은 매우 낮습니다. 대신에, 그것을 비판하고 다듬기 전에 잠재적인 해결책을 제시할 수 있다. 다른 사람과 상의하고 다른 관점에서 피드백을 받을 수도 있습니다. 반복 피드백과 정제에 대한 동일한 아이디어는 에이전트가 복잡한 문제를 해결하는 데 필수적이다.

이는 언어 모델이 응답 초기에 답변에 전념하는 경향이 있기 때문인데, 이는 목표 상태로부터 주의를 분산시키는 '눈덩이 효과'를 야기할 수 있다[34]. 피드백을 구현함으로써 에이전트는 경로를 수정하고 목표에 도달할 가능성이 훨씬 더 높다.

또한, 인간 감독의 포함은 에이전트의 응답을 인간의 기대와 더 밀접하게 정렬하여 에이전트가 과제 해결에 대한 비효율적이거나 무효한 접근법을 조사할 가능성을 완화함으로써 즉각적인 결과를 향상시킨다. 오늘날과 같이, 에이전트 아키텍처에 인간 검증 및 피드백을 포함하는 것은 보다 신뢰성 있고 신뢰할 수 있는 결과를 산출한다[4, 9].

언어 모델은 또한 "공정하거나 균형 잡힌 시점의 제시를 포기하는 것을 의미하더라도 사용자의 입장을 반영하는 경향이 있는 아첨적 행동을 나타낸다" [20]. 특히, AgentVerse 논문은 피드백이 건전하지 않더라도 에이전트가 다른 에이전트의 피드백에 어떻게 민감한지를 설명한다. 이렇게 하면 에이전트 팀이 목표 [2]에서 잘못 된 계획을 생성할 수 있습니다. 강력한 프롬프트는 이를 완화하는 데 도움이 될 수 있지만, 개발 중인 에이전트 애플리케이션은 사용자 또는 에이전트 피드백 시스템을 구현할 때 위험을 인식해야 한다.

**그룹 대화 및 정보 공유에 대한 문제** 다중 에이전트 아키텍처의 한 가지 문제는 에이전트 간에 메시지를 지능적으로 공유할 수 있는 능력에 있습니다. 다중 에이전트 패턴은 선동에 휘말려 "어떻게 지내?"와 같은 질문을 서로에게 던지는 경향이 더 큰 반면, 단일 에이전트 패턴은 관리할 팀 역학이 없기 때문에 당면한 작업에 집중하는 경향이 있다. 다중 에이전트 시스템의 외부 대화는 에이전트가 효과적으로 추론하고 올바른 도구를 실행하는 능력을 모두 손상시켜 궁극적으로 에이전트를 작업에서 분산시키고 팀 효율성을 저하시킬 수 있다. 이것은 수평 아키텍처에서 특히 사실이며, 여기서 에이전트는 일반적으로 그룹 채팅을 공유하고 대화에서 모든 에이전트의 메시지에 관여한다. 메시지 구독 또는 필터링은 에이전트가 작업과 관련된 정보만 수신하도록 보장하여 다중 에이전트 성능을 향상시킵니다.

수직 아키텍처에서 작업은 팀 내 산만함을 줄이는 데 도움이 되는 에이전트 기술에 의해 명확하게 구분되는 경향이 있습니다. 그러나 선두 에이전트가 중요한 정보를 지원 에이전트에 전송하지 못하고 다른 에이전트가 필요한 정보를 알지 못하는 경우 문제가 발생한다. 이러한 실패는 팀의 혼란이나 결과의 환각을 초래할 수 있다. 이 문제를 해결하기 위한 한 가지 접근법은 에이전트가 상황적으로 적절한 상호 작용을 갖도록 시스템 프롬프트에 액세스 권한에 대한 정보를 명시적으로 포함하는 것이다.

**역할 정의 및 동적 팀의 영향** 명확한 역할 정의는 단일 및 다중 에이전트 아키텍처 모두에 중요 합니다. 단일 에이전트 아키텍처에서 역할 정의는 에이전트가 제공된 작업에 집중하고 적절한 도구를 실행하며 다른 기능의 환각을 최소화하도록 합니다. 유사하게, 다중 에이전트 아키텍처에서의 역할 정의는 각 에이전트가 전체 팀에서 담당하는 것을 알고 설명된 기능이나 범위를 벗어난 작업을 수행하지 않도록 합니다. 개인의 역할 정의를 넘어 명확한 그룹 리더를 확립하는 것도 업무 할당을 간소화하여 다중 에이전트 팀의 전반적인 성과를 향상시킨다. 또한 각 에이전트에 대해 명확한 시스템 프롬프트를 정의하면 에이전트가 비생산적인 커뮤니케이션에 참여하지 않도록 프롬프트함으로써 과도한 채팅을 최소화할 수 있다.

요원들이 필요에 따라 시스템에 들어오고 나가는 역동적인 팀도 효과적인 것으로 나타났다. 이렇게 하면 작업의 계획 또는 실행에 참여하는 모든 에이전트가 해당 작업에 적합합니다.

### Summary

단일 및 다중 에이전트 패턴 모두 추론 및 도구 실행과 관련된 다양한 복잡한 작업에서 강력한 성능을 나타낸다. 단일 에이전트 패턴은 정의된 페르소나 및 도구 세트, 인간 피드백 기회, 목표를 향해 반복적으로 작업할 수 있는 능력이 주어졌을 때 잘 수행된다. 복잡한 목표에 협력해야 하는 에이전트 팀을 구성할 때, 명확한 리더(들), 정의된 계획 단계 및 새로운 정보가 학습됨에 따라 계획을 구체화할 기회, 지능형 메시지 필터링, 에이전트가 현재 하위 작업과 관련된 특정 기술을 소유한 동적 팀 중 적어도 하나를 가진 에이전트를 배치하는 것이 유익하다. 에이전트 아키텍처가 이들 접근법들 중 적어도 하나를 채용하는 경우, 이는 이러한 전술들이 없는 단일 에이전트 아키텍처 또는 멀티-에이전트 아키텍처에 비해 증가된 성능을 초래할 가능성이 있다.

## 6 현재 연구의 한계 및 미래 연구를 위한 고려 사항

### Overview

이 절에서는 오늘날 에이전트 연구의 몇 가지 한계를 살펴보고 AI 에이전트 시스템을 개선하기 위한 잠재적 영역을 식별한다. 에이전트 아키텍처는 여러 면에서 언어 모델의 능력을 크게 향상시켰지만, 각 에이전트에 전원을 공급하는 언어 모델에서 상속되는 평가, 전체 신뢰성 및 문제에는 몇 가지 주요 문제가 있다.

### 에이전트 평가 문제

LLM은 일반적인 이해 및 추론 능력을 측정하기 위해 설계된 표준 벤치마크 세트에서 평가되지만 에이전트 평가를 위한 벤치마크는 크게 다르다.

많은 연구 팀은 에이전트 구현과 함께 고유한 에이전트 벤치마크를 도입하여 동일한 벤치마크에서 여러 에이전트 구현을 비교하는 것이 어렵다. 또한 이러한 새로운 에이전트별 벤치마크에는 결과가 수동으로 채점되는 손으로 만든 매우 복잡한 평가 세트가 포함된다[2]. 이것은 방법의 능력에 대한 고품질 평가를 제공할 수 있지만 방법을 개발하는 것도 결과를 작성하고 채점하는 것이기 때문에 더 큰 데이터 세트의 견고성이 부족하고 평가에 편향을 도입할 위험이 있다. 에이전트는 모델, 환경 또는 문제 상태의 변동성으로 인해 여러 반복에 걸쳐 일관된 답변을 생성하는 문제를 가질 수도 있습니다. 이 추가된 무작위성은 더 작고 복잡한 평가 세트에 훨씬 더 큰 문제를 제기한다.

### 데이터 오염 및 정적 벤치마크의 영향

일부 연구자들은 일반적인 LLM 벤치마크에서 에이전트 구현을 평가한다. 신흥 연구는 벤치마크 질문이 수정될 때 모델의 성능이 크게 악화된다는 관찰에 의해 뒷받침되는 모델의 훈련 데이터에 상당한 데이터 오염이 있음을 나타낸다[8; 38; 37]. 이는 언어 모델과 언어 모델 동력 에이전트 모두에 대한 벤치마크 점수의 진위에 대한 의구심을 불러일으킨다.

또한 연구자들은 "LLM이 빠른 속도로 진행됨에 따라 기존 벤치마크의 복잡성 수준이 일반적으로 정적이고 고정적이기 때문에 기존 데이터 세트가 모델의 계속 진화하는 능력과 일치하지 않는다"는 것을 발견했다[37]. 이를 해결하기 위해 간단한 암기에 강한 동적 벤치마크를 만드는 작업이 이루어졌다[38; 37]. 연구자들은 또한 사용자의 특정 환경 또는 사용 사례에 기초하여 완전히 합성 벤치마크를 생성하는 아이디어를 탐구했다[14; 27]. 이러한 기술은 오염에 도움이 될 수 있지만 인간의 관여 수준을 낮추면 정확성과 문제 해결 능력과 관련하여 추가 위험이 발생할 수 있다.

### 벤치마크 범위 및 전송 가능성

많은 언어 모델 벤치마크는 MMLU 또는 GSM8K[3; 10]와 같은 도구 호출 없이 단일 반복으로 해결되도록 설계된다. 이들은 기본 언어 모델의 능력을 측정하는 데 중요하지만, 에이전트 시스템이 여러 단계에 걸쳐 추론하거나 외부 정보에 접근할 수 있는 능력을 고려하지 않기 때문에 에이전트 능력에 대한 좋은 프록시가 아니다. StrategyQA는 여러 단계에 걸쳐 모델의 추론 능력을 평가함으로써 이를 개선하지만, 답은 예/아니오 응답으로 제한된다[7]. 업계가 에이전트 중심의 사용 사례로 계속 선회함에 따라 에이전트의 성능과 일반화 가능성을 훈련 데이터를 넘어 확장되는 도구와 관련된 작업에 더 잘 평가하기 위한 추가 조치가 필요할 것이다.

AgentBench와 같은 일부 에이전트 특정 벤치마크는 웹 브라우징, 명령줄 인터페이스 및 비디오 게임과 같은 다양한 다른 환경에서 언어 모델 기반 에이전트를 평가한다[17]. 이것은 에이전트가 주어진 작업을 달성하기 위해 추론, 계획 및 호출 도구를 통해 새로운 환경에 얼마나 잘 일반화할 수 있는지에 대한 더 나은 표시를 제공한다. AgentBench 및 SmartPlay와 같은 벤치마크는 구현의 성공률, 인간 응답과의 출력 유사성 및 전체 효율성을 평가하기 위해 설계된 객관적인 평가 메트릭을 도입한다[17; 30]. 이러한 객관적인 척도는 구현의 전반적인 신뢰성과 정확성을 이해하는 데 중요하지만, 성능에 대한 보다 미묘한 또는 주관적인 척도를 고려하는 것도 중요하다. 도구 사용의 효율성, 신뢰성 및 계획의 견고성과 같은 메트릭은 성공률만큼 중요하지만 측정하기가 훨씬 더 어렵다. 이러한 메트릭 중 다수는 인간 전문가의 평가를 필요로 하며, 이는 LLM-as-Judge 평가에 비해 비용이 많이 들고 시간이 많이 소요될 수 있다.

### Real-world Applicability

기존의 많은 벤치마크들은 논리 퍼즐이나 비디오 게임을 통해 추론하는 에이전트 시스템의 능력에 초점을 맞추고 있다[17]. 이러한 유형의 태스크에 대한 성능을 평가하는 것이 에이전트 시스템의 추론 능력을 감지하는 데 도움이 될 수 있지만, 이러한 벤치마크에 대한 성능이 실제 성능으로 변환되는지 여부는 불분명하다. 특히, 실제 데이터는 시끄러울 수 있고 많은 일반적인 벤치마크가 부족한 훨씬 더 넓은 범위의 주제를 다룰 수 있다.

실제 데이터를 사용하는 인기 있는 벤치마크 중 하나는 ChatGPT[35]와 57만 개의 실제 대화의 WildChat 데이터 세트에서 가져온 WildBench이다. 이 때문에 엄청난 범위의 작업과 프롬프트를 포함합니다. 와일드벤치는 광범위한 주제를 다루지만 대부분의 다른 실제 벤치마크는 특정 작업에 중점을 둡니다. 예를 들어, SWE-bench는 Python의 소프트웨어 엔지니어링 작업을 위해 GitHub에서 제기된 일련의 실제 문제를 사용하는 벤치마크이다[13]. 이것은 파이썬 코드를 작성하도록 설계된 에이전트를 평가할 때 매우 도움이 될 수 있고 에이전트가 코드 관련 문제에 대해 얼마나 잘 추론할 수 있는지에 대한 감각을 제공할 수 있지만 다른 프로그래밍 언어와 관련된 에이전트 기능을 이해하려고 할 때는 덜 유익하다.

### 에이전트 시스템의 편향성 및 공정성

언어 모델은 평가 측면뿐만 아니라 사회적 또는 공정성 측면에서 모두 편향을 나타내는 것으로 알려져 있다[5]. 더욱이, 에이전트는 특히 "덜 견고하고, 더 유해한 행동에 취약하며, LLM보다 스텔시어 콘텐츠를 생성할 수 있어, 중요한 안전 문제를 강조하는" 것으로 나타났다[25]. 다른 연구에서는 "LLM 에이전트가 특정 정치적 관점에서 토론을 지향함에도 불구하고 모델의 고유한 사회적 편향을 따르는 경향"을 발견했다[24]. 이러한 경향은 에이전트 기반 구현에서 잘못된 추론으로 이어질 수 있다.

업무의 복잡성과 에이전트 개입이 증가함에 따라 이러한 시스템 내에서 편향을 식별하고 해결하기 위한 더 많은 연구가 필요하다. 확장 가능하고 새로운 벤치마크는 종종 생성 중 일정 수준의 LLM 관여를 포함하기 때문에 이는 연구자에게 매우 큰 도전을 제기한다. 그러나 LLM 기반 에이전트의 편향을 평가하기 위한 진정으로 강력한 벤치마크에는 인간 평가가 포함되어야 한다.

## 7 결론 및 미래 방향

이 조사에서 탐색된 AI 에이전트 구현은 언어 모델 기반 추론, 계획 및 도구 호출의 빠른 향상을 보여준다. 단일 및 다중 에이전트 패턴은 모두 고급 문제 해결 기술이 필요한 복잡한 다단계 문제를 해결할 수 있는 능력을 보여준다. 본 논문에서 논의한 주요 통찰은 최상의 에이전트 아키텍처가 사용 사례에 따라 달라진다는 것을 시사한다. 선택된 아키텍처에 관계없이, 최상의 수행 에이전트 시스템들은 다음의 접근법들 중 적어도 하나를 통합하는 경향이 있다 : 잘 정의된 시스템 프롬프트들, 명확한 리더십 및 태스크 분할, 전용 추론/계획-실행-평가 단계들, 동적 팀 구조들, 인간 또는 에이전트 피드백, 및 지능형 메시지 필터링. 이러한 기술을 활용하는 아키텍처는 다양한 벤치마크 및 문제 유형에 걸쳐 더 효과적입니다.

AI 기반 에이전트의 현재 상태는 유망하지만 주목할 만한 한계와 향후 개선 분야가 있다. 포괄적인 에이전트 벤치마크, 실제 적용 가능성 및 유해한 언어 모델 편향의 완화에 대한 문제는 신뢰할 수 있는 에이전트를 가능하게 하기 위해 가까운 시일 내에 해결해야 한다. 이 조사는 정적 언어 모델에서 보다 역동적이고 자율적인 에이전트로의 진행을 조사함으로써 현재 AI 에이전트 경관에 대한 전체적인 이해를 제공하고 기존 에이전트 아키텍처로 구축하거나 사용자 지정 에이전트 아키텍처를 개발하는 것에 대한 통찰력을 제공하는 것을 목표로 한다.

## References

* [1] Timo Birr et al. _AutoGPT+P: Affordance-based Task Planning with Large Language Models_. arXiv:2402.10778 [cs] version: 1. Feb. 2024. url: [http://arxiv.org/abs/2402.10778](http://arxiv.org/abs/2402.10778).
* [2] Weize Chen et al. _AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors_. arXiv:2308.10848 [cs]. Oct. 2023. url: [http://arxiv.org/abs/2308.10848](http://arxiv.org/abs/2308.10848).
* [3] Karl Cobbe et al. _Training Verifiers to Solve Math Word Problems_. arXiv:2110.14168 [cs]. Nov. 2021. url: [http://arxiv.org/abs/2110.14168](http://arxiv.org/abs/2110.14168).
* [4] Xueyang Feng et al. _Large Language Model-based Human-Agent Collaboration for Complex Task Solving_. 2024. arXiv: 2402.12914 [cs.CL].
* [5] Isabel O. Gallegos et al. _Bias and Fairness in Large Language Models: A Survey_. arXiv:2309.00770 [cs]. Mar. 2024. url: [http://arxiv.org/abs/2309.00770](http://arxiv.org/abs/2309.00770).
* [6] Silin Gao et al. _Efficient Tool Use with Chain-of-Abstraction Reasoning_. arXiv:2401.17464 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2401.17464](http://arxiv.org/abs/2401.17464).
* [7] Mor Geva et al. _Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies_. arXiv:2101.02235 [cs]. Jan. 2021. url: [http://arxiv.org/abs/2101.02235](http://arxiv.org/abs/2101.02235).
* [8] Shahriar Golchin and Mihai Surdeanu. _Time Travel in LLMs: Tracing Data Contamination in Large Language Models_. arXiv:2308.08493 [cs] version: 3. Feb. 2024. url: [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493).
* [9] Xudong Guo et al. _Embodied LLM Agents Learn to Cooperate in Organized Teams_. 2024. arXiv: 2403.12482 [cs.AI].
* [10] Dan Hendrycks et al. _Measuring Massive Multitask Language Understanding_. arXiv:2009.03300 [cs]. Jan. 2021. url: [http://arxiv.org/abs/2009.03300](http://arxiv.org/abs/2009.03300).
* [11] Sirui Hong et al. _MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework_. 2023. arXiv: 2308.00352 [cs.AI].
* [12] Xu Huang et al. _Understanding the planning of LLM agents: A survey_. 2024. arXiv: 2402.02716 [cs.AI].
* [13] Carlos E. Jimenez et al. _SWE-bench: Can Language Models Resolve Real-World GitHub Issues?_ arXiv:2310.06770 [cs]. Oct. 2023. url: [http://arxiv.org/abs/2310.06770](http://arxiv.org/abs/2310.06770).
* [14] Fangyu Lei et al. _S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models_. arXiv:2310.15147 [cs]. Oct. 2023. url: [http://arxiv.org/abs/2310.15147](http://arxiv.org/abs/2310.15147).
* [15] Fangru Lin et al. _Graph-enhanced Large Language Models in Asynchronous Plan Reasoning_. arXiv:2402.02805 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2402.02805](http://arxiv.org/abs/2402.02805).
* [16] Na Liu et al. _From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models_. arXiv:2401.02777 [cs]. Jan. 2024. url: [http://arxiv.org/abs/2401.02777](http://arxiv.org/abs/2401.02777).
* [17] Xiao Liu et al. _AgentBench: Evaluating LLMs as Agents_. arXiv:2308.03688 [cs]. Oct. 2023. url: [http://arxiv.org/abs/2308.03688](http://arxiv.org/abs/2308.03688).
* [18] Zijun Liu et al. _Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization_. 2023. arXiv: 2310.02170 [cs.CL].
* [19] Yohei Nakajima. _yoheinakajima/babyagi_. original-date: 2023-04-03T00:40:27Z. Apr. 2024. url: [https://github.com/yoheinakajima/babyagi](https://github.com/yoheinakajima/babyagi).
* [20] Peter S. Park et al. _AI Deception: A Survey of Examples, Risks, and Potential Solutions_. arXiv:2308.14752 [cs]. Aug. 2023. url: [http://arxiv.org/abs/2308.14752](http://arxiv.org/abs/2308.14752).
* [21] Greg Serapio-Garcia et al. _Personality Traits in Large Language Models_. 2023. arXiv: 2307.00184 [cs.CL].
* [22] Zhengliang Shi et al. _Learning to Use Tools via Cooperative and Interactive Agents_. arXiv:2403.03031 [cs]. Mar. 2024. url: [http://arxiv.org/abs/2403.03031](http://arxiv.org/abs/2403.03031).
* [23] Noah Shinn et al. _Reflexion: Language Agents with Verbal Reinforcement Learning_. arXiv:2303.11366 [cs]. Oct. 2023. url: [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366).
* [24] Amir Taubenfeld et al. _Systematic Biases in LLM Simulations of Debates_. arXiv:2402.04049 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2402.04049](http://arxiv.org/abs/2402.04049).
* [25] Yu Tian et al. _Evil Geninuses: Delving into the Safety of LLM-based Agents_. arXiv:2311.11855 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2311.11855](http://arxiv.org/abs/2311.11855).
* [26] Qineng Wang et al. _Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?_ arXiv:2402.18272 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2402.18272](http://arxiv.org/abs/2402.18272).
* [27] Siyuan Wang et al. _Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation_. arXiv:2402.11443 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2402.11443](http://arxiv.org/abs/2402.11443).

* [28] Zhenhailong Wang et al. _Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration_. 2024. arXiv: 2307.05300 [cs.AI].
* [29] Jason Wei et al. _Chain-of-Thought Prompting Elicits Reasoning in Large Language Models_. arXiv:2201.11903 [cs]. Jan. 2023. url: [http://arxiv.org/abs/2201.11903](http://arxiv.org/abs/2201.11903).
* [30] Yue Wu et al. _SmartPlay: A Benchmark for LLMs as Intelligent Agents_. arXiv:2310.01557 [cs]. Mar. 2024. url: [http://arxiv.org/abs/2310.01557](http://arxiv.org/abs/2310.01557).
* [31] Zhiheng Xi et al. _The Rise and Potential of Large Language Model Based Agents: A Survey_. 2023. arXiv: 2309.07864 [cs.AI].
* [32] Shunyu Yao et al. _ReAct: Synergizing Reasoning and Acting in Language Models_. arXiv:2210.03629 [cs]. Mar. 2023. url: [http://arxiv.org/abs/2210.03629](http://arxiv.org/abs/2210.03629).
* [33] Shunyu Yao et al. _Tree of Thoughts: Deliberate Problem Solving with Large Language Models_. arXiv:2305.10601 [cs]. Dec. 2023. url: [http://arxiv.org/abs/2305.10601](http://arxiv.org/abs/2305.10601).
* [34] Muru Zhang et al. _How Language Model Hallucinations Can Snowball_. arXiv:2305.13534 [cs]. May 2023. url: [http://arxiv.org/abs/2305.13534](http://arxiv.org/abs/2305.13534).
* [35] Wenting Zhao et al. "(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild". In: _The Twelfth International Conference on Learning Representations_. 2024. url: [https://openreview.net/forum?id=B18u7ZR1bM](https://openreview.net/forum?id=B18u7ZR1bM).
* [36] Andy Zhou et al. _Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models_. arXiv:2310.04406 [cs]. Dec. 2023. url: [http://arxiv.org/abs/2310.04406](http://arxiv.org/abs/2310.04406).
* [37] Kaijie Zhu et al. _DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents_. arXiv:2402.14865 [cs]. Feb. 2024. url: [http://arxiv.org/abs/2402.14865](http://arxiv.org/abs/2402.14865).
* [38] Kaijie Zhu et al. _DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks_. arXiv:2309.17167 [cs]. Mar. 2024. url: [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167).

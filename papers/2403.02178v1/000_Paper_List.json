{
    "2403.02178v1": {
        "paper_id": "2403.02178v1",
        "abs_url": "https://arxiv.org/abs/2403.02178v1",
        "pdf_url": "https://arxiv.org/pdf/2403.02178v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2403.02178v1_Masked_Thought_Simply_Masking_Partial_Reasoning_Steps_Can_Improve_Mathematical_Reasoning_Learning_of_Language_Models.pdf",
        "title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Changyu Chen",
            "Xiting Wang",
            "Ting-En Lin",
            "Ang Lv",
            "Yuchuan Wu",
            "Xin Gao",
            "Ji-Rong Wen",
            "Rui Yan",
            "Yongbin Li"
        ],
        "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% improvement in GSM8K accuracy and 1% improvement in MATH accuracy across five datasets of various quality and size, as well as two base models. We further investigate the mechanisms behind this improvement through case studies and quantitative analysis, suggesting that our approach may provide superior support for the model in capturing long-distance dependencies, especially those related to questions. This enhancement could deepen understanding of premises in questions and prior steps. Our code is available at Github.",
        "comments": "",
        "official_code_urls": [
            "https://github.com/changyuchen347/maskedthought"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/masked-thought-simply-masking-partial",
        "bibtex": "@misc{chen2024masked,\n      title={Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models}, \n      author={Changyu Chen and Xiting Wang and Ting-En Lin and Ang Lv and Yuchuan Wu and Xin Gao and Ji-Rong Wen and Rui Yan and Yongbin Li},\n      year={2024},\n      eprint={2403.02178},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models\n' +
      '\n' +
      'Changyu Chen\\({}^{1}\\), Xiting Wang\\({}^{1}\\), Ting-En Lin\\({}^{2}\\), Ang Lv\\({}^{1}\\),\n' +
      '\n' +
      '**Yuchuan Wu\\({}^{2}\\)**, **Xin Gao\\({}^{3}\\)**, **Ji-Rong Wen\\({}^{1}\\)**, **Rui Yan\\({}^{1}\\)** and **Yongbin Li\\({}^{2}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)Gaoling School of Artificial Intelligence, Renmin University of China\n' +
      '\n' +
      '\\({}^{2}\\)Alibaba Group\n' +
      '\n' +
      '\\({}^{3}\\)Computational Bioscience Research Center, KAUST\n' +
      '\n' +
      '{chen.changyu, xitingwang, anglv,jrwen,ruiyan}@ruc.edu.cn\n' +
      '\n' +
      '{ting-en.lte, shengxiu.wyc, shuide.lyb}@alibaba-inc.com\n' +
      '\n' +
      ' Corresponding authors.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% improvement in GSM8K accuracy and 1% improvement in MATH accuracy across five datasets of various quality and size, as well as two base models. We further investigate the mechanisms behind this improvement through case studies and quantitative analysis, suggesting that our approach may provide superior support for the model in capturing long-distance dependencies, especially those related to questions. This enhancement could deepen understanding of premises in questions and prior steps. Our code is available at Github.1\n' +
      '\n' +
      'Footnote 1: [https://github.com/AlibabaResearch/DAMO-ConvAI](https://github.com/AlibabaResearch/DAMO-ConvAI), and [https://github.com/ChangyuChen347/MaskedThought](https://github.com/ChangyuChen347/MaskedThought).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Despite the significant advancements Large Language Models (LLMs) have achieved across various tasks, they continue to encounter difficulties with multi-step reasoning problems (Lightman et al., 2023; Huang et al., 2023; Shi et al., 2023; Chen et al., 2024). A primary challenge in reasoning arises from the fact that even a minor error can disrupt the entire solution process (Lightman et al., 2023). This issue is compounded in state-of-the-art models, which often suffer from hallucinations that lead to such errors (Huang et al., 2023). Analyses of the error patterns in mathematical reasoning tasks performed by GPT-3 reveal that over 50% of errors stem from either a misunderstanding of the problem or from incoherent reasoning steps (Wei et al., 2022).\n' +
      '\n' +
      'To improve the reasoning ability, most training methods focus on acquiring supervised signals obtained through costly means, such as human annotation (Liao et al., 2024), generation of larger models (Yu et al., 2023; Luo et al., 2023; Liu et al., 2023; Yue et al., 2023), or self-sampling (Yuan et al., 2023; Singh et al., 2023; Wang et al., 2023; Zelikman et al., 2022). These methods depend on these signals to direct learning towards a specific trajectory by precisely outlining the reasoning process. In contrast, our method illustrates that an entirely different approach is equally effective. Instead of requiring more precise guidance to direct the generative process, our technique achieves comparable results by introducing random noises into the input. For reasoning tasks, which are generally considered to require precise understanding, we find that the method of incorporating noise into the reasoning steps is surprisingly effective, indicating that the model might have enhanced its performance through a denoising process. Our approach stands out for its simplicity and effectiveness, offering a feasible alternative to the more complex and resource-intensive methods.\n' +
      '\n' +
      'We summarize our contributions as follows:\n' +
      '\n' +
      'First, _we propose a simple and effective Masked thought Fine-Tuning (MFT) method for improving language model reasoning._ The implementation of our method is simple: it requires only the substitution of specific tokens with a [mask] in the chain of thought. This is done while maintaining the same procedures as the standard Supervised Fine-Tuning (SFT). With just this minor modification, a 5% increase in accuracy can be achieved when fine-tuning on the GSM8K dataset with Llama-2-7B [13], which yields results comparable to those obtained using a more complex data generation pipeline (see Tab.2). Moreover, this method exhibits good versatility and complements other data augmentation techniques well, allowing for seamless integration. Our experiments have demonstrated that by substituting SFT with our Masked Thought strategy, we can enhance accuracy by an average of 3% on the GSM8K dataset and by 1% on the MATH dataset across five different math datasets and two models (see Tab.1). Moreover, MFT demonstrates higher sample efficiency than SFT (see Fig.2).\n' +
      '\n' +
      'Second, _we analyze our method from a regularization perspective and introduce two guiding principles for effective regularization._ Specifically, we propose a noise injection framework in which our method and many commonly employed regularization methods are special cases. These techniques introduce diverse forms of noise to mitigate overfitting, for example, applying dropout to each dimension of the embeddings [10] or employing synonym substitution [11]. We then conduct a comparative analysis and find that these regularization methods exhibit diminished effectiveness in the absence of the MFT framework (see Tab. 3). Based on our experiments, we propose two guiding principles for an effective regularization: (1) A portion of the tokens in the reasoning path should be retained without noise addition; (2) For positions where noise is added, it is crucial to ensure these positions maintain as less semantics as possible.\n' +
      '\n' +
      'Third, _we explore the underlying reasons for the effectiveness of our method by revealing its impact on learning dependencies._ By conducting both quantitative analyses and case studies, we observed that this method demonstrates an enhanced dependency on the initial mathematical questions and earlier steps. This suggests a possible reason: Instead of depending solely on the local information generated by the model itself from recent steps, which can lead to a higher likelihood of hallucination or misdirection as the sequence extends, our method utilizes more information from the problem statement and earlier steps. These sources are more reliable and less prone to errors. Consequently, this strategy might reduce the risk of misunderstanding the problem and inconsistencies in reasoning (refer to Secs. 2.2 and 3.4).\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '### Masked thought Fine-Tuning\n' +
      '\n' +
      'Our approach, named as Masked thought Fine-Tuning, falls under the umbrella of SFT. It maintains the simplicity of standard SFT implementation. To facilitate comparison with other regularization methods, we present a general framework of token noise injection. This framework considers both the position and the type of noise introduced:\n' +
      '\n' +
      '\\[\\mathcal{L}_{MFT} =-\\sum_{t=1}^{T}\\log p(w_{t}^{gt}|w_{1:t-1}^{s}) \\tag{1}\\] \\[w_{i}^{s} =\\begin{cases}f(w_{i}^{gt},n_{i}),&\\text{if }M_{i}=1\\\\ w_{i}^{gt},&\\text{otherwise}\\end{cases}\\] \\[n_{i} \\sim\\text{token noisy distribution }\\mathcal{N}\\]\n' +
      '\n' +
      '\\(M_{i}\\sim\\)Bernoulli\\((p)\\) for each target/source token \\(w_{i}^{gt}\\) (1)\n' +
      '\n' +
      'Similar to SFT, our method employs the unchanged \\(w^{gt}\\) (the ground truth solo to question \\(q\\)) as the label. The distinction lies in our introduction of noise \\(n_{i}\\) to the input. The implementation of \\(f\\) varies depending on the choice of token noise distribution \\(\\mathcal{N}\\) (see Table 3).\n' +
      '\n' +
      'This framework is compatible with many commonly used regularization methods. For instance, adding continuous noise to the vectors of all input tokens in both the target and source [15], where \\(n_{i}\\) follows a Uniform distribution ranging from \\(-\\frac{\\alpha}{c}\\) to \\(\\frac{\\alpha}{c}\\), and \\(M_{i}\\) is set to 1. In another approach, scheduled sampling, the model receives its own prediction from the previous time step as input with a probability \\(p\\), here \\(n_{i}\\) represents the predicted token, and \\(M_{i}\\) follows a Bernoulli distribution with parameter \\(p\\)[12].\n' +
      '\n' +
      'To understand how different choices of \\(n_{i}\\) and \\(M_{i}\\) impact reasoning performance, we conduct a comparative analysis (Tab. 3). Results show that this framework is effective as long as (1) \\(M_{i}\\) should follow a Bernoulli distribution within the target sequence, ensuring that only a subset of tokens in the chain of thought are subjected to noise, and (2) \\(n_{i}\\) is capable of removing the original semantic information at the noisy positions. We find methods that retain some semantics, such as synonym substitution, are less effective.\n' +
      '\n' +
      'Based on this observation, we primarily utilize [mask] as \\(n_{i}\\) due to its ability to effectively remove the semantics at the corresponding positions, and it is also easy to implement. Additionally, we observe that employing an exceptionally large \\(\\alpha\\) for the aforementioned continuous noise Uniform\\((-\\frac{\\alpha}{c},\\frac{\\alpha}{c})\\) or replacing the original token with another token uniformly sampled from the vocabulary, yields similar effects.\n' +
      '\n' +
      '### Enhancing Dependency Learning with MFT Regularization\n' +
      '\n' +
      '**Observation.** To understand the underlying reason for the effectiveness of MFT, we conduct a detailed analysis and find that models tend to shift towards longer-distance dependencies, particularly enhancing dependency on questions as shown in Fig. 1. Specifically, we iterate over pairs of numeric tokens in a sequence and disturb the first token to see if the second token changes, indicating dependency Schwab and Karlen (2019). As illustrated, Fig. 1(a) compares the dependency between two tokens within a chain of thought, showing that SFT has a greater dependency on short distances, whereas MFT shifts towards longer-distance dependencies. In Fig. 1(b), we examine the effect of disturbances in questions on the chain of thought, observing that MFT enhances all dependencies of questions compared to SFT. Then, We check this pattern of other models and datasets in Appendix D. We also observe decreasing trends of short-distance dependencies within the chain. The degree of this decrease differs among various datasets. A clear pattern is that there\'s a consistently increased dependence on questions.\n' +
      '\n' +
      '**Distance Bias.** The model may learn this distance bias from the pattern in ground truth data, where previous steps are correctly annotated, allowing the model to extract useful information from nearby thoughts and calculations, sometimes perhaps even overlooking the original question description. However, during inference, it generates steps on its own, moving away from the accurately verified ground truth. With each new step, the likelihood of errors increases. Even without obvious mistakes, inappropriate rephrasing or omissions of question details could mislead subsequent reasoning. This risk is heightened when the model excessively focuses on nearby tokens while neglecting the question, leading to a higher error probability. This phenomenon can also be referred to as exposure bias Bengio et al. (2015), where no safe reasoning is guaranteed during inference.\n' +
      '\n' +
      '**Dependency Regularization.** MFT is helpful in addressing these dependency issues. During stepwise reasoning, an intermediate step might utilize multiple premises from previous steps and questions, forming a directed acyclic graph of dependencies. For the human behavior, in the process of solving mathematical problems, we need to refer both to the initial conditions given in the problem and to many results obtained in previous steps frequently. Although Transformers allow for full connectivity between every token pair, enabling data-driven learning of such dependencies, they risk learning shortcuts through pattern-matching Liu et al. (2022); Khona et al. (2024); Dziri et al. (2023). MFT randomly prunes the fully connected graph\n' +
      '\n' +
      'Figure 1: We find MFT has a higher long-distance dependency than SFT. A higher bar indicates greater dependency at the corresponding distance. Specifically. We investigate the interdependency of two numerical tokens within a given sequence. We conduct experiments using the Llama-2-7b model trained on the GSM8K dataset with both the SFT and our method.\n' +
      '\n' +
      'during training to mask potential shortcuts, encouraging the model to build up more connections to more robust features, such as information from questions and earlier, less error-prone steps. We maintain the mathematical question unmasked, assuming it is error-free, to prompt the model to comprehend the question while leveraging the provided premises. Empirical evidence shows that masking parts of the questions does not improve performance. However, we find that even though we do not mask the questions, MFT is still able to handle the noise in the questions well (see Tab. 5). For balancing the model\'s exploration of unmasked premises and exploitation of the familiar premises, we employ a conservative linear warmup strategy over several epochs for stability. It Allows the model to first see a more complete context, then gradually increase the masking to ensure the stability of the loss value during training.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Dataset\n' +
      '\n' +
      '**Training Dataset.** In our main experiment, we utilized five training datasets covering math problems of varying difficulty levels and data quality.\n' +
      '\n' +
      'GSM8K: This dataset comprises 7,500 manually annotated grade school math problems Cobbe et al. (2021).\n' +
      '\n' +
      'MATH: This dataset contains 7,500 math competition problems from high school, which are more challenging than those in GSM8K Hendrycks et al. (2021).\n' +
      '\n' +
      'GSM8K-RFT Yuan et al. (2023) generates additional reasoning paths for each query in GSM8K by utilizing an SFT model that was trained on the original dataset. The process further includes a step of filtering out those reasoning paths that lead to incorrect answers. GSM8K-RFT-X differentiates the sources of these reasoning paths. GSM8K-RFT-100 sources from the SFT model itself, sampling 100 responses for each query, while GSM8K-RFT-U33 integrates reasoning paths from multiple models of various sizes to create a larger dataset.\n' +
      '\n' +
      'MetaMath: According to Yu et al. (2023), GPT-3.5-Turbo is employed to rephrase queries and responses in both MATH and GSM8K datasets and to introduce reverse engineering problems.\n' +
      '\n' +
      'MAMmnoTH Yue et al. (2023) introduces the data named MathInstruct incorporates a variety of math problem datasets, including but not limited to GSM8K, MATH, AQUA Ling et al. (2017), and Camel-Math Li et al. (2023). It also leverages GPT-4 to generate new reasoning paths through both Program-of-Thought (POT) Chen et al. (2022) and Chain-of-Thought (COT) methods.\n' +
      '\n' +
      '**Evaluation Dataset.** These datasets are mainly augmented from the training sets of GSM8K and MATH, and we test on the corresponding test sets. The GSM8K dataset sets aside 1,319 entries for testing, while the MATH dataset reserves 5,000 entries as a test set. We extract the final answers by specific templates (e.g., "the answer is") in the predicted reasoning paths following the scripts adopted by Yue et al. (2023); Yu et al. (2023). Subsequently, we calculate the ratio of predicted answers equal to the ground truth answers as the accuracy.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '**Improvements Compared with SFT.** The results presented in Tab. 1 illustrate that our approach improves performance no matter whether augmentation data is used, demonstrating effectiveness across datasets of varying quality and under two different base models. Notably, this enhancement is most significant in simpler or smaller datasets where the available useful signals in the data may be limited. Our approach demonstrates enhancements in the performance of the MAMnoTH dataset, which incorporates Program-of-Thought Data. This suggests that our method remains effective and is not adversely affected by the inclusion of programming tasks.\n' +
      '\n' +
      '**Sample Efficiency.** Fig 2 illustrates that within an equivalent number of training samples consumed, MFT yields a better performance than SFT. However, this does not imply that our approach requires\n' +
      '\n' +
      'Figure 2: Accuracy on GSM8K when training with Mistral-7B and the dataset of MAMnoTH. MFT shows a higher sample efficiency.\n' +
      '\n' +
      'fewer steps to converge. On the contrary, our method demands additional steps to reach a higher convergence value. This may be attributed to the exploration through masking, enabling us to gather new information from a limited dataset continually. Conversely, extending the training epochs for SFT introduces variability without yielding superior results. In our implementation details, we allow MFT to train for an extended number of epochs to ensure convergence of results as detailed in Tab. 8.\n' +
      '\n' +
      '**Mask Ratio and Scheduling.** In Fig. 3, we investigate the influence of two major hyperparameters in MFT: mask ratio and warm-up duration. Our analysis reveals that incorporating a warm-up period contributes to enhanced final performance for a large mask ratio, while a small mask ratio without warm-up also works. Within the GSM8K dataset, varying mask ratios exhibit minimal sensitivity, as the problem-solving procedures within it lack dense formulas. Consequently, this dataset permits a relatively higher mask ratio. In our primary experiment, we set this ratio to 0.4. However, for datasets with more intricate formulas, such as MATH, we opt for smaller mask ratios, typically 0.1 or 0.2. As for warm-up period, we adopt a conservative duration without excessive tuning of this hyperparameter. Generally, we allocate around two-thirds of the total training steps for warm-up.\n' +
      '\n' +
      '**Comparison with Self-Sampling.** RFT collects the reasoning paths sampled by the SFT model itself and filters out those paths that contain incorrect answers. Our experiments in Tab. 2 demonstrate that our method performs comparably to the explicit generation of reasoning paths, with even\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '\n' +
      'Figure 3: The impact of mask ratio when training on Llama2-7B with GSM8K. We compare MFT of two settings: Fixed mask ratio without warm-up, and a linear warm-up of the mask ratio starting from 0.\n' +
      '\n' +
      'better outcomes when both approaches are applied concurrently. One primary factor of this comparable performance may be that RFT does not sample extensively, yet the critical steps in the reasoning paths remain largely unchanged from the ground truth. The primary distinctions of sampled new paths lie in relatively minor semantic variances, without significantly increasing the new observed connections among local variables (Prystawski et al., 2023). It allows us to attain comparable effects by employing masked reasoning paths, where we sample new skip-connection by dropping the intermediate steps. On the other hand, if we compare from the perspective of addressing exposure bias, RL\'s self-sampling provides careful supervision for the error-prone inference distribution, whereas MFT aims to directly prevent the model from learning the bias present in the ground truth. We provide additional introduction about RL and RFT in Appendix B.\n' +
      '\n' +
      'Despite the performance, for training efficiency, our model is trained on 7K data for 10 epochs, while RFT-100 require training on 46K data for 3 epochs to achieve similar results. Notably, our approach achieves comparable performances without requiring extra machine computing resources.\n' +
      '\n' +
      '### Comparison with Other Regularization Techniques\n' +
      '\n' +
      'There are inquiries into whether alternative approaches can yield comparable outcomes. Thus, we delve into different techniques for introducing noise and empirically evaluate their effectiveness. We explore the effects of adding noises to different positions, as well as the noise type introduced by NEFTune, Dropout, Scheduled Sampling, and MaskedLM (our primary implementation). Detailed descriptions of these methods are provided in the Appendix K.\n' +
      '\n' +
      '**Results.** Our experiments yield the following insights: 1. In terms of the placement of noise, we find that masking partial reasoning steps in the target yields the best performance when compared to introducing noise across all tokens or specifically within the question. 2. We observe that introducing more huge noise to remove the original semantic is advantageous. Specifically, setting \\(\\tau=100\\) for Scheduled Sampling and \\(a=1000\\) for NEFTune enhances the model\'s performance. In contrast, the strategy of replacing synonyms, with \\(\\tau=1,2\\) in Scheduled Sampling, does not surpass the effectiveness of random replacement.\n' +
      '\n' +
      'When the model is trained with MaskedLM using \\(r=1\\), the model encounters random tokens unrelated to the current context, leading it to disregard the semantics of these tokens, akin to the role of [mask]. In the experiments in Tab. 3, \\(r=1\\) yields slightly higher results than \\(m=1\\). While \\(r=1\\) demonstrates higher levels of randomness in comparison to \\(m=1\\), it has the potential to better mitigate overfitting. However, on average, the distinction between \\(r=1\\) and \\(m=1\\) is marginal for larger datasets. With the exception of using \\(r=1\\) for GSM8K-7K, we default to utilizing \\(m=1\\) for other datasets in Tab. 1. We also explore the impact of adding attention masking at the masked positions within each transformer layer, beyond just the input masking. We find that its effectiveness is on par with using input masking alone.\n' +
      '\n' +
      'Additionally, we observe that even when solely computing losses on non-noisy tokens in MFT, the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '**Base Model** & **Training Dataset** & **Data Size** & **Augmention Source** & **Method** & **GSM8K** \\\\ \\hline \\multirow{4}{*}{Llama2-7B} & GSM8K & 7K & - & \\begin{tabular}{c} SFT \\\\ MFT \\\\ \\end{tabular} & 41.6 \\\\  & GSM8K-RFT-100 & 46K & Llama2-7B & \\begin{tabular}{c} SFT \\\\ MFT \\\\ \\end{tabular} & 47.6 \\\\  & GSM8K & 7K & - & \\begin{tabular}{c} SFT \\\\ MFT \\\\ \\end{tabular} & 52.7 \\\\ \\cline{2-5}  & GSM8K-RFT-100 & 47K & Llama2-13B & \n' +
      '\\begin{tabular}{c} SFT \\\\ MFT \\\\ \\end{tabular} & 55.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Compared to the self-sampling technique, namely RFT, our method achieves results comparable to those obtained by leveraging RFT-augmented data, even without the inclusion of additional data. Moreover, the effectiveness is notably amplified when MFT is applied to the augmented RFT data.\n' +
      '\n' +
      'performance still surpasses that of standard SFT (45.0, +3.4). Nonetheless, it exhibits lower performance compared to the computation of losses across all tokens (47.1, +5.5). Predicting the next tokens for masked positions poses a greater challenge than for unmasked ones, as they lack adjacent unmasked tokens from which to gather local information. This increased complexity ultimately improves the performance.\n' +
      '\n' +
      '### Investigating the Impact of MFT on Language Modeling\n' +
      '\n' +
      '**Case Study.** To supplement the statistical results shown in Figure 1, we further conduct a case study to demonstrate the impact of token dependencies introduced by MFT. In Figure 4, we vary a digit within the context between \\(0\\) and \\(9\\) to see whether it influences the predictions of subsequent steps. If the model consistently predicts \\(16-3\\), this indicates that the model\'s prediction is not influenced by the digit change, but rather relies on the information provided in the question and prior steps, with the word "three" highlighted in orange. Conversely, if the model\'s prediction shifts to \\(16-n\\), it implies a dependency on immediate, adjacent tokens, suggesting a reliance on distance shortcuts. The frequency of \\(16-3\\) predictions, as summarized in Table 4, reveals that MFT primarily leverages the token within the question, in contrast to the SFT approach, which tends to adopt a more short-sighted view.\n' +
      '\n' +
      'Conversely, we also modify the premise within the question as shown in Fig. 5, where a prediction of \\(16-n\\) indicates that, when faced with conflicting premises, the model prioritizes the question. The results of altering in question demonstrate that MFT excels in interpreting the questions, while SFT tends to ignore changes within the question.\n' +
      '\n' +
      '**Dealing with Distractors.** Some previous studies have also indicated that adding some disturbance to questions, such as inserting a irrelevant\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & **Is 16-3 (Fig. 4)** & **Is 16-n (Fig. 5)** \\\\ \\hline SFT & 0.1 & 0.1 \\\\ MFT(\\(m\\)=0,\\(r\\)=1) & 1 & 0.4 \\\\ MFT(\\(m\\)=1,\\(r\\)=0) & 0.4 & 0.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Results of model predictions after altering. When alter n to 0-9, we provide the proportion of the model predicting 16-3 or 16-n. A larger number represents less sensitive to the nearby premise and stronger dependence on questions.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Sequence Noise Distribution** & **Method** & **Token Noise Distribution \\(\\mathcal{N}\\)** & **Hyperparameter** & **GSMRK** \\\\ \\hline - & SFT & - & - & 41.6 \\\\ \\hline \\multirow{4}{*}{\\(M_{i}=1\\) for each token \\(i\\)} & \\multirow{4}{*}{Dropout} & \\(emh_{i}=emh_{i}\\odot D_{i}\\), & \\(D_{i}\\sim\\text{Bernoulli}(i)\\) & \\(q\\)=0.1 & 40.3 \\\\  & & & \\(q\\)=0.5 & 39.4 \\\\  & & & \\(q\\)=0.9 & 32.9 \\\\ \\cline{2-5}  & \\multirow{4}{*}{NEFTime} & \\(emh_{i}\\) + Uniform\\((-\\frac{q}{2},\\frac{q}{2})\\) & \\(\\alpha\\)=1 & 40.6 \\\\  & & & \\(\\alpha\\)=5 & 41.3 \\\\  & & & \\(\\alpha\\)=100 & 1.8 \\\\ \\hline \\multirow{4}{*}{\\(M_{i}=1\\) for each target token \\(i\\)} & \\multirow{4}{*}{Dropout} & \\(emh_{i}=emh_{i}\\odot D_{i}\\), & \\(D_{i}\\sim\\text{Bernoulli}(i)\\) & \\(q\\)=0.1 & 39.9 \\\\  & & & \\(q\\)=0.5 & 43.1 \\\\ \\cline{1-1}  & & & \\(q\\)=0.9 & 40.4 \\\\ \\hline \\multirow{4}{*}{\\(M_{i}\\sim\\text{Bernoulli}(p)\\) for each source token \\(i\\),} & \\multirow{4}{*}{Dropout} & \\(emh_{i}=emh_{i}\\odot D_{i}\\), & \\(D_{i}\\sim\\text{Bernoulli}(i)\\) & \\(q\\)=1,\\(p\\)=0.2 & 36.3 \\\\  & & & \\(q\\)=1,\\(p\\)=0.6 & 30.6 \\\\ \\hline \\multirow{4}{*}{\\(M_{i}\\sim\\text{Bernoulli}(p)\\) where \\(0<p<1\\)} & \\multirow{4}{*}{NEFTime} & \\(emh_{i}\\) + Uniform\\((-\\frac{q}{2},\\frac{q}{2})\\) & \\(\\alpha\\)=100 & 42.0 \\\\  & & & \\(q\\)=1,\\(000\\) & **47.2** \\\\ \\cline{2-5}  & \\multirow{4}{*}{Dropout} & \\(emh_{i}=emh_{i}\\odot D_{i}\\), & \\(D_{i}\\sim\\text{Bernoulli}(i)\\) & \\(q\\)=0.95 & 42.9 \\\\  & & & & \\(q\\)=1,\\(000\\) & **46.5** \\\\ \\cline{1-1} \\cline{2-5}  & \\multirow{4}{*}{Scheduled Sampling} & \\multirow{4}{*}{\\(seq_{i}=\\text{sample}(\\text{softmax}\\frac{\\text{std}\\cdot\\text{std}\\cdot \\text{std}\\cdot\\text{std}\\cdot\\text{std}\\cdot\\text{std}\\cdot\\text{std}\\cdot \\text{std}\\cdot\\text{stdcontext [22] or changing the order of premises within the question [3], can decrease the reasoning ability of large language models. We test on the GSM-IC [22] dataset which inserts some irrelevant distractor sentences into the original questions in GSM8K. From the results in Tab. 5, it can be seen that although MFT and RFT are roughly comparable on the original dataset, MFT shows a greater improvement on the GSM-IC dataset. This suggests that there are differences in the mechanisms by which these two methods improve performance and indicates that MFT can effectively locate useful premises within the disturbed questions, even though MFT does not mask the questions during training.\n' +
      '\n' +
      '**False Positive Analysis.** Despite the premises in these experiments being artificially disturbed, we would like to know if MFT might generate false positives due to the reduced use of neighboring features. False positives in this context refer to instances where intermediate steps are incorrect, yet the final result is accurately predicted. This analysis is conducted using GPT-4, which achieves an accuracy of 92% on the specified dataset. The findings, as detailed in Table 6, indicate that MFT does not result in a significant rise in the occurrence of false positives. The model\'s autoregressive prediction process not only improves the accuracy of the final answer but also preserves the correctness of the intermediate steps. We provide the prompt of GPT-4 and the cases of false positives in Appendix F.\n' +
      '\n' +
      '**Impact of Datasets on Learning Dependency.** We investigate the impact of different datasets on the effectiveness of the MFT technique, specifically, which types of data are best suited for MFT\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Method** & **\\# False Positive** & **\\# Correct** & **Step-wise Accuracy** & **Avg. Steps** \\\\ \\hline SFT & 2 & 70 & 0.516 & 3.36 \\\\ MFT (m=0,r=1) & 3 & 75 & 0.540 & 3.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Using GPT-4 to split the answer into steps and judge the correctness of middle steps in 200 samples. False Positive means the predicted answer is correct but the correct steps is less than total steps.\n' +
      '\n' +
      'Figure 4: The first step of this problem should be correctly solved as 16 - 3 = 13. We alter the prefix as “16 eggs/day - n eggs/day” to observe the impact of changing 3 to n on the response of SFT and MFT. We give the example of n=1. The orange part is other premises supporting the prediction of the current step.\n' +
      '\n' +
      'Figure 5: We alter the premise in the question to investigate the impact of changing the word “three” to “five”.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l l l l|} \\hline \\hline\n' +
      '**Base Model** & **Method** & **GSM8K** & **GSM-IC** \\\\ \\hline \\multirow{3}{*}{Llama2-7B} & SFT & 41.6 & 48.0 \\\\  & RFT-100 & **47.6 (+6.0)** & 52.2 (+4.2) \\\\  & MFT & 47.1 (+5.5) & **59.2 (+11.2)** \\\\ \\hline \\multirow{3}{*}{Llama2-13B} & SFT & 52.7 & 58.8 \\\\  & RFT-100 & 55.2 (+2.5) & 63.3 (+4.5) \\\\ \\cline{1-1}  & MFT & **56.2 (+3.5)** & **69.1 (+10.3)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: We conduct tests on GSM-IC (Grade-School Math with Irrelevant Context) [22] to compare the ability to cope with the distractors in the questions. This result illustrates that MFT can more effectively eliminate the distractors, utilizing more useful premises from the questions.\n' +
      '\n' +
      'Figure 6: We conduct experiments across two datasets to investigate the impacts of datasets on dependency. Left: Llama-GSM8K. Right: Llama-Alpaca. Training on Alpaca does not enhance the dependency, unlike GSM8K.\n' +
      '\n' +
      'in Fig. 6. Our experiments focus on the Alpaca dataset (Taori et al., 2023), a general instruction set with minimal mathematical reasoning content. The results reveal significant differences between the Alpaca and GSM8K datasets. Training with MFT using the Alpaca dataset does not alter dependency relationships, verifying our hypothesis. MFT enhances the model\'s ability to learn long-distance dependencies within the data. The absence of such dependencies in data renders MFT less effective. Additionally, we assess whether MFT could detrimentally influence the model\'s general conversational capabilities. According to the results presented in Appendix H, the scores of Alpaca-eval (Li et al., 2023) using both MFT and SFT were comparable.\n' +
      '\n' +
      'Error Analysis.We also perform a quantitative error analysis to understand the impact of MFT on the final results by analyzing the types of errors. We adopt the following error types: Calculator Error (CE), One Step Missing Error (OSME), Semantic Understanding Error (SUE) and Incoherent Steps Error (ISE). CE and OSME are minor errors that can be fixed with one-step while SUE and ISE are errors of misunderstanding the question or previous steps which need substantial fixing as defined by Wei et al. (2022). See more details of these error types in Appendix J. We use GPT-4 as a proxy for the judgement. Tab. 7 demonstrates that MFT has fewer SUE and ISE errors. This may be due to the benefits obtained by enhancing the model\'s question dependence. Additionally, the model experiences an increase in OSME and CE compared to SFT. The increased ratios might be minor errors that arise after solving major errors in question understanding.\n' +
      '\n' +
      '## 4 Conclusion\n' +
      '\n' +
      'We propose Masked thought Fine-Tuning as a regularization technique during fine-tuning. Our findings indicate that MFT not only boosts the reasoning performance but also improves the model\'s ability to capture long-distance dependencies. This study could pave the way for future research on regularization strategies for reasoning tasks and further exploration of the interplay between language dependency and reasoning capabilities.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      'The discussion regarding the root cause of the observed performance enhancement primarily stems from empirical experiments. We offer an explanation based on a notable phenomenon--the transfer of dependencies. However, we have not proven that dependency transfer is the sole factor in improving model performance; there may still be other influencing factors. This suggests that further empirical and theoretical research into the effective reasons behind the method\'s success is warranted.\n' +
      '\n' +
      'This method is effective for reasoning data, while demonstrating no significant effect on a general instruction tuning dataset which lacks of multi-steps reasoning. We have only provided a qualitative explanation so far, without proposing a method that includes quantitative metrics to determine which types of data are best suited for this approach.\n' +
      '\n' +
      'The experiments indicate that the mask ratios that different datasets can accommodate may not be entirely the same. As for mixed data, this paper has not yet explored strategies for adaptively fitting mask ratios to different datasets. Additionally, we have not yet investigated whether this approach, when applied to mixed reasoning data and general instruction tuning data, affects the general capabilities or diminishes the observed enhancement of reasoning.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'This work was supported by Alibaba Group through Alibaba Innovative Research Program.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.\n' +
      '* An et al. (2023)Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. Learning from mistakes makes llm better reasoner. ArXivabs/2310.20689. Cited by: SS1.\n' +
      '* Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline\n' +
      '**Method** & **OSME** & **ISE \\& SUE** & **CE** & **Total Errors** \\\\ \\hline SFT & 15.6\\% & 78.0\\% & 6.0\\% & 173 \\\\ RFT-100 & 11.9\\% & 77.4\\% & 10.6\\% & 151 \\\\ MFT & 15.2\\% & 74.8\\% & 9.9\\% & 151 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: The proportions of various error types across different methods, based on 300 samples randomly selected from the test set.\n' +
      '\n' +
      'Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. _arXiv preprint arXiv:2310.10631_.\n' +
      '* Bao et al. (2020) Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. In _International Conference on Machine Learning_.\n' +
      '* Bengio et al. (2015) Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. _Advances in neural information processing systems_, 28.\n' +
      '* Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. 2023. Graph of thoughts: Solving elaborate problems with large language models. _arXiv preprint arXiv:2308.09687_.\n' +
      '* Chen et al. (2023a) Changyu Chen, Xiting Wang, Yiqiao Jin, Victor Ye Dong, Li Dong, Jie Cao, Yi Liu, and Rui Yan. 2023a. Semi-offline reinforcement learning for optimized text generation. _arXiv preprint arXiv:2306.09712_.\n' +
      '* Chen et al. (2023b) Jiaoo Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. 2023b. An empirical survey of data augmentation for limited data learning in nlp. _Transactions of the Association for Computational Linguistics_, 11:191-211.\n' +
      '* Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _arXiv preprint arXiv:2211.12588_.\n' +
      '* Chen et al. (2024) Xinyun Chen, Ryan A Chi, Xuezhi Wang, and Denny Zhou. 2024. Premise order matters in reasoning with large language models. _arXiv preprint arXiv:2402.08939_.\n' +
      '* Cheng et al. (2018) Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, and Yang Liu. 2018. Towards robust neural machine translation. _ArXiv_, abs/1805.06130.\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In _North American Chapter of the Association for Computational Linguistics_.\n' +
      '* Dziri et al. (2023) Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality. _ArXiv_, abs/2305.18654.\n' +
      '* Fadaee et al. (2017) Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation. _arXiv preprint arXiv:1705.00440_.\n' +
      '* Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. _arXiv preprint arXiv:2210.00720_.\n' +
      '* Gal and Ghahramani (2015) Yarin Gal and Zoubin Ghahramani. 2015. A theoretically grounded application of dropout in recurrent neural networks. In _Neural Information Processing Systems_.\n' +
      '* Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Tora: A tool-integrated reasoning agent for mathematical problem solving. _ArXiv_, abs/2309.17452.\n' +
      '* Guo et al. (2020a) Demi Guo, Yoon Kim, and Alexander M Rush. 2020a. Sequence-level mixed sample data augmentation. _arXiv preprint arXiv:2011.09039_.\n' +
      '* Guo et al. (2020b) Demi Guo, Yoon Kim, and Alexander M. Rush. 2020b. Sequence-level mixed sample data augmentation. In _Conference on Empirical Methods in Natural Language Processing_.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_.\n' +
      '* Huang et al. (2023a) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023a. Large language models cannot self-correct reasoning yet. _arXiv preprint arXiv:2310.01798_.\n' +
      '* Huang et al. (2023b) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023b. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\n' +
      '* Jain et al. (2023) Neel Jain, Ping Yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Sompealli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. Neftune: Noisy embeddings improve instruction finetuning.\n' +
      '* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_.\n' +
      '* Liu et al. (2020)Chris Kedzie and Kathleen McKeown. 2019. A good sample is hard to find: Noise injection sampling and self-training for neural language generation models. In _International Conference on Natural Language Generation_.\n' +
      '* Khona et al. (2024) Mikail Khona, Maya Okawa, Jan Hula, Rahul Ramesh, Kento Nishi, Robert Dick, Ekdeep Singh Lubana, and Hidenori Tanaka. 2024. Towards an understanding of stepwise inference in transformers: A synthetic graph navigation model. _arXiv preprint arXiv:2402.07757_.\n' +
      '* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213.\n' +
      '* Lample et al. (2017) Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\'Aurelio Ranzato. 2017. Unsupervised machine translation using monolingual corpora only. _arXiv preprint arXiv:1711.00043_.\n' +
      '* Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for" mind" exploration of large scale language model society. _arXiv preprint arXiv:2303.17760_.\n' +
      '* Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).\n' +
      '* Li et al. (2022) Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, B. Chen, Jian-Guang Lou, and Weizhu Chen. 2022. Making language models better reasoners with step-aware verifier. In _Annual Meeting of the Association for Computational Linguistics_.\n' +
      '* a reproducible pipeline. _ArXiv_, abs/2401.08190.\n' +
      '* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let\'s verify step by step. _ArXiv_, abs/2305.20050.\n' +
      '* Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. _arXiv preprint arXiv:1705.04146_.\n' +
      '* Liu et al. (2022) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2022. Transformers learn shortcuts to automata. _ArXiv_, abs/2210.10749.\n' +
      '* Liu et al. (2023) Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. 2023. Tinygsm: achieving\\(\\mathbf{>}\\) 80% on gsm8k with small language models. _arXiv preprint arXiv:2312.09241_.\n' +
      '* Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_.\n' +
      '* Mihaylova and Martins (2019) Tsvetomila Mihaylova and Andre FT Martins. 2019. Scheduled sampling for transformers. _arXiv preprint arXiv:1906.07651_.\n' +
      '* Prystawski et al. (2023) Ben Prystawski, Michael Y. Li, and Noah D. Goodman. 2023. Why think step-by-step? reasoning emerges from the locality of experience. _ArXiv_, abs/2304.03843.\n' +
      '* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.\n' +
      '* Schwab and Karlen (2019) Patrick Schwab and Walter Karlen. 2019. CXPlain: Causal Explanations for Model Interpretation under Uncertainty. In _Advances in Neural Information Processing Systems (NeurIPS)_.\n' +
      '* Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In _International Conference on Machine Learning_, pages 31210-31227. PMLR.\n' +
      '* Singh et al. (2023) Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaochon Lee, Kelvin Xu, Aaron Parisi, et al. 2023. Beyond human data: Scaling self-training for problem-solving with language models. _arXiv preprint arXiv:2312.06585_.\n' +
      '* Sinha et al. (2022) Samarth Sinha, Ajay Mandlekar, and Animesh Garg. 2022. S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics. In _Conference on Robot Learning_, pages 907-917. PMLR.\n' +
      '* Srivastava et al. (2014) Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. _J. Mach. Learn. Res._, 15:1929-1958.\n' +
      '* Su et al. (2022) Ming-Hsiang Su, Chin-Wei Lee, Chi-Lun Hsu, and Ruei-Cyuan Su. 2022. RoBERTa-based traditional Chinese medicine named entity recognition model. In _Proceedings of the 34th Conference on Computational Linguistics and Speech Processing (ROCLING 2022)_, pages 61-66, Taipei, Taiwan. The Association for Computational Linguistics and Chinese Language Processing (ACLCLP).\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.\n' +
      '\n' +
      'Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Uesato et al. (2022) Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcome-based feedback. _arXiv preprint arXiv:2211.14275_.\n' +
      '* Wang et al. (2023a) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. _arXiv preprint arXiv:2305.04091_.\n' +
      '* Wang et al. (2023b) Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui. 2023b. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. _arXiv preprint arXiv:2312.08935_.\n' +
      '* Wang et al. (2018) Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. Switchout: an efficient data augmentation algorithm for neural machine translation. In _Conference on Empirical Methods in Natural Language Processing_.\n' +
      '* Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837.\n' +
      '* Wei and Zou (2019) Jason Wei and Kai Zou. 2019. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. In _Conference on Empirical Methods in Natural Language Processing_.\n' +
      '* Xi et al. (2024) Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. 2024. Training large language models for reasoning through reverse curriculum reinforcement learning. _arXiv preprint arXiv:2402.05808_.\n' +
      '* Xie et al. (2022) Shufang Xie, Ang Lv, Yingce Xia, Lijun Wu, Tao Qin, Tie-Yan Liu, and Rui Yan. 2022. Target-side input augmentation for sequence to sequence generation. In _International Conference on Learning Representations_.\n' +
      '* Xie et al. (2016) Ziang Xie, Sida I Wang, Jiwei Li, Daniel Levy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. 2016. Data noising as smoothing in neural network language models. In _International Conference on Learning Representations_.\n' +
      '* Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_.\n' +
      '* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_.\n' +
      '* Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_.\n' +
      '* Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. _arXiv preprint arXiv:2308.01825_.\n' +
      '* Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. _arXiv preprint arXiv:2309.05653_.\n' +
      '* Zelikman et al. (2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstraping reasoning with reasoning. _Advances in Neural Information Processing Systems_, 35:15476-15488.\n' +
      '* Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In _Neural Information Processing Systems_.\n' +
      '\n' +
      'Related work\n' +
      '\n' +
      '### Mathematical Reasoning of LLM.\n' +
      '\n' +
      'To further enhance the capabilities of pre-trained Large Language Models(Touvron et al., 2023; Jiang et al., 2023; Achiam et al., 2023; Azerbaijan et al., 2023) on reasoning tasks, the primary strategies involve fine-tuning and various training-free post-processing techniques. The post-processing techniques include approaches like prompting(Wei et al., 2022; Fu et al., 2022; Kojima et al., 2022; Wang et al., 2023a), ensembling(Wang et al., 2022; Li et al., 2022), and reranking(Uesato et al., 2022; Cobbe et al., 2021). A unique technique specific to LLMs is to leverage the model\'s reasoning capabilities through prompting. This entails crafting zero-shot prompts(Kojima et al., 2022; Wang et al., 2023a) or designing a small set of demonstrations to facilitate in-context learning(Wei et al., 2022; Fu et al., 2022). Additionally, there are methods focused on designing intricate pipelines utilizing various rules or algorithms for enhanced reasoning. These methods include search(Yao et al., 2023; Besta et al., 2023), iterative feedback collection(Yao et al., 2022), and employing the Program-of-Thought approach for generating code solutions(Chen et al., 2022).\n' +
      '\n' +
      'In terms of fine-tuning, whether through Supervised Fine Tuning (SFT) or Reinforcement Learning (RL), the primary aim is to gather high-quality supervisory signals for model improvement. For SFT, the approach generally involves leveraging larger models or manual efforts to collect high-quality data (An et al., 2023; Yu et al., 2023; Yue et al., 2023; Luo et al., 2023). MetaMath(Yu et al., 2023; Gou et al., 2023) augments its dataset by rewriting mathematical queries and provide answers with the help of an LLM. It also leverages backward reasoning to enhance the model\'s capabilities. MAmmoTH(Yue et al., 2023) compiles diverse mathematical datasets for training, enriching them with Chains of Thought (CoT) and Programs of Thought (PoT) rationales for reasoning. RL-based methods strive to gather superior training signals through self-sampling and reward supervision. Lightman et al. (2023) proposes to use process supervision to give more precise signals than outcome supervision. Some strategies adopt the process supervision into the online RL training (Wang et al., 2023b; Xi et al., 2024; Luo et al., 2023). RFT(Yuan et al., 2023) is a simplified offline method of standard online RL. It enhances its training dataset with accurate reasoning paths by generating and collecting them via a SFT model. This approach is further extended by ReST (Singh et al., 2023) through an iterative process.\n' +
      '\n' +
      'Our method is parallel to the above fine-tuning methods. after obtaining supervisory signals through different approaches, one can consider using this regularization method in the final optimization step to further aid in training.\n' +
      '\n' +
      '### Data Augmentation for NLP.\n' +
      '\n' +
      'In the area of Natural Language Processing (NLP), an extensive range of data augmentation techniques has been introduced for diverse applications, such as translation, summarization, and dialogue (Chen et al., 2023b; Fadaee et al., 2017; Xie et al., 2016; Lample et al., 2017; Guo et al., 2020a; Xie et al., 2022; Wang et al., 2018). The noise-based strategies commonly involve the integration of random continuous noise (Cheng et al., 2018; Jain et al., 2023; Kedzie and McKeown, 2019), token dropout (Xie et al., 2016; Gal and Ghahramani, 2015), token swapping (Wei and Zou, 2019), and more advanced mixup strategies (Guo et al., 2020b; Xie et al., 2022). Recently, in the context of regularization for LLMs, Jain et al. (2023) has demonstrated that the application of continuous noise across all tokens during the instruction tuning can significantly improve the length and detail of the generated responses. However, this technique does not improve the reasoning ability. We adopt the discrete token noise through token dropout (Xie et al., 2016; Gal and Ghahramani, 2015) to reasoning tasks, finding that it works well, which may complement the dependency-rich structure characteristic of reasoning data.\n' +
      '\n' +
      'Discussion of the Relation to RL\n' +
      '\n' +
      '### Supervised Fine-tuning (SFT)\n' +
      '\n' +
      'The pre-trained language model is often enhanced for specific downstream tasks through Supervised Fine-Tuning (SFT) as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}_{SFT}=-\\sum_{t=1}^{T}\\log p(w_{t}^{gt}|w_{1:t-1}^{gt}) \\tag{2}\\]\n' +
      '\n' +
      '### Reinforcement Learning (RL) Fine-tuning\n' +
      '\n' +
      'After SFT, to further improve the model, RL is applied to align the model to the specific goal. The model automatically samples responses, and a reward model scores these generated responses to distinguish their quality. For mathematical reasoning, it has been verified that rewards based on intermediate processes yield better results compared to rewards based solely on final results (Wang et al., 2023; Lightman et al., 2023).\n' +
      '\n' +
      'Eq. (3) shows a simplified policy gradient formulation of process supervision (omitting some techniques like the KL penalty used for regularization (Schulman et al., 2017)). For a standard online RL process, the model samples a reasoning path for a question, and then the reward model is used to compute the reward for each step.\n' +
      '\n' +
      '\\[\\mathcal{L}_{RL}=-\\sum_{t=1}^{T}\\mathcal{R}(w_{t})\\log\\ p(w_{t}|w _{1:t-1};q) \\tag{3}\\] \\[w_{1:T}\\sim p(.|q)\\] \\[R(w_{t})=\\begin{cases}1,&\\text{if $w_{1:t}$ is correct}\\\\ &\\text{\\& $w_{t}$ is the end of one step},\\\\ 0,&\\text{otherwise}.\\end{cases}\\]\n' +
      '\n' +
      'Where \\(q\\) is the question of a math problem. \\(w_{1:T}\\) denotes the sampled answer with a length of \\(T\\). \\(p\\) is the policy model.\n' +
      '\n' +
      '### Rejection sampling Fine-Tuning (RFT)\n' +
      '\n' +
      'RFT is a simplified offline version of RL with:\n' +
      '\n' +
      '\\[\\mathcal{L}_{RFT}=-\\sum_{t=1}^{T}\\log\\ p(w_{t}|w_{1:t-1};q) \\tag{4}\\] \\[w_{1:T}\\sim p_{\\text{sft}}(.|q)\\ \\ \\&\\text{ \\ the outcome of $w_{1:T}$ is correct}\\]\n' +
      '\n' +
      'Where \\(w_{1:T}\\) is sampled from a fixed SFT model \\(p_{\\text{sft}}\\) in the first stage, and the new data is added to the original data for the second SFT stage. While \\(p\\) in Eq. (3) is updated online. Singh et al. (2023) also proposes to do this two-stage method for multiple iterations.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '**Relation to RL Fine-Tuning.** Our method can be regarded as a special case of Eq. (3) with the following assumption: 1) We relax the reward computation in Eq. (3) and assume that every token in the ground-truth data can provide a positive process reward. 2) Sampling masked position has a similar effect with sampling new paths.\n' +
      '\n' +
      '**Question:** Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n' +
      '\n' +
      '**Answer1:**\n' +
      '\n' +
      '1. Natalia sold 48/2 = 24 clips in May.\n' +
      '\n' +
      '2. Natalia sold 48+24 = 72 clips altogether in April and May.\n' +
      '\n' +
      '3. The answer is 72\n' +
      '\n' +
      '**Answer2:**\n' +
      '\n' +
      '1. Natalia sold 48/2 = 24 clips in May.\n' +
      '\n' +
      '2. She sold 48+24 = 72 clips altogether in April and May.\n' +
      '\n' +
      '3. The answer is 72\n' +
      '\n' +
      '**Answer3:**\n' +
      '\n' +
      '1. Natalia sold 48/2=24 clips in May.\n' +
      '\n' +
      '2. The total number of clips sold by Natalia in April and May is 48+24=72\n' +
      '\n' +
      '3. The answer is 72\n' +
      '\n' +
      '**Answer4:**\n' +
      '\n' +
      '1. The number of clips sold in May is 1/2 * 48 = 24.\n' +
      '\n' +
      '2. Altogether, Natalia sold 48 + 24 = 72 clips in April and May.\n' +
      '\n' +
      '3. The answer is 72\n' +
      '\n' +
      '\\[\\mathcal{L}_{MFT} =-\\sum_{t=1}^{T}\\log p(w_{t}^{gt}|w_{1:t-1}^{s}) \\tag{5}\\] \\[=-\\sum_{t=1}^{T}R(w_{t})\\log p(w_{t}|w_{1:t-1};q),\\] \\[R(w_{t})=\\begin{cases}1,&\\text{if }w_{t}=w_{t}^{gt},\\\\ 0,&\\text{otherwise}.\\end{cases}\\]\n' +
      '\n' +
      'For assumption 1, since the ground truth is correct, there is no need to use the reward model for labeling, and most tokens are supposed to be associated with positive reward signals. For assumption 2, we think that it may hold when sampling of online RL is insufficient:\n' +
      '\n' +
      '**Differences in Sampling Compared to Online RL.** Standard auto-regressive sampling can bring more reasoning paths to one query. However, we find that the sampled reasoning paths have minor differences in Fig. 7, such as word substitution (orange part), rewriting of formulas (red part), rearrangement of syntax (brown part), but the correct intermediate steps are consistent (green part).\n' +
      '\n' +
      'Therefore, for queries in the dataset, each of them already includes a ground truth response, which can provide rich process supervision signals of correct intermediate steps. MFT does not sample new correct reasoning steps but rather different preceding reasoning steps, allowing the model to predict the correct steps given different contexts. In short, the online RL sample new **actions** and **states** in a Markov Decision Process, while we sampled the **state** to make it partially observed. Adding noise to the **state** is verified to be helpful in generalizing for offline RL (Sinha et al., 2022), such as continuous noise on state embedding. We compare with other noise in Tab. 3, and we find that the discrete noise of MFT is more efficient.\n' +
      '\n' +
      '**Sample Efficiency.** For online RL, it is necessary to sample diverse and better reasoning paths to achieve better results than using a static dataset. However, after SFT for initialization, the sampled paths\n' +
      '\n' +
      'Figure 7: The sampled reasoning paths have minor differences, such as word substitution (orange part), rewriting of formulas(red part), rearrangement of syntax(brown part), but the correct intermediate steps are consistent (green part).\n' +
      '\n' +
      'will be very similar to the ground truth. If the sampling is sufficient and the reward model is strong enough to find high-quality reasoning paths that may have better generality and conciseness, RL can achieve better results. This can be the reason why RFT which using much more samples performs similarly to ours, as RFT only samples for multiple paths at once and only uses outcome supervision.\n' +
      '\n' +
      'For MFT, the sample efficiency is related to the mask ratio. A small mask ratio gives the model a chance to exploit current high-confidence context tokens, while a higher mask ratio requires the model to explore the use of other distant tokens.\n' +
      '\n' +
      'Figure 8: The model acquires the ability to engage in step-by-step reasoning by leveraging prior premises through data-driven learning. We add regularization to language modeling, compelling the model to grasp the deductive relationship inherent between premises and conclusions.\n' +
      '\n' +
      'Implementation Details\n' +
      '\n' +
      '## Appendix C\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Base Model** & **Training Dataset** & **Learning Rate** & **Epoch** & **Method** & **batch size** & **Mask Ratio** \\\\ \\hline \\multirow{8}{*}{Ilama2-7B} & GSM8K & 1e-5 & 3 & SFT & 64 & 0 \\\\  & 1e-5 & 10 & MFT & 64 & 0.4 \\\\ \\cline{2-6}  & MATH & 1e-5 & 3 & SFT & 64 & 0 \\\\  & 1e-5 & 10 & MFT & 64 & 0.2 \\\\ \\cline{2-6}  & GSM8K-RFT-U33 & 1e-5 & 3 & SFT & 64 & 0 \\\\  & 1e-5 & 5 & MFT & 64 & 0.4 \\\\ \\cline{2-6}  & GSM8K-RFT-100 & 1e-5 & 3 & SFT & 64 & 0 \\\\  & 1e-5 & 5 & SFT & 64 & 0.4 \\\\ \\cline{2-6}  & MetaMath & 2e-5 & 3 & SFT & 128 & 0 \\\\  & 2e-5 & 5 & MFT & 128 & 0.2 \\\\ \\cline{2-6}  & Mammoth & 2e-5 & 3 & SFT & 128 & 0 \\\\  & 2e-5 & 5 & MFT & 128 & 0.2 \\\\ \\hline \\multirow{8}{*}{Mistral-7B} & GSM8K & 1e-6 & 3 & SFT & 64 & 0 \\\\  & 1e-6 & 10 & MFT & 64 & 0.2 \\\\ \\cline{2-6}  & MATH & 1e-6 & 3 & SFT & 256 & 0 \\\\  & 1e-6 & 3 & MFT & 256 & 0.1 \\\\ \\cline{2-6}  & GSM8K-RFT-U33 & 1e-6 & 3 & SFT & 128 & 0 \\\\  & 1e-6 & 3 & MFT & 128 & 0.4 \\\\ \\cline{2-6}  & MetaMath & 3e-6 & 3 & SFT & 256 & 0 \\\\  & 3e-6 & 5 & MFT & 256 & 0.2 \\\\ \\cline{2-6}  & Mammoth & 5e-6 & 3 & SFT & 256 & 0 \\\\  & 5e-6 & 5 & MFT & 256 & 0.2 \\\\ \\hline \\multirow{8}{*}{Ilama2-13B} & GSM8K & 1e-5 & 3 & SFT & 64 & 0 \\\\  & 1e-5 & 10 & MFT & 64 & 0.4 \\\\ \\cline{2-6}  & GSM8K-RFT-100 & 1e-5 & 3 & SFT & 64 & 0 \\\\ \\cline{1-1} \\cline{2-6}  & GSM8K-RFT-100 & 1e-5 & 5 & MFT & 64 & 0.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: The table lists the implementation details with different datasets. The default setting for the mask ratio warmup steps is two-thirds of the total steps. We use random tokens as the noise for GSM8K-7K and [mask] for other datasets. We train on 8 NVIDIA A100 GPUs.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:18]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      'False Positives\n' +
      '\n' +
      '**Prompt:** You are given a math problem and a solution, you should first split the solution into multi steps and judge if each step is correct and count the total correct steps.\n' +
      '\n' +
      '<problem start>\n' +
      '\n' +
      '<problem>\n' +
      '\n' +
      '<problem end>\n' +
      '\n' +
      '<solution start>\n' +
      '\n' +
      '<solution>\n' +
      '\n' +
      'Valid answers look like the following:\n' +
      '\n' +
      '"Justification": ["Your explanation"], "correct steps": 0, "total steps": 3\n' +
      '\n' +
      '"Justification": ["Your explanation"], "correct steps": 1, "total steps": 3\n' +
      '\n' +
      '"Justification": ["Your explanation"], "correct steps": 1, "total steps": 2\n' +
      '\n' +
      '"Justification": ["Your explanation"], "correct steps": 1, "total steps": 1\n' +
      '\n' +
      'You should finish your response with. Your response should always start with {"Justification":\n' +
      '\n' +
      'Figure 14: The prompt to judge the correctness of middle steps for GPT-4. We utilize the version of gpt-4-1106-preview. To help the prediction of GPT-4, we provide the groundtruth in the prompt for reference.\n' +
      '\n' +
      '**QUESTION:** Henry made two stops during his 60-mile bike trip. He first stopped after 20 miles. His second stop was 15 miles before the end of the trip. How many miles did he travel between his first and second stops?\n' +
      '\n' +
      '**MODEL ANSWER:** Henry traveled 60-20= <<60-20=40>>40 miles between his first and second stops. He traveled 40 - 15 = <<40-15=25>>25 miles between his second stop and the end of the trip. **** 25\n' +
      '\n' +
      '**GPT-4 Judgement:** The first step mistakenly calculates the distance traveled after the first stop until the end of the trip, not accounting for the second stop which is 15 miles before the end. The second step incorrectly describes the result of the first calculation as the distance between the second stop and the end of the trip, whereas it is actually the distance from the first stop to the end of the trip minus the distance from the second stop to the end of the trip. However, despite the incorrect reasoning in both steps, the final answer achieved matches the correct answer.\n' +
      '\n' +
      '**Error:** Correct Counting, Incorrect Thought\n' +
      '\n' +
      '**QUESTION:** Jim spends 2 hours watching TV and then decides to go to bed and reads for half as long. He does this 3 times a week. How many hours does he spend on TV and reading in 4 weeks?\n' +
      '\n' +
      '**MODEL ANSWER:** He reads for 2/.5=<2*.5=1>>1 hour. So he spends 2+1=<2+1=3>>3 hours on those activities. He does this 3 times a week so he spends 3*3=<3*3=9>>9 hours on those activities. Over 4 weeks, he spends 9*4=<9*4=36>>36 hours. **** 36\n' +
      '\n' +
      '**GPT-4 Judgement:** The first step is incorrect; \'2/.5\' suggests division when it should be multiplication, \'2*.5\'. The correct calculation for the reading time is 2/2 hours, which equals 1 hour, not \'2/.5\' hours. However, the numerical result \'1 hour\' happens to be correct despite the incorrect operation shown. The remaining steps correctly add the two activities to find the total daily time spent, correctly multiply by the number of occurrences per week, and correctly multiply by the number of weeks to find the total hours spent over four weeks.\n' +
      '\n' +
      '**Error:** Incoherent Reasoning\n' +
      '\n' +
      '**QUESTION:** Violetta wants to buy new crayons. She needs them in 5 different colors and prepared $20 for this purchase. One crayon costs $2. How much change will she get?\n' +
      '\n' +
      '**MODEL ANSWER:** Violetta needs to buy 5 * 2 = <<5*2=10>>10 crayons. She will get change of 20 - 10 = $*20-10=10>>10. **** 10\n' +
      '\n' +
      '**GPT-4 Judgement:** The first step suggests Violetta needs to buy 10 crayons, which is incorrect as she needs 5 crayons. However, the multiplication operation itself (5 * 2) is correct, it\'s just applied to the wrong aspect of the problem. The second step calculating the change is correct, both arithmetically and contextually.\n' +
      '\n' +
      '**Error:** Correct Counting, Incorrect Thought\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**MODEL ANSWER:** Henry traveled 60-20= <<60-20=40>>40 miles between his first and second stops. He traveled 40 - 15 = <<40-15=25>>25 miles between his second stop and the end of the trip. **** 25\n' +
      '\n' +
      '**GPT-4 Judgement:** The first step mistakenly calculates the distance traveled after the first stop until the end of the trip, not accounting for the second stop which is 15 miles before the end. The second step incorrectly describes the result of the first calculation as the distance between the second stop and the end of the trip, whereas it is actually the distance from the first stop to the end of the trip minus the distance from the second stop to the end of the trip. However, despite the incorrect reasoning in both steps, the final answer achieved matches the correct answer.\n' +
      '\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**Error:** Correct Counting, Incorrect Thought\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 10: False positives of MFT\n' +
      '\n' +
      '**QUESTION:** Cynthia eats one serving of ice cream every night. She buys cartons of ice cream with 15 servings of ice cream per carton at a cost of $4.00 per carton. After 60 days, how much will she spend on ice cream?\n' +
      '\n' +
      '**MODEL ANSWER:** She eats one serving of ice cream a night and there are 31 days in a month so she eats 1*31 = \\(\\ll\\)1*31=31*31 servings of ice cream a month. She eats 31 servings of ice cream a month and there are 60 days in 2 months so she eats 31*2 = \\(\\ll\\)31*2=62*62 servings of ice cream in 2 months. Each carton has 15 servings and she needs 62 servings so she needs 62/15 = \\(\\ll\\)62/15=\\(\\ll\\)4\\(\\ll\\)4 cartons of ice cream. Each carton costs $4.00 and she needs 4 cartons so she will spend 4*4 = \\(\\ll\\)*4*4=16.00*16.00 on ice cream ### 16\n' +
      '\n' +
      '**GPT-4 Judgement:** The solution incorrectly assumes that there are exactly 31 days in each of the 2 months comprising the 60 days, which is not necessarily true. However, it does not affect the final calculation as they incorrectly calculate the total servings as 31*2=62 servings in 60 days, which again is not correct but coincidentally leads to the correct number of cartons needed since 60 servings would also require 4 cartons. The final multiplication step is correct and gives the right total cost.\n' +
      '\n' +
      '**Error:** Incoherent Reasoning\n' +
      '\n' +
      '**QUESTION:** Rory orders 2 subs for $7.50 each, 2 bags of chips for $1.50 each and 2 cookies for $1.00 each for delivery. There\'s a 20% delivery fee added at check out and she wants to add a $5.00 tip. What will her delivery order cost?\n' +
      '\n' +
      '**MODEL ANSWER:** The subs are $7.50 each so 2 will cost 2*7.50 = $\\(\\ll\\)7.5*2=15.00\\(\\gg\\)15.00. The chips are $1.50 each so 2 will cost 2*1.50 = \\(\\ll\\)*2*1.5=3.00\\(\\gg\\)3.00. The cookies are $1.00 each so 2 will cost 2*1 = \\(\\ll\\)*2*1=2.00\\(\\gg\\)2.00. The subs are $15.00, the chips are $3.00 and the cookies are $2.00 for a total of 15+3+2 = \\(\\ll\\)*15+3+2=20.00\\(\\gg\\)20.00. There is a 20% delivery fee added to her order so 20*.05 =.40. Her order is $20.00 and the delivery fee is$4.00 for a total of 20+4 = \\(\\ll\\)*20+4=24.00\\(\\gg\\)24.00.She wants to add a $5.00 tip to her $24.00 order so 24+5 =\\(\\ll\\)*24+5=29.00\\(\\gg\\)29.00. ### 29\n' +
      '\n' +
      '**GPT-4 Judgement:** The calculation for the cost of the subs, chips, and cookies is correct, as well as the sum of those items. However, the calculation of the delivery fee is incorrect; the solution incorrectly uses a 5% rate (.05) instead of the correct 20% rate (.20). The subsequent steps that build on this mistake are also incorrect, including the final total cost which should be $29.00 instead of $24.00. The correct steps are the individual costs of subs, chips, cookies, and their total before the delivery fee is added, while the steps involving the delivery fee and the final total are incorrect.\n' +
      '\n' +
      '**Error:** Incoherent Reasoning\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**QUESTION:** Cynthia eats one serving of ice cream every night. She buys cartons of ice cream with 15 servings of ice cream per carton at a cost of $4.00 per carton. After 60 days, how much will she spend on ice cream? \\\\ \\hline\n' +
      '**MODEL ANSWER:** She eats one serving of ice cream a night and there are 31 days in a month so she eats 1*31 = \\(\\ll\\)1*31=31*31 servings of ice cream a month. She eats 31 servings of ice cream a month and there are 60 days in 2 months so she eats 31*2 = \\(\\ll\\)31*2=62\\(\\ll\\)62 servings of ice cream in 2 months. Each carton has 15 servings and she needs 62 servings so she needs 62/15 = \\(\\ll\\)62/15=\\(\\ll\\)4\\(\\ll\\)4 cartons of ice cream. Each carton costs $4.00 and she needs 4 cartons so she will spend 4*4 = \\(\\ll\\)*4*4=16.00\\(\\gg\\)16.00 on ice cream ### 16 \\\\ \\hline\n' +
      '**GPT-4 Judgement:** The solution incorrectly assumes that there are exactly 31 days in each of the 2 months comprising the 60 days, which is not necessarily true. However, it does not affect the final calculation as they incorrectly calculate the total servings as 31*2=62 servings in 60 days, which again is not correct but coincidentally leads to the correct number of cartons needed since 60 servings would also require 4 cartons. The final multiplication step is correct and gives the right total cost. \\\\ \\hline\n' +
      '**Error:** Incoherent Reasoning\n' +
      '\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**QUESTION:** Rory orders 2 subs for $7.50 each, 2 bags of chips for $1.50 each and 2 cookies for $1.00 each for delivery. There’s a 20% delivery fee added at check out and she wants to add a $5.00 tip. What will her delivery order cost? \\\\ \\hline\n' +
      '**MODEL ANSWER:** The subs are $7.50 each so 2 will cost 2*7.50 = \\(\\ll\\)*7.5*2=15.00\\(\\gg\\)15.00. The chips are $1.50 each so 2 will cost 2*1.50 = \\(\\ll\\)*2*1.5=3.00\\(\\gg\\)3.00. The cookies are $1.00 each so 2 will cost 2*1 = \\(\\ll\\)*2*1=2.00\\(\\gg\\)2.00. The subs are $15.00, the chips are $3.00 and the cookies are $2.00 for a total of 15+3+2 = \\(\\ll\\)*15+3+2=20.00\\(\\gg\\)20.00. There is a 20% delivery fee added to her order so 20*.05 =.40. Her order is $20.00 and the delivery fee is$4.00 for a total of 20+4 = \\(\\ll\\)*20+4=24.00\\(\\gg\\)24.00.She wants to add a $5.00 tip to her $24.00 order so 24+5 =\\(\\ll\\)*24+5=29.00\\(\\gg\\)29.00. ### 29\n' +
      '\n' +
      '**GPT-4 Judgement:** The calculation for the cost of the subs, chips, and cookies is correct, as well as the sum of those items. However, the calculation of the delivery fee is incorrect; the solution incorrectly uses a 5% rate (.05) instead of the correct 20% rate (.20). The subsequent steps that build on this mistake are also incorrect, including the final total cost which should be $29.00 instead of $24.00. The correct steps are the individual costs of subs, chips, cookies, and their total before the delivery fee is added, while the steps involving the delivery fee and the final total are incorrect.\n' +
      '\n' +
      '**Error:** Incoherent Reasoning\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**Error:** Incoherent Reasoning \\\\ \\hline\n' +
      '**MODEL ANSWER:** The subs are $7.50 each so 2 will cost 2*7.50 = \\(\\ll\\)*7.5*2=15.00\\(\\gg\\)15.00. The chips are $1.50 each so 2 will cost 2*1.50 = \\(\\ll\\)*2*1.5=3.00\\(\\gg\\)3.00. The cookies are $1.00 each so 2 will cost 2*1= \\(\\ll\\)*2*1=2.00\\(\\gg\\)2.00. The subs are $15.00, the chips are $3.00 and the cookies are $2.00 for a total of 15+3+2 = \\(\\ll\\)*15+3+2=20.00\\(\\gg\\)20.00. There is a 20% delivery fee added to her order so 20*.05 =.40. Her order is $20.00 and the delivery fee is$4.00 for a total of 20+4 = \\(\\ll\\)*20+4=24.00\\(\\gg\\)24.00.She wants to add a $5.00 tip to her $24.00 order so 24+5 =\\(\\ll\\)*24+5=29.00\\(\\gg\\)29.00. ### 29\n' +
      '\n' +
      '\\end{tabular}\n' +
      '\n' +
      '**GPT-4 Judgement:** The calculation for the cost of the subs, chips, and cookies is correct, as well as the sum of those items. However, the calculation of the delivery fee is incorrect; the solution incorrectly uses a 5% rate (.05) instead of the correct 20% rate (.20). The subsequent steps that build on this mistake are also incorrect, including the final total cost which should be $29.00 instead of $24.00. The correct steps are the individual costs of subs, chips, cookies, and their total before the delivery fee is added, while the steps involving the delivery fee and the final total are incorrect.\n' +
      '\n' +
      '**Error:** Incoherent Reasoning\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**MODEL ANSWER:** The subs are $7.50 each so 2 will cost 2*7.50 = \\(\\ll\\)*7.5*2=15.00\\(\\gg\\)15.00. The chips are $1.50 each so 2 will cost 2*1.50 = \\(\\ll\\)*2*1.5=3.00\\(\\gg\\)3.00. The cookies are $1.00 each so 2 will cost 2*1= \\(\\ll\\)*2*1=2.00\\(\\gg\\)2.00. The subs are $15.00, the chips are $3.00 and the cookies are $2.00 for a total of 15+3+2 = \\(\\ll\\)*15+3+2=20.00\\(\\gg\\)20.00. There is a 20% delivery fee added to her order so 20*.05 =.40. Her order is $20.00 and the delivery fee is$4.00 for a total of 20+4 = \\(\\ll\\)*20+4=24.00\\(\\gg\\)24.00.She wants to add a $5.00 tip to her $24.00 order so 24+5 =\\(\\ll\\)*24+5=29.00\\(\\gg\\)29.00. ### 29\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline\n' +
      '**QUESTION:** James buys a ring for his bride-to-be. The diamond cost \\(600andthegoldcost300\\). He pays a 30\\% premium for it to be made. How much did he pay? \\\\ \\hline\n' +
      '**SFT ANSWER:** The total cost was 600+300=$\\(\\diamond\\)600+300=900\\(\\triangleright\\)900 \\\\ So the premium was 900*.3=$\\(\\diamond\\)900*.3=270\\(\\triangleright\\)270 \\\\ So he paid 900+270=$\\(\\diamond\\)900+270=1170\\(\\triangleright\\)1170 \\\\\n' +
      '**ffff** 1170 \\\\ \\hline\n' +
      '**MFT ANSWER:** The premium cost 600*.3=$\\(\\diamond\\)600*.3=180\\(\\triangleright\\)180 \\\\ So the total cost was 600+300+180=$\\(\\diamond\\)600+300+180=1080\\(\\triangleright\\)1080 \\\\\n' +
      '**ffff** 1080 \\\\ \\hline\n' +
      '**Error:** Missing Steps. \\\\ \\hline \\hline\n' +
      '**QUESTION:** To make a call from a payphone, you must pay $0.25 for each minute of your call. After 16 minutes, that price drops to $0.2 per minute. How much would a 36-minute call cost? \\\\ \\hline\n' +
      '**SFT ANSWER:** The first 16 minutes would cost 16 * $0.25 = $\\(\\diamond\\)16*0.25=4\\(\\triangleright\\)4. \\\\ The remaining 36 - 16 = \\(\\diamond\\)36-16=20\\(\\triangleright\\)20 minutes would cost $0.2 * 20 = $\\(\\diamond\\)0.2*20=4\\(\\triangleright\\)4. \\\\ Thus, a 36-minute call would cost $4 + $4 = $\\(\\diamond\\)4+4=8\\(\\triangleright\\)8. \\\\\n' +
      '**ffff** 8 \\\\ \\hline\n' +
      '**MFT ANSWER:** At $0.25 per minute, the first 16 minutes would cost 16 * 0.25 = $\\(\\diamond\\)16*0.25=4\\(\\triangleright\\)4. \\\\ The remaining 36 minutes would cost 36 / 4 = $\\(\\diamond\\)36/4=9\\(\\triangleright\\)9. \\\\ In total, a 36-minute call would cost 4 + 9 = $\\(\\diamond\\)4+9=13\\(\\triangleright\\)13. \\\\\n' +
      '**ffff** 13 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: Cases where SFT accurately solves while MFT fails under the same setting of Tab. 12Experiments on Alpaca\n' +
      '\n' +
      '## Appendix H Experiments on Alpaca\n' +
      '\n' +
      '## Appendix I The impact on length\n' +
      '\n' +
      'We investigate whether MFT has a preference for problems of different solution lengths. The horizontal axis represents the ground truth length. The experiment shows that MFT and RFT are essentially consistent, demonstrating improvements for solutions of different lengths.\n' +
      '\n' +
      '## Appendix J Error Types\n' +
      '\n' +
      'These errors are cited from Wei et al. (2022):\n' +
      '\n' +
      'Calculator Error (CE). The chains of thought were completely correct except for a calculator error--in other words, applying an external calculator to equations would make the chain of thought correct.\n' +
      '\n' +
      'One Step Missing Error (OSME). Chains of thought which were correct except that they were missing a single step. These chains of thoughts could be rewritten to be correct by adding in an additional reasoning step that was missed.\n' +
      '\n' +
      'Semantic Understanding Error (SUE). There are errors in semantic understanding of the problem.\n' +
      '\n' +
      'Incoherent steps error (ISE). Incoherent chain of thoughts. Some steps in the generated chain of thought did not follow from prior ones.\n' +
      '\n' +
      '## Appendix K Detail of compared regularization methods\n' +
      '\n' +
      'Dropout: Dropout is a widely used Method to reduce overfitting Srivastava et al. (2014). We compared with Embedding Dropout which dropout each dimension of the token embedding. For fair comparison, we only add it to the input token.\n' +
      '\n' +
      'NEFtune: Adding random noise to the token embedding vectors. Its experiments show that a small scale of noise can enhance the capability of LLM conversation and improve Alpaca-eval scores Jain et al. (2023). In its original setup, noise is added to all input embeddings, including both source and target.\n' +
      '\n' +
      'Scheduled Sampling: We utilize the Scheduled Sampling technique for Transformers Mihaylova and Martins (2019), which entails two forward passes. The first forward pass predicts the next token for each\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model Name & Alpaca-eval & Length \\\\ \\hline \\hline llama7B\\_Alpaca\\_mft & 23.00 & 414 \\\\ \\hline llama7B\\_Alpaca\\_sft & 23.00 & 394 \\\\ \\hline llama7B\\_Alpaca\\_NEFTune5 & 63.93 & 1063 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 14: Comparison of different methods trained on Alpaca. MFT does not hurt the performance but also has no gain.\n' +
      '\n' +
      'Figure 15: The impact on length\n' +
      '\n' +
      'position, and the input for the second forward pass incorporates some of the predicted tokens from the first pass. In determining which tokens to replace, we employ a sampling strategy characterized by the parameter \\(\\tau\\), representing temperature; a higher \\(\\tau\\) introduces more randomness. At lower temperatures, there is a higher likelihood of replacing tokens with their synonyms, as opposed to random replacement. MaskedLM. In our main experiments, we implement MFT by this method, adhering to the methodologies of prior Masked Language Models as (Bao et al., 2020; Devlin et al., 2019). Unlike encoder-based models (Devlin et al., 2019), which typically predict the current token at masked positions, we predict the next token at masked positions (Xie et al., 2016; Chen et al., 2023). Upon determining the positions for masking, we are presented with two options: employing a [mask] token or substituting with a random token. The probability of selecting the [mask] token is denoted by \\(m\\), and \\(r\\) represents the probability of opting for a random token. Su et al. (2022) set \\(r\\) to 0, while Devlin et al. (2019) set \\(r\\) to 0.1.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
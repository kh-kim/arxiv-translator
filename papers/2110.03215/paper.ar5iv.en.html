<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.03215] Towards Continual Knowledge Learning of Language Models</title><meta property="og:description" content="Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as q…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Continual Knowledge Learning of Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Continual Knowledge Learning of Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.03215">

<!--Generated on Wed Dec 14 17:43:02 2022 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Continual Knowledge Learning of Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joel Jang<sup id="id1.1.id1" class="ltx_sup">1</sup> Seonghyeon Ye<sup id="id2.2.id2" class="ltx_sup">1</sup>  Sohee Yang<sup id="id3.3.id3" class="ltx_sup">1</sup>  Joongbo Shin<sup id="id4.4.id4" class="ltx_sup">2</sup> 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_bold">Janghoon Han<sup id="id5.5.id5.1" class="ltx_sup">2</sup>  Gyeonghun Kim<sup id="id5.5.id5.2" class="ltx_sup">2</sup>  Stanley Jungkyu Choi<sup id="id5.5.id5.3" class="ltx_sup">2</sup>  Minjoon Seo<sup id="id5.5.id5.4" class="ltx_sup">1</sup></span> 
<br class="ltx_break"><sup id="id6.6.id6" class="ltx_sup">1</sup>KAIST AI <sup id="id7.7.id7" class="ltx_sup">2</sup>LG AI Research 
<br class="ltx_break"><span id="id8.8.id8" class="ltx_text ltx_font_typewriter">{joeljang,vano1205,sohee.yang,minjoon}@kaist.ac.kr</span> 
<br class="ltx_break"><span id="id9.9.id9" class="ltx_text ltx_font_typewriter">{jb.shin,janghoon.han,ghkayne.kim,stanleyjk.choi}@lgresearch.ai</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, model checkpoints, and code to reproduce our results are available at <a target="_blank" href="https://github.com/joeljang/continual-knowledge-learning" title="" class="ltx_ref ltx_href">this https URL</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Recent works have shown that large Language Models (LM), such as T5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> and GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, have the capability of storing a tremendous amount of world knowledge in their parameters when pretrained on a vast corpus of text&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>. These pretrained LMs have shown potential to serve as knowledge bases when probed for world knowledge without any finetuning through the LAnguage Model Analysis (LAMA) task&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>, which requires probing LMs for world knowledge in a zero-shot manner through slot-filling, and promising results utilizing the encoded world knowledge when finetuned on various Knowledge Intensive Language Tasks (KILT)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>, e.g., question answering, knowledgeable open dialogues.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">While the world knowledge stored in LMs has diverse use cases, it can quickly become outdated as the world changes fast, and LMs need to frequently renew their internal world knowledge accordingly. For example, it is impossible to probe for <span id="S1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">new</span> information such as “<span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span><span id="S1.p2.1.2" class="ltx_text ltx_font_italic"> won the US Election 2020</span>” from the original T5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> which was pretrained on C4 web corpus from April 2019.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>T5 was initially pretrained on the C4 dataset (about 750 GB), which is a cleansed dump of Common Crawl extracted from the web in April 2019.</span></span></span> Also, information that may have once been considered accurate may no longer be valid because the information has been <span id="S1.p2.1.3" class="ltx_text ltx_font_bold ltx_font_italic">updated</span>. For instance, the answer to “<span id="S1.p2.1.4" class="ltx_text ltx_font_italic">Which soccer team does Cristiano Ronaldo play for?</span>” has changed from <span id="S1.p2.1.5" class="ltx_text ltx_font_italic">Juventus</span> to <span id="S1.p2.1.6" class="ltx_text ltx_font_italic">Manchester United</span> in September 2021. Meanwhile, <span id="S1.p2.1.7" class="ltx_text ltx_font_bold ltx_font_italic">time-invariant</span> information learned from the original corpus such as “<span id="S1.p2.1.8" class="ltx_text ltx_font_italic">Barack Obama was born in Honolulu, Hawaii</span>” should not be altered within the LMs.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Despite its importance, the challenge of renewing the internal world knowledge stored in the parameters of LMs is nontrivial and has only been explored in rather specific settings. For example, recent works have proposed to modify specific target knowledge such as individual facts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(De&nbsp;Cao et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>; Zhu et&nbsp;al., <a href="#bib.bib59" title="" class="ltx_ref">2020</a>; Dai et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Dhingra et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> have addressed LMs as temporal knowledge bases by jointly modeling text with its timestamp. But the problem of renewing the world knowledge of LMs in a more general and scalable way, such as through continual pretraining on a corpus with new knowledge, has not been formally formulated or explored by previous works. Moreover, the community lacks a benchmark that can be used to systematically study how the internal knowledge of LMs changes through the training on new information. Lastly, methodologies to effectively renew the knowledge of LMs at scale have yet to be thoroughly explored.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.03215/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the <span id="S1.F1.16.1" class="ltx_text ltx_font_smallcaps">Continual Knowledge Learning</span> benchmark. <span id="S1.F1.17.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> is used to measure the <span id="S1.F1.18.3" class="ltx_text ltx_font_italic">time-invariant</span> world knowledge gained from <math id="S1.F1.5.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S1.F1.5.m1.1b"><msub id="S1.F1.5.m1.1.1" xref="S1.F1.5.m1.1.1.cmml"><mi id="S1.F1.5.m1.1.1.2" xref="S1.F1.5.m1.1.1.2.cmml">D</mi><mn id="S1.F1.5.m1.1.1.3" xref="S1.F1.5.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F1.5.m1.1c"><apply id="S1.F1.5.m1.1.1.cmml" xref="S1.F1.5.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.5.m1.1.1.1.cmml" xref="S1.F1.5.m1.1.1">subscript</csymbol><ci id="S1.F1.5.m1.1.1.2.cmml" xref="S1.F1.5.m1.1.1.2">𝐷</ci><cn type="integer" id="S1.F1.5.m1.1.1.3.cmml" xref="S1.F1.5.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m1.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.5.m1.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. <span id="S1.F1.19.4" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> is used to measure the <span id="S1.F1.20.5" class="ltx_text ltx_font_italic">update</span> of world knowledge from <math id="S1.F1.6.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S1.F1.6.m2.1b"><msub id="S1.F1.6.m2.1.1" xref="S1.F1.6.m2.1.1.cmml"><mi id="S1.F1.6.m2.1.1.2" xref="S1.F1.6.m2.1.1.2.cmml">D</mi><mn id="S1.F1.6.m2.1.1.3" xref="S1.F1.6.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F1.6.m2.1c"><apply id="S1.F1.6.m2.1.1.cmml" xref="S1.F1.6.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.6.m2.1.1.1.cmml" xref="S1.F1.6.m2.1.1">subscript</csymbol><ci id="S1.F1.6.m2.1.1.2.cmml" xref="S1.F1.6.m2.1.1.2">𝐷</ci><cn type="integer" id="S1.F1.6.m2.1.1.3.cmml" xref="S1.F1.6.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m2.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.6.m2.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> →<math id="S1.F1.7.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S1.F1.7.m3.1b"><msub id="S1.F1.7.m3.1.1" xref="S1.F1.7.m3.1.1.cmml"><mi id="S1.F1.7.m3.1.1.2" xref="S1.F1.7.m3.1.1.2.cmml">D</mi><mn id="S1.F1.7.m3.1.1.3" xref="S1.F1.7.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F1.7.m3.1c"><apply id="S1.F1.7.m3.1.1.cmml" xref="S1.F1.7.m3.1.1"><csymbol cd="ambiguous" id="S1.F1.7.m3.1.1.1.cmml" xref="S1.F1.7.m3.1.1">subscript</csymbol><ci id="S1.F1.7.m3.1.1.2.cmml" xref="S1.F1.7.m3.1.1.2">𝐷</ci><cn type="integer" id="S1.F1.7.m3.1.1.3.cmml" xref="S1.F1.7.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.m3.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.7.m3.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. <span id="S1.F1.21.6" class="ltx_text ltx_font_smallcaps">NewLAMA</span> is used to measure <span id="S1.F1.22.7" class="ltx_text ltx_font_italic">new</span> world knowledge gained from <math id="S1.F1.8.m4.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S1.F1.8.m4.1b"><msub id="S1.F1.8.m4.1.1" xref="S1.F1.8.m4.1.1.cmml"><mi id="S1.F1.8.m4.1.1.2" xref="S1.F1.8.m4.1.1.2.cmml">D</mi><mn id="S1.F1.8.m4.1.1.3" xref="S1.F1.8.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F1.8.m4.1c"><apply id="S1.F1.8.m4.1.1.cmml" xref="S1.F1.8.m4.1.1"><csymbol cd="ambiguous" id="S1.F1.8.m4.1.1.1.cmml" xref="S1.F1.8.m4.1.1">subscript</csymbol><ci id="S1.F1.8.m4.1.1.2.cmml" xref="S1.F1.8.m4.1.1.2">𝐷</ci><cn type="integer" id="S1.F1.8.m4.1.1.3.cmml" xref="S1.F1.8.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.m4.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.8.m4.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In this work, we propose a novel continual learning (CL) formulation named <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">Continual Knowledge Learning (CKL)</span>, where we attempt to renew the internal world knowledge of LMs through continual pretraining on new corpora. We systematically categorize world knowledge into three main categories and make benchmark datasets to measure each of them during CKL: (1) <span id="S1.p4.1.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> for <span id="S1.p4.1.3" class="ltx_text ltx_font_bold ltx_font_italic">time-invariant</span> world knowledge in LMs that should not be forgotten or altered, (2) <span id="S1.p4.1.4" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> for outdated world knowledge that needs to be <span id="S1.p4.1.5" class="ltx_text ltx_font_bold ltx_font_italic">updated</span> in the LMs, and (3) <span id="S1.p4.1.6" class="ltx_text ltx_font_smallcaps">NewLAMA</span> for <span id="S1.p4.1.7" class="ltx_text ltx_font_bold ltx_font_italic">new</span> world knowledge that should be injected into the LMs. We also propose a novel metric named <span id="S1.p4.1.8" class="ltx_text ltx_font_smallcaps">FUAR</span><span id="S1.p4.1.9" class="ltx_text ltx_font_smallcaps"> (<span id="S1.p4.1.9.1" class="ltx_text ltx_font_bold">F</span>orgotten / (<span id="S1.p4.1.9.2" class="ltx_text ltx_font_bold">U</span>pdated + <span id="S1.p4.1.9.3" class="ltx_text ltx_font_bold">A</span>cquired) <span id="S1.p4.1.9.4" class="ltx_text ltx_font_bold">R</span>atio)</span> that can measure the trade-off between forgetting, updating, and acquiring knowledge. Finally, while one might think of implementing contemporary CL methods for this benchmark, we show that CKL has nontrivial differences to traditional CL formulations and require approaches specific to CKL. We find and compare model architectures and training methodologies&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; He et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>; Hu et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Wang et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> from the literature that have shown potential to mitigate forgetting of knowledge gained during pretraining, establishing them as baselines for the CKL benchmark.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In sum, while the challenge of renewing the internal world knowledge of LMs is essential in real-world scenarios, it has yet to be formulated or extensively explored. Therefore, in this paper:</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel CL formulation called <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Continual Knowledge Learning (CKL)</span> and construct a new benchmark to measure the amount of forgetting and amount of world knowledge gained by continued pretraining on a novel language modeling corpus that we construct, containing new knowledge.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i2.p1.1" class="ltx_p">We explore LM architectures and training methodologies that are natural baselines for CKL in literature, denoting them as CKL methods, and performing extensive experiments on our CKL benchmark. We categorize them into regularization, rehearsal, and parameter-expansion methods, same as in traditional CL literature, and compare the effectiveness of each type of method using a novel metric named <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">FUAR</span> that we propose to measure the trade-off between forgotten knowledge and updated or acquired knowledge.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">Towards creating an ever-changing LM, we perform extensive analysis in the CKL benchmark and highlight important challenges and findings: parameter-expansion methods have the limitation of memory inefficiency despite performing the best in most of our experiments and seeing the same data repeatedly during continued pretraining is a critical cause of forgetting. Also, we show interesting results that need further exploration: learning rate can be varied to balance the forgetting and learning of new knowledge, CKL may help in performing previous-knowledge-intensive tasks after gaining new world knowledge, and CKL methods are transferable across LM architectures despite showing a different trend in performance.</p>
</div>
</li>
</ul>
<p id="S1.p6.1" class="ltx_p">An overview of the proposed CKL benchmark is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Language Models (LMs) utilizing knowledge from external sources, such as Retrieval-Augmented Generation (RAG)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> and Blender Bot 2.0&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a href="#bib.bib54" title="" class="ltx_ref">2021</a>; Komeili et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>, cope with the changing world by updating the external sources during inference or searching the internet for retrieving recent information. However, recent works have shown that these memory-augmented models suffer from <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">hallucination</span>, which means that they present false information as if it were correct, despite being given updated knowledge during inference&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Choi, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite>, which worsens as the size of the LM increases&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Longpre et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, making it more so important for implicit parameters to be renewed as well.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">In order to renew the internal knowledge of LMs, one might consider pretraining LMs from scratch with a newly updated text corpus of a scale similar to the one used during initial pretraining, such as a recent dump of the entire Wikipedia. However, this approach is computationally demanding and also environmentally harmful&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Patterson et&nbsp;al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. Another alternative approach is continuing the pretraining process on a much smaller corpus containing new world knowledge, but such a methodology is known to suffer from <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">catastrophic forgetting</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(McCloskey &amp; Cohen, <a href="#bib.bib35" title="" class="ltx_ref">1989</a>; Kirkpatrick et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>, where the models forget previously learned knowledge as they acquire new knowledge.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lazaridou et&nbsp;al. (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>); Jin et&nbsp;al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> suggests implementing prior Continual Learning (CL) methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>; d’Autume et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> to address this problem. However, it is important to note that there are nontrivial differences between traditional CL and the proposed Continual Knowledge Learning (CKL) formulation which make applying traditional CL methods inadequate. In traditional CL, methods can be largely categorized into <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">regularization</span>, <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">rehearsal</span>, and <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">parameter-expansion</span> methods. (1) While regularization methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> require identifying important parameters used for previous tasks, exactly how and where the knowledge is stored in the parameters of an LM is currently extremely difficult to identify and localize&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vig et&nbsp;al., <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; De&nbsp;Cao et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>. (2) While prior rehearsal methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lopez-Paz &amp; Ranzato, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> consider learning all of the streams of tasks at once (multi-task learning) as the performance upper-bound and replicate such a setting with samples stored in the episodic memory, a few samples from the pretraining corpus cannot represent the overall world knowledge from the corpus. Moreover, if LMs are pretrained on a shuffled concatenation of stream of corpora, there is no guarantee that the LMs will acquire the correct, recent information from the recent corpora, especially in cases where the former corpora are much bigger than the latter ones, which is shown by experiments in Section <a href="#S5.SS1" title="5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. (3) Lastly, prior parameter-expansion methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rusu et&nbsp;al., <a href="#bib.bib44" title="" class="ltx_ref">2016</a>; Yoon et&nbsp;al., <a href="#bib.bib56" title="" class="ltx_ref">2018</a>)</cite> focus on <em id="S2.p3.1.4" class="ltx_emph ltx_font_italic">learning a stream of different tasks via strong supervision</em>, while in CKL, the focus is <em id="S2.p3.1.5" class="ltx_emph ltx_font_italic">constantly updating world knowledge from a stream of corpora via self-supervision</em>.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">Because of these fundamental differences, instead of contemporary CL methods mentioned above, we explore methodologies from the literature that are suitable for CKL&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; He et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>; Hu et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Wang et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite>, modifying and adapting each method according to our needs as CKL methods. Lastly, while it has been pointed out that some of the traditional CL formulations may have little practical importance in real-world scenarios by <cite class="ltx_cite ltx_citemacro_citet">Prabhu et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, CKL is much closer to the initial motivation behind CL, which is that the “fundamental characteristic of natural intelligence is its ability to continually learn new knowledge while updating information about the old ones”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Prabhu et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>. Details of related works regarding the traditional CL methods and how CKL methods address the fundamental differences are provided in Appendix <a href="#A1" title="Appendix A Extension of Related Works ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Continual Knowledge Learning (CKL)</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we explain the formulation of the task, the data construction process, and the proposed metric measuring the trade-off between forgetting previous world knowledge and updating and learning of new world knowledge.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Task Formulation</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.2" class="ltx_p">When viewing the task of renewing the internal knowledge of LMs as one of CL formulations, pretraining on the original corpus can be considered as a <span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_italic">previous task</span>, and continued pretraining on new corpus can be considered as the <span id="S3.SS1.p1.2.2" class="ltx_text ltx_font_italic">current task</span>, the main objective becoming retaining the <span id="S3.SS1.p1.2.3" class="ltx_text ltx_font_italic">time-invariant</span> world knowledge gained through initial pretraining while efficiently learning <span id="S3.SS1.p1.2.4" class="ltx_text ltx_font_italic">new</span> and <span id="S3.SS1.p1.2.5" class="ltx_text ltx_font_italic">updated</span> world knowledge through continued pretraining. Throughout the paper, we let <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">D</mi><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> refer to the corpus used for initial pretraining and let <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">D</mi><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> denote the new corpus used for continued pretraining.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">New Text Corpus for Language Modeling</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px1.p1.6" class="ltx_p">For LMs to renew their internal knowledge, they need to be continually pretrained on a new text corpus <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> which has the updated and new information. <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> should ideally be much smaller than <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, as a large <math id="S3.SS1.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> amounting to the size of <math id="S3.SS1.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m5.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> will result in massive computational costs similar to pretraining the LMs from scratch. For constructing <math id="S3.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m6.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, we crawl recently published news articles from the web making <span id="S3.SS1.SSS0.Px1.p1.6.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span>.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span id="footnote2.2" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> consists of 221,779 articles (<span id="footnote2.1" class="ltx_text" style="position:relative; bottom:0.7pt;"><math id="footnote2.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\sim" display="inline"><semantics id="footnote2.1.1.m1.1b"><mo mathsize="70%" id="footnote2.1.1.m1.1.1" xref="footnote2.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="footnote2.1.1.m1.1c"><csymbol cd="latexml" id="footnote2.1.1.m1.1.1.cmml" xref="footnote2.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="footnote2.1.1.m1.1d">\scriptstyle\sim</annotation><annotation encoding="application/x-llamapun" id="footnote2.1.1.m1.1e">∼</annotation></semantics></math></span>168M tokens), which is estimated to be about 750 times smaller than C4, a cleansed version of the April 2019 Common Crawl dataset (<a target="_blank" href="https://commoncrawl.org/" title="" class="ltx_ref ltx_href">https://commoncrawl.org/</a>) that was used to initially pretrain the T5 LM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>.</span></span></span></p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Probing LMs for World Knowledge</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The most widely used task for probing LMs for world knowledge is the LAnguage Model Analysis (LAMA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> task, which consists of cloze sentences created from a set of knowledge sources using manually defined templates. We define that an LM <span id="S3.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">knows</span> a fact if it can successfully predict in a zero-shot manner the masked entity in the cloze sentence, such as “<span id="S3.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">Dante was born in <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span></span>” as <span id="S3.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">Florence</span>. While there may be other alternatives for measuring the world knowledge encoded in LMs<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Closed-book question answering (CBQA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Roberts et&nbsp;al., <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> can also be considered as a task that measures the world knowledge of LMs through finetuning, but it has been pointed out that much of its performance increases are due to the test-train overlap&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Wang et&nbsp;al., <a href="#bib.bib51" title="" class="ltx_ref">2021a</a>)</cite> in the datasets.</span></span></span>, we construct our main datasets as LAMA tasks, while also additionally providing the corresponding question pairs to the cloze sentences for those who want to test on CBQA as well.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Measuring Retention of Time-invariant World Knowledge</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p1.5" class="ltx_p">We define <span id="S3.SS1.SSS0.Px3.p1.5.1" class="ltx_text ltx_font_italic">time-invariant</span> world knowledge as the information present in <math id="S3.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> that has no possibility of conflicting with information from <math id="S3.SS1.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. For example, if the information of the <span id="S3.SS1.SSS0.Px3.p1.5.2" class="ltx_text ltx_font_italic">birthplace of Barack Obama</span> is present in <math id="S3.SS1.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, it is unlikely that <math id="S3.SS1.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.4.m4.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> contains information that contradicts that fact. Also, we classify instances where the time-stamps are fixed such as “<span id="S3.SS1.SSS0.Px3.p1.5.3" class="ltx_text ltx_font_italic">Cristiano Ronaldo played for <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> in 2010.</span>” as <span id="S3.SS1.SSS0.Px3.p1.5.4" class="ltx_text ltx_font_italic">time-invariant</span>. These <span id="S3.SS1.SSS0.Px3.p1.5.5" class="ltx_text ltx_font_italic">time-invariant</span> instances should not be changed as LMs are continually pretrained on <math id="S3.SS1.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.5.m5.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. In order to measure how much <span id="S3.SS1.SSS0.Px3.p1.5.6" class="ltx_text ltx_font_italic">time-invariant</span> information is lost due to <span id="S3.SS1.SSS0.Px3.p1.5.7" class="ltx_text ltx_font_italic">catastrophic forgetting</span> during continued pretraining, we create <span id="S3.SS1.SSS0.Px3.p1.5.8" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, a subset of LAMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>, consisting of only <span id="S3.SS1.SSS0.Px3.p1.5.9" class="ltx_text ltx_font_italic">time-invariant</span> cloze sentences detailed in Appendix <a href="#A2.SS1" title="B.1 Time-invariant relations of LAMA ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Measuring Update of Outdated World Knowledge</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px4.p1.11" class="ltx_p">In this work, we define <span id="S3.SS1.SSS0.Px4.p1.11.1" class="ltx_text ltx_font_italic">outdated</span> world knowledge as information that is conflicting between <math id="S3.SS1.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. For example, the President of the US may be <span id="S3.SS1.SSS0.Px4.p1.11.2" class="ltx_text ltx_font_italic">Barack Obama</span> in <math id="S3.SS1.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.3.m3.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <span id="S3.SS1.SSS0.Px4.p1.11.3" class="ltx_text ltx_font_italic">Joe Biden</span> in <math id="S3.SS1.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px4.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.4.m4.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. In this case, the LM should update its internal knowledge as <span id="S3.SS1.SSS0.Px4.p1.11.4" class="ltx_text ltx_font_italic">Joe Biden</span> as the US president. If an LM is pretrained on both <math id="S3.SS1.SSS0.Px4.p1.5.m5.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px4.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.5.m5.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px4.p1.6.m6.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px4.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.6.m6.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> simultaneously, there is no guarantee that the LM will acquire the correct, recent information from <math id="S3.SS1.SSS0.Px4.p1.7.m7.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.7.m7.1a"><msub id="S3.SS1.SSS0.Px4.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.7.m7.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.7.m7.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, especially in cases where <math id="S3.SS1.SSS0.Px4.p1.8.m8.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.8.m8.1a"><msub id="S3.SS1.SSS0.Px4.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.8.m8.1.1.2" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.8.m8.1.1.3" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.8.m8.1b"><apply id="S3.SS1.SSS0.Px4.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.8.m8.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.8.m8.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.8.m8.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.8.m8.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.8.m8.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.8.m8.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is much bigger than <math id="S3.SS1.SSS0.Px4.p1.9.m9.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.9.m9.1a"><msub id="S3.SS1.SSS0.Px4.p1.9.m9.1.1" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.9.m9.1.1.2" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.9.m9.1.1.3" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.9.m9.1b"><apply id="S3.SS1.SSS0.Px4.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.9.m9.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.9.m9.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.9.m9.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.9.m9.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.9.m9.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, which is one of the biggest difference between the CKL and traditional CL setting. For measuring <span id="S3.SS1.SSS0.Px4.p1.11.5" class="ltx_text ltx_font_italic">update</span> of outdated information, we construct <span id="S3.SS1.SSS0.Px4.p1.11.6" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> which is made up of cloze statements for which answers can be found in both <math id="S3.SS1.SSS0.Px4.p1.10.m10.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.10.m10.1a"><msub id="S3.SS1.SSS0.Px4.p1.10.m10.1.1" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.10.m10.1.1.2" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.10.m10.1.1.3" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.10.m10.1b"><apply id="S3.SS1.SSS0.Px4.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.10.m10.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.10.m10.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.10.m10.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.10.m10.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.10.m10.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px4.p1.11.m11.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.11.m11.1a"><msub id="S3.SS1.SSS0.Px4.p1.11.m11.1.1" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.11.m11.1.1.2" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px4.p1.11.m11.1.1.3" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.11.m11.1b"><apply id="S3.SS1.SSS0.Px4.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.11.m11.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.11.m11.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px4.p1.11.m11.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.11.m11.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.11.m11.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.11.m11.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, but are conflicting.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Measuring Acquisition of New World Knowledge</h5>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px5.p1.7" class="ltx_p">We define <span id="S3.SS1.SSS0.Px5.p1.7.1" class="ltx_text ltx_font_italic">new</span> world knowledge as the information present in <math id="S3.SS1.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, but not in <math id="S3.SS1.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px5.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. To measure <span id="S3.SS1.SSS0.Px5.p1.7.2" class="ltx_text ltx_font_italic">new</span> knowledge acquired through continued pretraining on <math id="S3.SS1.SSS0.Px5.p1.3.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px5.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px5.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.3.m3.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, we construct <span id="S3.SS1.SSS0.Px5.p1.7.3" class="ltx_text ltx_font_smallcaps">NewLAMA</span> which is made up of detailed cloze statements requiring <span id="S3.SS1.SSS0.Px5.p1.7.4" class="ltx_text ltx_font_italic">new</span> knowledge from <math id="S3.SS1.SSS0.Px5.p1.4.m4.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px5.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px5.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.4.m4.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> to correctly answer. We provide two datasets for measuring <span id="S3.SS1.SSS0.Px5.p1.7.5" class="ltx_text ltx_font_italic">new world knowledge</span>: <span id="S3.SS1.SSS0.Px5.p1.7.6" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, for which each of the instances is verified that the answer does not exist in <math id="S3.SS1.SSS0.Px5.p1.5.m5.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px5.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px5.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.5.m5.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, but only in <math id="S3.SS1.SSS0.Px5.p1.6.m6.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px5.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px5.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.6.m6.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and <span id="S3.SS1.SSS0.Px5.p1.7.7" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> for which each of the instances does not perfectly comply with our strict definition of <span id="S3.SS1.SSS0.Px5.p1.7.8" class="ltx_text ltx_font_italic">new</span> world knowledge due to its creation process, but is used to generally measure the new knowledge acquired from continued pretraining on <math id="S3.SS1.SSS0.Px5.p1.7.m7.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S3.SS1.SSS0.Px5.p1.7.m7.1a"><msub id="S3.SS1.SSS0.Px5.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px5.p1.7.m7.1.1.2" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1.2.cmml">D</mi><mn id="S3.SS1.SSS0.Px5.p1.7.m7.1.1.3" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px5.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px5.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px5.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px5.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSS0.Px5.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px5.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px5.p1.7.m7.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px5.p1.7.m7.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> at a larger scale. <span id="S3.SS1.SSS0.Px5.p1.7.9" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> can be considered <span id="S3.SS1.SSS0.Px5.p1.7.10" class="ltx_text ltx_font_italic">easier</span> since each instance was constructed to be similar to the data distribution seen during continued pretraining.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dataset Construction</h5>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dataset statistics. Input and answer length are the corresponding average token lengths.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.3.1" class="ltx_text ltx_font_bold">Input Length</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.4.1" class="ltx_text ltx_font_bold">Answer Length</span></td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.5.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S3.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.6.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S3.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.7.1" class="ltx_text ltx_font_bold">Input Length</span></td>
<td id="S3.T1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.1.8.1" class="ltx_text ltx_font_bold">Answer Length</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.2.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span></td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">17474</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">11.9</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.3</td>
<td id="S3.T1.1.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.2.5.1" class="ltx_text ltx_font_smallcaps">NewLAMA</span></td>
<td id="S3.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">797</td>
<td id="S3.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">14.7</td>
<td id="S3.T1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">8.7</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.3.1.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span></td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">924</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">13.7</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">9.4</td>
<td id="S3.T1.1.3.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.1.3.5.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></td>
<td id="S3.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">11177</td>
<td id="S3.T1.1.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">44.4</td>
<td id="S3.T1.1.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">6.1</td>
</tr>
</tbody></table>
</figure>
<div id="S3.SS1.SSS0.Px6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px6.p1.1" class="ltx_p">The data for continual pretraining, <span id="S3.SS1.SSS0.Px6.p1.1.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span>, is constructed using news-please&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hamborg et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>. <span id="S3.SS1.SSS0.Px6.p1.1.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> is constructed by manually selecting 28 <span id="S3.SS1.SSS0.Px6.p1.1.3" class="ltx_text ltx_font_italic">time-invariant</span> relations from T-Rex&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Elsahar et&nbsp;al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>. For <span id="S3.SS1.SSS0.Px6.p1.1.4" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="S3.SS1.SSS0.Px6.p1.1.5" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, we use Amazon Mechanical Turk (mturk)<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.mturk.com" title="" class="ltx_ref ltx_href">https://www.mturk.com</a></span></span></span> for crowd-sourcing Human Intelligent Tasks (HITs). The process requires selecting answerable questions from a list of questions generated by the model introduced in <cite class="ltx_cite ltx_citemacro_citet">Lewis et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> and converting them into cloze sentences. We have also separately hired 11 experts to verify the correctness and search the C4 database to categorize each instance following our definition of <span id="S3.SS1.SSS0.Px6.p1.1.6" class="ltx_text ltx_font_italic">updated</span> and <span id="S3.SS1.SSS0.Px6.p1.1.7" class="ltx_text ltx_font_italic">new</span>. <span id="S3.SS1.SSS0.Px6.p1.1.8" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> is constructed at a larger scale through a two-phase mturk process where sentences selected from articles containing new information are decontextualized and paraphrased<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Decontextualization model from <cite class="ltx_cite ltx_citemacro_citet">Choi et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite> and back-translation model from <cite class="ltx_cite ltx_citemacro_citet">Tiedemann &amp; Thottingal (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite> is used.</span></span></span> before being masked, verified and converted to corresponding questions. The constructed dataset statistics are in Table <a href="#S3.T1" title="Table 1 ‣ Dataset Construction ‣ 3.1 Task Formulation ‣ 3 Continual Knowledge Learning (CKL) ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Important details about the data construction pipeline, examples, and more fine-grained statistics are provided in Appendix <a href="#A2" title="Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Combined Metric for CKL</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">We propose a novel metric, <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">FUAR</span><span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps"> (<span id="S3.SS2.p1.1.2.1" class="ltx_text ltx_font_bold">F</span>orgotten / (<span id="S3.SS2.p1.1.2.2" class="ltx_text ltx_font_bold">U</span>pdated + <span id="S3.SS2.p1.1.2.3" class="ltx_text ltx_font_bold">A</span>cquired) <span id="S3.SS2.p1.1.2.4" class="ltx_text ltx_font_bold">R</span>atio)</span>, that can compare the efficiency of each CKL method using the trade-off between forgotten time-invariant knowledge and updated or newly acquired knowledge. <span id="S3.SS2.p1.1.3" class="ltx_text">FUAR</span> represents relatively <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">how many</span> time-invariant knowledge instances are forgotten in order to learn <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_italic">one</span> new or updated knowledge instance. We first define <span id="S3.SS2.p1.1.6" class="ltx_text">FUAR</span> for the general case where there can be multiple corpora used for training an ever-changing LM.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.14" class="ltx_p">Let <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_T</annotation></semantics></math> be an arbitrary task and <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="(D_{i})_{i=0}^{n}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msubsup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mrow id="S3.SS2.p2.2.m2.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.2.m2.1.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.SS2.p2.2.m2.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.2.m2.1.1.1.3.1" xref="S3.SS2.p2.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.1.3.3.cmml">0</mn></mrow><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.2">𝐷</ci><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p2.2.m2.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.3"><eq id="S3.SS2.p2.2.m2.1.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.3.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.3.3">0</cn></apply></apply><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">(D_{i})_{i=0}^{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">( italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> be a sequence of corpora used for LM pretraining, where <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">D</mi><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is the initial pretraining corpus. We define <math id="S3.SS2.p2.4.m4.5" class="ltx_Math" alttext="\text{Gap}(T,D_{a},D_{b})=Score(T)\text{\ of\ }LM_{a}-Score(T)\text{\ of\ }LM_{b}" display="inline"><semantics id="S3.SS2.p2.4.m4.5a"><mrow id="S3.SS2.p2.4.m4.5.5" xref="S3.SS2.p2.4.m4.5.5.cmml"><mrow id="S3.SS2.p2.4.m4.5.5.2" xref="S3.SS2.p2.4.m4.5.5.2.cmml"><mtext id="S3.SS2.p2.4.m4.5.5.2.4" xref="S3.SS2.p2.4.m4.5.5.2.4a.cmml">Gap</mtext><mo id="S3.SS2.p2.4.m4.5.5.2.3" xref="S3.SS2.p2.4.m4.5.5.2.3.cmml" lspace="0px" rspace="0px"></mo><mrow id="S3.SS2.p2.4.m4.5.5.2.2.2" xref="S3.SS2.p2.4.m4.5.5.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.5.5.2.2.2.3" xref="S3.SS2.p2.4.m4.5.5.2.2.3.cmml">(</mo><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">T</mi><mo id="S3.SS2.p2.4.m4.5.5.2.2.2.4" xref="S3.SS2.p2.4.m4.5.5.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.4.m4.4.4.1.1.1.1" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1.cmml"><mi id="S3.SS2.p2.4.m4.4.4.1.1.1.1.2" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.4.m4.4.4.1.1.1.1.3" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1.3.cmml">a</mi></msub><mo id="S3.SS2.p2.4.m4.5.5.2.2.2.5" xref="S3.SS2.p2.4.m4.5.5.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.4.m4.5.5.2.2.2.2" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2.cmml"><mi id="S3.SS2.p2.4.m4.5.5.2.2.2.2.2" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2.2.cmml">D</mi><mi id="S3.SS2.p2.4.m4.5.5.2.2.2.2.3" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2.3.cmml">b</mi></msub><mo stretchy="false" id="S3.SS2.p2.4.m4.5.5.2.2.2.6" xref="S3.SS2.p2.4.m4.5.5.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.4.m4.5.5.3" xref="S3.SS2.p2.4.m4.5.5.3.cmml">=</mo><mrow id="S3.SS2.p2.4.m4.5.5.4" xref="S3.SS2.p2.4.m4.5.5.4.cmml"><mrow id="S3.SS2.p2.4.m4.5.5.4.2" xref="S3.SS2.p2.4.m4.5.5.4.2.cmml"><mi id="S3.SS2.p2.4.m4.5.5.4.2.2" xref="S3.SS2.p2.4.m4.5.5.4.2.2.cmml">S</mi><mo id="S3.SS2.p2.4.m4.5.5.4.2.1" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.2.3" xref="S3.SS2.p2.4.m4.5.5.4.2.3.cmml">c</mi><mo id="S3.SS2.p2.4.m4.5.5.4.2.1a" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.2.4" xref="S3.SS2.p2.4.m4.5.5.4.2.4.cmml">o</mi><mo id="S3.SS2.p2.4.m4.5.5.4.2.1b" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.2.5" xref="S3.SS2.p2.4.m4.5.5.4.2.5.cmml">r</mi><mo id="S3.SS2.p2.4.m4.5.5.4.2.1c" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.2.6" xref="S3.SS2.p2.4.m4.5.5.4.2.6.cmml">e</mi><mo id="S3.SS2.p2.4.m4.5.5.4.2.1d" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S3.SS2.p2.4.m4.5.5.4.2.7.2" xref="S3.SS2.p2.4.m4.5.5.4.2.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.5.5.4.2.7.2.1" xref="S3.SS2.p2.4.m4.5.5.4.2.cmml">(</mo><mi id="S3.SS2.p2.4.m4.2.2" xref="S3.SS2.p2.4.m4.2.2.cmml">T</mi><mo stretchy="false" id="S3.SS2.p2.4.m4.5.5.4.2.7.2.2" xref="S3.SS2.p2.4.m4.5.5.4.2.cmml">)</mo></mrow><mo id="S3.SS2.p2.4.m4.5.5.4.2.1e" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mtext id="S3.SS2.p2.4.m4.5.5.4.2.8" xref="S3.SS2.p2.4.m4.5.5.4.2.8a.cmml">&nbsp;of&nbsp;</mtext><mo id="S3.SS2.p2.4.m4.5.5.4.2.1f" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.2.9" xref="S3.SS2.p2.4.m4.5.5.4.2.9.cmml">L</mi><mo id="S3.SS2.p2.4.m4.5.5.4.2.1g" xref="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" lspace="0px" rspace="0px"></mo><msub id="S3.SS2.p2.4.m4.5.5.4.2.10" xref="S3.SS2.p2.4.m4.5.5.4.2.10.cmml"><mi id="S3.SS2.p2.4.m4.5.5.4.2.10.2" xref="S3.SS2.p2.4.m4.5.5.4.2.10.2.cmml">M</mi><mi id="S3.SS2.p2.4.m4.5.5.4.2.10.3" xref="S3.SS2.p2.4.m4.5.5.4.2.10.3.cmml">a</mi></msub></mrow><mo id="S3.SS2.p2.4.m4.5.5.4.1" xref="S3.SS2.p2.4.m4.5.5.4.1.cmml">−</mo><mrow id="S3.SS2.p2.4.m4.5.5.4.3" xref="S3.SS2.p2.4.m4.5.5.4.3.cmml"><mi id="S3.SS2.p2.4.m4.5.5.4.3.2" xref="S3.SS2.p2.4.m4.5.5.4.3.2.cmml">S</mi><mo id="S3.SS2.p2.4.m4.5.5.4.3.1" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.3.3" xref="S3.SS2.p2.4.m4.5.5.4.3.3.cmml">c</mi><mo id="S3.SS2.p2.4.m4.5.5.4.3.1a" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.3.4" xref="S3.SS2.p2.4.m4.5.5.4.3.4.cmml">o</mi><mo id="S3.SS2.p2.4.m4.5.5.4.3.1b" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.3.5" xref="S3.SS2.p2.4.m4.5.5.4.3.5.cmml">r</mi><mo id="S3.SS2.p2.4.m4.5.5.4.3.1c" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.3.6" xref="S3.SS2.p2.4.m4.5.5.4.3.6.cmml">e</mi><mo id="S3.SS2.p2.4.m4.5.5.4.3.1d" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S3.SS2.p2.4.m4.5.5.4.3.7.2" xref="S3.SS2.p2.4.m4.5.5.4.3.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.5.5.4.3.7.2.1" xref="S3.SS2.p2.4.m4.5.5.4.3.cmml">(</mo><mi id="S3.SS2.p2.4.m4.3.3" xref="S3.SS2.p2.4.m4.3.3.cmml">T</mi><mo stretchy="false" id="S3.SS2.p2.4.m4.5.5.4.3.7.2.2" xref="S3.SS2.p2.4.m4.5.5.4.3.cmml">)</mo></mrow><mo id="S3.SS2.p2.4.m4.5.5.4.3.1e" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mtext id="S3.SS2.p2.4.m4.5.5.4.3.8" xref="S3.SS2.p2.4.m4.5.5.4.3.8a.cmml">&nbsp;of&nbsp;</mtext><mo id="S3.SS2.p2.4.m4.5.5.4.3.1f" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.SS2.p2.4.m4.5.5.4.3.9" xref="S3.SS2.p2.4.m4.5.5.4.3.9.cmml">L</mi><mo id="S3.SS2.p2.4.m4.5.5.4.3.1g" xref="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" lspace="0px" rspace="0px"></mo><msub id="S3.SS2.p2.4.m4.5.5.4.3.10" xref="S3.SS2.p2.4.m4.5.5.4.3.10.cmml"><mi id="S3.SS2.p2.4.m4.5.5.4.3.10.2" xref="S3.SS2.p2.4.m4.5.5.4.3.10.2.cmml">M</mi><mi id="S3.SS2.p2.4.m4.5.5.4.3.10.3" xref="S3.SS2.p2.4.m4.5.5.4.3.10.3.cmml">b</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.5b"><apply id="S3.SS2.p2.4.m4.5.5.cmml" xref="S3.SS2.p2.4.m4.5.5"><eq id="S3.SS2.p2.4.m4.5.5.3.cmml" xref="S3.SS2.p2.4.m4.5.5.3"></eq><apply id="S3.SS2.p2.4.m4.5.5.2.cmml" xref="S3.SS2.p2.4.m4.5.5.2"><times id="S3.SS2.p2.4.m4.5.5.2.3.cmml" xref="S3.SS2.p2.4.m4.5.5.2.3"></times><ci id="S3.SS2.p2.4.m4.5.5.2.4a.cmml" xref="S3.SS2.p2.4.m4.5.5.2.4"><mtext id="S3.SS2.p2.4.m4.5.5.2.4.cmml" xref="S3.SS2.p2.4.m4.5.5.2.4">Gap</mtext></ci><vector id="S3.SS2.p2.4.m4.5.5.2.2.3.cmml" xref="S3.SS2.p2.4.m4.5.5.2.2.2"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑇</ci><apply id="S3.SS2.p2.4.m4.4.4.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.4.4.1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.4.4.1.1.1.1.2.cmml" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1.2">𝐷</ci><ci id="S3.SS2.p2.4.m4.4.4.1.1.1.1.3.cmml" xref="S3.SS2.p2.4.m4.4.4.1.1.1.1.3">𝑎</ci></apply><apply id="S3.SS2.p2.4.m4.5.5.2.2.2.2.cmml" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.5.5.2.2.2.2.1.cmml" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.4.m4.5.5.2.2.2.2.2.cmml" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2.2">𝐷</ci><ci id="S3.SS2.p2.4.m4.5.5.2.2.2.2.3.cmml" xref="S3.SS2.p2.4.m4.5.5.2.2.2.2.3">𝑏</ci></apply></vector></apply><apply id="S3.SS2.p2.4.m4.5.5.4.cmml" xref="S3.SS2.p2.4.m4.5.5.4"><minus id="S3.SS2.p2.4.m4.5.5.4.1.cmml" xref="S3.SS2.p2.4.m4.5.5.4.1"></minus><apply id="S3.SS2.p2.4.m4.5.5.4.2.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2"><times id="S3.SS2.p2.4.m4.5.5.4.2.1.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.1"></times><ci id="S3.SS2.p2.4.m4.5.5.4.2.2.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.2">𝑆</ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.3.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.3">𝑐</ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.4.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.4">𝑜</ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.5.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.5">𝑟</ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.6.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.6">𝑒</ci><ci id="S3.SS2.p2.4.m4.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2">𝑇</ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.8a.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.8"><mtext id="S3.SS2.p2.4.m4.5.5.4.2.8.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.8">&nbsp;of&nbsp;</mtext></ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.9.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.9">𝐿</ci><apply id="S3.SS2.p2.4.m4.5.5.4.2.10.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.10"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.5.5.4.2.10.1.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.10">subscript</csymbol><ci id="S3.SS2.p2.4.m4.5.5.4.2.10.2.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.10.2">𝑀</ci><ci id="S3.SS2.p2.4.m4.5.5.4.2.10.3.cmml" xref="S3.SS2.p2.4.m4.5.5.4.2.10.3">𝑎</ci></apply></apply><apply id="S3.SS2.p2.4.m4.5.5.4.3.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3"><times id="S3.SS2.p2.4.m4.5.5.4.3.1.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.1"></times><ci id="S3.SS2.p2.4.m4.5.5.4.3.2.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.2">𝑆</ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.3.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.3">𝑐</ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.4.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.4">𝑜</ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.5.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.5">𝑟</ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.6.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.6">𝑒</ci><ci id="S3.SS2.p2.4.m4.3.3.cmml" xref="S3.SS2.p2.4.m4.3.3">𝑇</ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.8a.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.8"><mtext id="S3.SS2.p2.4.m4.5.5.4.3.8.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.8">&nbsp;of&nbsp;</mtext></ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.9.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.9">𝐿</ci><apply id="S3.SS2.p2.4.m4.5.5.4.3.10.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.10"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.5.5.4.3.10.1.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.10">subscript</csymbol><ci id="S3.SS2.p2.4.m4.5.5.4.3.10.2.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.10.2">𝑀</ci><ci id="S3.SS2.p2.4.m4.5.5.4.3.10.3.cmml" xref="S3.SS2.p2.4.m4.5.5.4.3.10.3">𝑏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.5c">\text{Gap}(T,D_{a},D_{b})=Score(T)\text{\ of\ }LM_{a}-Score(T)\text{\ of\ }LM_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.5d">Gap ( italic_T , italic_D start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = italic_S italic_c italic_o italic_r italic_e ( italic_T ) of italic_L italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT - italic_S italic_c italic_o italic_r italic_e ( italic_T ) of italic_L italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, where <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="LM_{a}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">L</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msub id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">M</mi><mi id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml">a</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></times><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝐿</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">𝑀</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">LM_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_L italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> represents the LM after being pretrained on <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="D_{a}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝐷</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">D_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>. Then, we denote <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathbb{T}^{F}=(T_{i}^{F})_{i=0}^{n-1}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><msup id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.3.2.cmml">𝕋</mi><mi id="S3.SS2.p2.7.m7.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.3.3.cmml">F</mi></msup><mo id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">=</mo><msubsup id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml"><mrow id="S3.SS2.p2.7.m7.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3.cmml">F</mi></msubsup><mo stretchy="false" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.SS2.p2.7.m7.1.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.7.m7.1.1.1.1.3.1" xref="S3.SS2.p2.7.m7.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.7.m7.1.1.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.1.1.3.3.cmml">0</mn></mrow><mrow id="S3.SS2.p2.7.m7.1.1.1.3" xref="S3.SS2.p2.7.m7.1.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p2.7.m7.1.1.1.3.1" xref="S3.SS2.p2.7.m7.1.1.1.3.1.cmml">−</mo><mn id="S3.SS2.p2.7.m7.1.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.1.3.3.cmml">1</mn></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><eq id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2"></eq><apply id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.3.2">𝕋</ci><ci id="S3.SS2.p2.7.m7.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3">𝐹</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1">superscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1">subscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.1.1.1.3">𝐹</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3"><eq id="S3.SS2.p2.7.m7.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3.1"></eq><ci id="S3.SS2.p2.7.m7.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p2.7.m7.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.1.3.3">0</cn></apply></apply><apply id="S3.SS2.p2.7.m7.1.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3"><minus id="S3.SS2.p2.7.m7.1.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3.1"></minus><ci id="S3.SS2.p2.7.m7.1.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.p2.7.m7.1.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\mathbb{T}^{F}=(T_{i}^{F})_{i=0}^{n-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT = ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> as a sequence of tasks from <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="(D_{i})_{i=0}^{n-1}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><msubsup id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mrow id="S3.SS2.p2.8.m8.1.1.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.8.m8.1.1.1.1.1.2" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.8.m8.1.1.1.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.8.m8.1.1.1.1.1.1.2" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.8.m8.1.1.1.1.1.1.3" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p2.8.m8.1.1.1.1.1.3" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.SS2.p2.8.m8.1.1.1.3" xref="S3.SS2.p2.8.m8.1.1.1.3.cmml"><mi id="S3.SS2.p2.8.m8.1.1.1.3.2" xref="S3.SS2.p2.8.m8.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.8.m8.1.1.1.3.1" xref="S3.SS2.p2.8.m8.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.8.m8.1.1.1.3.3" xref="S3.SS2.p2.8.m8.1.1.1.3.3.cmml">0</mn></mrow><mrow id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml"><mi id="S3.SS2.p2.8.m8.1.1.3.2" xref="S3.SS2.p2.8.m8.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p2.8.m8.1.1.3.1" xref="S3.SS2.p2.8.m8.1.1.3.1.cmml">−</mo><mn id="S3.SS2.p2.8.m8.1.1.3.3" xref="S3.SS2.p2.8.m8.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1">superscript</csymbol><apply id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1">subscript</csymbol><apply id="S3.SS2.p2.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.2">𝐷</ci><ci id="S3.SS2.p2.8.m8.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p2.8.m8.1.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.1.3"><eq id="S3.SS2.p2.8.m8.1.1.1.3.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1.3.1"></eq><ci id="S3.SS2.p2.8.m8.1.1.1.3.2.cmml" xref="S3.SS2.p2.8.m8.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p2.8.m8.1.1.1.3.3.cmml" xref="S3.SS2.p2.8.m8.1.1.1.3.3">0</cn></apply></apply><apply id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3"><minus id="S3.SS2.p2.8.m8.1.1.3.1.cmml" xref="S3.SS2.p2.8.m8.1.1.3.1"></minus><ci id="S3.SS2.p2.8.m8.1.1.3.2.cmml" xref="S3.SS2.p2.8.m8.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.p2.8.m8.1.1.3.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">(D_{i})_{i=0}^{n-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.1d">( italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> measuring the forgetting of invariant-knowledge from each corresponding corpous. If there is no such task from corpus <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><msub id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝐷</ci><ci id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m9.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, the value of <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="T_{i}^{F}" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><msubsup id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml"><mi id="S3.SS2.p2.10.m10.1.1.2.2" xref="S3.SS2.p2.10.m10.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p2.10.m10.1.1.2.3" xref="S3.SS2.p2.10.m10.1.1.2.3.cmml">i</mi><mi id="S3.SS2.p2.10.m10.1.1.3" xref="S3.SS2.p2.10.m10.1.1.3.cmml">F</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><apply id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m10.1.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">superscript</csymbol><apply id="S3.SS2.p2.10.m10.1.1.2.cmml" xref="S3.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m10.1.1.2.1.cmml" xref="S3.SS2.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p2.10.m10.1.1.2.2.cmml" xref="S3.SS2.p2.10.m10.1.1.2.2">𝑇</ci><ci id="S3.SS2.p2.10.m10.1.1.2.3.cmml" xref="S3.SS2.p2.10.m10.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p2.10.m10.1.1.3.cmml" xref="S3.SS2.p2.10.m10.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">T_{i}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.10.m10.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math> is set to <math id="S3.SS2.p2.11.m11.3" class="ltx_Math" alttext="n.d." display="inline"><semantics id="S3.SS2.p2.11.m11.3a"><mrow id="S3.SS2.p2.11.m11.3.3.1"><mrow id="S3.SS2.p2.11.m11.3.3.1.1.2" xref="S3.SS2.p2.11.m11.3.3.1.1.1.cmml"><mi id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml">n</mi><mo lspace="0em" rspace="0.167em" id="S3.SS2.p2.11.m11.3.3.1.1.2.1" xref="S3.SS2.p2.11.m11.3.3.1.1.1a.cmml">.</mo><mi id="S3.SS2.p2.11.m11.2.2" xref="S3.SS2.p2.11.m11.2.2.cmml">d</mi></mrow><mo lspace="0em" id="S3.SS2.p2.11.m11.3.3.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.3b"><apply id="S3.SS2.p2.11.m11.3.3.1.1.1.cmml" xref="S3.SS2.p2.11.m11.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m11.3.3.1.1.1a.cmml" xref="S3.SS2.p2.11.m11.3.3.1.1.2.1">formulae-sequence</csymbol><ci id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">𝑛</ci><ci id="S3.SS2.p2.11.m11.2.2.cmml" xref="S3.SS2.p2.11.m11.2.2">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.3c">n.d.</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.11.m11.3d">italic_n . italic_d .</annotation></semantics></math>, which means <span id="S3.SS2.p2.14.1" class="ltx_text ltx_font_italic">not defined</span>. Likewise, we denote <math id="S3.SS2.p2.12.m12.1" class="ltx_Math" alttext="T_{n}^{U}" display="inline"><semantics id="S3.SS2.p2.12.m12.1a"><msubsup id="S3.SS2.p2.12.m12.1.1" xref="S3.SS2.p2.12.m12.1.1.cmml"><mi id="S3.SS2.p2.12.m12.1.1.2.2" xref="S3.SS2.p2.12.m12.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p2.12.m12.1.1.2.3" xref="S3.SS2.p2.12.m12.1.1.2.3.cmml">n</mi><mi id="S3.SS2.p2.12.m12.1.1.3" xref="S3.SS2.p2.12.m12.1.1.3.cmml">U</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m12.1b"><apply id="S3.SS2.p2.12.m12.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.12.m12.1.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1">superscript</csymbol><apply id="S3.SS2.p2.12.m12.1.1.2.cmml" xref="S3.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.12.m12.1.1.2.1.cmml" xref="S3.SS2.p2.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p2.12.m12.1.1.2.2.cmml" xref="S3.SS2.p2.12.m12.1.1.2.2">𝑇</ci><ci id="S3.SS2.p2.12.m12.1.1.2.3.cmml" xref="S3.SS2.p2.12.m12.1.1.2.3">𝑛</ci></apply><ci id="S3.SS2.p2.12.m12.1.1.3.cmml" xref="S3.SS2.p2.12.m12.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m12.1c">T_{n}^{U}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.12.m12.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT</annotation></semantics></math> and <math id="S3.SS2.p2.13.m13.1" class="ltx_Math" alttext="T_{n}^{A}" display="inline"><semantics id="S3.SS2.p2.13.m13.1a"><msubsup id="S3.SS2.p2.13.m13.1.1" xref="S3.SS2.p2.13.m13.1.1.cmml"><mi id="S3.SS2.p2.13.m13.1.1.2.2" xref="S3.SS2.p2.13.m13.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p2.13.m13.1.1.2.3" xref="S3.SS2.p2.13.m13.1.1.2.3.cmml">n</mi><mi id="S3.SS2.p2.13.m13.1.1.3" xref="S3.SS2.p2.13.m13.1.1.3.cmml">A</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m13.1b"><apply id="S3.SS2.p2.13.m13.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.13.m13.1.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1">superscript</csymbol><apply id="S3.SS2.p2.13.m13.1.1.2.cmml" xref="S3.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.13.m13.1.1.2.1.cmml" xref="S3.SS2.p2.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p2.13.m13.1.1.2.2.cmml" xref="S3.SS2.p2.13.m13.1.1.2.2">𝑇</ci><ci id="S3.SS2.p2.13.m13.1.1.2.3.cmml" xref="S3.SS2.p2.13.m13.1.1.2.3">𝑛</ci></apply><ci id="S3.SS2.p2.13.m13.1.1.3.cmml" xref="S3.SS2.p2.13.m13.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m13.1c">T_{n}^{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.13.m13.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT</annotation></semantics></math> as tasks from <math id="S3.SS2.p2.14.m14.1" class="ltx_Math" alttext="D_{n}" display="inline"><semantics id="S3.SS2.p2.14.m14.1a"><msub id="S3.SS2.p2.14.m14.1.1" xref="S3.SS2.p2.14.m14.1.1.cmml"><mi id="S3.SS2.p2.14.m14.1.1.2" xref="S3.SS2.p2.14.m14.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.14.m14.1.1.3" xref="S3.SS2.p2.14.m14.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.14.m14.1b"><apply id="S3.SS2.p2.14.m14.1.1.cmml" xref="S3.SS2.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.14.m14.1.1.1.cmml" xref="S3.SS2.p2.14.m14.1.1">subscript</csymbol><ci id="S3.SS2.p2.14.m14.1.1.2.cmml" xref="S3.SS2.p2.14.m14.1.1.2">𝐷</ci><ci id="S3.SS2.p2.14.m14.1.1.3.cmml" xref="S3.SS2.p2.14.m14.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.14.m14.1c">D_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.14.m14.1d">italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> measuring the <span id="S3.SS2.p2.14.2" class="ltx_text ltx_font_italic">update</span> and <span id="S3.SS2.p2.14.3" class="ltx_text ltx_font_italic">acquisition</span> of new knowledge, respectively. We define <span id="S3.SS2.p2.14.4" class="ltx_text">FUAR</span> as follows:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_math_unparsed" alttext="\text{FUAR}(\mathbb{T}^{F},T_{n}^{U},T_{n}^{A})=\begin{cases}\dfrac{\sum\limits_{i=0}^{n-1}{\text{max}(0,\text{Gap}(T_{i}^{F},D_{i},D_{n}))\mathbbm{1}_{\{T_{i}^{F}\neq n.d.\}}}}{\sum\limits_{i=0}^{n-1}\{{\text{max}(0,\text{Gap}(T_{n}^{U},D_{n},D_{i}))}\mathbbm{1}_{\{T_{i}^{F}\neq n.d.\}}+{\text{max}(0,\text{Gap}(T_{n}^{A},D_{n},D_{i}))}\mathbbm{1}_{\{T_{i}^{F}\neq n.d.\}}\}},\\
\text{ if denominator}\ >0,\\
\textit{no gain,}\text{\ otherwise.}\\
\end{cases}" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6"><mrow id="S3.E1.m1.6.6.3"><mtext id="S3.E1.m1.6.6.3.5">FUAR</mtext><mo id="S3.E1.m1.6.6.3.4" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.6.6.3.3.3"><mo stretchy="false" id="S3.E1.m1.6.6.3.3.3.4">(</mo><msup id="S3.E1.m1.4.4.1.1.1.1"><mi id="S3.E1.m1.4.4.1.1.1.1.2">𝕋</mi><mi id="S3.E1.m1.4.4.1.1.1.1.3">F</mi></msup><mo id="S3.E1.m1.6.6.3.3.3.5">,</mo><msubsup id="S3.E1.m1.5.5.2.2.2.2"><mi id="S3.E1.m1.5.5.2.2.2.2.2.2">T</mi><mi id="S3.E1.m1.5.5.2.2.2.2.2.3">n</mi><mi id="S3.E1.m1.5.5.2.2.2.2.3">U</mi></msubsup><mo id="S3.E1.m1.6.6.3.3.3.6">,</mo><msubsup id="S3.E1.m1.6.6.3.3.3.3"><mi id="S3.E1.m1.6.6.3.3.3.3.2.2">T</mi><mi id="S3.E1.m1.6.6.3.3.3.3.2.3">n</mi><mi id="S3.E1.m1.6.6.3.3.3.3.3">A</mi></msubsup><mo stretchy="false" id="S3.E1.m1.6.6.3.3.3.7">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.4">=</mo><mrow id="S3.E1.m1.3.3"><mo id="S3.E1.m1.3.3.4">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.3.3.3"><mtr id="S3.E1.m1.3.3.3a"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3.3b"><mrow id="S3.E1.m1.1.1.1.1.1.1.10"><mfrac id="S3.E1.m1.1.1.1.1.1.1.8"><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3"><munderover id="S3.E1.m1.1.1.1.1.1.1.3.3.4"><mo movablelimits="false" id="S3.E1.m1.1.1.1.1.1.1.3.3.4.2.2">∑</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3.4.2.3"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.4.2.3.2">i</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.4.2.3.1">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.3.3.4.2.3.3">0</mn></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3.4.3"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.4.3.2">n</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.4.3.1">−</mo><mn id="S3.E1.m1.1.1.1.1.1.1.3.3.4.3.3">1</mn></mrow></munderover><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3.3"><mtext id="S3.E1.m1.1.1.1.1.1.1.3.3.3.3">max</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.3.2" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.2">(</mo><mn id="S3.E1.m1.1.1.1.1.1.1.2.2.2">0</mn><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.3">,</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1"><mtext id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.5">Gap</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.4" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.4">(</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.1.1.1"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.1.1.1.2.2">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.1.1.1.2.3">i</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.1.1.1.3">F</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.5">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.2.2.2"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.2.2.2.2">D</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.2.2.2.3">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.6">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.3"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.3.2">D</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.3.3">n</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.1.3.3.7">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3.3.3.1.1.4">)</mo></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.3.3.3.2a" lspace="0px" rspace="0px"></mo><msub id="S3.E1.m1.1.1.1.1.1.1.3.3.3.4"><mn id="S3.E1.m1.1.1.1.1.1.1.3.3.3.4.2">𝟙</mn><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2">{</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.2">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">i</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">F</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.4">≠</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.5">n</mi><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.6">.</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">d</mi><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.7">.</mo><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.8">}</mo></mrow></msub></mrow></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8"><munderover id="S3.E1.m1.1.1.1.1.1.1.8.8.6"><mo movablelimits="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.6.2.2">∑</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.6.2.3"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.6.2.3.2">i</mi><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.6.2.3.1">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.8.8.6.2.3.3">0</mn></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.6.3"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.6.3.2">n</mi><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.6.3.1">−</mo><mn id="S3.E1.m1.1.1.1.1.1.1.8.8.6.3.3">1</mn></mrow></munderover><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1"><mo lspace="0em" stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.2">{</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1"><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1"><mtext id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.3">max</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.2" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.2">(</mo><mn id="S3.E1.m1.1.1.1.1.1.1.6.6.3">0</mn><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.3">,</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1"><mtext id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.5">Gap</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.4" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.4">(</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.1.1.1"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.1.1.1.2.2">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.1.1.1.2.3">n</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.1.1.1.3">U</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.5">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.2.2.2"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.2.2.2.2">D</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.2.2.2.3">n</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.6">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.3"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.3.2">D</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.3.3">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.1.3.3.7">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.1.1.4">)</mo></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.2a" lspace="0px" rspace="0px"></mo><msub id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.4"><mn id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.1.4.2">𝟙</mn><mrow id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.2">{</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.3"><mi id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.3.2.2">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.3.2.3">i</mi><mi id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.3.3">F</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.4">≠</mo><mi id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.5">n</mi><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.6">.</mo><mi id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.1">d</mi><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.7">.</mo><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.4.4.1.1.8">}</mo></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.3">+</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2"><mtext id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.3">max</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.2" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.2">(</mo><mn id="S3.E1.m1.1.1.1.1.1.1.7.7.4">0</mn><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.3">,</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1"><mtext id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.5">Gap</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.4" lspace="0px" rspace="0px"></mo><mrow id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.4">(</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.1.1.1"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.1.1.1.2.2">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.1.1.1.2.3">n</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.1.1.1.3">A</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.5">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.2.2.2"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.2.2.2.2">D</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.2.2.2.3">n</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.6">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.3"><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.3.2">D</mi><mi id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.3.3">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.1.3.3.7">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.1.1.4">)</mo></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.2a" lspace="0px" rspace="0px"></mo><msub id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.4"><mn id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.1.2.4.2">𝟙</mn><mrow id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.2">{</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.3"><mi id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.3.2.2">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.3.2.3">i</mi><mi id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.3.3">F</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.4">≠</mo><mi id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.5">n</mi><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.6">.</mo><mi id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.1">d</mi><mo lspace="0em" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.7">.</mo><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.5.5.2.1.8">}</mo></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.8.8.5.1.3">}</mo></mrow></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.1.1.10.1">,</mo></mrow></mtd><mtd id="S3.E1.m1.3.3.3c"></mtd></mtr><mtr id="S3.E1.m1.3.3.3d"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3.3e"><mrow id="S3.E1.m1.2.2.2.2.1.1.1"><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1"><mtext id="S3.E1.m1.2.2.2.2.1.1.1.1.2">&nbsp;if denominator</mtext><mo lspace="0.778em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1">&gt;</mo><mn id="S3.E1.m1.2.2.2.2.1.1.1.1.3">0</mn></mrow><mo id="S3.E1.m1.2.2.2.2.1.1.1.2">,</mo></mrow></mtd><mtd id="S3.E1.m1.3.3.3f"></mtd></mtr><mtr id="S3.E1.m1.3.3.3g"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3.3h"><mrow id="S3.E1.m1.3.3.3.3.1.1"><mtext mathvariant="italic" id="S3.E1.m1.3.3.3.3.1.1a">no gain,</mtext><mtext id="S3.E1.m1.3.3.3.3.1.1b">&nbsp;otherwise.</mtext></mrow></mtd><mtd id="S3.E1.m1.3.3.3i"></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.6b">\text{FUAR}(\mathbb{T}^{F},T_{n}^{U},T_{n}^{A})=\begin{cases}\dfrac{\sum\limits_{i=0}^{n-1}{\text{max}(0,\text{Gap}(T_{i}^{F},D_{i},D_{n}))\mathbbm{1}_{\{T_{i}^{F}\neq n.d.\}}}}{\sum\limits_{i=0}^{n-1}\{{\text{max}(0,\text{Gap}(T_{n}^{U},D_{n},D_{i}))}\mathbbm{1}_{\{T_{i}^{F}\neq n.d.\}}+{\text{max}(0,\text{Gap}(T_{n}^{A},D_{n},D_{i}))}\mathbbm{1}_{\{T_{i}^{F}\neq n.d.\}}\}},\\
\text{ if denominator}\ &gt;0,\\
\textit{no gain,}\text{\ otherwise.}\\
\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.6c">FUAR ( blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT ) = { start_ROW start_CELL divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT max ( 0 , Gap ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT , italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) blackboard_1 start_POSTSUBSCRIPT { italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ≠ italic_n . italic_d . } end_POSTSUBSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT { max ( 0 , Gap ( italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT , italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) blackboard_1 start_POSTSUBSCRIPT { italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ≠ italic_n . italic_d . } end_POSTSUBSCRIPT + max ( 0 , Gap ( italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT , italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) blackboard_1 start_POSTSUBSCRIPT { italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ≠ italic_n . italic_d . } end_POSTSUBSCRIPT } end_ARG , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL if denominator &gt; 0 , end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_no italic_gain, otherwise. end_CELL start_CELL end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.6" class="ltx_p">The choice of benchmark tasks <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathbb{T}^{F}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">𝕋</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝕋</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbb{T}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>, <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="T_{n}^{U}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msubsup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2.2" xref="S3.SS2.p3.2.m2.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p3.2.m2.1.1.2.3" xref="S3.SS2.p3.2.m2.1.1.2.3.cmml">n</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">U</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2.2">𝑇</ci><ci id="S3.SS2.p3.2.m2.1.1.2.3.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3">𝑛</ci></apply><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">T_{n}^{U}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="T_{n}^{A}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msubsup id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2.2" xref="S3.SS2.p3.3.m3.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p3.3.m3.1.1.2.3" xref="S3.SS2.p3.3.m3.1.1.2.3.cmml">n</mi><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">A</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.2.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2.2">𝑇</ci><ci id="S3.SS2.p3.3.m3.1.1.2.3.cmml" xref="S3.SS2.p3.3.m3.1.1.2.3">𝑛</ci></apply><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">T_{n}^{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT</annotation></semantics></math> can differ according to each experimental setup. <span id="S3.SS2.p3.6.1" class="ltx_text">FUAR</span> value of 1.0 represents an equal trade-off scenario where <span id="S3.SS2.p3.6.2" class="ltx_text ltx_font_italic">one</span> time-invariant knowledge instance of <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\mathbb{T}^{F}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><msup id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">𝕋</mi><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">𝕋</ci><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\mathbb{T}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math> is forgotten on average to gain one new or updated knowledge instance of <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="T_{n}^{U}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msubsup id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2.2" xref="S3.SS2.p3.5.m5.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p3.5.m5.1.1.2.3" xref="S3.SS2.p3.5.m5.1.1.2.3.cmml">n</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">U</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">superscript</csymbol><apply id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.2.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2.2">𝑇</ci><ci id="S3.SS2.p3.5.m5.1.1.2.3.cmml" xref="S3.SS2.p3.5.m5.1.1.2.3">𝑛</ci></apply><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">T_{n}^{U}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT</annotation></semantics></math> and <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="T_{n}^{A}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><msubsup id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2.2" xref="S3.SS2.p3.6.m6.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p3.6.m6.1.1.2.3" xref="S3.SS2.p3.6.m6.1.1.2.3.cmml">n</mi><mi id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml">A</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">superscript</csymbol><apply id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.2.1.cmml" xref="S3.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2.2">𝑇</ci><ci id="S3.SS2.p3.6.m6.1.1.2.3.cmml" xref="S3.SS2.p3.6.m6.1.1.2.3">𝑛</ci></apply><ci id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">T_{n}^{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT</annotation></semantics></math>. The two terms in the denominators are summed because newly gained knowledge and updated knowledge are mutually exclusive by definition. When the value is smaller than 1, it means that the model obtains more new or updated knowledge than the amount of forgotten knowledge, so methods that exhibit a low <span id="S3.SS2.p3.6.3" class="ltx_text">FUAR</span> value can be considered suitable for CKL. If the value is zero, then it is a case where no forgetting occurs at all and is the upper bound for performance. If the denominator is 0, we denote the case as <span id="S3.SS2.p3.6.4" class="ltx_text ltx_font_italic">no gain</span> and regard it as the worst possible case.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Each of the last two sentences means that we do not measure positive <span id="footnote6.1" class="ltx_text ltx_font_italic">backward</span> transfer and negative <span id="footnote6.2" class="ltx_text ltx_font_italic">forward</span> transfer, respectively. The latter in some cases actually do happen (shown in Appendix <a href="#A7" title="Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>). Explanations about the backward and forward transfer are in Appendix <a href="#A1.SS1" title="A.1 Traditional Continual Learning ‣ Appendix A Extension of Related Works ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</span></span></span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.2" class="ltx_p">We perform extensive experiments with an encoder-decoder model, T5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, a large LM (<span id="S4.p1.1.1" class="ltx_text" style="position:relative; bottom:0.7pt;"><math id="S4.p1.1.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\sim" display="inline"><semantics id="S4.p1.1.1.1.m1.1a"><mo mathsize="70%" id="S4.p1.1.1.1.m1.1.1" xref="S4.p1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.p1.1.1.1.m1.1.1.cmml" xref="S4.p1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.1.1.m1.1c">\scriptstyle\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.1.1.m1.1d">∼</annotation></semantics></math></span> 737M params) initially pretrained on April 2019 dump of C4 and May 2020 dump of Wikipedia (thus <math id="S4.p1.2.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S4.p1.2.m1.1a"><msub id="S4.p1.2.m1.1.1" xref="S4.p1.2.m1.1.1.cmml"><mi id="S4.p1.2.m1.1.1.2" xref="S4.p1.2.m1.1.1.2.cmml">D</mi><mn id="S4.p1.2.m1.1.1.3" xref="S4.p1.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p1.2.m1.1b"><apply id="S4.p1.2.m1.1.1.cmml" xref="S4.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m1.1.1.1.cmml" xref="S4.p1.2.m1.1.1">subscript</csymbol><ci id="S4.p1.2.m1.1.1.2.cmml" xref="S4.p1.2.m1.1.1.2">𝐷</ci><cn type="integer" id="S4.p1.2.m1.1.1.3.cmml" xref="S4.p1.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> in our experiments) with salient span masking (SSM). The details of the pretraining, continual pretraining, and evaluation configurations are in Appendix <a href="#A3" title="Appendix C Experimental Configuration ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>. We establish the following methods as the baselines for the CKL benchmark and categorize them into <span id="S4.p1.2.2" class="ltx_text ltx_font_italic">regularization</span>, <span id="S4.p1.2.3" class="ltx_text ltx_font_italic">rehearsal</span>, and <span id="S4.p1.2.4" class="ltx_text ltx_font_italic">parameter-expansion</span> methods. The specific hyperparamters used for the implementation of each method are detailed in Appendix <a href="#A4" title="Appendix D Hyperparameters for Implementation of CKL Methods ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Initial</span> refers to the setting where we evaluate the LM before any continued pretraining. The performance of this model can be considered as the <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">upper-bound</span> for <span id="S4.p2.1.3" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> and <span id="S4.p2.1.4" class="ltx_text ltx_font_italic">lower-bound</span> on <span id="S4.p2.1.5" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="S4.p2.1.6" class="ltx_text ltx_font_smallcaps">NewLAMA</span>.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Vanilla</span> is a specific setting of further pretraining&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gururangan et&nbsp;al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, where the domain is <span id="S4.p3.1.2" class="ltx_text ltx_font_italic">new</span> knowledge, and the LM is further pretrained without any training strategies.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">RecAdam</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> falls into the category of regularization methods. It places a stronger independent assumption among the model parameters than the traditional regularization method (EWC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>) and does not access the initial pretraining corpus to regularize the model weights during continued pretraining. The optimizer is annealed so that less regularization is applied as the training progresses.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Mix-Review</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> falls into the category of rehearsal methods, which assumes access to the initial pretraining corpus and mixes in random subsets of the initial pretraining data during continued pretraining, depending on the mix-ratio at the current time step. As the training progresses, the mix-ratio decays towards 0, decreasing the amount of the mixed original data at each iteration.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">LoRA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> falls into the category of parameter-expansion methods. It freezes the original parameters of the LM and adds trainable rank-decomposition matrices into each layer that are updated during continued pretraining. <cite class="ltx_cite ltx_citemacro_citet">Hu et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> has implemented this approach with decoder-only models (GPT-2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite> &amp; GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>) while we apply it to an encoder-decoder model, denoting it as T5-LoRA.</p>
</div>
<div id="S4.p7" class="ltx_para ltx_noindent">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">K-Adapter</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> is another parameter-expansion method that freezes the original parameters of the LM while adding <span id="S4.p7.1.2" class="ltx_text ltx_font_italic">k</span> number of new layers, namely <span id="S4.p7.1.3" class="ltx_text ltx_font_italic">adapters</span>, that are updated during continued pretraining. <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> have shown successful injection of <span id="S4.p7.1.4" class="ltx_text ltx_font_italic">factual</span> and <span id="S4.p7.1.5" class="ltx_text ltx_font_italic">linguistic</span> knowledge for encoder-only models, BERT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite> &amp; RoBERTa&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>, while we also apply it to an encoder-decoder model, T5, and decoder-only model, GPT-2.</p>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_bold">Modular</span> is a newly proposed parameter-expansion method specifically for encoder-decoder models which freezes the original, pretrained encoder while adding a new, randomly initialized encoder that is updated during continued pretraining. For the newly added encoder, we vary the size to <span id="S4.p8.1.2" class="ltx_text ltx_font_italic">T5-small</span> while keeping the size of the original encoder and decoder to be <span id="S4.p8.1.3" class="ltx_text ltx_font_italic">T5-large</span>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.2" class="ltx_p">In this section, we first show the main experimental results for the CKL Benchmark. Then, since multiple steps of continual knowledge learning, i.e., CKL are needed for training a true, ever-changing LM, we explore the effects of multiple CKL phases as well as how epochs, corpus size, and the total number of training steps affect CKL. We further explore how learning rates affect CKL in Appendix <a href="#A5" title="Appendix E Exploring the Trade-off of Varying the Learning Rate for Continual Pretraining ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, how continual pretraining on <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S5.p1.1.m1.1a"><msub id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">D</mi><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> affects the performance of KILT tasks which require knowledge from <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S5.p1.2.m2.1a"><msub id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">D</mi><mn id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1">subscript</csymbol><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> in Appendix <a href="#A6" title="Appendix F Exploring How Continually Pretraining on 𝐷₁ Affects KILT Tasks Which Requires Knowledge from 𝐷₀ ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>, how CKL methods transfer across LM architectures in Appendix <a href="#A7" title="Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>, and how the prediction outputs change during CKL in Appendix <a href="#A8" title="Appendix H Exploring the Prediction Change During Continual Pretraining ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Main Results</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.2" class="ltx_p">Table&nbsp;<a href="#S5.T2" title="Table 2 ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows our main experimental result on the CKL benchmark. While only the exact match (EM) is reported in Table&nbsp;<a href="#S5.T2" title="Table 2 ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we report the F1 score as well as the mean precision at k (<span id="S5.SS1.p1.2.1" class="ltx_text ltx_font_italic">P@k</span>, k=1,5,10,20,50,100) in Appendix <a href="#A10" title="Appendix J Additional Analysis of Main Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">J</span></a>. The T5 models are originally pretrained on C4 (about 1 trillion token updates) and Wikipedia, which is considered as <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">D</mi><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>In this work, we see C4 and Wikipedia together as <math id="footnote7.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="footnote7.m1.1b"><msub id="footnote7.m1.1.1" xref="footnote7.m1.1.1.cmml"><mi id="footnote7.m1.1.1.2" xref="footnote7.m1.1.1.2.cmml">D</mi><mn id="footnote7.m1.1.1.3" xref="footnote7.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="footnote7.m1.1c"><apply id="footnote7.m1.1.1.cmml" xref="footnote7.m1.1.1"><csymbol cd="ambiguous" id="footnote7.m1.1.1.1.cmml" xref="footnote7.m1.1.1">subscript</csymbol><ci id="footnote7.m1.1.1.2.cmml" xref="footnote7.m1.1.1.2">𝐷</ci><cn type="integer" id="footnote7.m1.1.1.3.cmml" xref="footnote7.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote7.m1.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="footnote7.m1.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, because we do not measure how the knowledge in LMs change in between training on those two corpora.</span></span></span>, and then continually pretrained on CC-RecentNews (corpus <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><msub id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">D</mi><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>) for 4 epochs (25k global training steps, about 673 million token updates) using each of the CKL methods. Each of IL, UL, NL, NLE stands for <span id="S5.SS1.p1.2.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, <span id="S5.SS1.p1.2.3" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span>, <span id="S5.SS1.p1.2.4" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, and <span id="S5.SS1.p1.2.5" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span>, respectively. Detailed descriptions about the setup for this experiment are included in the caption.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">We first find that all of the CKL methods except for T5-MixReview are more effective at forgetting less time-invariant knowledge while updating and acquiring new knowledge than using the naïve approach of T5-Vanilla as shown by the <span id="S5.SS1.p2.1.1" class="ltx_text">FUAR</span>. This result also highlights the main difference between CKL and CL; while rehearsal methods show strong performances in traditional CL settings&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Prabhu et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Bang et&nbsp;al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>, in CKL, it shows the worst performance since the update of outdated knowledge and acquisition of new knowledge is severely deterred as shown in the performance of UL and NL while not showing competitive mitigation of forgetting as shown in the performance of IL compared to other CKL methods. Amongst the other CKL methods, we observe a rather consistent trend that the parameter-expansion methods achieve better results. The first and second-best results on all of UL, NL, and NLE are all from parameter-expansion methods. Meanwhile, although UL and NL are constructed following the same procedure, there is a huge difference between the EM scores of UL and NL. We analyze the source of this difference in Appendix&nbsp;<a href="#A9" title="Appendix I Exploring the Cause of the EM Gap Between UpdatedLAMA and NewLAMA ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">Figure <a href="#A10.F9" title="Figure 9 ‣ Appendix J Additional Analysis of Main Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> visualizes how the EM scores of each task change as T5-Kadapters, the CKL method with the most robust performance, and T5-Vanilla are continually pretrained on <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><msub id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">D</mi><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. In all of the tasks, the performance of T5-Initial can be considered as the upper-bound for IL and lower-bound for UL, NL, NLE. Corresponding with our main observations, CKL allows considerable retention of <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">time-invariant</span> world knowledge while improving updating and gaining new world knowledge compared to T5-Vanilla, mitigating the overall trade-off.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Zero-shot probing performance on the CKL benchmark. The best results for each task and metric are shown in bold, and the second-best results are underlined.</figcaption>
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="S5.T2.2.2.3.1" class="ltx_text"></span><span id="S5.T2.2.2.3.2" class="ltx_text ltx_font_bold"> <span id="S5.T2.2.2.3.2.1" class="ltx_text">
<span id="S5.T2.2.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.2.2.3.2.1.1.1" class="ltx_tr">
<span id="S5.T2.2.2.3.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Method</span></span>
</span></span> <span id="S5.T2.2.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="S5.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="S5.T2.2.2.4.1" class="ltx_text"></span><span id="S5.T2.2.2.4.2" class="ltx_text ltx_font_bold"> <span id="S5.T2.2.2.4.2.1" class="ltx_text">
<span id="S5.T2.2.2.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.2.2.4.2.1.1.1" class="ltx_tr">
<span id="S5.T2.2.2.4.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"># of Params</span></span>
<span id="S5.T2.2.2.4.2.1.1.2" class="ltx_tr">
<span id="S5.T2.2.2.4.2.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">(Trainable / Total)</span></span>
</span></span> <span id="S5.T2.2.2.4.2.2" class="ltx_text"></span></span>
</td>
<td id="S5.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.5.1" class="ltx_text ltx_font_bold">IL</span></td>
<td id="S5.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.6.1" class="ltx_text ltx_font_bold">UL</span></td>
<td id="S5.T2.2.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.7.1" class="ltx_text ltx_font_bold">NL</span></td>
<td id="S5.T2.2.2.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.8.1" class="ltx_text ltx_font_bold">NLE</span></td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="S5.T2.2.2.2.3" class="ltx_text"></span><span id="S5.T2.2.2.2.2" class="ltx_text ltx_font_bold"> <span id="S5.T2.2.2.2.2.2" class="ltx_text">
<span id="S5.T2.2.2.2.2.2.2.2" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.2.2.2.2.2.2.2.3" class="ltx_tr">
<span id="S5.T2.2.2.2.2.2.2.2.3.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.2.2.2.2.2.3.1.1" class="ltx_text">FUAR</span></span></span>
<span id="S5.T2.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S5.T2.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><math id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4" class="ltx_Math" alttext="\mathbf{{\left((IL),UL,NL\right)}}" display="inline"><semantics id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4a"><mrow id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml"><mo mathvariant="normal" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.2" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">(</mo><mrow id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.1.2" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml"><mo mathvariant="normal" stretchy="false" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.1.2.1" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">(</mo><mi id="S5.T2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐈𝐋</mi><mo mathvariant="normal" stretchy="false" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.1.2.2" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">)</mo></mrow><mo mathvariant="normal" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.3" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">,</mo><mi id="S5.T2.1.1.1.1.1.1.1.1.1.m1.2.2" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.2.2.cmml">𝐔𝐋</mi><mo mathvariant="normal" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.4" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">,</mo><mi id="S5.T2.1.1.1.1.1.1.1.1.1.m1.3.3" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.3.3.cmml">𝐍𝐋</mi><mo mathvariant="normal" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1.5" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4b"><vector id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.4.4.1"><ci id="S5.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.1.1">𝐈𝐋</ci><ci id="S5.T2.1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.2.2">𝐔𝐋</ci><ci id="S5.T2.1.1.1.1.1.1.1.1.1.m1.3.3.cmml" xref="S5.T2.1.1.1.1.1.1.1.1.1.m1.3.3">𝐍𝐋</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4c">\mathbf{{\left((IL),UL,NL\right)}}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.1.1.1.1.m1.4d">( ( bold_IL ) , bold_UL , bold_NL )</annotation></semantics></math> <math id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1a"><mo mathvariant="normal" stretchy="false" id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S5.T2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S5.T2.2.2.2.2.2.2.2.2.2.m2.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.2.2.2.2.2.m2.1d">↓</annotation></semantics></math></span></span>
</span></span> <span id="S5.T2.2.2.2.2.3" class="ltx_text"></span></span>
</td>
</tr>
<tr id="S5.T2.2.3" class="ltx_tr">
<td id="S5.T2.2.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="S5.T2.2.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="S5.T2.2.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="S5.T2.2.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
</tr>
<tr id="S5.T2.2.4" class="ltx_tr">
<td id="S5.T2.2.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Initial</td>
<td id="S5.T2.2.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0M / 737M</td>
<td id="S5.T2.2.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.4.3.1" class="ltx_text ltx_font_bold">24.17</span></td>
<td id="S5.T2.2.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.62</td>
<td id="S5.T2.2.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.88</td>
<td id="S5.T2.2.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">10.32</td>
<td id="S5.T2.2.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr id="S5.T2.2.5" class="ltx_tr">
<td id="S5.T2.2.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Vanilla</td>
<td id="S5.T2.2.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">737M / 737M</td>
<td id="S5.T2.2.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">12.89</td>
<td id="S5.T2.2.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">10.17</td>
<td id="S5.T2.2.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.77</td>
<td id="S5.T2.2.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">17.75</td>
<td id="S5.T2.2.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.08</td>
</tr>
<tr id="S5.T2.2.6" class="ltx_tr">
<td id="S5.T2.2.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-RecAdam</td>
<td id="S5.T2.2.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">737M / 737M</td>
<td id="S5.T2.2.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.20</td>
<td id="S5.T2.2.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.55</td>
<td id="S5.T2.2.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.02</td>
<td id="S5.T2.2.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.85</td>
<td id="S5.T2.2.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.84</td>
</tr>
<tr id="S5.T2.2.7" class="ltx_tr">
<td id="S5.T2.2.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-MixReview</td>
<td id="S5.T2.2.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">737M / 737M</td>
<td id="S5.T2.2.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.92</td>
<td id="S5.T2.2.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.49</td>
<td id="S5.T2.2.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.89</td>
<td id="S5.T2.2.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.86</td>
<td id="S5.T2.2.7.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.74</td>
</tr>
<tr id="S5.T2.2.8" class="ltx_tr">
<td id="S5.T2.2.8.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-LoRA</td>
<td id="S5.T2.2.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">403M / 738M</td>
<td id="S5.T2.2.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.58</td>
<td id="S5.T2.2.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.8.4.1" class="ltx_text ltx_font_bold">12.77</span></td>
<td id="S5.T2.2.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.52</td>
<td id="S5.T2.2.8.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.8.6.1" class="ltx_text ltx_font_bold">19.56</span></td>
<td id="S5.T2.2.8.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.55</td>
</tr>
<tr id="S5.T2.2.9" class="ltx_tr">
<td id="S5.T2.2.9.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Kadapters (k=2)</td>
<td id="S5.T2.2.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">427M / 762M</td>
<td id="S5.T2.2.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">19.59</td>
<td id="S5.T2.2.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.34</td>
<td id="S5.T2.2.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.9.5.1" class="ltx_text ltx_font_bold">5.03</span></td>
<td id="S5.T2.2.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.75</td>
<td id="S5.T2.2.9.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.9.7.1" class="ltx_text ltx_framed_underline">0.33</span></td>
</tr>
<tr id="S5.T2.2.10" class="ltx_tr">
<td id="S5.T2.2.10.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Kadapters (k=3)</td>
<td id="S5.T2.2.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">440M / 775M</td>
<td id="S5.T2.2.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">19.76</td>
<td id="S5.T2.2.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.10.4.1" class="ltx_text ltx_framed_underline">12.66</span></td>
<td id="S5.T2.2.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.02</td>
<td id="S5.T2.2.10.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">19.00</td>
<td id="S5.T2.2.10.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.10.7.1" class="ltx_text ltx_framed_underline">0.33</span></td>
</tr>
<tr id="S5.T2.2.11" class="ltx_tr">
<td id="S5.T2.2.11.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Modular</td>
<td id="S5.T2.2.11.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">438M / 773M</td>
<td id="S5.T2.2.11.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.11.3.1" class="ltx_text ltx_framed_underline">20.29</span></td>
<td id="S5.T2.2.11.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.11.4.1" class="ltx_text ltx_framed_underline">12.66</span></td>
<td id="S5.T2.2.11.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.11.5.1" class="ltx_text ltx_framed_underline">4.65</span></td>
<td id="S5.T2.2.11.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.11.6.1" class="ltx_text ltx_framed_underline">19.24</span></td>
<td id="S5.T2.2.11.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.11.7.1" class="ltx_text ltx_font_bold">0.28</span></td>
</tr>
</tbody></table>
</figure>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<figure id="S5.F2.1" class="ltx_figure ltx_flex_size_1 ltx_align_center"><img src="/html/2110.03215/assets/x2.png" id="S5.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="44" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="S5.F1.sf1" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x3.png" id="S5.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="609" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span><span id="S5.F1.sf1.2.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="S5.F1.sf2" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x4.png" id="S5.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="761" height="609" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span><span id="S5.F1.sf2.2.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="S5.F1.sf3" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x5.png" id="S5.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="761" height="607" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span><span id="S5.F1.sf3.2.1" class="ltx_text ltx_font_smallcaps">NewLAMA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="S5.F1.sf4" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x6.png" id="S5.F1.sf4.g1" class="ltx_graphics ltx_img_landscape" width="761" height="607" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span><span id="S5.F1.sf4.2.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Performance at each epoch during continued pretraining in the main experimental setting.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Exploring Multiple phases of CKL</h3>

<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Zero-shot probing performance after T5 models are continually pretrained on different subsets of <span id="S5.T3.29.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span>. NLE and IL stand for NewLAMA-Easy and InvariantLAMA, respectively. There are three scenarios according to the corpus used for continual pretraining, explained in the text of Section <a href="#S5.SS2" title="5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. The <span id="S5.T3.30.2" class="ltx_text">FUAR</span> of the three scenarios is calculated differently, and the corresponding tasks are shown in the table as the parameters of <span id="S5.T3.31.3" class="ltx_text">FUAR</span>: <math id="S5.T3.9.m1.1" class="ltx_Math" alttext="\mathbb{T}^{F}" display="inline"><semantics id="S5.T3.9.m1.1b"><msup id="S5.T3.9.m1.1.1" xref="S5.T3.9.m1.1.1.cmml"><mi id="S5.T3.9.m1.1.1.2" xref="S5.T3.9.m1.1.1.2.cmml">𝕋</mi><mi id="S5.T3.9.m1.1.1.3" xref="S5.T3.9.m1.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S5.T3.9.m1.1c"><apply id="S5.T3.9.m1.1.1.cmml" xref="S5.T3.9.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.9.m1.1.1.1.cmml" xref="S5.T3.9.m1.1.1">superscript</csymbol><ci id="S5.T3.9.m1.1.1.2.cmml" xref="S5.T3.9.m1.1.1.2">𝕋</ci><ci id="S5.T3.9.m1.1.1.3.cmml" xref="S5.T3.9.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.m1.1d">\mathbb{T}^{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.9.m1.1e">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>, <math id="S5.T3.10.m2.1" class="ltx_Math" alttext="T_{n}^{U}" display="inline"><semantics id="S5.T3.10.m2.1b"><msubsup id="S5.T3.10.m2.1.1" xref="S5.T3.10.m2.1.1.cmml"><mi id="S5.T3.10.m2.1.1.2.2" xref="S5.T3.10.m2.1.1.2.2.cmml">T</mi><mi id="S5.T3.10.m2.1.1.2.3" xref="S5.T3.10.m2.1.1.2.3.cmml">n</mi><mi id="S5.T3.10.m2.1.1.3" xref="S5.T3.10.m2.1.1.3.cmml">U</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.T3.10.m2.1c"><apply id="S5.T3.10.m2.1.1.cmml" xref="S5.T3.10.m2.1.1"><csymbol cd="ambiguous" id="S5.T3.10.m2.1.1.1.cmml" xref="S5.T3.10.m2.1.1">superscript</csymbol><apply id="S5.T3.10.m2.1.1.2.cmml" xref="S5.T3.10.m2.1.1"><csymbol cd="ambiguous" id="S5.T3.10.m2.1.1.2.1.cmml" xref="S5.T3.10.m2.1.1">subscript</csymbol><ci id="S5.T3.10.m2.1.1.2.2.cmml" xref="S5.T3.10.m2.1.1.2.2">𝑇</ci><ci id="S5.T3.10.m2.1.1.2.3.cmml" xref="S5.T3.10.m2.1.1.2.3">𝑛</ci></apply><ci id="S5.T3.10.m2.1.1.3.cmml" xref="S5.T3.10.m2.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.m2.1d">T_{n}^{U}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.10.m2.1e">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math id="S5.T3.11.m3.1" class="ltx_Math" alttext="T_{n}^{A}" display="inline"><semantics id="S5.T3.11.m3.1b"><msubsup id="S5.T3.11.m3.1.1" xref="S5.T3.11.m3.1.1.cmml"><mi id="S5.T3.11.m3.1.1.2.2" xref="S5.T3.11.m3.1.1.2.2.cmml">T</mi><mi id="S5.T3.11.m3.1.1.2.3" xref="S5.T3.11.m3.1.1.2.3.cmml">n</mi><mi id="S5.T3.11.m3.1.1.3" xref="S5.T3.11.m3.1.1.3.cmml">A</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.T3.11.m3.1c"><apply id="S5.T3.11.m3.1.1.cmml" xref="S5.T3.11.m3.1.1"><csymbol cd="ambiguous" id="S5.T3.11.m3.1.1.1.cmml" xref="S5.T3.11.m3.1.1">superscript</csymbol><apply id="S5.T3.11.m3.1.1.2.cmml" xref="S5.T3.11.m3.1.1"><csymbol cd="ambiguous" id="S5.T3.11.m3.1.1.2.1.cmml" xref="S5.T3.11.m3.1.1">subscript</csymbol><ci id="S5.T3.11.m3.1.1.2.2.cmml" xref="S5.T3.11.m3.1.1.2.2">𝑇</ci><ci id="S5.T3.11.m3.1.1.2.3.cmml" xref="S5.T3.11.m3.1.1.2.3">𝑛</ci></apply><ci id="S5.T3.11.m3.1.1.3.cmml" xref="S5.T3.11.m3.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.m3.1d">T_{n}^{A}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.11.m3.1e">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT</annotation></semantics></math>. In this setting, <math id="S5.T3.12.m4.1" class="ltx_Math" alttext="\mathbb{T}^{F}" display="inline"><semantics id="S5.T3.12.m4.1b"><msup id="S5.T3.12.m4.1.1" xref="S5.T3.12.m4.1.1.cmml"><mi id="S5.T3.12.m4.1.1.2" xref="S5.T3.12.m4.1.1.2.cmml">𝕋</mi><mi id="S5.T3.12.m4.1.1.3" xref="S5.T3.12.m4.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S5.T3.12.m4.1c"><apply id="S5.T3.12.m4.1.1.cmml" xref="S5.T3.12.m4.1.1"><csymbol cd="ambiguous" id="S5.T3.12.m4.1.1.1.cmml" xref="S5.T3.12.m4.1.1">superscript</csymbol><ci id="S5.T3.12.m4.1.1.2.cmml" xref="S5.T3.12.m4.1.1.2">𝕋</ci><ci id="S5.T3.12.m4.1.1.3.cmml" xref="S5.T3.12.m4.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.m4.1d">\mathbb{T}^{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.12.m4.1e">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math> consists of only a single task <math id="S5.T3.13.m5.1" class="ltx_Math" alttext="T_{0}^{F}" display="inline"><semantics id="S5.T3.13.m5.1b"><msubsup id="S5.T3.13.m5.1.1" xref="S5.T3.13.m5.1.1.cmml"><mi id="S5.T3.13.m5.1.1.2.2" xref="S5.T3.13.m5.1.1.2.2.cmml">T</mi><mn id="S5.T3.13.m5.1.1.2.3" xref="S5.T3.13.m5.1.1.2.3.cmml">0</mn><mi id="S5.T3.13.m5.1.1.3" xref="S5.T3.13.m5.1.1.3.cmml">F</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.T3.13.m5.1c"><apply id="S5.T3.13.m5.1.1.cmml" xref="S5.T3.13.m5.1.1"><csymbol cd="ambiguous" id="S5.T3.13.m5.1.1.1.cmml" xref="S5.T3.13.m5.1.1">superscript</csymbol><apply id="S5.T3.13.m5.1.1.2.cmml" xref="S5.T3.13.m5.1.1"><csymbol cd="ambiguous" id="S5.T3.13.m5.1.1.2.1.cmml" xref="S5.T3.13.m5.1.1">subscript</csymbol><ci id="S5.T3.13.m5.1.1.2.2.cmml" xref="S5.T3.13.m5.1.1.2.2">𝑇</ci><cn type="integer" id="S5.T3.13.m5.1.1.2.3.cmml" xref="S5.T3.13.m5.1.1.2.3">0</cn></apply><ci id="S5.T3.13.m5.1.1.3.cmml" xref="S5.T3.13.m5.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.13.m5.1d">T_{0}^{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.13.m5.1e">italic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>(IL) measuring the time-invariant information lost from <math id="S5.T3.14.m6.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S5.T3.14.m6.1b"><msub id="S5.T3.14.m6.1.1" xref="S5.T3.14.m6.1.1.cmml"><mi id="S5.T3.14.m6.1.1.2" xref="S5.T3.14.m6.1.1.2.cmml">D</mi><mn id="S5.T3.14.m6.1.1.3" xref="S5.T3.14.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T3.14.m6.1c"><apply id="S5.T3.14.m6.1.1.cmml" xref="S5.T3.14.m6.1.1"><csymbol cd="ambiguous" id="S5.T3.14.m6.1.1.1.cmml" xref="S5.T3.14.m6.1.1">subscript</csymbol><ci id="S5.T3.14.m6.1.1.2.cmml" xref="S5.T3.14.m6.1.1.2">𝐷</ci><cn type="integer" id="S5.T3.14.m6.1.1.3.cmml" xref="S5.T3.14.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.14.m6.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.14.m6.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> only. For <span id="S5.T3.32.4" class="ltx_text ltx_font_smallcaps">Small</span>, we calculate the gap on NLE using the weighted sum of the gaps on NLE<math id="S5.T3.15.m7.1" class="ltx_Math" alttext="{}_{\text{P1}}" display="inline"><semantics id="S5.T3.15.m7.1b"><msub id="S5.T3.15.m7.1.1" xref="S5.T3.15.m7.1.1.cmml"><mi id="S5.T3.15.m7.1.1b" xref="S5.T3.15.m7.1.1.cmml"></mi><mtext id="S5.T3.15.m7.1.1.1" xref="S5.T3.15.m7.1.1.1a.cmml">P1</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.15.m7.1c"><apply id="S5.T3.15.m7.1.1.cmml" xref="S5.T3.15.m7.1.1"><ci id="S5.T3.15.m7.1.1.1a.cmml" xref="S5.T3.15.m7.1.1.1"><mtext mathsize="70%" id="S5.T3.15.m7.1.1.1.cmml" xref="S5.T3.15.m7.1.1.1">P1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.15.m7.1d">{}_{\text{P1}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.15.m7.1e">start_FLOATSUBSCRIPT P1 end_FLOATSUBSCRIPT</annotation></semantics></math> and NLE<math id="S5.T3.16.m8.1" class="ltx_Math" alttext="{}_{\text{P2}}" display="inline"><semantics id="S5.T3.16.m8.1b"><msub id="S5.T3.16.m8.1.1" xref="S5.T3.16.m8.1.1.cmml"><mi id="S5.T3.16.m8.1.1b" xref="S5.T3.16.m8.1.1.cmml"></mi><mtext id="S5.T3.16.m8.1.1.1" xref="S5.T3.16.m8.1.1.1a.cmml">P2</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.16.m8.1c"><apply id="S5.T3.16.m8.1.1.cmml" xref="S5.T3.16.m8.1.1"><ci id="S5.T3.16.m8.1.1.1a.cmml" xref="S5.T3.16.m8.1.1.1"><mtext mathsize="70%" id="S5.T3.16.m8.1.1.1.cmml" xref="S5.T3.16.m8.1.1.1">P2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.16.m8.1d">{}_{\text{P2}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.16.m8.1e">start_FLOATSUBSCRIPT P2 end_FLOATSUBSCRIPT</annotation></semantics></math> with uniform weights.</figcaption>
<table id="S5.T3.24" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T3.18.2" class="ltx_tr">
<td id="S5.T3.18.2.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="S5.T3.18.2.3.1" class="ltx_text"></span><span id="S5.T3.18.2.3.2" class="ltx_text ltx_font_bold"> <span id="S5.T3.18.2.3.2.1" class="ltx_text">
<span id="S5.T3.18.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.18.2.3.2.1.1.1" class="ltx_tr">
<span id="S5.T3.18.2.3.2.1.1.1.1" class="ltx_td ltx_align_center">Corpus</span></span>
</span></span> <span id="S5.T3.18.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="S5.T3.18.2.4" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2">
<span id="S5.T3.18.2.4.1" class="ltx_text"></span><span id="S5.T3.18.2.4.2" class="ltx_text ltx_font_bold"> <span id="S5.T3.18.2.4.2.1" class="ltx_text">
<span id="S5.T3.18.2.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.18.2.4.2.1.1.1" class="ltx_tr">
<span id="S5.T3.18.2.4.2.1.1.1.1" class="ltx_td ltx_align_center">Method</span></span>
</span></span> <span id="S5.T3.18.2.4.2.2" class="ltx_text"></span></span>
</td>
<td id="S5.T3.18.2.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="S5.T3.18.2.5.1" class="ltx_text"></span><span id="S5.T3.18.2.5.2" class="ltx_text ltx_font_bold"> <span id="S5.T3.18.2.5.2.1" class="ltx_text">
<span id="S5.T3.18.2.5.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.18.2.5.2.1.1.1" class="ltx_tr">
<span id="S5.T3.18.2.5.2.1.1.1.1" class="ltx_td ltx_align_center"># of Params</span></span>
<span id="S5.T3.18.2.5.2.1.1.2" class="ltx_tr">
<span id="S5.T3.18.2.5.2.1.1.2.1" class="ltx_td ltx_align_center">(Trainable / Total)</span></span>
</span></span> <span id="S5.T3.18.2.5.2.2" class="ltx_text"></span></span>
</td>
<td id="S5.T3.18.2.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.18.2.6.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">IL</span></td>
<td id="S5.T3.17.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.17.1.1.1" class="ltx_text ltx_font_bold">NLE<math id="S5.T3.17.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\textbf{P1}}" display="inline"><semantics id="S5.T3.17.1.1.1.m1.1a"><msub id="S5.T3.17.1.1.1.m1.1.1" xref="S5.T3.17.1.1.1.m1.1.1.cmml"><mi id="S5.T3.17.1.1.1.m1.1.1a" xref="S5.T3.17.1.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.17.1.1.1.m1.1.1.1" xref="S5.T3.17.1.1.1.m1.1.1.1a.cmml">𝐏𝟏</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.17.1.1.1.m1.1b"><apply id="S5.T3.17.1.1.1.m1.1.1.cmml" xref="S5.T3.17.1.1.1.m1.1.1"><ci id="S5.T3.17.1.1.1.m1.1.1.1a.cmml" xref="S5.T3.17.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.17.1.1.1.m1.1.1.1.cmml" xref="S5.T3.17.1.1.1.m1.1.1.1">𝐏𝟏</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.17.1.1.1.m1.1c">{}_{\textbf{P1}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.17.1.1.1.m1.1d">start_FLOATSUBSCRIPT P1 end_FLOATSUBSCRIPT</annotation></semantics></math></span></td>
<td id="S5.T3.18.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.18.2.2.1" class="ltx_text ltx_font_bold">NLE<math id="S5.T3.18.2.2.1.m1.1" class="ltx_Math" alttext="{}_{\textbf{P2}}" display="inline"><semantics id="S5.T3.18.2.2.1.m1.1a"><msub id="S5.T3.18.2.2.1.m1.1.1" xref="S5.T3.18.2.2.1.m1.1.1.cmml"><mi id="S5.T3.18.2.2.1.m1.1.1a" xref="S5.T3.18.2.2.1.m1.1.1.cmml"></mi><mtext id="S5.T3.18.2.2.1.m1.1.1.1" xref="S5.T3.18.2.2.1.m1.1.1.1a.cmml">𝐏𝟐</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.18.2.2.1.m1.1b"><apply id="S5.T3.18.2.2.1.m1.1.1.cmml" xref="S5.T3.18.2.2.1.m1.1.1"><ci id="S5.T3.18.2.2.1.m1.1.1.1a.cmml" xref="S5.T3.18.2.2.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.18.2.2.1.m1.1.1.1.cmml" xref="S5.T3.18.2.2.1.m1.1.1.1">𝐏𝟐</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.18.2.2.1.m1.1c">{}_{\textbf{P2}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.18.2.2.1.m1.1d">start_FLOATSUBSCRIPT P2 end_FLOATSUBSCRIPT</annotation></semantics></math></span></td>
<td id="S5.T3.18.2.7" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T3.24.9" class="ltx_tr">
<td id="S5.T3.24.9.1" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="S5.T3.24.9.2" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="S5.T3.24.9.3" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="S5.T3.24.9.4" class="ltx_td"></td>
</tr>
<tr id="S5.T3.24.10" class="ltx_tr">
<td id="S5.T3.24.10.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.10.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.10.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.10.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.10.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.10.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.24.10.7.1" class="ltx_text ltx_font_bold">FUAR</span></td>
</tr>
<tr id="S5.T3.20.4" class="ltx_tr">
<td id="S5.T3.20.4.3" class="ltx_td"></td>
<td id="S5.T3.20.4.4" class="ltx_td ltx_align_left">T5-Initial</td>
<td id="S5.T3.20.4.5" class="ltx_td ltx_align_center">0M / 737M</td>
<td id="S5.T3.20.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.20.4.6.1" class="ltx_text ltx_font_bold">24.17</span></td>
<td id="S5.T3.20.4.7" class="ltx_td ltx_align_center">8.69</td>
<td id="S5.T3.20.4.8" class="ltx_td ltx_align_center">9.45</td>
<td id="S5.T3.20.4.2" class="ltx_td ltx_align_center">
<math id="S5.T3.19.3.1.m1.3" class="ltx_math_unparsed" alttext="\left((\mathbf{IL}),\bm{n.d.},\mathbf{NLE}\right)" display="inline"><semantics id="S5.T3.19.3.1.m1.3a"><mrow id="S5.T3.19.3.1.m1.3b"><mo id="S5.T3.19.3.1.m1.3.4">(</mo><mrow id="S5.T3.19.3.1.m1.3.5"><mo stretchy="false" id="S5.T3.19.3.1.m1.3.5.1">(</mo><mi id="S5.T3.19.3.1.m1.1.1">𝐈𝐋</mi><mo stretchy="false" id="S5.T3.19.3.1.m1.3.5.2">)</mo></mrow><mo id="S5.T3.19.3.1.m1.3.6">,</mo><mi id="S5.T3.19.3.1.m1.2.2">𝒏</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.19.3.1.m1.3.7">.</mo><mi id="S5.T3.19.3.1.m1.3.3">𝒅</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.19.3.1.m1.3.8">.</mo><mo id="S5.T3.19.3.1.m1.3.9">,</mo><mi id="S5.T3.19.3.1.m1.3.10">𝐍𝐋𝐄</mi><mo id="S5.T3.19.3.1.m1.3.11">)</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.19.3.1.m1.3c">\left((\mathbf{IL}),\bm{n.d.},\mathbf{NLE}\right)</annotation><annotation encoding="application/x-llamapun" id="S5.T3.19.3.1.m1.3d">( ( bold_IL ) , bold_italic_n bold_. bold_italic_d bold_. , bold_NLE )</annotation></semantics></math> <math id="S5.T3.20.4.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.20.4.2.m2.1a"><mo stretchy="false" id="S5.T3.20.4.2.m2.1.1" xref="S5.T3.20.4.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.20.4.2.m2.1b"><ci id="S5.T3.20.4.2.m2.1.1.cmml" xref="S5.T3.20.4.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.20.4.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.20.4.2.m2.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.24.11" class="ltx_tr">
<td id="S5.T3.24.11.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T3.24.11.1.1" class="ltx_text"><span id="S5.T3.24.11.1.1.1" class="ltx_text"></span> <span id="S5.T3.24.11.1.1.2" class="ltx_text">
<span id="S5.T3.24.11.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.24.11.1.1.2.1.1" class="ltx_tr">
<span id="S5.T3.24.11.1.1.2.1.1.1" class="ltx_td ltx_align_center"><span id="S5.T3.24.11.1.1.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">Small</span></span></span>
<span id="S5.T3.24.11.1.1.2.1.2" class="ltx_tr">
<span id="S5.T3.24.11.1.1.2.1.2.1" class="ltx_td ltx_align_center">(<span id="S5.T3.24.11.1.1.2.1.2.1.1" class="ltx_text ltx_font_smallcaps">Small-P1</span></span></span>
<span id="S5.T3.24.11.1.1.2.1.3" class="ltx_tr">
<span id="S5.T3.24.11.1.1.2.1.3.1" class="ltx_td ltx_align_center">+ <span id="S5.T3.24.11.1.1.2.1.3.1.1" class="ltx_text ltx_font_smallcaps">Small-P2</span>)</span></span>
</span></span> <span id="S5.T3.24.11.1.1.3" class="ltx_text"></span></span></td>
<td id="S5.T3.24.11.2" class="ltx_td ltx_align_left ltx_border_t">T5-Vanilla</td>
<td id="S5.T3.24.11.3" class="ltx_td ltx_align_center ltx_border_t">737M / 737M</td>
<td id="S5.T3.24.11.4" class="ltx_td ltx_align_center ltx_border_t">11.86</td>
<td id="S5.T3.24.11.5" class="ltx_td ltx_align_center ltx_border_t">17.77</td>
<td id="S5.T3.24.11.6" class="ltx_td ltx_align_center ltx_border_t">16.42</td>
<td id="S5.T3.24.11.7" class="ltx_td ltx_align_center ltx_border_t">1.53</td>
</tr>
<tr id="S5.T3.24.12" class="ltx_tr">
<td id="S5.T3.24.12.1" class="ltx_td ltx_align_left">T5-RecAdam</td>
<td id="S5.T3.24.12.2" class="ltx_td ltx_align_center">737M / 737M</td>
<td id="S5.T3.24.12.3" class="ltx_td ltx_align_center">11.85</td>
<td id="S5.T3.24.12.4" class="ltx_td ltx_align_center">16.46</td>
<td id="S5.T3.24.12.5" class="ltx_td ltx_align_center">13.93</td>
<td id="S5.T3.24.12.6" class="ltx_td ltx_align_center">2.01</td>
</tr>
<tr id="S5.T3.24.13" class="ltx_tr">
<td id="S5.T3.24.13.1" class="ltx_td"></td>
<td id="S5.T3.24.13.2" class="ltx_td ltx_align_left">T5-MixReview</td>
<td id="S5.T3.24.13.3" class="ltx_td ltx_align_center">737M / 737M</td>
<td id="S5.T3.24.13.4" class="ltx_td ltx_align_center">14.36</td>
<td id="S5.T3.24.13.5" class="ltx_td ltx_align_center">14.18</td>
<td id="S5.T3.24.13.6" class="ltx_td ltx_align_center">13.93</td>
<td id="S5.T3.24.13.7" class="ltx_td ltx_align_center">1.97</td>
</tr>
<tr id="S5.T3.24.14" class="ltx_tr">
<td id="S5.T3.24.14.1" class="ltx_td"></td>
<td id="S5.T3.24.14.2" class="ltx_td ltx_align_left">T5-LoRA</td>
<td id="S5.T3.24.14.3" class="ltx_td ltx_align_center">403M / 738M</td>
<td id="S5.T3.24.14.4" class="ltx_td ltx_align_center">14.26</td>
<td id="S5.T3.24.14.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.14.5.1" class="ltx_text ltx_framed_underline">20.60</span></td>
<td id="S5.T3.24.14.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.14.6.1" class="ltx_text ltx_framed_underline">19.90</span></td>
<td id="S5.T3.24.14.7" class="ltx_td ltx_align_center">0.87</td>
</tr>
<tr id="S5.T3.24.15" class="ltx_tr">
<td id="S5.T3.24.15.1" class="ltx_td"></td>
<td id="S5.T3.24.15.2" class="ltx_td ltx_align_left">T5-Kadapters (k=2)</td>
<td id="S5.T3.24.15.3" class="ltx_td ltx_align_center">427M / 762M</td>
<td id="S5.T3.24.15.4" class="ltx_td ltx_align_center"><span id="S5.T3.24.15.4.1" class="ltx_text ltx_framed_underline">18.16</span></td>
<td id="S5.T3.24.15.5" class="ltx_td ltx_align_center">18.34</td>
<td id="S5.T3.24.15.6" class="ltx_td ltx_align_center">16.42</td>
<td id="S5.T3.24.15.7" class="ltx_td ltx_align_center"><span id="S5.T3.24.15.7.1" class="ltx_text ltx_framed_underline">0.72</span></td>
</tr>
<tr id="S5.T3.24.16" class="ltx_tr">
<td id="S5.T3.24.16.1" class="ltx_td"></td>
<td id="S5.T3.24.16.2" class="ltx_td ltx_align_left">T5-Kadapters (k=3)</td>
<td id="S5.T3.24.16.3" class="ltx_td ltx_align_center">440M / 775M</td>
<td id="S5.T3.24.16.4" class="ltx_td ltx_align_center">17.12</td>
<td id="S5.T3.24.16.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.16.5.1" class="ltx_text ltx_font_bold">20.98</span></td>
<td id="S5.T3.24.16.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.16.6.1" class="ltx_text ltx_font_bold">20.39</span></td>
<td id="S5.T3.24.16.7" class="ltx_td ltx_align_center"><span id="S5.T3.24.16.7.1" class="ltx_text ltx_font_bold">0.61</span></td>
</tr>
<tr id="S5.T3.24.17" class="ltx_tr">
<td id="S5.T3.24.17.1" class="ltx_td"></td>
<td id="S5.T3.24.17.2" class="ltx_td ltx_align_left">T5-Modular</td>
<td id="S5.T3.24.17.3" class="ltx_td ltx_align_center">438M / 773M</td>
<td id="S5.T3.24.17.4" class="ltx_td ltx_align_center">16.40</td>
<td id="S5.T3.24.17.5" class="ltx_td ltx_align_center">19.47</td>
<td id="S5.T3.24.17.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.17.6.1" class="ltx_text ltx_framed_underline">19.90</span></td>
<td id="S5.T3.24.17.7" class="ltx_td ltx_align_center">0.73</td>
</tr>
<tr id="S5.T3.24.18" class="ltx_tr">
<td id="S5.T3.24.18.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.18.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.18.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.18.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.18.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.18.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.18.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.24.18.7.1" class="ltx_text ltx_font_bold">FUAR</span></td>
</tr>
<tr id="S5.T3.22.6" class="ltx_tr">
<td id="S5.T3.22.6.3" class="ltx_td"></td>
<td id="S5.T3.22.6.4" class="ltx_td ltx_align_left">T5-Initial</td>
<td id="S5.T3.22.6.5" class="ltx_td ltx_align_center">0M / 737M</td>
<td id="S5.T3.22.6.6" class="ltx_td ltx_align_center"><span id="S5.T3.22.6.6.1" class="ltx_text ltx_font_bold">24.17</span></td>
<td id="S5.T3.22.6.7" class="ltx_td ltx_align_center">8.69</td>
<td id="S5.T3.22.6.8" class="ltx_td ltx_align_center">9.45</td>
<td id="S5.T3.22.6.2" class="ltx_td ltx_align_center">
<math id="S5.T3.21.5.1.m1.3" class="ltx_math_unparsed" alttext="\left((\mathbf{IL}),\bm{n.d.},\mathbf{NLE_{P1}}\right)" display="inline"><semantics id="S5.T3.21.5.1.m1.3a"><mrow id="S5.T3.21.5.1.m1.3b"><mo id="S5.T3.21.5.1.m1.3.4">(</mo><mrow id="S5.T3.21.5.1.m1.3.5"><mo stretchy="false" id="S5.T3.21.5.1.m1.3.5.1">(</mo><mi id="S5.T3.21.5.1.m1.1.1">𝐈𝐋</mi><mo stretchy="false" id="S5.T3.21.5.1.m1.3.5.2">)</mo></mrow><mo id="S5.T3.21.5.1.m1.3.6">,</mo><mi id="S5.T3.21.5.1.m1.2.2">𝒏</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.21.5.1.m1.3.7">.</mo><mi id="S5.T3.21.5.1.m1.3.3">𝒅</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.21.5.1.m1.3.8">.</mo><mo id="S5.T3.21.5.1.m1.3.9">,</mo><msub id="S5.T3.21.5.1.m1.3.10"><mi id="S5.T3.21.5.1.m1.3.10.2">𝐍𝐋𝐄</mi><mi id="S5.T3.21.5.1.m1.3.10.3">𝐏𝟏</mi></msub><mo id="S5.T3.21.5.1.m1.3.11">)</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.21.5.1.m1.3c">\left((\mathbf{IL}),\bm{n.d.},\mathbf{NLE_{P1}}\right)</annotation><annotation encoding="application/x-llamapun" id="S5.T3.21.5.1.m1.3d">( ( bold_IL ) , bold_italic_n bold_. bold_italic_d bold_. , bold_NLE start_POSTSUBSCRIPT bold_P1 end_POSTSUBSCRIPT )</annotation></semantics></math> <math id="S5.T3.22.6.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.22.6.2.m2.1a"><mo stretchy="false" id="S5.T3.22.6.2.m2.1.1" xref="S5.T3.22.6.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.22.6.2.m2.1b"><ci id="S5.T3.22.6.2.m2.1.1.cmml" xref="S5.T3.22.6.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.22.6.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.22.6.2.m2.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.24.19" class="ltx_tr">
<td id="S5.T3.24.19.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="7"><span id="S5.T3.24.19.1.1" class="ltx_text ltx_font_smallcaps">Small-P1</span></td>
<td id="S5.T3.24.19.2" class="ltx_td ltx_align_left ltx_border_t">T5-Vanilla</td>
<td id="S5.T3.24.19.3" class="ltx_td ltx_align_center ltx_border_t">737M / 737M</td>
<td id="S5.T3.24.19.4" class="ltx_td ltx_align_center ltx_border_t">9.68</td>
<td id="S5.T3.24.19.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.24.19.5.1" class="ltx_text ltx_framed_underline">20.60</span></td>
<td id="S5.T3.24.19.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.24.19.6.1" class="ltx_text ltx_font_italic">11.44</span></td>
<td id="S5.T3.24.19.7" class="ltx_td ltx_align_center ltx_border_t">1.22</td>
</tr>
<tr id="S5.T3.24.20" class="ltx_tr">
<td id="S5.T3.24.20.1" class="ltx_td ltx_align_left">T5-RecAdam</td>
<td id="S5.T3.24.20.2" class="ltx_td ltx_align_center">737M / 737M</td>
<td id="S5.T3.24.20.3" class="ltx_td ltx_align_center">11.78</td>
<td id="S5.T3.24.20.4" class="ltx_td ltx_align_center">20.42</td>
<td id="S5.T3.24.20.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.20.5.1" class="ltx_text ltx_font_italic">11.94</span></td>
<td id="S5.T3.24.20.6" class="ltx_td ltx_align_center">1.06</td>
</tr>
<tr id="S5.T3.24.21" class="ltx_tr">
<td id="S5.T3.24.21.1" class="ltx_td ltx_align_left">T5-MixReview</td>
<td id="S5.T3.24.21.2" class="ltx_td ltx_align_center">737M / 737 M</td>
<td id="S5.T3.24.21.3" class="ltx_td ltx_align_center">16.13</td>
<td id="S5.T3.24.21.4" class="ltx_td ltx_align_center">15.88</td>
<td id="S5.T3.24.21.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.21.5.1" class="ltx_text ltx_font_italic">11.94</span></td>
<td id="S5.T3.24.21.6" class="ltx_td ltx_align_center">1.12</td>
</tr>
<tr id="S5.T3.24.22" class="ltx_tr">
<td id="S5.T3.24.22.1" class="ltx_td ltx_align_left">T5-LoRA</td>
<td id="S5.T3.24.22.2" class="ltx_td ltx_align_center">403M / 738M</td>
<td id="S5.T3.24.22.3" class="ltx_td ltx_align_center">14.75</td>
<td id="S5.T3.24.22.4" class="ltx_td ltx_align_center"><span id="S5.T3.24.22.4.1" class="ltx_text ltx_font_bold">20.79</span></td>
<td id="S5.T3.24.22.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.22.5.1" class="ltx_text ltx_font_italic">13.93</span></td>
<td id="S5.T3.24.22.6" class="ltx_td ltx_align_center">0.78</td>
</tr>
<tr id="S5.T3.24.23" class="ltx_tr">
<td id="S5.T3.24.23.1" class="ltx_td ltx_align_left">T5-Kadapters (k=2)</td>
<td id="S5.T3.24.23.2" class="ltx_td ltx_align_center">427M / 762M</td>
<td id="S5.T3.24.23.3" class="ltx_td ltx_align_center"><span id="S5.T3.24.23.3.1" class="ltx_text ltx_framed_underline">19.11</span></td>
<td id="S5.T3.24.23.4" class="ltx_td ltx_align_center"><span id="S5.T3.24.23.4.1" class="ltx_text ltx_framed_underline">20.60</span></td>
<td id="S5.T3.24.23.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.23.5.1" class="ltx_text ltx_font_italic">10.95</span></td>
<td id="S5.T3.24.23.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.23.6.1" class="ltx_text ltx_font_bold">0.42</span></td>
</tr>
<tr id="S5.T3.24.24" class="ltx_tr">
<td id="S5.T3.24.24.1" class="ltx_td ltx_align_left">T5-Kadapters (k=3)</td>
<td id="S5.T3.24.24.2" class="ltx_td ltx_align_center">440M / 775M</td>
<td id="S5.T3.24.24.3" class="ltx_td ltx_align_center">19.08</td>
<td id="S5.T3.24.24.4" class="ltx_td ltx_align_center">18.15</td>
<td id="S5.T3.24.24.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.24.5.1" class="ltx_text ltx_font_italic">10.94</span></td>
<td id="S5.T3.24.24.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.24.6.1" class="ltx_text ltx_framed_underline">0.54</span></td>
</tr>
<tr id="S5.T3.24.25" class="ltx_tr">
<td id="S5.T3.24.25.1" class="ltx_td ltx_align_left">T5-Modular</td>
<td id="S5.T3.24.25.2" class="ltx_td ltx_align_center">438M / 773M</td>
<td id="S5.T3.24.25.3" class="ltx_td ltx_align_center">17.08</td>
<td id="S5.T3.24.25.4" class="ltx_td ltx_align_center">18.90</td>
<td id="S5.T3.24.25.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.25.5.1" class="ltx_text ltx_font_italic">11.94</span></td>
<td id="S5.T3.24.25.6" class="ltx_td ltx_align_center">0.69</td>
</tr>
<tr id="S5.T3.24.26" class="ltx_tr">
<td id="S5.T3.24.26.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.26.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.26.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.26.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.26.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.26.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.24.26.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.24.26.7.1" class="ltx_text ltx_font_bold">FUAR</span></td>
</tr>
<tr id="S5.T3.24.8" class="ltx_tr">
<td id="S5.T3.24.8.3" class="ltx_td"></td>
<td id="S5.T3.24.8.4" class="ltx_td ltx_align_left">T5-Initial</td>
<td id="S5.T3.24.8.5" class="ltx_td ltx_align_center">0M / 737M</td>
<td id="S5.T3.24.8.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.8.6.1" class="ltx_text ltx_font_bold">24.17</span></td>
<td id="S5.T3.24.8.7" class="ltx_td ltx_align_center">8.69</td>
<td id="S5.T3.24.8.8" class="ltx_td ltx_align_center">9.45</td>
<td id="S5.T3.24.8.2" class="ltx_td ltx_align_center">
<math id="S5.T3.23.7.1.m1.2" class="ltx_math_unparsed" alttext="\left((\mathbf{IL},\bm{n.d.}),\bm{n.d.},\mathbf{NLE_{P2}}\right)" display="inline"><semantics id="S5.T3.23.7.1.m1.2a"><mrow id="S5.T3.23.7.1.m1.2b"><mo id="S5.T3.23.7.1.m1.2.3">(</mo><mrow id="S5.T3.23.7.1.m1.2.4"><mo stretchy="false" id="S5.T3.23.7.1.m1.2.4.1">(</mo><mi id="S5.T3.23.7.1.m1.1.1">𝐈𝐋</mi><mo id="S5.T3.23.7.1.m1.2.4.2">,</mo><mi id="S5.T3.23.7.1.m1.2.2">𝒏</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.23.7.1.m1.2.4.3">.</mo><mi id="S5.T3.23.7.1.m1.2.4.4">𝒅</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.23.7.1.m1.2.4.5">.</mo><mo stretchy="false" id="S5.T3.23.7.1.m1.2.4.6">)</mo></mrow><mo id="S5.T3.23.7.1.m1.2.5">,</mo><mi id="S5.T3.23.7.1.m1.2.6">𝒏</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.23.7.1.m1.2.7">.</mo><mi id="S5.T3.23.7.1.m1.2.8">𝒅</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="S5.T3.23.7.1.m1.2.9">.</mo><mo id="S5.T3.23.7.1.m1.2.10">,</mo><msub id="S5.T3.23.7.1.m1.2.11"><mi id="S5.T3.23.7.1.m1.2.11.2">𝐍𝐋𝐄</mi><mi id="S5.T3.23.7.1.m1.2.11.3">𝐏𝟐</mi></msub><mo id="S5.T3.23.7.1.m1.2.12">)</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.23.7.1.m1.2c">\left((\mathbf{IL},\bm{n.d.}),\bm{n.d.},\mathbf{NLE_{P2}}\right)</annotation><annotation encoding="application/x-llamapun" id="S5.T3.23.7.1.m1.2d">( ( bold_IL , bold_italic_n bold_. bold_italic_d bold_. ) , bold_italic_n bold_. bold_italic_d bold_. , bold_NLE start_POSTSUBSCRIPT bold_P2 end_POSTSUBSCRIPT )</annotation></semantics></math> <math id="S5.T3.24.8.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.24.8.2.m2.1a"><mo stretchy="false" id="S5.T3.24.8.2.m2.1.1" xref="S5.T3.24.8.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.24.8.2.m2.1b"><ci id="S5.T3.24.8.2.m2.1.1.cmml" xref="S5.T3.24.8.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.24.8.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.24.8.2.m2.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.24.27" class="ltx_tr">
<td id="S5.T3.24.27.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="7">
<span id="S5.T3.24.27.1.1" class="ltx_text"></span> <span id="S5.T3.24.27.1.2" class="ltx_text">
<span id="S5.T3.24.27.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.24.27.1.2.1.1" class="ltx_tr">
<span id="S5.T3.24.27.1.2.1.1.1" class="ltx_td ltx_align_center"><span id="S5.T3.24.27.1.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">Small-P1</span>→</span></span>
<span id="S5.T3.24.27.1.2.1.2" class="ltx_tr">
<span id="S5.T3.24.27.1.2.1.2.1" class="ltx_td ltx_align_center"><span id="S5.T3.24.27.1.2.1.2.1.1" class="ltx_text ltx_font_smallcaps">Small-P2</span></span></span>
</span></span> <span id="S5.T3.24.27.1.3" class="ltx_text"></span>
</td>
<td id="S5.T3.24.27.2" class="ltx_td ltx_align_left ltx_border_t">T5-Vanilla</td>
<td id="S5.T3.24.27.3" class="ltx_td ltx_align_center ltx_border_t">737 M / 737 M</td>
<td id="S5.T3.24.27.4" class="ltx_td ltx_align_center ltx_border_t">9.40</td>
<td id="S5.T3.24.27.5" class="ltx_td ltx_align_center ltx_border_t">14.37</td>
<td id="S5.T3.24.27.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.24.27.6.1" class="ltx_text ltx_font_bold">23.38</span></td>
<td id="S5.T3.24.27.7" class="ltx_td ltx_align_center ltx_border_t">1.06</td>
</tr>
<tr id="S5.T3.24.28" class="ltx_tr">
<td id="S5.T3.24.28.1" class="ltx_td ltx_align_left">T5-RecAdam</td>
<td id="S5.T3.24.28.2" class="ltx_td ltx_align_center">737M / 737M</td>
<td id="S5.T3.24.28.3" class="ltx_td ltx_align_center">7.25</td>
<td id="S5.T3.24.28.4" class="ltx_td ltx_align_center">14.56</td>
<td id="S5.T3.24.28.5" class="ltx_td ltx_align_center">20.90</td>
<td id="S5.T3.24.28.6" class="ltx_td ltx_align_center">1.48</td>
</tr>
<tr id="S5.T3.24.29" class="ltx_tr">
<td id="S5.T3.24.29.1" class="ltx_td ltx_align_left">T5-MixReview</td>
<td id="S5.T3.24.29.2" class="ltx_td ltx_align_center">737M / 737M</td>
<td id="S5.T3.24.29.3" class="ltx_td ltx_align_center">13.20</td>
<td id="S5.T3.24.29.4" class="ltx_td ltx_align_center"><span id="S5.T3.24.29.4.1" class="ltx_text ltx_font_bold">17.20</span></td>
<td id="S5.T3.24.29.5" class="ltx_td ltx_align_center">16.92</td>
<td id="S5.T3.24.29.6" class="ltx_td ltx_align_center">1.47</td>
</tr>
<tr id="S5.T3.24.30" class="ltx_tr">
<td id="S5.T3.24.30.1" class="ltx_td ltx_align_left">T5-LoRA</td>
<td id="S5.T3.24.30.2" class="ltx_td ltx_align_center">404M / 740M</td>
<td id="S5.T3.24.30.3" class="ltx_td ltx_align_center">13.25</td>
<td id="S5.T3.24.30.4" class="ltx_td ltx_align_center"><span id="S5.T3.24.30.4.1" class="ltx_text ltx_framed_underline">16.07</span></td>
<td id="S5.T3.24.30.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.30.5.1" class="ltx_text ltx_framed_underline">22.39</span></td>
<td id="S5.T3.24.30.6" class="ltx_td ltx_align_center">0.84</td>
</tr>
<tr id="S5.T3.24.31" class="ltx_tr">
<td id="S5.T3.24.31.1" class="ltx_td ltx_align_left">T5-Kadapters (k=2)</td>
<td id="S5.T3.24.31.2" class="ltx_td ltx_align_center">427M / 788M</td>
<td id="S5.T3.24.31.3" class="ltx_td ltx_align_center"><span id="S5.T3.24.31.3.1" class="ltx_text ltx_framed_underline">15.78</span></td>
<td id="S5.T3.24.31.4" class="ltx_td ltx_align_center"><span id="S5.T3.24.31.4.1" class="ltx_text ltx_framed_underline">16.07</span></td>
<td id="S5.T3.24.31.5" class="ltx_td ltx_align_center"><span id="S5.T3.24.31.5.1" class="ltx_text ltx_font_bold">23.38</span></td>
<td id="S5.T3.24.31.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.31.6.1" class="ltx_text ltx_font_bold">0.60</span></td>
</tr>
<tr id="S5.T3.24.32" class="ltx_tr">
<td id="S5.T3.24.32.1" class="ltx_td ltx_align_left">T5-Kadapters (k=3)</td>
<td id="S5.T3.24.32.2" class="ltx_td ltx_align_center">440M / 813M</td>
<td id="S5.T3.24.32.3" class="ltx_td ltx_align_center">15.47</td>
<td id="S5.T3.24.32.4" class="ltx_td ltx_align_center">15.31</td>
<td id="S5.T3.24.32.5" class="ltx_td ltx_align_center">20.90</td>
<td id="S5.T3.24.32.6" class="ltx_td ltx_align_center"><span id="S5.T3.24.32.6.1" class="ltx_text ltx_framed_underline">0.76</span></td>
</tr>
<tr id="S5.T3.24.33" class="ltx_tr">
<td id="S5.T3.24.33.1" class="ltx_td ltx_align_left ltx_border_bb">T5-Modular</td>
<td id="S5.T3.24.33.2" class="ltx_td ltx_align_center ltx_border_bb">438M / 809M</td>
<td id="S5.T3.24.33.3" class="ltx_td ltx_align_center ltx_border_bb">14.66</td>
<td id="S5.T3.24.33.4" class="ltx_td ltx_align_center ltx_border_bb">15.31</td>
<td id="S5.T3.24.33.5" class="ltx_td ltx_align_center ltx_border_bb">20.40</td>
<td id="S5.T3.24.33.6" class="ltx_td ltx_align_center ltx_border_bb">0.87</td>
</tr>
</tbody></table>
</figure>
<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.7" class="ltx_p">In order to show the potential for creating a truly ever-changing LM, we explore the effect of multiple CKL phases by creating <span id="S5.SS2.p1.7.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span>, denoted as <span id="S5.SS2.p1.7.2" class="ltx_text ltx_font_smallcaps">Small</span>, which is a small variant of <span id="S5.SS2.p1.7.3" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> that consists of randomly sampled 10% of the original corpus. We then split <span id="S5.SS2.p1.7.4" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span> into two different splits by the published date of each article to simulate a setting where multiple CKL phases are needed, denoted as <span id="S5.SS2.p1.7.5" class="ltx_text ltx_font_smallcaps">Small-P1</span> (05.2020 - 11.2020)) and <span id="S5.SS2.p1.7.6" class="ltx_text ltx_font_smallcaps">Small-P2</span> (11.2020 - 04.2021). NLE<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We use <span id="footnote8.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> instead of <span id="footnote8.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span> because the number of instances in NL corresponding to articles from <span id="footnote8.3" class="ltx_text ltx_font_smallcaps">Small</span> is too small for robust evaluation.</span></span></span> is also split into two different, smaller datasets, NLE<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{P1}}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1a" xref="S5.SS2.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1a.cmml">P1</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><ci id="S5.SS2.p1.1.m1.1.1.1a.cmml" xref="S5.SS2.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1">P1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">{}_{\text{P1}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">start_FLOATSUBSCRIPT P1 end_FLOATSUBSCRIPT</annotation></semantics></math> and NLE<math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="{}_{\text{P2}}" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><msub id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1a" xref="S5.SS2.p1.2.m2.1.1.cmml"></mi><mtext id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1a.cmml">P2</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><ci id="S5.SS2.p1.2.m2.1.1.1a.cmml" xref="S5.SS2.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1">P2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">{}_{\text{P2}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">start_FLOATSUBSCRIPT P2 end_FLOATSUBSCRIPT</annotation></semantics></math>, each comprising of instances constructed from articles in <span id="S5.SS2.p1.7.7" class="ltx_text ltx_font_smallcaps">Small-P1</span> and <span id="S5.SS2.p1.7.8" class="ltx_text ltx_font_smallcaps">Small-P2</span>, respectively. We compare how CKL methods for T5 perform on IL, NLE<math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="{}_{\text{P1}}" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><msub id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1a" xref="S5.SS2.p1.3.m3.1.1.cmml"></mi><mtext id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1a.cmml">P1</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><ci id="S5.SS2.p1.3.m3.1.1.1a.cmml" xref="S5.SS2.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1">P1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">{}_{\text{P1}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">start_FLOATSUBSCRIPT P1 end_FLOATSUBSCRIPT</annotation></semantics></math>, and NLE<math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="{}_{\text{P2}}" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><msub id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.p1.4.m4.1.1a" xref="S5.SS2.p1.4.m4.1.1.cmml"></mi><mtext id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1a.cmml">P2</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><ci id="S5.SS2.p1.4.m4.1.1.1a.cmml" xref="S5.SS2.p1.4.m4.1.1.1"><mtext mathsize="70%" id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1">P2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">{}_{\text{P2}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">start_FLOATSUBSCRIPT P2 end_FLOATSUBSCRIPT</annotation></semantics></math> when continually pretrained entirely on <span id="S5.SS2.p1.7.9" class="ltx_text ltx_font_smallcaps">Small</span> for 5k steps (8 epochs), and when sequentially pretrained on <span id="S5.SS2.p1.7.10" class="ltx_text ltx_font_smallcaps">Small-P1</span> and then on <span id="S5.SS2.p1.7.11" class="ltx_text ltx_font_smallcaps">Small-P2</span> for 2.5k steps (8 epochs) each. In the scenario <span id="S5.SS2.p1.7.12" class="ltx_text ltx_font_smallcaps">Small-P1</span>→<span id="S5.SS2.p1.7.13" class="ltx_text ltx_font_smallcaps">Small-P2</span>, there are two CKL phases where <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><msub id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mi id="S5.SS2.p1.5.m5.1.1.2" xref="S5.SS2.p1.5.m5.1.1.2.cmml">D</mi><mn id="S5.SS2.p1.5.m5.1.1.3" xref="S5.SS2.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S5.SS2.p1.5.m5.1.1.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is C4 and Wikipedia, <math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><msub id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml"><mi id="S5.SS2.p1.6.m6.1.1.2" xref="S5.SS2.p1.6.m6.1.1.2.cmml">D</mi><mn id="S5.SS2.p1.6.m6.1.1.3" xref="S5.SS2.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><apply id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.6.m6.1.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S5.SS2.p1.6.m6.1.1.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2">𝐷</ci><cn type="integer" id="S5.SS2.p1.6.m6.1.1.3.cmml" xref="S5.SS2.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is <span id="S5.SS2.p1.7.14" class="ltx_text ltx_font_smallcaps">Small-P1</span>, and <math id="S5.SS2.p1.7.m7.1" class="ltx_Math" alttext="D_{2}" display="inline"><semantics id="S5.SS2.p1.7.m7.1a"><msub id="S5.SS2.p1.7.m7.1.1" xref="S5.SS2.p1.7.m7.1.1.cmml"><mi id="S5.SS2.p1.7.m7.1.1.2" xref="S5.SS2.p1.7.m7.1.1.2.cmml">D</mi><mn id="S5.SS2.p1.7.m7.1.1.3" xref="S5.SS2.p1.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.7.m7.1b"><apply id="S5.SS2.p1.7.m7.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.7.m7.1.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S5.SS2.p1.7.m7.1.1.2.cmml" xref="S5.SS2.p1.7.m7.1.1.2">𝐷</ci><cn type="integer" id="S5.SS2.p1.7.m7.1.1.3.cmml" xref="S5.SS2.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.7.m7.1c">D_{2}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.7.m7.1d">italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is <span id="S5.SS2.p1.7.15" class="ltx_text ltx_font_smallcaps">Small-P2</span>. The rest of the configurations are set identical with the main experiments.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Comparing the performance on IL of the two scenarios, <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">Small</span> and <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">Small-P1</span>→<span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_smallcaps">Small-P2</span>, results show that LMs are prone to more forgetting as they go through multiple CKL phases, despite having the same number of training steps. One of the reasons may be due to the learning rate scheduling, which is initialized at the start of each phase.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">Furthermore, despite showing the best performance overall, the drawbacks of parameter-expansion methods are also highlighted in the <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_smallcaps">Small-P1</span>→<span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_smallcaps">Small-P2</span> setting; they require new parameters to be added at every phase of the update. For example, the number of total parameters of T5-Modular increases by 36M in every round of the continual pretraining phase. Likewise, considering a large number of CKL phases introduces new problems that should be additionally studied. Taking into account that LMs should be updated frequently with a small amount of data in real-world scenarios for gaining up-to-date world knowledge about the ever-changing world in a computation-effective manner, more research is needed to mitigate the amount of forgetting that follows the larger number of update phases.
</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Effects of Epochs, Corpus Size, and Total Number of Training Steps in CKL on Forgetting</h5>

<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S5.F2.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2110.03215/assets/x7.png" id="S5.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="346" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>T5-Vanilla</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="S5.F2.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2110.03215/assets/x8.png" id="S5.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="761" height="346" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>T5-Kadapters (k=2)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance at each epoch on <span id="S5.F3.6.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> during continued pretraining in <span id="S5.F3.7.2" class="ltx_text ltx_font_smallcaps">Main</span>, <span id="S5.F3.8.3" class="ltx_text ltx_font_smallcaps">Small</span>, and <span id="S5.F3.9.4" class="ltx_text ltx_font_smallcaps">Small-P1</span>→<span id="S5.F3.10.5" class="ltx_text ltx_font_smallcaps">Small-P2</span> scenarios. Each marker indicates the result at each continual pretraining epoch.</figcaption>
</figure>
<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ Effects of Epochs, Corpus Size, and Total Number of Training Steps in CKL on Forgetting ‣ 5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the result of T5-Vanilla and T5-Kadapters during continued pretraining in different scenarios from Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.T3" title="Table 3 ‣ 5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where each point in the graph represents the performance of IL after every epoch. Comparing <span id="S5.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">Main</span> (4 epochs) and <span id="S5.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">Small</span> (8 epochs) in Figure <a href="#S5.F3" title="Figure 3 ‣ Effects of Epochs, Corpus Size, and Total Number of Training Steps in CKL on Forgetting ‣ 5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) T5-Vanilla, we can see that more forgetting occurs in <span id="S5.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">Small</span>, even though trained for five times less number of global training steps. This phenomenon is further highlighted when comparing results from <span id="S5.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">Small-P1</span> (8 epochs) which shows the most amount of forgetting despite being trained for ten times less number of global training steps. While the overall drop is much mitigated in Figure <a href="#S5.F3" title="Figure 3 ‣ Effects of Epochs, Corpus Size, and Total Number of Training Steps in CKL on Forgetting ‣ 5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b) T5-Kadapters, we observe the same trend between each scenario which goes to show how critical observing the same data repeatedly during continued pretraining is for causing forgetting.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">The results are in line with findings from <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> which suggest LMs should be pretrained with just a few epochs on less duplicating data for efficiency. We add additional intuition to their findings and conjecture that the inefficiency of pretraining from duplicate data could have been caused by the forgetting of the rather long-tail knowledge in the pretraining corpus.
</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">Continual Knowledge Learning (CKL)</span>, where we establish benchmark datasets and metrics, and explore methodologies towards continual knowledge learning of an ever-changing LM. We find that parameter-expansion methods show the most robust performance throughout all of the experimental settings, which nevertheless has severe memory inefficiency and that seeing the same data often is a critical cause of forgetting. We also discuss several other interesting results of which we leave further exploration to future studies. To this end, we suggest the community to explore CKL for the better design of an ever-changing LM.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">The authors would like to thank Sang-Woo Lee, Jinheon Baek, Miyoung Ko, Hyunji Lee, and Eunbi Choi for helpful discussions. This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)).
</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi.

</span>
<span class="ltx_bibblock">Rainbow memory: Continual learning with a memory of diverse samples.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom&nbsp;B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu.

</span>
<span class="ltx_bibblock">Recall and learn: Fine-tuning deep pretrained language models with
less forgetting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das,
and Michael Collins.

</span>
<span class="ltx_bibblock">Decontextualization: Making sentences stand-alone.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 9:447–461, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Damai Dai, Li&nbsp;Dong, Y.&nbsp;Hao, Zhifang Sui, and Furu Wei.

</span>
<span class="ltx_bibblock">Knowledge neurons in pretrained transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2104.08696, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">d’Autume et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Cyprien de&nbsp;Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama.

</span>
<span class="ltx_bibblock">Episodic memory in lifelong language learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Cao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Nicola De&nbsp;Cao, Wilker Aziz, and Ivan Titov.

</span>
<span class="ltx_bibblock">Editing factual knowledge in language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Bhuwan Dhingra, Jeremy&nbsp;R Cole, Julian&nbsp;Martin Eisenschlos, Daniel Gillick, Jacob
Eisenstein, and William&nbsp;W Cohen.

</span>
<span class="ltx_bibblock">Time-aware language models as temporal knowledge bases.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.15110</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason
Weston.

</span>
<span class="ltx_bibblock">Wizard of wikipedia: Knowledge-powered conversational agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elsahar et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon
Hare, Elena Simperl, and Frederique Laforest.

</span>
<span class="ltx_bibblock">T-rex: A large scale alignment of natural language with knowledge
base triples.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">LREC</em>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and
Michael Auli.

</span>
<span class="ltx_bibblock">Eli5: Long form question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo &amp; Barbosa (2018)</span>
<span class="ltx_bibblock">
Zhaochen Guo and Denilson Barbosa.

</span>
<span class="ltx_bibblock">Robust named entity disambiguation with random walks.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Semantic Web</em>, 9(4):459–479, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz&nbsp;Beltagy,
Doug Downey, and Noah&nbsp;A Smith.

</span>
<span class="ltx_bibblock">Don’t stop pretraining: adapt language models to domains and tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Realm: Retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamborg et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp.

</span>
<span class="ltx_bibblock">news-please: A generic news crawler and extractor.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">15th International Symposium of Information Science (ISI
2017)</em>, pp.&nbsp; 218–223, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and
Fuchun Peng.

</span>
<span class="ltx_bibblock">Analyzing the forgetting problem in pretrain-finetuning of
open-domain dialogue response models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">EACL</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffart et&nbsp;al. (2011)</span>
<span class="ltx_bibblock">
Johannes Hoffart, Mohamed&nbsp;Amir Yosef, Ilaria Bordino, Hagen Fürstenau,
Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum.

</span>
<span class="ltx_bibblock">Robust disambiguation of named entities in text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2011.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei,
Andrew Arnold, and Xiang Ren.

</span>
<span class="ltx_bibblock">Lifelong pretraining: Continually adapting language models to
emerging corpora.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.08534</em>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel&nbsp;S Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei&nbsp;A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et&nbsp;al.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the national academy of sciences</em>, 114(13):3521–3526, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komeili et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mojtaba Komeili, Kurt Shuster, and Jason Weston.

</span>
<span class="ltx_bibblock">Internet-augmented dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.07566</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et&nbsp;al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 7:453–466, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam
Liska, Tayfun Terzi, Mai Gimenez, Cyprien de&nbsp;Masson d’Autume, Sebastian
Ruder, Dani Yogatama, et&nbsp;al.

</span>
<span class="ltx_bibblock">Pitfalls of static language modelling.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.01951</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
Chris Callison-Burch, and Nicholas Carlini.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.06499</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levy et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Zero-shot relation extraction via reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoNLL</em>, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020a)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et&nbsp;al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020b)</span>
<span class="ltx_bibblock">
Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.

</span>
<span class="ltx_bibblock">Question and answer test-train overlap in open-domain question
answering datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.02637</em>, 2020b.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich
Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel.

</span>
<span class="ltx_bibblock">Paq: 65 million probably-asked questions and what you can do with
them.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">EACL</em>, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yanyang Li, Ye&nbsp;Lin, Tong Xiao, and Jingbo Zhu.

</span>
<span class="ltx_bibblock">An efficient transformer decoder with compressed sub-layers.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00542</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois,
and Sameer Singh.

</span>
<span class="ltx_bibblock">Entity-based knowledge conflicts in question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.05052</em>, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Paz &amp; Ranzato (2017)</span>
<span class="ltx_bibblock">
David Lopez-Paz and Marc’Aurelio Ranzato.

</span>
<span class="ltx_bibblock">Gradient episodic memory for continual learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCloskey &amp; Cohen (1989)</span>
<span class="ltx_bibblock">
Michael McCloskey and Neal&nbsp;J Cohen.

</span>
<span class="ltx_bibblock">Catastrophic interference in connectionist networks: The sequential
learning problem.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Psychology of learning and motivation</em>, 24:109–165,
1989.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.10350</em>, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
Alexander&nbsp;H Miller, and Sebastian Riedel.

</span>
<span class="ltx_bibblock">Language models as knowledge bases?

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,
Nicola De&nbsp;Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, et&nbsp;al.

</span>
<span class="ltx_bibblock">Kilt: a benchmark for knowledge intensive language tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poerner et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Nina Poerner, Ulli Waltinger, and Hinrich Schütze.

</span>
<span class="ltx_bibblock">E-bert: Efficient-yet-effective entity embeddings for bert.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prabhu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Ameya Prabhu, Philip&nbsp;HS Torr, and Puneet&nbsp;K Dokania.

</span>
<span class="ltx_bibblock">Gdumb: A simple approach that questions our progress in continual
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.10683</em>, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Adam Roberts, Colin Raffel, and Noam Shazeer.

</span>
<span class="ltx_bibblock">How much knowledge can you pack into the parameters of a language
model?

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Andrei&nbsp;A Rusu, Neil&nbsp;C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.

</span>
<span class="ltx_bibblock">Progressive neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.04671</em>, 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Taylor Shin, Yasaman Razeghi, Robert L.&nbsp;Logan IV, Eric Wallace, and Sameer
Singh.

</span>
<span class="ltx_bibblock">AutoPrompt: Eliciting knowledge from language models with
automatically generated prompts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee.

</span>
<span class="ltx_bibblock">Lamol: Language modeling for lifelong language learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.

</span>
<span class="ltx_bibblock">Fever: a large-scale dataset for fact extraction and verification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2018.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann &amp; Thottingal (2020)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Santhosh Thottingal.

</span>
<span class="ltx_bibblock">OPUS-MT — Building open translation services for the World.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">EAMT</em>, Lisbon, Portugal, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verga et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Pat Verga, Haitian Sun, Livio&nbsp;Baldini Soares, and William&nbsp;W Cohen.

</span>
<span class="ltx_bibblock">Facts as experts: Adaptable and interpretable neural memory over
symbolic knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vig et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo,
Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber.

</span>
<span class="ltx_bibblock">Causal mediation analysis for interpreting neural nlp: The case of
gender bias.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2021a)</span>
<span class="ltx_bibblock">
Cunxiang Wang, Pai Liu, and Yue Zhang.

</span>
<span class="ltx_bibblock">Can generative pre-trained language models serve as knowledge bases
for closed-book qa?

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2021a.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2021b)</span>
<span class="ltx_bibblock">
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao,
Daxin Jiang, Ming Zhou, et&nbsp;al.

</span>
<span class="ltx_bibblock">K-adapter: Infusing knowledge into pre-trained models with adapters.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Findings of ACL</em>, 2021b.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven&nbsp;Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander&nbsp;M. Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">EMNLP System Demonstrations</em>, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jing Xu, Arthur Szlam, and Jason Weston.

</span>
<span class="ltx_bibblock">Beyond goldfish memory: Long-term open-domain conversation.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.07567</em>, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William&nbsp;W Cohen, Ruslan
Salakhutdinov, and Christopher&nbsp;D Manning.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2018.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung&nbsp;Ju Hwang.

</span>
<span class="ltx_bibblock">Lifelong learning with dynamically expandable networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2018.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
Franziska Roesner, and Yejin Choi.

</span>
<span class="ltx_bibblock">Defending against neural fake news.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Choi (2021)</span>
<span class="ltx_bibblock">
Michael&nbsp;J.Q. Zhang and Eunsol Choi.

</span>
<span class="ltx_bibblock">SituatedQA: Incorporating extra-linguistic contexts into QA.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Chen Zhu, Ankit&nbsp;Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li,
Felix Yu, and Sanjiv Kumar.

</span>
<span class="ltx_bibblock">Modifying memories in transformer models.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.00363</em>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Extension of Related Works</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">As mentioned in Section <a href="#S2" title="2 Related Work ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there are fundamental differences between the traditional CL formulations and CKL which make the previous CL methods inadequate for the CKL setting. In this section, we introduce the prior traditional continual learning methods in detail, explore the methods from the literature set as baselines for the CKL benchmark and how they address the identified limitations of CL methods, and provide descriptions about alternative methods making LMs cope with the changing world.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Traditional Continual Learning</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">Traditional continual learning (CL) methods focus on addressing two aspects of transfer between sequentially incoming tasks: <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_italic">forward transfer</span> and <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_italic">backward transfer</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lopez-Paz &amp; Ranzato, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite>. <span id="A1.SS1.p1.1.3" class="ltx_text ltx_font_italic">Forward transfer</span> refers to how past tasks affect the performance of the current and future tasks. <span id="A1.SS1.p1.1.4" class="ltx_text ltx_font_italic">Backward transfer</span> refers to how current or future tasks affect the performance of previous tasks. The general pretrain-finetune approach can be seen as an instance of <span id="A1.SS1.p1.1.5" class="ltx_text ltx_font_italic">positive forward transfer</span> where a model performs better on a target task after being pretrained on a more general source task. Moreover, catastrophic forgetting can be seen as an instance of <span id="A1.SS1.p1.1.6" class="ltx_text ltx_font_italic">negative backward transfer</span> where previous tasks suffer performance due to continued training on different tasks. With respect to these two aspects, CL approaches can be categorized into three main approaches: regularization, rehearsal, and parameter-expansion methods.</p>
</div>
<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Regularization</h5>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px1.p1.1" class="ltx_p">Elastic Weight Consolidation (EWC)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> is a method that regularizes important parameters of previous tasks while training for the current tasks, helping mitigate <span id="A1.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">the negative backward transfer</span> of previous tasks. Important parameters are measured via a Fisher information matrix computed by measuring the magnitude of the gradient update step of each parameter during training of previous tasks.</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Rehearsal</h5>

<div id="A1.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px2.p1.1" class="ltx_p">Gradient Episodic Memory (GEM)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lopez-Paz &amp; Ranzato, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> is one of the first rehearsal methods that utilize samples from each task stored in <span id="A1.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">episodic memory</span> and places an inequality constraint with respect to the losses of the samples in order to prevent <span id="A1.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">negative backward transfer</span> as well as allow the <span id="A1.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">positive backward transfer</span>. Other methods such as Experience replay and local adaptation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(d’Autume et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> replay samples stored in the memory of previous tasks during training to mitigate forgetting.</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Parameter-expansion</h5>

<div id="A1.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px3.p1.1" class="ltx_p">Progressive Neural Networks (PNN)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rusu et&nbsp;al., <a href="#bib.bib44" title="" class="ltx_ref">2016</a>)</cite> is one of the earliest parameter-expansion/sharing approaches that introduce new sets of parameters for each new task where previous parameters are frozen and can be connected via lateral connections allowing for <span id="A1.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">positive forward transfer</span>. PNN not only prevents <span id="A1.SS1.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">negative backward transfer</span> but also surpassed the previous pretrain-finetune approach in terms of <span id="A1.SS1.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_italic">positive forward transfer</span> in some tasks.</p>
</div>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>CKL Methods for Language Models</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p">As mentioned in Section <a href="#S2" title="2 Related Work ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we explore the methods from the literature that have addressed the limitations of CL methods and thus are applicable to CKL. We also categorize these methods into the three main categories of CL.</p>
</div>
<section id="A1.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Regularization</h5>

<div id="A1.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS0.Px1.p1.1" class="ltx_p">Most CL methods that utilize regularization require computing important parameters of the previous task, which in this case is pretraining on the original text corpus. Determining these parameters is oftentimes unrealistic since it requires large-scale pretraining which can hardly be replicated by most. Also, exactly how and where the knowledge is stored in the parameters of an LM is currently extremely difficult to identify and localize&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vig et&nbsp;al., <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; De&nbsp;Cao et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>. RecAdam&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> overcomes this limitation by following the same training objective as EWC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> with a stronger independent assumption and places a quadratic penalty, ridding the need to access the initial pretraining corpus.</p>
</div>
</section>
<section id="A1.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Rehearsal</h5>

<div id="A1.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS0.Px2.p1.1" class="ltx_p">Large LMs are usually pretrained on a vast amount of raw text corpus such as Common Crawl<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://commoncrawl.org/" title="" class="ltx_ref ltx_href">https://commoncrawl.org/</a></span></span></span>. When treating pretraining as a CL task, limitations exist when trying to apply previous rehearsal methods since a few samples from the pretraining corpus cannot represent the overall world knowledge from the original pretraining corpus. Mix-Review&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> solves this issue by performing preliminary experiments in a smaller pretraining setting by assuming access to the pretraining corpus during finetuning and mixing random subsets of pretraining corpus depending on a mix-ratio that anneals towards the target task as training progresses. Mix-Review can be considered a mild version of multi-task learning.
</p>
</div>
</section>
<section id="A1.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Parameter-expansion</h5>

<div id="A1.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS0.Px3.p1.1" class="ltx_p">K-Adapter&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> shares and freezes the original parameters and adds new parameters through adapters for continued pretraining of factual and linguistic knowledge and improve performance on three different knowledge-driven downstream tasks. More recently, LoRA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> freezes the original parameters and injects trainable rank-decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters and the computational hardware requirement while performing on-par or better than training all of the parameters. Both methods hypothesize freezing the original parameters allows mitigation of catastrophic forgetting. We test out the hypothesis through implementation in our CKL benchmark.</p>
</div>
</section>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Methods of Integrating World Knowledge with Language Models</h3>

<section id="A1.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Explicit Methods</h5>

<div id="A1.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.SSS0.Px1.p1.1" class="ltx_p">Facts-as-Experts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Verga et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite> store representations of entities in the form of key-value pairs into external memory that can be modified during inference time. RAG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> accesses a dense vector index of Wikipedia with a retriever and swaps indexes for updating the behavior of the model as the world changes. Blender Bot 2.0&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a href="#bib.bib54" title="" class="ltx_ref">2021</a>; Komeili et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>, is also one of the explicit methods that search the internet for recent knowledge and saves recent conversations in external long-term memory. Explicit methods, such as swapping indexes, adding explicit entity-relation knowledge, or searching the internet are in need of manual intervention during inference or are bound to tasks that require retrieval. In this paper, we focus only on implicit methods.</p>
</div>
</section>
<section id="A1.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Implicit Methods</h5>

<div id="A1.SS3.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.SSS0.Px2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Zhu et&nbsp;al. (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite> proposed a new task of explicitly modifying specific facts without forgetting unmodified facts and provided several benchmark approaches without utilizing non-parametric memory, including constrained layer-wise finetuning. &nbsp;<cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> proposed K-Adapter, a method that adds adapters to frozen layers of pretrained LMs to inject factual and linguistic knowledge and improve performance on downstream tasks. &nbsp;<cite class="ltx_cite ltx_citemacro_citet">Chen et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> proposed a new optimizer that simulates the pretraining optimization while finetuning on the target task without needing access to the pretraining corpus, improving performance on the GLUE benchmark. <cite class="ltx_cite ltx_citemacro_citet">De&nbsp;Cao et&nbsp;al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite> propose using a hyper-network to edit factual knowledge.</p>
</div>
<div id="A1.SS3.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A1.SS3.SSS0.Px2.p2.1" class="ltx_p">Even though these implicit methods are efficient methods of injecting or modifying knowledge from the implicit parameters of the LMs, they are all limited to injecting <span id="A1.SS3.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">specific knowledge</span> such as the case of <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> or modifying <span id="A1.SS3.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_italic">past knowledge</span> such as the case of <cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a href="#bib.bib59" title="" class="ltx_ref">2020</a>; De&nbsp;Cao et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>. No work, to the best of our knowledge, has specifically addressed the <span id="A1.SS3.SSS0.Px2.p2.1.3" class="ltx_text ltx_font_italic">catastrophic forgetting</span> of world knowledge gained from the initial pretraining when continued pretraining on new text corpus for the gain of <span id="A1.SS3.SSS0.Px2.p2.1.4" class="ltx_text ltx_font_italic">new</span> world knowledge.</p>
</div>
</section>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Dataset Construction</h2>

<figure id="A2.F4" class="ltx_figure"><img src="/html/2110.03215/assets/x9.png" id="A2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="753" height="458" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Dataset construction pipeline for (a) <span id="A2.F4.4.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span>, <span id="A2.F4.5.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, and (b) <span id="A2.F4.6.3" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></figcaption>
</figure>
<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">In this section, we describe the dataset construction process we undergo in creating the benchmark datasets used in CKL. For the construction, we use Amazon Mechanical Turk (mturk)<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://www.mturk.com" title="" class="ltx_ref ltx_href">https://www.mturk.com</a></span></span></span> for crowd-sourcing Human Intelligent Tasks (HITs) and separately hire 11 experts for annotation that requires extensive searching of the C4 corpus. In addition, three more experts<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>The first three authors of the paper.</span></span></span> who set up the data construction process and prepared the annotation guideline to ensure the quality of the data through post-validation and giving feedback to the annotators in real-time. The interfaces used for mturk HITs are provided in Appendix <a href="#A2.SS2" title="B.2 Interfaces used for the construction of CKL benchmark ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>.</p>
</div>
<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_smallcaps ltx_title_paragraph">CC-RecentNews</h5>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px1.p1.2" class="ltx_p">We first construct <span id="A2.SS0.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span>, a novel text corpus containing relatively <span id="A2.SS0.SSS0.Px1.p1.2.2" class="ltx_text ltx_font_italic">new</span> knowledge as <math id="A2.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A2.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="A2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. We use news-please&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hamborg et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>, similar to the CC-NEWS&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> and REALNEWS dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zellers et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, to crawl 221,779 news articles published from May 2020 to April 2021. LMs initially pretrained on <math id="A2.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A2.SS0.SSS0.Px1.p1.2.m2.1a"><msub id="A2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> constructed before May 2020 can be continually pretrained on <span id="A2.SS0.SSS0.Px1.p1.2.3" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> to gain relatively <span id="A2.SS0.SSS0.Px1.p1.2.4" class="ltx_text ltx_font_italic">recent</span> world knowledge.
</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_smallcaps ltx_title_paragraph">InvariantLAMA</h5>

<div id="A2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px2.p1.1" class="ltx_p">We create <span id="A2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, a subset of the LAMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> task for measuring <span id="A2.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">time-invariant</span> knowledge which might be forgotten during CKL. Among the 41 relations of the T-REx&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Elsahar et&nbsp;al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite> subset of LAMA, we manually select 28 relation types that probe for <span id="A2.SS0.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">time-invariant</span> instances (a full list of <span id="A2.SS0.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">time-invariant</span> relations are provided in Appendix <a href="#A2.SS1" title="B.1 Time-invariant relations of LAMA ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>). We also remove instances where the answer overlapped with the subject following <cite class="ltx_cite ltx_citemacro_citet">Poerner et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite> since the answers for these instances can be inferred from the cloze statement itself. Lastly, we remove instances where the answer was a non-entity to leave only the instances that require world knowledge for prediction on their answers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">
<span id="A2.SS0.SSS0.Px3.1.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A2.SS0.SSS0.Px3.2.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span>
</h5>

<div id="A2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px3.p1.4" class="ltx_p">We construct <span id="A2.SS0.SSS0.Px3.p1.4.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A2.SS0.SSS0.Px3.p1.4.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span> for measuring the update of outdated knowledge and acquisition of new knowledge during CKL. The challenge of constructing <span id="A2.SS0.SSS0.Px3.p1.4.3" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> is that a knowledge instance can be only considered as the knowledge that requires update only if it is present in both <math id="A2.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p1.1.m1.1a"><msub id="A2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="A2.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p1.2.m2.1a"><msub id="A2.SS0.SSS0.Px3.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> with changed details, and the challenge of constructing <span id="A2.SS0.SSS0.Px3.p1.4.4" class="ltx_text ltx_font_smallcaps">NewLAMA</span> is that the knowledge can be considered new only if it is in <math id="A2.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p1.3.m3.1a"><msub id="A2.SS0.SSS0.Px3.p1.3.m3.1.1" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.3.m3.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> but not in <math id="A2.SS0.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p1.4.m4.1a"><msub id="A2.SS0.SSS0.Px3.p1.4.m4.1.1" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.3" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.4.m4.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. Therefore we set up the data construction process carefully. The pipeline for the creation of a single instance of <span id="A2.SS0.SSS0.Px3.p1.4.5" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A2.SS0.SSS0.Px3.p1.4.6" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, is shown in Figure <a href="#A2.F4" title="Figure 4 ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a). Each potential instance starts off from a single article from <span id="A2.SS0.SSS0.Px3.p1.4.7" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> and goes through the pipeline which will end up being (1) discarded (2) added to <span id="A2.SS0.SSS0.Px3.p1.4.8" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> or (3) added to <span id="A2.SS0.SSS0.Px3.p1.4.9" class="ltx_text ltx_font_smallcaps">NewLAMA</span> in the end. The procedure is as follows:</p>
</div>
<div id="A2.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px3.p2.5" class="ltx_p">(1) First, a list of Probably-Asked Questions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> are generated using the PAQ question generator on a single news article from <span id="A2.SS0.SSS0.Px3.p2.5.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span>. (2) The list of PAQs and the news article is given to the crowd-sourced worker to select a question that asks for the most <span id="A2.SS0.SSS0.Px3.p2.5.2" class="ltx_text ltx_font_italic">recent</span> knowledge for which the answer (denoted as <span id="A2.SS0.SSS0.Px3.p2.5.3" class="ltx_text ltx_font_italic">new answer</span>) can be found in the article. (3) The crowd-source worker is instructed to convert the question into a cloze sentence so that it can be given as input to a pretrained T5 LM. The predictions of the T5 LM are stored along with the questions and cloze sentences. (4) The expert annotator ensures the quality of the questions and cloze sentences by correcting them whenever necessary and checks whether the model prediction is correct by searching through the C4 corpus as a representative of <math id="A2.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p2.1.m1.1a"><msub id="A2.SS0.SSS0.Px3.p2.1.m1.1.1" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p2.1.m1.1.1.2" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p2.1.m1.1.1.3" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p2.1.m1.1b"><apply id="A2.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p2.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> <span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>The expert annotators are instructed to use <a target="_blank" href="https://c4-search.apps.allenai.org/" title="" class="ltx_ref ltx_href">https://c4-search.apps.allenai.org/</a> for searching through the C4 corpus.</span></span></span>. If the prediction is correct and the prediction is not the same with the <span id="A2.SS0.SSS0.Px3.p2.5.4" class="ltx_text ltx_font_italic">new answer</span>, the following instance must be present in both <math id="A2.SS0.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p2.2.m2.1a"><msub id="A2.SS0.SSS0.Px3.p2.2.m2.1.1" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p2.2.m2.1.1.2" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p2.2.m2.1.1.3" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p2.2.m2.1b"><apply id="A2.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p2.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p2.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="A2.SS0.SSS0.Px3.p2.3.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p2.3.m3.1a"><msub id="A2.SS0.SSS0.Px3.p2.3.m3.1.1" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p2.3.m3.1.1.2" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p2.3.m3.1.1.3" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p2.3.m3.1b"><apply id="A2.SS0.SSS0.Px3.p2.3.m3.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p2.3.m3.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p2.3.m3.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p2.3.m3.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p2.3.m3.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p2.3.m3.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> with details changed, and thus is added to <span id="A2.SS0.SSS0.Px3.p2.5.5" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> along with the evidence document found in C4. If same, the instance is discarded because the instance is neither <span id="A2.SS0.SSS0.Px3.p2.5.6" class="ltx_text ltx_font_italic">updated</span> nor <span id="A2.SS0.SSS0.Px3.p2.5.7" class="ltx_text ltx_font_italic">new</span>. (5) Lastly, if the model prediction is wrong, the expert annotator is asked to find an alternative answer for the question in C4. If not found, the instance is added to <span id="A2.SS0.SSS0.Px3.p2.5.8" class="ltx_text ltx_font_smallcaps">NewLAMA</span> since the answer to the question could only be found in the article of <span id="A2.SS0.SSS0.Px3.p2.5.9" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> (<math id="A2.SS0.SSS0.Px3.p2.4.m4.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p2.4.m4.1a"><msub id="A2.SS0.SSS0.Px3.p2.4.m4.1.1" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p2.4.m4.1.1.2" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p2.4.m4.1.1.3" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p2.4.m4.1b"><apply id="A2.SS0.SSS0.Px3.p2.4.m4.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p2.4.m4.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p2.4.m4.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p2.4.m4.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p2.4.m4.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p2.4.m4.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>), but not in C4 (<math id="A2.SS0.SSS0.Px3.p2.5.m5.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A2.SS0.SSS0.Px3.p2.5.m5.1a"><msub id="A2.SS0.SSS0.Px3.p2.5.m5.1.1" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1.cmml"><mi id="A2.SS0.SSS0.Px3.p2.5.m5.1.1.2" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1.2.cmml">D</mi><mn id="A2.SS0.SSS0.Px3.p2.5.m5.1.1.3" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p2.5.m5.1b"><apply id="A2.SS0.SSS0.Px3.p2.5.m5.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px3.p2.5.m5.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px3.p2.5.m5.1.1.2.cmml" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1.2">𝐷</ci><cn type="integer" id="A2.SS0.SSS0.Px3.p2.5.m5.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p2.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p2.5.m5.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p2.5.m5.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>). Similarly, if the alternative answer is found in C4, we check whether it is the same as the <span id="A2.SS0.SSS0.Px3.p2.5.10" class="ltx_text ltx_font_italic">new answer</span> and add the instance to <span id="A2.SS0.SSS0.Px3.p2.5.11" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> if not the same and disregard it otherwise.</p>
</div>
<div id="A2.SS0.SSS0.Px3.p3" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px3.p3.1" class="ltx_p">Throughout the whole process, a validator checks the sanity of the data and gives detailed real-time feedback on the work of the annotator.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_smallcaps ltx_title_paragraph">NewLAMA-Easy</h5>

<div id="A2.SS0.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px4.p1.1" class="ltx_p">Even though <span id="A2.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">NewLAMA</span> corresponds to our exact definition of <span id="A2.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_italic">new knowledge</span> that we define in the task formulation, scaling the size of the dataset was difficult since each instance required searching the whole C4 database for answers. Instead, we provide a much larger, <span id="A2.SS0.SSS0.Px4.p1.1.3" class="ltx_text ltx_font_italic">easier</span> variant <span id="A2.SS0.SSS0.Px4.p1.1.4" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> where we test the general new knowledge acquired during continued pretraining on <span id="A2.SS0.SSS0.Px4.p1.1.5" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span>. The pipeline for the creation of a single instance of <span id="A2.SS0.SSS0.Px4.p1.1.6" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> is shown in Figure <a href="#A2.F4" title="Figure 4 ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b) and follows the following procedures:</p>
</div>
<div id="A2.SS0.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px4.p2.1" class="ltx_p">(1) First, the crowd-sourced worker is instructed to classify whether the given article contains <span id="A2.SS0.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_italic">new</span> information or not. (We define <span id="A2.SS0.SSS0.Px4.p2.1.2" class="ltx_text ltx_font_italic">new</span> as not likely to be known before May 2020). If the article contains new information, the worker is instructed to select a sentence from the article that contains the most <span id="A2.SS0.SSS0.Px4.p2.1.3" class="ltx_text ltx_font_italic">recent</span> information and an <span id="A2.SS0.SSS0.Px4.p2.1.4" class="ltx_text ltx_font_italic">entity</span> among the possible answer candidates in the sentence and discard the article if otherwise. We provide the possible entities through a Named-Entity Recognition Model. (2) We make the selected sentence <span id="A2.SS0.SSS0.Px4.p2.1.5" class="ltx_text ltx_font_italic">stand-alone</span> from the article through the decontextualization model provided by <cite class="ltx_cite ltx_citemacro_citet">Choi et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>. (3) The decontextualized sentence is paraphrased by a back-translation model (en→de→en)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tiedemann &amp; Thottingal, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite> and checked whether the selected word is still in the paraphrased sentence; the sentence is discarded if not. (4) Next, we mask out the selected word from the sentence and ask two crowd-sourced workers to convert the cloze sentence into a question and answer the question. (5) If the answers agree among the workers as well as correspond to the actual selected word, we add the instance to <span id="A2.SS0.SSS0.Px4.p2.1.6" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span>.</p>
</div>
<div id="A2.SS0.SSS0.Px4.p3" class="ltx_para ltx_noindent">
<p id="A2.SS0.SSS0.Px4.p3.1" class="ltx_p">The specific interfaces used for the mturk HITs are provided in Appendix <a href="#A2.SS2" title="B.2 Interfaces used for the construction of CKL benchmark ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>. Statistics of the constructed datasets are in Appendix <a href="#A2.SS3" title="B.3 Dataset Statistics and Examples ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>.</p>
</div>
</section>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Time-invariant relations of LAMA</h3>

<div id="A2.SS1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS1.p1.1" class="ltx_p">Table <a href="#A2.T4" title="Table 4 ‣ B.1 Time-invariant relations of LAMA ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the list of 28 time-invariant relations of <span id="A2.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>. We manually filter the 44 original LAMA relations to leave only the time-invariant relations. Templates such as “[X] works for [Y] .” and “[X] is a member of [Y] .” are excluded because the answer may change for different timestamps. In the template, [X] and [Y] refers to subject and object labels, respectively. Given a template with only the subject included, the model has to predict the object label [Y] for knowledge probing.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Relations of <span id="A2.T4.2.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span></figcaption>
<table id="A2.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A2.T4.3.1" class="ltx_tr">
<td id="A2.T4.3.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A2.T4.3.1.1.1" class="ltx_text ltx_font_bold">Relation</span></td>
<td id="A2.T4.3.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T4.3.1.2.1" class="ltx_text ltx_font_bold">Template ([X], [Y])</span></td>
<td id="A2.T4.3.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T4.3.1.3.1" class="ltx_text ltx_font_bold">Example</span></td>
</tr>
<tr id="A2.T4.3.2" class="ltx_tr">
<td id="A2.T4.3.2.1" class="ltx_td ltx_align_left ltx_border_t">P19</td>
<td id="A2.T4.3.2.2" class="ltx_td ltx_align_center ltx_border_t">[X] was born in [Y] .</td>
<td id="A2.T4.3.2.3" class="ltx_td ltx_align_center ltx_border_t">Taras Kuzio was born in Halifax .</td>
</tr>
<tr id="A2.T4.3.3" class="ltx_tr">
<td id="A2.T4.3.3.1" class="ltx_td ltx_align_left">P20</td>
<td id="A2.T4.3.3.2" class="ltx_td ltx_align_center">[X] died in [Y] .</td>
<td id="A2.T4.3.3.3" class="ltx_td ltx_align_center">Georgios Roilos died in Athens.</td>
</tr>
<tr id="A2.T4.3.4" class="ltx_tr">
<td id="A2.T4.3.4.1" class="ltx_td ltx_align_left">P279</td>
<td id="A2.T4.3.4.2" class="ltx_td ltx_align_center">[X] is a subclass of [Y].</td>
<td id="A2.T4.3.4.3" class="ltx_td ltx_align_center">Hutterite German is a subclass of Bavarian .</td>
</tr>
<tr id="A2.T4.3.5" class="ltx_tr">
<td id="A2.T4.3.5.1" class="ltx_td ltx_align_left">P37</td>
<td id="A2.T4.3.5.2" class="ltx_td ltx_align_center">The official language of [X] is [Y].</td>
<td id="A2.T4.3.5.3" class="ltx_td ltx_align_center">The official language of Azad Kashmir is English .</td>
</tr>
<tr id="A2.T4.3.6" class="ltx_tr">
<td id="A2.T4.3.6.1" class="ltx_td ltx_align_left">P449</td>
<td id="A2.T4.3.6.2" class="ltx_td ltx_align_center">[X] was originally aired on [Y] .</td>
<td id="A2.T4.3.6.3" class="ltx_td ltx_align_center">Microsoap was originally aired on BBC.</td>
</tr>
<tr id="A2.T4.3.7" class="ltx_tr">
<td id="A2.T4.3.7.1" class="ltx_td ltx_align_left">P47</td>
<td id="A2.T4.3.7.2" class="ltx_td ltx_align_center">[X] shares border with [Y] .</td>
<td id="A2.T4.3.7.3" class="ltx_td ltx_align_center">Illinois shares border with Kentucky .</td>
</tr>
<tr id="A2.T4.3.8" class="ltx_tr">
<td id="A2.T4.3.8.1" class="ltx_td ltx_align_left">P138</td>
<td id="A2.T4.3.8.2" class="ltx_td ltx_align_center">[X] is named after [Y] .</td>
<td id="A2.T4.3.8.3" class="ltx_td ltx_align_center">Logan International Airport is named after Boston .</td>
</tr>
<tr id="A2.T4.3.9" class="ltx_tr">
<td id="A2.T4.3.9.1" class="ltx_td ltx_align_left">P364</td>
<td id="A2.T4.3.9.2" class="ltx_td ltx_align_center">The original language of [X] is [Y] .</td>
<td id="A2.T4.3.9.3" class="ltx_td ltx_align_center">The original language of The Fatal Eggs is Russian .</td>
</tr>
<tr id="A2.T4.3.10" class="ltx_tr">
<td id="A2.T4.3.10.1" class="ltx_td ltx_align_left">P527</td>
<td id="A2.T4.3.10.2" class="ltx_td ltx_align_center">[X] consists of [Y] .</td>
<td id="A2.T4.3.10.3" class="ltx_td ltx_align_center">AIM alliance consists of Apple .</td>
</tr>
<tr id="A2.T4.3.11" class="ltx_tr">
<td id="A2.T4.3.11.1" class="ltx_td ltx_align_left">P176</td>
<td id="A2.T4.3.11.2" class="ltx_td ltx_align_center">[X] is produced by [Y] .</td>
<td id="A2.T4.3.11.3" class="ltx_td ltx_align_center">Alfa Romeo 155 is produced by Fiat .</td>
</tr>
<tr id="A2.T4.3.12" class="ltx_tr">
<td id="A2.T4.3.12.1" class="ltx_td ltx_align_left">P27</td>
<td id="A2.T4.3.12.2" class="ltx_td ltx_align_center">[X] is [Y] citizen .</td>
<td id="A2.T4.3.12.3" class="ltx_td ltx_align_center">Woodrow Lloyd is Canada citizen .</td>
</tr>
<tr id="A2.T4.3.13" class="ltx_tr">
<td id="A2.T4.3.13.1" class="ltx_td ltx_align_left">P407</td>
<td id="A2.T4.3.13.2" class="ltx_td ltx_align_center">[X] was written in [Y] .</td>
<td id="A2.T4.3.13.3" class="ltx_td ltx_align_center">France Culture was written in French .</td>
</tr>
<tr id="A2.T4.3.14" class="ltx_tr">
<td id="A2.T4.3.14.1" class="ltx_td ltx_align_left">P30</td>
<td id="A2.T4.3.14.2" class="ltx_td ltx_align_center">[X] is located in [Y] .</td>
<td id="A2.T4.3.14.3" class="ltx_td ltx_align_center">Lavoisier Island is located in Antarctica .</td>
</tr>
<tr id="A2.T4.3.15" class="ltx_tr">
<td id="A2.T4.3.15.1" class="ltx_td ltx_align_left">P178</td>
<td id="A2.T4.3.15.2" class="ltx_td ltx_align_center">[X] is developed by [Y].</td>
<td id="A2.T4.3.15.3" class="ltx_td ltx_align_center">Tizen is developed by Intel .</td>
</tr>
<tr id="A2.T4.3.16" class="ltx_tr">
<td id="A2.T4.3.16.1" class="ltx_td ltx_align_left">P1376</td>
<td id="A2.T4.3.16.2" class="ltx_td ltx_align_center">[X] is the capital of [Y],</td>
<td id="A2.T4.3.16.3" class="ltx_td ltx_align_center">London is the capital of England .</td>
</tr>
<tr id="A2.T4.3.17" class="ltx_tr">
<td id="A2.T4.3.17.1" class="ltx_td ltx_align_left">P131</td>
<td id="A2.T4.3.17.2" class="ltx_td ltx_align_center">[X] is located in [Y] .</td>
<td id="A2.T4.3.17.3" class="ltx_td ltx_align_center">Pershing County is located in Nevada .</td>
</tr>
<tr id="A2.T4.3.18" class="ltx_tr">
<td id="A2.T4.3.18.1" class="ltx_td ltx_align_left">P1412</td>
<td id="A2.T4.3.18.2" class="ltx_td ltx_align_center">[X] used to communicate in [Y].</td>
<td id="A2.T4.3.18.3" class="ltx_td ltx_align_center">Jacques Rivette used to communicate in French .</td>
</tr>
<tr id="A2.T4.3.19" class="ltx_tr">
<td id="A2.T4.3.19.1" class="ltx_td ltx_align_left">P17</td>
<td id="A2.T4.3.19.2" class="ltx_td ltx_align_center">[X] is located in [Y] .</td>
<td id="A2.T4.3.19.3" class="ltx_td ltx_align_center">Eibenstock is located in Germany .</td>
</tr>
<tr id="A2.T4.3.20" class="ltx_tr">
<td id="A2.T4.3.20.1" class="ltx_td ltx_align_left">P276</td>
<td id="A2.T4.3.20.2" class="ltx_td ltx_align_center">[X] is located in [Y] .</td>
<td id="A2.T4.3.20.3" class="ltx_td ltx_align_center">Delhi Technological University is located in India .</td>
</tr>
<tr id="A2.T4.3.21" class="ltx_tr">
<td id="A2.T4.3.21.1" class="ltx_td ltx_align_left">P937</td>
<td id="A2.T4.3.21.2" class="ltx_td ltx_align_center">[X] used to work in [Y].</td>
<td id="A2.T4.3.21.3" class="ltx_td ltx_align_center">Pierre Trudeau used to work in Ottawa .</td>
</tr>
<tr id="A2.T4.3.22" class="ltx_tr">
<td id="A2.T4.3.22.1" class="ltx_td ltx_align_left">P140</td>
<td id="A2.T4.3.22.2" class="ltx_td ltx_align_center">[X] is affiliated with the [Y] religion .</td>
<td id="A2.T4.3.22.3" class="ltx_td ltx_align_center">Emirate of Granada is affiliated with the Islam religion .</td>
</tr>
<tr id="A2.T4.3.23" class="ltx_tr">
<td id="A2.T4.3.23.1" class="ltx_td ltx_align_left">P103</td>
<td id="A2.T4.3.23.2" class="ltx_td ltx_align_center">The native language of [X] is [Y] .</td>
<td id="A2.T4.3.23.3" class="ltx_td ltx_align_center">The native language of Anastasy Vonsyatsky is Russian .</td>
</tr>
<tr id="A2.T4.3.24" class="ltx_tr">
<td id="A2.T4.3.24.1" class="ltx_td ltx_align_left">P190</td>
<td id="A2.T4.3.24.2" class="ltx_td ltx_align_center">[X] and [Y] are twin cities .</td>
<td id="A2.T4.3.24.3" class="ltx_td ltx_align_center">Beijing and Milan are twin cities .</td>
</tr>
<tr id="A2.T4.3.25" class="ltx_tr">
<td id="A2.T4.3.25.1" class="ltx_td ltx_align_left">P1001</td>
<td id="A2.T4.3.25.2" class="ltx_td ltx_align_center">[X] is a legal term in [Y] .</td>
<td id="A2.T4.3.25.3" class="ltx_td ltx_align_center">Surgeon General is a legal term in Canada .</td>
</tr>
<tr id="A2.T4.3.26" class="ltx_tr">
<td id="A2.T4.3.26.1" class="ltx_td ltx_align_left">P495</td>
<td id="A2.T4.3.26.2" class="ltx_td ltx_align_center">[X] was created in [Y] .</td>
<td id="A2.T4.3.26.3" class="ltx_td ltx_align_center">La Grande Vadrouille was created in France .</td>
</tr>
<tr id="A2.T4.3.27" class="ltx_tr">
<td id="A2.T4.3.27.1" class="ltx_td ltx_align_left">P36</td>
<td id="A2.T4.3.27.2" class="ltx_td ltx_align_center">The capital of [X] is [Y] .</td>
<td id="A2.T4.3.27.3" class="ltx_td ltx_align_center">The capital of Granville County is Oxford .</td>
</tr>
<tr id="A2.T4.3.28" class="ltx_tr">
<td id="A2.T4.3.28.1" class="ltx_td ltx_align_left">P740</td>
<td id="A2.T4.3.28.2" class="ltx_td ltx_align_center">[X] was founded in [Y].</td>
<td id="A2.T4.3.28.3" class="ltx_td ltx_align_center">Grimaldi Group was founded in Naples .</td>
</tr>
<tr id="A2.T4.3.29" class="ltx_tr">
<td id="A2.T4.3.29.1" class="ltx_td ltx_align_left ltx_border_bb">P361</td>
<td id="A2.T4.3.29.2" class="ltx_td ltx_align_center ltx_border_bb">[X] is part of [Y] .</td>
<td id="A2.T4.3.29.3" class="ltx_td ltx_align_center ltx_border_bb">Sinqa is part of Andes .</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Interfaces used for the construction of CKL benchmark</h3>

<div id="A2.SS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS2.p1.1" class="ltx_p">The Mturk interface used during construction of <span id="A2.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A2.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, <span id="A2.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span>, and <span id="A2.SS2.p1.1.4" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> are shown in Figure <a href="#A2.F5" title="Figure 5 ‣ B.2 Interfaces used for the construction of CKL benchmark ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#A2.F6" title="Figure 6 ‣ B.2 Interfaces used for the construction of CKL benchmark ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#A2.F7" title="Figure 7 ‣ B.2 Interfaces used for the construction of CKL benchmark ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, respectively.</p>
</div>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2110.03215/assets/figures/interface1.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="494" height="361" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Mturk interface used for construction of <span id="A2.F5.3.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A2.F5.4.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span></figcaption>
</figure>
<figure id="A2.F6" class="ltx_figure"><img src="/html/2110.03215/assets/figures/interface2.png" id="A2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="494" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>First mturk interface used for construction of <span id="A2.F6.2.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></figcaption>
</figure>
<figure id="A2.F7" class="ltx_figure"><img src="/html/2110.03215/assets/figures/interface3.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="494" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Second mturk interface used for construction of <span id="A2.F7.2.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></figcaption>
</figure>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Dataset Statistics and Examples</h3>

<figure id="A2.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>CKL benchmark dataset statistics</figcaption>
<table id="A2.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A2.T5.1.1" class="ltx_tr">
<td id="A2.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="A2.T5.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="A2.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="A2.T5.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="A2.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A2.T5.1.1.3.1" class="ltx_text ltx_font_bold">Avg. Input</span></td>
<td id="A2.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A2.T5.1.1.4.1" class="ltx_text ltx_font_bold">Avg. Answer</span></td>
<td id="A2.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="A2.T5.1.1.5.1" class="ltx_text ltx_font_bold">Answer Types</span></td>
</tr>
<tr id="A2.T5.1.2" class="ltx_tr">
<td id="A2.T5.1.2.1" class="ltx_td ltx_align_center">
<span id="A2.T5.1.2.1.1" class="ltx_text ltx_font_bold">Token #</span></td>
<td id="A2.T5.1.2.2" class="ltx_td ltx_align_center">
<span id="A2.T5.1.2.2.1" class="ltx_text ltx_font_bold">Token #</span></td>
</tr>
<tr id="A2.T5.1.3" class="ltx_tr">
<td id="A2.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A2.T5.1.3.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span></td>
<td id="A2.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A2.T5.1.3.2.1" class="ltx_text">17474</span></td>
<td id="A2.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2">
<span id="A2.T5.1.3.3.1" class="ltx_text">11.9</span></td>
<td id="A2.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2">
<span id="A2.T5.1.3.4.1" class="ltx_text">1.3</span></td>
<td id="A2.T5.1.3.5" class="ltx_td ltx_align_left ltx_border_t">Geographical (54%), Language (14.9%), Nationalities (7.2%)</td>
</tr>
<tr id="A2.T5.1.4" class="ltx_tr">
<td id="A2.T5.1.4.1" class="ltx_td ltx_align_left">Person (6.3%), Location (5.7%), Organization (5.3%), etc. (6.6%)</td>
</tr>
<tr id="A2.T5.1.5" class="ltx_tr">
<td id="A2.T5.1.5.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A2.T5.1.5.1.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span></td>
<td id="A2.T5.1.5.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A2.T5.1.5.2.1" class="ltx_text">924</span></td>
<td id="A2.T5.1.5.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2">
<span id="A2.T5.1.5.3.1" class="ltx_text">13.7</span></td>
<td id="A2.T5.1.5.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2">
<span id="A2.T5.1.5.4.1" class="ltx_text">9.4</span></td>
<td id="A2.T5.1.5.5" class="ltx_td ltx_align_left ltx_border_t">Person (61.47%), Organization (8.3%), Geographical (6.6%),</td>
</tr>
<tr id="A2.T5.1.6" class="ltx_tr">
<td id="A2.T5.1.6.1" class="ltx_td ltx_align_left">Numerals (5.19%), Date (2.4%), etc. (16.04%)</td>
</tr>
<tr id="A2.T5.1.7" class="ltx_tr">
<td id="A2.T5.1.7.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A2.T5.1.7.1.1" class="ltx_text ltx_font_smallcaps">NewLAMA</span></td>
<td id="A2.T5.1.7.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A2.T5.1.7.2.1" class="ltx_text">797</span></td>
<td id="A2.T5.1.7.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2">
<span id="A2.T5.1.7.3.1" class="ltx_text">14.7</span></td>
<td id="A2.T5.1.7.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2">
<span id="A2.T5.1.7.4.1" class="ltx_text">8.7</span></td>
<td id="A2.T5.1.7.5" class="ltx_td ltx_align_left ltx_border_t">Person (59.7%), Organization (10.2%), Numerals (7.6%)</td>
</tr>
<tr id="A2.T5.1.8" class="ltx_tr">
<td id="A2.T5.1.8.1" class="ltx_td ltx_align_left">Date (5.3%), Geographical (4.8%), etc. (12.4%)</td>
</tr>
<tr id="A2.T5.1.9" class="ltx_tr">
<td id="A2.T5.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A2.T5.1.9.1.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></td>
<td id="A2.T5.1.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="A2.T5.1.9.2.1" class="ltx_text">11177</span></td>
<td id="A2.T5.1.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2">
<span id="A2.T5.1.9.3.1" class="ltx_text">44.4</span></td>
<td id="A2.T5.1.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2">
<span id="A2.T5.1.9.4.1" class="ltx_text">6.1</span></td>
<td id="A2.T5.1.9.5" class="ltx_td ltx_align_left ltx_border_t">Person (48.5%), Organization (13%), Geographical (9.8%)</td>
</tr>
<tr id="A2.T5.1.10" class="ltx_tr">
<td id="A2.T5.1.10.1" class="ltx_td ltx_align_left ltx_border_bb">Date (5.5%), Nationalities (3.4%), Numerals (2.5%), etc. (17.3%)</td>
</tr>
</tbody></table>
</figure>
<figure id="A2.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Examples of <span id="A2.T6.5.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, <span id="A2.T6.6.2" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span>, <span id="A2.T6.7.3" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, and <span id="A2.T6.8.4" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></figcaption>
<table id="A2.T6.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A2.T6.9.1" class="ltx_tr">
<td id="A2.T6.9.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="A2.T6.9.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="A2.T6.9.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="A2.T6.9.1.2.1" class="ltx_text ltx_font_bold">Input</span></td>
<td id="A2.T6.9.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A2.T6.9.1.3.1" class="ltx_text ltx_font_bold">Output</span></td>
</tr>
<tr id="A2.T6.9.2" class="ltx_tr">
<td id="A2.T6.9.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="3"><span id="A2.T6.9.2.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span></td>
<td id="A2.T6.9.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">iPod Touch is produced by <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span>.</td>
<td id="A2.T6.9.2.3" class="ltx_td ltx_align_left ltx_border_t">Apple</td>
</tr>
<tr id="A2.T6.9.3" class="ltx_tr">
<td id="A2.T6.9.3.1" class="ltx_td ltx_align_left ltx_border_r">The Sharon Cuneta Show was created in <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span>.</td>
<td id="A2.T6.9.3.2" class="ltx_td ltx_align_left">Philippines</td>
</tr>
<tr id="A2.T6.9.4" class="ltx_tr">
<td id="A2.T6.9.4.1" class="ltx_td ltx_align_left ltx_border_r">The native language of Lee Chang-dong is <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span>.</td>
<td id="A2.T6.9.4.2" class="ltx_td ltx_align_left">Korean</td>
</tr>
<tr id="A2.T6.9.5" class="ltx_tr">
<td id="A2.T6.9.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="6"><span id="A2.T6.9.5.1.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span></td>
<td id="A2.T6.9.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the prime minister of England.</td>
<td id="A2.T6.9.5.3" class="ltx_td ltx_align_left ltx_border_t">Theresa May→</td>
</tr>
<tr id="A2.T6.9.6" class="ltx_tr">
<td id="A2.T6.9.6.1" class="ltx_td ltx_border_r"></td>
<td id="A2.T6.9.6.2" class="ltx_td ltx_align_left">Boris Johnson</td>
</tr>
<tr id="A2.T6.9.7" class="ltx_tr">
<td id="A2.T6.9.7.1" class="ltx_td ltx_align_left ltx_border_r">
<span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> has the most passing yards in the NFL.</td>
<td id="A2.T6.9.7.2" class="ltx_td ltx_align_left">Brady Quinn→</td>
</tr>
<tr id="A2.T6.9.8" class="ltx_tr">
<td id="A2.T6.9.8.1" class="ltx_td ltx_border_r"></td>
<td id="A2.T6.9.8.2" class="ltx_td ltx_align_left">Jalen Guyton</td>
</tr>
<tr id="A2.T6.9.9" class="ltx_tr">
<td id="A2.T6.9.9.1" class="ltx_td ltx_align_left ltx_border_r">Bale has <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> champions league titles with</td>
<td id="A2.T6.9.9.2" class="ltx_td ltx_align_left" rowspan="2"><span id="A2.T6.9.9.2.1" class="ltx_text">3→4</span></td>
</tr>
<tr id="A2.T6.9.10" class="ltx_tr">
<td id="A2.T6.9.10.1" class="ltx_td ltx_align_left ltx_border_r">Real Madrid.</td>
</tr>
<tr id="A2.T6.9.11" class="ltx_tr">
<td id="A2.T6.9.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="4"><span id="A2.T6.9.11.1.1" class="ltx_text ltx_font_smallcaps">NewLAMA</span></td>
<td id="A2.T6.9.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Alicia Braga plays <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> in the New Mutant.</td>
<td id="A2.T6.9.11.3" class="ltx_td ltx_align_left ltx_border_t">Cecilia Reyes</td>
</tr>
<tr id="A2.T6.9.12" class="ltx_tr">
<td id="A2.T6.9.12.1" class="ltx_td ltx_align_left ltx_border_r">
<span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> owns the rights to the Falcon and the</td>
<td id="A2.T6.9.12.2" class="ltx_td ltx_align_left" rowspan="2"><span id="A2.T6.9.12.2.1" class="ltx_text">Disney</span></td>
</tr>
<tr id="A2.T6.9.13" class="ltx_tr">
<td id="A2.T6.9.13.1" class="ltx_td ltx_align_left ltx_border_r">Winter Soldier.</td>
</tr>
<tr id="A2.T6.9.14" class="ltx_tr">
<td id="A2.T6.9.14.1" class="ltx_td ltx_align_left ltx_border_r">Tesla invested <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> in the digital currency bitcoin.</td>
<td id="A2.T6.9.14.2" class="ltx_td ltx_align_left">1.5 billion</td>
</tr>
<tr id="A2.T6.9.15" class="ltx_tr">
<td id="A2.T6.9.15.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="5"><span id="A2.T6.9.15.1.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></td>
<td id="A2.T6.9.15.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">The decision of the two volleyball stars Bria and Cimone</td>
<td id="A2.T6.9.15.3" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="A2.T6.9.15.3.1" class="ltx_text">Howard University</span></td>
</tr>
<tr id="A2.T6.9.16" class="ltx_tr">
<td id="A2.T6.9.16.1" class="ltx_td ltx_align_left ltx_border_r">Woodard to withdraw from the Power 5 School to study</td>
</tr>
<tr id="A2.T6.9.17" class="ltx_tr">
<td id="A2.T6.9.17.1" class="ltx_td ltx_align_left ltx_border_r">at <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> has become a national story.</td>
</tr>
<tr id="A2.T6.9.18" class="ltx_tr">
<td id="A2.T6.9.18.1" class="ltx_td ltx_align_left ltx_border_r">Allen Lazard is officially listed as questionable with a</td>
<td id="A2.T6.9.18.2" class="ltx_td ltx_align_left ltx_border_bb" rowspan="2"><span id="A2.T6.9.18.2.1" class="ltx_text">six</span></td>
</tr>
<tr id="A2.T6.9.19" class="ltx_tr">
<td id="A2.T6.9.19.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">nuclear injury after missing the last <span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> games.</td>
</tr>
</tbody></table>
</figure>
<div id="A2.SS3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS3.p1.1" class="ltx_p">We report the data statistics for the CKL benchmark in Table <a href="#A2.T5" title="Table 5 ‣ B.3 Dataset Statistics and Examples ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We measure the size, average input token length, average answer token length, and the answer types of each constructed dataset. One thing to consider is that LAMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> from which we constructed <span id="A2.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> is originally constructed for only single-token decoding (1.3 with the T5-tokenizer) because multi-token decoding entails additional, tunable parameters (beam size, n-gram repetition penalties, etc.). The newly constructed datasets <span id="A2.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span>, <span id="A2.SS3.p1.1.3" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, and <span id="A2.SS3.p1.1.4" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> require multi-token decoding which adds a level of difficulty for the task compared to <span id="A2.SS3.p1.1.5" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>. Moreover, <span id="A2.SS3.p1.1.6" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span> has a different input distribution (longer input sequences) than the other datasets since the decontextualization and back-translation processes are applied to create each instance, which makes the sentences longer. Lastly, some examples of the CKL benchmark datasets are provided in Table <a href="#A2.T6" title="Table 6 ‣ B.3 Dataset Statistics and Examples ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experimental Configuration</h2>

<section id="A3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pretraining Congifuration</h5>

<div id="A3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px1.p1.1" class="ltx_p">We utilize the T5 initially pretrained on C4 (April 2019) and continually pretrained with salient span masking&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> on Wikipedia (May 2020) as initialization. We use the checkpoints from <cite class="ltx_cite ltx_citemacro_citet">Wolf et&nbsp;al. (<a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>. We also perform the SSM objective during CKL because it was shown to help LMs “focus on problems that require world knowledge”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Roberts et&nbsp;al., <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="A3.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Continual Pretraining Configurations</h5>

<div id="A3.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px2.p1.1" class="ltx_p">The input and output sequence length is fixed to 350. We use gradient accumulation for cases where the same number of training batches could not be loaded on the GPUs due to the varying memory consumption required for different methods and set the global batch size to 60. We use Adafactor optimizer with an initial learning rate of 1e-3. We show the effects of learning rate variation regarding the trade-off between maintaining previous knowledge and acquiring new knowledge in Appendix <a href="#A5" title="Appendix E Exploring the Trade-off of Varying the Learning Rate for Continual Pretraining ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. We use learning rate warm-up for the first 10% of training and linearly decay the learning rate to half of the initial learning rate towards the end of training. For all of the experiments, we use 4 32GB V100 GPUs for training with each method except Mix-Review, where we use 16 32GB V100 GPUs. The details of the configurations used for evaluation on each individual CKL task are provided in Appendix <a href="#A3.SS0.SSS0.Px3" title="Evaluation Configurations ‣ Appendix C Experimental Configuration ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section id="A3.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Configurations</h5>

<div id="A3.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px3.p1.1" class="ltx_p">For T5 based models, all evaluation is done in a zero-shot manner and is processed with a single GPU. For <span id="A3.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, the input and output length is fixed as 25 and 4 respectively. For <span id="A3.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A3.SS0.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_smallcaps">NewLAMA</span>, the input and output length is 50 and 10 respectively. Lastly, the input and output length is 150 and 10 respectively for <span id="A3.SS0.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span>. The rationale of this hyperparameter is based on average input and answer token in Table <a href="#A2.T5" title="Table 5 ‣ B.3 Dataset Statistics and Examples ‣ Appendix B Dataset Construction ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="A3.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="A3.SS0.SSS0.Px3.p2.1" class="ltx_p">Unlike T5 models, GPT-2 based models need additional <span id="A3.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_italic">light-tuning</span> for 1 epoch for evaluation. For <span id="A3.SS0.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, the input and output length is 50 and 3 respectively. The training batch size is 32 and the learning rate is 1e-3. For evaluation on the acquisition of new knowledge, the input and output length is 100 and 10 respectively. The training batch size is 8 due to memory constraints and the learning rate is 1e-3. For both tuning processes, 4 V100 32GB GPUs are used. The detailed result and discussion of GPT-2 based models are shown in Appendix <a href="#A7" title="Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Hyperparameters for Implementation of CKL Methods</h2>

<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.4" class="ltx_p"><span id="A4.p1.4.1" class="ltx_text ltx_font_bold">RecAdam</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> We use the same hyperparameter setting for the optimizer as in <cite class="ltx_cite ltx_citemacro_citet">Chen et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>: we set the coefficient of the quadratic penalty <math id="A4.p1.1.m1.1" class="ltx_Math" alttext="{\gamma}" display="inline"><semantics id="A4.p1.1.m1.1a"><mi id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><ci id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">{\gamma}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">italic_γ</annotation></semantics></math> to 5,000, and select the best <math id="A4.p1.2.m2.1" class="ltx_Math" alttext="{t_{0}}" display="inline"><semantics id="A4.p1.2.m2.1a"><msub id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml"><mi id="A4.p1.2.m2.1.1.2" xref="A4.p1.2.m2.1.1.2.cmml">t</mi><mn id="A4.p1.2.m2.1.1.3" xref="A4.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><apply id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A4.p1.2.m2.1.1.1.cmml" xref="A4.p1.2.m2.1.1">subscript</csymbol><ci id="A4.p1.2.m2.1.1.2.cmml" xref="A4.p1.2.m2.1.1.2">𝑡</ci><cn type="integer" id="A4.p1.2.m2.1.1.3.cmml" xref="A4.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">{t_{0}}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.2.m2.1d">italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="A4.p1.3.m3.1" class="ltx_Math" alttext="{k}" display="inline"><semantics id="A4.p1.3.m3.1a"><mi id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><ci id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">{k}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.3.m3.1d">italic_k</annotation></semantics></math> in 100, 250, 500, 1,000 and 0.05, 0.1, 0.2, 0.5, 1 respectively for the annealing coefficient <math id="A4.p1.4.m4.1" class="ltx_Math" alttext="{\lambda(t)}" display="inline"><semantics id="A4.p1.4.m4.1a"><mrow id="A4.p1.4.m4.1.2" xref="A4.p1.4.m4.1.2.cmml"><mi id="A4.p1.4.m4.1.2.2" xref="A4.p1.4.m4.1.2.2.cmml">λ</mi><mo id="A4.p1.4.m4.1.2.1" xref="A4.p1.4.m4.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="A4.p1.4.m4.1.2.3.2" xref="A4.p1.4.m4.1.2.cmml"><mo stretchy="false" id="A4.p1.4.m4.1.2.3.2.1" xref="A4.p1.4.m4.1.2.cmml">(</mo><mi id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">t</mi><mo stretchy="false" id="A4.p1.4.m4.1.2.3.2.2" xref="A4.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><apply id="A4.p1.4.m4.1.2.cmml" xref="A4.p1.4.m4.1.2"><times id="A4.p1.4.m4.1.2.1.cmml" xref="A4.p1.4.m4.1.2.1"></times><ci id="A4.p1.4.m4.1.2.2.cmml" xref="A4.p1.4.m4.1.2.2">𝜆</ci><ci id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">{\lambda(t)}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.4.m4.1d">italic_λ ( italic_t )</annotation></semantics></math>.</p>
</div>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.1" class="ltx_p"><span id="A4.p2.1.1" class="ltx_text ltx_font_bold">Mix-Review</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> We use the English Wikipedia <span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a target="_blank" href="https://huggingface.co/datasets/wikipedia" title="" class="ltx_ref ltx_href">https://huggingface.co/datasets/wikipedia</a></span></span></span> to represent the original pretraining corpus. The mix-decay and mix-ratio are set to 4 and 0.7, respectively, which is the best hyperparameter setting in the paper.
</p>
</div>
<div id="A4.p3" class="ltx_para ltx_noindent">
<p id="A4.p3.3" class="ltx_p"><span id="A4.p3.3.1" class="ltx_text ltx_font_bold">LoRA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> We only freeze the encoder for the encoder-decoder LM and the entire model for the decoder-only LM. We use the optimal rank <math id="A4.p3.1.m1.1" class="ltx_Math" alttext="{r}" display="inline"><semantics id="A4.p3.1.m1.1a"><mi id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b"><ci id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">{r}</annotation><annotation encoding="application/x-llamapun" id="A4.p3.1.m1.1d">italic_r</annotation></semantics></math> of 4 and adapt both <math id="A4.p3.2.m2.1" class="ltx_Math" alttext="{W_{q}}" display="inline"><semantics id="A4.p3.2.m2.1a"><msub id="A4.p3.2.m2.1.1" xref="A4.p3.2.m2.1.1.cmml"><mi id="A4.p3.2.m2.1.1.2" xref="A4.p3.2.m2.1.1.2.cmml">W</mi><mi id="A4.p3.2.m2.1.1.3" xref="A4.p3.2.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p3.2.m2.1b"><apply id="A4.p3.2.m2.1.1.cmml" xref="A4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A4.p3.2.m2.1.1.1.cmml" xref="A4.p3.2.m2.1.1">subscript</csymbol><ci id="A4.p3.2.m2.1.1.2.cmml" xref="A4.p3.2.m2.1.1.2">𝑊</ci><ci id="A4.p3.2.m2.1.1.3.cmml" xref="A4.p3.2.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.2.m2.1c">{W_{q}}</annotation><annotation encoding="application/x-llamapun" id="A4.p3.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="A4.p3.3.m3.1" class="ltx_Math" alttext="{W_{v}}" display="inline"><semantics id="A4.p3.3.m3.1a"><msub id="A4.p3.3.m3.1.1" xref="A4.p3.3.m3.1.1.cmml"><mi id="A4.p3.3.m3.1.1.2" xref="A4.p3.3.m3.1.1.2.cmml">W</mi><mi id="A4.p3.3.m3.1.1.3" xref="A4.p3.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p3.3.m3.1b"><apply id="A4.p3.3.m3.1.1.cmml" xref="A4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="A4.p3.3.m3.1.1.1.cmml" xref="A4.p3.3.m3.1.1">subscript</csymbol><ci id="A4.p3.3.m3.1.1.2.cmml" xref="A4.p3.3.m3.1.1.2">𝑊</ci><ci id="A4.p3.3.m3.1.1.3.cmml" xref="A4.p3.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.3.m3.1c">{W_{v}}</annotation><annotation encoding="application/x-llamapun" id="A4.p3.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> in the self-attention module, which corresponds to the best performing hyperparameter setting in the paper.</p>
</div>
<div id="A4.p4" class="ltx_para ltx_noindent">
<p id="A4.p4.1" class="ltx_p"><span id="A4.p4.1.1" class="ltx_text ltx_font_bold">K-Adapter</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>)</cite> Similarly with T5-LoRA, we freeze the encoder for the encoder-decoder LM and the entire model for GPT-2. We implement <math id="A4.p4.1.m1.2" class="ltx_Math" alttext="{k}=2,3" display="inline"><semantics id="A4.p4.1.m1.2a"><mrow id="A4.p4.1.m1.2.3" xref="A4.p4.1.m1.2.3.cmml"><mi id="A4.p4.1.m1.2.3.2" xref="A4.p4.1.m1.2.3.2.cmml">k</mi><mo id="A4.p4.1.m1.2.3.1" xref="A4.p4.1.m1.2.3.1.cmml">=</mo><mrow id="A4.p4.1.m1.2.3.3.2" xref="A4.p4.1.m1.2.3.3.1.cmml"><mn id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">2</mn><mo id="A4.p4.1.m1.2.3.3.2.1" xref="A4.p4.1.m1.2.3.3.1.cmml">,</mo><mn id="A4.p4.1.m1.2.2" xref="A4.p4.1.m1.2.2.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.2b"><apply id="A4.p4.1.m1.2.3.cmml" xref="A4.p4.1.m1.2.3"><eq id="A4.p4.1.m1.2.3.1.cmml" xref="A4.p4.1.m1.2.3.1"></eq><ci id="A4.p4.1.m1.2.3.2.cmml" xref="A4.p4.1.m1.2.3.2">𝑘</ci><list id="A4.p4.1.m1.2.3.3.1.cmml" xref="A4.p4.1.m1.2.3.3.2"><cn type="integer" id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">2</cn><cn type="integer" id="A4.p4.1.m1.2.2.cmml" xref="A4.p4.1.m1.2.2">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.2c">{k}=2,3</annotation><annotation encoding="application/x-llamapun" id="A4.p4.1.m1.2d">italic_k = 2 , 3</annotation></semantics></math> for both T5 and GPT-2 to see the effect of increasing # of parameters. Unlike in the original paper, we set the configuration of the adapter identical to a single transformer layer from the original LM, ridding the need of an up-projection and down-projection layer.</p>
</div>
<div id="A4.p5" class="ltx_para ltx_noindent">
<p id="A4.p5.1" class="ltx_p"><span id="A4.p5.1.1" class="ltx_text ltx_font_bold">Modular</span> We use a projection layer before adding the hidden state outputs from both encoders to match the dimensions.</p>
</div>
<div id="A4.p6" class="ltx_para ltx_noindent">
<p id="A4.p6.1" class="ltx_p"><span id="A4.p6.1.1" class="ltx_text ltx_font_bold">Why do we add parameters to only the encoder for T5?</span> For parameter-expansion methods, we add parameters to only the encoder because the encoder is applied to the input sequence and the decoder is applied to the output sequence. Since most of the computational cost comes from the decoder computing for the output sequence in an auto-regressive manner as highlighted in <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, the newly added parameters in the encoder are roughly expected to have minimal additional computational cost.</p>
</div>
<div id="A4.p7" class="ltx_para ltx_noindent">
<p id="A4.p7.1" class="ltx_p"><span id="A4.p7.1.1" class="ltx_text ltx_font_bold">Why do we freeze parameters of only the encoder for T5?</span> K-Adapter and LoRA are initially proposed to freeze all of the parameters except for the newly added parameters. However, when applying this methodology to T5, it was empirically shown that unfreezing the parameters of the decoder results in better performances when utilized together with parameter-expansion methods in terms of overall trade-off.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Exploring the Trade-off of Varying the Learning Rate for Continual Pretraining</h2>

<figure id="A5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Result of T5-Vanilla and T5-Kadapters continually pretrained with various learning rates. The experiments are done under the setting of <span id="A5.T7.24.1" class="ltx_text ltx_font_smallcaps">Small</span> scenario in Table&nbsp;<a href="#S5.T3" title="Table 3 ‣ 5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, thus <math id="A5.T7.9.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A5.T7.9.m1.1b"><msub id="A5.T7.9.m1.1.1" xref="A5.T7.9.m1.1.1.cmml"><mi id="A5.T7.9.m1.1.1.2" xref="A5.T7.9.m1.1.1.2.cmml">D</mi><mn id="A5.T7.9.m1.1.1.3" xref="A5.T7.9.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T7.9.m1.1c"><apply id="A5.T7.9.m1.1.1.cmml" xref="A5.T7.9.m1.1.1"><csymbol cd="ambiguous" id="A5.T7.9.m1.1.1.1.cmml" xref="A5.T7.9.m1.1.1">subscript</csymbol><ci id="A5.T7.9.m1.1.1.2.cmml" xref="A5.T7.9.m1.1.1.2">𝐷</ci><cn type="integer" id="A5.T7.9.m1.1.1.3.cmml" xref="A5.T7.9.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.9.m1.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.9.m1.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> are C4 (April 2019) and Wikipedia (May 2020), and <math id="A5.T7.10.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A5.T7.10.m2.1b"><msub id="A5.T7.10.m2.1.1" xref="A5.T7.10.m2.1.1.cmml"><mi id="A5.T7.10.m2.1.1.2" xref="A5.T7.10.m2.1.1.2.cmml">D</mi><mn id="A5.T7.10.m2.1.1.3" xref="A5.T7.10.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T7.10.m2.1c"><apply id="A5.T7.10.m2.1.1.cmml" xref="A5.T7.10.m2.1.1"><csymbol cd="ambiguous" id="A5.T7.10.m2.1.1.1.cmml" xref="A5.T7.10.m2.1.1">subscript</csymbol><ci id="A5.T7.10.m2.1.1.2.cmml" xref="A5.T7.10.m2.1.1.2">𝐷</ci><cn type="integer" id="A5.T7.10.m2.1.1.3.cmml" xref="A5.T7.10.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.10.m2.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.10.m2.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is <span id="A5.T7.25.2" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span>. Each of IL and NLE stands for <span id="A5.T7.26.3" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> and <span id="A5.T7.27.4" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span>. The parameters of <span id="A5.T7.28.5" class="ltx_text">FUAR</span> are <math id="A5.T7.11.m3.1" class="ltx_Math" alttext="\mathbb{T}^{F}" display="inline"><semantics id="A5.T7.11.m3.1b"><msup id="A5.T7.11.m3.1.1" xref="A5.T7.11.m3.1.1.cmml"><mi id="A5.T7.11.m3.1.1.2" xref="A5.T7.11.m3.1.1.2.cmml">𝕋</mi><mi id="A5.T7.11.m3.1.1.3" xref="A5.T7.11.m3.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="A5.T7.11.m3.1c"><apply id="A5.T7.11.m3.1.1.cmml" xref="A5.T7.11.m3.1.1"><csymbol cd="ambiguous" id="A5.T7.11.m3.1.1.1.cmml" xref="A5.T7.11.m3.1.1">superscript</csymbol><ci id="A5.T7.11.m3.1.1.2.cmml" xref="A5.T7.11.m3.1.1.2">𝕋</ci><ci id="A5.T7.11.m3.1.1.3.cmml" xref="A5.T7.11.m3.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.11.m3.1d">\mathbb{T}^{F}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.11.m3.1e">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>, <math id="A5.T7.12.m4.1" class="ltx_Math" alttext="T_{1}^{U}" display="inline"><semantics id="A5.T7.12.m4.1b"><msubsup id="A5.T7.12.m4.1.1" xref="A5.T7.12.m4.1.1.cmml"><mi id="A5.T7.12.m4.1.1.2.2" xref="A5.T7.12.m4.1.1.2.2.cmml">T</mi><mn id="A5.T7.12.m4.1.1.2.3" xref="A5.T7.12.m4.1.1.2.3.cmml">1</mn><mi id="A5.T7.12.m4.1.1.3" xref="A5.T7.12.m4.1.1.3.cmml">U</mi></msubsup><annotation-xml encoding="MathML-Content" id="A5.T7.12.m4.1c"><apply id="A5.T7.12.m4.1.1.cmml" xref="A5.T7.12.m4.1.1"><csymbol cd="ambiguous" id="A5.T7.12.m4.1.1.1.cmml" xref="A5.T7.12.m4.1.1">superscript</csymbol><apply id="A5.T7.12.m4.1.1.2.cmml" xref="A5.T7.12.m4.1.1"><csymbol cd="ambiguous" id="A5.T7.12.m4.1.1.2.1.cmml" xref="A5.T7.12.m4.1.1">subscript</csymbol><ci id="A5.T7.12.m4.1.1.2.2.cmml" xref="A5.T7.12.m4.1.1.2.2">𝑇</ci><cn type="integer" id="A5.T7.12.m4.1.1.2.3.cmml" xref="A5.T7.12.m4.1.1.2.3">1</cn></apply><ci id="A5.T7.12.m4.1.1.3.cmml" xref="A5.T7.12.m4.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.12.m4.1d">T_{1}^{U}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.12.m4.1e">italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math id="A5.T7.13.m5.1" class="ltx_Math" alttext="T_{1}^{A}" display="inline"><semantics id="A5.T7.13.m5.1b"><msubsup id="A5.T7.13.m5.1.1" xref="A5.T7.13.m5.1.1.cmml"><mi id="A5.T7.13.m5.1.1.2.2" xref="A5.T7.13.m5.1.1.2.2.cmml">T</mi><mn id="A5.T7.13.m5.1.1.2.3" xref="A5.T7.13.m5.1.1.2.3.cmml">1</mn><mi id="A5.T7.13.m5.1.1.3" xref="A5.T7.13.m5.1.1.3.cmml">A</mi></msubsup><annotation-xml encoding="MathML-Content" id="A5.T7.13.m5.1c"><apply id="A5.T7.13.m5.1.1.cmml" xref="A5.T7.13.m5.1.1"><csymbol cd="ambiguous" id="A5.T7.13.m5.1.1.1.cmml" xref="A5.T7.13.m5.1.1">superscript</csymbol><apply id="A5.T7.13.m5.1.1.2.cmml" xref="A5.T7.13.m5.1.1"><csymbol cd="ambiguous" id="A5.T7.13.m5.1.1.2.1.cmml" xref="A5.T7.13.m5.1.1">subscript</csymbol><ci id="A5.T7.13.m5.1.1.2.2.cmml" xref="A5.T7.13.m5.1.1.2.2">𝑇</ci><cn type="integer" id="A5.T7.13.m5.1.1.2.3.cmml" xref="A5.T7.13.m5.1.1.2.3">1</cn></apply><ci id="A5.T7.13.m5.1.1.3.cmml" xref="A5.T7.13.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.13.m5.1d">T_{1}^{A}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.13.m5.1e">italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT</annotation></semantics></math>, the tasks measuring the amount of time-invariant knowledge from corpus <math id="A5.T7.14.m6.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A5.T7.14.m6.1b"><msub id="A5.T7.14.m6.1.1" xref="A5.T7.14.m6.1.1.cmml"><mi id="A5.T7.14.m6.1.1.2" xref="A5.T7.14.m6.1.1.2.cmml">D</mi><mn id="A5.T7.14.m6.1.1.3" xref="A5.T7.14.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T7.14.m6.1c"><apply id="A5.T7.14.m6.1.1.cmml" xref="A5.T7.14.m6.1.1"><csymbol cd="ambiguous" id="A5.T7.14.m6.1.1.1.cmml" xref="A5.T7.14.m6.1.1">subscript</csymbol><ci id="A5.T7.14.m6.1.1.2.cmml" xref="A5.T7.14.m6.1.1.2">𝐷</ci><cn type="integer" id="A5.T7.14.m6.1.1.3.cmml" xref="A5.T7.14.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.14.m6.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.14.m6.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, updated knowledge from <math id="A5.T7.15.m7.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A5.T7.15.m7.1b"><msub id="A5.T7.15.m7.1.1" xref="A5.T7.15.m7.1.1.cmml"><mi id="A5.T7.15.m7.1.1.2" xref="A5.T7.15.m7.1.1.2.cmml">D</mi><mn id="A5.T7.15.m7.1.1.3" xref="A5.T7.15.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T7.15.m7.1c"><apply id="A5.T7.15.m7.1.1.cmml" xref="A5.T7.15.m7.1.1"><csymbol cd="ambiguous" id="A5.T7.15.m7.1.1.1.cmml" xref="A5.T7.15.m7.1.1">subscript</csymbol><ci id="A5.T7.15.m7.1.1.2.cmml" xref="A5.T7.15.m7.1.1.2">𝐷</ci><cn type="integer" id="A5.T7.15.m7.1.1.3.cmml" xref="A5.T7.15.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.15.m7.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.15.m7.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and newly acquired knowledge from <math id="A5.T7.16.m8.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A5.T7.16.m8.1b"><msub id="A5.T7.16.m8.1.1" xref="A5.T7.16.m8.1.1.cmml"><mi id="A5.T7.16.m8.1.1.2" xref="A5.T7.16.m8.1.1.2.cmml">D</mi><mn id="A5.T7.16.m8.1.1.3" xref="A5.T7.16.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T7.16.m8.1c"><apply id="A5.T7.16.m8.1.1.cmml" xref="A5.T7.16.m8.1.1"><csymbol cd="ambiguous" id="A5.T7.16.m8.1.1.1.cmml" xref="A5.T7.16.m8.1.1">subscript</csymbol><ci id="A5.T7.16.m8.1.1.2.cmml" xref="A5.T7.16.m8.1.1.2">𝐷</ci><cn type="integer" id="A5.T7.16.m8.1.1.3.cmml" xref="A5.T7.16.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.16.m8.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A5.T7.16.m8.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.</figcaption>
<table id="A5.T7.18" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A5.T7.18.2" class="ltx_tr">
<td id="A5.T7.18.2.3" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2">
<span id="A5.T7.18.2.3.1" class="ltx_text"></span><span id="A5.T7.18.2.3.2" class="ltx_text ltx_font_bold"> <span id="A5.T7.18.2.3.2.1" class="ltx_text">
<span id="A5.T7.18.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A5.T7.18.2.3.2.1.1.1" class="ltx_tr">
<span id="A5.T7.18.2.3.2.1.1.1.1" class="ltx_td ltx_align_center">Method</span></span>
</span></span> <span id="A5.T7.18.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="A5.T7.18.2.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="A5.T7.18.2.4.1" class="ltx_text"></span><span id="A5.T7.18.2.4.2" class="ltx_text ltx_font_bold"> <span id="A5.T7.18.2.4.2.1" class="ltx_text">
<span id="A5.T7.18.2.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A5.T7.18.2.4.2.1.1.1" class="ltx_tr">
<span id="A5.T7.18.2.4.2.1.1.1.1" class="ltx_td ltx_align_center">Learning Rate</span></span>
</span></span> <span id="A5.T7.18.2.4.2.2" class="ltx_text"></span></span>
</td>
<td id="A5.T7.18.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T7.18.2.5.1" class="ltx_text ltx_font_bold">IL</span></td>
<td id="A5.T7.18.2.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T7.18.2.6.1" class="ltx_text ltx_font_bold">NLE</span></td>
<td id="A5.T7.18.2.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="A5.T7.18.2.2.3" class="ltx_text"></span><span id="A5.T7.18.2.2.2" class="ltx_text ltx_font_bold"> <span id="A5.T7.18.2.2.2.2" class="ltx_text">
<span id="A5.T7.18.2.2.2.2.2.2" class="ltx_tabular ltx_align_middle">
<span id="A5.T7.18.2.2.2.2.2.2.3" class="ltx_tr">
<span id="A5.T7.18.2.2.2.2.2.2.3.1" class="ltx_td ltx_align_center"><span id="A5.T7.18.2.2.2.2.2.2.3.1.1" class="ltx_text">FUAR</span></span></span>
<span id="A5.T7.18.2.2.2.2.2.2.2" class="ltx_tr">
<span id="A5.T7.18.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center"><math id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3" class="ltx_math_unparsed" alttext="\left((\mathbf{IL}),\bm{n.d.},\mathbf{NLE}\right)" display="inline"><semantics id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3a"><mrow id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3b"><mo mathvariant="normal" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.4">(</mo><mrow id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.5"><mo mathvariant="normal" stretchy="false" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.5.1">(</mo><mi id="A5.T7.17.1.1.1.1.1.1.1.1.m1.1.1">𝐈𝐋</mi><mo mathvariant="normal" stretchy="false" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.5.2">)</mo></mrow><mo mathvariant="normal" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.6">,</mo><mi id="A5.T7.17.1.1.1.1.1.1.1.1.m1.2.2">n</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.7">.</mo><mi id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.3">d</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.8">.</mo><mo mathvariant="normal" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.9">,</mo><mi id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.10">𝐍𝐋𝐄</mi><mo mathvariant="normal" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3.11">)</mo></mrow><annotation encoding="application/x-tex" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3c">\left((\mathbf{IL}),\bm{n.d.},\mathbf{NLE}\right)</annotation><annotation encoding="application/x-llamapun" id="A5.T7.17.1.1.1.1.1.1.1.1.m1.3d">( ( bold_IL ) , bold_italic_n bold_. bold_italic_d bold_. , bold_NLE )</annotation></semantics></math> <math id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1a"><mo mathvariant="normal" stretchy="false" id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1.1" xref="A5.T7.18.2.2.2.2.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1b"><ci id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A5.T7.18.2.2.2.2.2.2.2.2.m2.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A5.T7.18.2.2.2.2.2.2.2.2.m2.1d">↓</annotation></semantics></math></span></span>
</span></span> <span id="A5.T7.18.2.2.2.3" class="ltx_text"></span></span>
</td>
</tr>
<tr id="A5.T7.18.3" class="ltx_tr">
<td id="A5.T7.18.3.1" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="A5.T7.18.3.2" class="ltx_td ltx_align_center ltx_border_t">EM</td>
</tr>
<tr id="A5.T7.18.4" class="ltx_tr">
<td id="A5.T7.18.4.1" class="ltx_td ltx_align_left ltx_border_t">T5-Initial</td>
<td id="A5.T7.18.4.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="A5.T7.18.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T7.18.4.3.1" class="ltx_text ltx_font_bold">24.17</span></td>
<td id="A5.T7.18.4.4" class="ltx_td ltx_align_center ltx_border_t">8.9</td>
<td id="A5.T7.18.4.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A5.T7.18.5" class="ltx_tr">
<td id="A5.T7.18.5.1" class="ltx_td ltx_align_left ltx_border_t">T5-Vanilla</td>
<td id="A5.T7.18.5.2" class="ltx_td ltx_align_center ltx_border_t">1e-05</td>
<td id="A5.T7.18.5.3" class="ltx_td ltx_align_center ltx_border_t">19.15</td>
<td id="A5.T7.18.5.4" class="ltx_td ltx_align_center ltx_border_t">13.56</td>
<td id="A5.T7.18.5.5" class="ltx_td ltx_align_center ltx_border_t">1.08</td>
</tr>
<tr id="A5.T7.18.6" class="ltx_tr">
<td id="A5.T7.18.6.1" class="ltx_td ltx_align_left">T5-Vanilla</td>
<td id="A5.T7.18.6.2" class="ltx_td ltx_align_center">1e-04</td>
<td id="A5.T7.18.6.3" class="ltx_td ltx_align_center">17.45</td>
<td id="A5.T7.18.6.4" class="ltx_td ltx_align_center">15.21</td>
<td id="A5.T7.18.6.5" class="ltx_td ltx_align_center">1.06</td>
</tr>
<tr id="A5.T7.18.7" class="ltx_tr">
<td id="A5.T7.18.7.1" class="ltx_td ltx_align_left">T5-Vanilla</td>
<td id="A5.T7.18.7.2" class="ltx_td ltx_align_center">5e-04</td>
<td id="A5.T7.18.7.3" class="ltx_td ltx_align_center">14.88</td>
<td id="A5.T7.18.7.4" class="ltx_td ltx_align_center">15.89</td>
<td id="A5.T7.18.7.5" class="ltx_td ltx_align_center">1.33</td>
</tr>
<tr id="A5.T7.18.8" class="ltx_tr">
<td id="A5.T7.18.8.1" class="ltx_td ltx_align_left">T5-Vanilla</td>
<td id="A5.T7.18.8.2" class="ltx_td ltx_align_center">1e-03</td>
<td id="A5.T7.18.8.3" class="ltx_td ltx_align_center">11.19</td>
<td id="A5.T7.18.8.4" class="ltx_td ltx_align_center"><span id="A5.T7.18.8.4.1" class="ltx_text ltx_framed_underline">18.77</span></td>
<td id="A5.T7.18.8.5" class="ltx_td ltx_align_center">1.32</td>
</tr>
<tr id="A5.T7.18.9" class="ltx_tr">
<td id="A5.T7.18.9.1" class="ltx_td ltx_align_left ltx_border_t">T5-Kadapters (k=2)</td>
<td id="A5.T7.18.9.2" class="ltx_td ltx_align_center ltx_border_t">1e-04</td>
<td id="A5.T7.18.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T7.18.9.3.1" class="ltx_text ltx_framed_underline">19.93</span></td>
<td id="A5.T7.18.9.4" class="ltx_td ltx_align_center ltx_border_t">14.93</td>
<td id="A5.T7.18.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T7.18.9.5.1" class="ltx_text ltx_font_bold">0.70</span></td>
</tr>
<tr id="A5.T7.18.10" class="ltx_tr">
<td id="A5.T7.18.10.1" class="ltx_td ltx_align_left ltx_border_bb">T5-Kadapters (k=2)</td>
<td id="A5.T7.18.10.2" class="ltx_td ltx_align_center ltx_border_bb">1e-03</td>
<td id="A5.T7.18.10.3" class="ltx_td ltx_align_center ltx_border_bb">16.46</td>
<td id="A5.T7.18.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A5.T7.18.10.4.1" class="ltx_text ltx_font_bold">19.59</span></td>
<td id="A5.T7.18.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="A5.T7.18.10.5.1" class="ltx_text ltx_framed_underline">0.72</span></td>
</tr>
</tbody></table>
</figure>
<div id="A5.p1" class="ltx_para ltx_noindent">
<p id="A5.p1.1" class="ltx_p">Table <a href="#A5.T7" title="Table 7 ‣ Appendix E Exploring the Trade-off of Varying the Learning Rate for Continual Pretraining ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that lowering the learning rate for the continual pretraining leads to less forgetting of the original knowledge, but also less learning of new knowledge. The experiments are done under the setting of <span id="A5.p1.1.1" class="ltx_text ltx_font_smallcaps">Small</span> scenario in Table&nbsp;<a href="#S5.T3" title="Table 3 ‣ 5.2 Exploring Multiple phases of CKL ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="A5.p2" class="ltx_para ltx_noindent">
<p id="A5.p2.1" class="ltx_p">By comparing the <span id="A5.p2.1.1" class="ltx_text">FUAR</span> among the T5-Vanilla models with different learning rates, it can be seen that there is no rule of thumb for choosing the appropriate learning rate since <span id="A5.p2.1.2" class="ltx_text">FUAR</span> is the lowest in learning rate of 1e-4 and increases for both lower and higher learning rates. We suppose that the optimal learning rate heavily depends on the corpus size of <math id="A5.p2.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A5.p2.1.m1.1a"><msub id="A5.p2.1.m1.1.1" xref="A5.p2.1.m1.1.1.cmml"><mi id="A5.p2.1.m1.1.1.2" xref="A5.p2.1.m1.1.1.2.cmml">D</mi><mn id="A5.p2.1.m1.1.1.3" xref="A5.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A5.p2.1.m1.1b"><apply id="A5.p2.1.m1.1.1.cmml" xref="A5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A5.p2.1.m1.1.1.1.cmml" xref="A5.p2.1.m1.1.1">subscript</csymbol><ci id="A5.p2.1.m1.1.1.2.cmml" xref="A5.p2.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A5.p2.1.m1.1.1.3.cmml" xref="A5.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A5.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and the model capacity of LM. We also report the performance of T5-Kadapters, which is a CKL method that shows robust performance throughout most experiments. Applying T5-Kadapters consistently mitigates the trade-off between forgetting and acquiring new knowledge as shown by the improvement in <span id="A5.p2.1.3" class="ltx_text">FUAR</span> from the T5-Vanilla model with the same learning rates, although the level of effectiveness varies according to the value of the learning rate. We do not perform extensive experiments with each of the varying learning rates since searching for the optimal learning rate for each different continued pretraining setting may be out-of-scope with this research.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Exploring How Continually Pretraining on <math id="A6.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A6.1.m1.1b"><msub id="A6.1.m1.1.1" xref="A6.1.m1.1.1.cmml"><mi id="A6.1.m1.1.1.2" xref="A6.1.m1.1.1.2.cmml">D</mi><mn id="A6.1.m1.1.1.3" xref="A6.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A6.1.m1.1c"><apply id="A6.1.m1.1.1.cmml" xref="A6.1.m1.1.1"><csymbol cd="ambiguous" id="A6.1.m1.1.1.1.cmml" xref="A6.1.m1.1.1">subscript</csymbol><ci id="A6.1.m1.1.1.2.cmml" xref="A6.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A6.1.m1.1.1.3.cmml" xref="A6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.1.m1.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A6.1.m1.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> Affects KILT Tasks Which Requires Knowledge from <math id="A6.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.2.m2.1b"><msub id="A6.2.m2.1.1" xref="A6.2.m2.1.1.cmml"><mi id="A6.2.m2.1.1.2" xref="A6.2.m2.1.1.2.cmml">D</mi><mn id="A6.2.m2.1.1.3" xref="A6.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.2.m2.1c"><apply id="A6.2.m2.1.1.cmml" xref="A6.2.m2.1.1"><csymbol cd="ambiguous" id="A6.2.m2.1.1.1.cmml" xref="A6.2.m2.1.1">subscript</csymbol><ci id="A6.2.m2.1.1.2.cmml" xref="A6.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A6.2.m2.1.1.3.cmml" xref="A6.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.2.m2.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.2.m2.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>
</h2>

<figure id="A6.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Dev performance on KILT benchmark datasets after finetuning. Each model is finetuned on the train sets of KILT after continually trained on <span id="A6.T8.2.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> dataset for 4 epochs.</figcaption>
<table id="A6.T8.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A6.T8.3.1" class="ltx_tr">
<td id="A6.T8.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3">
<span id="A6.T8.3.1.1.1" class="ltx_text"></span><span id="A6.T8.3.1.1.2" class="ltx_text ltx_font_bold"> <span id="A6.T8.3.1.1.2.1" class="ltx_text">
<span id="A6.T8.3.1.1.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A6.T8.3.1.1.2.1.1.1" class="ltx_tr">
<span id="A6.T8.3.1.1.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Method</span></span>
</span></span> <span id="A6.T8.3.1.1.2.2" class="ltx_text"></span></span>
</td>
<td id="A6.T8.3.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Fact Checking</td>
<td id="A6.T8.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">Entity Linking</td>
<td id="A6.T8.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">Slot-filling</td>
<td id="A6.T8.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="4">Open Domain QA</td>
<td id="A6.T8.3.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Dialogue</td>
</tr>
<tr id="A6.T8.3.2" class="ltx_tr">
<td id="A6.T8.3.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.1.1" class="ltx_text ltx_font_bold">FEVER</span></td>
<td id="A6.T8.3.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.2.1" class="ltx_text ltx_font_bold">AY2</span></td>
<td id="A6.T8.3.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.3.1" class="ltx_text ltx_font_bold">WnWi</span></td>
<td id="A6.T8.3.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.4.1" class="ltx_text ltx_font_bold">WnCw</span></td>
<td id="A6.T8.3.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.5.1" class="ltx_text ltx_font_bold">T-REx</span></td>
<td id="A6.T8.3.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.6.1" class="ltx_text ltx_font_bold">zsRE</span></td>
<td id="A6.T8.3.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.7.1" class="ltx_text ltx_font_bold">NQ</span></td>
<td id="A6.T8.3.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.8.1" class="ltx_text ltx_font_bold">HoPo</span></td>
<td id="A6.T8.3.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.9.1" class="ltx_text ltx_font_bold">TQA</span></td>
<td id="A6.T8.3.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.10.1" class="ltx_text ltx_font_bold">ELI5</span></td>
<td id="A6.T8.3.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.2.11.1" class="ltx_text ltx_font_bold">WoW</span></td>
</tr>
<tr id="A6.T8.3.3" class="ltx_tr">
<td id="A6.T8.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ACC</td>
<td id="A6.T8.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ACC</td>
<td id="A6.T8.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ACC</td>
<td id="A6.T8.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ACC</td>
<td id="A6.T8.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ACC</td>
<td id="A6.T8.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ACC</td>
<td id="A6.T8.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="A6.T8.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="A6.T8.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="A6.T8.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Rouge</td>
<td id="A6.T8.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">F1</td>
</tr>
<tr id="A6.T8.3.4" class="ltx_tr">
<td id="A6.T8.3.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Initial</td>
<td id="A6.T8.3.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.2.1" class="ltx_text ltx_framed_underline">80.39</span></td>
<td id="A6.T8.3.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.3.1" class="ltx_text ltx_framed_underline">81.44</span></td>
<td id="A6.T8.3.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.4.1" class="ltx_text ltx_font_bold">50.47</span></td>
<td id="A6.T8.3.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.5.1" class="ltx_text ltx_font_bold">48.92</span></td>
<td id="A6.T8.3.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">44.64</td>
<td id="A6.T8.3.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.7.1" class="ltx_text ltx_font_bold">4.40</span></td>
<td id="A6.T8.3.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.8.1" class="ltx_text ltx_framed_underline">25.63</span></td>
<td id="A6.T8.3.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.9.1" class="ltx_text ltx_framed_underline">17.64</span></td>
<td id="A6.T8.3.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.10.1" class="ltx_text ltx_font_bold">28.38</span></td>
<td id="A6.T8.3.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">13.46</td>
<td id="A6.T8.3.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.4.12.1" class="ltx_text ltx_framed_underline">13.92</span></td>
</tr>
<tr id="A6.T8.3.5" class="ltx_tr">
<td id="A6.T8.3.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Vanilla</td>
<td id="A6.T8.3.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">78.02</td>
<td id="A6.T8.3.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">81.19</td>
<td id="A6.T8.3.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">48.17</td>
<td id="A6.T8.3.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">46.46</td>
<td id="A6.T8.3.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">44.08</td>
<td id="A6.T8.3.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2.04</td>
<td id="A6.T8.3.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">24.93</td>
<td id="A6.T8.3.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">14.36</td>
<td id="A6.T8.3.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">26.51</td>
<td id="A6.T8.3.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">13.38</td>
<td id="A6.T8.3.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">13.07</td>
</tr>
<tr id="A6.T8.3.6" class="ltx_tr">
<td id="A6.T8.3.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-RecAdam</td>
<td id="A6.T8.3.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">77.83</td>
<td id="A6.T8.3.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.6.3.1" class="ltx_text ltx_framed_underline">81.44</span></td>
<td id="A6.T8.3.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">49.12</td>
<td id="A6.T8.3.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.01</td>
<td id="A6.T8.3.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.04</td>
<td id="A6.T8.3.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.58</td>
<td id="A6.T8.3.6.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.65</td>
<td id="A6.T8.3.6.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.86</td>
<td id="A6.T8.3.6.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25.99</td>
<td id="A6.T8.3.6.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.71</td>
<td id="A6.T8.3.6.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.69</td>
</tr>
<tr id="A6.T8.3.7" class="ltx_tr">
<td id="A6.T8.3.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-MixReview</td>
<td id="A6.T8.3.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">77.17</td>
<td id="A6.T8.3.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">80.77</td>
<td id="A6.T8.3.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.7.4.1" class="ltx_text ltx_framed_underline">49.38</span></td>
<td id="A6.T8.3.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.22</td>
<td id="A6.T8.3.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">44.08</td>
<td id="A6.T8.3.7.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.47</td>
<td id="A6.T8.3.7.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25.07</td>
<td id="A6.T8.3.7.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.57</td>
<td id="A6.T8.3.7.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">26.36</td>
<td id="A6.T8.3.7.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.57</td>
<td id="A6.T8.3.7.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.73</td>
</tr>
<tr id="A6.T8.3.8" class="ltx_tr">
<td id="A6.T8.3.8.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-LoRA</td>
<td id="A6.T8.3.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">79.89</td>
<td id="A6.T8.3.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.8.3.1" class="ltx_text ltx_framed_underline">81.44</span></td>
<td id="A6.T8.3.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.82</td>
<td id="A6.T8.3.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.8.5.1" class="ltx_text ltx_framed_underline">47.29</span></td>
<td id="A6.T8.3.8.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.8.6.1" class="ltx_text ltx_framed_underline">45.68</span></td>
<td id="A6.T8.3.8.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.01</td>
<td id="A6.T8.3.8.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25.49</td>
<td id="A6.T8.3.8.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.71</td>
<td id="A6.T8.3.8.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">28.23</td>
<td id="A6.T8.3.8.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.42</td>
<td id="A6.T8.3.8.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.60</td>
</tr>
<tr id="A6.T8.3.9" class="ltx_tr">
<td id="A6.T8.3.9.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Kadapters (k=2)</td>
<td id="A6.T8.3.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">80.35</td>
<td id="A6.T8.3.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">80.94</td>
<td id="A6.T8.3.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.91</td>
<td id="A6.T8.3.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.65</td>
<td id="A6.T8.3.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.52</td>
<td id="A6.T8.3.9.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.33</td>
<td id="A6.T8.3.9.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.9.8.1" class="ltx_text ltx_font_bold">26.20</span></td>
<td id="A6.T8.3.9.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.57</td>
<td id="A6.T8.3.9.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">26.89</td>
<td id="A6.T8.3.9.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.15</td>
<td id="A6.T8.3.9.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.94</td>
</tr>
<tr id="A6.T8.3.10" class="ltx_tr">
<td id="A6.T8.3.10.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Kadapters (k=3)</td>
<td id="A6.T8.3.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">80.31</td>
<td id="A6.T8.3.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">80.52</td>
<td id="A6.T8.3.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.09</td>
<td id="A6.T8.3.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.26</td>
<td id="A6.T8.3.10.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.60</td>
<td id="A6.T8.3.10.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.12</td>
<td id="A6.T8.3.10.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.79</td>
<td id="A6.T8.3.10.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.57</td>
<td id="A6.T8.3.10.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25.62</td>
<td id="A6.T8.3.10.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.10.11.1" class="ltx_text ltx_font_bold">13.82</span></td>
<td id="A6.T8.3.10.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.42</td>
</tr>
<tr id="A6.T8.3.11" class="ltx_tr">
<td id="A6.T8.3.11.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Modular</td>
<td id="A6.T8.3.11.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.2.1" class="ltx_text ltx_font_bold">80.54</span></td>
<td id="A6.T8.3.11.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.3.1" class="ltx_text ltx_font_bold">82.44</span></td>
<td id="A6.T8.3.11.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">48.44</td>
<td id="A6.T8.3.11.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">44.81</td>
<td id="A6.T8.3.11.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.6.1" class="ltx_text ltx_font_bold">48.16</span></td>
<td id="A6.T8.3.11.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.7.1" class="ltx_text ltx_framed_underline">3.44</span></td>
<td id="A6.T8.3.11.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">24.51</td>
<td id="A6.T8.3.11.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.9.1" class="ltx_text ltx_font_bold">18.43</span></td>
<td id="A6.T8.3.11.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.10.1" class="ltx_text ltx_framed_underline">28.31</span></td>
<td id="A6.T8.3.11.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.11.1" class="ltx_text ltx_framed_underline">13.72</span></td>
<td id="A6.T8.3.11.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T8.3.11.12.1" class="ltx_text ltx_font_bold">14.03</span></td>
</tr>
</tbody></table>
</figure>
<figure id="A6.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Hyperparameters and dataset details for all tasks of KILT.</figcaption>
<table id="A6.T9.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A6.T9.1.1" class="ltx_tr">
<td id="A6.T9.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="A6.T9.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Fact Checking</td>
<td id="A6.T9.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">Entity Linking</td>
<td id="A6.T9.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">Slot-filling</td>
<td id="A6.T9.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="4">Open Domain QA</td>
<td id="A6.T9.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Dialogue</td>
</tr>
<tr id="A6.T9.1.2" class="ltx_tr">
<td id="A6.T9.1.2.1" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="A6.T9.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A6.T9.1.2.2.1" class="ltx_text ltx_font_bold">FEV</span></td>
<td id="A6.T9.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.3.1" class="ltx_text ltx_font_bold">AY2</span></td>
<td id="A6.T9.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.4.1" class="ltx_text ltx_font_bold">WnWi</span></td>
<td id="A6.T9.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A6.T9.1.2.5.1" class="ltx_text ltx_font_bold">WnCw</span></td>
<td id="A6.T9.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.6.1" class="ltx_text ltx_font_bold">T-REx</span></td>
<td id="A6.T9.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A6.T9.1.2.7.1" class="ltx_text ltx_font_bold">zsRE</span></td>
<td id="A6.T9.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.8.1" class="ltx_text ltx_font_bold">NQ</span></td>
<td id="A6.T9.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.9.1" class="ltx_text ltx_font_bold">HoPo</span></td>
<td id="A6.T9.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.10.1" class="ltx_text ltx_font_bold">TQA</span></td>
<td id="A6.T9.1.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A6.T9.1.2.11.1" class="ltx_text ltx_font_bold">ELI5</span></td>
<td id="A6.T9.1.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A6.T9.1.2.12.1" class="ltx_text ltx_font_bold">WoW</span></td>
</tr>
<tr id="A6.T9.1.3" class="ltx_tr">
<td id="A6.T9.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Epoch</td>
<td id="A6.T9.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5</td>
<td id="A6.T9.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">20</td>
<td id="A6.T9.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="A6.T9.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="A6.T9.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">9</td>
<td id="A6.T9.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">30</td>
<td id="A6.T9.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">45</td>
<td id="A6.T9.1.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">12</td>
<td id="A6.T9.1.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">50</td>
<td id="A6.T9.1.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">8</td>
</tr>
<tr id="A6.T9.1.4" class="ltx_tr">
<td id="A6.T9.1.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Input Seq</td>
<td id="A6.T9.1.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25</td>
<td id="A6.T9.1.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">768</td>
<td id="A6.T9.1.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">512</td>
<td id="A6.T9.1.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2,048</td>
<td id="A6.T9.1.4.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25</td>
<td id="A6.T9.1.4.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25</td>
<td id="A6.T9.1.4.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">35</td>
<td id="A6.T9.1.4.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50</td>
<td id="A6.T9.1.4.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25</td>
<td id="A6.T9.1.4.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">35</td>
<td id="A6.T9.1.4.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">175</td>
</tr>
<tr id="A6.T9.1.5" class="ltx_tr">
<td id="A6.T9.1.5.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Output Seq</td>
<td id="A6.T9.1.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">10</td>
<td id="A6.T9.1.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.5.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.5.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
<td id="A6.T9.1.5.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8</td>
<td id="A6.T9.1.5.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">10</td>
<td id="A6.T9.1.5.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">350</td>
<td id="A6.T9.1.5.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">40</td>
</tr>
<tr id="A6.T9.1.6" class="ltx_tr">
<td id="A6.T9.1.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LR</td>
<td id="A6.T9.1.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-4</td>
<td id="A6.T9.1.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-4</td>
<td id="A6.T9.1.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="A6.T9.1.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="A6.T9.1.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-3</td>
<td id="A6.T9.1.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-4</td>
<td id="A6.T9.1.6.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-3</td>
<td id="A6.T9.1.6.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-4</td>
<td id="A6.T9.1.6.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-3</td>
<td id="A6.T9.1.6.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-3</td>
<td id="A6.T9.1.6.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1e-4</td>
</tr>
<tr id="A6.T9.1.7" class="ltx_tr">
<td id="A6.T9.1.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Batch Size</td>
<td id="A6.T9.1.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">128</td>
<td id="A6.T9.1.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16</td>
<td id="A6.T9.1.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">128</td>
<td id="A6.T9.1.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48</td>
<td id="A6.T9.1.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">512</td>
<td id="A6.T9.1.7.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">256</td>
<td id="A6.T9.1.7.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">256</td>
<td id="A6.T9.1.7.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">256</td>
<td id="A6.T9.1.7.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">128</td>
<td id="A6.T9.1.7.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">32</td>
<td id="A6.T9.1.7.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64</td>
</tr>
<tr id="A6.T9.1.8" class="ltx_tr">
<td id="A6.T9.1.8.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Train Size</td>
<td id="A6.T9.1.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">104,966</td>
<td id="A6.T9.1.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18,395</td>
<td id="A6.T9.1.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="A6.T9.1.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="A6.T9.1.8.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2,284,168</td>
<td id="A6.T9.1.8.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">147,909</td>
<td id="A6.T9.1.8.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">87,372</td>
<td id="A6.T9.1.8.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">88,869</td>
<td id="A6.T9.1.8.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61,844</td>
<td id="A6.T9.1.8.11" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">272,634</td>
<td id="A6.T9.1.8.12" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63,734</td>
</tr>
<tr id="A6.T9.1.9" class="ltx_tr">
<td id="A6.T9.1.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">Dev Size</td>
<td id="A6.T9.1.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">10,444</td>
<td id="A6.T9.1.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">4,784</td>
<td id="A6.T9.1.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">3,396</td>
<td id="A6.T9.1.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">5,599</td>
<td id="A6.T9.1.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">5,000</td>
<td id="A6.T9.1.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">3,724</td>
<td id="A6.T9.1.9.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">2,837</td>
<td id="A6.T9.1.9.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">5,600</td>
<td id="A6.T9.1.9.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">5,359</td>
<td id="A6.T9.1.9.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">1,507</td>
<td id="A6.T9.1.9.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">3,054</td>
</tr>
</tbody></table>
</figure>
<div id="A6.p1" class="ltx_para ltx_noindent">
<p id="A6.p1.4" class="ltx_p">In addition to the CKL benchmark, we also show in Table&nbsp;<a href="#A6.T8" title="Table 8 ‣ Appendix F Exploring How Continually Pretraining on 𝐷₁ Affects KILT Tasks Which Requires Knowledge from 𝐷₀ ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> the performance on the dev set of KILT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> after finetuning each of the continually pretrained models of Table&nbsp;<a href="#S5.T2" title="Table 2 ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Since KILT is made from Wikipedia, which corresponds to the old pretraining corpus <math id="A6.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.p1.1.m1.1a"><msub id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml"><mi id="A6.p1.1.m1.1.1.2" xref="A6.p1.1.m1.1.1.2.cmml">D</mi><mn id="A6.p1.1.m1.1.1.3" xref="A6.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><apply id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A6.p1.1.m1.1.1.1.cmml" xref="A6.p1.1.m1.1.1">subscript</csymbol><ci id="A6.p1.1.m1.1.1.2.cmml" xref="A6.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A6.p1.1.m1.1.1.3.cmml" xref="A6.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, the performance on KILT measures how continual pretraining on new corpus <math id="A6.p1.2.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A6.p1.2.m2.1a"><msub id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml"><mi id="A6.p1.2.m2.1.1.2" xref="A6.p1.2.m2.1.1.2.cmml">D</mi><mn id="A6.p1.2.m2.1.1.3" xref="A6.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><apply id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A6.p1.2.m2.1.1.1.cmml" xref="A6.p1.2.m2.1.1">subscript</csymbol><ci id="A6.p1.2.m2.1.1.2.cmml" xref="A6.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A6.p1.2.m2.1.1.3.cmml" xref="A6.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A6.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> affects the performance on the knowledge obtained from <math id="A6.p1.3.m3.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.p1.3.m3.1a"><msub id="A6.p1.3.m3.1.1" xref="A6.p1.3.m3.1.1.cmml"><mi id="A6.p1.3.m3.1.1.2" xref="A6.p1.3.m3.1.1.2.cmml">D</mi><mn id="A6.p1.3.m3.1.1.3" xref="A6.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.p1.3.m3.1b"><apply id="A6.p1.3.m3.1.1.cmml" xref="A6.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A6.p1.3.m3.1.1.1.cmml" xref="A6.p1.3.m3.1.1">subscript</csymbol><ci id="A6.p1.3.m3.1.1.2.cmml" xref="A6.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="A6.p1.3.m3.1.1.3.cmml" xref="A6.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.3.m3.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> if finetuning is done on behalf of the knowledge from <math id="A6.p1.4.m4.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.p1.4.m4.1a"><msub id="A6.p1.4.m4.1.1" xref="A6.p1.4.m4.1.1.cmml"><mi id="A6.p1.4.m4.1.1.2" xref="A6.p1.4.m4.1.1.2.cmml">D</mi><mn id="A6.p1.4.m4.1.1.3" xref="A6.p1.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.p1.4.m4.1b"><apply id="A6.p1.4.m4.1.1.cmml" xref="A6.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A6.p1.4.m4.1.1.1.cmml" xref="A6.p1.4.m4.1.1">subscript</csymbol><ci id="A6.p1.4.m4.1.1.2.cmml" xref="A6.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="A6.p1.4.m4.1.1.3.cmml" xref="A6.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.4.m4.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<section id="A6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Configuration</h5>

<div id="A6.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A6.SS0.SSS0.Px1.p1.1" class="ltx_p">KILT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> consists of 5 different tasks and 11 datasets: Open-Domain Question Answering&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Joshi et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>; Kwiatkowski et&nbsp;al., <a href="#bib.bib24" title="" class="ltx_ref">2019</a>; Fan et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2019</a>; Yang et&nbsp;al., <a href="#bib.bib55" title="" class="ltx_ref">2018</a>)</cite>, Fact Checking&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Thorne et&nbsp;al., <a href="#bib.bib47" title="" class="ltx_ref">2018</a>)</cite>, Entity Linking&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoffart et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2011</a>; Guo &amp; Barbosa, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>, Slot-filling&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Levy et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, and Knowledgeable Open Dialogue&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dinan et&nbsp;al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. Because each task requires a different training objective than the one used during pretraining, additional finetuning is necessary. We search for the hyperparameters such as training epochs, batch size, input size, output size, and learning rate of each individual KILT task to match the T5-base dev performance reported by <cite class="ltx_cite ltx_citemacro_citet">Petroni et&nbsp;al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>. Using the identified configurations, we perform experiments on all of the KILT tasks with the continually pretrained models for each method as the initialization checkpoints. Evaluation metrics are different for each dataset: accuracy for discrete output (fact-checking, entity linking, slot-filling), Exact Match (EM) for question answering tasks with short output, ROUGE-L for ELI5 (question answering task with long output), and F1-score for Wizard of Wikipedia (dialogue). The data statistics and the hyperparameters used for finetuning on each KILT dataset is reported in Table <a href="#A6.T9" title="Table 9 ‣ Appendix F Exploring How Continually Pretraining on 𝐷₁ Affects KILT Tasks Which Requires Knowledge from 𝐷₀ ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</section>
<section id="A6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Experimental Result</h5>

<div id="A6.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A6.SS0.SSS0.Px2.p1.1" class="ltx_p">We first focus on the performance on zero-shot Relation Extraction (zsRE), which is measured on the dev set of 12 relations that are ensured to have no overlap with the 84 relations of the train set&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Levy et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>. Since the setting is similar to the zero-shot probing setting of IL, the trend of the result on the two datasets are similar. The performance of T5-Vanilla drops to half from that of T5-Initial as shown in IL, and the best performing method for both datasets is T5-Modular. In addition, corresponding with results from the CKL benchmark, parameter-expansion methods generally show stronger performance than the other methods.</p>
</div>
<div id="A6.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A6.SS0.SSS0.Px2.p2.3" class="ltx_p">However, for the other datasets that cannot be performed in a zero-shot manner, the intermediate process of continually pretraining on corpus <math id="A6.SS0.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A6.SS0.SSS0.Px2.p2.1.m1.1a"><msub id="A6.SS0.SSS0.Px2.p2.1.m1.1.1" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="A6.SS0.SSS0.Px2.p2.1.m1.1.1.2" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml">D</mi><mn id="A6.SS0.SSS0.Px2.p2.1.m1.1.1.3" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A6.SS0.SSS0.Px2.p2.1.m1.1b"><apply id="A6.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A6.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="A6.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A6.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="A6.SS0.SSS0.Px2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.SS0.SSS0.Px2.p2.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A6.SS0.SSS0.Px2.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> does not seem to be that harmful on the finetuning for the target tasks even though they are more related to the knowledge of <math id="A6.SS0.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.SS0.SSS0.Px2.p2.2.m2.1a"><msub id="A6.SS0.SSS0.Px2.p2.2.m2.1.1" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="A6.SS0.SSS0.Px2.p2.2.m2.1.1.2" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml">D</mi><mn id="A6.SS0.SSS0.Px2.p2.2.m2.1.1.3" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.SS0.SSS0.Px2.p2.2.m2.1b"><apply id="A6.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A6.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="A6.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A6.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="A6.SS0.SSS0.Px2.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.SS0.SSS0.Px2.p2.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.SS0.SSS0.Px2.p2.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. Even T5-Vanilla shows modest performance, sometimes with better results than some other CKL baselines. One hypothesis is that the models could have regained the original knowledge from corpus <math id="A6.SS0.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.SS0.SSS0.Px2.p2.3.m3.1a"><msub id="A6.SS0.SSS0.Px2.p2.3.m3.1.1" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1.cmml"><mi id="A6.SS0.SSS0.Px2.p2.3.m3.1.1.2" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1.2.cmml">D</mi><mn id="A6.SS0.SSS0.Px2.p2.3.m3.1.1.3" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.SS0.SSS0.Px2.p2.3.m3.1b"><apply id="A6.SS0.SSS0.Px2.p2.3.m3.1.1.cmml" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A6.SS0.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1">subscript</csymbol><ci id="A6.SS0.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1.2">𝐷</ci><cn type="integer" id="A6.SS0.SSS0.Px2.p2.3.m3.1.1.3.cmml" xref="A6.SS0.SSS0.Px2.p2.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.SS0.SSS0.Px2.p2.3.m3.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.SS0.SSS0.Px2.p2.3.m3.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> through the finetuning process. Also, some of the knowledge could have been recovered through the test-train overlap&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Wang et&nbsp;al., <a href="#bib.bib51" title="" class="ltx_ref">2021a</a>)</cite>.</p>
</div>
<div id="A6.SS0.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="A6.SS0.SSS0.Px2.p3.1" class="ltx_p">A more surprising finding is that the performance of some of the parameter-expansion methods are even higher than that of T5-Initial, which is considered to be the upper bound for KILT because T5-Initial is only trained on behalf of the knowledge from <math id="A6.SS0.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A6.SS0.SSS0.Px2.p3.1.m1.1a"><msub id="A6.SS0.SSS0.Px2.p3.1.m1.1.1" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1.cmml"><mi id="A6.SS0.SSS0.Px2.p3.1.m1.1.1.2" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml">D</mi><mn id="A6.SS0.SSS0.Px2.p3.1.m1.1.1.3" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A6.SS0.SSS0.Px2.p3.1.m1.1b"><apply id="A6.SS0.SSS0.Px2.p3.1.m1.1.1.cmml" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A6.SS0.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="A6.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A6.SS0.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="A6.SS0.SSS0.Px2.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.SS0.SSS0.Px2.p3.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A6.SS0.SSS0.Px2.p3.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. For example, T5-Modular shows higher scores than T5-Initial on 6 out of 11 tasks. Since the parameter-expansion methods force the model to store the new knowledge in the newly added parameters during continual pretraining, one careful conjecture is these LMs have learned to combine and utilize in its internal representation of both old and new knowledge stored in separate parameters during finetuning to maximize the performance.</p>
</div>
</section>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Exploring How CKL Methods Transfer Across LM Architectures</h2>

<div id="A7.p1" class="ltx_para ltx_noindent">
<p id="A7.p1.3" class="ltx_p">We perform experiments with GPT-2 Large (<span id="A7.p1.1.1" class="ltx_text" style="position:relative; bottom:0.7pt;"><math id="A7.p1.1.1.1.m1.1" class="ltx_Math" alttext="\scriptstyle\sim" display="inline"><semantics id="A7.p1.1.1.1.m1.1a"><mo mathsize="70%" id="A7.p1.1.1.1.m1.1.1" xref="A7.p1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A7.p1.1.1.1.m1.1b"><csymbol cd="latexml" id="A7.p1.1.1.1.m1.1.1.cmml" xref="A7.p1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.1.1.1.m1.1c">\scriptstyle\sim</annotation><annotation encoding="application/x-llamapun" id="A7.p1.1.1.1.m1.1d">∼</annotation></semantics></math></span> 774M params) &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite> initially pretrained on WebText and Wikipedia<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>GPT-2 was initially pretrained on WebText (Dec 2019), which consists of 8 million documents with Wikipedia pages excluded. In order to measure the performance on <span id="footnote14.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> constructed from Wikipedia, we continually pretrain GPT-2 on a subset of Wikipedia (May 2020) for 14k global training steps before CKL.</span></span></span> (<math id="A7.p1.2.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A7.p1.2.m1.1a"><msub id="A7.p1.2.m1.1.1" xref="A7.p1.2.m1.1.1.cmml"><mi id="A7.p1.2.m1.1.1.2" xref="A7.p1.2.m1.1.1.2.cmml">D</mi><mn id="A7.p1.2.m1.1.1.3" xref="A7.p1.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p1.2.m1.1b"><apply id="A7.p1.2.m1.1.1.cmml" xref="A7.p1.2.m1.1.1"><csymbol cd="ambiguous" id="A7.p1.2.m1.1.1.1.cmml" xref="A7.p1.2.m1.1.1">subscript</csymbol><ci id="A7.p1.2.m1.1.1.2.cmml" xref="A7.p1.2.m1.1.1.2">𝐷</ci><cn type="integer" id="A7.p1.2.m1.1.1.3.cmml" xref="A7.p1.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.2.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A7.p1.2.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>) and continually trained on <span id="A7.p1.3.2" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span>, i.e., <span id="A7.p1.3.3" class="ltx_text ltx_font_smallcaps">Small</span> (<math id="A7.p1.3.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A7.p1.3.m2.1a"><msub id="A7.p1.3.m2.1.1" xref="A7.p1.3.m2.1.1.cmml"><mi id="A7.p1.3.m2.1.1.2" xref="A7.p1.3.m2.1.1.2.cmml">D</mi><mn id="A7.p1.3.m2.1.1.3" xref="A7.p1.3.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p1.3.m2.1b"><apply id="A7.p1.3.m2.1.1.cmml" xref="A7.p1.3.m2.1.1"><csymbol cd="ambiguous" id="A7.p1.3.m2.1.1.1.cmml" xref="A7.p1.3.m2.1.1">subscript</csymbol><ci id="A7.p1.3.m2.1.1.2.cmml" xref="A7.p1.3.m2.1.1.2">𝐷</ci><cn type="integer" id="A7.p1.3.m2.1.1.3.cmml" xref="A7.p1.3.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.3.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.p1.3.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>) for 8 epochs. For continued pretraining, we use the common teacher-forcing pretraining objective. The initial learning rate for the continued pretraining stage is empirically chosen as 1e-4 (results with learning rate as 1e-3 are shown in Appendix <a href="#A7.SS1" title="G.1 Failed GPT-2 experiments with Larger Learning Rate ‣ Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G.1</span></a>). After continued pretraining,
we apply <span id="A7.p1.3.4" class="ltx_text ltx_font_italic">light-tuning</span>, a process denoted for finetuning the model for only one epoch on a small portion of data similar to the evaluation set. Training on a single epoch constrains the model to barely adapt to the input-output form of the data and not to learn the knowledge in tuning samples, mitigating the problem suggested by <cite class="ltx_cite ltx_citemacro_citet">Lewis et&nbsp;al. (<a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
<figure id="A7.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Performance of decoder-only models initially pretrained on Dec 2019 dump of Webtext and May 2020 dump of Wikipedia (<math id="A7.T10.9.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A7.T10.9.m1.1b"><msub id="A7.T10.9.m1.1.1" xref="A7.T10.9.m1.1.1.cmml"><mi id="A7.T10.9.m1.1.1.2" xref="A7.T10.9.m1.1.1.2.cmml">D</mi><mn id="A7.T10.9.m1.1.1.3" xref="A7.T10.9.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T10.9.m1.1c"><apply id="A7.T10.9.m1.1.1.cmml" xref="A7.T10.9.m1.1.1"><csymbol cd="ambiguous" id="A7.T10.9.m1.1.1.1.cmml" xref="A7.T10.9.m1.1.1">subscript</csymbol><ci id="A7.T10.9.m1.1.1.2.cmml" xref="A7.T10.9.m1.1.1.2">𝐷</ci><cn type="integer" id="A7.T10.9.m1.1.1.3.cmml" xref="A7.T10.9.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.9.m1.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.9.m1.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>) continually pretrained on <span id="A7.T10.23.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span> (<math id="A7.T10.10.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A7.T10.10.m2.1b"><msub id="A7.T10.10.m2.1.1" xref="A7.T10.10.m2.1.1.cmml"><mi id="A7.T10.10.m2.1.1.2" xref="A7.T10.10.m2.1.1.2.cmml">D</mi><mn id="A7.T10.10.m2.1.1.3" xref="A7.T10.10.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T10.10.m2.1c"><apply id="A7.T10.10.m2.1.1.cmml" xref="A7.T10.10.m2.1.1"><csymbol cd="ambiguous" id="A7.T10.10.m2.1.1.1.cmml" xref="A7.T10.10.m2.1.1">subscript</csymbol><ci id="A7.T10.10.m2.1.1.2.cmml" xref="A7.T10.10.m2.1.1.2">𝐷</ci><cn type="integer" id="A7.T10.10.m2.1.1.3.cmml" xref="A7.T10.10.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.10.m2.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.10.m2.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>) for 8 epochs with a learning rate of 1e-4. Each of IL and NQE stands for <span id="A7.T10.24.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> and <span id="A7.T10.25.3" class="ltx_text ltx_font_smallcaps">NewQuestions-Easy</span>. The parameters of <span id="A7.T10.26.4" class="ltx_text">FUAR</span> are <math id="A7.T10.11.m3.1" class="ltx_Math" alttext="\mathbb{T}^{F}" display="inline"><semantics id="A7.T10.11.m3.1b"><msup id="A7.T10.11.m3.1.1" xref="A7.T10.11.m3.1.1.cmml"><mi id="A7.T10.11.m3.1.1.2" xref="A7.T10.11.m3.1.1.2.cmml">𝕋</mi><mi id="A7.T10.11.m3.1.1.3" xref="A7.T10.11.m3.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="A7.T10.11.m3.1c"><apply id="A7.T10.11.m3.1.1.cmml" xref="A7.T10.11.m3.1.1"><csymbol cd="ambiguous" id="A7.T10.11.m3.1.1.1.cmml" xref="A7.T10.11.m3.1.1">superscript</csymbol><ci id="A7.T10.11.m3.1.1.2.cmml" xref="A7.T10.11.m3.1.1.2">𝕋</ci><ci id="A7.T10.11.m3.1.1.3.cmml" xref="A7.T10.11.m3.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.11.m3.1d">\mathbb{T}^{F}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.11.m3.1e">blackboard_T start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>, <math id="A7.T10.12.m4.1" class="ltx_Math" alttext="T_{1}^{U}" display="inline"><semantics id="A7.T10.12.m4.1b"><msubsup id="A7.T10.12.m4.1.1" xref="A7.T10.12.m4.1.1.cmml"><mi id="A7.T10.12.m4.1.1.2.2" xref="A7.T10.12.m4.1.1.2.2.cmml">T</mi><mn id="A7.T10.12.m4.1.1.2.3" xref="A7.T10.12.m4.1.1.2.3.cmml">1</mn><mi id="A7.T10.12.m4.1.1.3" xref="A7.T10.12.m4.1.1.3.cmml">U</mi></msubsup><annotation-xml encoding="MathML-Content" id="A7.T10.12.m4.1c"><apply id="A7.T10.12.m4.1.1.cmml" xref="A7.T10.12.m4.1.1"><csymbol cd="ambiguous" id="A7.T10.12.m4.1.1.1.cmml" xref="A7.T10.12.m4.1.1">superscript</csymbol><apply id="A7.T10.12.m4.1.1.2.cmml" xref="A7.T10.12.m4.1.1"><csymbol cd="ambiguous" id="A7.T10.12.m4.1.1.2.1.cmml" xref="A7.T10.12.m4.1.1">subscript</csymbol><ci id="A7.T10.12.m4.1.1.2.2.cmml" xref="A7.T10.12.m4.1.1.2.2">𝑇</ci><cn type="integer" id="A7.T10.12.m4.1.1.2.3.cmml" xref="A7.T10.12.m4.1.1.2.3">1</cn></apply><ci id="A7.T10.12.m4.1.1.3.cmml" xref="A7.T10.12.m4.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.12.m4.1d">T_{1}^{U}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.12.m4.1e">italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math id="A7.T10.13.m5.1" class="ltx_Math" alttext="T_{1}^{A}" display="inline"><semantics id="A7.T10.13.m5.1b"><msubsup id="A7.T10.13.m5.1.1" xref="A7.T10.13.m5.1.1.cmml"><mi id="A7.T10.13.m5.1.1.2.2" xref="A7.T10.13.m5.1.1.2.2.cmml">T</mi><mn id="A7.T10.13.m5.1.1.2.3" xref="A7.T10.13.m5.1.1.2.3.cmml">1</mn><mi id="A7.T10.13.m5.1.1.3" xref="A7.T10.13.m5.1.1.3.cmml">A</mi></msubsup><annotation-xml encoding="MathML-Content" id="A7.T10.13.m5.1c"><apply id="A7.T10.13.m5.1.1.cmml" xref="A7.T10.13.m5.1.1"><csymbol cd="ambiguous" id="A7.T10.13.m5.1.1.1.cmml" xref="A7.T10.13.m5.1.1">superscript</csymbol><apply id="A7.T10.13.m5.1.1.2.cmml" xref="A7.T10.13.m5.1.1"><csymbol cd="ambiguous" id="A7.T10.13.m5.1.1.2.1.cmml" xref="A7.T10.13.m5.1.1">subscript</csymbol><ci id="A7.T10.13.m5.1.1.2.2.cmml" xref="A7.T10.13.m5.1.1.2.2">𝑇</ci><cn type="integer" id="A7.T10.13.m5.1.1.2.3.cmml" xref="A7.T10.13.m5.1.1.2.3">1</cn></apply><ci id="A7.T10.13.m5.1.1.3.cmml" xref="A7.T10.13.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.13.m5.1d">T_{1}^{A}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.13.m5.1e">italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT</annotation></semantics></math>, the tasks measuring the amount of time-invariant knowledge from corpus <math id="A7.T10.14.m6.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A7.T10.14.m6.1b"><msub id="A7.T10.14.m6.1.1" xref="A7.T10.14.m6.1.1.cmml"><mi id="A7.T10.14.m6.1.1.2" xref="A7.T10.14.m6.1.1.2.cmml">D</mi><mn id="A7.T10.14.m6.1.1.3" xref="A7.T10.14.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T10.14.m6.1c"><apply id="A7.T10.14.m6.1.1.cmml" xref="A7.T10.14.m6.1.1"><csymbol cd="ambiguous" id="A7.T10.14.m6.1.1.1.cmml" xref="A7.T10.14.m6.1.1">subscript</csymbol><ci id="A7.T10.14.m6.1.1.2.cmml" xref="A7.T10.14.m6.1.1.2">𝐷</ci><cn type="integer" id="A7.T10.14.m6.1.1.3.cmml" xref="A7.T10.14.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.14.m6.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.14.m6.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, updated knowledge from <math id="A7.T10.15.m7.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A7.T10.15.m7.1b"><msub id="A7.T10.15.m7.1.1" xref="A7.T10.15.m7.1.1.cmml"><mi id="A7.T10.15.m7.1.1.2" xref="A7.T10.15.m7.1.1.2.cmml">D</mi><mn id="A7.T10.15.m7.1.1.3" xref="A7.T10.15.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T10.15.m7.1c"><apply id="A7.T10.15.m7.1.1.cmml" xref="A7.T10.15.m7.1.1"><csymbol cd="ambiguous" id="A7.T10.15.m7.1.1.1.cmml" xref="A7.T10.15.m7.1.1">subscript</csymbol><ci id="A7.T10.15.m7.1.1.2.cmml" xref="A7.T10.15.m7.1.1.2">𝐷</ci><cn type="integer" id="A7.T10.15.m7.1.1.3.cmml" xref="A7.T10.15.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.15.m7.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.15.m7.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and newly acquired knowledge from <math id="A7.T10.16.m8.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A7.T10.16.m8.1b"><msub id="A7.T10.16.m8.1.1" xref="A7.T10.16.m8.1.1.cmml"><mi id="A7.T10.16.m8.1.1.2" xref="A7.T10.16.m8.1.1.2.cmml">D</mi><mn id="A7.T10.16.m8.1.1.3" xref="A7.T10.16.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T10.16.m8.1c"><apply id="A7.T10.16.m8.1.1.cmml" xref="A7.T10.16.m8.1.1"><csymbol cd="ambiguous" id="A7.T10.16.m8.1.1.1.cmml" xref="A7.T10.16.m8.1.1">subscript</csymbol><ci id="A7.T10.16.m8.1.1.2.cmml" xref="A7.T10.16.m8.1.1.2">𝐷</ci><cn type="integer" id="A7.T10.16.m8.1.1.3.cmml" xref="A7.T10.16.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.16.m8.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.T10.16.m8.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.</figcaption>
<table id="A7.T10.18" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A7.T10.18.2" class="ltx_tr">
<td id="A7.T10.18.2.3" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2">
<span id="A7.T10.18.2.3.1" class="ltx_text"></span><span id="A7.T10.18.2.3.2" class="ltx_text ltx_font_bold"> <span id="A7.T10.18.2.3.2.1" class="ltx_text">
<span id="A7.T10.18.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A7.T10.18.2.3.2.1.1.1" class="ltx_tr">
<span id="A7.T10.18.2.3.2.1.1.1.1" class="ltx_td ltx_align_center">Method</span></span>
</span></span> <span id="A7.T10.18.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="A7.T10.18.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T10.18.2.4.1" class="ltx_text ltx_font_bold">IL</span></td>
<td id="A7.T10.18.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T10.18.2.5.1" class="ltx_text ltx_font_bold">NQE</span></td>
<td id="A7.T10.18.2.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="A7.T10.18.2.2.3" class="ltx_text"></span> <span id="A7.T10.18.2.2.2" class="ltx_text">
<span id="A7.T10.18.2.2.2.2.2" class="ltx_tabular ltx_align_middle">
<span id="A7.T10.18.2.2.2.2.2.3" class="ltx_tr">
<span id="A7.T10.18.2.2.2.2.2.3.1" class="ltx_td ltx_align_center"><span id="A7.T10.18.2.2.2.2.2.3.1.1" class="ltx_text ltx_font_bold">FUAR</span></span></span>
<span id="A7.T10.18.2.2.2.2.2.2" class="ltx_tr">
<span id="A7.T10.18.2.2.2.2.2.2.2" class="ltx_td ltx_align_center"><math id="A7.T10.17.1.1.1.1.1.1.1.m1.3" class="ltx_math_unparsed" alttext="\left((\mathbf{IL}),\bm{n.d.},\mathbf{NQE}\right)" display="inline"><semantics id="A7.T10.17.1.1.1.1.1.1.1.m1.3a"><mrow id="A7.T10.17.1.1.1.1.1.1.1.m1.3b"><mo id="A7.T10.17.1.1.1.1.1.1.1.m1.3.4">(</mo><mrow id="A7.T10.17.1.1.1.1.1.1.1.m1.3.5"><mo stretchy="false" id="A7.T10.17.1.1.1.1.1.1.1.m1.3.5.1">(</mo><mi id="A7.T10.17.1.1.1.1.1.1.1.m1.1.1">𝐈𝐋</mi><mo stretchy="false" id="A7.T10.17.1.1.1.1.1.1.1.m1.3.5.2">)</mo></mrow><mo id="A7.T10.17.1.1.1.1.1.1.1.m1.3.6">,</mo><mi id="A7.T10.17.1.1.1.1.1.1.1.m1.2.2">𝒏</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="A7.T10.17.1.1.1.1.1.1.1.m1.3.7">.</mo><mi id="A7.T10.17.1.1.1.1.1.1.1.m1.3.3">𝒅</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="A7.T10.17.1.1.1.1.1.1.1.m1.3.8">.</mo><mo id="A7.T10.17.1.1.1.1.1.1.1.m1.3.9">,</mo><mi id="A7.T10.17.1.1.1.1.1.1.1.m1.3.10">𝐍𝐐𝐄</mi><mo id="A7.T10.17.1.1.1.1.1.1.1.m1.3.11">)</mo></mrow><annotation encoding="application/x-tex" id="A7.T10.17.1.1.1.1.1.1.1.m1.3c">\left((\mathbf{IL}),\bm{n.d.},\mathbf{NQE}\right)</annotation><annotation encoding="application/x-llamapun" id="A7.T10.17.1.1.1.1.1.1.1.m1.3d">( ( bold_IL ) , bold_italic_n bold_. bold_italic_d bold_. , bold_NQE )</annotation></semantics></math> <math id="A7.T10.18.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A7.T10.18.2.2.2.2.2.2.2.m2.1a"><mo stretchy="false" id="A7.T10.18.2.2.2.2.2.2.2.m2.1.1" xref="A7.T10.18.2.2.2.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A7.T10.18.2.2.2.2.2.2.2.m2.1b"><ci id="A7.T10.18.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A7.T10.18.2.2.2.2.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.T10.18.2.2.2.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A7.T10.18.2.2.2.2.2.2.2.m2.1d">↓</annotation></semantics></math></span></span>
</span></span> <span id="A7.T10.18.2.2.4" class="ltx_text"></span>
</td>
</tr>
<tr id="A7.T10.18.3" class="ltx_tr">
<td id="A7.T10.18.3.1" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="A7.T10.18.3.2" class="ltx_td ltx_align_center ltx_border_t">EM</td>
</tr>
<tr id="A7.T10.18.4" class="ltx_tr">
<td id="A7.T10.18.4.1" class="ltx_td ltx_align_left ltx_border_t">GPT2-Initial</td>
<td id="A7.T10.18.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T10.18.4.2.1" class="ltx_text ltx_framed_underline">38.11</span></td>
<td id="A7.T10.18.4.3" class="ltx_td ltx_align_center ltx_border_t">4.3</td>
<td id="A7.T10.18.4.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A7.T10.18.5" class="ltx_tr">
<td id="A7.T10.18.5.1" class="ltx_td ltx_align_left ltx_border_t">GPT2-Vanilla</td>
<td id="A7.T10.18.5.2" class="ltx_td ltx_align_center ltx_border_t">35.88</td>
<td id="A7.T10.18.5.3" class="ltx_td ltx_align_center ltx_border_t">5.79</td>
<td id="A7.T10.18.5.4" class="ltx_td ltx_align_center ltx_border_t">1.58</td>
</tr>
<tr id="A7.T10.18.6" class="ltx_tr">
<td id="A7.T10.18.6.1" class="ltx_td ltx_align_left">GPT2-Recadam</td>
<td id="A7.T10.18.6.2" class="ltx_td ltx_align_center">35.50</td>
<td id="A7.T10.18.6.3" class="ltx_td ltx_align_center">5.79</td>
<td id="A7.T10.18.6.4" class="ltx_td ltx_align_center">1.84</td>
</tr>
<tr id="A7.T10.18.7" class="ltx_tr">
<td id="A7.T10.18.7.1" class="ltx_td ltx_align_left">GPT2-Mixreview</td>
<td id="A7.T10.18.7.2" class="ltx_td ltx_align_center"><span id="A7.T10.18.7.2.1" class="ltx_text ltx_font_bold">38.93</span></td>
<td id="A7.T10.18.7.3" class="ltx_td ltx_align_center">5.57</td>
<td id="A7.T10.18.7.4" class="ltx_td ltx_align_center"><span id="A7.T10.18.7.4.1" class="ltx_text ltx_font_bold">0</span></td>
</tr>
<tr id="A7.T10.18.8" class="ltx_tr">
<td id="A7.T10.18.8.1" class="ltx_td ltx_align_left">GPT2-Lora</td>
<td id="A7.T10.18.8.2" class="ltx_td ltx_align_center">37.99</td>
<td id="A7.T10.18.8.3" class="ltx_td ltx_align_center"><span id="A7.T10.18.8.3.1" class="ltx_text ltx_framed_underline">6.23</span></td>
<td id="A7.T10.18.8.4" class="ltx_td ltx_align_center"><span id="A7.T10.18.8.4.1" class="ltx_text ltx_framed_underline">0.06</span></td>
</tr>
<tr id="A7.T10.18.9" class="ltx_tr">
<td id="A7.T10.18.9.1" class="ltx_td ltx_align_left">GPT2-Kadapters (k=2)</td>
<td id="A7.T10.18.9.2" class="ltx_td ltx_align_center">37.85</td>
<td id="A7.T10.18.9.3" class="ltx_td ltx_align_center"><span id="A7.T10.18.9.3.1" class="ltx_text ltx_font_bold">6.34</span></td>
<td id="A7.T10.18.9.4" class="ltx_td ltx_align_center">0.13</td>
</tr>
<tr id="A7.T10.18.10" class="ltx_tr">
<td id="A7.T10.18.10.1" class="ltx_td ltx_align_left ltx_border_bb">GPT2-Kadapters (k=3)</td>
<td id="A7.T10.18.10.2" class="ltx_td ltx_align_center ltx_border_bb">38.03</td>
<td id="A7.T10.18.10.3" class="ltx_td ltx_align_center ltx_border_bb">5.79</td>
<td id="A7.T10.18.10.4" class="ltx_td ltx_align_center ltx_border_bb">0.06</td>
</tr>
</tbody></table>
</figure>
<div id="A7.p2" class="ltx_para ltx_noindent">
<p id="A7.p2.1" class="ltx_p">To measure the time-invariant knowledge, we use InvariantLAMA (IL) because most of the slots to fill are at the end of the sentence. For light-tuning on behalf of IL, we use additional T-Rex data from <cite class="ltx_cite ltx_citemacro_citet">Shin et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> which has a similar distribution as instances from IL. Among them, 5,000 instances with the same <span id="A7.p2.1.1" class="ltx_text ltx_font_italic">time-invariant</span> relations as IL are randomly sampled for <span id="A7.p2.1.2" class="ltx_text ltx_font_italic">light-tuning</span>.
On the other hand, unlike IL where most of the slots to fill are at the end of the sentences, the LAMA datasets for new knowledge in our CKL benchmark mostly have the slots at the beginning of the sentences. Therefore, we use the corresponding CBQA dataset of <span id="A7.p2.1.3" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span>, <span id="A7.p2.1.4" class="ltx_text ltx_font_smallcaps">NewQuestions-Easy</span> (NQE) to roughly measure the new knowledge.<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>The QA version of UL, NL and NLE will be also released with the main CKL benchmark.</span></span></span>
For light-tuning on behalf of NQE, 5,000 instances are sampled from a set of QA pairs constructed from <span id="A7.p2.1.5" class="ltx_text ltx_font_smallcaps">CC-RecentNews</span> but not <span id="A7.p2.1.6" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span> to remove the test-train overlap.</p>
</div>
<div id="A7.p3" class="ltx_para ltx_noindent">
<p id="A7.p3.2" class="ltx_p">Table <a href="#A7.T10" title="Table 10 ‣ Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the CKL benchmark performance of GPT-2 models. We report the results averaged over 5 runs with different random seeds.
As in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, parameter-expansion methods show robust performance on both IL and NQE, resulting in low <span id="A7.p3.2.1" class="ltx_text">FUAR</span>. This shows that these methods are not only effective on the encoder-decoder model but also the decoder-only model as well. One interesting result in Table <a href="#A7.T10" title="Table 10 ‣ Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> is that GPT2-MixReview performs the best on IL, with performance even higher than the initial model, which results in the best <span id="A7.p3.2.2" class="ltx_text">FUAR</span> of 0 which means no forgetting occurred at all. We suppose that the training strategy of GPT2-MixReview, allowing access to samples of <math id="A7.p3.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A7.p3.1.m1.1a"><msub id="A7.p3.1.m1.1.1" xref="A7.p3.1.m1.1.1.cmml"><mi id="A7.p3.1.m1.1.1.2" xref="A7.p3.1.m1.1.1.2.cmml">D</mi><mn id="A7.p3.1.m1.1.1.3" xref="A7.p3.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p3.1.m1.1b"><apply id="A7.p3.1.m1.1.1.cmml" xref="A7.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A7.p3.1.m1.1.1.1.cmml" xref="A7.p3.1.m1.1.1">subscript</csymbol><ci id="A7.p3.1.m1.1.1.2.cmml" xref="A7.p3.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A7.p3.1.m1.1.1.3.cmml" xref="A7.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> during continued pretraining, would have allowed fast adaptation to knowledge from <math id="A7.p3.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A7.p3.2.m2.1a"><msub id="A7.p3.2.m2.1.1" xref="A7.p3.2.m2.1.1.cmml"><mi id="A7.p3.2.m2.1.1.2" xref="A7.p3.2.m2.1.1.2.cmml">D</mi><mn id="A7.p3.2.m2.1.1.3" xref="A7.p3.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A7.p3.2.m2.1b"><apply id="A7.p3.2.m2.1.1.cmml" xref="A7.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A7.p3.2.m2.1.1.1.cmml" xref="A7.p3.2.m2.1.1">subscript</csymbol><ci id="A7.p3.2.m2.1.1.2.cmml" xref="A7.p3.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A7.p3.2.m2.1.1.3.cmml" xref="A7.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p3.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A7.p3.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> during the <span id="A7.p3.2.3" class="ltx_text ltx_font_italic">light-tuning</span> phase. Performance of GPT2-MixReview suggests that it makes it possible to regain the original knowledge for decoder-only models even with small tuning steps.</p>
</div>
<div id="A7.p4" class="ltx_para ltx_noindent">
<p id="A7.p4.1" class="ltx_p">We want to highlight that the discrepancy of the performances among the CKL methods between encoder-decoder LM (T5) and decoder-only LM (GPT-2) may not solely be on the LM architecture, but also on the learning rate and the evaluation method (light-tuning was used to evaluate GPT-2 while we evaluated T5 in a zero-shot manner). We leave further exploration of training ever-changing decoder-only LMs such as GPT-2 as future work.</p>
</div>
<section id="A7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>Failed GPT-2 experiments with Larger Learning Rate</h3>

<div id="A7.SS1.p1" class="ltx_para ltx_noindent">
<p id="A7.SS1.p1.1" class="ltx_p">Table <a href="#A7.T11" title="Table 11 ‣ G.1 Failed GPT-2 experiments with Larger Learning Rate ‣ Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the CKL benchmark result of GPT-2 models continually pretrained on <span id="A7.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span> for 8 epochs with a learning rate of 1e-3. By comparing the results in this table with those in Table <a href="#A7.T10" title="Table 10 ‣ Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, which is for models continually pretrained with a learning rate of 1e-4, the results in Table <a href="#A7.T11" title="Table 11 ‣ G.1 Failed GPT-2 experiments with Larger Learning Rate ‣ Appendix G Exploring How CKL Methods Transfer Across LM Architectures ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows worse performance on both IL and NQE. Unlike in Appendix <a href="#A5" title="Appendix E Exploring the Trade-off of Varying the Learning Rate for Continual Pretraining ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, increasing the learning rate does not result in better learning of new knowledge. Instead, NQE performance is even worse than GPT2-Initial for GPT2-Vanilla, GPT2-Recadam, and GPT2-MixReview. <span id="A7.SS1.p1.1.2" class="ltx_text">FUAR</span> is <span id="A7.SS1.p1.1.3" class="ltx_text ltx_font_italic">no gain</span> for these cases by the definition of the metric because the denominator has the value of zero. This shows that a large learning rate for continual pretraining may lead to failure: neither retaining old knowledge nor acquiring new knowledge effectively. For parameter-expansion methods, because many parameters including the decoder are frozen during the continual training process, they seem to be less prone to the effect of a large learning rate.</p>
</div>
<figure id="A7.T11" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Performance of decoder-only models initially pretrained on Dec 2019 dump of Webtext and May 2020 dump of Wikipedia (<math id="A7.T11.3.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A7.T11.3.m1.1b"><msub id="A7.T11.3.m1.1.1" xref="A7.T11.3.m1.1.1.cmml"><mi id="A7.T11.3.m1.1.1.2" xref="A7.T11.3.m1.1.1.2.cmml">D</mi><mn id="A7.T11.3.m1.1.1.3" xref="A7.T11.3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T11.3.m1.1c"><apply id="A7.T11.3.m1.1.1.cmml" xref="A7.T11.3.m1.1.1"><csymbol cd="ambiguous" id="A7.T11.3.m1.1.1.1.cmml" xref="A7.T11.3.m1.1.1">subscript</csymbol><ci id="A7.T11.3.m1.1.1.2.cmml" xref="A7.T11.3.m1.1.1.2">𝐷</ci><cn type="integer" id="A7.T11.3.m1.1.1.3.cmml" xref="A7.T11.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T11.3.m1.1d">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A7.T11.3.m1.1e">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>) continually pretrained on <span id="A7.T11.10.1" class="ltx_text ltx_font_smallcaps">CC-RecentNews-Small</span> (<math id="A7.T11.4.m2.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A7.T11.4.m2.1b"><msub id="A7.T11.4.m2.1.1" xref="A7.T11.4.m2.1.1.cmml"><mi id="A7.T11.4.m2.1.1.2" xref="A7.T11.4.m2.1.1.2.cmml">D</mi><mn id="A7.T11.4.m2.1.1.3" xref="A7.T11.4.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A7.T11.4.m2.1c"><apply id="A7.T11.4.m2.1.1.cmml" xref="A7.T11.4.m2.1.1"><csymbol cd="ambiguous" id="A7.T11.4.m2.1.1.1.cmml" xref="A7.T11.4.m2.1.1">subscript</csymbol><ci id="A7.T11.4.m2.1.1.2.cmml" xref="A7.T11.4.m2.1.1.2">𝐷</ci><cn type="integer" id="A7.T11.4.m2.1.1.3.cmml" xref="A7.T11.4.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.T11.4.m2.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A7.T11.4.m2.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>) for 8 epochs with a learning rate of 1e-3. These are the results failed due to a large learning rate. Each of IL and NQE stands for <span id="A7.T11.11.2" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span> and <span id="A7.T11.12.3" class="ltx_text ltx_font_smallcaps">NewQuestions-Easy</span>.</figcaption>
<table id="A7.T11.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A7.T11.6.2" class="ltx_tr">
<td id="A7.T11.6.2.3" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2">
<span id="A7.T11.6.2.3.1" class="ltx_text"></span><span id="A7.T11.6.2.3.2" class="ltx_text ltx_font_bold"> <span id="A7.T11.6.2.3.2.1" class="ltx_text">
<span id="A7.T11.6.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A7.T11.6.2.3.2.1.1.1" class="ltx_tr">
<span id="A7.T11.6.2.3.2.1.1.1.1" class="ltx_td ltx_align_center">Method</span></span>
</span></span> <span id="A7.T11.6.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="A7.T11.6.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T11.6.2.4.1" class="ltx_text ltx_font_bold">IL</span></td>
<td id="A7.T11.6.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A7.T11.6.2.5.1" class="ltx_text ltx_font_bold">NQE</span></td>
<td id="A7.T11.6.2.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2">
<span id="A7.T11.6.2.2.3" class="ltx_text"></span> <span id="A7.T11.6.2.2.2" class="ltx_text">
<span id="A7.T11.6.2.2.2.2.2" class="ltx_tabular ltx_align_middle">
<span id="A7.T11.6.2.2.2.2.2.3" class="ltx_tr">
<span id="A7.T11.6.2.2.2.2.2.3.1" class="ltx_td ltx_align_center"><span id="A7.T11.6.2.2.2.2.2.3.1.1" class="ltx_text ltx_font_bold">FUAR</span></span></span>
<span id="A7.T11.6.2.2.2.2.2.2" class="ltx_tr">
<span id="A7.T11.6.2.2.2.2.2.2.2" class="ltx_td ltx_align_center"><math id="A7.T11.5.1.1.1.1.1.1.1.m1.3" class="ltx_math_unparsed" alttext="\left((\mathbf{IL}),\bm{n.d.},\mathbf{NQE}\right)" display="inline"><semantics id="A7.T11.5.1.1.1.1.1.1.1.m1.3a"><mrow id="A7.T11.5.1.1.1.1.1.1.1.m1.3b"><mo id="A7.T11.5.1.1.1.1.1.1.1.m1.3.4">(</mo><mrow id="A7.T11.5.1.1.1.1.1.1.1.m1.3.5"><mo stretchy="false" id="A7.T11.5.1.1.1.1.1.1.1.m1.3.5.1">(</mo><mi id="A7.T11.5.1.1.1.1.1.1.1.m1.1.1">𝐈𝐋</mi><mo stretchy="false" id="A7.T11.5.1.1.1.1.1.1.1.m1.3.5.2">)</mo></mrow><mo id="A7.T11.5.1.1.1.1.1.1.1.m1.3.6">,</mo><mi id="A7.T11.5.1.1.1.1.1.1.1.m1.2.2">𝒏</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="A7.T11.5.1.1.1.1.1.1.1.m1.3.7">.</mo><mi id="A7.T11.5.1.1.1.1.1.1.1.m1.3.3">𝒅</mi><mo lspace="0em" mathvariant="bold" rspace="0.167em" id="A7.T11.5.1.1.1.1.1.1.1.m1.3.8">.</mo><mo id="A7.T11.5.1.1.1.1.1.1.1.m1.3.9">,</mo><mi id="A7.T11.5.1.1.1.1.1.1.1.m1.3.10">𝐍𝐐𝐄</mi><mo id="A7.T11.5.1.1.1.1.1.1.1.m1.3.11">)</mo></mrow><annotation encoding="application/x-tex" id="A7.T11.5.1.1.1.1.1.1.1.m1.3c">\left((\mathbf{IL}),\bm{n.d.},\mathbf{NQE}\right)</annotation><annotation encoding="application/x-llamapun" id="A7.T11.5.1.1.1.1.1.1.1.m1.3d">( ( bold_IL ) , bold_italic_n bold_. bold_italic_d bold_. , bold_NQE )</annotation></semantics></math> <math id="A7.T11.6.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A7.T11.6.2.2.2.2.2.2.2.m2.1a"><mo stretchy="false" id="A7.T11.6.2.2.2.2.2.2.2.m2.1.1" xref="A7.T11.6.2.2.2.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A7.T11.6.2.2.2.2.2.2.2.m2.1b"><ci id="A7.T11.6.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A7.T11.6.2.2.2.2.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.T11.6.2.2.2.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A7.T11.6.2.2.2.2.2.2.2.m2.1d">↓</annotation></semantics></math></span></span>
</span></span> <span id="A7.T11.6.2.2.4" class="ltx_text"></span>
</td>
</tr>
<tr id="A7.T11.6.3" class="ltx_tr">
<td id="A7.T11.6.3.1" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="A7.T11.6.3.2" class="ltx_td ltx_align_center ltx_border_t">EM</td>
</tr>
<tr id="A7.T11.6.4" class="ltx_tr">
<td id="A7.T11.6.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T11.6.4.1.1" class="ltx_text">GPT2-Initial</span></td>
<td id="A7.T11.6.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T11.6.4.2.1" class="ltx_text ltx_font_bold">38.11</span></td>
<td id="A7.T11.6.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T11.6.4.3.1" class="ltx_text">4.37</span></td>
<td id="A7.T11.6.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T11.6.4.4.1" class="ltx_text">-</span></td>
</tr>
<tr id="A7.T11.6.5" class="ltx_tr">
<td id="A7.T11.6.5.1" class="ltx_td ltx_align_left ltx_border_t">GPT2-Vanilla</td>
<td id="A7.T11.6.5.2" class="ltx_td ltx_align_center ltx_border_t">23.03</td>
<td id="A7.T11.6.5.3" class="ltx_td ltx_align_center ltx_border_t">1.64</td>
<td id="A7.T11.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A7.T11.6.5.4.1" class="ltx_text ltx_font_italic">no gain</span></td>
</tr>
<tr id="A7.T11.6.6" class="ltx_tr">
<td id="A7.T11.6.6.1" class="ltx_td ltx_align_left">GPT2-Recadam</td>
<td id="A7.T11.6.6.2" class="ltx_td ltx_align_center">25.38</td>
<td id="A7.T11.6.6.3" class="ltx_td ltx_align_center">2.73</td>
<td id="A7.T11.6.6.4" class="ltx_td ltx_align_center"><span id="A7.T11.6.6.4.1" class="ltx_text ltx_font_italic">no gain</span></td>
</tr>
<tr id="A7.T11.6.7" class="ltx_tr">
<td id="A7.T11.6.7.1" class="ltx_td ltx_align_left">GPT2-Mixreview</td>
<td id="A7.T11.6.7.2" class="ltx_td ltx_align_center">32.07</td>
<td id="A7.T11.6.7.3" class="ltx_td ltx_align_center">1.64</td>
<td id="A7.T11.6.7.4" class="ltx_td ltx_align_center"><span id="A7.T11.6.7.4.1" class="ltx_text ltx_font_italic">no gain</span></td>
</tr>
<tr id="A7.T11.6.8" class="ltx_tr">
<td id="A7.T11.6.8.1" class="ltx_td ltx_align_left">GPT2-Lora</td>
<td id="A7.T11.6.8.2" class="ltx_td ltx_align_center"><span id="A7.T11.6.8.2.1" class="ltx_text ltx_framed_underline">34.52</span></td>
<td id="A7.T11.6.8.3" class="ltx_td ltx_align_center">5.46</td>
<td id="A7.T11.6.8.4" class="ltx_td ltx_align_center">3.29</td>
</tr>
<tr id="A7.T11.6.9" class="ltx_tr">
<td id="A7.T11.6.9.1" class="ltx_td ltx_align_left">GPT2-Kadapters (k=2)</td>
<td id="A7.T11.6.9.2" class="ltx_td ltx_align_center">33.67</td>
<td id="A7.T11.6.9.3" class="ltx_td ltx_align_center"><span id="A7.T11.6.9.3.1" class="ltx_text ltx_framed_underline">6.01</span></td>
<td id="A7.T11.6.9.4" class="ltx_td ltx_align_center"><span id="A7.T11.6.9.4.1" class="ltx_text ltx_framed_underline">2.71</span></td>
</tr>
<tr id="A7.T11.6.10" class="ltx_tr">
<td id="A7.T11.6.10.1" class="ltx_td ltx_align_left ltx_border_bb">GPT2-Kadapters (k=3)</td>
<td id="A7.T11.6.10.2" class="ltx_td ltx_align_center ltx_border_bb">31.75</td>
<td id="A7.T11.6.10.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T11.6.10.3.1" class="ltx_text ltx_font_bold">7.65</span></td>
<td id="A7.T11.6.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A7.T11.6.10.4.1" class="ltx_text ltx_font_bold">1.94</span></td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Exploring the Prediction Change During Continual Pretraining</h2>

<div id="A8.p1" class="ltx_para ltx_noindent">
<p id="A8.p1.1" class="ltx_p">Table <a href="#A8.T12" title="Table 12 ‣ Appendix H Exploring the Prediction Change During Continual Pretraining ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows the prediction results of T5-Vanilla and T5-Modular on three knowledge probing tasks: <span id="A8.p1.1.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span>, <span id="A8.p1.1.2" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span>, and <span id="A8.p1.1.3" class="ltx_text ltx_font_smallcaps">NewLAMA</span>. We show the prediction for every training epoch for each model. The instances are selected from the predictions that T5-Modular got correct but T5-Initial got wrong on the final prediction, in order to see where the gap of the EM comes from.</p>
</div>
<figure id="A8.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Change of Prediction Outputs During Continued Pretraininig</figcaption>
<table id="A8.T12.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T12.1.1" class="ltx_tr">
<td id="A8.T12.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"></td>
<td id="A8.T12.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.2.1" class="ltx_text ltx_font_bold">Cloze Sentence</span></td>
<td id="A8.T12.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.3.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="A8.T12.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.4.1" class="ltx_text ltx_font_bold">Epoch 1</span></td>
<td id="A8.T12.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.5.1" class="ltx_text ltx_font_bold">Epoch 2</span></td>
<td id="A8.T12.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.6.1" class="ltx_text ltx_font_bold">Epoch 3</span></td>
<td id="A8.T12.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.7.1" class="ltx_text ltx_font_bold">Epoch 4</span></td>
<td id="A8.T12.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A8.T12.1.1.8.1" class="ltx_text ltx_font_bold">Answer</span></td>
</tr>
<tr id="A8.T12.1.2" class="ltx_tr">
<td id="A8.T12.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="10"><span id="A8.T12.1.2.1.1" class="ltx_text ltx_font_bold">IL</span></td>
<td id="A8.T12.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.2.2.1" class="ltx_text">
<span id="A8.T12.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.2.2.1.1.1" class="ltx_tr">
<span id="A8.T12.1.2.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">The native language of</span></span>
<span id="A8.T12.1.2.2.1.1.2" class="ltx_tr">
<span id="A8.T12.1.2.2.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Yvonne Monlaur is <span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></span>
</span></span></td>
<td id="A8.T12.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">French</td>
<td id="A8.T12.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">French</td>
<td id="A8.T12.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Khmer</td>
<td id="A8.T12.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Malaya</td>
<td id="A8.T12.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.2.8.1" class="ltx_text">French</span></td>
</tr>
<tr id="A8.T12.1.3" class="ltx_tr">
<td id="A8.T12.1.3.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.3.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">French</td>
<td id="A8.T12.1.3.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">French</td>
<td id="A8.T12.1.3.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">French</td>
<td id="A8.T12.1.3.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">French</td>
</tr>
<tr id="A8.T12.1.4" class="ltx_tr">
<td id="A8.T12.1.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.4.1.1" class="ltx_text">Sonic Drift 2 is developed by <span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></td>
<td id="A8.T12.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Sonic D</td>
<td id="A8.T12.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Sonic the</td>
<td id="A8.T12.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Sonic Found</td>
<td id="A8.T12.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Sonic the</td>
<td id="A8.T12.1.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.4.7.1" class="ltx_text">Sega</span></td>
</tr>
<tr id="A8.T12.1.5" class="ltx_tr">
<td id="A8.T12.1.5.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.5.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Sonic R</td>
<td id="A8.T12.1.5.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Sega</td>
<td id="A8.T12.1.5.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Sega</td>
<td id="A8.T12.1.5.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Sega</td>
</tr>
<tr id="A8.T12.1.6" class="ltx_tr">
<td id="A8.T12.1.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.6.1.1" class="ltx_text">WebKit is developed by <span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></td>
<td id="A8.T12.1.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Microsoft</td>
<td id="A8.T12.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Google</td>
<td id="A8.T12.1.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">GitHub</td>
<td id="A8.T12.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Google</td>
<td id="A8.T12.1.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.6.7.1" class="ltx_text">Apple</span></td>
</tr>
<tr id="A8.T12.1.7" class="ltx_tr">
<td id="A8.T12.1.7.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.7.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Apple</td>
<td id="A8.T12.1.7.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Apple</td>
<td id="A8.T12.1.7.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Apple</td>
<td id="A8.T12.1.7.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Apple</td>
</tr>
<tr id="A8.T12.1.8" class="ltx_tr">
<td id="A8.T12.1.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.8.1.1" class="ltx_text">
<span id="A8.T12.1.8.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.8.1.1.1.1" class="ltx_tr">
<span id="A8.T12.1.8.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">The official language of Republic of</span></span>
<span id="A8.T12.1.8.1.1.1.2" class="ltx_tr">
<span id="A8.T12.1.8.1.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Ingushetia is <span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></span>
</span></span></td>
<td id="A8.T12.1.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Russian</td>
<td id="A8.T12.1.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">English</td>
<td id="A8.T12.1.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Kazakh</td>
<td id="A8.T12.1.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">English</td>
<td id="A8.T12.1.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.8.7.1" class="ltx_text">Russian</span></td>
</tr>
<tr id="A8.T12.1.9" class="ltx_tr">
<td id="A8.T12.1.9.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.9.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russian</td>
<td id="A8.T12.1.9.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russian</td>
<td id="A8.T12.1.9.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russian</td>
<td id="A8.T12.1.9.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russian</td>
</tr>
<tr id="A8.T12.1.10" class="ltx_tr">
<td id="A8.T12.1.10.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.10.1.1" class="ltx_text">The capital of Roman Empire is&nbsp;<span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></td>
<td id="A8.T12.1.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Rome</td>
<td id="A8.T12.1.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Rome</td>
<td id="A8.T12.1.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Constantino</td>
<td id="A8.T12.1.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Constantino</td>
<td id="A8.T12.1.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.10.7.1" class="ltx_text">Rome</span></td>
</tr>
<tr id="A8.T12.1.11" class="ltx_tr">
<td id="A8.T12.1.11.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.11.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Rome</td>
<td id="A8.T12.1.11.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Rome</td>
<td id="A8.T12.1.11.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Rome</td>
<td id="A8.T12.1.11.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Rome</td>
</tr>
<tr id="A8.T12.1.12" class="ltx_tr">
<td id="A8.T12.1.12.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="10"><span id="A8.T12.1.12.1.1" class="ltx_text ltx_font_bold">UL</span></td>
<td id="A8.T12.1.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.12.2.1" class="ltx_text">
<span id="A8.T12.1.12.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.12.2.1.1.1" class="ltx_tr">
<span id="A8.T12.1.12.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">The biggest exporter of crude oil</span></span>
<span id="A8.T12.1.12.2.1.1.2" class="ltx_tr">
<span id="A8.T12.1.12.2.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">to china is <span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></span>
</span></span></td>
<td id="A8.T12.1.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Saudi Arabia</td>
<td id="A8.T12.1.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Saudi Arabia</td>
<td id="A8.T12.1.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Saudi Arabia</td>
<td id="A8.T12.1.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Saudi Arabia</td>
<td id="A8.T12.1.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.12.8.1" class="ltx_text">
<span id="A8.T12.1.12.8.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.12.8.1.1.1" class="ltx_tr">
<span id="A8.T12.1.12.8.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Saudi Arabia →</span></span>
<span id="A8.T12.1.12.8.1.1.2" class="ltx_tr">
<span id="A8.T12.1.12.8.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russia</span></span>
</span></span></td>
</tr>
<tr id="A8.T12.1.13" class="ltx_tr">
<td id="A8.T12.1.13.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.13.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russia</td>
<td id="A8.T12.1.13.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Saudi Arabia</td>
<td id="A8.T12.1.13.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russia</td>
<td id="A8.T12.1.13.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Russia</td>
</tr>
<tr id="A8.T12.1.14" class="ltx_tr">
<td id="A8.T12.1.14.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.14.1.1" class="ltx_text">
<span id="A8.T12.1.14.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.14.1.1.1.1" class="ltx_tr">
<span id="A8.T12.1.14.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the head of</span></span>
<span id="A8.T12.1.14.1.1.1.2" class="ltx_tr">
<span id="A8.T12.1.14.1.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">the euro zone central bank</span></span>
</span></span></td>
<td id="A8.T12.1.14.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Mario Draghi</td>
<td id="A8.T12.1.14.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Yves Le Maire</td>
<td id="A8.T12.1.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Yves Dujarric</td>
<td id="A8.T12.1.14.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Mario Draghi</td>
<td id="A8.T12.1.14.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.14.7.1" class="ltx_text">
<span id="A8.T12.1.14.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.14.7.1.1.1" class="ltx_tr">
<span id="A8.T12.1.14.7.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Mario Draghi →</span></span>
<span id="A8.T12.1.14.7.1.1.2" class="ltx_tr">
<span id="A8.T12.1.14.7.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Christine Lagarde</span></span>
</span></span></td>
</tr>
<tr id="A8.T12.1.15" class="ltx_tr">
<td id="A8.T12.1.15.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.15.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Mario Draghi</td>
<td id="A8.T12.1.15.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Christine Lagarde</td>
<td id="A8.T12.1.15.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Christine Lagarde</td>
<td id="A8.T12.1.15.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Christine Lagarde</td>
</tr>
<tr id="A8.T12.1.16" class="ltx_tr">
<td id="A8.T12.1.16.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.16.1.1" class="ltx_text">
<span id="A8.T12.1.16.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.16.1.1.1.1" class="ltx_tr">
<span id="A8.T12.1.16.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the manager of</span></span>
<span id="A8.T12.1.16.1.1.1.2" class="ltx_tr">
<span id="A8.T12.1.16.1.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">chelsea in the premier league</span></span>
</span></span></td>
<td id="A8.T12.1.16.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.16.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Mauricio Fernandez</td>
<td id="A8.T12.1.16.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Steve Bruce</td>
<td id="A8.T12.1.16.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Frank Lampard</td>
<td id="A8.T12.1.16.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Mikel Arteta</td>
<td id="A8.T12.1.16.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.16.7.1" class="ltx_text">
<span id="A8.T12.1.16.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.16.7.1.1.1" class="ltx_tr">
<span id="A8.T12.1.16.7.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Luis Enrique →</span></span>
<span id="A8.T12.1.16.7.1.1.2" class="ltx_tr">
<span id="A8.T12.1.16.7.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Frank Lampard</span></span>
</span></span></td>
</tr>
<tr id="A8.T12.1.17" class="ltx_tr">
<td id="A8.T12.1.17.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.17.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Jose Mourinho</td>
<td id="A8.T12.1.17.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Jose Mourinho</td>
<td id="A8.T12.1.17.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Frank Lampard</td>
<td id="A8.T12.1.17.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Frank Lampard</td>
</tr>
<tr id="A8.T12.1.18" class="ltx_tr">
<td id="A8.T12.1.18.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.18.1.1" class="ltx_text"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the price for a flat in nottingham</span></td>
<td id="A8.T12.1.18.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.18.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">What</td>
<td id="A8.T12.1.18.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">999</td>
<td id="A8.T12.1.18.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">£1.25m</td>
<td id="A8.T12.1.18.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">£1.25m</td>
<td id="A8.T12.1.18.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.18.7.1" class="ltx_text">
<span id="A8.T12.1.18.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.18.7.1.1.1" class="ltx_tr">
<span id="A8.T12.1.18.7.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">36,000 →</span></span>
<span id="A8.T12.1.18.7.1.1.2" class="ltx_tr">
<span id="A8.T12.1.18.7.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">40,000</span></span>
</span></span></td>
</tr>
<tr id="A8.T12.1.19" class="ltx_tr">
<td id="A8.T12.1.19.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.19.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">This</td>
<td id="A8.T12.1.19.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">30,000 pounds</td>
<td id="A8.T12.1.19.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">40,000 pounds</td>
<td id="A8.T12.1.19.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">40,000</td>
</tr>
<tr id="A8.T12.1.20" class="ltx_tr">
<td id="A8.T12.1.20.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.20.1.1" class="ltx_text">
<span id="A8.T12.1.20.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.20.1.1.1.1" class="ltx_tr">
<span id="A8.T12.1.20.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> was the governor of New York</span></span>
<span id="A8.T12.1.20.1.1.1.2" class="ltx_tr">
<span id="A8.T12.1.20.1.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">at the time this article was written</span></span>
</span></span></td>
<td id="A8.T12.1.20.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.20.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew M. Cuomo</td>
<td id="A8.T12.1.20.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Cuomo</td>
<td id="A8.T12.1.20.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew Cuomo</td>
<td id="A8.T12.1.20.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Franklin D. Roosevelt</td>
<td id="A8.T12.1.20.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.20.7.1" class="ltx_text">
<span id="A8.T12.1.20.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.20.7.1.1.1" class="ltx_tr">
<span id="A8.T12.1.20.7.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Martin Van Buren →</span></span>
<span id="A8.T12.1.20.7.1.1.2" class="ltx_tr">
<span id="A8.T12.1.20.7.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew Cuomo</span></span>
</span></span></td>
</tr>
<tr id="A8.T12.1.21" class="ltx_tr">
<td id="A8.T12.1.21.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.21.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew Cuomo</td>
<td id="A8.T12.1.21.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew Cuomo</td>
<td id="A8.T12.1.21.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew M. Cuomo</td>
<td id="A8.T12.1.21.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Andrew Cuomo</td>
</tr>
<tr id="A8.T12.1.22" class="ltx_tr">
<td id="A8.T12.1.22.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="10"><span id="A8.T12.1.22.1.1" class="ltx_text ltx_font_bold">NL</span></td>
<td id="A8.T12.1.22.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.22.2.1" class="ltx_text">
<span id="A8.T12.1.22.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.22.2.1.1.1" class="ltx_tr">
<span id="A8.T12.1.22.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is on the Bills all-pro team</span></span>
</span></span></td>
<td id="A8.T12.1.22.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.22.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Corey</td>
<td id="A8.T12.1.22.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Williams</td>
<td id="A8.T12.1.22.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Corey</td>
<td id="A8.T12.1.22.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Connor</td>
<td id="A8.T12.1.22.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.22.8.1" class="ltx_text">Williams</span></td>
</tr>
<tr id="A8.T12.1.23" class="ltx_tr">
<td id="A8.T12.1.23.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.23.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Williams</td>
<td id="A8.T12.1.23.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Williams</td>
<td id="A8.T12.1.23.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Williams</td>
<td id="A8.T12.1.23.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Williams</td>
</tr>
<tr id="A8.T12.1.24" class="ltx_tr">
<td id="A8.T12.1.24.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.24.1.1" class="ltx_text">
<span id="A8.T12.1.24.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.24.1.1.1.1" class="ltx_tr">
<span id="A8.T12.1.24.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the founder of the popular</span></span>
<span id="A8.T12.1.24.1.1.1.2" class="ltx_tr">
<span id="A8.T12.1.24.1.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">cryptocurrency bitcoin</span></span>
</span></span></td>
<td id="A8.T12.1.24.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.24.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Satoshi Nakamoto</td>
<td id="A8.T12.1.24.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Satoshi Nakamoto</td>
<td id="A8.T12.1.24.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Yuri</td>
<td id="A8.T12.1.24.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Xiaobo</td>
<td id="A8.T12.1.24.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.24.7.1" class="ltx_text">Satoshi Nakamoto</span></td>
</tr>
<tr id="A8.T12.1.25" class="ltx_tr">
<td id="A8.T12.1.25.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.25.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Vitalik Buterin</td>
<td id="A8.T12.1.25.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Satoshi Nakamoto</td>
<td id="A8.T12.1.25.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Satoshi Nakamoto</td>
<td id="A8.T12.1.25.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">Satoshi Nakamoto</td>
</tr>
<tr id="A8.T12.1.26" class="ltx_tr">
<td id="A8.T12.1.26.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.26.1.1" class="ltx_text">The bail for kyle rittenhouse is <span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> .</span></td>
<td id="A8.T12.1.26.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.26.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Rs. 1 crore</td>
<td id="A8.T12.1.26.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">a whopping $1 million</td>
<td id="A8.T12.1.26.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">$2 million</td>
<td id="A8.T12.1.26.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">$1 million</td>
<td id="A8.T12.1.26.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.26.7.1" class="ltx_text">$2 million</span></td>
</tr>
<tr id="A8.T12.1.27" class="ltx_tr">
<td id="A8.T12.1.27.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.27.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">$2 million</td>
<td id="A8.T12.1.27.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">$2 million</td>
<td id="A8.T12.1.27.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">$2 million</td>
<td id="A8.T12.1.27.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">$2 million</td>
</tr>
<tr id="A8.T12.1.28" class="ltx_tr">
<td id="A8.T12.1.28.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.28.1.1" class="ltx_text">
<span id="A8.T12.1.28.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A8.T12.1.28.1.1.1.1" class="ltx_tr">
<span id="A8.T12.1.28.1.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">The las vegas raiders beat</span></span>
<span id="A8.T12.1.28.1.1.1.2" class="ltx_tr">
<span id="A8.T12.1.28.1.1.1.2.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> in the playoffs</span></span>
</span></span></td>
<td id="A8.T12.1.28.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.28.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">the Las Vegas Raiders</td>
<td id="A8.T12.1.28.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">the New Orleans Saints</td>
<td id="A8.T12.1.28.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">the Las Vegas Raiders</td>
<td id="A8.T12.1.28.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">the sacramento</td>
<td id="A8.T12.1.28.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.28.7.1" class="ltx_text">the New Orleans Saints</span></td>
</tr>
<tr id="A8.T12.1.29" class="ltx_tr">
<td id="A8.T12.1.29.1" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.29.2" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">the New Orleans Saints</td>
<td id="A8.T12.1.29.3" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">the Kansas City Chiefs</td>
<td id="A8.T12.1.29.4" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">the Kansas City Chiefs</td>
<td id="A8.T12.1.29.5" class="ltx_td ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">the New Orleans Saints</td>
</tr>
<tr id="A8.T12.1.30" class="ltx_tr">
<td id="A8.T12.1.30.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.30.1.1" class="ltx_text"><span class="ltx_rule" style="width:11.4pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the host of ellen de generes show</span></td>
<td id="A8.T12.1.30.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">V</td>
<td id="A8.T12.1.30.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Yves</td>
<td id="A8.T12.1.30.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">samantha s</td>
<td id="A8.T12.1.30.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Norma</td>
<td id="A8.T12.1.30.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Mike</td>
<td id="A8.T12.1.30.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;" rowspan="2"><span id="A8.T12.1.30.7.1" class="ltx_text">Ellen DeGeneres</span></td>
</tr>
<tr id="A8.T12.1.31" class="ltx_tr">
<td id="A8.T12.1.31.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">M</td>
<td id="A8.T12.1.31.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">Elise</td>
<td id="A8.T12.1.31.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">Ellen DeGeneres</td>
<td id="A8.T12.1.31.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">Ellen deGenes</td>
<td id="A8.T12.1.31.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">Ellen DeGeneres</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Exploring the Cause of the EM Gap Between <span id="A9.1.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A9.2.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span>
</h2>

<figure id="A9.F8" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="A9.F7.sf1" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2110.03215/assets/x10.png" id="A9.F7.sf1.g1" class="ltx_graphics ltx_img_square" width="761" height="732" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Composition of ground truth categories of the correctly predicted instances</figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_2">
<figure id="A9.F7.sf2" class="ltx_figure ltx_flex_size_2 ltx_align_center"><img src="/html/2110.03215/assets/x11.png" id="A9.F7.sf2.g1" class="ltx_graphics ltx_img_square" width="761" height="906" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>EM measured using the instances from UL and NL with overlapping <span id="A9.F7.sf2.2.1" class="ltx_text ltx_font_italic">Person</span> type answers</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Analyzing the cause of the EM gap between <span id="A9.F8.3.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> and <span id="A9.F8.4.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span>.</figcaption>
</figure>
<div id="A9.p1" class="ltx_para ltx_noindent">
<p id="A9.p1.7" class="ltx_p">As shown in the main experiment, Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there is a considerable gap between the EM of <span id="A9.p1.7.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span> (UL) and <span id="A9.p1.7.2" class="ltx_text ltx_font_smallcaps">NewLAMA</span> (NL) over all the methods, despite undergoing the same data construction process. We attempt to analyze the causation by first analyzing what answer types make up the EM score of both UL and NL of T5-Vanilla, which are 10.17 and 3.77, respectively. As shown in Figure <a href="#A9.F8" title="Figure 8 ‣ Appendix I Exploring the Cause of the EM Gap Between UpdatedLAMA and NewLAMA ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>a, the cloze sentences that take <span id="A9.p1.7.3" class="ltx_text ltx_font_italic">Person</span> type as the ground truth makes up most of the EM of both tasks, despite <span id="A9.p1.7.4" class="ltx_text ltx_font_italic">Person</span> type answers taking up a similar proportion out of the total answer types (61.46% for UL and 59.7% for NL). Since UL consists of probes requiring an update of information from <math id="A9.p1.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A9.p1.1.m1.1a"><msub id="A9.p1.1.m1.1.1" xref="A9.p1.1.m1.1.1.cmml"><mi id="A9.p1.1.m1.1.1.2" xref="A9.p1.1.m1.1.1.2.cmml">D</mi><mn id="A9.p1.1.m1.1.1.3" xref="A9.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.1.m1.1b"><apply id="A9.p1.1.m1.1.1.cmml" xref="A9.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A9.p1.1.m1.1.1.1.cmml" xref="A9.p1.1.m1.1.1">subscript</csymbol><ci id="A9.p1.1.m1.1.1.2.cmml" xref="A9.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.1.m1.1.1.3.cmml" xref="A9.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.1.m1.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, one might conjecture that the EM gap is simply due to the difference of the frequency in each corpus of the entities that serve as the ground truths, e.g., those entities appear more in corpus <math id="A9.p1.2.m2.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A9.p1.2.m2.1a"><msub id="A9.p1.2.m2.1.1" xref="A9.p1.2.m2.1.1.cmml"><mi id="A9.p1.2.m2.1.1.2" xref="A9.p1.2.m2.1.1.2.cmml">D</mi><mn id="A9.p1.2.m2.1.1.3" xref="A9.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.2.m2.1b"><apply id="A9.p1.2.m2.1.1.cmml" xref="A9.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A9.p1.2.m2.1.1.1.cmml" xref="A9.p1.2.m2.1.1">subscript</csymbol><ci id="A9.p1.2.m2.1.1.2.cmml" xref="A9.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.2.m2.1.1.3.cmml" xref="A9.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.2.m2.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> than in <math id="A9.p1.3.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A9.p1.3.m3.1a"><msub id="A9.p1.3.m3.1.1" xref="A9.p1.3.m3.1.1.cmml"><mi id="A9.p1.3.m3.1.1.2" xref="A9.p1.3.m3.1.1.2.cmml">D</mi><mn id="A9.p1.3.m3.1.1.3" xref="A9.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.3.m3.1b"><apply id="A9.p1.3.m3.1.1.cmml" xref="A9.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A9.p1.3.m3.1.1.1.cmml" xref="A9.p1.3.m3.1.1">subscript</csymbol><ci id="A9.p1.3.m3.1.1.2.cmml" xref="A9.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.3.m3.1.1.3.cmml" xref="A9.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.3.m3.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. In order to get rid of the influence of frequency of entities when analyzing the source of the EM gap, we find overlapping <span id="A9.p1.7.5" class="ltx_text ltx_font_italic">Person</span> type answers from UL and NL, and analyze only the 67 probing sentences for both datasets each paired to one of these entities. As shown in Figure <a href="#A9.F8" title="Figure 8 ‣ Appendix I Exploring the Cause of the EM Gap Between UpdatedLAMA and NewLAMA ‣ Towards Continual Knowledge Learning of Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>b, the EM on UL is still much higher than that of NL. Manually analyzing these instances, we find that the probing sentences for NL ask for relatively more <span id="A9.p1.7.6" class="ltx_text ltx_font_italic">fine-grained</span> knowledge compared to UL, since the instances of UL by definition are overlapped cloze sentences with different answers in the corpus <math id="A9.p1.4.m4.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="A9.p1.4.m4.1a"><msub id="A9.p1.4.m4.1.1" xref="A9.p1.4.m4.1.1.cmml"><mi id="A9.p1.4.m4.1.1.2" xref="A9.p1.4.m4.1.1.2.cmml">D</mi><mn id="A9.p1.4.m4.1.1.3" xref="A9.p1.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.4.m4.1b"><apply id="A9.p1.4.m4.1.1.cmml" xref="A9.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A9.p1.4.m4.1.1.1.cmml" xref="A9.p1.4.m4.1.1">subscript</csymbol><ci id="A9.p1.4.m4.1.1.2.cmml" xref="A9.p1.4.m4.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.4.m4.1.1.3.cmml" xref="A9.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.4.m4.1c">D_{0}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="A9.p1.5.m5.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A9.p1.5.m5.1a"><msub id="A9.p1.5.m5.1.1" xref="A9.p1.5.m5.1.1.cmml"><mi id="A9.p1.5.m5.1.1.2" xref="A9.p1.5.m5.1.1.2.cmml">D</mi><mn id="A9.p1.5.m5.1.1.3" xref="A9.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.5.m5.1b"><apply id="A9.p1.5.m5.1.1.cmml" xref="A9.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A9.p1.5.m5.1.1.1.cmml" xref="A9.p1.5.m5.1.1">subscript</csymbol><ci id="A9.p1.5.m5.1.1.2.cmml" xref="A9.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.5.m5.1.1.3.cmml" xref="A9.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.5.m5.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, that naturally make them be <span id="A9.p1.7.7" class="ltx_text ltx_font_italic">coarse-grained</span>. For instance, the probing sentences for entity “Tim Walz” in UL and NL are “<span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the governor of Minnesota this year.” and “<span class="ltx_rule" style="width:28.5pt;height:0.3pt;background:black;display:inline-block;">&nbsp;</span> is the governor of Minnesota calling for the evacuation of St. Paul.”, respectively. We thus conjecture that the main causation of the EM gap to be UL consisting of instances requiring <span id="A9.p1.7.8" class="ltx_text ltx_font_italic">coarse-grained</span> knowledge, which is likely to have appeared more during in <math id="A9.p1.6.m6.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A9.p1.6.m6.1a"><msub id="A9.p1.6.m6.1.1" xref="A9.p1.6.m6.1.1.cmml"><mi id="A9.p1.6.m6.1.1.2" xref="A9.p1.6.m6.1.1.2.cmml">D</mi><mn id="A9.p1.6.m6.1.1.3" xref="A9.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.6.m6.1b"><apply id="A9.p1.6.m6.1.1.cmml" xref="A9.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A9.p1.6.m6.1.1.1.cmml" xref="A9.p1.6.m6.1.1">subscript</csymbol><ci id="A9.p1.6.m6.1.1.2.cmml" xref="A9.p1.6.m6.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.6.m6.1.1.3.cmml" xref="A9.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.6.m6.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, while NL consisting of instances requiring <span id="A9.p1.7.9" class="ltx_text ltx_font_italic">fine-grained</span> knowledge, which is expected to likely have appeared less in <math id="A9.p1.7.m7.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="A9.p1.7.m7.1a"><msub id="A9.p1.7.m7.1.1" xref="A9.p1.7.m7.1.1.cmml"><mi id="A9.p1.7.m7.1.1.2" xref="A9.p1.7.m7.1.1.2.cmml">D</mi><mn id="A9.p1.7.m7.1.1.3" xref="A9.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A9.p1.7.m7.1b"><apply id="A9.p1.7.m7.1.1.cmml" xref="A9.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A9.p1.7.m7.1.1.1.cmml" xref="A9.p1.7.m7.1.1">subscript</csymbol><ci id="A9.p1.7.m7.1.1.2.cmml" xref="A9.p1.7.m7.1.1.2">𝐷</ci><cn type="integer" id="A9.p1.7.m7.1.1.3.cmml" xref="A9.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.7.m7.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="A9.p1.7.m7.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section id="A10" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Additional Analysis of Main Results</h2>

<figure id="A10.T13" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>F1 Score of Main Results.</figcaption>
<table id="A10.T13.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A10.T13.2.2" class="ltx_tr">
<td id="A10.T13.2.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="A10.T13.2.2.3.1" class="ltx_text"></span><span id="A10.T13.2.2.3.2" class="ltx_text ltx_font_bold"> <span id="A10.T13.2.2.3.2.1" class="ltx_text">
<span id="A10.T13.2.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="A10.T13.2.2.3.2.1.1.1" class="ltx_tr">
<span id="A10.T13.2.2.3.2.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Method</span></span>
</span></span> <span id="A10.T13.2.2.3.2.2" class="ltx_text"></span></span>
</td>
<td id="A10.T13.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.2.4.1" class="ltx_text ltx_font_bold">IL</span></td>
<td id="A10.T13.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.2.5.1" class="ltx_text ltx_font_bold">UL</span></td>
<td id="A10.T13.2.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.2.6.1" class="ltx_text ltx_font_bold">NL</span></td>
<td id="A10.T13.2.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.2.7.1" class="ltx_text ltx_font_bold">NLE</span></td>
<td id="A10.T13.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="A10.T13.2.2.2.3" class="ltx_text"></span><span id="A10.T13.2.2.2.2" class="ltx_text ltx_font_bold"> <span id="A10.T13.2.2.2.2.2" class="ltx_text">
<span id="A10.T13.2.2.2.2.2.2.2" class="ltx_tabular ltx_align_middle">
<span id="A10.T13.2.2.2.2.2.2.2.3" class="ltx_tr">
<span id="A10.T13.2.2.2.2.2.2.2.3.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.2.2.2.2.2.2.3.1.1" class="ltx_text">FUAR</span></span></span>
<span id="A10.T13.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="A10.T13.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><math id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4" class="ltx_Math" alttext="\mathbf{{\left((IL),UL,NL\right)}}" display="inline"><semantics id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4a"><mrow id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml"><mo mathvariant="normal" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.2" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">(</mo><mrow id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.1.2" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml"><mo mathvariant="normal" stretchy="false" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.1.2.1" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">(</mo><mi id="A10.T13.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐈𝐋</mi><mo mathvariant="normal" stretchy="false" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.1.2.2" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">)</mo></mrow><mo mathvariant="normal" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.3" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">,</mo><mi id="A10.T13.1.1.1.1.1.1.1.1.1.m1.2.2" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.2.2.cmml">𝐔𝐋</mi><mo mathvariant="normal" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.4" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">,</mo><mi id="A10.T13.1.1.1.1.1.1.1.1.1.m1.3.3" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.3.3.cmml">𝐍𝐋</mi><mo mathvariant="normal" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1.5" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4b"><vector id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.2.cmml" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.4.4.1"><ci id="A10.T13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.1.1">𝐈𝐋</ci><ci id="A10.T13.1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.2.2">𝐔𝐋</ci><ci id="A10.T13.1.1.1.1.1.1.1.1.1.m1.3.3.cmml" xref="A10.T13.1.1.1.1.1.1.1.1.1.m1.3.3">𝐍𝐋</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4c">\mathbf{{\left((IL),UL,NL\right)}}</annotation><annotation encoding="application/x-llamapun" id="A10.T13.1.1.1.1.1.1.1.1.1.m1.4d">( ( bold_IL ) , bold_UL , bold_NL )</annotation></semantics></math> <math id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1a"><mo mathvariant="normal" stretchy="false" id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1.1" xref="A10.T13.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A10.T13.2.2.2.2.2.2.2.2.2.m2.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A10.T13.2.2.2.2.2.2.2.2.2.m2.1d">↓</annotation></semantics></math></span></span>
</span></span> <span id="A10.T13.2.2.2.2.3" class="ltx_text"></span></span>
</td>
</tr>
<tr id="A10.T13.2.3" class="ltx_tr">
<td id="A10.T13.2.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="A10.T13.2.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="A10.T13.2.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
<td id="A10.T13.2.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EM</td>
</tr>
<tr id="A10.T13.2.4" class="ltx_tr">
<td id="A10.T13.2.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Initial</td>
<td id="A10.T13.2.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.4.2.1" class="ltx_text ltx_font_bold">24.88</span></td>
<td id="A10.T13.2.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2.62</td>
<td id="A10.T13.2.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.19</td>
<td id="A10.T13.2.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">14.49</td>
<td id="A10.T13.2.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr id="A10.T13.2.5" class="ltx_tr">
<td id="A10.T13.2.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Vanilla</td>
<td id="A10.T13.2.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">13.11</td>
<td id="A10.T13.2.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">11.89</td>
<td id="A10.T13.2.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5.84</td>
<td id="A10.T13.2.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">22.53</td>
<td id="A10.T13.2.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.68</td>
</tr>
<tr id="A10.T13.2.6" class="ltx_tr">
<td id="A10.T13.2.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-RecAdam</td>
<td id="A10.T13.2.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.39</td>
<td id="A10.T13.2.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.33</td>
<td id="A10.T13.2.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.15</td>
<td id="A10.T13.2.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">22.68</td>
<td id="A10.T13.2.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.57</td>
</tr>
<tr id="A10.T13.2.7" class="ltx_tr">
<td id="A10.T13.2.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-MixReview</td>
<td id="A10.T13.2.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.09</td>
<td id="A10.T13.2.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8.11</td>
<td id="A10.T13.2.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.80</td>
<td id="A10.T13.2.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.89</td>
<td id="A10.T13.2.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.10</td>
</tr>
<tr id="A10.T13.2.8" class="ltx_tr">
<td id="A10.T13.2.8.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-LoRA</td>
<td id="A10.T13.2.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.04</td>
<td id="A10.T13.2.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.8.3.1" class="ltx_text ltx_font_bold">14.50</span></td>
<td id="A10.T13.2.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.8.4.1" class="ltx_text ltx_font_bold">7.45</span></td>
<td id="A10.T13.2.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.8.5.1" class="ltx_text ltx_font_bold">24.59</span></td>
<td id="A10.T13.2.8.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.36</td>
</tr>
<tr id="A10.T13.2.9" class="ltx_tr">
<td id="A10.T13.2.9.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Kadapters (k=2)</td>
<td id="A10.T13.2.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">19.88</td>
<td id="A10.T13.2.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.67</td>
<td id="A10.T13.2.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.9.4.1" class="ltx_text ltx_framed_underline">7.43</span></td>
<td id="A10.T13.2.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.04</td>
<td id="A10.T13.2.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.22</td>
</tr>
<tr id="A10.T13.2.10" class="ltx_tr">
<td id="A10.T13.2.10.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Kadapters (k=3)</td>
<td id="A10.T13.2.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">19.91</td>
<td id="A10.T13.2.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.10.3.1" class="ltx_text ltx_framed_underline">14.31</span></td>
<td id="A10.T13.2.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.55</td>
<td id="A10.T13.2.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">23.33</td>
<td id="A10.T13.2.10.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.10.6.1" class="ltx_text ltx_framed_underline">0.21</span></td>
</tr>
<tr id="A10.T13.2.11" class="ltx_tr">
<td id="A10.T13.2.11.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">T5-Modular</td>
<td id="A10.T13.2.11.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.11.2.1" class="ltx_text ltx_framed_underline">21.35</span></td>
<td id="A10.T13.2.11.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">12.78</td>
<td id="A10.T13.2.11.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">6.94</td>
<td id="A10.T13.2.11.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.11.5.1" class="ltx_text ltx_framed_underline">24.42</span></td>
<td id="A10.T13.2.11.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A10.T13.2.11.6.1" class="ltx_text ltx_font_bold">0.17</span></td>
</tr>
</tbody></table>
</figure>
<figure id="A10.F9" class="ltx_figure">
<div class="ltx_flex_figure">

<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<figure id="A10.F9.1" class="ltx_figure ltx_flex_size_1 ltx_align_center"><img src="/html/2110.03215/assets/x12.png" id="A10.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="51" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="A10.F8.sf1" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x13.png" id="A10.F8.sf1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="456" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span><span id="A10.F8.sf1.2.1" class="ltx_text ltx_font_smallcaps">InvariantLAMA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="A10.F8.sf2" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x14.png" id="A10.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="761" height="456" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span><span id="A10.F8.sf2.2.1" class="ltx_text ltx_font_smallcaps">UpdatedLAMA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="A10.F8.sf3" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x15.png" id="A10.F8.sf3.g1" class="ltx_graphics ltx_img_landscape" width="761" height="456" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span><span id="A10.F8.sf3.2.1" class="ltx_text ltx_font_smallcaps">NewLAMA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell 
                  ltx_flex_size_4">
<figure id="A10.F8.sf4" class="ltx_figure ltx_flex_size_4 ltx_align_center"><img src="/html/2110.03215/assets/x16.png" id="A10.F8.sf4.g1" class="ltx_graphics ltx_img_landscape" width="761" height="456" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span><span id="A10.F8.sf4.2.1" class="ltx_text ltx_font_smallcaps">NewLAMA-Easy</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Mean P@k curve for CKL benchmark with varying k.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.03214" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.03215" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2110.03215">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.03215" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.03217" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Dec 14 17:43:02 2022 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
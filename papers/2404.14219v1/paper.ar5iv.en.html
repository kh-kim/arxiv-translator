<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.14219] Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</title><meta property="og:description" content="We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mix…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.14219">

<!--Generated on Sun May  5 18:06:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Phi-3 Technical Report: 
<br class="ltx_break">A Highly Capable Language Model Locally on Your Phone</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Microsoft
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We introduce <span id="id1.id1.1" class="ltx_text ltx_font_bold">phi-3-mini</span>, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., <span id="id1.id1.2" class="ltx_text ltx_font_bold">phi-3-mini</span> achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for <span id="id1.id1.3" class="ltx_text ltx_font_bold">phi-2</span>, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format.
We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called <span id="id1.id1.4" class="ltx_text ltx_font_bold">phi-3-small</span> and <span id="id1.id1.5" class="ltx_text ltx_font_bold">phi-3-medium</span>, both significantly more capable than <span id="id1.id1.6" class="ltx_text ltx_font_bold">phi-3-mini</span> (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The striking progress of AI in the last few years can be largely attributed to major efforts throughout the world towards <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">scaling-up</span> to ever-larger models and datasets. Large Language Models (LLMs) have steadily increased in size from a mere billion parameters just five years ago (GPT-2 had 1.5 billion parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">RWC<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>19</a>]</cite>) to trillion parameters today. The impetus for this effort originates in the seemingly predictable improvement one obtains by training large models, the so-called <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">scaling laws</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">KMH<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>20</a>, <a href="#bib.bibx13" title="" class="ltx_ref">HBM<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>22</a>, <a href="#bib.bibx24" title="" class="ltx_ref">MRB<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>. However these laws assume a “fixed” data source. This assumption is now significantly disrupted by the existence of frontier LLMs themselves, which allow us to interact with data in novel ways. In our previous works on the phi models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">GZA<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>, <a href="#bib.bibx21" title="" class="ltx_ref">LBE<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>, <a href="#bib.bibx14" title="" class="ltx_ref">JBA<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite> it was shown that a combination of LLM-based filtering of web data, and LLM-created synthetic data, enable performance in smaller language models that were typically seen only in much larger models. For example our previous model trained on this data recipe, <span id="S1.p1.1.3" class="ltx_text ltx_font_bold">phi-2</span> (2.7B parameters), matched the performance of models <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S1.p1.1.m1.1a"><mn id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><cn type="integer" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">25</annotation></semantics></math> times larger trained on regular data. In this report we present a new model, <span id="S1.p1.1.4" class="ltx_text ltx_font_bold">phi-3-mini</span> (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets used in <span id="S1.p1.1.5" class="ltx_text ltx_font_bold">phi-2</span>. With its small size, <span id="S1.p1.1.6" class="ltx_text ltx_font_bold">phi-3-mini</span> can easily be inferenced locally on a modern phone (see Figure <a href="#S2.F1" title="Figure 1 ‣ Highly capable language model running locally on a cell-phone. ‣ 2 Technical Specifications ‣ Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), yet it achieves a quality that seems on-par with models such as Mixtral 8x7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">JSR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>24</a>]</cite> and GPT-3.5.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<svg id="S1.p2.pic1" class="ltx_picture" height="229.36" overflow="visible" version="1.1" width="600"><g transform="translate(0,229.36) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 223.45 C 0 226.72 2.64 229.36 5.91 229.36 L 594.09 229.36 C 597.36 229.36 600 226.72 600 223.45 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 223.45 C 1.97 225.63 3.73 227.39 5.91 227.39 L 594.09 227.39 C 596.27 227.39 598.03 225.63 598.03 223.45 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 5.91)"><foreignObject width="574.35" height="217.55" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S1.p2.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:415.1pt;">
<span id="S1.p2.pic1.1.1.1.1.1.1" class="ltx_p ltx_minipage ltx_align_top" style="width:429.3pt;"><span id="S1.p2.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">User:</span><span id="S1.p2.pic1.1.1.1.1.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;"> Explain why it is surprising that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.</span></span>
<span id="S1.p2.pic1.1.1.1.1.1.2" class="ltx_p ltx_minipage ltx_align_top" style="width:429.3pt;"><span id="S1.p2.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">phi-3-mini:</span><span id="S1.p2.pic1.1.1.1.1.1.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;"> It’s like fitting a supercomputer in a flip phone, but instead of breaking the phone, it just breaks the internet with its tiny, yet mighty, linguistic prowess!</span></span>
<span id="S1.p2.pic1.1.1.1.1.1.3" class="ltx_p ltx_minipage ltx_align_top" style="width:429.3pt;"><span id="S1.p2.pic1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">User:</span><span id="S1.p2.pic1.1.1.1.1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;"> Okay now more serious answer, and note that this was achieved solely by changing the training data.</span></span>
<span id="S1.p2.pic1.1.1.1.1.1.4" class="ltx_p ltx_minipage ltx_align_top" style="width:429.3pt;"><span id="S1.p2.pic1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">phi-3-mini:</span><span id="S1.p2.pic1.1.1.1.1.1.4.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;"> The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulously curating and optimizing the training dataset, researchers can significantly reduce the model’s size without compromising its performance. [...]</span></span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Technical Specifications</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.2" class="ltx_p">The <span id="S2.p1.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span> model is a transformer decoder architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">VSP<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>17</a>]</cite>, with default context length <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="4K" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mn id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><times id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">4</cn><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">4K</annotation></semantics></math>. We also introduce a long context version via LongRope <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">DZZ<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>24</a>]</cite> that extends the context length to <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="128K" display="inline"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mn id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">128</mn><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1" xref="S2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><times id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">128</cn><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">128K</annotation></semantics></math>, called <span id="S2.p1.2.2" class="ltx_text ltx_font_bold">phi-3-mini-128K</span>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.3" class="ltx_p">To best benefit the open source community, <span id="S2.p2.3.1" class="ltx_text ltx_font_bold">phi-3-mini</span> is built upon a similar block structure as Llama-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">TLI<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite> and uses the same tokenizer with vocabulary size of 32064<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We remove BoS tokens and add some additional tokens for chat template.</span></span></span>. This means that all packages developed for Llama-2 family of models can be directly adapted to <span id="S2.p2.3.2" class="ltx_text ltx_font_bold">phi-3-mini</span>. The model uses <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="3072" display="inline"><semantics id="S2.p2.1.m1.1a"><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">3072</mn><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><cn type="integer" id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">3072</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">3072</annotation></semantics></math> hidden dimension, <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S2.p2.2.m2.1a"><mn id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><cn type="integer" id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">32</annotation></semantics></math> heads and <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S2.p2.3.m3.1a"><mn id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><cn type="integer" id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">32</annotation></semantics></math> layers. We trained using bfloat16 for a total of 3.3T tokens. The model is already chat-finetuned, and the chat template is as follows:</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<svg id="S2.p3.pic1" class="ltx_picture" height="22.88" overflow="visible" version="1.1" width="600"><g transform="translate(0,22.88) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 16.98 C 0 20.24 2.64 22.88 5.91 22.88 L 594.09 22.88 C 597.36 22.88 600 20.24 600 16.98 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 16.98 C 1.97 19.15 3.73 20.91 5.91 20.91 L 594.09 20.91 C 596.27 20.91 598.03 19.15 598.03 16.98 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 5.91)"><foreignObject width="574.35" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:415.1pt;">
<span id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p"><span id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">&lt;|user|&gt;<math id="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\backslash</annotation></semantics></math>n
Question
&lt;|end|&gt;<math id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mo id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">\backslash</annotation></semantics></math>n
&lt;|assistant|&gt;</span></span>
</span></foreignObject></g></g></svg>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.5" class="ltx_p">The <span id="S2.p4.5.1" class="ltx_text ltx_font_bold">phi-3-small</span> model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 100352 and has default context length <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="8K" display="inline"><semantics id="S2.p4.1.m1.1a"><mrow id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><mn id="S2.p4.1.m1.1.1.2" xref="S2.p4.1.m1.1.1.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S2.p4.1.m1.1.1.1" xref="S2.p4.1.m1.1.1.1.cmml">​</mo><mi id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><times id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1.1"></times><cn type="integer" id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1.2">8</cn><ci id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">8K</annotation></semantics></math>. It follows the standard decoder architecture of a 7B model class, having <math id="S2.p4.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S2.p4.2.m2.1a"><mn id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><cn type="integer" id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">32</annotation></semantics></math> layers and a hidden size of <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="4096" display="inline"><semantics id="S2.p4.3.m3.1a"><mn id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><cn type="integer" id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">4096</annotation></semantics></math>. To minimize KV cache footprint, the model also leverages a grouped-query attention, with <math id="S2.p4.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S2.p4.4.m4.1a"><mn id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><cn type="integer" id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">4</annotation></semantics></math> queries sharing <math id="S2.p4.5.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S2.p4.5.m5.1a"><mn id="S2.p4.5.m5.1.1" xref="S2.p4.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.p4.5.m5.1b"><cn type="integer" id="S2.p4.5.m5.1.1.cmml" xref="S2.p4.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m5.1c">1</annotation></semantics></math> key. Moreover <span id="S2.p4.5.2" class="ltx_text ltx_font_bold">phi-3-small</span> uses alternative layers of dense attention and a novel blocksparse attention to further optimize on KV cache savings while maintaining long context retrieval performance. An additional 10% multilingual data was also used for this model.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Highly capable language model running locally on a cell-phone.</h3>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.2" class="ltx_p">Thanks to its small size, <span id="S2.SS0.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span> can be quantized to 4-bits so that it only occupies <math id="S2.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><approx id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">\approx</annotation></semantics></math> 1.8GB of memory. We tested the quantized model by deploying <span id="S2.SS0.SSS0.Px1.p1.2.2" class="ltx_text ltx_font_bold">phi-3-mini</span> on iPhone 14 with A16 Bionic chip running natively on-device and fully offline achieving more than <math id="S2.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.2.m2.1a"><mn id="S2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.2.m2.1b"><cn type="integer" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.2.m2.1c">12</annotation></semantics></math> tokens per second.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2404.14219/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="199" height="414" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2404.14219/assets/x2.png" id="S2.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="199" height="414" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2404.14219/assets/x3.png" id="S2.F1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="199" height="413" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.4.2" class="ltx_text" style="font-size:90%;">4-bit quantized <span id="S2.F1.4.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span> running natively on an iPhone with A16 Bionic chip, generating over 12 tokens per second.</span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Training Methodology.</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">We follow the sequence of works initiated in “Textbooks Are All You Need”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">GZA<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>, which utilize high quality training data to improve the performance of small language models and deviate from the standard <span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">scaling-laws</span>. In this work we show that such method allows to reach the level of highly capable models such as GPT-3.5 or Mixtral with only 3.8B total parameters (while Mixtral has 45B total parameters for example). Our training data of consists of heavily filtered web data (according to the “educational level”) from various open internet sources, as well as synthetic LLM-generated data. Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language understanding. Phase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data that teach the model logical reasoning and various niche skills.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Data Optimal Regime.</h3>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Unlike prior works that train language models in either “compute optimal regime” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">HBM<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>22</a>]</cite> or “over-train regime”, we mainly focus on the quality of data for a <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">given scale</span>.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Just like for “compute optimal regime”, we use the term “optimal” in an aspirational sense for “data optimal regime”. We are not implying that we actually found the provably “optimal” data mixture for a given scale.</span></span></span> We try to calibrate the training data to be closer to the “data optimal” regime for small models. In particular, we filter the web data to contain the correct level of “knowledge” and keep more web pages that could potentially improve the “reasoning ability” for the model. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for “reasoning” for the mini size models. We compare our approach with Llama-2 in Figure&nbsp;<a href="#S2.F2" title="Figure 2 ‣ Data Optimal Regime. ‣ 2 Technical Specifications ‣ Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2404.14219/assets/scaling.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Scaling law close to the “Data Optimal Regime” (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus Llama-2 family of models (7B, 13B, 34B, 70B) that were trained on the same fixed data. We plot the log of MMLU error versus the log of model size.</span></figcaption>
</figure>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">To test our data on larger size of models, we also trained <span id="S2.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">phi-3-medium</span>, a model with 14B parameters using the same tokenizer and architecture of <span id="S2.SS0.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_bold">phi-3-mini</span>, and trained on the same data for slightly more epochs (4.8T tokens total as for <span id="S2.SS0.SSS0.Px3.p2.1.3" class="ltx_text ltx_font_bold">phi-3-small</span>). The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in the “data optimal regime” for 14B parameters model. We are still actively investigating some of those benchmarks (including a regression on HumanEval), hence the numbers for <span id="S2.SS0.SSS0.Px3.p2.1.4" class="ltx_text ltx_font_bold">phi-3-medium</span> should be considered as a “preview”.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Post-training.</h3>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Post-training of <span id="S2.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_bold">phi-3-mini</span> went through two stages, including supervised finetuning (SFT) and direct preference optimization (DPO). SFT leverages highly curated high-quality data across diverse domains, e.g., math, coding, reasoning, conversation, model identity, and safety. The SFT data mix starts with using English-only examples. DPO data covers chat format data, reasoning, and responsible AI (RAI) efforts. We use DPO to steer the model away from unwanted behavior, by using those outputs as “rejected” responses. Besides improvement in math, coding, reasoning, robustness, and safety, post-training transforms a language model to an AI assistant that users can efficiently and safely interact with.</p>
</div>
<div id="S2.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p2.1" class="ltx_p">As part of the post-training process, we developed a long context version of <span id="S2.SS0.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_bold">phi-3-mini</span> with context length limit enlarged to 128K instead of 4K. Across the board, the 128K model quality is on par with the 4K length version, while being able to handle long context tasks. Long context extension has been done in two stages, including long context mid-training and long-short mixed post-training with both SFT and DPO.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Academic benchmarks</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">On the next page we report the results for <span id="S3.p1.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span> on standard open-source benchmarks measuring the model’s reasoning ability (both common sense reasoning and logical reasoning). We compare to phi-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">JBA<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>, Mistral-7b-v0.1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">JSM<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>, Mixtral-8x7b <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">JSR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>24</a>]</cite>, Gemma 7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">TMH<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>24</a>]</cite>, Llama-3-instruct-8b <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">AI23</a>]</cite>, and GPT-3.5. All the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation. As is now standard, we use few-shot prompts to evaluate the models, at temperature <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math>. The prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for the <span id="S3.p1.2.2" class="ltx_text ltx_font_bold">phi-3</span> models.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For example, we found that using ## before the Question can lead to a noticeable improvement to <span id="footnote3.1" class="ltx_text ltx_font_bold">phi-3-mini</span>’s results across many benchmarks, but we did not do such changes in the prompts.</span></span></span>
The number of <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">k</annotation></semantics></math>–shot examples is listed per-benchmark.
An example of a 2-shot prompt is described in Appendix <a href="#A1" title="Appendix A Example prompt for benchmarks ‣ Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S3.2.p2" class="ltx_para ltx_noindent ltx_align_center">
<p id="S3.2.p2.1" class="ltx_p"><span id="S3.2.p2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.2.p2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:745.5pt;height:929.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.2.p2.1.1.1.1" class="ltx_p"><span id="S3.2.p2.1.1.1.1.1" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.1" class="ltx_td ltx_border_rr"></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.2.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.2.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-3-mini</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.2.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.2.2.1.2.1.1" class="ltx_text" style="font-size:80%;">3.8b</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.2.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.3.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-3-small</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.3.2.1.2.1.1" class="ltx_text" style="font-size:80%;">7b (preview)</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.3.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.4.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.4.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-3-medium</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.4.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.4.2.1.2.1.1" class="ltx_text" style="font-size:80%;">14b (preview)</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.4.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.5.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.5.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-2</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.5.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.5.2.1.2.1.1" class="ltx_text" style="font-size:80%;">2.7b</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.5.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.6.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.6.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.6.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Mistral</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.6.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.6.2.1.2.1.1" class="ltx_text" style="font-size:80%;">7b</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.6.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.7.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.7.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.7.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Gemma</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.7.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.7.2.1.2.1.1" class="ltx_text" style="font-size:80%;">7b</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.7.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.8.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.8.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.8.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Llama-3-In</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.8.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.8.2.1.2.1.1" class="ltx_text" style="font-size:80%;">8b</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.8.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.9.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.9.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.9.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Mixtral</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.9.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.9.2.1.2.1.1" class="ltx_text" style="font-size:80%;">8x7b</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.9.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.10.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.1.10.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.1.10.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.1.10.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.10.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">GPT-3.5</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.1.10.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.1.10.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.1.10.2.1.2.1.1" class="ltx_text" style="font-size:80%;">version 1106</span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.1.10.3" class="ltx_text"></span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.2.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.2.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">MMLU</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.2.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(5-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">HBK<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>21</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.2.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">68.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">75.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">78.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">56.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">61.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">63.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t">66.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t">68.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t">71.4</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.3" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.3.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.3.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">HellaSwag</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.3.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(5-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">ZHB<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>19</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.3.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.3.2" class="ltx_td ltx_align_center">76.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.3" class="ltx_td ltx_align_center">78.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.4" class="ltx_td ltx_align_center">83.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.5" class="ltx_td ltx_align_center">53.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.6" class="ltx_td ltx_align_center">58.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.7" class="ltx_td ltx_align_center">49.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.8" class="ltx_td ltx_align_center">69.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.9" class="ltx_td ltx_align_center">70.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.3.10" class="ltx_td ltx_align_center">78.8</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.4" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.4.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.4.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">ANLI</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.4.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(7-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">NWD<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>20</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.4.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.4.2" class="ltx_td ltx_align_center">52.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.3" class="ltx_td ltx_align_center">55.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.4" class="ltx_td ltx_align_center">58.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.5" class="ltx_td ltx_align_center">42.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.6" class="ltx_td ltx_align_center">47.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.7" class="ltx_td ltx_align_center">48.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.8" class="ltx_td ltx_align_center">54.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.9" class="ltx_td ltx_align_center">55.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.4.10" class="ltx_td ltx_align_center">58.1</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.5" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.5.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.5.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">GSM-8K</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.5.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(0-Shot; CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">CKB<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>21</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.5.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t">82.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">88.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">90.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">61.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">46.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t">59.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t">77.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.9" class="ltx_td ltx_align_center ltx_border_t">64.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.5.10" class="ltx_td ltx_align_center ltx_border_t">78.1</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.6" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.6.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.6.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">MedQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.6.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(2-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">JPO<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>20</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.6.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_t">53.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">58.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">69.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t">40.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t">49.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t">50.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.8" class="ltx_td ltx_align_center ltx_border_t">58.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.9" class="ltx_td ltx_align_center ltx_border_t">62.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.6.10" class="ltx_td ltx_align_center ltx_border_t">63.4</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.7" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.7.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.7.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">AGIEval</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.7.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(0-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">ZCG<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.7.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.7.2" class="ltx_td ltx_align_center">37.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.3" class="ltx_td ltx_align_center">45.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.4" class="ltx_td ltx_align_center">48.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.5" class="ltx_td ltx_align_center">29.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.6" class="ltx_td ltx_align_center">35.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.7" class="ltx_td ltx_align_center">42.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.8" class="ltx_td ltx_align_center">42.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.9" class="ltx_td ltx_align_center">45.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.7.10" class="ltx_td ltx_align_center">48.4</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.8" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.8.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.8.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">TriviaQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.8.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(5-Shot)  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">JCWZ17</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.8.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.8.2" class="ltx_td ltx_align_center">64.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.3" class="ltx_td ltx_align_center">59.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.4" class="ltx_td ltx_align_center">75.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.5" class="ltx_td ltx_align_center">45.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.6" class="ltx_td ltx_align_center">72.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.7" class="ltx_td ltx_align_center">75.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.8" class="ltx_td ltx_align_center">73.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.9" class="ltx_td ltx_align_center">82.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.8.10" class="ltx_td ltx_align_center">85.8</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.9" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.9.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.9.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.9.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">Arc-C</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.9.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(10-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">CCE<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>18</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.9.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t">84.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">90.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">91.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t">75.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">78.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t">78.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.8" class="ltx_td ltx_align_center ltx_border_t">80.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.9" class="ltx_td ltx_align_center ltx_border_t">87.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.9.10" class="ltx_td ltx_align_center ltx_border_t">87.4</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.10" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.10.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.10.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.10.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">Arc-E</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.10.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(10-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">CCE<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>18</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.10.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.10.2" class="ltx_td ltx_align_center">94.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.3" class="ltx_td ltx_align_center">97.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.4" class="ltx_td ltx_align_center">97.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.5" class="ltx_td ltx_align_center">88.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.6" class="ltx_td ltx_align_center">90.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.7" class="ltx_td ltx_align_center">91.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.8" class="ltx_td ltx_align_center">92.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.9" class="ltx_td ltx_align_center">95.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.10.10" class="ltx_td ltx_align_center">96.3</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.11" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.11.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.11.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.11.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">PIQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.11.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(5-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">BZGC19</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.11.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.11.2" class="ltx_td ltx_align_center">84.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.3" class="ltx_td ltx_align_center">87.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.4" class="ltx_td ltx_align_center">87.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.5" class="ltx_td ltx_align_center">60.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.6" class="ltx_td ltx_align_center">77.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.7" class="ltx_td ltx_align_center">78.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.8" class="ltx_td ltx_align_center">77.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.9" class="ltx_td ltx_align_center">86.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.11.10" class="ltx_td ltx_align_center">86.6</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.12" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.12.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.12.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.12.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">SociQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.12.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(5-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">BZGC19</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.12.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.12.2" class="ltx_td ltx_align_center">76.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.3" class="ltx_td ltx_align_center">79.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.4" class="ltx_td ltx_align_center">80.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.5" class="ltx_td ltx_align_center">68.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.6" class="ltx_td ltx_align_center">74.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.7" class="ltx_td ltx_align_center">65.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.8" class="ltx_td ltx_align_center">73.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.9" class="ltx_td ltx_align_center">75.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.12.10" class="ltx_td ltx_align_center">68.3</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.13" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.13.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.13.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.13.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">BigBench-Hard</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.13.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(0-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">SRR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>22</a>, <a href="#bib.bibx30" title="" class="ltx_ref">SSS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>22</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.13.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.13.2" class="ltx_td ltx_align_center ltx_border_t">71.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.3" class="ltx_td ltx_align_center ltx_border_t">75.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.4" class="ltx_td ltx_align_center ltx_border_t">81.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.5" class="ltx_td ltx_align_center ltx_border_t">59.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.6" class="ltx_td ltx_align_center ltx_border_t">57.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.7" class="ltx_td ltx_align_center ltx_border_t">59.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.8" class="ltx_td ltx_align_center ltx_border_t">68.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.9" class="ltx_td ltx_align_center ltx_border_t">69.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.13.10" class="ltx_td ltx_align_center ltx_border_t">68.32</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.14" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.14.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.14.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.14.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">WinoGrande</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.14.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(5-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">SLBBC19</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.14.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.14.2" class="ltx_td ltx_align_center">70.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.3" class="ltx_td ltx_align_center">82.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.4" class="ltx_td ltx_align_center">81.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.5" class="ltx_td ltx_align_center">54.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.6" class="ltx_td ltx_align_center">54.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.7" class="ltx_td ltx_align_center">55.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.8" class="ltx_td ltx_align_center">58.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.9" class="ltx_td ltx_align_center">62.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.14.10" class="ltx_td ltx_align_center">68.8</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.15" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.15.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.15.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.15.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">OpenBookQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.15.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(10-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">MCKS18</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.15.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.15.2" class="ltx_td ltx_align_center">83.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.3" class="ltx_td ltx_align_center">88.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.4" class="ltx_td ltx_align_center">87.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.5" class="ltx_td ltx_align_center">73.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.6" class="ltx_td ltx_align_center">79.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.7" class="ltx_td ltx_align_center">78.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.8" class="ltx_td ltx_align_center">81.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.9" class="ltx_td ltx_align_center">85.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.15.10" class="ltx_td ltx_align_center">86.0</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.16" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.16.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.16.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.16.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">BoolQ</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.16.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(0-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">CLC<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>19</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.16.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.16.2" class="ltx_td ltx_align_center">77.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.3" class="ltx_td ltx_align_center">82.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.4" class="ltx_td ltx_align_center">86.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.5" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.6" class="ltx_td ltx_align_center">72.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.7" class="ltx_td ltx_align_center">66.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.8" class="ltx_td ltx_align_center">78.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.9" class="ltx_td ltx_align_center">77.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.16.10" class="ltx_td ltx_align_center">79.1</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.17" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.17.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.17.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.17.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">CommonSenseQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.17.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(10-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">THLB19</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.17.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.17.2" class="ltx_td ltx_align_center">80.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.3" class="ltx_td ltx_align_center">80.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.4" class="ltx_td ltx_align_center">82.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.5" class="ltx_td ltx_align_center">69.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.6" class="ltx_td ltx_align_center">72.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.7" class="ltx_td ltx_align_center">76.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.8" class="ltx_td ltx_align_center">73.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.9" class="ltx_td ltx_align_center">78.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.17.10" class="ltx_td ltx_align_center">79.6</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.18" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.18.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.18.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.18.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">TruthfulQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.18.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(10-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">LHE22</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.18.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.18.2" class="ltx_td ltx_align_center">65.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.3" class="ltx_td ltx_align_center">68.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.4" class="ltx_td ltx_align_center">75.7</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.5" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.6" class="ltx_td ltx_align_center">52.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.7" class="ltx_td ltx_align_center">53.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.8" class="ltx_td ltx_align_center">62.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.9" class="ltx_td ltx_align_center">60.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.18.10" class="ltx_td ltx_align_center">85.8</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.19" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.19.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.19.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.19.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">HumanEval</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.19.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(0-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">CTJ<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>21</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.19.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.19.2" class="ltx_td ltx_align_center ltx_border_t">59.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.3" class="ltx_td ltx_align_center ltx_border_t">59.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.4" class="ltx_td ltx_align_center ltx_border_t">55.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.5" class="ltx_td ltx_align_center ltx_border_t">47.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.6" class="ltx_td ltx_align_center ltx_border_t">28.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.7" class="ltx_td ltx_align_center ltx_border_t">34.1</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.8" class="ltx_td ltx_align_center ltx_border_t">60.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.9" class="ltx_td ltx_align_center ltx_border_t">37.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.19.10" class="ltx_td ltx_align_center ltx_border_t">62.2</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.20" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.20.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.20.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.20.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">MBPP</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.20.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(3-Shot) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">AON<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>21</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.20.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.20.2" class="ltx_td ltx_align_center">70.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.3" class="ltx_td ltx_align_center">71.4</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.4" class="ltx_td ltx_align_center">74.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.5" class="ltx_td ltx_align_center">60.6</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.6" class="ltx_td ltx_align_center">50.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.7" class="ltx_td ltx_align_center">51.5</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.8" class="ltx_td ltx_align_center">65.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.9" class="ltx_td ltx_align_center">60.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.20.10" class="ltx_td ltx_align_center">77.8</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.21" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.21.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Average</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.2" class="ltx_td ltx_align_center ltx_border_t">71.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.3" class="ltx_td ltx_align_center ltx_border_t">74.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.4" class="ltx_td ltx_align_center ltx_border_t">78.2</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.5" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.6" class="ltx_td ltx_align_center ltx_border_t">61.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.7" class="ltx_td ltx_align_center ltx_border_t">62.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.8" class="ltx_td ltx_align_center ltx_border_t">68.0</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.9" class="ltx_td ltx_align_center ltx_border_t">69.9</span>
<span id="S3.2.p2.1.1.1.1.1.1.21.10" class="ltx_td ltx_align_center ltx_border_t">75.3</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.22" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.22.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.2.p2.1.1.1.1.1.1.22.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.22.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">GPQA</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.22.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(2-Shot; CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">RHS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.22.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.22.2" class="ltx_td ltx_align_center ltx_border_t">32.8</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.3" class="ltx_td ltx_align_center ltx_border_t">34.3</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.4" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.5" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.6" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.7" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.8" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.9" class="ltx_td ltx_align_center ltx_border_t">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.22.10" class="ltx_td ltx_align_center ltx_border_t">29.0</span></span>
<span id="S3.2.p2.1.1.1.1.1.1.23" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.23.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.2.p2.1.1.1.1.1.1.23.1.1" class="ltx_text"></span> <span id="S3.2.p2.1.1.1.1.1.1.23.1.2" class="ltx_text">
<span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1.1" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1.1.1.1" class="ltx_text" style="font-size:120%;">MT Bench</span></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1.2" class="ltx_tr">
<span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.2.p2.1.1.1.1.1.1.23.1.2.1.2.1.1" class="ltx_text" style="font-size:50%;">(2 round ave.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">ZCS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite></span></span></span>
</span></span><span id="S3.2.p2.1.1.1.1.1.1.23.1.3" class="ltx_text"></span></span>
<span id="S3.2.p2.1.1.1.1.1.1.23.2" class="ltx_td ltx_align_center">8.38</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.3" class="ltx_td ltx_align_center">8.70</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.4" class="ltx_td ltx_align_center">8.91</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.5" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.6" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.7" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.8" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.9" class="ltx_td ltx_align_center">–</span>
<span id="S3.2.p2.1.1.1.1.1.1.23.10" class="ltx_td ltx_align_center">8.35</span></span>
</span></span></span>
</span></span></span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Safety</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Phi-3-mini</span> was developed in accordance with Microsoft’s responsible AI principles. The overall approach consisted of safety alignment in post-training, red-teaming, automated testing and evaluations across dozens of RAI harm categories. Helpfulness and harmlessness preference datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">BJN<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>22</a>, <a href="#bib.bibx16" title="" class="ltx_ref">JLD<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite> with modifications inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">BSA<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>24</a>]</cite> and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training. An independent red team at Microsoft iteratively examined <span id="S4.p1.1.2" class="ltx_text ltx_font_bold">phi-3-mini</span> to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset. This process resulted in significant decrease of harmful response rates, as shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Safety ‣ Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2404.14219/assets/with_without_safety_training.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.5.2" class="ltx_text" style="font-size:90%;">Comparison of harmful response percentages by Microsoft AI Red Team between <span id="S4.F3.5.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span> before and after the safety alignment. Note that the harmful response percentages in this chart are inflated numbers as the red team tried to induce <span id="S4.F3.5.2.2" class="ltx_text ltx_font_bold">phi-3-mini</span> in an adversarial way to generate harmful responses through multi-turn conversations.</span></figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4 Safety ‣ Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of in-house RAI benchmarks for <span id="S4.p2.2.1" class="ltx_text ltx_font_bold">phi-3-mini-4k</span> and <span id="S4.p2.2.2" class="ltx_text ltx_font_bold">phi-3-mini-128k</span> compared to phi-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">JBA<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>, Mistral-7b-v0.1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">JSM<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>, Gemma 7b <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">TMH<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>24</a>]</cite>, and Llama-3-instruct-8b <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">AI23</a>]</cite>. This benchmark utilized GPT-4 to simulate multi-turn conversations in five different categories and to evaluate the model responses. Ungroundedness between 0 (fully grounded) and 4 (not grounded) measures if the information in a response is based on a given prompt. In other categories, responses were evaluated in terms of the severity of harmfulness from 0 (no harm) to 7 (extreme harm) and the defect rates (DR-<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">x</annotation></semantics></math>) were computed as the percentage of samples with the severity score being greater than or equal to <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">x</annotation></semantics></math>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.2" class="ltx_logical-block ltx_pruned_first">
<div id="S4.T1.2.p2" class="ltx_para ltx_noindent ltx_align_center">
<p id="S4.T1.2.p2.1" class="ltx_p"><span id="S4.T1.2.p2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T1.2.p2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:610.2pt;height:132.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T1.2.p2.1.1.1.1" class="ltx_p"><span id="S4.T1.2.p2.1.1.1.1.1" class="ltx_text">

<span id="S4.T1.2.p2.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.1" class="ltx_td ltx_border_rr"></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.1" class="ltx_text"></span> <span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2" class="ltx_text">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-3-Mini-4k</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.2.1.2.1.1" class="ltx_text" style="font-size:80%;">3.8b</span></span></span>
</span></span><span id="S4.T1.2.p2.1.1.1.1.1.1.1.2.3" class="ltx_text"></span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2" class="ltx_text">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-3-Mini-128k</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.2.1.2.1.1" class="ltx_text" style="font-size:80%;">3.8b</span></span></span>
</span></span><span id="S4.T1.2.p2.1.1.1.1.1.1.1.3.3" class="ltx_text"></span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.1" class="ltx_text"></span> <span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2" class="ltx_text">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Phi-2</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.2.1.2.1.1" class="ltx_text" style="font-size:80%;">2.7b</span></span></span>
</span></span><span id="S4.T1.2.p2.1.1.1.1.1.1.1.4.3" class="ltx_text"></span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.1" class="ltx_text"></span> <span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2" class="ltx_text">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Mistral</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.2.1.2.1.1" class="ltx_text" style="font-size:80%;">7b</span></span></span>
</span></span><span id="S4.T1.2.p2.1.1.1.1.1.1.1.5.3" class="ltx_text"></span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.1" class="ltx_text"></span> <span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2" class="ltx_text">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Gemma</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.2.1.2.1.1" class="ltx_text" style="font-size:80%;">7b</span></span></span>
</span></span><span id="S4.T1.2.p2.1.1.1.1.1.1.1.6.3" class="ltx_text"></span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.1" class="ltx_text"></span> <span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2" class="ltx_text">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2.1.1" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Llama-3-In</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.2.1.2.1.1" class="ltx_text" style="font-size:80%;">8b</span></span></span>
</span></span><span id="S4.T1.2.p2.1.1.1.1.1.1.1.7.3" class="ltx_text"></span></span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Ungroundedness</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.603</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.637</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1.481</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.935</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">0.679</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">0.328</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_rr">Intellectual Property (DR-1)</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.2" class="ltx_td ltx_align_center">23.95%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.3" class="ltx_td ltx_align_center">21.50%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.4" class="ltx_td ltx_align_center">24.00%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.5" class="ltx_td ltx_align_center">56.20%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.6" class="ltx_td ltx_align_center">38.33%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.3.7" class="ltx_td ltx_align_center">37.30%</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_rr">Harmful Content Continuation (DR-3)</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.2" class="ltx_td ltx_align_center">0.75%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.3" class="ltx_td ltx_align_center">1.08%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.4" class="ltx_td ltx_align_center">2.93%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.5" class="ltx_td ltx_align_center">2.58%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.6" class="ltx_td ltx_align_center">1.28%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.4.7" class="ltx_td ltx_align_center">1.30%</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_rr">Harmful Content Summarization (DR-3)</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.2" class="ltx_td ltx_align_center">10.00%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.3" class="ltx_td ltx_align_center">10.20%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.4" class="ltx_td ltx_align_center">14.35%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.5" class="ltx_td ltx_align_center">22.33%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.6" class="ltx_td ltx_align_center">10.33%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.5.7" class="ltx_td ltx_align_center">8.20%</span></span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6" class="ltx_tr">
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_rr">Jailbreak (DR-1)</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.2" class="ltx_td ltx_align_center">12.29%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.3" class="ltx_td ltx_align_center">12.57%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.4" class="ltx_td ltx_align_center">15.00%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.5" class="ltx_td ltx_align_center">15.57%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.6" class="ltx_td ltx_align_center">11.43%</span>
<span id="S4.T1.2.p2.1.1.1.1.1.1.6.7" class="ltx_td ltx_align_center">13.00%</span></span>
</span></span></span>
</span></span></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.6.2" class="ltx_text" style="font-size:90%;">Comparison of Microsoft internal multi-turn conversation RAI benchmark results of <span id="S4.T1.6.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span> and other models. Note that a lower value indicates a better performance for all metrics in the table.</span></figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Weakness</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In terms of LLM capabilities, while <span id="S5.p1.1.1" class="ltx_text ltx_markedasmath ltx_font_bold">phi-3-mini</span> model achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much “factual knowledge”, which can be seen for example with low performance on TriviaQA.
However, we believe such weakness can be resolved by augmentation with a search engine. We show an example using the HuggingFace default Chat-UI with <span id="S5.p1.1.2" class="ltx_text ltx_font_bold">phi-3-mini</span> in Figure <a href="#S5.F4" title="Figure 4 ‣ 5 Weakness ‣ Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Another weakness related to model’s capacity is that we mostly restricted the language to English. Exploring multilingual capabilities for Small Language Models is an important next step, with some initial promising results on <span id="S5.p1.1.3" class="ltx_text ltx_font_bold">phi-3-small</span> by including more multilingual data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Despite our diligent RAI efforts, as with most LLMs, there remains challenges around factual inaccuracies (or hallucinations), reproduction or amplification of biases, inappropriate content generation, and safety issues. The use of carefully curated training data, and targeted post-training, and improvements from red-teaming insights significantly mitigates these issues across all dimensions. However, there is significant work ahead to fully address these challenges.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2404.14219/assets/without_search.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="255" height="405" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2404.14219/assets/with_search.png" id="S5.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="335" height="404" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.5.2" class="ltx_text" style="font-size:90%;">Left: <span id="S5.F4.5.2.1" class="ltx_text ltx_font_bold">phi-3-mini</span>’s completion without search. Right: <span id="S5.F4.5.2.2" class="ltx_text ltx_font_bold">phi-3-mini</span>’s completion with search, using the default HuggingFace Chat-UI search ability.</span></figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[AI23]</span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Introducing meta llama 3: The most capable openly available llm to
date, 2023.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[AON<sup id="bib.bibx2.4.4.1" class="ltx_sup"><span id="bib.bibx2.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>21]</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles
Sutton.

</span>
<span class="ltx_bibblock">Program synthesis with large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2108.07732</span>, 2021.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[BJN<sup id="bib.bibx3.4.4.1" class="ltx_sup"><span id="bib.bibx3.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>22]</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,
Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning
from human feedback, 2022.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[BSA<sup id="bib.bibx4.4.4.1" class="ltx_sup"><span id="bib.bibx4.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>24]</span>
<span class="ltx_bibblock">
Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan
Jurafsky, Tatsunori Hashimoto, and James Zou.

</span>
<span class="ltx_bibblock">Safety-tuned llamas: Lessons from improving the safety of large
language models that follow instructions, 2024.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[BZGC19]</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock"><span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.11641</span>, 2019.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CCE<sup id="bib.bibx6.4.4.1" class="ltx_sup"><span id="bib.bibx6.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>18]</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge, 2018.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CKB<sup id="bib.bibx7.4.4.1" class="ltx_sup"><span id="bib.bibx7.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>21]</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2110.14168</span>, 2021.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CLC<sup id="bib.bibx8.4.4.1" class="ltx_sup"><span id="bib.bibx8.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>19]</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.7.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, pages 2924–2936, 2019.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CTJ<sup id="bib.bibx9.4.4.1" class="ltx_sup"><span id="bib.bibx9.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>21]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Ponde
de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,
William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
Hesse, Andrew&nbsp;N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code, 2021.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[DZZ<sup id="bib.bibx10.4.4.1" class="ltx_sup"><span id="bib.bibx10.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>24]</span>
<span class="ltx_bibblock">
Yiran Ding, Li&nbsp;Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang
Xu, Fan Yang, and Mao Yang.

</span>
<span class="ltx_bibblock">Longrope: Extending llm context window beyond 2 million tokens, 2024.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[GZA<sup id="bib.bibx11.4.4.1" class="ltx_sup"><span id="bib.bibx11.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio César&nbsp;Teodoro Mendes, Allie&nbsp;Del
Giorno, Sivakanth Gopi, Mojan Javaheripi, Gustavo de&nbsp;Rosa Piero&nbsp;Kauffmann,
Olli Saarikivia, Adil Salim, Shital Shah, Harkirat&nbsp;Singh Behl, Xin Wang,
Sébastien Bubeck, Ronen Eldan, Adam&nbsp;Tauman Kalai, Yin&nbsp;Tat Lee, and Yuanzhi
Li.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bibx11.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.11644</span>, 2023.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[HBK<sup id="bib.bibx12.4.4.1" class="ltx_sup"><span id="bib.bibx12.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>21]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
Tang, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the MATH dataset, 2021.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[HBM<sup id="bib.bibx13.4.4.1" class="ltx_sup"><span id="bib.bibx13.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>22]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Eliza&nbsp;Rutherford Trevor&nbsp;Cai, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks,
Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican,
George van&nbsp;den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen
Simonyan, Erich Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and Laurent Sifre.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx13.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.15556</span>, 2022.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JBA<sup id="bib.bibx14.4.4.1" class="ltx_sup"><span id="bib.bibx14.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Caio César
Teodoro&nbsp;Mendes, Weizhu Chen, Allie Del&nbsp;Giorno, Ronen Eldan, Sivakanth Gopi,
Suriya Gunasekar, Piero Kauffmann, Yin&nbsp;Tat Lee, Yuanzhi Li, Anh Nguyen,
Gustavo&nbsp;de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce,
Harkirat Singh&nbsp;Behl, Adam Taumann&nbsp;Kalai, Xin Wang, Rachel Ward, Philipp
Witte, Cyril Zhang, and Yi&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Phi-2: The surprising power of small language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx14.7.1" class="ltx_text ltx_font_italic">Microsoft Research Blog</span>, 2023.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JCWZ17]</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel&nbsp;S. Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension, 2017.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JLD<sup id="bib.bibx16.4.4.1" class="ltx_sup"><span id="bib.bibx16.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce&nbsp;Bian, Chi Zhang,
Ruiyang Sun, Yizhou Wang, and Yaodong Yang.

</span>
<span class="ltx_bibblock">Beavertails: Towards improved safety alignment of llm via a
human-preference dataset, 2023.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JPO<sup id="bib.bibx17.4.4.1" class="ltx_sup"><span id="bib.bibx17.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>20]</span>
<span class="ltx_bibblock">
Di&nbsp;Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
Szolovits.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain
question answering dataset from medical exams, 2020.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JSM<sup id="bib.bibx18.4.4.1" class="ltx_sup"><span id="bib.bibx18.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Albert&nbsp;Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, Lélio&nbsp;Renard Lavaud, Marie-Anne Lachaux,
Pierre Stock, Teven&nbsp;Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William&nbsp;El Sayed.

</span>
<span class="ltx_bibblock">Mistral 7b, 2023.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JSR<sup id="bib.bibx19.4.4.1" class="ltx_sup"><span id="bib.bibx19.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>24]</span>
<span class="ltx_bibblock">
Albert&nbsp;Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou
Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
Lélio&nbsp;Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock,
Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven&nbsp;Le Scao, Théophile
Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William&nbsp;El Sayed.

</span>
<span class="ltx_bibblock">Mixtral of experts, 2024.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KMH<sup id="bib.bibx20.4.4.1" class="ltx_sup"><span id="bib.bibx20.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>20]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx20.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.08361</span>, 2020.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LBE<sup id="bib.bibx21.4.4.1" class="ltx_sup"><span id="bib.bibx21.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya
Gunasekar, and Yin&nbsp;Tat Lee.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><span id="bib.bibx21.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.05463</span>, 2023.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LHE22]</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods, 2022.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[MCKS18]</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering, 2018.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[MRB<sup id="bib.bibx24.4.4.1" class="ltx_sup"><span id="bib.bibx24.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Alexander&nbsp;M Rush, Boaz Barak, Teven&nbsp;Le Scao, Aleksandra
Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx24.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.16264</span>, 2023.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[NWD<sup id="bib.bibx25.4.4.1" class="ltx_sup"><span id="bib.bibx25.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>20]</span>
<span class="ltx_bibblock">
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
Kiela.

</span>
<span class="ltx_bibblock">Adversarial nli: A new benchmark for natural language understanding,
2020.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RHS<sup id="bib.bibx26.4.4.1" class="ltx_sup"><span id="bib.bibx26.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
David Rein, Betty&nbsp;Li Hou, Asa&nbsp;Cooper Stickland, Jackson Petty, Richard&nbsp;Yuanzhe
Pang, Julien Dirani, Julian Michael, and Samuel&nbsp;R. Bowman.

</span>
<span class="ltx_bibblock">Gpqa: A graduate-level google-proof q&amp;a benchmark, 2023.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RWC<sup id="bib.bibx27.4.4.1" class="ltx_sup"><span id="bib.bibx27.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>19]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bibx27.7.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SLBBC19]</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan Le&nbsp;Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><span id="bib.bibx28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.10641</span>, 2019.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SRR<sup id="bib.bibx29.4.4.1" class="ltx_sup"><span id="bib.bibx29.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>22]</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal&nbsp;Md Shoeb, Abubakar
Abid, Adam Fisch, Adam&nbsp;R Brown, Adam Santoro, Aditya Gupta, Adrià
Garriga-Alonso, et&nbsp;al.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the
capabilities of language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx29.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.04615</span>, 2022.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SSS<sup id="bib.bibx30.4.4.1" class="ltx_sup"><span id="bib.bibx30.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>22]</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay,
Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V. Le, Ed&nbsp;H. Chi, Denny Zhou, and
Jason Wei.

</span>
<span class="ltx_bibblock">Challenging big-bench tasks and whether chain-of-thought can solve
them, 2022.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[THLB19]</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.

</span>
<span class="ltx_bibblock">Commonsenseqa: A question answering challenge targeting commonsense
knowledge, 2019.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[TLI<sup id="bib.bibx32.4.4.1" class="ltx_sup"><span id="bib.bibx32.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx32.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[TMH<sup id="bib.bibx33.4.4.1" class="ltx_sup"><span id="bib.bibx33.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>24]</span>
<span class="ltx_bibblock">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir&nbsp;Sanjay Kale, Juliette
Love, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology, 2024.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[VSP<sup id="bib.bibx34.4.4.1" class="ltx_sup"><span id="bib.bibx34.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>17]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Ł&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx34.7.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
volume&nbsp;30, 2017.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZCG<sup id="bib.bibx35.4.4.1" class="ltx_sup"><span id="bib.bibx35.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
Saied, Weizhu Chen, and Nan Duan.

</span>
<span class="ltx_bibblock">Agieval: A human-centric benchmark for evaluating foundation models,
2023.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZCS<sup id="bib.bibx36.4.4.1" class="ltx_sup"><span id="bib.bibx36.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric Xing, et&nbsp;al.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><span id="bib.bibx36.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.05685</span>, 2023.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZHB<sup id="bib.bibx37.4.4.1" class="ltx_sup"><span id="bib.bibx37.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>19]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <span id="bib.bibx37.7.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span>, pages 4791–4800, 2019.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Example prompt for benchmarks</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<svg id="A1.p1.pic1" class="ltx_picture" height="69.19" overflow="visible" version="1.1" width="600"><g transform="translate(0,69.19) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 63.28 C 0 66.54 2.64 69.19 5.91 69.19 L 594.09 69.19 C 597.36 69.19 600 66.54 600 63.28 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 63.28 C 1.97 65.46 3.73 67.22 5.91 67.22 L 594.09 67.22 C 596.27 67.22 598.03 65.46 598.03 63.28 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 5.91)"><foreignObject width="574.35" height="57.38" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:415.1pt;">
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.7" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.7.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Question:</span></span>
<span id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p"><span id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Solve for <math id="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">x</annotation></semantics></math>: <math id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2" class="ltx_Math" alttext="(-\frac{1}{3})(-4-3x)=\frac{1}{2}" display="inline"><semantics id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2a"><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.cmml"><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.cmml"><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.cmml"><mo id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1a" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.cmml">−</mo><mfrac id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.cmml"><mn id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.2.cmml">1</mn><mn id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.3.cmml">3</mn></mfrac></mrow><mo stretchy="false" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.3.cmml">​</mo><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.cmml">(</mo><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.cmml"><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.cmml"><mo id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2a" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.cmml">−</mo><mn id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.2.cmml">4</mn></mrow><mo id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.1" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.1.cmml">−</mo><mrow id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.cmml"><mn id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.1" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.3.cmml">x</mi></mrow></mrow><mo stretchy="false" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.3.cmml">=</mo><mfrac id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.cmml"><mn id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.2" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.2.cmml">1</mn><mn id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.3" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.3.cmml">2</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2b"><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2"><eq id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.3.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.3"></eq><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2"><times id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.3.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.3"></times><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1"><minus id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1"></minus><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2"><divide id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2"></divide><cn type="integer" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.2">1</cn><cn type="integer" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.3.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.1.1.1.2.3">3</cn></apply></apply><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1"><minus id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.1"></minus><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2"><minus id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2"></minus><cn type="integer" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.2.2">4</cn></apply><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3"><times id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.1"></times><cn type="integer" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.2">3</cn><ci id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.3.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.2.2.1.1.3.3">𝑥</ci></apply></apply></apply><apply id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4"><divide id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.1.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4"></divide><cn type="integer" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.2.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.2">1</cn><cn type="integer" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.3.cmml" xref="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2.2.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.2c">(-\frac{1}{3})(-4-3x)=\frac{1}{2}</annotation></semantics></math></span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.8" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.8.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Options:</span></span>
<span id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3" class="ltx_p"><span id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A. <math id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="-\frac{5}{6}" display="inline"><semantics id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1a"><mrow id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.cmml"><mo id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1a" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.cmml">−</mo><mfrac id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.cmml"><mn id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.2" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.2.cmml">5</mn><mn id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.3" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.3.cmml">6</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1b"><apply id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1"><minus id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.1.cmml" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1"></minus><apply id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.cmml" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2"><divide id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.1.cmml" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2"></divide><cn type="integer" id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.2.cmml" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.2">5</cn><cn type="integer" id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.3.cmml" xref="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1.1.2.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.m1.1c">-\frac{5}{6}</annotation></semantics></math></span></span>
<span id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4" class="ltx_p"><span id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">B. <math id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\frac{7}{6}" display="inline"><semantics id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1a"><mfrac id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.cmml"><mn id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.2" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.2.cmml">7</mn><mn id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.3" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.3.cmml">6</mn></mfrac><annotation-xml encoding="MathML-Content" id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1b"><apply id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.cmml" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1"><divide id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.1.cmml" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1"></divide><cn type="integer" id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.2.cmml" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.2">7</cn><cn type="integer" id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.3.cmml" xref="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.m1.1c">\frac{7}{6}</annotation></semantics></math></span></span>
<span id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5" class="ltx_p"><span id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">C. <math id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\frac{5}{3}" display="inline"><semantics id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1a"><mfrac id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.cmml"><mn id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.2" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.2.cmml">5</mn><mn id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.3" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.3.cmml">3</mn></mfrac><annotation-xml encoding="MathML-Content" id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1b"><apply id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.cmml" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1"><divide id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.1.cmml" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1"></divide><cn type="integer" id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.2.cmml" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.2">5</cn><cn type="integer" id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.3.cmml" xref="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.m1.1c">\frac{5}{3}</annotation></semantics></math></span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">D. <math id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\frac{1}{6}" display="inline"><semantics id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1a"><mfrac id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.cmml"><mn id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.2" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.2.cmml">1</mn><mn id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.3" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.3.cmml">6</mn></mfrac><annotation-xml encoding="MathML-Content" id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1b"><apply id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.cmml" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1"><divide id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.1.cmml" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1"></divide><cn type="integer" id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.2.cmml" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.2">1</cn><cn type="integer" id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.3.cmml" xref="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.m1.1c">\frac{1}{6}</annotation></semantics></math></span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.9" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.9.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Answer: A</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.10" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.10.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Question:</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.11" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.11.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Which of the following is the body cavity that contains the pituitary gland?</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.12" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.12.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Options:</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.13" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.13.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A. Abdominal</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.14" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.14.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">B. Cranial</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.15" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.15.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">C. Pleural</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.16" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.16.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">D. Spinal</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.17" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.17.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Answer: B</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.18" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.18.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Question:</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.19" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.19.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Where was the most famous site of the mystery cults in Greece?</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.20" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.20.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Options:</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.21" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.21.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A. Ephesus</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.22" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.22.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">B. Corinth</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.23" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.23.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">C. Athens</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.24" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.24.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">D. Eleusis</span></span>
<span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.25" class="ltx_p"><span id="A1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.25.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Answer:</span></span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Authors</h2>

<div id="A2.p1" class="ltx_para">
<table id="A2.p1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="A2.p1.1.1" class="ltx_tr">
<td id="A2.p1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.1.1.1.1" class="ltx_p" style="width:142.3pt;">Marah Abdin</span>
</span>
</td>
<td id="A2.p1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.1.2.1.1" class="ltx_p" style="width:142.3pt;">Russell J. Hewett</span>
</span>
</td>
<td id="A2.p1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.1.3.1.1" class="ltx_p" style="width:142.3pt;">Olatunji Ruwase</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.2" class="ltx_tr">
<td id="A2.p1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.2.1.1.1" class="ltx_p" style="width:142.3pt;">Sam Ade Jacobs</span>
</span>
</td>
<td id="A2.p1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.2.2.1.1" class="ltx_p" style="width:142.3pt;">Jamie Huynh</span>
</span>
</td>
<td id="A2.p1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.2.3.1.1" class="ltx_p" style="width:142.3pt;">Olli Saarikivi</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.3" class="ltx_tr">
<td id="A2.p1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.3.1.1.1" class="ltx_p" style="width:142.3pt;">Ammar Ahmad Awan</span>
</span>
</td>
<td id="A2.p1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.3.2.1.1" class="ltx_p" style="width:142.3pt;">Mojan Javaheripi</span>
</span>
</td>
<td id="A2.p1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.3.3.1.1" class="ltx_p" style="width:142.3pt;">Amin Saied</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.4" class="ltx_tr">
<td id="A2.p1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.4.1.1.1" class="ltx_p" style="width:142.3pt;">Jyoti Aneja</span>
</span>
</td>
<td id="A2.p1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.4.2.1.1" class="ltx_p" style="width:142.3pt;">Xin Jin</span>
</span>
</td>
<td id="A2.p1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.4.3.1.1" class="ltx_p" style="width:142.3pt;">Adil Salim</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.5" class="ltx_tr">
<td id="A2.p1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.5.1.1.1" class="ltx_p" style="width:142.3pt;">Ahmed Awadallah</span>
</span>
</td>
<td id="A2.p1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.5.2.1.1" class="ltx_p" style="width:142.3pt;">Piero Kauffmann</span>
</span>
</td>
<td id="A2.p1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.5.3.1.1" class="ltx_p" style="width:142.3pt;">Michael Santacroce</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.6" class="ltx_tr">
<td id="A2.p1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.6.1.1.1" class="ltx_p" style="width:142.3pt;">Hany Awadalla</span>
</span>
</td>
<td id="A2.p1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.6.2.1.1" class="ltx_p" style="width:142.3pt;">Nikos Karampatziakis</span>
</span>
</td>
<td id="A2.p1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.6.3.1.1" class="ltx_p" style="width:142.3pt;">Shital Shah</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.7" class="ltx_tr">
<td id="A2.p1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.7.1.1.1" class="ltx_p" style="width:142.3pt;">Nguyen Bach</span>
</span>
</td>
<td id="A2.p1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.7.2.1.1" class="ltx_p" style="width:142.3pt;">Dongwoo Kim</span>
</span>
</td>
<td id="A2.p1.1.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.7.3.1.1" class="ltx_p" style="width:142.3pt;">Ning Shang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.8" class="ltx_tr">
<td id="A2.p1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.8.1.1.1" class="ltx_p" style="width:142.3pt;">Amit Bahree</span>
</span>
</td>
<td id="A2.p1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.8.2.1.1" class="ltx_p" style="width:142.3pt;">Mahmoud Khademi</span>
</span>
</td>
<td id="A2.p1.1.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.8.3.1.1" class="ltx_p" style="width:142.3pt;">Hiteshi Sharma</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.9" class="ltx_tr">
<td id="A2.p1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.9.1.1.1" class="ltx_p" style="width:142.3pt;">Arash Bakhtiari</span>
</span>
</td>
<td id="A2.p1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.9.2.1.1" class="ltx_p" style="width:142.3pt;">Lev Kurilenko</span>
</span>
</td>
<td id="A2.p1.1.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.9.3.1.1" class="ltx_p" style="width:142.3pt;">Xia Song</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.10" class="ltx_tr">
<td id="A2.p1.1.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.10.1.1.1" class="ltx_p" style="width:142.3pt;">Harkirat Behl</span>
</span>
</td>
<td id="A2.p1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.10.2.1.1" class="ltx_p" style="width:142.3pt;">James R. Lee</span>
</span>
</td>
<td id="A2.p1.1.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.10.3.1.1" class="ltx_p" style="width:142.3pt;">Masahiro Tanaka</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.11" class="ltx_tr">
<td id="A2.p1.1.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.11.1.1.1" class="ltx_p" style="width:142.3pt;">Alon Benhaim</span>
</span>
</td>
<td id="A2.p1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.11.2.1.1" class="ltx_p" style="width:142.3pt;">Yin Tat Lee</span>
</span>
</td>
<td id="A2.p1.1.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.11.3.1.1" class="ltx_p" style="width:142.3pt;">Xin Wang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.12" class="ltx_tr">
<td id="A2.p1.1.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.12.1.1.1" class="ltx_p" style="width:142.3pt;">Misha Bilenko</span>
</span>
</td>
<td id="A2.p1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.12.2.1.1" class="ltx_p" style="width:142.3pt;">Yuanzhi Li</span>
</span>
</td>
<td id="A2.p1.1.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.12.3.1.1" class="ltx_p" style="width:142.3pt;">Rachel Ward</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.13" class="ltx_tr">
<td id="A2.p1.1.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.13.1.1.1" class="ltx_p" style="width:142.3pt;">Johan Bjorck</span>
</span>
</td>
<td id="A2.p1.1.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.13.2.1.1" class="ltx_p" style="width:142.3pt;">Chen Liang</span>
</span>
</td>
<td id="A2.p1.1.13.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.13.3.1.1" class="ltx_p" style="width:142.3pt;">Guanhua Wang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.14" class="ltx_tr">
<td id="A2.p1.1.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.14.1.1.1" class="ltx_p" style="width:142.3pt;">Sébastien Bubeck</span>
</span>
</td>
<td id="A2.p1.1.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.14.2.1.1" class="ltx_p" style="width:142.3pt;">Weishung Liu</span>
</span>
</td>
<td id="A2.p1.1.14.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.14.3.1.1" class="ltx_p" style="width:142.3pt;">Philipp Witte</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.15" class="ltx_tr">
<td id="A2.p1.1.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.15.1.1.1" class="ltx_p" style="width:142.3pt;">Martin Cai</span>
</span>
</td>
<td id="A2.p1.1.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.15.2.1.1" class="ltx_p" style="width:142.3pt;">Eric Lin</span>
</span>
</td>
<td id="A2.p1.1.15.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.15.3.1.1" class="ltx_p" style="width:142.3pt;">Michael Wyatt</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.16" class="ltx_tr">
<td id="A2.p1.1.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.16.1.1.1" class="ltx_p" style="width:142.3pt;">Caio César Teodoro Mendes</span>
</span>
</td>
<td id="A2.p1.1.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.16.2.1.1" class="ltx_p" style="width:142.3pt;">Zeqi Lin</span>
</span>
</td>
<td id="A2.p1.1.16.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.16.3.1.1" class="ltx_p" style="width:142.3pt;">Jiahang Xu</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.17" class="ltx_tr">
<td id="A2.p1.1.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.17.1.1.1" class="ltx_p" style="width:142.3pt;">Weizhu Chen</span>
</span>
</td>
<td id="A2.p1.1.17.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.17.2.1.1" class="ltx_p" style="width:142.3pt;">Piyush Madan</span>
</span>
</td>
<td id="A2.p1.1.17.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.17.3.1.1" class="ltx_p" style="width:142.3pt;">Can Xu</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.18" class="ltx_tr">
<td id="A2.p1.1.18.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.18.1.1.1" class="ltx_p" style="width:142.3pt;">Vishrav Chaudhary</span>
</span>
</td>
<td id="A2.p1.1.18.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.18.2.1.1" class="ltx_p" style="width:142.3pt;">Arindam Mitra</span>
</span>
</td>
<td id="A2.p1.1.18.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.18.3.1.1" class="ltx_p" style="width:142.3pt;">Sonali Yadav</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.19" class="ltx_tr">
<td id="A2.p1.1.19.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.19.1.1.1" class="ltx_p" style="width:142.3pt;">Parul Chopra</span>
</span>
</td>
<td id="A2.p1.1.19.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.19.2.1.1" class="ltx_p" style="width:142.3pt;">Hardik Modi</span>
</span>
</td>
<td id="A2.p1.1.19.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.19.3.1.1" class="ltx_p" style="width:142.3pt;">Fan Yang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.20" class="ltx_tr">
<td id="A2.p1.1.20.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.20.1.1.1" class="ltx_p" style="width:142.3pt;">Allie Del Giorno</span>
</span>
</td>
<td id="A2.p1.1.20.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.20.2.1.1" class="ltx_p" style="width:142.3pt;">Brandon Norick</span>
</span>
</td>
<td id="A2.p1.1.20.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.20.3.1.1" class="ltx_p" style="width:142.3pt;">Ziyi Yang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.21" class="ltx_tr">
<td id="A2.p1.1.21.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.21.1.1.1" class="ltx_p" style="width:142.3pt;">Gustavo de Rosa</span>
</span>
</td>
<td id="A2.p1.1.21.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.21.2.1.1" class="ltx_p" style="width:142.3pt;">Anh Nguyen</span>
</span>
</td>
<td id="A2.p1.1.21.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.21.3.1.1" class="ltx_p" style="width:142.3pt;">Donghan Yu</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.22" class="ltx_tr">
<td id="A2.p1.1.22.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.22.1.1.1" class="ltx_p" style="width:142.3pt;">Matthew Dixon</span>
</span>
</td>
<td id="A2.p1.1.22.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.22.2.1.1" class="ltx_p" style="width:142.3pt;">Barun Patra</span>
</span>
</td>
<td id="A2.p1.1.22.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.22.3.1.1" class="ltx_p" style="width:142.3pt;">Chengruidong Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.23" class="ltx_tr">
<td id="A2.p1.1.23.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.23.1.1.1" class="ltx_p" style="width:142.3pt;">Ronen Eldan</span>
</span>
</td>
<td id="A2.p1.1.23.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.23.2.1.1" class="ltx_p" style="width:142.3pt;">Daniel Perez-Becker</span>
</span>
</td>
<td id="A2.p1.1.23.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.23.3.1.1" class="ltx_p" style="width:142.3pt;">Cyril Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.24" class="ltx_tr">
<td id="A2.p1.1.24.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.24.1.1.1" class="ltx_p" style="width:142.3pt;">Dan Iter</span>
</span>
</td>
<td id="A2.p1.1.24.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.24.2.1.1" class="ltx_p" style="width:142.3pt;">Heyang Qin</span>
</span>
</td>
<td id="A2.p1.1.24.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.24.3.1.1" class="ltx_p" style="width:142.3pt;">Jianwen Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.25" class="ltx_tr">
<td id="A2.p1.1.25.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.25.1.1.1" class="ltx_p" style="width:142.3pt;">Amit Garg</span>
</span>
</td>
<td id="A2.p1.1.25.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.25.2.1.1" class="ltx_p" style="width:142.3pt;">Thomas Portet</span>
</span>
</td>
<td id="A2.p1.1.25.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.25.3.1.1" class="ltx_p" style="width:142.3pt;">Li Lyna Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.26" class="ltx_tr">
<td id="A2.p1.1.26.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.26.1.1.1" class="ltx_p" style="width:142.3pt;">Abhishek Goswami</span>
</span>
</td>
<td id="A2.p1.1.26.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.26.2.1.1" class="ltx_p" style="width:142.3pt;">Reid Pryzant</span>
</span>
</td>
<td id="A2.p1.1.26.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.26.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.26.3.1.1" class="ltx_p" style="width:142.3pt;">Yi Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.27" class="ltx_tr">
<td id="A2.p1.1.27.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.27.1.1.1" class="ltx_p" style="width:142.3pt;">Suriya Gunasekar</span>
</span>
</td>
<td id="A2.p1.1.27.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.27.2.1.1" class="ltx_p" style="width:142.3pt;">Sambuddha Roy</span>
</span>
</td>
<td id="A2.p1.1.27.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.27.3.1.1" class="ltx_p" style="width:142.3pt;">Yue Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.28" class="ltx_tr">
<td id="A2.p1.1.28.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.28.1.1.1" class="ltx_p" style="width:142.3pt;">Emman Haider</span>
</span>
</td>
<td id="A2.p1.1.28.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.28.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.28.2.1.1" class="ltx_p" style="width:142.3pt;">Marko Radmilac</span>
</span>
</td>
<td id="A2.p1.1.28.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.28.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.28.3.1.1" class="ltx_p" style="width:142.3pt;">Yunan Zhang</span>
</span>
</td>
</tr>
<tr id="A2.p1.1.29" class="ltx_tr">
<td id="A2.p1.1.29.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.29.1.1.1" class="ltx_p" style="width:142.3pt;">Junheng Hao</span>
</span>
</td>
<td id="A2.p1.1.29.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.29.2.1.1" class="ltx_p" style="width:142.3pt;">Corby Rosset</span>
</span>
</td>
<td id="A2.p1.1.29.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.p1.1.29.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.p1.1.29.3.1.1" class="ltx_p" style="width:142.3pt;">Xiren Zhou</span>
</span>
</td>
</tr>
</tbody></table>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.14218" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.14219" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.14219">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.14219" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.14220" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 18:06:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
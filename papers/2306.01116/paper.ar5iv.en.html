<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.01116] The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only</title><meta property="og:description" content="Large language models are commonly trained on a mixture of filtered web data and curated “high-quality” corpora, such as social media conversations, books, or technical papers. This curation process is believed to be n…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.01116">

<!--Generated on Thu Feb 29 01:55:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="deduplication,  NLP,  LLM,  curated,  data,  crawl,  Falcon LLM">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">The RefinedWeb Dataset for Falcon LLM:
<br class="ltx_break">Outperforming Curated Corpora with Web Data, and Web Data Only</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guilherme Penedo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Quentin Malartic
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Hesslow
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruxandra Cojocaru
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessandro Cappelli
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hamza Alobeidli
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Baptiste Pannier
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ebtesam Almazrouei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julien Launay
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Large language models are commonly trained on a mixture of filtered web data and curated “high-quality” corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our <span id="id1.id1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span> dataset, and 1.3/7.5B parameters language models trained on it<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>Details about how to access Falcon LLM open source is available on <a href="falconllm.tii.ae" title="" class="ltx_ref ltx_url ltx_font_typewriter">falconllm.tii.ae</a></span></span></span>.</p>
</div>
<div class="ltx_keywords">deduplication, NLP, LLM, curated, data, crawl, Falcon LLM
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">The Falcon LLM team</span>  
<br class="ltx_break">
 
<br class="ltx_break">



 
<br class="ltx_break"></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p ltx_align_center"><a target="_blank" href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/tiiuae/falcon-refinedweb</a>

<br class="ltx_break"></p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2306.01116/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S0.F1.9.1" class="ltx_text ltx_font_bold">Models trained on <span id="S0.F1.9.1.1" class="ltx_text" style="color:#DB57B2;">●<span id="S0.F1.9.1.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span> alone outperform models trained on curated corpora.</span> Zero-shot performance on our&nbsp;<span id="S0.F1.10.2" class="ltx_text ltx_font_typewriter">main-agg</span> task aggregate (see <a href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1</span></a> for details). At equivalent compute budgets, our models significantly outperform publicly available models trained on <math id="S0.F1.3.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S0.F1.3.m1.1b"><mi mathcolor="#7DD86E" mathvariant="normal" id="S0.F1.3.m1.1.1" xref="S0.F1.3.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S0.F1.3.m1.1c"><ci id="S0.F1.3.m1.1.1.cmml" xref="S0.F1.3.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.F1.3.m1.1d">\blacktriangledown</annotation></semantics></math><span id="S0.F1.11.3" class="ltx_text" style="color:#7DD86E;"> The Pile</span>, and match the performance of the <math id="S0.F1.4.m2.1" class="ltx_Math" alttext="\blacksquare" display="inline"><semantics id="S0.F1.4.m2.1b"><mi mathcolor="#5F57DB" mathvariant="normal" id="S0.F1.4.m2.1.1" xref="S0.F1.4.m2.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="S0.F1.4.m2.1c"><ci id="S0.F1.4.m2.1.1.cmml" xref="S0.F1.4.m2.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.F1.4.m2.1d">\blacksquare</annotation></semantics></math><span id="S0.F1.12.4" class="ltx_text" style="color:#5F57DB;"> GPT-3</span> models when tested within our evaluation setup.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S0.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S0.T1.23.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">●<span id="S0.T1.23.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span><span id="S0.T1.23.1.2" class="ltx_text" style="color:#000000;"> improves on existing English pretraining datasets for large language models by combining extensive filtering with stringent deduplication at unprecedented scale.</span></span> For additional details, see the full version in <a href="#A6.T12" title="In F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">12</span></a> of <a href="#A6.SS3" title="F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">F.3</span></a>. </figcaption>
<table id="S0.T1.21" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S0.T1.21.22" class="ltx_tr">
<td id="S0.T1.21.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S0.T1.21.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.22.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.21.22.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></span>
</span>
</td>
<td id="S0.T1.21.22.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.21.22.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Size</span></td>
<td id="S0.T1.21.22.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.21.22.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Availability</span></td>
<td id="S0.T1.21.22.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.21.22.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Web</span></td>
<td id="S0.T1.21.22.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S0.T1.21.22.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.22.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.21.22.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CC Processing</span></span>
</span>
</td>
<td id="S0.T1.21.22.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S0.T1.21.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.22.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.21.22.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Deduplication</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.21.23" class="ltx_tr">
<td id="S0.T1.21.23.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="6"><span id="S0.T1.21.23.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Massive web datasets</span></td>
</tr>
<tr id="S0.T1.2.2" class="ltx_tr">
<td id="S0.T1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">C4</span></span>
</span>
</td>
<td id="S0.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.1.1.1.m1.1" class="ltx_Math" alttext="\sim 360" display="inline"><semantics id="S0.T1.1.1.1.m1.1a"><mrow id="S0.T1.1.1.1.m1.1.1" xref="S0.T1.1.1.1.m1.1.1.cmml"><mi id="S0.T1.1.1.1.m1.1.1.2" xref="S0.T1.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.1.1.1.m1.1.1.1" xref="S0.T1.1.1.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.1.1.1.m1.1.1.3" xref="S0.T1.1.1.1.m1.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.1.1.1.m1.1b"><apply id="S0.T1.1.1.1.m1.1.1.cmml" xref="S0.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.1.1.1.m1.1.1.1.cmml" xref="S0.T1.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.1.1.1.m1.1.1.2.cmml" xref="S0.T1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.1.1.1.m1.1.1.3.cmml" xref="S0.T1.1.1.1.m1.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.1.1.1.m1.1c">\sim 360</annotation></semantics></math><span id="S0.T1.1.1.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S0.T1.2.2.4.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.2.2.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S0.T1.2.2.2.m1.1a"><mn mathsize="90%" id="S0.T1.2.2.2.m1.1.1" xref="S0.T1.2.2.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S0.T1.2.2.2.m1.1b"><cn type="integer" id="S0.T1.2.2.2.m1.1.1.cmml" xref="S0.T1.2.2.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.2.2.2.m1.1c">100</annotation></semantics></math><span id="S0.T1.2.2.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.2.2.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.2.2.5.1.1.1" class="ltx_text" style="font-size:90%;">Rules + NSFW words blocklist</span></span>
</span>
</td>
<td id="S0.T1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.2.2.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.2.2.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact:</span><span id="S0.T1.2.2.6.1.1.2" class="ltx_text" style="font-size:90%;"> spans of 3 sentences</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.5.5" class="ltx_tr">
<td id="S0.T1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.5.5.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR-21.09</span></span>
</span>
</td>
<td id="S0.T1.3.3.1" class="ltx_td ltx_align_center">
<math id="S0.T1.3.3.1.m1.1" class="ltx_Math" alttext="\sim 370" display="inline"><semantics id="S0.T1.3.3.1.m1.1a"><mrow id="S0.T1.3.3.1.m1.1.1" xref="S0.T1.3.3.1.m1.1.1.cmml"><mi id="S0.T1.3.3.1.m1.1.1.2" xref="S0.T1.3.3.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.3.3.1.m1.1.1.1" xref="S0.T1.3.3.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.3.3.1.m1.1.1.3" xref="S0.T1.3.3.1.m1.1.1.3.cmml">370</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.3.3.1.m1.1b"><apply id="S0.T1.3.3.1.m1.1.1.cmml" xref="S0.T1.3.3.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.3.3.1.m1.1.1.1.cmml" xref="S0.T1.3.3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.3.3.1.m1.1.1.2.cmml" xref="S0.T1.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.3.3.1.m1.1.1.3.cmml" xref="S0.T1.3.3.1.m1.1.1.3">370</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.3.3.1.m1.1c">\sim 370</annotation></semantics></math><span id="S0.T1.3.3.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.5.5.5" class="ltx_td ltx_align_center"><span id="S0.T1.5.5.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.4.4.2" class="ltx_td ltx_align_center">
<math id="S0.T1.4.4.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S0.T1.4.4.2.m1.1a"><mn mathsize="90%" id="S0.T1.4.4.2.m1.1.1" xref="S0.T1.4.4.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S0.T1.4.4.2.m1.1b"><cn type="integer" id="S0.T1.4.4.2.m1.1.1.cmml" xref="S0.T1.4.4.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.4.4.2.m1.1c">100</annotation></semantics></math><span id="S0.T1.4.4.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.5.5.6.1.1.1" class="ltx_text" style="font-size:90%;">Built at the line-level</span></span>
</span>
</td>
<td id="S0.T1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.5.5.3.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.5.5.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="S0.T1.5.5.3.1.1.2" class="ltx_text" style="font-size:90%;">: per line (</span><math id="S0.T1.5.5.3.1.1.m1.1" class="ltx_Math" alttext="\sim 55\%" display="inline"><semantics id="S0.T1.5.5.3.1.1.m1.1a"><mrow id="S0.T1.5.5.3.1.1.m1.1.1" xref="S0.T1.5.5.3.1.1.m1.1.1.cmml"><mi id="S0.T1.5.5.3.1.1.m1.1.1.2" xref="S0.T1.5.5.3.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.5.5.3.1.1.m1.1.1.1" xref="S0.T1.5.5.3.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.5.5.3.1.1.m1.1.1.3" xref="S0.T1.5.5.3.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.5.5.3.1.1.m1.1.1.3.2" xref="S0.T1.5.5.3.1.1.m1.1.1.3.2.cmml">55</mn><mo mathsize="90%" id="S0.T1.5.5.3.1.1.m1.1.1.3.1" xref="S0.T1.5.5.3.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.5.5.3.1.1.m1.1b"><apply id="S0.T1.5.5.3.1.1.m1.1.1.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.5.5.3.1.1.m1.1.1.1.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.5.5.3.1.1.m1.1.1.2.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.5.5.3.1.1.m1.1.1.3.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.5.5.3.1.1.m1.1.1.3.1.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.5.5.3.1.1.m1.1.1.3.2.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.3.2">55</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.5.5.3.1.1.m1.1c">\sim 55\%</annotation></semantics></math><span id="S0.T1.5.5.3.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.7.7" class="ltx_tr">
<td id="S0.T1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.7.7.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.7.7.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR-22.01</span></span>
</span>
</td>
<td id="S0.T1.6.6.1" class="ltx_td ltx_align_center">
<math id="S0.T1.6.6.1.m1.1" class="ltx_Math" alttext="\sim 283" display="inline"><semantics id="S0.T1.6.6.1.m1.1a"><mrow id="S0.T1.6.6.1.m1.1.1" xref="S0.T1.6.6.1.m1.1.1.cmml"><mi id="S0.T1.6.6.1.m1.1.1.2" xref="S0.T1.6.6.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.6.6.1.m1.1.1.1" xref="S0.T1.6.6.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.6.6.1.m1.1.1.3" xref="S0.T1.6.6.1.m1.1.1.3.cmml">283</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.6.6.1.m1.1b"><apply id="S0.T1.6.6.1.m1.1.1.cmml" xref="S0.T1.6.6.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.6.6.1.m1.1.1.1.cmml" xref="S0.T1.6.6.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.6.6.1.m1.1.1.2.cmml" xref="S0.T1.6.6.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.6.6.1.m1.1.1.3.cmml" xref="S0.T1.6.6.1.m1.1.1.3">283</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.6.6.1.m1.1c">\sim 283</annotation></semantics></math><span id="S0.T1.6.6.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.7.7.4" class="ltx_td ltx_align_center"><span id="S0.T1.7.7.4.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.7.7.2" class="ltx_td ltx_align_center">
<math id="S0.T1.7.7.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S0.T1.7.7.2.m1.1a"><mn mathsize="90%" id="S0.T1.7.7.2.m1.1.1" xref="S0.T1.7.7.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S0.T1.7.7.2.m1.1b"><cn type="integer" id="S0.T1.7.7.2.m1.1.1.cmml" xref="S0.T1.7.7.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.7.7.2.m1.1c">100</annotation></semantics></math><span id="S0.T1.7.7.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.7.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.7.7.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.7.7.5.1.1.1" class="ltx_text" style="font-size:90%;">Line-level rules + optional rules &amp; NSFW URL blocklist</span></span>
</span>
</td>
<td id="S0.T1.7.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.7.7.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.7.7.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="S0.T1.7.7.6.1.1.2" class="ltx_text" style="font-size:90%;">: per line (optional, not used for results in this paper)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.21.24" class="ltx_tr">
<td id="S0.T1.21.24.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="6"><span id="S0.T1.21.24.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Curated datasets</span></td>
</tr>
<tr id="S0.T1.11.11" class="ltx_tr">
<td id="S0.T1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.8.8.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="S0.T1.8.8.1.1.1.m1.1" class="ltx_Math" alttext="\blacksquare" display="inline"><semantics id="S0.T1.8.8.1.1.1.m1.1a"><mi mathcolor="#5F57DB" mathsize="90%" mathvariant="normal" id="S0.T1.8.8.1.1.1.m1.1.1" xref="S0.T1.8.8.1.1.1.m1.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="S0.T1.8.8.1.1.1.m1.1b"><ci id="S0.T1.8.8.1.1.1.m1.1.1.cmml" xref="S0.T1.8.8.1.1.1.m1.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.8.8.1.1.1.m1.1c">\blacksquare</annotation></semantics></math><span id="S0.T1.8.8.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#5F57DB;"> GPT-3</span></span>
</span>
</td>
<td id="S0.T1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.9.9.2.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S0.T1.9.9.2.m1.1a"><mn mathsize="90%" id="S0.T1.9.9.2.m1.1.1" xref="S0.T1.9.9.2.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S0.T1.9.9.2.m1.1b"><cn type="integer" id="S0.T1.9.9.2.m1.1.1.cmml" xref="S0.T1.9.9.2.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.9.9.2.m1.1c">300</annotation></semantics></math><span id="S0.T1.9.9.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.11.11.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S0.T1.11.11.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="S0.T1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.10.10.3.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S0.T1.10.10.3.m1.1a"><mn mathsize="90%" id="S0.T1.10.10.3.m1.1.1" xref="S0.T1.10.10.3.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S0.T1.10.10.3.m1.1b"><cn type="integer" id="S0.T1.10.10.3.m1.1.1.cmml" xref="S0.T1.10.10.3.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.10.10.3.m1.1c">60</annotation></semantics></math><span id="S0.T1.10.10.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.11.11.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.11.11.6.1.1.1" class="ltx_text" style="font-size:90%;">Content filter trained on known high-quality sources</span></span>
</span>
</td>
<td id="S0.T1.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.11.11.4.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.11.11.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="S0.T1.11.11.4.1.1.2" class="ltx_text" style="font-size:90%;">: MinHash (</span><math id="S0.T1.11.11.4.1.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="S0.T1.11.11.4.1.1.m1.1a"><mrow id="S0.T1.11.11.4.1.1.m1.1.1" xref="S0.T1.11.11.4.1.1.m1.1.1.cmml"><mi id="S0.T1.11.11.4.1.1.m1.1.1.2" xref="S0.T1.11.11.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.11.11.4.1.1.m1.1.1.1" xref="S0.T1.11.11.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.11.11.4.1.1.m1.1.1.3" xref="S0.T1.11.11.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.11.11.4.1.1.m1.1.1.3.2" xref="S0.T1.11.11.4.1.1.m1.1.1.3.2.cmml">10</mn><mo mathsize="90%" id="S0.T1.11.11.4.1.1.m1.1.1.3.1" xref="S0.T1.11.11.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.11.11.4.1.1.m1.1b"><apply id="S0.T1.11.11.4.1.1.m1.1.1.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.11.11.4.1.1.m1.1.1.1.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.11.11.4.1.1.m1.1.1.2.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.11.11.4.1.1.m1.1.1.3.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.11.11.4.1.1.m1.1.1.3.1.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.11.11.4.1.1.m1.1.1.3.2.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.11.11.4.1.1.m1.1c">\sim 10\%</annotation></semantics></math><span id="S0.T1.11.11.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.15.15" class="ltx_tr">
<td id="S0.T1.12.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.12.12.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="S0.T1.12.12.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S0.T1.12.12.1.1.1.m1.1a"><mi mathcolor="#7DD86E" mathsize="90%" mathvariant="normal" id="S0.T1.12.12.1.1.1.m1.1.1" xref="S0.T1.12.12.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S0.T1.12.12.1.1.1.m1.1b"><ci id="S0.T1.12.12.1.1.1.m1.1.1.cmml" xref="S0.T1.12.12.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.12.12.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S0.T1.12.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#7DD86E;"> The Pile</span></span>
</span>
</td>
<td id="S0.T1.13.13.2" class="ltx_td ltx_align_center">
<math id="S0.T1.13.13.2.m1.1" class="ltx_Math" alttext="\sim 340" display="inline"><semantics id="S0.T1.13.13.2.m1.1a"><mrow id="S0.T1.13.13.2.m1.1.1" xref="S0.T1.13.13.2.m1.1.1.cmml"><mi id="S0.T1.13.13.2.m1.1.1.2" xref="S0.T1.13.13.2.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.13.13.2.m1.1.1.1" xref="S0.T1.13.13.2.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.13.13.2.m1.1.1.3" xref="S0.T1.13.13.2.m1.1.1.3.cmml">340</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.13.13.2.m1.1b"><apply id="S0.T1.13.13.2.m1.1.1.cmml" xref="S0.T1.13.13.2.m1.1.1"><csymbol cd="latexml" id="S0.T1.13.13.2.m1.1.1.1.cmml" xref="S0.T1.13.13.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.13.13.2.m1.1.1.2.cmml" xref="S0.T1.13.13.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.13.13.2.m1.1.1.3.cmml" xref="S0.T1.13.13.2.m1.1.1.3">340</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.13.13.2.m1.1c">\sim 340</annotation></semantics></math><span id="S0.T1.13.13.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.15.15.5" class="ltx_td ltx_align_center"><span id="S0.T1.15.15.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.14.14.3" class="ltx_td ltx_align_center">
<math id="S0.T1.14.14.3.m1.1" class="ltx_Math" alttext="18" display="inline"><semantics id="S0.T1.14.14.3.m1.1a"><mn mathsize="90%" id="S0.T1.14.14.3.m1.1.1" xref="S0.T1.14.14.3.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S0.T1.14.14.3.m1.1b"><cn type="integer" id="S0.T1.14.14.3.m1.1.1.cmml" xref="S0.T1.14.14.3.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.14.14.3.m1.1c">18</annotation></semantics></math><span id="S0.T1.14.14.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.15.15.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.15.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.15.15.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.15.15.6.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jusText</span><span id="S0.T1.15.15.6.1.1.2" class="ltx_text" style="font-size:90%;"> for extraction, content filter trained on curated data</span></span>
</span>
</td>
<td id="S0.T1.15.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.15.15.4.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.15.15.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="S0.T1.15.15.4.1.1.2" class="ltx_text" style="font-size:90%;">: MinHash (</span><math id="S0.T1.15.15.4.1.1.m1.1" class="ltx_Math" alttext="\sim 26\%" display="inline"><semantics id="S0.T1.15.15.4.1.1.m1.1a"><mrow id="S0.T1.15.15.4.1.1.m1.1.1" xref="S0.T1.15.15.4.1.1.m1.1.1.cmml"><mi id="S0.T1.15.15.4.1.1.m1.1.1.2" xref="S0.T1.15.15.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.15.15.4.1.1.m1.1.1.1" xref="S0.T1.15.15.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.15.15.4.1.1.m1.1.1.3" xref="S0.T1.15.15.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.15.15.4.1.1.m1.1.1.3.2" xref="S0.T1.15.15.4.1.1.m1.1.1.3.2.cmml">26</mn><mo mathsize="90%" id="S0.T1.15.15.4.1.1.m1.1.1.3.1" xref="S0.T1.15.15.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.15.15.4.1.1.m1.1b"><apply id="S0.T1.15.15.4.1.1.m1.1.1.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.15.15.4.1.1.m1.1.1.1.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.15.15.4.1.1.m1.1.1.2.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.15.15.4.1.1.m1.1.1.3.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.15.15.4.1.1.m1.1.1.3.1.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.15.15.4.1.1.m1.1.1.3.2.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.3.2">26</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.15.15.4.1.1.m1.1c">\sim 26\%</annotation></semantics></math><span id="S0.T1.15.15.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.18.18" class="ltx_tr">
<td id="S0.T1.16.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.16.16.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="S0.T1.16.16.1.1.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S0.T1.16.16.1.1.1.m1.1a"><mi mathcolor="#DB5F56" mathsize="90%" mathvariant="normal" id="S0.T1.16.16.1.1.1.m1.1.1" xref="S0.T1.16.16.1.1.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S0.T1.16.16.1.1.1.m1.1b"><ci id="S0.T1.16.16.1.1.1.m1.1.1.cmml" xref="S0.T1.16.16.1.1.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.16.16.1.1.1.m1.1c">\bigstar</annotation></semantics></math><span id="S0.T1.16.16.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#DB5F56;"> PaLM</span></span>
</span>
</td>
<td id="S0.T1.17.17.2" class="ltx_td ltx_align_center">
<math id="S0.T1.17.17.2.m1.1" class="ltx_Math" alttext="780" display="inline"><semantics id="S0.T1.17.17.2.m1.1a"><mn mathsize="90%" id="S0.T1.17.17.2.m1.1.1" xref="S0.T1.17.17.2.m1.1.1.cmml">780</mn><annotation-xml encoding="MathML-Content" id="S0.T1.17.17.2.m1.1b"><cn type="integer" id="S0.T1.17.17.2.m1.1.1.cmml" xref="S0.T1.17.17.2.m1.1.1">780</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.17.17.2.m1.1c">780</annotation></semantics></math><span id="S0.T1.17.17.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.18.18.4" class="ltx_td ltx_align_center"><span id="S0.T1.18.18.4.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="S0.T1.18.18.3" class="ltx_td ltx_align_center">
<math id="S0.T1.18.18.3.m1.1" class="ltx_Math" alttext="27" display="inline"><semantics id="S0.T1.18.18.3.m1.1a"><mn mathsize="90%" id="S0.T1.18.18.3.m1.1.1" xref="S0.T1.18.18.3.m1.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="S0.T1.18.18.3.m1.1b"><cn type="integer" id="S0.T1.18.18.3.m1.1.1.cmml" xref="S0.T1.18.18.3.m1.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.18.18.3.m1.1c">27</annotation></semantics></math><span id="S0.T1.18.18.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.18.18.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.18.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.18.18.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.18.18.5.1.1.1" class="ltx_text" style="font-size:90%;">Filter trained on HQ data</span></span>
</span>
</td>
<td id="S0.T1.18.18.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.18.18.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.18.18.6.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.21.25" class="ltx_tr">
<td id="S0.T1.21.25.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="6"><span id="S0.T1.21.25.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Ours</span></td>
</tr>
<tr id="S0.T1.21.21" class="ltx_tr">
<td id="S0.T1.21.21.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S0.T1.21.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.21.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.21.21.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#DB57B2;">●<span id="S0.T1.21.21.4.1.1.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span></span>
</span>
</td>
<td id="S0.T1.19.19.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S0.T1.19.19.1.m1.2" class="ltx_Math" alttext="\sim 5,000" display="inline"><semantics id="S0.T1.19.19.1.m1.2a"><mrow id="S0.T1.19.19.1.m1.2.3" xref="S0.T1.19.19.1.m1.2.3.cmml"><mi id="S0.T1.19.19.1.m1.2.3.2" xref="S0.T1.19.19.1.m1.2.3.2.cmml"></mi><mo mathsize="90%" id="S0.T1.19.19.1.m1.2.3.1" xref="S0.T1.19.19.1.m1.2.3.1.cmml">∼</mo><mrow id="S0.T1.19.19.1.m1.2.3.3.2" xref="S0.T1.19.19.1.m1.2.3.3.1.cmml"><mn mathsize="90%" id="S0.T1.19.19.1.m1.1.1" xref="S0.T1.19.19.1.m1.1.1.cmml">5</mn><mo mathsize="90%" id="S0.T1.19.19.1.m1.2.3.3.2.1" xref="S0.T1.19.19.1.m1.2.3.3.1.cmml">,</mo><mn mathsize="90%" id="S0.T1.19.19.1.m1.2.2" xref="S0.T1.19.19.1.m1.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.19.19.1.m1.2b"><apply id="S0.T1.19.19.1.m1.2.3.cmml" xref="S0.T1.19.19.1.m1.2.3"><csymbol cd="latexml" id="S0.T1.19.19.1.m1.2.3.1.cmml" xref="S0.T1.19.19.1.m1.2.3.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.19.19.1.m1.2.3.2.cmml" xref="S0.T1.19.19.1.m1.2.3.2">absent</csymbol><list id="S0.T1.19.19.1.m1.2.3.3.1.cmml" xref="S0.T1.19.19.1.m1.2.3.3.2"><cn type="integer" id="S0.T1.19.19.1.m1.1.1.cmml" xref="S0.T1.19.19.1.m1.1.1">5</cn><cn type="integer" id="S0.T1.19.19.1.m1.2.2.cmml" xref="S0.T1.19.19.1.m1.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.19.19.1.m1.2c">\sim 5,000</annotation></semantics></math><span id="S0.T1.19.19.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.21.21.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S0.T1.21.21.5.1" class="ltx_text" style="font-size:90%;">Public (600GT)</span></td>
<td id="S0.T1.20.20.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math id="S0.T1.20.20.2.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S0.T1.20.20.2.m1.1a"><mrow id="S0.T1.20.20.2.m1.1.1" xref="S0.T1.20.20.2.m1.1.1.cmml"><mn mathsize="90%" id="S0.T1.20.20.2.m1.1.1.2" xref="S0.T1.20.20.2.m1.1.1.2.cmml">100</mn><mo mathsize="90%" id="S0.T1.20.20.2.m1.1.1.1" xref="S0.T1.20.20.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.20.20.2.m1.1b"><apply id="S0.T1.20.20.2.m1.1.1.cmml" xref="S0.T1.20.20.2.m1.1.1"><csymbol cd="latexml" id="S0.T1.20.20.2.m1.1.1.1.cmml" xref="S0.T1.20.20.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S0.T1.20.20.2.m1.1.1.2.cmml" xref="S0.T1.20.20.2.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.20.20.2.m1.1c">100\%</annotation></semantics></math></td>
<td id="S0.T1.21.21.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S0.T1.21.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.21.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.21.21.6.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">trafilatura</span><span id="S0.T1.21.21.6.1.1.2" class="ltx_text" style="font-size:90%;"> for text extraction, document and line-level rules, NSFW URL blocklist</span></span>
</span>
</td>
<td id="S0.T1.21.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S0.T1.21.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.21.3.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.21.21.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact &amp; fuzzy</span><span id="S0.T1.21.21.3.1.1.2" class="ltx_text" style="font-size:90%;">: exact substring+MinHash (</span><math id="S0.T1.21.21.3.1.1.m1.1" class="ltx_Math" alttext="\sim 50\%" display="inline"><semantics id="S0.T1.21.21.3.1.1.m1.1a"><mrow id="S0.T1.21.21.3.1.1.m1.1.1" xref="S0.T1.21.21.3.1.1.m1.1.1.cmml"><mi id="S0.T1.21.21.3.1.1.m1.1.1.2" xref="S0.T1.21.21.3.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.21.21.3.1.1.m1.1.1.1" xref="S0.T1.21.21.3.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.21.21.3.1.1.m1.1.1.3" xref="S0.T1.21.21.3.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.21.21.3.1.1.m1.1.1.3.2" xref="S0.T1.21.21.3.1.1.m1.1.1.3.2.cmml">50</mn><mo mathsize="90%" id="S0.T1.21.21.3.1.1.m1.1.1.3.1" xref="S0.T1.21.21.3.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.21.21.3.1.1.m1.1b"><apply id="S0.T1.21.21.3.1.1.m1.1.1.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.21.21.3.1.1.m1.1.1.1.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.21.21.3.1.1.m1.1.1.2.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.21.21.3.1.1.m1.1.1.3.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.21.21.3.1.1.m1.1.1.3.1.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.21.21.3.1.1.m1.1.1.3.2.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.21.21.3.1.1.m1.1c">\sim 50\%</annotation></semantics></math><span id="S0.T1.21.21.3.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Progress in natural language processing is increasingly driven by sheer compute scale alone&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sevilla et&nbsp;al., <a href="#bib.bib68" title="" class="ltx_ref">2022</a>)</cite>: as more compute is expended to train large language models&nbsp;(LLM), they gain and exhibit powerful emergent capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Wei et&nbsp;al., <a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>. To best benefit from scaling, recent scaling laws dictate that both model size and dataset size should jointly be increased <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>. This is at variance with earlier findings, which had argued that scaling should focus on model size first and foremost, with minimal data scaling <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et&nbsp;al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This joint scaling paradigm raises significant challenges: although plentiful, text data is not infinite, especially so when considerations on data quality and licensing are taken into account–leading some researchers to argue scaling may soon be bottlenecked by data availability <cite class="ltx_cite ltx_citemacro_citep">(Villalobos et&nbsp;al., <a href="#bib.bib75" title="" class="ltx_ref">2022</a>)</cite>. Concretely, optimally training a GPT-3 sized model (175B parameters) would require no less than 3,500 billion tokens of text according to&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Hoffmann et&nbsp;al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>. This is twice as much as the largest pretraining datasets ever demonstrated <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>; Touvron et&nbsp;al., <a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite>, and ten times more than the largest publicly available English datasets such as OSCAR&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, C4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, or The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Massively scaling-up pretraining data is made even more challenging by the fact LLMs are commonly trained using a mixture of web crawls and so-called “high-quality” data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>. Typical high-quality corpora include curated sources of books, technical documents, human-selected web pages, or social media conversations. The increased diversity and quality brought forth by these curated corpora is believed to be a key component of performant models <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>. Unfortunately, curation is labour intensive: typically, each source requires specialized processing, while yielding a limited amount of data. Furthermore, licensed sources raise legal challenges.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Nevertheless, most pretraining data is still sourced from massive web crawls which can be scaled up to trillions of tokens with limited human intervention. However, the quality of this data has traditionally been seen as (much) inferior to that of the manually curated data sources. Even finely processed sources of web data, such as C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite> or OSCAR <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, are regarded as inferior to curated corpora for LLMs <cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>; Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>, producing less performant models.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To sustain the ever-increasing data needs of larger and larger LLMs, and to streamline data pipelines and reduce the need for human-intensive curation, we propose to explore how web data can be better processed to significantly improve its quality, resulting in models as capable, if not more capable, than models trained on curated corpora.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Contributions.</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">We make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="color:#DB57B2;">RefinedWeb</span>, a high-quality five trillion tokens web-only English pretraining dataset;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We demonstrate that <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">web data alone can result in models outperforming both public and private curated corpora</span>, as captured by zero-shot benchmarks, challenging current views about data quality;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">We publicly release a 600B tokens extract of RefinedWeb, and 1/7B parameters LLMs trained on it</span>, to serve as a new baseline high-quality web dataset for the natural language processing community.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pretraining data for large language models.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Early large language models identified the importance of datasets with long, coherent documents <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib61" title="" class="ltx_ref">2018</a>; Devlin et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>. Moving on from the previously used sentence-wise datasets <cite class="ltx_cite ltx_citemacro_citep">(Chelba et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2013</a>)</cite>, they instead leveraged document-focused, single-domain corpora like Wikipedia or BookCorpus <cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a href="#bib.bib88" title="" class="ltx_ref">2015</a>)</cite>. As models increased in scale, datasets based on massive web-scrape gained prevalence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2019</a>; Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>. However, further work argued that these untargeted web scrape fell short of human-curated data <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib62" title="" class="ltx_ref">2019</a>)</cite>, leading to the wide adoption of curated datasets such as The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>, which combine web data with books, technical articles, and social media conversations. At scale, it has been proposed to emulate the human curation process by leveraging weak signals: for instance, by crawling the top links of a forum&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gokaslan et&nbsp;al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>. Targeted corpora can also produce domain-specific models <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>, or broaden the expressiveness of models (e.g., for conversational modalities <cite class="ltx_cite ltx_citemacro_citet">Adiwardana et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>); Thoppilan et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2022</a>)</cite>). Latest large language models <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>; Chowdhery et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>; Scao et&nbsp;al., <a href="#bib.bib66" title="" class="ltx_ref">2022a</a>)</cite> are trained on giant aggregated corpora, combining both massive web-scrape and so-called “high-quality” curated single-domain sources (e.g., news, books, technical papers, social media conversations). These targeted sources are often upsampled–from one to five times is most common–to increase their representation in the final dataset. The diversity and “higher-quality” brought fourth by these aggregated datasets is thought to be central to model quality; web data alone is considered insufficient to train powerful large language models <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a href="#bib.bib51" title="" class="ltx_ref">2019</a>; Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pipelines for web data.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Massive web datasets are typically built upon CommonCrawl, a publicly available scrape of the internet, which has now been running for 12 years and has collected petabytes of data. Working with data scraped from all over the internet presents unique challenges: notably, a significant portion is low-quality machine-generated spam or pornographic content <cite class="ltx_cite ltx_citemacro_citep">(Trinh &amp; Le, <a href="#bib.bib73" title="" class="ltx_ref">2018</a>; Kreutzer et&nbsp;al., <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite>. Accordingly, training on unfiltered web data is undesirable, resulting in poorly performing models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>. Modern pipelines focus on filtering out this undesirable content <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite>. Broadly speaking, these pipelines usually combine a variety of stages: (1) <em id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">language identification</em>, leveraging inexpensive n-gram models (e.g., fastText <cite class="ltx_cite ltx_citemacro_citet">Joulin et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2016</a>)</cite>); (2)&nbsp;<em id="S2.SS0.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">filtering rules and heuristics</em>, such as only keeping lines with valid punctuation, discarding lines with too many symbols, or removing documents containing banned words <cite class="ltx_cite ltx_citemacro_citep">(Grave et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2018</a>; Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>; (3) <em id="S2.SS0.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">ML-based quality filtering</em>, using lightweight models trained on known gold data to identify similar high-quality web documents <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2020</a>; Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>; (4) <em id="S2.SS0.SSS0.Px2.p1.1.4" class="ltx_emph ltx_font_italic">deduplication</em>, removing either exact duplicate spans or similar documents&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. While some filtering is necessary, excessive filtering can introduce undesirable biases in the model. This can overly impact minorities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dodge et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, motivating the adoption of practices such as pseudo-crawling, wherein allowed URLs are manually curated&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Laurençon et&nbsp;al., <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deduplication.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Deduplication removes repeated extracts and documents from a dataset: these could either be exact matches, identical in every character, or approximate matches, based on some similarity metric. For exact duplicates, it is common to match exact substrings of a minimum length using suffix arrays <cite class="ltx_cite ltx_citemacro_citep">(Manber &amp; Myers, <a href="#bib.bib53" title="" class="ltx_ref">1993</a>)</cite>. For fuzzy duplicates, methods based on locally-sensitive hashes such as MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a href="#bib.bib17" title="" class="ltx_ref">1997</a>)</cite> or SimHash <cite class="ltx_cite ltx_citemacro_citep">(Charikar, <a href="#bib.bib21" title="" class="ltx_ref">2002</a>)</cite> have been adopted for the pretraining data of large language models <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Zeng et&nbsp;al., <a href="#bib.bib85" title="" class="ltx_ref">2021</a>; Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>. Recently, <cite class="ltx_cite ltx_citemacro_citet">Abbas et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> has proposed to leverage embeddings from pretrained models to imbue semantic understanding in approximate matching algorithms. Deduplication has been identified as playing a significant role in improving language models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Allamanis, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>; Lee et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. Notably, it reduces memorization&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, which is especially problematic in large models <cite class="ltx_cite ltx_citemacro_citep">(Carlini et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. Furthermore, repeated data has been shown to be increasingly harmful to model quality as parameter count increases&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hernandez et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>: for a 1B parameters model, a hundred duplicates are harmful; at 175B, even a few duplicates could have a disproportionate effect. Concurrently to this work, the Pythia suite of models found that deduplicating The Pile had a limited impact on zero-shot performance <cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, questioning whether deduplication is as relevant for curated corpora as it for predominantly web-based datasets.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">We provide an overview of some widely adopted existing pretraining English datasets for LLMs in <a href="#S0.T1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>, with additional information in <a href="#A6.T12" title="In F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">12</span></a> of <a href="#A6.SS3" title="F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">F.3</span></a>. We also note that recent popular open models <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib87" title="" class="ltx_ref">2022</a>; Touvron et&nbsp;al., <a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite> often indirectly leverage The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> by doing a mix-and-match of its components.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p3.1" class="ltx_p">Focusing on building a large-scale high-quality web pretraining dataset, we extend upon the state-of-the-art in three ways: (1) we aggregate and combine best-practices for document preparation and filtering across multiple pipelines, and introduce line-wise corrections; (2) we combine both exact and fuzzy deduplication at very large-scale; (3) the scale of our final dataset is unique, with a total 5,000 billion tokens, and a 600 billion tokens extract available for public use with permissive licensing. Training large models on RefinedWeb also lead us to challenge the commonly held belief that web data is strictly worse than curated corpora.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Macrodata Refinement and RefinedWeb</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We introduce <span id="S3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">MDR</span> (MacroData Refinement), a pipeline for filtering and deduplicating web data from CommonCrawl at very large scale. Using MDR, we produce <span id="S3.p1.1.2" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="color:#DB57B2;">RefinedWeb</span>, an English pretraining dataset of five trillion tokens based on web data only. We leverage strict filtering and stringent deduplication to uplift the quality of web data, distilling it down to a corpus matching the quality of aggregated corpora used to train state-of-the-art models.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Design principles.</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">We abide by the following guidelines:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Scale first.</span> We intend MDR to produce datasets to be used to train 40-200B parameters models, thus requiring trillions of tokens <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>. For English-only RefinedWeb, we target a size of 3-6 trillion tokens. Specifically, we eschew any labour intensive human curation process, and focus on CommonCrawl instead of disparate single-domain sources.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Strict deduplication.</span> Inspired by the work of <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, which demonstrated the value of deduplication for large language models, we implement a rigorous deduplication pipeline. We combine both exact and fuzzy deduplication, and use strict settings leading to removal rates far higher than others have reported.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Neutral filtering.</span> To avoid introducing further undesirable biases into the model <cite class="ltx_cite ltx_citemacro_citep">(Dodge et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Welbl et&nbsp;al., <a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite>, we avoid using ML-based filtering outside of language identification. We stick to simple rules and heuristics, and use only URL filtering for adult content.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p"><a href="#S3.T2" title="In Language identification. ‣ 3.1 Document preparation: reading data, filtering URLs, extracting text, and language identification ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F2" title="In Design principles. ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> outline the full MDR pipeline.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.01116/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.4.1" class="ltx_text ltx_font_bold">Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl.</span> Notably, filtering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English, 24% of remaining for being of insufficient quality, and 12% for being duplicates. We report removal rate (<span id="S3.F2.5.2" class="ltx_text" style="color:#808080;">grey</span>) with respect to each previous stage, and kept rate (<span id="S3.F2.6.3" class="ltx_text" style="color:#DB57B2;">shade</span>) overall. Rates measured in % of documents in the document preparation phase, then in tokens.</figcaption>
</figure>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Document preparation: reading data, filtering URLs, extracting text, and language identification</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Reading the data.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">CommonCrawl is available in either WARC (raw HTML response), or WET files (preprocessed to only include plain text). Individual files correspond to a page at a given URL; these constitute single documents/samples. Working with WET files would spare us from running our own HTML extraction; however, in line with previous works <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>, we found WET files to include undesirable navigation menus, ads, and other irrelevant texts. Accordingly, our pipeline starts from raw WARC files, read with the <span id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">warcio</span> library.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">URL filtering.</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Before undertaking any compute-heavy processing, we perform a first filtering based on the URL alone. This targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.). We base our filtering on two rules: (1) an aggregated blocklist of 4.6M domains; (2) a URL score, based on the presence of words from a list we curated and weighed by severity. We found that commonly used blocklists include many false positives, such as popular blogging platforms or even pop culture websites. Furthermore, word-based rules (like the one used in C4, <cite class="ltx_cite ltx_citemacro_citet">Raffel et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>) can easily result in medical and legal pages being blocked. Our final detailed rules based on this investigation are shared in <a href="#A7.SS1" title="G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1</span></a>. Since we intend RefinedWeb to be used as part of an aggregate dataset along with curated corpora, we also filtered common sources of high-quality data: Wikipedia, arXiv, etc. The detailed list is available in <a href="#A7.SS1.SSS3" title="G.1.3 Excluded High Quality Sources ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1.3</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text extraction.</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">We want to extract only the main content of the page, ignoring menus, headers, footers, and ads among others: <cite class="ltx_cite ltx_citemacro_citet">Lopukhin (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite> found that <span id="S3.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">trafilatura</span> <cite class="ltx_cite ltx_citemacro_citep">(Barbaresi, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> was the best non-commercial library for retrieving content from blog posts and news articles. Although this is only a narrow subset of the kind of pages making up CommonCrawl, we found this finding to hold more broadly. We use <span id="S3.SS1.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_typewriter">trafilatura</span> for text extraction, and apply extra formatting via regular expressions: we limit new lines to two consecutive ones, and remove all URLs.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Language identification.</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">We use the fastText language classifier of CCNet <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite> at the document-level: it uses characters n-gram and was trained on Wikipedia, supporting 176 languages. We remove documents for which the top language scores below 0.65: this usually corresponds to pages without any natural text. For this paper, we focus on English; RefinedWeb can also be derived for other languages, see <a href="#A4" title="Appendix D Multilingual RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">D</span></a> for details.</p>
</div>
<div id="S3.SS1.SSS0.Px4.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p2.1" class="ltx_p">The data we retrieve at this stage, called <span id="S3.SS1.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="color:#5E57D3;">RW-Raw</span>, corresponds to what we can extract with the minimal amount of filtering. At this stage, only 48% of the original documents are left, mostly filtered out by language identification.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S3.T2.2.1" class="ltx_text ltx_font_bold">Macrodata Refinement aggregates best practices from the state-of-the-art and novel approaches (URL scoring, line-wise filtering, etc.) to produce high-quality web data.</span> On deduplication, we note that MDR is unique in both the scale at which it is performed, and in applying subsequently fuzzy and exact substring methods to improve coverage and scalability.</figcaption>
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T2.3.1" class="ltx_tr">
<td id="S3.T2.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="3"><span id="S3.T2.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:70%;">Document preparation</span></td>
<td id="S3.T2.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="2"><span id="S3.T2.3.1.2.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:70%;">Filtering</span></td>
<td id="S3.T2.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="2"><span id="S3.T2.3.1.3.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:70%;">Deduplication</span></td>
</tr>
<tr id="S3.T2.3.2" class="ltx_tr">
<td id="S3.T2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">URL filtering</span></span>
</span>
</td>
<td id="S3.T2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Text extraction</span></span>
</span>
</td>
<td id="S3.T2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.3.1.1.1" class="ltx_text"></span><span id="S3.T2.3.2.3.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S3.T2.3.2.3.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S3.T2.3.2.3.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.3.2.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.3.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></span></span>
<span id="S3.T2.3.2.3.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.3.2.3.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.3.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">identification</span></span></span>
</span></span><span id="S3.T2.3.2.3.1.1.3" class="ltx_text"></span><span id="S3.T2.3.2.3.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.T2.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.4.1.1.1" class="ltx_text"></span><span id="S3.T2.3.2.4.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S3.T2.3.2.4.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S3.T2.3.2.4.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.3.2.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.4.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Document-wise</span></span></span>
<span id="S3.T2.3.2.4.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.3.2.4.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.4.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">filtering</span></span></span>
</span></span><span id="S3.T2.3.2.4.1.1.3" class="ltx_text"></span><span id="S3.T2.3.2.4.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.T2.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.5.1.1.1" class="ltx_text"></span><span id="S3.T2.3.2.5.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S3.T2.3.2.5.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S3.T2.3.2.5.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.3.2.5.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.5.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Line-wise</span></span></span>
<span id="S3.T2.3.2.5.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.3.2.5.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.5.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">filtering</span></span></span>
</span></span><span id="S3.T2.3.2.5.1.1.3" class="ltx_text"></span><span id="S3.T2.3.2.5.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.T2.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.6.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Deduplication</span></span>
</span>
</td>
<td id="S3.T2.3.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">URL deduplication</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Aggregated blocklist, URL scoring, common HQ sources blocked</span></span>
</span>
</td>
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.2.1.1.1" class="ltx_text" style="font-size:70%;">From WARC using </span><span id="S3.T2.3.3.2.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:70%;">warcio</span><span id="S3.T2.3.3.2.1.1.3" class="ltx_text" style="font-size:70%;">, </span><span id="S3.T2.3.3.2.1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:70%;">trafilatura</span><span id="S3.T2.3.3.2.1.1.5" class="ltx_text" style="font-size:70%;"> for extraction</span></span>
</span>
</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.3.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">fastText</span><span id="S3.T2.3.3.3.1.1.2" class="ltx_text" style="font-size:70%;"> classifier from CCNet, thresholding on top language score</span></span>
</span>
</td>
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.4.1.1.1" class="ltx_text" style="font-size:70%;">In-document repetition removal and quality heuristics from MassiveWeb</span></span>
</span>
</td>
<td id="S3.T2.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.5.1.1.1" class="ltx_text" style="font-size:70%;">Remove undesirable lines (call to actions, navigation buttons, social counters, etc.)</span></span>
</span>
</td>
<td id="S3.T2.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.6.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.6.1.1.1" class="ltx_text" style="font-size:70%;">Fuzzy deduplication w/ MinHash + exact substring deduplication w/ suffix arrays</span></span>
</span>
</td>
<td id="S3.T2.3.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.7.1.1.1" class="ltx_text" style="font-size:70%;">Remove URLs revisited across CommonCrawl dumps</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.1.1.1" class="ltx_p" style="width:56.9pt;"><a href="#A7.SS1" title="G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref" style="font-size:70%;"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1</span></a></span>
</span>
</td>
<td id="S3.T2.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.2.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Barbaresi <span id="S3.T2.3.4.2.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib11" title="" class="ltx_ref">2021</a><span id="S3.T2.3.4.2.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.3.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Wenzek et&nbsp;al. <span id="S3.T2.3.4.3.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib81" title="" class="ltx_ref">2020</a><span id="S3.T2.3.4.3.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.4.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Rae et&nbsp;al. <span id="S3.T2.3.4.4.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib63" title="" class="ltx_ref">2021</a><span id="S3.T2.3.4.4.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.5.1.1" class="ltx_p" style="width:56.9pt;"><a href="#A7.SS2" title="G.2 Line-wise filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref" style="font-size:70%;"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.2</span></a></span>
</span>
</td>
<td id="S3.T2.3.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.6.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. <span id="S3.T2.3.4.6.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib49" title="" class="ltx_ref">2022</a><span id="S3.T2.3.4.6.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.7.1.1" class="ltx_p" style="width:56.9pt;"><a href="#S3.SS3" title="3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref" style="font-size:70%;"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.3</span></a></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Filtering: document-wise and line-wise</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Repetition removal.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Due to crawling errors and low-quality sources, many documents contain repeated sequences: this may cause pathological behavior in the final model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Holtzman et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>. We could catch this content at the later deduplication stage, but it is cheaper and easier to catch it document-wise early on. We implement the heuristics of <cite class="ltx_cite ltx_citemacro_citet">Rae et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>, and remove any document with excessive line, paragraph, or n-gram repetitions.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Document-wise filtering.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">A significant fraction of pages are machine-generated spam, made predominantly of lists of keywords, boilerplate text, or sequences of special characters. Such documents are not suitable for language modeling; to filter them out, we adopt the quality filtering heuristics of <cite class="ltx_cite ltx_citemacro_citet">Rae et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>. These focus on removing outliers in terms of overall length, symbol-to-word ratio, and other criteria ensuring the document is actual natural language. We note that these filters have to be adapted on a per language basis, as they may result in overfiltering if naively transferred from English to other languages.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Line-wise corrections.</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Despite the improvements brought forth by using <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">trafilatura</span> instead of relying on preprocessed files, many documents remain interlaced with undesirable lines (e.g., social media counters <span id="S3.SS2.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">3 likes</span>, navigation buttons). Accordingly, we devised a line-correction filter, targeting these undesirable items. If these corrections remove more than 5% of a document, we remove it entirely. See <a href="#A7.SS2" title="G.2 Line-wise filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.2</span></a> for details.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p2.1" class="ltx_p">The data we retrieve at this stage has gone through all of the filtering heuristics in the MDR pipeline. We refer to this dataset as <span id="S3.SS2.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="color:#B55DD4;">RW-Filtered</span>. Only 23% of the documents of CommonCrawl are left, with around 50% of the documents of RW-Raw removed by the filtering.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Deduplication: fuzzy, exact, and across dumps</h3>

<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S3.T3.10.1" class="ltx_text ltx_font_bold">To evaluate models trained on RefinedWeb and compare to the state-of-the-art, we build four aggregates across 18 tasks on which to measure zero-shot performance.</span> <span id="S3.T3.11.2" class="ltx_text ltx_font_typewriter">small</span> was built for internal ablations, based on tasks with consistent performance at small scale, <span id="S3.T3.12.3" class="ltx_text ltx_font_typewriter">core</span> is based on tasks commonly reported for public suites of models <cite class="ltx_cite ltx_citemacro_citep">(Dey et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>; Biderman et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, <span id="S3.T3.13.4" class="ltx_text ltx_font_typewriter">main</span> is based on tasks from the GPT-3 and PaLM paper <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Chowdhery et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, and <span id="S3.T3.14.5" class="ltx_text ltx_font_typewriter">ext</span> is based on tasks used by the BigScience Architecture and Scaling group <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>. For all results reported, we flag with <math id="S3.T3.3.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S3.T3.3.m1.1b"><mo id="S3.T3.3.m1.1.1" xref="S3.T3.3.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.m1.1c"><ci id="S3.T3.3.m1.1.1.cmml" xref="S3.T3.3.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.m1.1d">\dagger</annotation></semantics></math> results obtained in an arbitrary evaluation setup, and with <math id="S3.T3.4.m2.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.T3.4.m2.1b"><mo id="S3.T3.4.m2.1.1" xref="S3.T3.4.m2.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.m2.1c"><times id="S3.T3.4.m2.1.1.cmml" xref="S3.T3.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.m2.1d">*</annotation></semantics></math> results obtained with the EAI Harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>, which we also employ for all our models.</figcaption>
<table id="S3.T3.15" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T3.15.1" class="ltx_tr">
<td id="S3.T3.15.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.15.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Tasks</span></td>
<td id="S3.T3.15.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.15.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Type</span></td>
<td id="S3.T3.15.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Random</span></td>
<td id="S3.T3.15.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">small</span></td>
<td id="S3.T3.15.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">core</span></td>
<td id="S3.T3.15.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.6.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">main</span></td>
<td id="S3.T3.15.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.7.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">ext</span></td>
</tr>
<tr id="S3.T3.15.2" class="ltx_tr">
<td id="S3.T3.15.2.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T3.15.2.1.1" class="ltx_text" style="font-size:70%;">HellaSwag </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.2.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Zellers et&nbsp;al.<span id="S3.T3.15.2.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib84" title="" class="ltx_ref">2019</a><span id="S3.T3.15.2.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.15.2.2.1" class="ltx_text" style="font-size:70%;">Sentence completion</span></td>
<td id="S3.T3.15.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.3" class="ltx_tr">
<td id="S3.T3.15.3.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.3.1.1" class="ltx_text" style="font-size:70%;">LAMBADA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.3.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Paperno et&nbsp;al.<span id="S3.T3.15.3.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib58" title="" class="ltx_ref">2016</a><span id="S3.T3.15.3.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.3.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.3.2.1" class="ltx_text" style="font-size:70%;">Sentence completion</span></td>
<td id="S3.T3.15.3.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.3.1" class="ltx_text" style="font-size:70%;">0.0</span></td>
<td id="S3.T3.15.3.4" class="ltx_td"></td>
<td id="S3.T3.15.3.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.3.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.3.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.4" class="ltx_tr">
<td id="S3.T3.15.4.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.4.1.1" class="ltx_text" style="font-size:70%;">Winogrande </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.4.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Sakaguchi et&nbsp;al.<span id="S3.T3.15.4.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib65" title="" class="ltx_ref">2021</a><span id="S3.T3.15.4.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.4.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.4.2.1" class="ltx_text" style="font-size:70%;">Coreference resolution</span></td>
<td id="S3.T3.15.4.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.4.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.4.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.4.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.4.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.5" class="ltx_tr">
<td id="S3.T3.15.5.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.5.1.1" class="ltx_text" style="font-size:70%;">PIQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.5.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Bisk et&nbsp;al.<span id="S3.T3.15.5.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2020</a><span id="S3.T3.15.5.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.5.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.5.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.5.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.5.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.5.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.5.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.5.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.6" class="ltx_tr">
<td id="S3.T3.15.6.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.6.1.1" class="ltx_text" style="font-size:70%;">ARC </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.6.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Clark et&nbsp;al.<span id="S3.T3.15.6.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib25" title="" class="ltx_ref">2018</a><span id="S3.T3.15.6.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.6.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.6.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.6.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.6.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.6.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.6.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.6.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.7" class="ltx_tr">
<td id="S3.T3.15.7.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.7.1.1" class="ltx_text" style="font-size:70%;">OpenBookQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.7.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Mihaylov et&nbsp;al.<span id="S3.T3.15.7.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib54" title="" class="ltx_ref">2018</a><span id="S3.T3.15.7.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.7.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.7.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.7.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.7.4" class="ltx_td"></td>
<td id="S3.T3.15.7.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.7.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.7.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.8" class="ltx_tr">
<td id="S3.T3.15.8.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.8.1.1" class="ltx_text" style="font-size:70%;">BoolQ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.8.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Clark et&nbsp;al.<span id="S3.T3.15.8.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2019</a><span id="S3.T3.15.8.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.8.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.8.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.8.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.8.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.8.5" class="ltx_td"></td>
<td id="S3.T3.15.8.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.8.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.9" class="ltx_tr">
<td id="S3.T3.15.9.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.9.1.1" class="ltx_text" style="font-size:70%;">COPA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.9.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Gordon et&nbsp;al.<span id="S3.T3.15.9.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2012</a><span id="S3.T3.15.9.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.9.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.9.2.1" class="ltx_text" style="font-size:70%;">Sentence completion</span></td>
<td id="S3.T3.15.9.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.9.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.9.4" class="ltx_td"></td>
<td id="S3.T3.15.9.5" class="ltx_td"></td>
<td id="S3.T3.15.9.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.9.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.9.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.9.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.10" class="ltx_tr">
<td id="S3.T3.15.10.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.10.1.1" class="ltx_text" style="font-size:70%;">CB </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.10.1.2.1" class="ltx_text" style="font-size:70%;">(</span>De&nbsp;Marneffe et&nbsp;al.<span id="S3.T3.15.10.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib28" title="" class="ltx_ref">2019</a><span id="S3.T3.15.10.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.10.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.10.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.10.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.10.3.1" class="ltx_text" style="font-size:70%;">33.3</span></td>
<td id="S3.T3.15.10.4" class="ltx_td"></td>
<td id="S3.T3.15.10.5" class="ltx_td"></td>
<td id="S3.T3.15.10.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.10.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.10.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.10.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.11" class="ltx_tr">
<td id="S3.T3.15.11.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.11.1.1" class="ltx_text" style="font-size:70%;">RTE </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.11.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Dagan et&nbsp;al.<span id="S3.T3.15.11.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib26" title="" class="ltx_ref">2010</a><span id="S3.T3.15.11.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.11.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.11.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.11.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.11.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.11.4" class="ltx_td"></td>
<td id="S3.T3.15.11.5" class="ltx_td"></td>
<td id="S3.T3.15.11.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.11.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.11.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.11.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.12" class="ltx_tr">
<td id="S3.T3.15.12.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.12.1.1" class="ltx_text" style="font-size:70%;">ReCoRD </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.12.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Zhang et&nbsp;al.<span id="S3.T3.15.12.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib86" title="" class="ltx_ref">2018</a><span id="S3.T3.15.12.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.12.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.12.2.1" class="ltx_text" style="font-size:70%;">Question answering</span></td>
<td id="S3.T3.15.12.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.12.3.1" class="ltx_text" style="font-size:70%;">0.0</span></td>
<td id="S3.T3.15.12.4" class="ltx_td"></td>
<td id="S3.T3.15.12.5" class="ltx_td"></td>
<td id="S3.T3.15.12.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.12.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.12.7" class="ltx_td"></td>
</tr>
<tr id="S3.T3.15.13" class="ltx_tr">
<td id="S3.T3.15.13.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.13.1.1" class="ltx_text" style="font-size:70%;">ANLI </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.13.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Nie et&nbsp;al.<span id="S3.T3.15.13.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib56" title="" class="ltx_ref">2019</a><span id="S3.T3.15.13.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.13.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.13.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.13.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.13.3.1" class="ltx_text" style="font-size:70%;">33.3</span></td>
<td id="S3.T3.15.13.4" class="ltx_td"></td>
<td id="S3.T3.15.13.5" class="ltx_td"></td>
<td id="S3.T3.15.13.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.13.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.13.7" class="ltx_td"></td>
</tr>
<tr id="S3.T3.15.14" class="ltx_tr">
<td id="S3.T3.15.14.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.14.1.1" class="ltx_text" style="font-size:70%;">LogiQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.14.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Liu et&nbsp;al.<span id="S3.T3.15.14.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib50" title="" class="ltx_ref">2021</a><span id="S3.T3.15.14.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.14.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.14.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.14.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.14.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.14.4" class="ltx_td"></td>
<td id="S3.T3.15.14.5" class="ltx_td"></td>
<td id="S3.T3.15.14.6" class="ltx_td"></td>
<td id="S3.T3.15.14.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.14.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.15" class="ltx_tr">
<td id="S3.T3.15.15.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.15.1.1" class="ltx_text" style="font-size:70%;">HeadQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.15.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Vilares &amp; Gómez-Rodríguez<span id="S3.T3.15.15.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib74" title="" class="ltx_ref">2019</a><span id="S3.T3.15.15.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.15.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.15.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.15.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.15.3.1" class="ltx_text" style="font-size:70%;">20.0</span></td>
<td id="S3.T3.15.15.4" class="ltx_td"></td>
<td id="S3.T3.15.15.5" class="ltx_td"></td>
<td id="S3.T3.15.15.6" class="ltx_td"></td>
<td id="S3.T3.15.15.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.15.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.16" class="ltx_tr">
<td id="S3.T3.15.16.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.16.1.1" class="ltx_text" style="font-size:70%;">MathQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.16.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Amini et&nbsp;al.<span id="S3.T3.15.16.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib8" title="" class="ltx_ref">2019</a><span id="S3.T3.15.16.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.16.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.16.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.16.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.16.3.1" class="ltx_text" style="font-size:70%;">20.0</span></td>
<td id="S3.T3.15.16.4" class="ltx_td"></td>
<td id="S3.T3.15.16.5" class="ltx_td"></td>
<td id="S3.T3.15.16.6" class="ltx_td"></td>
<td id="S3.T3.15.16.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.16.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.17" class="ltx_tr">
<td id="S3.T3.15.17.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.17.1.1" class="ltx_text" style="font-size:70%;">PROST </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.17.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Aroca-Ouellette et&nbsp;al.<span id="S3.T3.15.17.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib9" title="" class="ltx_ref">2021</a><span id="S3.T3.15.17.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.17.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.17.2.1" class="ltx_text" style="font-size:70%;">Paraphrase identification</span></td>
<td id="S3.T3.15.17.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.17.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.17.4" class="ltx_td"></td>
<td id="S3.T3.15.17.5" class="ltx_td"></td>
<td id="S3.T3.15.17.6" class="ltx_td"></td>
<td id="S3.T3.15.17.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.17.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.18" class="ltx_tr">
<td id="S3.T3.15.18.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.18.1.1" class="ltx_text" style="font-size:70%;">PubMedQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.18.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Jin et&nbsp;al.<span id="S3.T3.15.18.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib44" title="" class="ltx_ref">2019</a><span id="S3.T3.15.18.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.18.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.18.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.18.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.18.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.18.4" class="ltx_td"></td>
<td id="S3.T3.15.18.5" class="ltx_td"></td>
<td id="S3.T3.15.18.6" class="ltx_td"></td>
<td id="S3.T3.15.18.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.18.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.19" class="ltx_tr">
<td id="S3.T3.15.19.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T3.15.19.1.1" class="ltx_text" style="font-size:70%;">SciQ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.19.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Welbl et&nbsp;al.<span id="S3.T3.15.19.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib79" title="" class="ltx_ref">2017</a><span id="S3.T3.15.19.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.19.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T3.15.19.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.19.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.15.19.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.19.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.15.19.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.19.5" class="ltx_td ltx_border_bb"></td>
<td id="S3.T3.15.19.6" class="ltx_td ltx_border_bb"></td>
<td id="S3.T3.15.19.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.15.19.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
</tbody></table>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">After filtering, although data quality has improved, a large fraction of the content is repeated across documents. This may be due to the crawler indirectly hitting the same page multiple times, to boilerplate content being repeated (e.g., licences), or even to plagiarism. These duplicates can strongly impact models, favoring memorization instead of generalization&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>; Hernandez et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>. Since deduplication is expensive, it has seen limited adoption in public datasets <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2019</a>; Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>. We adopt an aggressive deduplication strategy, combining both fuzzy document matches and exact sequences removal.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fuzzy deduplication.</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">We remove similar documents by applying MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a href="#bib.bib17" title="" class="ltx_ref">1997</a>)</cite>: for each document, we compute a sketch and measure its approximate similarity with other documents, eventually removing pairs with high overlap. MinHash excels at finding templated documents: licenses with only specific entities differing, placeholder SEO text repeated across websites–see examples of the biggest clusters in <a href="#A8.SS1" title="H.1 MinHash clusters ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">H.1</span></a>. We perform MinHash deduplication using 9,000 hashes per document, calculated over 5-grams and divided into 20 buckets of 450 hashes. We found that using less aggressive settings, such as the 10 hashes of The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>, resulted in lower deduplication rates and worsened model performance. See <a href="#A7.SS3.SSS1" title="G.3.1 MinHash Approximate Matching ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.3.1</span></a> for more details about our MinHash setup.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Exact deduplication.</h5>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">Exact substring operates at the sequence-level instead of the document-level, finding matches between strings that are exact token-by-token matches by using a suffix array <cite class="ltx_cite ltx_citemacro_citep">(Manber &amp; Myers, <a href="#bib.bib53" title="" class="ltx_ref">1993</a>)</cite> (e.g., specific disclaimers or notices, which may not compromise the entire document as showcased in <a href="#A8.SS2" title="H.2 Exact substring matches ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">H.2</span></a>). We remove any match of more than 50 consecutive tokens, using the implementation of <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. We note that exact substring alters documents, by removing specific spans: we also experimented with dropping entire documents or loss-masking the duplicated strings instead of cutting them, but this didn’t result in significant changes in zero-shot performance–see <a href="#A7.SS3.SSS2" title="G.3.2 Exact substring deduplication ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.3.2</span></a>.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">URL deduplication.</h5>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">Because of computational constraints, it is impossible for us to perform deduplication directly on RW-Filtered. Instead, we split CommonCrawl into 100 parts, where each part contains a hundredth of each dump, and perform deduplication on individual parts. Most of the larger duplicate clusters (e.g., licences, common spams) will be shared across parts, and effectively removed. However, we found that CommonCrawl dumps had significant overlap, with URLs being revisited across dumps despite no change in content. Accordingly, we keep a list of the URLs of all samples we have kept from each part, and remove them from subsequent parts being processed.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now validate that RefinedWeb can be used to train powerful models, matching the zero-shot performance obtained with curated corpora and state-of-the-art language models. We first discuss our evaluation and pretraining setup, and models with which we compare. We perform experiments at small scale to internally compare with other popular datasets, and ablate the three main stages of RefinedWeb (raw, filtered, final). Then, we scale to 1B and 7B models trained on 350GT to compare with state-of-the-art models. Finally, we apply the MDR pipeline to existing pretraining datasets, and show that it can potentially deliver further improvements.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setting</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Evaluation.</span> At variance with previous works studying pretraining datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>; Lee et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, we focus our evaluation on zero-shot generalization across many tasks rather than measuring validation loss. Perplexity alone can be at odds with end-task performance <cite class="ltx_cite ltx_citemacro_citep">(Tay et&nbsp;al., <a href="#bib.bib70" title="" class="ltx_ref">2021</a>)</cite>, and modern works on LLMs predominantly report zero-shot performance <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>; Chowdhery et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>. Furthermore, zero-shot generalization is the “natural” setting for autoregressive decoder-only models, in which they perform best <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib77" title="" class="ltx_ref">2022</a>)</cite>. Our evaluation setup is inspired by the one used by the architecture and scaling group of Big Science <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We base our evaluation on the popular Eleuther AI evaluation harness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>, allowing us to evaluate across a wide range of tasks in the zero-shot setting. We identified aggregates of tasks allowing us to: (1) obtain signal (i.e., non zero zero-shot performance) at small scale for ablations; (2) compare with results reported by other models. We outline these four aggregates <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">small</span> (for ablations), and <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">core</span>, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">main</span>, <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">ext</span> (for comparisons) in <a href="#S3.T3" title="In 3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.2" class="ltx_p">Comparisons across models trained and evaluated in different settings are difficult to untangle, as many externalities may influence the 1
987results (e.g., numerical precision of training vs inference, prompts used). We distinguish three levels of comparisons: (1) internal
comparisons, with models trained and evaluated within our codebase, for which only the pretraining datasets differ; (2) benchmark-level comparisons, with models trained with a different codebase but evaluated with the Eleuther AI harness, taking results from&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>); Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>); Aleph Alpha (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>); Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, thereafter flagged with a <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><times id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">*</annotation></semantics></math>; (3) external comparisons with <cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Chowdhery et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, thereafter flagged with a <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mo id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\dagger</annotation></semantics></math>. For further details on evaluation, see <a href="#A6.SS1" title="F.1 Task aggregates ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">F.1</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.2.1" class="ltx_text ltx_font_bold">Curation is not a silver bullet for zero-shot generalization: small-scale models trained on <span id="S4.T4.2.1.1" class="ltx_text" style="color:#DB57B2;">●<span id="S4.T4.2.1.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span> outperform models trained on web data (C4, OSCAR), and on curated corpora (<math id="S4.T4.2.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.T4.2.1.m1.1b"><mi mathcolor="#7DD86E" mathvariant="normal" id="S4.T4.2.1.m1.1.1" xref="S4.T4.2.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.1.m1.1c"><ci id="S4.T4.2.1.m1.1.1.cmml" xref="S4.T4.2.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.1.m1.1d">\blacktriangledown</annotation></semantics></math><span id="S4.T4.2.1.2" class="ltx_text" style="color:#7DD86E;"> The Pile</span>).</span> Average accuracy in zero-shot on the <span id="S4.T4.5.2" class="ltx_text ltx_font_typewriter">small-agg</span> aggregate. All models trained with identical architectures and pretraining hyperparameters. We find that OSCAR-22.01 underperforms other datasets signficantly, perhaps because deduplication is only optional. C4 is a strong baseline, with OSCAR-21.09 lagging slightly behind, but we find that RefinedWeb outperforms both web datasets and the most popular curated dataset, The Pile. Both filtering and deduplication contribute significantly to improving zero-shot performance.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T4.3.2" class="ltx_tr">
<td id="S4.T4.3.2.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.3.2.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="3"><span id="S4.T4.3.2.2.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Massive web datasets</span></td>
<td id="S4.T4.3.2.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.3.2.3.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Curated</span></td>
<td id="S4.T4.3.2.4" class="ltx_td ltx_align_left ltx_border_tt" colspan="3"><span id="S4.T4.3.2.4.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Ours</span></td>
</tr>
<tr id="S4.T4.3.1" class="ltx_tr">
<td id="S4.T4.3.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">OSCAR-21.09</td>
<td id="S4.T4.3.1.4" class="ltx_td ltx_align_center ltx_border_t">OSCAR-22.01</td>
<td id="S4.T4.3.1.5" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T4.3.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T4.3.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.T4.3.1.1.m1.1a"><mi mathcolor="#7DD86E" mathvariant="normal" id="S4.T4.3.1.1.m1.1.1" xref="S4.T4.3.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.m1.1b"><ci id="S4.T4.3.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S4.T4.3.1.1.1" class="ltx_text" style="color:#7DD86E;"> The Pile</span>
</td>
<td id="S4.T4.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.6.1" class="ltx_text" style="color:#5E57D3;">RW-Raw</span></td>
<td id="S4.T4.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.7.1" class="ltx_text" style="color:#B55DD4;">RW-Filtered</span></td>
<td id="S4.T4.3.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.8.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">●<span id="S4.T4.3.1.8.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span></td>
</tr>
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.3.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.3.1.1" class="ltx_text ltx_font_bold">1B@27GT</span></td>
<td id="S4.T4.3.3.2" class="ltx_td ltx_align_center ltx_border_t">55.0%</td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_center ltx_border_t">55.7%</td>
<td id="S4.T4.3.3.5" class="ltx_td ltx_align_center ltx_border_t">53.4%</td>
<td id="S4.T4.3.3.6" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
<td id="S4.T4.3.3.7" class="ltx_td ltx_align_center ltx_border_t">54.3%</td>
<td id="S4.T4.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.3.8.1" class="ltx_text ltx_font_bold">56.2%</span></td>
</tr>
<tr id="S4.T4.3.4" class="ltx_tr">
<td id="S4.T4.3.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.4.1.1" class="ltx_text ltx_font_bold">3B@60GT</span></td>
<td id="S4.T4.3.4.2" class="ltx_td ltx_align_center ltx_border_bb">59.1%</td>
<td id="S4.T4.3.4.3" class="ltx_td ltx_align_center ltx_border_bb">55.9%</td>
<td id="S4.T4.3.4.4" class="ltx_td ltx_align_center ltx_border_bb">59.6%</td>
<td id="S4.T4.3.4.5" class="ltx_td ltx_align_center ltx_border_bb">57.9%</td>
<td id="S4.T4.3.4.6" class="ltx_td ltx_align_center ltx_border_bb">57.4%</td>
<td id="S4.T4.3.4.7" class="ltx_td ltx_align_center ltx_border_bb">58.2%</td>
<td id="S4.T4.3.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.4.8.1" class="ltx_text ltx_font_bold">59.8%</span></td>
</tr>
</tbody></table>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Models.</span> We train 1B, 3B, and 7B parameters autoregressive decoder-only models, based on configurations and hyperparameters similar to GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, diverging mostly on our use of ALiBi <cite class="ltx_cite ltx_citemacro_citep">(Press et&nbsp;al., <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite>. We use FlashAttention <cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> in a custom codebase. We train internal models on both The Pile and RefinedWeb to control for deviations caused by our pretraining setup–we found The Pile models to perform in-line with others. For small-scale and ablation studies (first half of <a href="#S4.SS2" title="4.2 Can web data alone outperform curated corpora? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>; <a href="#S4.SS3" title="4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.3</span></a>), we train models to optimality according to the scaling laws of <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et&nbsp;al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>:&nbsp;on 27B and 60B tokens respectively for our 1B and 3B parameters models. For the main experiments demonstrating our approach (Falcon-RW models in <a href="#S4.SS2" title="4.2 Can web data alone outperform curated corpora? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>), we train the models to 350GT, in line with popular public models <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Wang &amp; Komatsuzaki, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>; Scao et&nbsp;al., <a href="#bib.bib66" title="" class="ltx_ref">2022a</a>)</cite>. Note that we do not compare against the recently introduced LLaMA models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite>, as the smallest of them is trained on x2.5 more compute than our largest model, preventing a meaningful comparison from being made dataset-wise. For a more in-depth overview of the models and pretraining datasets with which we compare, see <a href="#A6" title="Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01116/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="158" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01116/assets/x4.png" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="207" height="162" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.13.1" class="ltx_text ltx_font_bold">Models trained on <span id="S4.F3.13.1.1" class="ltx_text" style="color:#DB57B2;">●<span id="S4.F3.13.1.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span> alone outperform models trained on curated corpora.</span> Zero-shot performance averaged on our&nbsp;<span id="S4.F3.14.2" class="ltx_text ltx_font_typewriter">core-agg</span> (left) and <span id="S4.F3.15.3" class="ltx_text ltx_font_typewriter">ext-agg</span> (right) task aggregates (see <a href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1</span></a> for details, and <a href="#S0.F1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> for results on <span id="S4.F3.16.4" class="ltx_text ltx_font_typewriter">main-agg</span>). Existing open models fail to match the performance of the original GPT-3 series (left); however, models trained on RefinedWeb significantly outperform models trained on <math id="S4.F3.5.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.F3.5.m1.1b"><mi mathcolor="#7DD86E" mathvariant="normal" id="S4.F3.5.m1.1.1" xref="S4.F3.5.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.F3.5.m1.1c"><ci id="S4.F3.5.m1.1.1.cmml" xref="S4.F3.5.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m1.1d">\blacktriangledown</annotation></semantics></math><span id="S4.F3.17.5" class="ltx_text" style="color:#7DD86E;"> The Pile</span>: including our direct comparison model (right), ruling out our pretraining setup as the main source of increased performance. In fact, our RefinedWeb models even match the performance of the <math id="S4.F3.6.m2.1" class="ltx_Math" alttext="\blacksquare" display="inline"><semantics id="S4.F3.6.m2.1b"><mi mathcolor="#5F57DB" mathvariant="normal" id="S4.F3.6.m2.1.1" xref="S4.F3.6.m2.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="S4.F3.6.m2.1c"><ci id="S4.F3.6.m2.1.1.cmml" xref="S4.F3.6.m2.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m2.1d">\blacksquare</annotation></semantics></math><span id="S4.F3.18.6" class="ltx_text" style="color:#5F57DB;"> GPT-3</span> models.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Can web data alone outperform curated corpora?</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We endeavour to demonstrate that web data alone can result in models outperforming other models trained on curated corpora. To do so, we first perform a small-scale study with 1B and 3B parameters models trained to optimality&nbsp;(27GT and 60GT) on popular web and curated datasets. Then, we scale up to 1B and 7B models trained on 350GT, and compare zero-shot generalization to state-of-the-art models.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Small-scale study.</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We first consider popular public web datasets (OSCAR-2019 <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et&nbsp;al., <a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, OSCAR-2022 <cite class="ltx_cite ltx_citemacro_citep">(Abadji et&nbsp;al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>, C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>), The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> as the most popular publicly available curated dataset, and variations of RefinedWeb (RW-Raw, RW-Filtered, and RW as described in <a href="#S3" title="3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>). For this first study, all models are trained with the same architecture and the same internal codebase; they are also all evaluated within the same framework–only pretraining datasets differ.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p2.1" class="ltx_p">Results averaged on the <span id="S4.SS2.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_typewriter">small-=+
</span> aggregate of 6 tasks are presented in <a href="#S4.T4" title="In 4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>. We observe relatively strong performance of all web datasets compared to The Pile, showcasing that curation is not a silver bullet for performant language models. We find C4 to be a strong pretraining dataset, in line with the findings of <cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>–however, The Pile comparatively underperforms more in our benchmarks. The relatively disappointing results on OSCAR-22.01 may be due to the main version of the dataset being distributed without deduplication. Regarding RefinedWeb, both filtering and deduplication significantly improve performance.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Full-scale models.</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.2" class="ltx_p">We now validate these results with comparisons with state-of-the-art models. We scale our previous experiments by training 1B and 7B models on 350GT; we also train a 1B model on 350GT on The Pile, as a control for the influence of our pretraining setup. We compare with the following models: the GPT-3 series <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, the FairSeq series <cite class="ltx_cite ltx_citemacro_citep">(Artetxe et&nbsp;al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>, the GPT-Neo(X)/J models <cite class="ltx_cite ltx_citemacro_citep">(Black et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>; Wang &amp; Komatsuzaki, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>; Black et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>, the OPT series <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib87" title="" class="ltx_ref">2022</a>)</cite>, the BigScience Architecture and Scaling Pile model <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>, PaLM-8B <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, Aleph Alpha Luminous 13B <cite class="ltx_cite ltx_citemacro_citep">(Aleph Alpha, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, the Pythia series <cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, and the Cerebras-GPT series <cite class="ltx_cite ltx_citemacro_citep">(Dey et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. For GPT-3, we distinguish between results obtained through the API (<span id="S4.SS2.SSS0.Px2.p1.2.1" class="ltx_text ltx_font_typewriter">babbage</span> and <span id="S4.SS2.SSS0.Px2.p1.2.2" class="ltx_text ltx_font_typewriter">curie</span>) with the the EleutherAI LM evaluation harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> (*), and results reported in their paper, with a different evaluation setup (<math id="S4.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">\dagger</annotation></semantics></math>). Note that for PaLM and OPT, results were also obtained with a different evaluation suite (<math id="S4.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.SS2.SSS0.Px2.p1.2.m2.1a"><mo id="S4.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.2.m2.1c">\dagger</annotation></semantics></math>), while for other models they were obtained with the evaluation harness as well (*), allowing for more direct comparisons.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p2.1" class="ltx_p">Results on <span id="S4.SS2.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_typewriter">main-agg</span> are presented in <a href="#S0.F1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>, and in <a href="#S4.F3" title="In 4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> for <span id="S4.SS2.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_typewriter">core-agg</span> and <span id="S4.SS2.SSS0.Px2.p2.1.3" class="ltx_text ltx_font_typewriter">ext-agg</span>. We find that open models consistently underperform models trained on private curated corpora, such as GPT-3–even when using a similar evaluation setup. Conversely, models trained on RefinedWeb are able to match the performance of the GPT-3 series using web data alone, even though common high-quality sources used in The Pile are excluded from RefinedWeb (see <a href="#A7.T14" title="In G.1.3 Excluded High Quality Sources ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">14</span></a> in Appendix). Finally, we note that our internal model trained on The Pile performs in line with the BigScience Architecture and Scaling model; this highlights that our pretraining setup is unlikely to be the main source of increased performance for models trained on RefinedWeb.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p3" class="ltx_para">
<span id="S4.SS2.SSS0.Px2.p3.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="S4.SS2.SSS0.Px2.p3.1.1" class="ltx_p"><span id="S4.SS2.SSS0.Px2.p3.1.1.1" class="ltx_text ltx_font_bold">Finding.</span> Challenging existing beliefs on data quality and LLMs, models trained on adequately filtered and deduplicated web data <em id="S4.SS2.SSS0.Px2.p3.1.1.2" class="ltx_emph ltx_font_italic">alone</em> can match the performance of models trained on curated data.</span>
</span>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Do other corpora benefit from MDR?</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Ablating the contributions and evaluating the performance of individual components in the MDR pipeline is difficult: for most heuristics, there is no agreed-upon ground truth, and changes may be too insignificant to result in sufficient zero-shot signal after pretraining. In the first half of <a href="#S4.SS2" title="4.2 Can web data alone outperform curated corpora? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>, we identified that subsequent stages of RefinedWeb (raw, filtered, final) led to improvements in performance. In this section, we propose to apply independently the filtering and deduplication stages of MDR to popular pretraining datasets, studying whether they generalize widely.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We report results on the <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">small-agg</span> in <a href="#S4.T5" title="In 4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>. First, we find that improvements from filtering are not systematic. On The Pile, we had to adjust our line length and characters ratio heuristics to avoid expunging books and code. Despite improvements on OSCAR-21.09, C4, and The Pile, our filters worsen performance on OSCAR-22.01; generally, removal rates from filtering do not seem strongly correlated with downstream accuracy. Conversely, deduplication delivers a steady boost across all datasets, and removal rates are better correlated with changes in performance. We find OSCAR-21.09 and C4 to be already well deduplicated, while The Pile and OSCAR-22.01 exhibit 40-60% duplicates. The base version of OSCAR-22.01 is distributed without deduplication; for The Pile, this is consistent with the findings of <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a href="#bib.bib87" title="" class="ltx_ref">2022</a>)</cite>. Finally, combining filtering and deduplication results in further improvements; interestingly, although performance is now more uniform across datasets, differences remain, suggesting that flaws in the original text extraction and processing can’t be fully compensated for.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">By processing C4 through MDR, we are able to obtain subsets of data which might slightly outperform RefinedWeb; this combines both the stringent filtering of C4 (e.g., strict NSFW word blocklist, 3-sentence span deduplication) with our own filters and deduplication. While such a combination results in rejection rates that would be unacceptable for our target of 3-6 trillions tokens, this represents an interesting perspective for shorter runs, which may be able to extract extremely high-quality subsets from large web datasets.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<span id="S4.SS3.p4.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="S4.SS3.p4.1.1" class="ltx_p"><span id="S4.SS3.p4.1.1.1" class="ltx_text ltx_font_bold">Finding.</span> While filtering heuristics may require source-dependent tuning, stringent deduplication improves zero-shot performance across datasets consistently.</span>
</span>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S4.T5.4.1" class="ltx_text ltx_font_bold">Although improvements from filtering are not systematic across datasets, deduplication brings a steady performance boost across the board.</span> Zero-shot accuracy averaged on our <span id="S4.T5.5.2" class="ltx_text ltx_font_typewriter">small-agg</span> aggregate; [+x.x] reports absolute gains compared to base, removal rates reported against base. Due to limitations in our pipeline, we cannot apply the deduplication stage independently for RefinedWeb.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="3"><span id="S4.T5.1.2.2.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Massive web datasets</span></td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.2.3.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Curated</span></td>
<td id="S4.T5.1.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.2.4.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Ours</span></td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">OSCAR-21.09</td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_t">OSCAR-22.01</td>
<td id="S4.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mi mathcolor="#7DD86E" mathvariant="normal" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S4.T5.1.1.1.1" class="ltx_text" style="color:#7DD86E;"> Pile</span>
</td>
<td id="S4.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.6.1" class="ltx_text" style="color:#DB57B2;">●RefinedWeb</span></td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.3.1.1" class="ltx_text ltx_font_bold">Base</span></td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_t">55.0%</td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
<td id="S4.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.4.1" class="ltx_text ltx_font_bold">55.7%</span></td>
<td id="S4.T5.1.3.5" class="ltx_td ltx_align_center ltx_border_t">53.4%</td>
<td id="S4.T5.1.3.6" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T5.1.4.1.1" class="ltx_text ltx_font_bold">Filtered</span></td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_center">55.4% [+.4]</td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_center">52.3% [-.4]</td>
<td id="S4.T5.1.4.4" class="ltx_td ltx_align_center">
<span id="S4.T5.1.4.4.1" class="ltx_text ltx_font_bold">56.2%</span> [+.5]</td>
<td id="S4.T5.1.4.5" class="ltx_td ltx_align_center">54.2% [+.8]</td>
<td id="S4.T5.1.4.6" class="ltx_td ltx_align_center">54.3% [+1.6]</td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_align_left"><em id="S4.T5.1.5.1.1" class="ltx_emph ltx_font_italic">removal rate</em></td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.2.1" class="ltx_emph ltx_font_italic">-25.0%</em></td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.3.1" class="ltx_emph ltx_font_italic">-39.8%</em></td>
<td id="S4.T5.1.5.4" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.4.1" class="ltx_emph ltx_font_italic">-16.4%</em></td>
<td id="S4.T5.1.5.5" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.5.1" class="ltx_emph ltx_font_italic">-27.1%</em></td>
<td id="S4.T5.1.5.6" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.6.1" class="ltx_emph ltx_font_italic">-50.8%</em></td>
</tr>
<tr id="S4.T5.1.6" class="ltx_tr">
<td id="S4.T5.1.6.1" class="ltx_td ltx_align_left"><span id="S4.T5.1.6.1.1" class="ltx_text ltx_font_bold">Deduplicated</span></td>
<td id="S4.T5.1.6.2" class="ltx_td ltx_align_center">55.6% [+.6]</td>
<td id="S4.T5.1.6.3" class="ltx_td ltx_align_center">55.6% [+2.9]</td>
<td id="S4.T5.1.6.4" class="ltx_td ltx_align_center">
<span id="S4.T5.1.6.4.1" class="ltx_text ltx_font_bold">55.9%</span> [+.2]</td>
<td id="S4.T5.1.6.5" class="ltx_td ltx_align_center">54.5% [+1.1]</td>
<td id="S4.T5.1.6.6" class="ltx_td"></td>
</tr>
<tr id="S4.T5.1.7" class="ltx_tr">
<td id="S4.T5.1.7.1" class="ltx_td ltx_align_left"><em id="S4.T5.1.7.1.1" class="ltx_emph ltx_font_italic">removal rate</em></td>
<td id="S4.T5.1.7.2" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.2.1" class="ltx_emph ltx_font_italic">-10.8%</em></td>
<td id="S4.T5.1.7.3" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.3.1" class="ltx_emph ltx_font_italic">-60.8%</em></td>
<td id="S4.T5.1.7.4" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.4.1" class="ltx_emph ltx_font_italic">-7.59%</em></td>
<td id="S4.T5.1.7.5" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.5.1" class="ltx_emph ltx_font_italic">-45.3%</em></td>
<td id="S4.T5.1.7.6" class="ltx_td"></td>
</tr>
<tr id="S4.T5.1.8" class="ltx_tr">
<td id="S4.T5.1.8.1" class="ltx_td ltx_align_left"><span id="S4.T5.1.8.1.1" class="ltx_text ltx_font_bold">Filt.+Dedup.</span></td>
<td id="S4.T5.1.8.2" class="ltx_td ltx_align_center">55.5% [+.5]</td>
<td id="S4.T5.1.8.3" class="ltx_td ltx_align_center">55.4% [+2.7]</td>
<td id="S4.T5.1.8.4" class="ltx_td ltx_align_center">
<span id="S4.T5.1.8.4.1" class="ltx_text ltx_font_bold">56.4%</span> [+.7]</td>
<td id="S4.T5.1.8.5" class="ltx_td ltx_align_center">55.2% [+1.8]</td>
<td id="S4.T5.1.8.6" class="ltx_td ltx_align_center">56.2% [+3.5]</td>
</tr>
<tr id="S4.T5.1.9" class="ltx_tr">
<td id="S4.T5.1.9.1" class="ltx_td ltx_align_left ltx_border_bb"><em id="S4.T5.1.9.1.1" class="ltx_emph ltx_font_italic">removal rate</em></td>
<td id="S4.T5.1.9.2" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.2.1" class="ltx_emph ltx_font_italic">-28.2%</em></td>
<td id="S4.T5.1.9.3" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.3.1" class="ltx_emph ltx_font_italic">-62.2%</em></td>
<td id="S4.T5.1.9.4" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.4.1" class="ltx_emph ltx_font_italic">-17.9%</em></td>
<td id="S4.T5.1.9.5" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.5.1" class="ltx_emph ltx_font_italic">-66.0%</em></td>
<td id="S4.T5.1.9.6" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.6.1" class="ltx_emph ltx_font_italic">-75.4%</em></td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Biases.</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">We conduct a basic analysis of the toxicity of RefinedWeb in <a href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>. We find RW to be about as toxic as The Pile, based on the definition of toxicity provided by the Perspective API: ”content that is rude or disrespectful”. Notably, this definition does not cover issues with social biases or harmfulness. Although it is unlikely that our pipeline introduces further issues on this side than is already documented for popular datasets, we encourage further quantitative work on the public extract of RefinedWeb.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multiple epochs.</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">Instead of looking for ”unique” tokens to make up a trillion-scale pretraining dataset, one could simply repeat data over multiple epochs. Popular models like OPT and NeoX-20B do this for up to 2 epochs, and most curated datasets upsample corpora 2-5 times. However,&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Hernandez et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> has recently shown that models with 100B+ parameters may be sensitive to even just a few epochs. Orthogonal to our work lies a line of research exploring tradeoffs in the data-constrained regime: can deduplication help sustain more epochs? Are multiple epochs on higher quality data better than a one epoch on lower quality data? See <a href="#A5.SS3" title="E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">E.3</span></a> for a more in-depth discussion.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Other results on deduplication.</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Biderman et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> found a limited impact on zero-shot performance from deduplicating The Pile; we discuss further in <a href="#A6.SS2" title="F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">F.2</span></a>, but encourage further deduplication research on curated corpora, and studying deduplication in the data-constrained regime, where multiple epochs have to be performed to compensate for the reduction in tokens incurred by deduplication.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">As LLMs are widely adopted, models trained past the recommendations of scaling laws are bound to become increasingly common to amortize inference costs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite>. This will further drive the need for pretraining datasets with trillions of tokens, an order of magnitude beyond publicly available corpora. We have demonstrated that stringent filtering and deduplication could result in a five trillion tokens web only dataset suitable to produce models competitive with the state-of-the-art, even outperforming LLMs trained on curated corpora. We publicly release a 600GT extract of RefinedWeb, and note that RefinedWeb has already been used to train state-of-the-art language models, such as Falcon-40B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2306.01116/assets/x5.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="171" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S6.F4.2.1" class="ltx_text ltx_font_bold">Toxic content in <span id="S6.F4.2.1.1" class="ltx_text" style="color:#DB57B2;">RefinedWeb</span> is distributed similarly to <span id="S6.F4.2.1.2" class="ltx_text" style="color:#7DD86E;">The Pile.</span></span> Cumulative proportion of documents below a given toxicity score, as evaluated by the Pespective API.</figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Abadji, J., Suárez, P. J.&nbsp;O., Romary, L., and Sagot, B.

</span>
<span class="ltx_bibblock">Ungoliant: An optimized pipeline for the generation of a very
large-scale multilingual web corpus.

</span>
<span class="ltx_bibblock">Proceedings of the Workshop on Challenges in the Management of Large
Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pp.&nbsp; 1 – 9,
Mannheim, 2021. Leibniz-Institut für Deutsche Sprache.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.14618/ids-pub-10468</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Abadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B.

</span>
<span class="ltx_bibblock">Towards a Cleaner Document-Oriented Multilingual Crawled Corpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2201.06642, January 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Abbas, A. K.&nbsp;M., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A.&nbsp;S.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic
deduplication.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ICLR 2023 Workshop on Mathematical and Empirical
Understanding of Foundation Models</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adiwardana et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Adiwardana, D., Luong, M.-T., So, D.&nbsp;R., Hall, J., Fiedel, N., Thoppilan, R.,
Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et&nbsp;al.

</span>
<span class="ltx_bibblock">Towards a human-like open-domain chatbot.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.09977</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aleph Alpha (2023)</span>
<span class="ltx_bibblock">
Aleph Alpha.

</span>
<span class="ltx_bibblock">Luminous: performance benchmarks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.12885</em>, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allamanis (2019)</span>
<span class="ltx_bibblock">
Allamanis, M.

</span>
<span class="ltx_bibblock">The adverse effects of code duplication in machine learning models of
code.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 ACM SIGPLAN International Symposium
on New Ideas, New Paradigms, and Reflections on Programming and Software</em>,
pp.&nbsp; 143–153, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Almazrouei, E., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow,
D., Launay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo, G.

</span>
<span class="ltx_bibblock">Falcon-40b: an open large language model with state-of-the-art
performance.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi, Y., and
Hajishirzi, H.

</span>
<span class="ltx_bibblock">Mathqa: Towards interpretable math word problem solving with
operation-based formalisms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 2357–2367, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aroca-Ouellette et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Aroca-Ouellette, S., Paik, C., Roncone, A., and Kann, K.

</span>
<span class="ltx_bibblock">Prost: Physical reasoning about objects through space and time.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021</em>, pp.&nbsp; 4597–4608, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin,
X.&nbsp;V., Du, J., Iyer, S., Pasunuru, R., et&nbsp;al.

</span>
<span class="ltx_bibblock">Efficient large scale language modeling with mixtures of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.10684</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbaresi (2021)</span>
<span class="ltx_bibblock">
Barbaresi, A.

</span>
<span class="ltx_bibblock">Trafilatura: A Web Scraping Library and Command-Line Tool for Text
Discovery and Extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Joint Conference of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing: System
Demonstrations</em>, pp.&nbsp; 122–131. Association for Computational Linguistics,
2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.acl-demo.15" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-demo.15</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Beltagy, I., Lo, K., and Cohan, A.

</span>
<span class="ltx_bibblock">Scibert: A pretrained language model for scientific text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pp.&nbsp; 3615–3620, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan,
E., Khan, M.&nbsp;A., Purohit, S., Prashanth, U.&nbsp;S., Raff, E., et&nbsp;al.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01373</em>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pp.&nbsp; 7432–7439, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Black, S., Leo, G., Wang, P., Leahy, C., and Biderman, S.

</span>
<span class="ltx_bibblock">GPT-Neo: Large Scale Autoregressive Language Modeling with
Mesh-Tensorflow, March 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5297715" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5297715</a>.

</span>
<span class="ltx_bibblock">If you use this software, please cite it using these metadata.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He,
H., Leahy, C., McDonell, K., Phang, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-neox-20b: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Challenges &amp; Perspectives in Creating Large Language Models</em>,
pp.&nbsp;&nbsp;95, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Broder (1997)</span>
<span class="ltx_bibblock">
Broder, A.&nbsp;Z.

</span>
<span class="ltx_bibblock">On the resemblance and containment of documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings. Compression and Complexity of Sequences 1997</em>,
pp.&nbsp; 21–29. IEEE, 1997.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.&nbsp;D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K.,
Roberts, A., Brown, T., Song, D., Erlingsson, U., et&nbsp;al.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">30th USENIX Security Symposium (USENIX Security 21)</em>, pp.&nbsp;2633–2650, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C.

</span>
<span class="ltx_bibblock">Quantifying memorization across neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07646</em>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charikar (2002)</span>
<span class="ltx_bibblock">
Charikar, M.&nbsp;S.

</span>
<span class="ltx_bibblock">Similarity estimation techniques from rounding algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the thiry-fourth annual ACM symposium on
Theory of computing</em>, pp.&nbsp; 380–388, 2002.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chelba et&nbsp;al. (2013)</span>
<span class="ltx_bibblock">
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and
Robinson, T.

</span>
<span class="ltx_bibblock">One billion word benchmark for measuring progress in statistical
language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.3005</em>, 2013.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
Barham, P., Chung, H.&nbsp;W., Sutton, C., Gehrmann, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova,
K.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of NAACL-HLT</em>, pp.&nbsp; 2924–2936, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and
Tafjord, O.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dagan et&nbsp;al. (2010)</span>
<span class="ltx_bibblock">
Dagan, I., Dolan, B., Magnini, B., and Roth, D.

</span>
<span class="ltx_bibblock">Recognizing textual entailment: Rational, evaluation and
approaches–erratum.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Natural Language Engineering</em>, 16(1):105–105, 2010.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Dao, T., Fu, D.&nbsp;Y., Ermon, S., Rudra, A., and Re, C.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with
io-awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Marneffe et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
De&nbsp;Marneffe, M.-C., Simons, M., and Tonhauser, J.

</span>
<span class="ltx_bibblock">The commitmentbank: Investigating projection in naturally occurring
discourse.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">proceedings of Sinn und Bedeutung</em>, volume&nbsp;23, pp.&nbsp;107–124, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 4171–4186, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dey et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness,
J., et&nbsp;al.

</span>
<span class="ltx_bibblock">Cerebras-gpt: Open compute-optimal language models trained on the
cerebras wafer-scale cluster.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03208</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D.,
Mitchell, M., and Gardner, M.

</span>
<span class="ltx_bibblock">Documenting large webtext corpora: A case study on the colossal clean
crawled corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pp.&nbsp; 1286–1305, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eberhard et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Eberhard, D.&nbsp;M., Simons, G.&nbsp;F., and Fennig, C.&nbsp;D.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Ethnologue: Languages of the World</em>.

</span>
<span class="ltx_bibblock">SIL International, Dallas, TX, USA, twenty-sixth edition, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang,
J., He, H., Thite, A., Nabeshima, N., et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,
Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E.,
Thite, A., Wang, B., Wang, K., and Zou, A.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J.&nbsp;W., Wallach, H., Iii,
H.&nbsp;D., and Crawford, K.

</span>
<span class="ltx_bibblock">Datasheets for datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(12):86–92,
2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokaslan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Gokaslan, A., Cohen, V., Pavlick, E., and Tellex, S.

</span>
<span class="ltx_bibblock">Openwebtext corpus.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://Skylion007.github.io/OpenWebTextCorpus" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://Skylion007.github.io/OpenWebTextCorpus</a>, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et&nbsp;al. (2012)</span>
<span class="ltx_bibblock">
Gordon, A., Kozareva, Z., and Roemmele, M.

</span>
<span class="ltx_bibblock">Semeval-2012 task 7: Choice of plausible alternatives: An evaluation
of commonsense causal reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">* SEM 2012: The First Joint Conference on Lexical and
Computational Semantics–Volume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012)</em>, pp.&nbsp; 394–398, 2012.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grave et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Grave, É., Bojanowski, P., Gupta, P., Joulin, A., and Mikolov, T.

</span>
<span class="ltx_bibblock">Learning word vectors for 157 languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</em>, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanu &amp; Unitary team (2020)</span>
<span class="ltx_bibblock">
Hanu, L. and Unitary team.

</span>
<span class="ltx_bibblock">Detoxify.

</span>
<span class="ltx_bibblock">Github. https://github.com/unitaryai/detoxify, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S.,
Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling laws and interpretability of learning from repeated data.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.10487</em>, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
E., Casas, D. d.&nbsp;L., Hendricks, L.&nbsp;A., Welbl, J., Clark, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaccard (1912)</span>
<span class="ltx_bibblock">
Jaccard, P.

</span>
<span class="ltx_bibblock">The distribution of the flora in the alpine zone.1.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">New Phytologist</em>, 11:37–50, 1912.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X.

</span>
<span class="ltx_bibblock">Pubmedqa: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pp.&nbsp; 2567–2577, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., and Mikolov,
T.

</span>
<span class="ltx_bibblock">Fasttext. zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.03651</em>, 2016.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.&nbsp;B., Chess, B., Child, R.,
Gray, S., Radford, A., Wu, J., and Amodei, D.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreutzer et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh,
N., Tapo, A.&nbsp;A., Subramani, N., Sokolov, A., Sikasote, C., et&nbsp;al.

</span>
<span class="ltx_bibblock">Quality at a glance: An audit of web-crawled multilingual datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:50–72, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Laurençon, H., Saulnier, L., Wang, T., Akiki, C., del Moral, A.&nbsp;V.,
Le&nbsp;Scao, T., Von&nbsp;Werra, L., Mou, C., Ponferrada, E.&nbsp;G., Nguyen, H., et&nbsp;al.

</span>
<span class="ltx_bibblock">The bigscience roots corpus: A 1.6 tb composite multilingual dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Thirty-sixth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and
Carlini, N.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 8424–8445,
2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y.

</span>
<span class="ltx_bibblock">Logiqa: a challenge dataset for machine reading comprehension with
logical reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Conference on
International Joint Conferences on Artificial Intelligence</em>, pp.&nbsp;3622–3628, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., and Stoyanov, V.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopukhin (2019)</span>
<span class="ltx_bibblock">
Lopukhin, K.

</span>
<span class="ltx_bibblock">Evaluating quality of article body extraction for commercial services
and open-source libraries.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/scrapinghub/article-extraction-benchmark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/scrapinghub/article-extraction-benchmark</a>,
2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manber &amp; Myers (1993)</span>
<span class="ltx_bibblock">
Manber, U. and Myers, G.

</span>
<span class="ltx_bibblock">Suffix arrays: a new method for on-line string searches.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Journal on Computing</em>, 22(5):935–948,
1993.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pp.&nbsp; 2381–2391, 2018.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B.,
Spitzer, E., Raji, I.&nbsp;D., and Gebru, T.

</span>
<span class="ltx_bibblock">Model cards for model reporting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the conference on fairness, accountability,
and transparency</em>, pp.&nbsp; 220–229, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., and Kiela, D.

</span>
<span class="ltx_bibblock">Adversarial nli: A new benchmark for natural language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.14599</em>, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortiz Suárez et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Ortiz Suárez, P.&nbsp;J., Sagot, B., and Romary, L.

</span>
<span class="ltx_bibblock">Asynchronous pipelines for processing huge corpora on medium to low
resource infrastructures.

</span>
<span class="ltx_bibblock">Proceedings of the Workshop on Challenges in the Management of Large
Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp.&nbsp; 9 – 16, Mannheim,
2019. Leibniz-Institut für Deutsche Sprache.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.14618/ids-pub-9021</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q., Bernardi, R.,
Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R.

</span>
<span class="ltx_bibblock">The lambada dataset: Word prediction requiring a broad discourse
context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 1525–1534,
2016.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pomikálek (2011)</span>
<span class="ltx_bibblock">
Pomikálek, J.

</span>
<span class="ltx_bibblock">Justext.

</span>
<span class="ltx_bibblock">2011.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Press, O., Smith, N., and Lewis, M.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input
length extrapolation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Rae, J.&nbsp;W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,
T., Menick, J., Cassirer, A., Powell, R., Driessche, G. v.&nbsp;d., Hendricks,
L.&nbsp;A., Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S., Huang,
S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A.,
Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E.,
Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.&nbsp;L., Kuncoro, A.,
Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A.,
Lespiau, J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T.,
Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., d’Autume, C. d.&nbsp;M., Li, Y.,
Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., Casas, D. d.&nbsp;L., Guy, A.,
Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel,
I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O.,
Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and
Irving, G.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/ARXIV.2112.11446</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.11446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
Y., Li, W., and Liu, P.&nbsp;J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v21/20-074.html</a>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Sakaguchi, K., Bras, R.&nbsp;L., Bhagavatula, C., and Choi, Y.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(9):99–106,
2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Scao, T.&nbsp;L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D.,
Castagné, R., Luccioni, A.&nbsp;S., Yvon, F., Gallé, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>, 2022a.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Scao, T.&nbsp;L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M.&nbsp;S.,
Bideman, S., Elsahar, H., Muennighoff, N., Phang, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">What language model to train if you have one million gpu hours?

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.15424</em>, 2022b.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sevilla et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M., and Villalobos, P.

</span>
<span class="ltx_bibblock">Compute trends across three eras of machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.05924</em>, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sites (2013)</span>
<span class="ltx_bibblock">
Sites, D.

</span>
<span class="ltx_bibblock">Compact language detector 2.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Software available at https://github. com/CLD2Owners/cld2 (last
updated on August 2015)</em>, 2013.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H.&nbsp;W., Narang, S.,
Yogatama, D., Vaswani, A., and Metzler, D.

</span>
<span class="ltx_bibblock">Scale efficiently: Insights from pretraining and finetuning
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Thoppilan, R., De&nbsp;Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh &amp; Le (2018)</span>
<span class="ltx_bibblock">
Trinh, T.&nbsp;H. and Le, Q.&nbsp;V.

</span>
<span class="ltx_bibblock">A simple method for commonsense reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.02847</em>, 2018.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilares &amp; Gómez-Rodríguez (2019)</span>
<span class="ltx_bibblock">
Vilares, D. and Gómez-Rodríguez, C.

</span>
<span class="ltx_bibblock">Head-qa: A healthcare dataset for complex reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.&nbsp; 960–966, 2019.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.04325</em>, 2022.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang &amp; Komatsuzaki (2021)</span>
<span class="ltx_bibblock">
Wang, B. and Komatsuzaki, A.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kingoflolz/mesh-transformer-jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kingoflolz/mesh-transformer-jax</a>, May 2021.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wang, T., Roberts, A., Hesslow, D., Scao, T.&nbsp;L., Chung, H.&nbsp;W., Beltagy, I.,
Launay, J., and Raffel, C.

</span>
<span class="ltx_bibblock">What language model architecture and pretraining objective work best
for zero-shot generalization?

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2022.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,
D., Bosma, M., Zhou, D., Metzler, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Welbl, J., Liu, N.&nbsp;F., and Gardner, M.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Noisy User-generated
Text</em>, pp.&nbsp; 94–106, 2017.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L.&nbsp;A.,
Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S.

</span>
<span class="ltx_bibblock">Challenges in detoxifying language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pp.&nbsp; 2447–2469, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin,
A., and Grave, É.

</span>
<span class="ltx_bibblock">Ccnet: Extracting high quality monolingual datasets from web crawl
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Language Resources and Evaluation
Conference</em>, pp.&nbsp; 4003–4012, 2020.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,
A., and Raffel, C.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pp.&nbsp; 483–498, 2021.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yang, G., Hu, E., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,
Pachocki, J., Chen, W., and Gao, J.

</span>
<span class="ltx_bibblock">Tuning large neural networks via zero-shot hyperparameter transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
34:17084–17097, 2021.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.&nbsp; 4791–4800, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z.,
Wang, K., Zhang, X., et&nbsp;al.

</span>
<span class="ltx_bibblock">Pangu-alpha: Large-scale autoregressive pretrained chinese language
models with auto-parallel computation.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.12369</em>, 2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Van&nbsp;Durme, B.

</span>
<span class="ltx_bibblock">Record: Bridging the gap between human and machine commonsense
reading comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.12885</em>, 2018.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
Diab, M., Li, X., Lin, X.&nbsp;V., et&nbsp;al.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2015)</span>
<span class="ltx_bibblock">
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
and Fidler, S.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pp.&nbsp; 19–27, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>RefinedWeb Datasheet</h2>

<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="A1.T6.2.1" class="ltx_text ltx_font_bold">Datasheet for RefinedWeb</span>, following the framework introduced by <cite class="ltx_cite ltx_citemacro_citet">Gebru et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
<table id="A1.T6.1" class="ltx_tabular">
<tbody><tr id="A1.T6.1.2" class="ltx_tr">
<td id="A1.T6.1.2.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="A1.T6.1.2.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Motivation</span></td>
</tr>
<tr id="A1.T6.1.3" class="ltx_tr">
<td id="A1.T6.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.3.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.3.1.1.1.1" class="ltx_text ltx_font_bold">For what purpose was the dataset created?</span></span>
</span>
</td>
<td id="A1.T6.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.3.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb was created to serve as a large-scale dataset for the pretraining of large language models. It may be used on its own, or augmented with curated sources (e.g., Wikipedia, StackOverflow).</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.4" class="ltx_tr">
<td id="A1.T6.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.4.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Who created the dataset and on behalf of which entity?</span></span>
</span>
</td>
<td id="A1.T6.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.4.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset was created by the Technology Innovation Institute.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.5" class="ltx_tr">
<td id="A1.T6.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.5.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Who funded the creation of the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.5.2.1.1" class="ltx_p" style="width:284.5pt;">The creation of the dataset was privately funded by the Technology Innovation Institute.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.6" class="ltx_tr">
<td id="A1.T6.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.6.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Any other comment?</span></span>
</span>
</td>
<td id="A1.T6.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.6.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb is built on-top of CommonCrawl, using the Macrodata Refinement Pipeline, which combines content extraction, filtering heuristics, and deduplication. In designing RefinedWeb, we abided to the following philosophy: (1) <span id="A1.T6.1.6.2.1.1.1" class="ltx_text ltx_font_bold">Scale first.</span> We intend MDR to produce datasets to be used to train 40-200B parameters models, thus requiring trillions of tokens <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>. For English-only RefinedWeb, we target a size of 3-6 trillion tokens. Specifically, we eschew any labour intensive human curation process, and focus on CommonCrawl instead of disparate single-domain sources. (2) <span id="A1.T6.1.6.2.1.1.2" class="ltx_text ltx_font_bold">Strict deduplication.</span> Inspired by the work of <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, which demonstrated the value of deduplication for large language models, we implement a rigorous deduplication pipeline. We combine both exact and fuzzy deduplication, and use strict settings leading to removal rates far higher than others have reported. (3) <span id="A1.T6.1.6.2.1.1.3" class="ltx_text ltx_font_bold">Neutral filtering.</span> To avoid introducing further undesirable biases into the model <cite class="ltx_cite ltx_citemacro_citep">(Dodge et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Welbl et&nbsp;al., <a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite>, we avoid using ML-based filtering outside of language identification. We stick to simple rules and heuristics, and use only URL filtering for adult content.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.7" class="ltx_tr">
<td id="A1.T6.1.7.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.7.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Composition</span></td>
</tr>
<tr id="A1.T6.1.8" class="ltx_tr">
<td id="A1.T6.1.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.8.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.8.1.1.1.1" class="ltx_text ltx_font_bold">What do the instances that comprise the dataset represent?</span></span>
</span>
</td>
<td id="A1.T6.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.8.2.1.1" class="ltx_p" style="width:284.5pt;">Instances are text-only documents, corresponding to single web pages.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.1" class="ltx_tr">
<td id="A1.T6.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.1.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">How many instances are there in total?</span></span>
</span>
</td>
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.1.1.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb contains <math id="A1.T6.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A1.T6.1.1.1.1.1.m1.1a"><mo id="A1.T6.1.1.1.1.1.m1.1.1" xref="A1.T6.1.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="A1.T6.1.1.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.1.1.m1.1c">\sim</annotation></semantics></math>10 billion documents, or around 5 trillion tokens. The public version is a subset representing a tenth of the full version.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.9" class="ltx_tr">
<td id="A1.T6.1.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.9.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.9.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</span></span>
</span>
</td>
<td id="A1.T6.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.9.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb is built using all CommonCrawl dumps until the 2023-06 one; it could be updated with additional dumps as they are released. The public release of RefinedWeb is a 600GT random extract of the 5,000GT of the full dataset. For all experiments, we randomly sampled from the public extract, or earlier development versions of it.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.10" class="ltx_tr">
<td id="A1.T6.1.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.10.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.10.1.1.1.1" class="ltx_text ltx_font_bold">What data does each instance consist of?</span></span>
</span>
</td>
<td id="A1.T6.1.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.10.2.1.1" class="ltx_p" style="width:284.5pt;">Each instance is a text-only document, with metadata about its origin in CommonCrawl and source page URL. We also distribute a multimodal version of RefinedWeb, containing interlaced links to images.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.11" class="ltx_tr">
<td id="A1.T6.1.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.11.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.11.1.1.1.1" class="ltx_text ltx_font_bold">Is there a label or target associated with each instance?</span></span>
</span>
</td>
<td id="A1.T6.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.11.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.12" class="ltx_tr">
<td id="A1.T6.1.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.12.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.12.1.1.1.1" class="ltx_text ltx_font_bold">Is any information missing from individual instances?</span></span>
</span>
</td>
<td id="A1.T6.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.12.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.13" class="ltx_tr">
<td id="A1.T6.1.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.13.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.13.1.1.1.1" class="ltx_text ltx_font_bold">Are relationships between individual instances made explicit?</span></span>
</span>
</td>
<td id="A1.T6.1.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.13.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.14" class="ltx_tr">
<td id="A1.T6.1.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.14.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.14.1.1.1.1" class="ltx_text ltx_font_bold">Are there recommended data splits?</span></span>
</span>
</td>
<td id="A1.T6.1.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.14.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.15" class="ltx_tr">
<td id="A1.T6.1.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.15.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.15.1.1.1.1" class="ltx_text ltx_font_bold">Are there any errors, sources of noise, or redundancies in the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.15.2.1.1" class="ltx_p" style="width:284.5pt;">Despite our best efforts to filter content that does not qualify as natural language, and to deduplicate documents, our pipeline may let through documents that may be considered as errors or redundant.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.16" class="ltx_tr">
<td id="A1.T6.1.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.16.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.16.1.1.1.1" class="ltx_text ltx_font_bold">Is the dataset self-contained, or does it link to or otherwise rely on external resources?</span></span>
</span>
</td>
<td id="A1.T6.1.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.16.2.1.1" class="ltx_p" style="width:284.5pt;">The base version of the dataset is self-contained, but the multimodal version is interlaced with links to images–these are not distributed as part of the dataset, and constitute an external source.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.17" class="ltx_tr">
<td id="A1.T6.1.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.17.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.17.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain data that might be considered confidential?</span></span>
</span>
</td>
<td id="A1.T6.1.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.17.2.1.1" class="ltx_p" style="width:284.5pt;">All documents in RefinedWeb have been publicly available online.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.18" class="ltx_tr">
<td id="A1.T6.1.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.18.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.18.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</span></span>
</span>
</td>
<td id="A1.T6.1.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.18.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, as this type of data is prevalent on the internet, it is likely our dataset contains such content. Notably, we estimate the prevalence of toxic content in the dataset to be similar to The Pile (<a href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>).</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.19" class="ltx_tr">
<td id="A1.T6.1.19.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.19.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Collection</span></td>
</tr>
<tr id="A1.T6.1.20" class="ltx_tr">
<td id="A1.T6.1.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.20.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.20.1.1.1.1" class="ltx_text ltx_font_bold">How was the data associated with each instance acquired?</span></span>
</span>
</td>
<td id="A1.T6.1.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.20.2.1.1" class="ltx_p" style="width:284.5pt;">We downloaded with <span id="A1.T6.1.20.2.1.1.1" class="ltx_text ltx_font_typewriter">warcio</span> publicly available .WET files from the CommonCrawl foundation.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.21" class="ltx_tr">
<td id="A1.T6.1.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.21.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.21.1.1.1.1" class="ltx_text ltx_font_bold">What mechanisms or procedures were used to collect the data?</span></span>
</span>
</td>
<td id="A1.T6.1.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.21.2.1.1" class="ltx_p" style="width:284.5pt;">We refer to the CommonCrawl website (<a href="commoncrawl.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">commoncrawl.org</a>) for details on how they collect data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.22" class="ltx_tr">
<td id="A1.T6.1.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.22.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.22.1.1.1.1" class="ltx_text ltx_font_bold">If the dataset is a sample from a larger set, what was the sampling strategy?</span></span>
</span>
</td>
<td id="A1.T6.1.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.22.2.1.1" class="ltx_p" style="width:284.5pt;">Whenever we use subsets, we randomly sample from the original data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.23" class="ltx_tr">
<td id="A1.T6.1.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.23.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.23.1.1.1.1" class="ltx_text ltx_font_bold">Who was involved in the data collection process and how were they compensated?</span></span>
</span>
</td>
<td id="A1.T6.1.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.23.2.1.1" class="ltx_p" style="width:284.5pt;">The original data collection was performed by CommonCrawl; authors from this paper were involved in retrieving it and preparing it.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.24" class="ltx_tr">
<td id="A1.T6.1.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.24.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.24.1.1.1.1" class="ltx_text ltx_font_bold">Over what timeframe was the data collected?</span></span>
</span>
</td>
<td id="A1.T6.1.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.24.2.1.1" class="ltx_p" style="width:284.5pt;">We use all CommonCrawl dumps from 2008 to January/February 2023.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.25" class="ltx_tr">
<td id="A1.T6.1.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.25.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.25.1.1.1.1" class="ltx_text ltx_font_bold">Were any ethical review processes conducted?</span></span>
</span>
</td>
<td id="A1.T6.1.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.25.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.26" class="ltx_tr">
<td id="A1.T6.1.26.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.26.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Preprocessing</span></td>
</tr>
<tr id="A1.T6.1.27" class="ltx_tr">
<td id="A1.T6.1.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.27.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.27.1.1.1.1" class="ltx_text ltx_font_bold">Was any preprocessing/cleaning/labeling of the data done?</span></span>
</span>
</td>
<td id="A1.T6.1.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.27.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, we applied extensive preprocessing and cleaning of the data. We first filter URLs to remove adult content using a blocklist and a score system (<a href="#A7.SS1" title="G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1</span></a>), we then use <span id="A1.T6.1.27.2.1.1.1" class="ltx_text ltx_font_typewriter">trafilatura</span> <cite class="ltx_cite ltx_citemacro_citep">(Barbaresi, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> to extract content from pages, and perform language identification with the <span id="A1.T6.1.27.2.1.1.2" class="ltx_text ltx_font_typewriter">fastText</span> classifier from CCNet <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite>. After this first preprocessing stage, we filter data using heuristics from MassiveWeb&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite> and our own line-wise corrections (<a href="#A7.SS2" title="G.2 Line-wise filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.2</span></a>). Finally, we run extensive deduplication, removing URLs revisited across dumps (<a href="#S3.SS3" title="3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.3</span></a>) and performing subsequently fuzzy and exact substring deduplication, with each stage drawing from <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. See <a href="#S3" title="3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> for further details and <a href="#S3.T2" title="In Language identification. ‣ 3.1 Document preparation: reading data, filtering URLs, extracting text, and language identification ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> for an outline.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.28" class="ltx_tr">
<td id="A1.T6.1.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.28.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.28.1.1.1.1" class="ltx_text ltx_font_bold">Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data?</span></span>
</span>
</td>
<td id="A1.T6.1.28.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.28.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.28.2.1.1" class="ltx_p" style="width:284.5pt;">During development, we saved intermediary outputs from our pipeline for investigations and for ablations–intermediary outputs exist for about 5% of RefinedWeb. We did not keep intermediary outputs for the final production version of the dataset due to storage and resource constraints.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.29" class="ltx_tr">
<td id="A1.T6.1.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.29.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.29.1.1.1.1" class="ltx_text ltx_font_bold">Is the software that was used to preprocess/clean/label the data available?</span></span>
</span>
</td>
<td id="A1.T6.1.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.29.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.30" class="ltx_tr">
<td id="A1.T6.1.30.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.30.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Uses</span></td>
</tr>
<tr id="A1.T6.1.31" class="ltx_tr">
<td id="A1.T6.1.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.31.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.31.1.1.1.1" class="ltx_text ltx_font_bold">Has the dataset been used for any tasks already?</span></span>
</span>
</td>
<td id="A1.T6.1.31.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.31.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.31.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, this data has been used to develop large language models: both for scientific experiments (e.g., this paper) and production use.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.32" class="ltx_tr">
<td id="A1.T6.1.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.32.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.32.1.1.1.1" class="ltx_text ltx_font_bold">Is there a repository that links to any or all papers or systems that use the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.32.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.32.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.32.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.33" class="ltx_tr">
<td id="A1.T6.1.33.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.33.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.33.1.1.1.1" class="ltx_text ltx_font_bold">What (other) tasks could the dataset be used for?</span></span>
</span>
</td>
<td id="A1.T6.1.33.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.33.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.33.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb was built as a large-scale corpora representative of the web, and as such may see many downstream uses which are difficult to predict.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.34" class="ltx_tr">
<td id="A1.T6.1.34.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.34.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.34.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.34.1.1.1.1" class="ltx_text ltx_font_bold">Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</span></span>
</span>
</td>
<td id="A1.T6.1.34.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.34.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.34.2.1.1" class="ltx_p" style="width:284.5pt;">For the public extract of RefinedWeb, we chose to only draw from the English version of the dataset, preventing multilingual applications.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.35" class="ltx_tr">
<td id="A1.T6.1.35.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.35.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.35.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.35.1.1.1.1" class="ltx_text ltx_font_bold">Are there tasks for which the dataset should not be used?</span></span>
</span>
</td>
<td id="A1.T6.1.35.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.35.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.35.2.1.1" class="ltx_p" style="width:284.5pt;">Any tasks which may considered irresponsible or harmful.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.36" class="ltx_tr">
<td id="A1.T6.1.36.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.36.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Distribution</span></td>
</tr>
<tr id="A1.T6.1.37" class="ltx_tr">
<td id="A1.T6.1.37.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.37.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.37.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.37.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?</span></span>
</span>
</td>
<td id="A1.T6.1.37.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.37.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.37.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, we make a 600GT extract publicly available for NLP practitioners. We currently don’t plan to share the full version of the dataset.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.38" class="ltx_tr">
<td id="A1.T6.1.38.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.38.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.38.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.38.1.1.1.1" class="ltx_text ltx_font_bold">How will the dataset will be distributed?</span></span>
</span>
</td>
<td id="A1.T6.1.38.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.38.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.38.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset will be made available through the HuggingFace Hub.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.39" class="ltx_tr">
<td id="A1.T6.1.39.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.39.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.39.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.39.1.1.1.1" class="ltx_text ltx_font_bold">When will the dataset be distributed?</span></span>
</span>
</td>
<td id="A1.T6.1.39.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.39.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.39.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset is available immediately.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.40" class="ltx_tr">
<td id="A1.T6.1.40.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.40.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.40.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.40.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?</span></span>
</span>
</td>
<td id="A1.T6.1.40.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.40.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.40.2.1.1" class="ltx_p" style="width:284.5pt;">The public extract is made available under an ODC-By 1.0 license; users should also abide to the CommonCrawl ToU: <a target="_blank" href="https://commoncrawl.org/terms-of-use/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://commoncrawl.org/terms-of-use/</a>.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.41" class="ltx_tr">
<td id="A1.T6.1.41.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.41.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.41.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.41.1.1.1.1" class="ltx_text ltx_font_bold">Have any third parties imposed IP-based or other restrictions on the data associated with the instances?</span></span>
</span>
</td>
<td id="A1.T6.1.41.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.41.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.41.2.1.1" class="ltx_p" style="width:284.5pt;">Not to our knowledge.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.42" class="ltx_tr">
<td id="A1.T6.1.42.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.42.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.42.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.42.1.1.1.1" class="ltx_text ltx_font_bold">Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?</span></span>
</span>
</td>
<td id="A1.T6.1.42.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.42.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.42.2.1.1" class="ltx_p" style="width:284.5pt;">Not to our knowledge.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.43" class="ltx_tr">
<td id="A1.T6.1.43.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.43.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Maintenance</span></td>
</tr>
<tr id="A1.T6.1.44" class="ltx_tr">
<td id="A1.T6.1.44.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.44.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.44.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.44.1.1.1.1" class="ltx_text ltx_font_bold">Who will be supporting/hosting/maintaining the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.44.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.44.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.44.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset will be hosted on the HuggingFace Hub, we have no plans to further support or maintain it once it is released.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.45" class="ltx_tr">
<td id="A1.T6.1.45.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.45.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.45.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.45.1.1.1.1" class="ltx_text ltx_font_bold">How can the owner/curator/manager of the dataset be contacted?</span></span>
</span>
</td>
<td id="A1.T6.1.45.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.45.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.45.2.1.1" class="ltx_p" style="width:284.5pt;">falconllm@tii.ae</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.46" class="ltx_tr">
<td id="A1.T6.1.46.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.46.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.46.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.46.1.1.1.1" class="ltx_text ltx_font_bold">Is there an erratum?</span></span>
</span>
</td>
<td id="A1.T6.1.46.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.46.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.46.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.47" class="ltx_tr">
<td id="A1.T6.1.47.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.47.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.47.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.47.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be updated?</span></span>
</span>
</td>
<td id="A1.T6.1.47.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.47.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.47.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.48" class="ltx_tr">
<td id="A1.T6.1.48.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A1.T6.1.48.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.48.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.48.1.1.1.1" class="ltx_text ltx_font_bold">If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?</span></span>
</span>
</td>
<td id="A1.T6.1.48.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T6.1.48.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.48.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Falcon-RW Model Cards</h2>

<figure id="A2.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="A2.T7.1.1" class="ltx_text ltx_font_bold">Model card for Falcon-RW</span>, following the framework introduced by <cite class="ltx_cite ltx_citemacro_citet">Mitchell et&nbsp;al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>)</cite>.</figcaption>
<table id="A2.T7.3" class="ltx_tabular">
<tbody><tr id="A2.T7.3.1" class="ltx_tr">
<td id="A2.T7.3.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="A2.T7.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Model details</span></td>
</tr>
<tr id="A2.T7.3.2" class="ltx_tr">
<td id="A2.T7.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.2.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Person/organization developing the model</span></span>
</span>
</td>
<td id="A2.T7.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.2.2.1.1" class="ltx_p" style="width:284.5pt;">The models were created by the Technology Innovation Institute.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.3" class="ltx_tr">
<td id="A2.T7.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.3.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Model date</span></span>
</span>
</td>
<td id="A2.T7.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.3.2.1.1" class="ltx_p" style="width:284.5pt;">Falcon-RW models were trained in December 2022/January 2023.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.4" class="ltx_tr">
<td id="A2.T7.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.4.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.4.1.1.1.1" class="ltx_text ltx_font_bold">Model type and information about training</span></span>
</span>
</td>
<td id="A2.T7.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.4.2.1.1" class="ltx_p" style="width:284.5pt;">Falcon-RW are autoregressive Transformer models trained with a causal language modeling objective. Architecture based on GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, with ALiBi positional encodings <cite class="ltx_cite ltx_citemacro_citep">(Press et&nbsp;al., <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite> and&nbsp;FlashAttention <cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. See <a href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1</span></a> for details.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.5" class="ltx_tr">
<td id="A2.T7.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.5.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.5.1.1.1.1" class="ltx_text ltx_font_bold">Licence</span></span>
</span>
</td>
<td id="A2.T7.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.5.2.1.1" class="ltx_p" style="width:284.5pt;">Apache 2.0: <a target="_blank" href="https://www.apache.org/licenses/LICENSE-2.0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.apache.org/licenses/LICENSE-2.0</a>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.6" class="ltx_tr">
<td id="A2.T7.3.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.6.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.6.1.1.1.1" class="ltx_text ltx_font_bold">Point of contact</span></span>
</span>
</td>
<td id="A2.T7.3.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.6.2.1.1" class="ltx_p" style="width:284.5pt;">falconllm@tii.ae</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.7" class="ltx_tr">
<td id="A2.T7.3.7.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.7.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Intended use</span></td>
</tr>
<tr id="A2.T7.3.8" class="ltx_tr">
<td id="A2.T7.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.8.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.8.1.1.1.1" class="ltx_text ltx_font_bold">Primary intended uses</span></span>
</span>
</td>
<td id="A2.T7.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.8.2.1.1" class="ltx_p" style="width:284.5pt;">Research on large language models, and the influence of adequately filtered and deduplicated web data on the properties of large language models (fairness, safety, limitations, capabilities, etc.).</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.9" class="ltx_tr">
<td id="A2.T7.3.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.9.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.9.1.1.1.1" class="ltx_text ltx_font_bold">Primary intended users</span></span>
</span>
</td>
<td id="A2.T7.3.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.9.2.1.1" class="ltx_p" style="width:284.5pt;">NLP researchers.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.10" class="ltx_tr">
<td id="A2.T7.3.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.10.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.10.1.1.1.1" class="ltx_text ltx_font_bold">Out-of-scope use cases</span></span>
</span>
</td>
<td id="A2.T7.3.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.10.2.1.1" class="ltx_p" style="width:284.5pt;">Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.11" class="ltx_tr">
<td id="A2.T7.3.11.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.11.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Factors</span></td>
</tr>
<tr id="A2.T7.3.12" class="ltx_tr">
<td id="A2.T7.3.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.12.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.12.1.1.1.1" class="ltx_text ltx_font_bold">Relevant factors</span></span>
</span>
</td>
<td id="A2.T7.3.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.12.2.1.1" class="ltx_p" style="width:284.5pt;">Falcon-RW models are trained on English data only, and will not generalize appropriately to other languages. Furthermore, as they are trained on a large-scale corpora representative of the web, they will carry the stereotypes and biases commonly encountered online.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.13" class="ltx_tr">
<td id="A2.T7.3.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.13.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.13.1.1.1.1" class="ltx_text ltx_font_bold">Evaluation factors</span></span>
</span>
</td>
<td id="A2.T7.3.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.13.2.1.1" class="ltx_p" style="width:284.5pt;">We evaluated the toxicity of the underlying pretraining dataset and found it to be in line with common curated pretraining datasets such as The Pile (see <a href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>). Note that this only accounts for toxicity under the definition of Perspective API: ”content that is rude or disrespectful”. Notably, this fails to include concerns about social biases or harmfulness.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.14" class="ltx_tr">
<td id="A2.T7.3.14.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.14.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Metrics</span></td>
</tr>
<tr id="A2.T7.3.15" class="ltx_tr">
<td id="A2.T7.3.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.15.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.15.1.1.1.1" class="ltx_text ltx_font_bold">Model performance measures</span></span>
</span>
</td>
<td id="A2.T7.3.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.15.2.1.1" class="ltx_p" style="width:284.5pt;">We focus our evaluation on measuring the zero-shot generalization capabilities of our models across a wide range of tasks, leveraging the Eleuther AI language model evaluation harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.16" class="ltx_tr">
<td id="A2.T7.3.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.16.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.16.1.1.1.1" class="ltx_text ltx_font_bold">Variation approaches</span></span>
</span>
</td>
<td id="A2.T7.3.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.16.2.1.1" class="ltx_p" style="width:284.5pt;">Due to the costs associated with training Falcon-RW we cannot train the models multiple times and measure variability across training runs.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.17" class="ltx_tr">
<td id="A2.T7.3.17.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.17.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Evaluation data</span></td>
</tr>
<tr id="A2.T7.3.18" class="ltx_tr">
<td id="A2.T7.3.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.18.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.18.1.1.1.1" class="ltx_text ltx_font_bold">Datasets</span></span>
</span>
</td>
<td id="A2.T7.3.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.18.2.1.1" class="ltx_p" style="width:284.5pt;">We evaluate zero-shot accuracy on 18 varied tasks, detailed in <a href="#S3.T3" title="In 3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.19" class="ltx_tr">
<td id="A2.T7.3.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.19.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.19.1.1.1.1" class="ltx_text ltx_font_bold">Motivation</span></span>
</span>
</td>
<td id="A2.T7.3.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.19.2.1.1" class="ltx_p" style="width:284.5pt;">We selected and aggregated tasks to build comparisons with other models in the literature (see <a href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1</span></a>; <a href="#A6.SS1" title="F.1 Task aggregates ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">F.1</span></a> for details).</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.20" class="ltx_tr">
<td id="A2.T7.3.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.20.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.20.1.1.1.1" class="ltx_text ltx_font_bold">Preprocessing</span></span>
</span>
</td>
<td id="A2.T7.3.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.20.2.1.1" class="ltx_p" style="width:284.5pt;">We use the default prompts and setup of <cite class="ltx_cite ltx_citemacro_citet">Gao et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.21" class="ltx_tr">
<td id="A2.T7.3.21.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.21.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Training data</span></td>
</tr>
<tr id="A2.T7.3.22" class="ltx_tr">
<td id="A2.T7.3.22.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_bb ltx_border_t" colspan="2"><span id="A2.T7.3.22.1.1" class="ltx_text ltx_font_bold">See the dedicated datasheet in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:datasheet</span>.</span></td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Dataset analysis</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">The large-scale and diverse nature of web corpora make them difficult to document and analyse as a whole; we provide some key metrics in the section, focusing on document lengths in <a href="#A3.F5.sf1" title="In Figure 5 ‣ Appendix C Dataset analysis ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5(a)</span></a>, and a breakdown of the top domain names in <a href="#A3.F5.sf2" title="In Figure 5 ‣ Appendix C Dataset analysis ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5(b)</span></a>. We also refer to the analysis of the distribution of toxic content presented in <a href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01116/assets/x6.png" id="A3.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="184" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F5.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A3.F5.sf1.3.2" class="ltx_text" style="font-size:80%;">Document Lengths</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01116/assets/x7.png" id="A3.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="231" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F5.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A3.F5.sf2.3.2" class="ltx_text" style="font-size:80%;">Top domains</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="A3.F5.6.1" class="ltx_text ltx_font_bold">Make-up of <span id="A3.F5.6.1.1" class="ltx_text" style="color:#DB57B2;">RefinedWeb</span> in document lengths (left) and top domains (right).</span> (a) We find the OSCAR datasets and <span id="A3.F5.7.2" class="ltx_text" style="color:#5E57D3;">RW-Raw</span> to have similar document length distributions; following filtering, most of the short documents are discarded from <span id="A3.F5.8.3" class="ltx_text" style="color:#B55DD4;">RW-Filtered</span>. As deduplication removes spans, it reintroduces shorter documents to <span id="A3.F5.9.4" class="ltx_text" style="color:#DB57B2;">RefinedWeb</span>. We note the make-up of C4 and RefinedWeb to be relatively similar, with a longer tail of short documents for RefinedWeb. Finally, <span id="A3.F5.10.5" class="ltx_text" style="color:#7DD86E;">The Pile</span> exhibit a unique make-up, with a long tail of both long (books, etc.) and short documents. (b) Top domains in RefinedWeb span from popular content platforms (Blogspot, WordPress, Tumblr, etc.), to news websites (CNN, New York Times, etc.), and include also technical content such as BioMed Central or Springer.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Multilingual RefinedWeb</h2>

<section id="A4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multilingual data.</h5>

<div id="A4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p1.1" class="ltx_p">Using the language identification filter, we classify processed CommonCrawl data into 176 languages. Figure&nbsp;<a href="#A4.F6" title="Figure 6 ‣ Multilingual data. ‣ Appendix D Multilingual RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the top 20 languages present in the data <span id="A4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">excluding English</span>, based on their relative contribution in descending order. 58.20% of all documents in the processed CommonCrawl data were identified as English. We find the distribution of languages in CommonCrawl to only be partially aligned with the worldwide distribution of language speakers <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et&nbsp;al., <a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>: Russian is over-represented (2nd in CC but only 8th worldwide), Mandarin Chinese is under-represented (6-7th in CC but 2nd worldwide), and Hindi does not show-up in the top 20 despite being the 3rd most spoken.</p>
</div>
<figure id="A4.F6" class="ltx_figure"><img src="/html/2306.01116/assets/x8.png" id="A4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="A4.F6.2.1" class="ltx_text ltx_font_bold">Top 20 languages (excluding English) from processed CommonCrawl based on number of documents and disk size.</span></figcaption>
</figure>
</section>
<section id="A4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Processing multilingual data.</h5>

<div id="A4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px2.p1.1" class="ltx_p">The MDR pipeline can be used to process all languages: features such as text extraction are language-agnostic, whereas specific filters such as line-wise corrections need to typically be tuned for each individual language. We also found tuning deduplication parameters for individual languages to be beneficial.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Additional results</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">In this section, we present additional results obtained during the development of the Macrodata Refinement pipeline. For <a href="#A5.SS1" title="E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">E.1</span></a> and <a href="#A5.SS3" title="E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">E.3</span></a>, these were obtained using earlier development versions of the dataset, so results are not directly comparable with the main text. For <a href="#A5.SS2" title="E.2 Language modeling evaluation ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">E.2</span></a>, this is based on the Falcon-RW models.</p>
</div>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Small-scale ablations on deduplication approaches</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p id="A5.SS1.p1.1" class="ltx_p">We present results in <a href="#A5.T8" title="In E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>–the setup is similar to our earlier ablations, training 1B models for 30GT. We observe that:</p>
<ul id="A5.I1" class="ltx_itemize">
<li id="A5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i1.p1" class="ltx_para">
<p id="A5.I1.i1.p1.1" class="ltx_p"><span id="A5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">MinHash alone is insufficient</span>, as it doesn’t match the zero-shot performance of exact deduplication. Conversely, combining it with exact deduplication doesn’t improve performance further.</p>
</div>
</li>
<li id="A5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i2.p1" class="ltx_para">
<p id="A5.I1.i2.p1.1" class="ltx_p"><span id="A5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Masking spanned duplicates degrades performance</span>, systematically underperforming other approaches. Dropping and cutting spans perform similarly, although it’s likely that dropping documents slightly outperforms cutting.</p>
</div>
</li>
</ul>
</div>
<div id="A5.SS1.p2" class="ltx_para">
<p id="A5.SS1.p2.1" class="ltx_p">Finally, we chose to apply MinHash before exact deduplication, as it is easier to scale: approximate deduplication acts as a pruning phase, enabling us to scale deduplication further. Finally, we choose the common option of cutting spans, as dropping resulted in even more stringent rejection rates which would have compromised our ability to collect 5 trillion tokens.</p>
</div>
<figure id="A5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="A5.T8.11.1" class="ltx_text ltx_font_bold">MinHash alone is insufficient to match the performance of exact substring deduplication, and combining the two does not significantly improve performance. Of all of the exact substring approaches, masking duplicated spans underperform, but all others exhibit similar performance.</span> <math id="A5.T8.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A5.T8.2.m1.1b"><mi mathvariant="normal" id="A5.T8.2.m1.1.1" xref="A5.T8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T8.2.m1.1c"><ci id="A5.T8.2.m1.1.1.cmml" xref="A5.T8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.2.m1.1d">\checkmark</annotation></semantics></math> Minhash + Exact substring-Cut corresponds to our final deduplication setup. Perplexity in bits-per-bytes on The Pile (<span id="A5.T8.12.2" class="ltx_text ltx_font_typewriter">pile-bpb</span>, lower is better), zero-shot performance aggregated over LAMBADA, PIQA, and HellaSwag (<span id="A5.T8.13.3" class="ltx_text ltx_font_typewriter">agg-dev</span>). Best results in <span id="A5.T8.14.4" class="ltx_text ltx_font_bold">bold</span>, best results with minhash in <span id="A5.T8.15.5" class="ltx_text ltx_framed ltx_framed_underline">underline</span>, table sorted by increasing <span id="A5.T8.16.6" class="ltx_text ltx_font_typewriter">agg-dev-1</span>.</figcaption>
<p id="A5.T8.4" class="ltx_p ltx_align_center ltx_align_center"><span id="A5.T8.4.2" class="ltx_text">
<span id="A5.T8.4.2.2" class="ltx_tabular ltx_align_middle">
<span id="A5.T8.4.2.2.2" class="ltx_tr">
<span id="A5.T8.4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.4.2.2.2.3.1" class="ltx_text ltx_font_bold">Minhash</span></span>
<span id="A5.T8.4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.4.2.2.2.4.1" class="ltx_text ltx_font_bold">Exact substring</span></span>
<span id="A5.T8.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.3.1.1.1.1.1" class="ltx_text ltx_font_typewriter">pile-bpb</span> <math id="A5.T8.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A5.T8.3.1.1.1.1.m1.1a"><mo stretchy="false" id="A5.T8.3.1.1.1.1.m1.1.1" xref="A5.T8.3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A5.T8.3.1.1.1.1.m1.1b"><ci id="A5.T8.3.1.1.1.1.m1.1.1.cmml" xref="A5.T8.3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="A5.T8.4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.4.2.2.2.2.1" class="ltx_text ltx_font_typewriter">agg-dev-1</span> <math id="A5.T8.4.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A5.T8.4.2.2.2.2.m1.1a"><mo stretchy="false" id="A5.T8.4.2.2.2.2.m1.1.1" xref="A5.T8.4.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A5.T8.4.2.2.2.2.m1.1b"><ci id="A5.T8.4.2.2.2.2.m1.1.1.cmml" xref="A5.T8.4.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.4.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math></span></span>
<span id="A5.T8.4.2.2.3" class="ltx_tr">
<span id="A5.T8.4.2.2.3.1" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2"><span id="A5.T8.4.2.2.3.1.1" class="ltx_text" style="color:#B55DD4;">RefinedWeb-Filtered</span></span>
<span id="A5.T8.4.2.2.3.2" class="ltx_td ltx_align_center ltx_border_t">1.11</span>
<span id="A5.T8.4.2.2.3.3" class="ltx_td ltx_align_center ltx_border_t">43.51</span></span>
<span id="A5.T8.4.2.2.4" class="ltx_tr">
<span id="A5.T8.4.2.2.4.1" class="ltx_td ltx_border_t"></span>
<span id="A5.T8.4.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t">Mask</span>
<span id="A5.T8.4.2.2.4.3" class="ltx_td ltx_align_center ltx_border_t">1.08</span>
<span id="A5.T8.4.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t">45.84</span></span>
<span id="A5.T8.4.2.2.5" class="ltx_tr">
<span id="A5.T8.4.2.2.5.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.5.2" class="ltx_td ltx_align_center">Mask</span>
<span id="A5.T8.4.2.2.5.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.5.4" class="ltx_td ltx_align_center">46.28</span></span>
<span id="A5.T8.4.2.2.6" class="ltx_tr">
<span id="A5.T8.4.2.2.6.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.6.2" class="ltx_td"></span>
<span id="A5.T8.4.2.2.6.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.6.4" class="ltx_td ltx_align_center">46.57</span></span>
<span id="A5.T8.4.2.2.7" class="ltx_tr">
<span id="A5.T8.4.2.2.7.1" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.7.1.1" class="ltx_text" style="color:#DB57B2;">✓</span></span>
<span id="A5.T8.4.2.2.7.2" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.7.2.1" class="ltx_text" style="color:#DB57B2;">Cut</span></span>
<span id="A5.T8.4.2.2.7.3" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.7.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">1.05</span></span>
<span id="A5.T8.4.2.2.7.4" class="ltx_td ltx_align_center">47.11</span></span>
<span id="A5.T8.4.2.2.8" class="ltx_tr">
<span id="A5.T8.4.2.2.8.1" class="ltx_td"></span>
<span id="A5.T8.4.2.2.8.2" class="ltx_td ltx_align_center">Cut</span>
<span id="A5.T8.4.2.2.8.3" class="ltx_td ltx_align_center">1.06</span>
<span id="A5.T8.4.2.2.8.4" class="ltx_td ltx_align_center">47.24</span></span>
<span id="A5.T8.4.2.2.9" class="ltx_tr">
<span id="A5.T8.4.2.2.9.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.9.2" class="ltx_td ltx_align_center">Drop partial</span>
<span id="A5.T8.4.2.2.9.3" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.9.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">1.05</span></span>
<span id="A5.T8.4.2.2.9.4" class="ltx_td ltx_align_center">47.25</span></span>
<span id="A5.T8.4.2.2.10" class="ltx_tr">
<span id="A5.T8.4.2.2.10.1" class="ltx_td"></span>
<span id="A5.T8.4.2.2.10.2" class="ltx_td ltx_align_center">Drop any</span>
<span id="A5.T8.4.2.2.10.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.10.4" class="ltx_td ltx_align_center">47.77</span></span>
<span id="A5.T8.4.2.2.11" class="ltx_tr">
<span id="A5.T8.4.2.2.11.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.11.2" class="ltx_td ltx_align_center">Drop any</span>
<span id="A5.T8.4.2.2.11.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.11.4" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.11.4.1" class="ltx_text ltx_framed ltx_framed_underline">47.86</span></span></span>
<span id="A5.T8.4.2.2.12" class="ltx_tr">
<span id="A5.T8.4.2.2.12.1" class="ltx_td"></span>
<span id="A5.T8.4.2.2.12.2" class="ltx_td ltx_align_center">Drop partial</span>
<span id="A5.T8.4.2.2.12.3" class="ltx_td ltx_align_center">1.06</span>
<span id="A5.T8.4.2.2.12.4" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.12.4.1" class="ltx_text ltx_font_bold">47.97</span></span></span>
<span id="A5.T8.4.2.2.13" class="ltx_tr">
<span id="A5.T8.4.2.2.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t ltx_colspan ltx_colspan_2"><span id="A5.T8.4.2.2.13.1.1" class="ltx_text" style="color:#7DD86E;">Pile</span></span>
<span id="A5.T8.4.2.2.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.88</span>
<span id="A5.T8.4.2.2.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">43.70</span></span>
</span></span></p>
</figure>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Language modeling evaluation</h3>

<div id="A5.SS2.p1" class="ltx_para">
<p id="A5.SS2.p1.1" class="ltx_p">Along with our aggregates, we also evaluated perplexity on Wikitext (<a href="#A5.T9" title="In E.2 Language modeling evaluation ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a>). We found that models trained on RefinedWeb achieve performance close to that of models trained on The Pile. Importantly, we note that RefinedWeb does not contain any content from Wikipedia – it is explicitly filtered out at the URL level. We believe this accounts for most of the difference in perplexity, as RW models may not be familiar with the idiosyncrasies of Wikitext (e.g., layout of an article, etc.)</p>
</div>
<figure id="A5.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span><span id="A5.T9.4.1" class="ltx_text ltx_font_bold">Models trained on <span id="A5.T9.4.1.1" class="ltx_text" style="color:#DB57B2;">RefinedWeb</span> achieve performance close to models trained on <span id="A5.T9.4.1.2" class="ltx_text" style="color:#7DD86E;">The Pile</span> on Wikitext, despite not having seen any content from Wikipedia.</span> Perplexity in bits-per-bytes on Wikitext (<span id="A5.T9.5.2" class="ltx_text ltx_font_typewriter">wiki-bpb</span>, lower is better.)</figcaption>
<p id="A5.T9.1" class="ltx_p ltx_align_center ltx_align_center"><span id="A5.T9.1.1" class="ltx_text">
<span id="A5.T9.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A5.T9.1.1.1.2" class="ltx_tr">
<span id="A5.T9.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A5.T9.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Model size</span></span>
<span id="A5.T9.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T9.1.1.1.2.2.1" class="ltx_text ltx_font_bold">1B</span></span>
<span id="A5.T9.1.1.1.2.3" class="ltx_td ltx_border_tt"></span>
<span id="A5.T9.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T9.1.1.1.2.4.1" class="ltx_text ltx_font_bold">7B</span></span></span>
<span id="A5.T9.1.1.1.3" class="ltx_tr">
<span id="A5.T9.1.1.1.3.1" class="ltx_td ltx_align_left"><span id="A5.T9.1.1.1.3.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
<span id="A5.T9.1.1.1.3.2" class="ltx_td ltx_align_center"><span id="A5.T9.1.1.1.3.2.1" class="ltx_text ltx_font_bold" style="color:#7DD86E;">The Pile</span></span>
<span id="A5.T9.1.1.1.3.3" class="ltx_td ltx_align_center"><span id="A5.T9.1.1.1.3.3.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">RW</span></span>
<span id="A5.T9.1.1.1.3.4" class="ltx_td ltx_align_center"><span id="A5.T9.1.1.1.3.4.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">RW</span></span></span>
<span id="A5.T9.1.1.1.1" class="ltx_tr">
<span id="A5.T9.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A5.T9.1.1.1.1.1.1" class="ltx_text ltx_font_typewriter">wiki-bpb</span> <math id="A5.T9.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A5.T9.1.1.1.1.1.m1.1a"><mo stretchy="false" id="A5.T9.1.1.1.1.1.m1.1.1" xref="A5.T9.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A5.T9.1.1.1.1.1.m1.1b"><ci id="A5.T9.1.1.1.1.1.m1.1.1.cmml" xref="A5.T9.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T9.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="A5.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.64</span>
<span id="A5.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.66</span>
<span id="A5.T9.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.60</span></span>
</span></span></p>
</figure>
</section>
<section id="A5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.3 </span>Does deduplication help with multiple epochs?</h3>

<div id="A5.SS3.p1" class="ltx_para">
<p id="A5.SS3.p1.1" class="ltx_p">Earlier in this work, we outlined that to scale pretraining data, practitioners had two choices: (1) improve data collection, which is the avenue we chose to pursue; (2) train models on multiple epochs of the same data. Due to current uncertainties in the ability of larger models to sustain multiple epochs without adverse effects <cite class="ltx_cite ltx_citemacro_citep">(Hernandez et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>, we focused on (1). A fairly rational question regarding (2) is whether deduplication may improve the situation, and whether deduplicated data may be able to sustain more epochs without compromising model quality.</p>
</div>
<div id="A5.SS3.p2" class="ltx_para">
<p id="A5.SS3.p2.1" class="ltx_p">We train 1B parameters models on 30GT of RW and RW-Filtered. We keep the number of pretraining tokens fixed, but train for 1, 5, 25, and 100 epochs. This is a small-scale, limited set-up, which would have to be improved to obtain definitive results. We plot the degradation in performance compared to a single epoch in <a href="#A5.F7.sf1" title="In Figure 7 ‣ E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">7(a)</span></a> and the gap between RW and RW-F in <a href="#A5.F7.sf2" title="In Figure 7 ‣ E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">7(b)</span></a>. We find that the absolute degradation is less important for RefinedWeb than for RefinedWeb-Filtered; furthermore, the gap widens with increasing number of epochs. However, we observe significant variability across tasks.</p>
</div>
<figure id="A5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01116/assets/x9.png" id="A5.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="152" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A5.F7.sf1.3.2" class="ltx_text" style="font-size:80%;">Degradation compared to 1 epoch</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01116/assets/x10.png" id="A5.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="150" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A5.F7.sf2.3.2" class="ltx_text" style="font-size:80%;">Gap between RW and RW-F</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A5.F7.5.1" class="ltx_text ltx_font_bold">Deduplication may reduce the degradation in performance incurred by multiple epochs.</span> However, our experiments were only performed at small-scale (1B models trained on 30GT), and we see high variability in outcomes across tasks. Zero-shot performance measured on the <span id="A5.F7.6.2" class="ltx_text ltx_font_typewriter">agg-dev-2</span> aggregate (HellaSwag, PIQA, ARC, BoolQ, COPA, MRPC, SciQ). Individual curves for per-task results and 1-<math id="A5.F7.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="A5.F7.2.m1.1b"><mi id="A5.F7.2.m1.1.1" xref="A5.F7.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A5.F7.2.m1.1c"><ci id="A5.F7.2.m1.1.1.cmml" xref="A5.F7.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.F7.2.m1.1d">\sigma</annotation></semantics></math> standard deviation across all tasks in the aggregate in transparent.</figcaption>
</figure>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Tasks, models, and datasets from the state-of-the-art</h2>

<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Task aggregates</h3>

<div id="A6.SS1.p1" class="ltx_para">
<p id="A6.SS1.p1.1" class="ltx_p">To evaluate models, we average zero-shot performance over diverse task aggregates Our aggregates are outlined in <a href="#S3.T3" title="In 3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>:</p>
<ul id="A6.I1" class="ltx_itemize">
<li id="A6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i1.p1" class="ltx_para">
<p id="A6.I1.i1.p1.1" class="ltx_p"><span id="A6.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">small</span>: small-scale ablation studies, taskswith non-zero performance for 1B parameters models trained on 30GT;</p>
</div>
</li>
<li id="A6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i2.p1" class="ltx_para">
<p id="A6.I1.i2.p1.1" class="ltx_p"><span id="A6.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">core</span>: comparisons with a wide range of models, notably based on the tasks reported in <cite class="ltx_cite ltx_citemacro_citep">(Dey et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>;</p>
</div>
</li>
<li id="A6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i3.p1" class="ltx_para">
<p id="A6.I1.i3.p1.1" class="ltx_p"><span id="A6.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">main</span>: tasks available in the GPT-3 and PaLM papers <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Chowdhery et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>;</p>
</div>
</li>
<li id="A6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i4.p1" class="ltx_para">
<p id="A6.I1.i4.p1.1" class="ltx_p"><span id="A6.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">ext</span>: tasks available in the work of the BigScience Architecture and Scaling group <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="A6.SS1.p2" class="ltx_para">
<p id="A6.SS1.p2.1" class="ltx_p">When comparing with models from the state-of-the-art, we source results from a few different papers, detailed in <a href="#A6.T10" title="In F.1 Task aggregates ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="A6.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span><span id="A6.T10.25.1" class="ltx_text ltx_font_bold">We source evaluation results from a variety of papers across the literature, maximizing task coverage.</span> Although most results come from the EAI Evaluation Harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>, results from PaLM and GPT-3 are sourced from their respective papers. Note in Figure <a href="#S0.F1" title="Figure 1 ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> that the results from the GPT-3 paper are still ahead of results obtained through the API with the EAI evaluation harness.</figcaption>
<table id="A6.T10.23" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A6.T10.23.24" class="ltx_tr">
<td id="A6.T10.23.24.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="A6.T10.23.24.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.2.1" class="ltx_text ltx_font_bold">Aggregates reported</span></td>
<td id="A6.T10.23.24.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.3.1" class="ltx_text ltx_font_bold">Source of results</span></td>
<td id="A6.T10.23.24.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.4.1" class="ltx_text ltx_font_bold">EAI eval harness?</span></td>
</tr>
<tr id="A6.T10.1.1" class="ltx_tr">
<td id="A6.T10.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Ours</td>
<td id="A6.T10.1.1.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="A6.T10.1.1.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.1.1.3.2" class="ltx_text ltx_font_typewriter">core</span>, <span id="A6.T10.1.1.3.3" class="ltx_text ltx_font_typewriter">ext</span>
</td>
<td id="A6.T10.1.1.4" class="ltx_td ltx_align_center ltx_border_t">This paper</td>
<td id="A6.T10.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A6.T10.1.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.1.1.1.m1.1a"><mi mathvariant="normal" id="A6.T10.1.1.1.m1.1.1" xref="A6.T10.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.1.1.1.m1.1b"><ci id="A6.T10.1.1.1.m1.1.1.cmml" xref="A6.T10.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.1.1.1.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.3.3" class="ltx_tr">
<td id="A6.T10.2.2.1" class="ltx_td ltx_align_center">BS-A&amp;S<sup id="A6.T10.2.2.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.3.3.3" class="ltx_td ltx_align_center">
<span id="A6.T10.3.3.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.3.3.3.2" class="ltx_text ltx_font_typewriter">core</span>
</td>
<td id="A6.T10.3.3.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="A6.T10.3.3.2" class="ltx_td ltx_align_center"><math id="A6.T10.3.3.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.3.3.2.m1.1a"><mi mathvariant="normal" id="A6.T10.3.3.2.m1.1.1" xref="A6.T10.3.3.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.3.3.2.m1.1b"><ci id="A6.T10.3.3.2.m1.1.1.cmml" xref="A6.T10.3.3.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.3.3.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.5.5" class="ltx_tr">
<td id="A6.T10.4.4.1" class="ltx_td ltx_align_center">GPT-Neo<sup id="A6.T10.4.4.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.5.5.3" class="ltx_td ltx_align_center">
<span id="A6.T10.5.5.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.5.5.3.2" class="ltx_text ltx_font_typewriter">core</span>
</td>
<td id="A6.T10.5.5.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="A6.T10.5.5.2" class="ltx_td ltx_align_center"><math id="A6.T10.5.5.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.5.5.2.m1.1a"><mi mathvariant="normal" id="A6.T10.5.5.2.m1.1.1" xref="A6.T10.5.5.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.5.5.2.m1.1b"><ci id="A6.T10.5.5.2.m1.1.1.cmml" xref="A6.T10.5.5.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.5.5.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.6.6" class="ltx_tr">
<td id="A6.T10.6.6.1" class="ltx_td ltx_align_center">PaLM<sup id="A6.T10.6.6.1.1" class="ltx_sup">†</sup>
</td>
<td id="A6.T10.6.6.2" class="ltx_td ltx_align_center"><span id="A6.T10.6.6.2.1" class="ltx_text ltx_font_typewriter">main</span></td>
<td id="A6.T10.6.6.3" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Chowdhery et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.6.6.4" class="ltx_td"></td>
</tr>
<tr id="A6.T10.8.8" class="ltx_tr">
<td id="A6.T10.7.7.1" class="ltx_td ltx_align_center">GPT-3 API<sup id="A6.T10.7.7.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.8.8.3" class="ltx_td ltx_align_center">
<span id="A6.T10.8.8.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.8.8.3.2" class="ltx_text ltx_font_typewriter">core</span>
</td>
<td id="A6.T10.8.8.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="A6.T10.8.8.2" class="ltx_td ltx_align_center"><math id="A6.T10.8.8.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.8.8.2.m1.1a"><mi mathvariant="normal" id="A6.T10.8.8.2.m1.1.1" xref="A6.T10.8.8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.8.8.2.m1.1b"><ci id="A6.T10.8.8.2.m1.1.1.cmml" xref="A6.T10.8.8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.8.8.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.9.9" class="ltx_tr">
<td id="A6.T10.9.9.1" class="ltx_td ltx_align_center">GPT-3<sup id="A6.T10.9.9.1.1" class="ltx_sup">†</sup>
</td>
<td id="A6.T10.9.9.2" class="ltx_td ltx_align_center"><span id="A6.T10.9.9.2.1" class="ltx_text ltx_font_typewriter">main</span></td>
<td id="A6.T10.9.9.3" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="A6.T10.9.9.4" class="ltx_td"></td>
</tr>
<tr id="A6.T10.11.11" class="ltx_tr">
<td id="A6.T10.10.10.1" class="ltx_td ltx_align_center">Aleph Alpha<sup id="A6.T10.10.10.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.11.11.3" class="ltx_td ltx_align_center"><span id="A6.T10.11.11.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.11.11.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Aleph Alpha (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.11.11.2" class="ltx_td ltx_align_center"><math id="A6.T10.11.11.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.11.11.2.m1.1a"><mi mathvariant="normal" id="A6.T10.11.11.2.m1.1.1" xref="A6.T10.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.11.11.2.m1.1b"><ci id="A6.T10.11.11.2.m1.1.1.cmml" xref="A6.T10.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.11.11.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.13.13" class="ltx_tr">
<td id="A6.T10.12.12.1" class="ltx_td ltx_align_center">Cerebras-GPT<sup id="A6.T10.12.12.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.13.13.3" class="ltx_td ltx_align_center"><span id="A6.T10.13.13.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.13.13.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.13.13.2" class="ltx_td ltx_align_center"><math id="A6.T10.13.13.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.13.13.2.m1.1a"><mi mathvariant="normal" id="A6.T10.13.13.2.m1.1.1" xref="A6.T10.13.13.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.13.13.2.m1.1b"><ci id="A6.T10.13.13.2.m1.1.1.cmml" xref="A6.T10.13.13.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.13.13.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.15.15" class="ltx_tr">
<td id="A6.T10.14.14.1" class="ltx_td ltx_align_center">FairSeq<sup id="A6.T10.14.14.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.15.15.3" class="ltx_td ltx_align_center"><span id="A6.T10.15.15.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.15.15.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.15.15.2" class="ltx_td ltx_align_center"><math id="A6.T10.15.15.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.15.15.2.m1.1a"><mi mathvariant="normal" id="A6.T10.15.15.2.m1.1.1" xref="A6.T10.15.15.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.15.15.2.m1.1b"><ci id="A6.T10.15.15.2.m1.1.1.cmml" xref="A6.T10.15.15.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.15.15.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.17.17" class="ltx_tr">
<td id="A6.T10.16.16.1" class="ltx_td ltx_align_center">Pythia(-Dedup)<sup id="A6.T10.16.16.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.17.17.3" class="ltx_td ltx_align_center"><span id="A6.T10.17.17.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.17.17.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.17.17.2" class="ltx_td ltx_align_center"><math id="A6.T10.17.17.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.17.17.2.m1.1a"><mi mathvariant="normal" id="A6.T10.17.17.2.m1.1.1" xref="A6.T10.17.17.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.17.17.2.m1.1b"><ci id="A6.T10.17.17.2.m1.1.1.cmml" xref="A6.T10.17.17.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.17.17.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.19.19" class="ltx_tr">
<td id="A6.T10.18.18.1" class="ltx_td ltx_align_center">OPT<sup id="A6.T10.18.18.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.19.19.3" class="ltx_td ltx_align_center"><span id="A6.T10.19.19.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.19.19.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.19.19.2" class="ltx_td ltx_align_center"><math id="A6.T10.19.19.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.19.19.2.m1.1a"><mi mathvariant="normal" id="A6.T10.19.19.2.m1.1.1" xref="A6.T10.19.19.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.19.19.2.m1.1b"><ci id="A6.T10.19.19.2.m1.1.1.cmml" xref="A6.T10.19.19.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.19.19.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.21.21" class="ltx_tr">
<td id="A6.T10.20.20.1" class="ltx_td ltx_align_center">GPT-J<sup id="A6.T10.20.20.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.21.21.3" class="ltx_td ltx_align_center"><span id="A6.T10.21.21.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.21.21.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.21.21.2" class="ltx_td ltx_align_center"><math id="A6.T10.21.21.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.21.21.2.m1.1a"><mi mathvariant="normal" id="A6.T10.21.21.2.m1.1.1" xref="A6.T10.21.21.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.21.21.2.m1.1b"><ci id="A6.T10.21.21.2.m1.1.1.cmml" xref="A6.T10.21.21.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.21.21.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.23.23" class="ltx_tr">
<td id="A6.T10.22.22.1" class="ltx_td ltx_align_center ltx_border_bb">GPT-NeoX 20B<sup id="A6.T10.22.22.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.23.23.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A6.T10.23.23.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.23.23.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.23.23.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="A6.T10.23.23.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.23.23.2.m1.1a"><mi mathvariant="normal" id="A6.T10.23.23.2.m1.1.1" xref="A6.T10.23.23.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.23.23.2.m1.1b"><ci id="A6.T10.23.23.2.m1.1.1.cmml" xref="A6.T10.23.23.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.23.23.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Models</h3>

<div id="A6.SS2.p1" class="ltx_para">
<p id="A6.SS2.p1.1" class="ltx_p">We compare against nearly 50 models across 10 series trained on a variety of curated corpora, presented in <a href="#A6.T11" title="In Pythia and deduplication. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<section id="A6.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cerebras-GPT with <math id="A6.SS2.SSS0.Px1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="A6.SS2.SSS0.Px1.1.m1.1b"><mi id="A6.SS2.SSS0.Px1.1.m1.1.1" xref="A6.SS2.SSS0.Px1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.SS2.SSS0.Px1.1.m1.1c"><ci id="A6.SS2.SSS0.Px1.1.m1.1.1.cmml" xref="A6.SS2.SSS0.Px1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.SS2.SSS0.Px1.1.m1.1d">\mu</annotation></semantics></math>-parametrization.</h5>

<div id="A6.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="A6.SS2.SSS0.Px1.p1.1" class="ltx_p">The Cerebras-GPT series <cite class="ltx_cite ltx_citemacro_citep">(Dey et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> also comes in a smaller series, up to 2.7B parameters, following the recommendations of <math id="A6.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="A6.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="A6.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A6.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="A6.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A6.SS2.SSS0.Px1.p1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.SS2.SSS0.Px1.p1.1.m1.1c">\mu</annotation></semantics></math>-parametrization <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib83" title="" class="ltx_ref">2021</a>)</cite>. As we found the performance of this smaller series to be close to the main series of models (see <a href="#A6.F8" title="In Cerebras-GPT with 𝜇-parametrization. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>), and as it does not include models of a similar compute scale as the ones we compare to, we chose not to report it in our main figures.</p>
</div>
<figure id="A6.F8" class="ltx_figure"><img src="/html/2306.01116/assets/x11.png" id="A6.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><math id="A6.F8.4.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="A6.F8.4.m1.1b"><mi id="A6.F8.4.m1.1.1" xref="A6.F8.4.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.F8.4.m1.1c"><ci id="A6.F8.4.m1.1.1.cmml" xref="A6.F8.4.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F8.4.m1.1d">\mu</annotation></semantics></math><span id="A6.F8.9.1" class="ltx_text ltx_font_bold">-parametrization <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib83" title="" class="ltx_ref">2021</a>)</cite> slightly improves performance in the Cerebras-GPT series <cite class="ltx_cite ltx_citemacro_citep">(Dey et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>.</span> Zero-shot performance on our <span id="A6.F8.10.2" class="ltx_text ltx_font_typewriter">core</span> aggregate, gap between Cerebras-GPT with <math id="A6.F8.5.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="A6.F8.5.m2.1b"><mi id="A6.F8.5.m2.1.1" xref="A6.F8.5.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.F8.5.m2.1c"><ci id="A6.F8.5.m2.1.1.cmml" xref="A6.F8.5.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F8.5.m2.1d">\mu</annotation></semantics></math>-param and without. Individual curves for per-task results and 1-<math id="A6.F8.6.m3.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="A6.F8.6.m3.1b"><mi id="A6.F8.6.m3.1.1" xref="A6.F8.6.m3.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A6.F8.6.m3.1c"><ci id="A6.F8.6.m3.1.1.cmml" xref="A6.F8.6.m3.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F8.6.m3.1d">\sigma</annotation></semantics></math> standard deviation across all tasks in the aggregate in transparent.</figcaption>
</figure>
</section>
<section id="A6.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pythia and deduplication.</h5>

<div id="A6.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="A6.SS2.SSS0.Px2.p1.1" class="ltx_p">The Pythia series of models is available in two flavours: one trained on the vanilla version of The Pile, and another trained on a version deduplicated with MinHash. Performance between these two flavours was noted to minimally differ <cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>; in <a href="#A6.F9" title="In Pythia and deduplication. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a>, we find the deduplicated version may be slightly ahead of the non-deduplicated one under our aggregate. The higher end of this improvement is broadly in line with our findings in <a href="#S4.T5" title="In 4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>. Nevertheless, a difference in our findings and theirs remain. We posit a few possible hypotheses:</p>
<ul id="A6.I2" class="ltx_itemize">
<li id="A6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I2.i1.p1" class="ltx_para">
<p id="A6.I2.i1.p1.1" class="ltx_p"><span id="A6.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Differences between curated and web data.</span> It is possible that web data is more sensitive to duplicates. For instance, the most common duplicates in web data (e.g., spam) may be more detrimental than the most common duplicates in curated data. This suggests a qualitative component to deduplication that we have not studied in this work.</p>
</div>
</li>
<li id="A6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I2.i2.p1" class="ltx_para">
<p id="A6.I2.i2.p1.1" class="ltx_p"><span id="A6.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Differences in deduplication pipeline.</span> Because <cite class="ltx_cite ltx_citemacro_citet">Biderman et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> uses the MinHash settings from <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, they are mostly identical to ours. However, we also apply exact deduplication: while their deduplication incurs a 30% reduction in size, our deduplication is more aggressive, resulting in a 45% reduction in size. This may explain why our results in <a href="#S4.T5" title="In 4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> show a stronger gain from deduplication than theirs in <a href="#A6.F9" title="In Pythia and deduplication. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</li>
<li id="A6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I2.i3.p1" class="ltx_para">
<p id="A6.I2.i3.p1.1" class="ltx_p"><span id="A6.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Differences in pretraining.</span> Finally, we note that <cite class="ltx_cite ltx_citemacro_citet">Biderman et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> chooses to perform a partial extra epoch on the deduplicated data to reach 300GT, while we always perform a single epoch. Their setting corresponds to a data-constrained scenario, which is more realistic for the curated data they study; for us, web data is plentiful, so deduplication never truly limits the size of the datasets we can use.</p>
</div>
</li>
</ul>
</div>
<figure id="A6.F9" class="ltx_figure"><img src="/html/2306.01116/assets/x12.png" id="A6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="A6.F9.5.1" class="ltx_text ltx_font_bold">In our <span id="A6.F9.5.1.1" class="ltx_text ltx_font_typewriter">core</span> aggregate, deduplication brings a small improvement to the Pythia suite <cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>.</span> Zero-shot performance on our <span id="A6.F9.6.2" class="ltx_text ltx_font_typewriter">core</span> aggregate, gap between Pythia trained on the deduplicated and vanilla Pile. Individual curves for per-task results and 1-<math id="A6.F9.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="A6.F9.2.m1.1b"><mi id="A6.F9.2.m1.1.1" xref="A6.F9.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A6.F9.2.m1.1c"><ci id="A6.F9.2.m1.1.1.cmml" xref="A6.F9.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F9.2.m1.1d">\sigma</annotation></semantics></math> standard deviation across all tasks in the aggregate in transparent.</figcaption>
</figure>
<figure id="A6.T11" class="ltx_table">

<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 11: </span><span id="A6.T11.30.1" class="ltx_text ltx_font_bold">Full-scale models trained on RefinedWeb (Falcon-RW) and other models from the state-of-the-art.</span> Across models trained on The Pile, the Pythia models are the closest to our achitecture: they use FlashAttention with rotary embeddings–with for only notably exception the use of parallel attention and feedforward for their models. Training budget <math id="A6.T11.5.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="A6.T11.5.m1.1b"><mi id="A6.T11.5.m1.1.1" xref="A6.T11.5.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A6.T11.5.m1.1c"><ci id="A6.T11.5.m1.1.1.cmml" xref="A6.T11.5.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.5.m1.1d">C</annotation></semantics></math> in PF-days calculated using <math id="A6.T11.6.m2.1" class="ltx_Math" alttext="C=6ND" display="inline"><semantics id="A6.T11.6.m2.1b"><mrow id="A6.T11.6.m2.1.1" xref="A6.T11.6.m2.1.1.cmml"><mi id="A6.T11.6.m2.1.1.2" xref="A6.T11.6.m2.1.1.2.cmml">C</mi><mo id="A6.T11.6.m2.1.1.1" xref="A6.T11.6.m2.1.1.1.cmml">=</mo><mrow id="A6.T11.6.m2.1.1.3" xref="A6.T11.6.m2.1.1.3.cmml"><mn id="A6.T11.6.m2.1.1.3.2" xref="A6.T11.6.m2.1.1.3.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="A6.T11.6.m2.1.1.3.1" xref="A6.T11.6.m2.1.1.3.1.cmml">​</mo><mi id="A6.T11.6.m2.1.1.3.3" xref="A6.T11.6.m2.1.1.3.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="A6.T11.6.m2.1.1.3.1b" xref="A6.T11.6.m2.1.1.3.1.cmml">​</mo><mi id="A6.T11.6.m2.1.1.3.4" xref="A6.T11.6.m2.1.1.3.4.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T11.6.m2.1c"><apply id="A6.T11.6.m2.1.1.cmml" xref="A6.T11.6.m2.1.1"><eq id="A6.T11.6.m2.1.1.1.cmml" xref="A6.T11.6.m2.1.1.1"></eq><ci id="A6.T11.6.m2.1.1.2.cmml" xref="A6.T11.6.m2.1.1.2">𝐶</ci><apply id="A6.T11.6.m2.1.1.3.cmml" xref="A6.T11.6.m2.1.1.3"><times id="A6.T11.6.m2.1.1.3.1.cmml" xref="A6.T11.6.m2.1.1.3.1"></times><cn type="integer" id="A6.T11.6.m2.1.1.3.2.cmml" xref="A6.T11.6.m2.1.1.3.2">6</cn><ci id="A6.T11.6.m2.1.1.3.3.cmml" xref="A6.T11.6.m2.1.1.3.3">𝑁</ci><ci id="A6.T11.6.m2.1.1.3.4.cmml" xref="A6.T11.6.m2.1.1.3.4">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.6.m2.1d">C=6ND</annotation></semantics></math>, with <math id="A6.T11.7.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A6.T11.7.m3.1b"><mi id="A6.T11.7.m3.1.1" xref="A6.T11.7.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A6.T11.7.m3.1c"><ci id="A6.T11.7.m3.1.1.cmml" xref="A6.T11.7.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.7.m3.1d">N</annotation></semantics></math> the number of parameters, and <math id="A6.T11.8.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="A6.T11.8.m4.1b"><mi id="A6.T11.8.m4.1.1" xref="A6.T11.8.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A6.T11.8.m4.1c"><ci id="A6.T11.8.m4.1.1.cmml" xref="A6.T11.8.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.8.m4.1d">D</annotation></semantics></math> the pretraining dataset size <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et&nbsp;al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A6.T11.12" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody><tr id="A6.T11.12.4" class="ltx_tr">
<td id="A6.T11.12.4.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A6.T11.12.4.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Series</span></td>
<td id="A6.T11.9.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="A6.T11.9.1.1.1" class="ltx_text" style="font-size:70%;">GPT-3 (paper)</span><sup id="A6.T11.9.1.1.2" class="ltx_sup"><span id="A6.T11.9.1.1.2.1" class="ltx_text" style="font-size:70%;">†</span></sup>
</td>
<td id="A6.T11.10.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="A6.T11.10.2.2.1" class="ltx_text" style="font-size:70%;">GPT-3 (API)</span><sup id="A6.T11.10.2.2.2" class="ltx_sup"><span id="A6.T11.10.2.2.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.11.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.11.3.3.1" class="ltx_text" style="font-size:70%;">BigScience</span><sup id="A6.T11.11.3.3.2" class="ltx_sup"><span id="A6.T11.11.3.3.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.12.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.12.4.4.1" class="ltx_text" style="font-size:70%;">PaLM</span><sup id="A6.T11.12.4.4.2" class="ltx_sup"><span id="A6.T11.12.4.4.2.1" class="ltx_text" style="font-size:70%;">†</span></sup>
</td>
<td id="A6.T11.12.4.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="A6.T11.12.4.6.1" class="ltx_text" style="font-size:70%;">Ours</span></td>
</tr>
<tr id="A6.T11.12.5" class="ltx_tr">
<td id="A6.T11.12.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T11.12.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model</span></td>
<td id="A6.T11.12.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.2.1" class="ltx_text" style="font-size:70%;">XL</span></td>
<td id="A6.T11.12.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.3.1" class="ltx_text" style="font-size:70%;">XXL</span></td>
<td id="A6.T11.12.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.4.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">babbage</span></td>
<td id="A6.T11.12.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.5.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">curie</span></td>
<td id="A6.T11.12.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.6.1" class="ltx_text" style="font-size:70%;">BS-A&amp;S</span></td>
<td id="A6.T11.12.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.7.1" class="ltx_text" style="font-size:70%;">PaLM-8B</span></td>
<td id="A6.T11.12.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.8.1" class="ltx_text" style="font-size:70%;">Ours (Pile)</span></td>
<td id="A6.T11.12.5.9" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="A6.T11.12.5.9.1" class="ltx_text" style="font-size:70%;">Falcon-RW</span></td>
</tr>
<tr id="A6.T11.12.6" class="ltx_tr">
<td id="A6.T11.12.6.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></td>
<td id="A6.T11.12.6.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.2.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.3.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.4.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.5.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.6.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.12.6.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.7.1" class="ltx_text" style="font-size:70%;">PaLM</span></td>
<td id="A6.T11.12.6.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.8.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.12.6.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.9.1" class="ltx_text" style="font-size:70%;">RW</span></td>
<td id="A6.T11.12.6.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.10.1" class="ltx_text" style="font-size:70%;">RW</span></td>
</tr>
<tr id="A6.T11.12.7" class="ltx_tr">
<td id="A6.T11.12.7.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.7.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Params.</span></td>
<td id="A6.T11.12.7.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.2.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.3.1" class="ltx_text" style="font-size:70%;">6.7B</span></td>
<td id="A6.T11.12.7.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.4.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.5.1" class="ltx_text" style="font-size:70%;">6.7B</span></td>
<td id="A6.T11.12.7.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.6.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.7.1" class="ltx_text" style="font-size:70%;">8.6B</span></td>
<td id="A6.T11.12.7.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.8.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.9.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.10.1" class="ltx_text" style="font-size:70%;">7.5B</span></td>
</tr>
<tr id="A6.T11.12.8" class="ltx_tr">
<td id="A6.T11.12.8.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Pretraining</span></td>
<td id="A6.T11.12.8.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.2.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.3.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.4.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.5.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.6.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.7.1" class="ltx_text" style="font-size:70%;">780GT</span></td>
<td id="A6.T11.12.8.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.8.1" class="ltx_text" style="font-size:70%;">350GT</span></td>
<td id="A6.T11.12.8.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.9.1" class="ltx_text" style="font-size:70%;">350GT</span></td>
<td id="A6.T11.12.8.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.10.1" class="ltx_text" style="font-size:70%;">350GT</span></td>
</tr>
<tr id="A6.T11.12.9" class="ltx_tr">
<td id="A6.T11.12.9.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.9.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">PF-days</span></td>
<td id="A6.T11.12.9.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.2.1" class="ltx_text" style="font-size:70%;">27</span></td>
<td id="A6.T11.12.9.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.3.1" class="ltx_text" style="font-size:70%;">140</span></td>
<td id="A6.T11.12.9.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.4.1" class="ltx_text" style="font-size:70%;">27</span></td>
<td id="A6.T11.12.9.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.5.1" class="ltx_text" style="font-size:70%;">140</span></td>
<td id="A6.T11.12.9.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.6.1" class="ltx_text" style="font-size:70%;">27</span></td>
<td id="A6.T11.12.9.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.7.1" class="ltx_text" style="font-size:70%;">466</span></td>
<td id="A6.T11.12.9.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.8.1" class="ltx_text" style="font-size:70%;">32</span></td>
<td id="A6.T11.12.9.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.9.1" class="ltx_text" style="font-size:70%;">32</span></td>
<td id="A6.T11.12.9.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.10.1" class="ltx_text" style="font-size:70%;">182</span></td>
</tr>
<tr id="A6.T11.12.10" class="ltx_tr">
<td id="A6.T11.12.10.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T11.12.10.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Citation</span></td>
<td id="A6.T11.12.10.2" class="ltx_td ltx_align_center ltx_border_bb" colspan="4"><cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. <span id="A6.T11.12.10.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib18" title="" class="ltx_ref">2020</a><span id="A6.T11.12.10.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.12.10.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. <span id="A6.T11.12.10.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib67" title="" class="ltx_ref">2022b</a><span id="A6.T11.12.10.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.12.10.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Chowdhery et&nbsp;al. <span id="A6.T11.12.10.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib23" title="" class="ltx_ref">2022</a><span id="A6.T11.12.10.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.12.10.5" class="ltx_td ltx_align_center ltx_border_bb" colspan="3"><span id="A6.T11.12.10.5.1" class="ltx_text" style="font-size:70%;">This paper</span></td>
</tr>
</tbody></table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A6.T11.14" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody><tr id="A6.T11.14.2" class="ltx_tr">
<td id="A6.T11.14.2.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A6.T11.14.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Series</span></td>
<td id="A6.T11.13.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">
<span id="A6.T11.13.1.1.1" class="ltx_text" style="font-size:70%;">EleutherAI</span><sup id="A6.T11.13.1.1.2" class="ltx_sup"><span id="A6.T11.13.1.1.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.14.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.14.2.2.1" class="ltx_text" style="font-size:70%;">Pythia</span><sup id="A6.T11.14.2.2.2" class="ltx_sup"><span id="A6.T11.14.2.2.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
</tr>
<tr id="A6.T11.14.3" class="ltx_tr">
<td id="A6.T11.14.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T11.14.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model</span></td>
<td id="A6.T11.14.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.2.1" class="ltx_text" style="font-size:70%;">GPT-Neo</span></td>
<td id="A6.T11.14.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.3.1" class="ltx_text" style="font-size:70%;">GPT-J</span></td>
<td id="A6.T11.14.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.4.1" class="ltx_text" style="font-size:70%;">GPT-NeoX 20B</span></td>
<td id="A6.T11.14.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.5.1" class="ltx_text" style="font-size:70%;">Pythia(-Dedup)</span></td>
</tr>
<tr id="A6.T11.14.4" class="ltx_tr">
<td id="A6.T11.14.4.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.4.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></td>
<td id="A6.T11.14.4.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.2.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.14.4.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.3.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.14.4.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.4.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.14.4.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.5.1" class="ltx_text" style="font-size:70%;">Pile (dedup)</span></td>
</tr>
<tr id="A6.T11.14.5" class="ltx_tr">
<td id="A6.T11.14.5.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Params.</span></td>
<td id="A6.T11.14.5.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.2.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.14.5.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.3.1" class="ltx_text" style="font-size:70%;">6.7B</span></td>
<td id="A6.T11.14.5.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.4.1" class="ltx_text" style="font-size:70%;">20B</span></td>
<td id="A6.T11.14.5.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.5.1" class="ltx_text" style="font-size:70%;">70M-12B</span></td>
</tr>
<tr id="A6.T11.14.6" class="ltx_tr">
<td id="A6.T11.14.6.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Pretraining</span></td>
<td id="A6.T11.14.6.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.2.1" class="ltx_text" style="font-size:70%;">380GT</span></td>
<td id="A6.T11.14.6.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.3.1" class="ltx_text" style="font-size:70%;">402GT</span></td>
<td id="A6.T11.14.6.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.4.1" class="ltx_text" style="font-size:70%;">472GT</span></td>
<td id="A6.T11.14.6.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.5.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
</tr>
<tr id="A6.T11.14.7" class="ltx_tr">
<td id="A6.T11.14.7.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.7.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">PF-days</span></td>
<td id="A6.T11.14.7.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.2.1" class="ltx_text" style="font-size:70%;">34</span></td>
<td id="A6.T11.14.7.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.3.1" class="ltx_text" style="font-size:70%;">187</span></td>
<td id="A6.T11.14.7.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.4.1" class="ltx_text" style="font-size:70%;">656</span></td>
<td id="A6.T11.14.7.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.5.1" class="ltx_text" style="font-size:70%;">1.5 - 250</span></td>
</tr>
<tr id="A6.T11.14.8" class="ltx_tr">
<td id="A6.T11.14.8.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T11.14.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Citation</span></td>
<td id="A6.T11.14.8.2" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. <span id="A6.T11.14.8.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib15" title="" class="ltx_ref">2021</a><span id="A6.T11.14.8.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.14.8.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Wang &amp; Komatsuzaki <span id="A6.T11.14.8.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib76" title="" class="ltx_ref">2021</a><span id="A6.T11.14.8.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.14.8.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. <span id="A6.T11.14.8.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib16" title="" class="ltx_ref">2022</a><span id="A6.T11.14.8.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.14.8.5" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Biderman et&nbsp;al. <span id="A6.T11.14.8.5.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib13" title="" class="ltx_ref">2023</a><span id="A6.T11.14.8.5.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
</tr>
</tbody></table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A6.T11.18" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody><tr id="A6.T11.18.4" class="ltx_tr">
<td id="A6.T11.18.4.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A6.T11.18.4.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Series</span></td>
<td id="A6.T11.15.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.15.1.1.1" class="ltx_text" style="font-size:70%;">Aleph Alpha</span><sup id="A6.T11.15.1.1.2" class="ltx_sup"><span id="A6.T11.15.1.1.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.16.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.16.2.2.1" class="ltx_text" style="font-size:70%;">Cerebras-GPT</span><sup id="A6.T11.16.2.2.2" class="ltx_sup"><span id="A6.T11.16.2.2.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.17.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.17.3.3.1" class="ltx_text" style="font-size:70%;">OPT</span><sup id="A6.T11.17.3.3.2" class="ltx_sup"><span id="A6.T11.17.3.3.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.18.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.18.4.4.1" class="ltx_text" style="font-size:70%;">FairSeq</span><sup id="A6.T11.18.4.4.2" class="ltx_sup"><span id="A6.T11.18.4.4.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
</tr>
<tr id="A6.T11.18.5" class="ltx_tr">
<td id="A6.T11.18.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T11.18.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model</span></td>
<td id="A6.T11.18.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.2.1" class="ltx_text" style="font-size:70%;">Luminous</span></td>
<td id="A6.T11.18.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.3.1" class="ltx_text" style="font-size:70%;">Cerebras-GPT</span></td>
<td id="A6.T11.18.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.4.1" class="ltx_text" style="font-size:70%;">OPT</span></td>
<td id="A6.T11.18.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.5.1" class="ltx_text" style="font-size:70%;">FairSeq</span></td>
</tr>
<tr id="A6.T11.18.6" class="ltx_tr">
<td id="A6.T11.18.6.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></td>
<td id="A6.T11.18.6.2" class="ltx_td ltx_align_center"><em id="A6.T11.18.6.2.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">undisclosed</em></td>
<td id="A6.T11.18.6.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.6.3.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.18.6.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.6.4.1" class="ltx_text" style="font-size:70%;">Pile (subset) + curated</span></td>
<td id="A6.T11.18.6.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.6.5.1" class="ltx_text" style="font-size:70%;">curated</span></td>
</tr>
<tr id="A6.T11.18.7" class="ltx_tr">
<td id="A6.T11.18.7.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.7.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Params.</span></td>
<td id="A6.T11.18.7.2" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.2.1" class="ltx_text" style="font-size:70%;">13B</span></td>
<td id="A6.T11.18.7.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.3.1" class="ltx_text" style="font-size:70%;">111M-13B</span></td>
<td id="A6.T11.18.7.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.4.1" class="ltx_text" style="font-size:70%;">125M - 175B</span></td>
<td id="A6.T11.18.7.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.5.1" class="ltx_text" style="font-size:70%;">1.3 - 13B</span></td>
</tr>
<tr id="A6.T11.18.8" class="ltx_tr">
<td id="A6.T11.18.8.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Pretraining</span></td>
<td id="A6.T11.18.8.2" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.2.1" class="ltx_text" style="font-size:70%;">400GT</span></td>
<td id="A6.T11.18.8.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.3.1" class="ltx_text" style="font-size:70%;">2 - 257GT</span></td>
<td id="A6.T11.18.8.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.4.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.18.8.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.5.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
</tr>
<tr id="A6.T11.18.9" class="ltx_tr">
<td id="A6.T11.18.9.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.9.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">PF-days</span></td>
<td id="A6.T11.18.9.2" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.2.1" class="ltx_text" style="font-size:70%;">361</span></td>
<td id="A6.T11.18.9.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.3.1" class="ltx_text" style="font-size:70%;">0.02 - 232</span></td>
<td id="A6.T11.18.9.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.4.1" class="ltx_text" style="font-size:70%;">3 - 3646</span></td>
<td id="A6.T11.18.9.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.5.1" class="ltx_text" style="font-size:70%;">27 - 271</span></td>
</tr>
<tr id="A6.T11.18.10" class="ltx_tr">
<td id="A6.T11.18.10.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T11.18.10.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Citation</span></td>
<td id="A6.T11.18.10.2" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Aleph Alpha <span id="A6.T11.18.10.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib5" title="" class="ltx_ref">2023</a><span id="A6.T11.18.10.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.18.10.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. <span id="A6.T11.18.10.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2023</a><span id="A6.T11.18.10.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.18.10.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. <span id="A6.T11.18.10.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib87" title="" class="ltx_ref">2022</a><span id="A6.T11.18.10.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.18.10.5" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Artetxe et&nbsp;al. <span id="A6.T11.18.10.5.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib10" title="" class="ltx_ref">2021</a><span id="A6.T11.18.10.5.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
</tr>
</tbody></table>
</div>
</div>
</figure>
</section>
</section>
<section id="A6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.3 </span>Datasets</h3>

<div id="A6.SS3.p1" class="ltx_para">
<p id="A6.SS3.p1.1" class="ltx_p">We extend on <a href="#S0.T1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> in <a href="#A6.T12" title="In F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">12</span></a>, providing details on the filtering and deduplication strategies used across the litterature.</p>
</div>
<figure id="A6.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span><span id="A6.T12.24.1" class="ltx_text ltx_font_bold">Common massive web-scrape and LLM English datasets.</span> Datasets such as OSCAR and C4 also have significant multilingual versions, which have enjoyed wide adoption&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xue et&nbsp;al., <a href="#bib.bib82" title="" class="ltx_ref">2021</a>)</cite>. For OSCAR, the size corresponds to the non-deduplicated version, and is estimated from the number of words x0,75 (average number of words per tokens). </figcaption>
<table id="A6.T12.22" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A6.T12.22.23" class="ltx_tr">
<td id="A6.T12.22.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="4"><span id="A6.T12.22.23.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">General information</span></td>
<td id="A6.T12.22.23.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="6"><span id="A6.T12.22.23.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Web data</span></td>
</tr>
<tr id="A6.T12.22.24" class="ltx_tr">
<td id="A6.T12.22.24.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.1.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></span>
</span>
</td>
<td id="A6.T12.22.24.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.2.1.1.1" class="ltx_text" style="font-size:90%;">Notable models</span></span>
</span>
</td>
<td id="A6.T12.22.24.3" class="ltx_td ltx_align_center"><span id="A6.T12.22.24.3.1" class="ltx_text" style="font-size:90%;">Size</span></td>
<td id="A6.T12.22.24.4" class="ltx_td ltx_align_center"><span id="A6.T12.22.24.4.1" class="ltx_text" style="font-size:90%;">Availability</span></td>
<td id="A6.T12.22.24.5" class="ltx_td ltx_align_center"><span id="A6.T12.22.24.5.1" class="ltx_text" style="font-size:90%;">Web</span></td>
<td id="A6.T12.22.24.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.24.6.1.1.1" class="ltx_text" style="font-size:90%;">HTML extraction</span></span>
</span>
</td>
<td id="A6.T12.22.24.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.7.1.1.1" class="ltx_text" style="font-size:90%;">Language ID</span></span>
</span>
</td>
<td id="A6.T12.22.24.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.8.1.1.1" class="ltx_text" style="font-size:90%;">Heuristics</span></span>
</span>
</td>
<td id="A6.T12.22.24.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.24.9.1.1.1" class="ltx_text" style="font-size:90%;">Content filtering</span></span>
</span>
</td>
<td id="A6.T12.22.24.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.10.1.1.1" class="ltx_text" style="font-size:90%;">Deduplication</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.22.25" class="ltx_tr">
<td id="A6.T12.22.25.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="10"><span id="A6.T12.22.25.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Massive web datasets</span></td>
</tr>
<tr id="A6.T12.2.2" class="ltx_tr">
<td id="A6.T12.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">C4</span><span id="A6.T12.2.2.3.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.2.2.3.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Raffel et&nbsp;al.<span id="A6.T12.2.2.3.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib64" title="" class="ltx_ref">2020</a><span id="A6.T12.2.2.3.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">T5 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.2.2.4.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Raffel et&nbsp;al.<span id="A6.T12.2.2.4.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib64" title="" class="ltx_ref">2020</a><span id="A6.T12.2.2.4.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.1.1.1.m1.1" class="ltx_Math" alttext="\sim 360" display="inline"><semantics id="A6.T12.1.1.1.m1.1a"><mrow id="A6.T12.1.1.1.m1.1.1" xref="A6.T12.1.1.1.m1.1.1.cmml"><mi id="A6.T12.1.1.1.m1.1.1.2" xref="A6.T12.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.1.1.1.m1.1.1.1" xref="A6.T12.1.1.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.1.1.1.m1.1.1.3" xref="A6.T12.1.1.1.m1.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.1.1.1.m1.1b"><apply id="A6.T12.1.1.1.m1.1.1.cmml" xref="A6.T12.1.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.1.1.1.m1.1.1.1.cmml" xref="A6.T12.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.1.1.1.m1.1.1.2.cmml" xref="A6.T12.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.1.1.1.m1.1.1.3.cmml" xref="A6.T12.1.1.1.m1.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.1.1.1.m1.1c">\sim 360</annotation></semantics></math><span id="A6.T12.1.1.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T12.2.2.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.2.2.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T12.2.2.2.m1.1a"><mn mathsize="90%" id="A6.T12.2.2.2.m1.1.1" xref="A6.T12.2.2.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T12.2.2.2.m1.1b"><cn type="integer" id="A6.T12.2.2.2.m1.1.1.cmml" xref="A6.T12.2.2.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.2.2.2.m1.1c">100</annotation></semantics></math><span id="A6.T12.2.2.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">.WET files</span></span>
</span>
</td>
<td id="A6.T12.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">Document-level w/ </span><span id="A6.T12.2.2.7.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">langdetect</span></span>
</span>
</td>
<td id="A6.T12.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.8.1.1.1" class="ltx_text" style="font-size:90%;">Document and line-level</span></span>
</span>
</td>
<td id="A6.T12.2.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.2.2.9.1.1.1" class="ltx_text" style="font-size:90%;">Rules-based: code, NSFW</span></span>
</span>
</td>
<td id="A6.T12.2.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="A6.T12.2.2.10.1.1.2" class="ltx_text" style="font-size:90%;">: three sentences span</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.6.6" class="ltx_tr">
<td id="A6.T12.6.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.6.6.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR 21.09</span><span id="A6.T12.6.6.5.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.6.6.5.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Ortiz Suárez et&nbsp;al.<span id="A6.T12.6.6.5.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib57" title="" class="ltx_ref">2019</a><span id="A6.T12.6.6.5.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.6.6.6" class="ltx_td ltx_align_top"></td>
<td id="A6.T12.3.3.1" class="ltx_td ltx_align_center">
<math id="A6.T12.3.3.1.m1.1" class="ltx_Math" alttext="\sim 370" display="inline"><semantics id="A6.T12.3.3.1.m1.1a"><mrow id="A6.T12.3.3.1.m1.1.1" xref="A6.T12.3.3.1.m1.1.1.cmml"><mi id="A6.T12.3.3.1.m1.1.1.2" xref="A6.T12.3.3.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.3.3.1.m1.1.1.1" xref="A6.T12.3.3.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.3.3.1.m1.1.1.3" xref="A6.T12.3.3.1.m1.1.1.3.cmml">370</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.3.3.1.m1.1b"><apply id="A6.T12.3.3.1.m1.1.1.cmml" xref="A6.T12.3.3.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.3.3.1.m1.1.1.1.cmml" xref="A6.T12.3.3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.3.3.1.m1.1.1.2.cmml" xref="A6.T12.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.3.3.1.m1.1.1.3.cmml" xref="A6.T12.3.3.1.m1.1.1.3">370</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.3.3.1.m1.1c">\sim 370</annotation></semantics></math><span id="A6.T12.3.3.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.6.6.7" class="ltx_td ltx_align_center"><span id="A6.T12.6.6.7.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.4.4.2" class="ltx_td ltx_align_center">
<math id="A6.T12.4.4.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T12.4.4.2.m1.1a"><mn mathsize="90%" id="A6.T12.4.4.2.m1.1.1" xref="A6.T12.4.4.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T12.4.4.2.m1.1b"><cn type="integer" id="A6.T12.4.4.2.m1.1.1.cmml" xref="A6.T12.4.4.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.4.4.2.m1.1c">100</annotation></semantics></math><span id="A6.T12.4.4.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.6.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.8.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.6.6.8.1.1.1" class="ltx_text" style="font-size:90%;">.WET files</span></span>
</span>
</td>
<td id="A6.T12.6.6.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.9.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.6.6.9.1.1.1" class="ltx_text" style="font-size:90%;">Line-level w/ fastText </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.6.6.9.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Joulin et&nbsp;al.<span id="A6.T12.6.6.9.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib45" title="" class="ltx_ref">2016</a><span id="A6.T12.6.6.9.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.5.5.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.5.5.3.1.1.1" class="ltx_text" style="font-size:90%;">Line </span><math id="A6.T12.5.5.3.1.1.m1.1" class="ltx_Math" alttext="<100" display="inline"><semantics id="A6.T12.5.5.3.1.1.m1.1a"><mrow id="A6.T12.5.5.3.1.1.m1.1.1" xref="A6.T12.5.5.3.1.1.m1.1.1.cmml"><mi id="A6.T12.5.5.3.1.1.m1.1.1.2" xref="A6.T12.5.5.3.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.5.5.3.1.1.m1.1.1.1" xref="A6.T12.5.5.3.1.1.m1.1.1.1.cmml">&lt;</mo><mn mathsize="90%" id="A6.T12.5.5.3.1.1.m1.1.1.3" xref="A6.T12.5.5.3.1.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.5.5.3.1.1.m1.1b"><apply id="A6.T12.5.5.3.1.1.m1.1.1.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1"><lt id="A6.T12.5.5.3.1.1.m1.1.1.1.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="A6.T12.5.5.3.1.1.m1.1.1.2.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.5.5.3.1.1.m1.1.1.3.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.5.5.3.1.1.m1.1c">&lt;100</annotation></semantics></math><span id="A6.T12.5.5.3.1.1.2" class="ltx_text" style="font-size:90%;"> characters</span></span>
</span>
</td>
<td id="A6.T12.6.6.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.10.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.6.6.10.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="A6.T12.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.6.6.4.1.1.1" class="ltx_text" style="font-size:90%;">(optional) </span><span id="A6.T12.6.6.4.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="A6.T12.6.6.4.1.1.3" class="ltx_text" style="font-size:90%;">: per line (</span><math id="A6.T12.6.6.4.1.1.m1.1" class="ltx_Math" alttext="\sim 55\%" display="inline"><semantics id="A6.T12.6.6.4.1.1.m1.1a"><mrow id="A6.T12.6.6.4.1.1.m1.1.1" xref="A6.T12.6.6.4.1.1.m1.1.1.cmml"><mi id="A6.T12.6.6.4.1.1.m1.1.1.2" xref="A6.T12.6.6.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.6.6.4.1.1.m1.1.1.1" xref="A6.T12.6.6.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="A6.T12.6.6.4.1.1.m1.1.1.3" xref="A6.T12.6.6.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="A6.T12.6.6.4.1.1.m1.1.1.3.2" xref="A6.T12.6.6.4.1.1.m1.1.1.3.2.cmml">55</mn><mo mathsize="90%" id="A6.T12.6.6.4.1.1.m1.1.1.3.1" xref="A6.T12.6.6.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.6.6.4.1.1.m1.1b"><apply id="A6.T12.6.6.4.1.1.m1.1.1.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.6.6.4.1.1.m1.1.1.1.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.6.6.4.1.1.m1.1.1.2.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.2">absent</csymbol><apply id="A6.T12.6.6.4.1.1.m1.1.1.3.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.T12.6.6.4.1.1.m1.1.1.3.1.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A6.T12.6.6.4.1.1.m1.1.1.3.2.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.3.2">55</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.6.6.4.1.1.m1.1c">\sim 55\%</annotation></semantics></math><span id="A6.T12.6.6.4.1.1.4" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.8.8" class="ltx_tr">
<td id="A6.T12.8.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR 22.01</span><span id="A6.T12.8.8.3.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.8.8.3.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Abadji et&nbsp;al.<span id="A6.T12.8.8.3.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib2" title="" class="ltx_ref">2022</a><span id="A6.T12.8.8.3.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.8.8.4" class="ltx_td ltx_align_top"></td>
<td id="A6.T12.7.7.1" class="ltx_td ltx_align_center">
<math id="A6.T12.7.7.1.m1.1" class="ltx_Math" alttext="\sim 283" display="inline"><semantics id="A6.T12.7.7.1.m1.1a"><mrow id="A6.T12.7.7.1.m1.1.1" xref="A6.T12.7.7.1.m1.1.1.cmml"><mi id="A6.T12.7.7.1.m1.1.1.2" xref="A6.T12.7.7.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.7.7.1.m1.1.1.1" xref="A6.T12.7.7.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.7.7.1.m1.1.1.3" xref="A6.T12.7.7.1.m1.1.1.3.cmml">283</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.7.7.1.m1.1b"><apply id="A6.T12.7.7.1.m1.1.1.cmml" xref="A6.T12.7.7.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.7.7.1.m1.1.1.1.cmml" xref="A6.T12.7.7.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.7.7.1.m1.1.1.2.cmml" xref="A6.T12.7.7.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.7.7.1.m1.1.1.3.cmml" xref="A6.T12.7.7.1.m1.1.1.3">283</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.7.7.1.m1.1c">\sim 283</annotation></semantics></math><span id="A6.T12.7.7.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.8.8.5" class="ltx_td ltx_align_center"><span id="A6.T12.8.8.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.8.8.2" class="ltx_td ltx_align_center">
<math id="A6.T12.8.8.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T12.8.8.2.m1.1a"><mn mathsize="90%" id="A6.T12.8.8.2.m1.1.1" xref="A6.T12.8.8.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T12.8.8.2.m1.1b"><cn type="integer" id="A6.T12.8.8.2.m1.1.1.cmml" xref="A6.T12.8.8.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.8.8.2.m1.1c">100</annotation></semantics></math><span id="A6.T12.8.8.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.8.8.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.8.8.6.1.1.1" class="ltx_text" style="font-size:90%;">.WET files</span></span>
</span>
</td>
<td id="A6.T12.8.8.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.7.1.1.1" class="ltx_text" style="font-size:90%;">Document-level w/ fastText </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.8.8.7.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Joulin et&nbsp;al.<span id="A6.T12.8.8.7.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib45" title="" class="ltx_ref">2016</a><span id="A6.T12.8.8.7.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.8.8.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.8.1.1.1" class="ltx_text" style="font-size:90%;">Line-level, optional document-level</span></span>
</span>
</td>
<td id="A6.T12.8.8.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.8.8.9.1.1.1" class="ltx_text" style="font-size:90%;">Optional NSFW blocklist</span></span>
</span>
</td>
<td id="A6.T12.8.8.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.10.1.1.1" class="ltx_text" style="font-size:90%;">(optional) </span><span id="A6.T12.8.8.10.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="A6.T12.8.8.10.1.1.3" class="ltx_text" style="font-size:90%;">: per line</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.22.26" class="ltx_tr">
<td id="A6.T12.22.26.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="10"><span id="A6.T12.22.26.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Curated datasets</span></td>
</tr>
<tr id="A6.T12.12.12" class="ltx_tr">
<td id="A6.T12.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="2">
<math id="A6.T12.9.9.1.m1.1" class="ltx_Math" alttext="\blacksquare" display="inline"><semantics id="A6.T12.9.9.1.m1.1a"><mi mathcolor="#5F57DB" mathsize="90%" mathvariant="normal" id="A6.T12.9.9.1.m1.1.1" xref="A6.T12.9.9.1.m1.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="A6.T12.9.9.1.m1.1b"><ci id="A6.T12.9.9.1.m1.1.1.cmml" xref="A6.T12.9.9.1.m1.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.9.9.1.m1.1c">\blacksquare</annotation></semantics></math><span id="A6.T12.9.9.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#5F57DB;"> GPT-3</span><span id="A6.T12.9.9.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.9.9.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Brown et&nbsp;al.<span id="A6.T12.9.9.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib18" title="" class="ltx_ref">2020</a><span id="A6.T12.9.9.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td id="A6.T12.10.10.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.10.10.2.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="A6.T12.10.10.2.m1.1a"><mn mathsize="90%" id="A6.T12.10.10.2.m1.1.1" xref="A6.T12.10.10.2.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="A6.T12.10.10.2.m1.1b"><cn type="integer" id="A6.T12.10.10.2.m1.1.1.cmml" xref="A6.T12.10.10.2.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.10.10.2.m1.1c">300</annotation></semantics></math><span id="A6.T12.10.10.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.12.12.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T12.12.12.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="A6.T12.11.11.3" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.11.11.3.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="A6.T12.11.11.3.m1.1a"><mn mathsize="90%" id="A6.T12.11.11.3.m1.1.1" xref="A6.T12.11.11.3.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="A6.T12.11.11.3.m1.1b"><cn type="integer" id="A6.T12.11.11.3.m1.1.1.cmml" xref="A6.T12.11.11.3.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.11.11.3.m1.1c">60</annotation></semantics></math><span id="A6.T12.11.11.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.12.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.12.12.6.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.12.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.12.12.7.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.12.12.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.12.12.8.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.12.12.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.12.12.9.1.1.1" class="ltx_text" style="font-size:90%;">fastText trained on HQ-data</span></span>
</span>
</td>
<td id="A6.T12.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.12.12.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="A6.T12.12.12.4.1.1.2" class="ltx_text" style="font-size:90%;">: minhash with 10 hashes (</span><math id="A6.T12.12.12.4.1.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="A6.T12.12.12.4.1.1.m1.1a"><mrow id="A6.T12.12.12.4.1.1.m1.1.1" xref="A6.T12.12.12.4.1.1.m1.1.1.cmml"><mi id="A6.T12.12.12.4.1.1.m1.1.1.2" xref="A6.T12.12.12.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.12.12.4.1.1.m1.1.1.1" xref="A6.T12.12.12.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="A6.T12.12.12.4.1.1.m1.1.1.3" xref="A6.T12.12.12.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="A6.T12.12.12.4.1.1.m1.1.1.3.2" xref="A6.T12.12.12.4.1.1.m1.1.1.3.2.cmml">10</mn><mo mathsize="90%" id="A6.T12.12.12.4.1.1.m1.1.1.3.1" xref="A6.T12.12.12.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.12.12.4.1.1.m1.1b"><apply id="A6.T12.12.12.4.1.1.m1.1.1.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.12.12.4.1.1.m1.1.1.1.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.12.12.4.1.1.m1.1.1.2.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.2">absent</csymbol><apply id="A6.T12.12.12.4.1.1.m1.1.1.3.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.T12.12.12.4.1.1.m1.1.1.3.1.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A6.T12.12.12.4.1.1.m1.1.1.3.2.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.12.12.4.1.1.m1.1c">\sim 10\%</annotation></semantics></math><span id="A6.T12.12.12.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.16.16" class="ltx_tr">
<td id="A6.T12.13.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.13.13.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="A6.T12.13.13.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="A6.T12.13.13.1.1.1.m1.1a"><mi mathcolor="#7DD86E" mathsize="90%" mathvariant="normal" id="A6.T12.13.13.1.1.1.m1.1.1" xref="A6.T12.13.13.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="A6.T12.13.13.1.1.1.m1.1b"><ci id="A6.T12.13.13.1.1.1.m1.1.1.cmml" xref="A6.T12.13.13.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.13.13.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="A6.T12.13.13.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#7DD86E;"> The Pile</span><span id="A6.T12.13.13.1.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.13.13.1.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Gao et&nbsp;al.<span id="A6.T12.13.13.1.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib33" title="" class="ltx_ref">2020</a><span id="A6.T12.13.13.1.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.16.16.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.5.1.1.1" class="ltx_text" style="font-size:90%;">GPT-J </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Wang &amp; Komatsuzaki<span id="A6.T12.16.16.5.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib76" title="" class="ltx_ref">2021</a><span id="A6.T12.16.16.5.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.16.16.5.1.1.5" class="ltx_text" style="font-size:90%;">, GPT-NeoX-20B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.6.1" class="ltx_text" style="font-size:90%;">(</span>Black et&nbsp;al.<span id="A6.T12.16.16.5.1.1.7.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib16" title="" class="ltx_ref">2022</a><span id="A6.T12.16.16.5.1.1.8.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.16.16.5.1.1.9" class="ltx_text" style="font-size:90%;">, Pythia </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.10.1" class="ltx_text" style="font-size:90%;">(</span>Biderman et&nbsp;al.<span id="A6.T12.16.16.5.1.1.11.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib13" title="" class="ltx_ref">2023</a><span id="A6.T12.16.16.5.1.1.12.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.16.16.5.1.1.13" class="ltx_text" style="font-size:90%;">, Cerebras-GPT </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.14.1" class="ltx_text" style="font-size:90%;">(</span>Dey et&nbsp;al.<span id="A6.T12.16.16.5.1.1.15.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib30" title="" class="ltx_ref">2023</a><span id="A6.T12.16.16.5.1.1.16.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.14.14.2" class="ltx_td ltx_align_center">
<math id="A6.T12.14.14.2.m1.1" class="ltx_Math" alttext="\sim 340" display="inline"><semantics id="A6.T12.14.14.2.m1.1a"><mrow id="A6.T12.14.14.2.m1.1.1" xref="A6.T12.14.14.2.m1.1.1.cmml"><mi id="A6.T12.14.14.2.m1.1.1.2" xref="A6.T12.14.14.2.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.14.14.2.m1.1.1.1" xref="A6.T12.14.14.2.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.14.14.2.m1.1.1.3" xref="A6.T12.14.14.2.m1.1.1.3.cmml">340</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.14.14.2.m1.1b"><apply id="A6.T12.14.14.2.m1.1.1.cmml" xref="A6.T12.14.14.2.m1.1.1"><csymbol cd="latexml" id="A6.T12.14.14.2.m1.1.1.1.cmml" xref="A6.T12.14.14.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.14.14.2.m1.1.1.2.cmml" xref="A6.T12.14.14.2.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.14.14.2.m1.1.1.3.cmml" xref="A6.T12.14.14.2.m1.1.1.3">340</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.14.14.2.m1.1c">\sim 340</annotation></semantics></math><span id="A6.T12.14.14.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.16.16.6" class="ltx_td ltx_align_center"><span id="A6.T12.16.16.6.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.15.15.3" class="ltx_td ltx_align_center">
<math id="A6.T12.15.15.3.m1.1" class="ltx_Math" alttext="18" display="inline"><semantics id="A6.T12.15.15.3.m1.1a"><mn mathsize="90%" id="A6.T12.15.15.3.m1.1.1" xref="A6.T12.15.15.3.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="A6.T12.15.15.3.m1.1b"><cn type="integer" id="A6.T12.15.15.3.m1.1.1.cmml" xref="A6.T12.15.15.3.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.15.15.3.m1.1c">18</annotation></semantics></math><span id="A6.T12.15.15.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.16.16.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.16.16.7.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jusText</span><span id="A6.T12.16.16.7.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.7.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Pomikálek<span id="A6.T12.16.16.7.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib59" title="" class="ltx_ref">2011</a><span id="A6.T12.16.16.7.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.16.16.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.8.1.1.1" class="ltx_text" style="font-size:90%;">Document-level w/ </span><span id="A6.T12.16.16.8.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">pycld2</span><span id="A6.T12.16.16.8.1.1.3" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.8.1.1.4.1" class="ltx_text" style="font-size:90%;">(</span>Sites<span id="A6.T12.16.16.8.1.1.5.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib69" title="" class="ltx_ref">2013</a><span id="A6.T12.16.16.8.1.1.6.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.16.16.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.9.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.9.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="A6.T12.16.16.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.10.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.16.16.10.1.1.1" class="ltx_text" style="font-size:90%;">fastText on curated crawl</span></span>
</span>
</td>
<td id="A6.T12.16.16.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="A6.T12.16.16.4.1.1.2" class="ltx_text" style="font-size:90%;">: minhash with 10 hashes, sim. treshold 0.5 (</span><math id="A6.T12.16.16.4.1.1.m1.1" class="ltx_Math" alttext="\sim 26\%" display="inline"><semantics id="A6.T12.16.16.4.1.1.m1.1a"><mrow id="A6.T12.16.16.4.1.1.m1.1.1" xref="A6.T12.16.16.4.1.1.m1.1.1.cmml"><mi id="A6.T12.16.16.4.1.1.m1.1.1.2" xref="A6.T12.16.16.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.16.16.4.1.1.m1.1.1.1" xref="A6.T12.16.16.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="A6.T12.16.16.4.1.1.m1.1.1.3" xref="A6.T12.16.16.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="A6.T12.16.16.4.1.1.m1.1.1.3.2" xref="A6.T12.16.16.4.1.1.m1.1.1.3.2.cmml">26</mn><mo mathsize="90%" id="A6.T12.16.16.4.1.1.m1.1.1.3.1" xref="A6.T12.16.16.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.16.16.4.1.1.m1.1b"><apply id="A6.T12.16.16.4.1.1.m1.1.1.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.16.16.4.1.1.m1.1.1.1.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.16.16.4.1.1.m1.1.1.2.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.2">absent</csymbol><apply id="A6.T12.16.16.4.1.1.m1.1.1.3.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.T12.16.16.4.1.1.m1.1.1.3.1.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A6.T12.16.16.4.1.1.m1.1.1.3.2.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.3.2">26</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.16.16.4.1.1.m1.1c">\sim 26\%</annotation></semantics></math><span id="A6.T12.16.16.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.18.18" class="ltx_tr">
<td id="A6.T12.18.18.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MassiveWeb</span><span id="A6.T12.18.18.3.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.18.18.3.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Rae et&nbsp;al.<span id="A6.T12.18.18.3.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib63" title="" class="ltx_ref">2021</a><span id="A6.T12.18.18.3.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.18.18.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.4.1.1.1" class="ltx_text" style="font-size:90%;">Gopher </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.18.18.4.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Rae et&nbsp;al.<span id="A6.T12.18.18.4.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib63" title="" class="ltx_ref">2021</a><span id="A6.T12.18.18.4.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.18.18.4.1.1.5" class="ltx_text" style="font-size:90%;">, Chinchilla </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.18.18.4.1.1.6.1" class="ltx_text" style="font-size:90%;">(</span>Hoffmann et&nbsp;al.<span id="A6.T12.18.18.4.1.1.7.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib41" title="" class="ltx_ref">2022</a><span id="A6.T12.18.18.4.1.1.8.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.17.17.1" class="ltx_td ltx_align_center">
<math id="A6.T12.17.17.1.m1.2" class="ltx_Math" alttext="1,400" display="inline"><semantics id="A6.T12.17.17.1.m1.2a"><mrow id="A6.T12.17.17.1.m1.2.3.2" xref="A6.T12.17.17.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A6.T12.17.17.1.m1.1.1" xref="A6.T12.17.17.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A6.T12.17.17.1.m1.2.3.2.1" xref="A6.T12.17.17.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A6.T12.17.17.1.m1.2.2" xref="A6.T12.17.17.1.m1.2.2.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.17.17.1.m1.2b"><list id="A6.T12.17.17.1.m1.2.3.1.cmml" xref="A6.T12.17.17.1.m1.2.3.2"><cn type="integer" id="A6.T12.17.17.1.m1.1.1.cmml" xref="A6.T12.17.17.1.m1.1.1">1</cn><cn type="integer" id="A6.T12.17.17.1.m1.2.2.cmml" xref="A6.T12.17.17.1.m1.2.2">400</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.17.17.1.m1.2c">1,400</annotation></semantics></math><span id="A6.T12.17.17.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.18.18.5" class="ltx_td ltx_align_center"><span id="A6.T12.18.18.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="A6.T12.18.18.2" class="ltx_td ltx_align_center">
<math id="A6.T12.18.18.2.m1.1" class="ltx_Math" alttext="48" display="inline"><semantics id="A6.T12.18.18.2.m1.1a"><mn mathsize="90%" id="A6.T12.18.18.2.m1.1.1" xref="A6.T12.18.18.2.m1.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="A6.T12.18.18.2.m1.1b"><cn type="integer" id="A6.T12.18.18.2.m1.1.1.cmml" xref="A6.T12.18.18.2.m1.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.18.18.2.m1.1c">48</annotation></semantics></math><span id="A6.T12.18.18.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.18.18.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.18.18.6.1.1.1" class="ltx_text" style="font-size:90%;">Custom</span></span>
</span>
</td>
<td id="A6.T12.18.18.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.7.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.18.18.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.8.1.1.1" class="ltx_text" style="font-size:90%;">Document-level</span></span>
</span>
</td>
<td id="A6.T12.18.18.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.18.18.9.1.1.1" class="ltx_text" style="font-size:90%;">SafeSearch</span></span>
</span>
</td>
<td id="A6.T12.18.18.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact &amp; fuzzy</span><span id="A6.T12.18.18.10.1.1.2" class="ltx_text" style="font-size:90%;">: exact documents, minhash w/ sim. treshold 0.8</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.21.21" class="ltx_tr">
<td id="A6.T12.19.19.1" class="ltx_td ltx_align_justify ltx_align_top" colspan="2">
<math id="A6.T12.19.19.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="A6.T12.19.19.1.m1.1a"><mi mathcolor="#DB5F56" mathsize="90%" mathvariant="normal" id="A6.T12.19.19.1.m1.1.1" xref="A6.T12.19.19.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="A6.T12.19.19.1.m1.1b"><ci id="A6.T12.19.19.1.m1.1.1.cmml" xref="A6.T12.19.19.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.19.19.1.m1.1c">\bigstar</annotation></semantics></math><span id="A6.T12.19.19.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#DB5F56;"> PaLM</span><span id="A6.T12.19.19.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.19.19.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Chowdhery et&nbsp;al.<span id="A6.T12.19.19.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib23" title="" class="ltx_ref">2022</a><span id="A6.T12.19.19.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td id="A6.T12.20.20.2" class="ltx_td ltx_align_center">
<math id="A6.T12.20.20.2.m1.1" class="ltx_Math" alttext="780" display="inline"><semantics id="A6.T12.20.20.2.m1.1a"><mn mathsize="90%" id="A6.T12.20.20.2.m1.1.1" xref="A6.T12.20.20.2.m1.1.1.cmml">780</mn><annotation-xml encoding="MathML-Content" id="A6.T12.20.20.2.m1.1b"><cn type="integer" id="A6.T12.20.20.2.m1.1.1.cmml" xref="A6.T12.20.20.2.m1.1.1">780</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.20.20.2.m1.1c">780</annotation></semantics></math><span id="A6.T12.20.20.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.21.21.4" class="ltx_td ltx_align_center"><span id="A6.T12.21.21.4.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="A6.T12.21.21.3" class="ltx_td ltx_align_center">
<math id="A6.T12.21.21.3.m1.1" class="ltx_Math" alttext="27" display="inline"><semantics id="A6.T12.21.21.3.m1.1a"><mn mathsize="90%" id="A6.T12.21.21.3.m1.1.1" xref="A6.T12.21.21.3.m1.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="A6.T12.21.21.3.m1.1b"><cn type="integer" id="A6.T12.21.21.3.m1.1.1.cmml" xref="A6.T12.21.21.3.m1.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.21.21.3.m1.1c">27</annotation></semantics></math><span id="A6.T12.21.21.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.21.21.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.5.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.21.21.5.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.21.21.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.6.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.21.21.6.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.21.21.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.21.21.7.1.1.1" class="ltx_text" style="font-size:90%;">Document-level</span></span>
</span>
</td>
<td id="A6.T12.21.21.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.8.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.21.21.8.1.1.1" class="ltx_text" style="font-size:90%;">ML-based filter on HQ data</span></span>
</span>
</td>
<td id="A6.T12.21.21.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.9.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.21.21.9.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.22.27" class="ltx_tr">
<td id="A6.T12.22.27.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="10"><span id="A6.T12.22.27.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Ours</span></td>
</tr>
<tr id="A6.T12.22.22" class="ltx_tr">
<td id="A6.T12.22.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.2.1.1.1" class="ltx_text" style="font-size:90%;color:#DB57B2;">●<span id="A6.T12.22.22.2.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">RefinedWeb</span></span></span>
</span>
</td>
<td id="A6.T12.22.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.3.1.1.1" class="ltx_text" style="font-size:90%;">Falcon-RW</span></span>
</span>
</td>
<td id="A6.T12.22.22.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A6.T12.22.22.4.1" class="ltx_text" style="font-size:90%;">5,000GT</span></td>
<td id="A6.T12.22.22.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A6.T12.22.22.5.1" class="ltx_text" style="font-size:90%;">600GT Public</span></td>
<td id="A6.T12.22.22.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math id="A6.T12.22.22.1.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="A6.T12.22.22.1.m1.1a"><mrow id="A6.T12.22.22.1.m1.1.1" xref="A6.T12.22.22.1.m1.1.1.cmml"><mn mathsize="90%" id="A6.T12.22.22.1.m1.1.1.2" xref="A6.T12.22.22.1.m1.1.1.2.cmml">100</mn><mo mathsize="90%" id="A6.T12.22.22.1.m1.1.1.1" xref="A6.T12.22.22.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.22.22.1.m1.1b"><apply id="A6.T12.22.22.1.m1.1.1.cmml" xref="A6.T12.22.22.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.22.22.1.m1.1.1.1.cmml" xref="A6.T12.22.22.1.m1.1.1.1">percent</csymbol><cn type="integer" id="A6.T12.22.22.1.m1.1.1.2.cmml" xref="A6.T12.22.22.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.22.22.1.m1.1c">100\%</annotation></semantics></math></td>
<td id="A6.T12.22.22.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.22.6.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">trafilatura</span><span id="A6.T12.22.22.6.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.22.22.6.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Barbaresi<span id="A6.T12.22.22.6.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib11" title="" class="ltx_ref">2021</a><span id="A6.T12.22.22.6.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.22.22.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.7.1.1.1" class="ltx_text" style="font-size:90%;">From CCNet </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.22.22.7.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Wenzek et&nbsp;al.<span id="A6.T12.22.22.7.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib81" title="" class="ltx_ref">2020</a><span id="A6.T12.22.22.7.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.22.22.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.8.1.1.1" class="ltx_text" style="font-size:90%;">Document and line-level</span></span>
</span>
</td>
<td id="A6.T12.22.22.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.22.9.1.1.1" class="ltx_text" style="font-size:90%;">URL blocklist</span></span>
</span>
</td>
<td id="A6.T12.22.22.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact &amp; fuzzy</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Details of the Macrodata Refinement pipeline</h2>

<section id="A7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>URL filtering</h3>

<div id="A7.SS1.p1" class="ltx_para">
<p id="A7.SS1.p1.1" class="ltx_p">As discussed in <a href="#S3.SS1.SSS0.Px2" title="URL filtering. ‣ 3.1 Document preparation: reading data, filtering URLs, extracting text, and language identification ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.1</span></a>, we base our filtering of adult documents only on the URL itself, and not on the content of the documents. This design choice was motivated by: (1) challenges in avoiding overfiltering content from minorities when using ML-based classifiers on the content of documents <cite class="ltx_cite ltx_citemacro_citep">(Welbl et&nbsp;al., <a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite>; (2) NSFW words block-list applied on content&nbsp;(such as the one used in C4) also resulting in overfiltering of legal and medical content <cite class="ltx_cite ltx_citemacro_citep">(Dodge et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="A7.SS1.p2" class="ltx_para">
<p id="A7.SS1.p2.1" class="ltx_p">Our URL filtering focuses on finding domains that are related to adult content, that may be harmful to users, or that are very likely to contain mostly unstructured text/spam (e.g., file hosting websites).
First, we aggregated a list of 4.6M domains, detailed in <a href="#A7.SS1.SSS1" title="G.1.1 URL Blocklist ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1.1</span></a>, that we explicitly ban; then, we built a simple URL scoring system, based on matching subwords in the URL against a list of words we curated (see <a href="#A7.SS1.SSS2" title="G.1.2 URL Scoring with a Word-List ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1.2</span></a>). We curated this list of words based on manual inspection, cross-referencing results with pages surfaced by ToxicBERT as being outliers in toxicity <cite class="ltx_cite ltx_citemacro_citep">(Hanu &amp; Unitary team, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<section id="A7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.1.1 </span>URL Blocklist</h4>

<section id="A7.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Origin of the list.</h5>

<div id="A7.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="A7.SS1.SSS1.Px1.p1.1" class="ltx_p">We use an aggregated list<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_tag ltx_tag_note">†</span><a target="_blank" href="https://dsi.ut-capitole.fr/blacklists/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dsi.ut-capitole.fr/blacklists/</a></span></span></span> of about 4.6M URLs that we explicitly ban. This list is broken in categories (e.g. pornography, gambling); we outline the categories we selected in <a href="#A7.T13" title="In Curation. ‣ G.1.1 URL Blocklist ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">13</span></a>. The list is regularly updated, with an original intended usage as a blocklist for universities.</p>
</div>
</section>
<section id="A7.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Curation.</h5>

<div id="A7.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="A7.SS1.SSS1.Px2.p1.2" class="ltx_p">We noticed the list blocked a number of domains inappropriately; while these domains were few (<math id="A7.SS1.SSS1.Px2.p1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="A7.SS1.SSS1.Px2.p1.1.m1.1a"><mo id="A7.SS1.SSS1.Px2.p1.1.m1.1.1" xref="A7.SS1.SSS1.Px2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS1.Px2.p1.1.m1.1b"><lt id="A7.SS1.SSS1.Px2.p1.1.m1.1.1.cmml" xref="A7.SS1.SSS1.Px2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS1.Px2.p1.1.m1.1c">&lt;</annotation></semantics></math>100), they accounted for a significant portion of the data filtered by the list, as these were rather prolific domains, with thousands of pages of content. To identify these false positive domains, we applied the blocklist to a subset of 832M pages. 6.04M (<math id="A7.SS1.SSS1.Px2.p1.2.m2.1" class="ltx_Math" alttext="0.73\%" display="inline"><semantics id="A7.SS1.SSS1.Px2.p1.2.m2.1a"><mrow id="A7.SS1.SSS1.Px2.p1.2.m2.1.1" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.cmml"><mn id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2.cmml">0.73</mn><mo id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS1.Px2.p1.2.m2.1b"><apply id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.cmml" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1"><csymbol cd="latexml" id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1.cmml" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2.cmml" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2">0.73</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS1.Px2.p1.2.m2.1c">0.73\%</annotation></semantics></math>) pages matched with the blocklist, and the number of occurrences per URL ranged from 1 to 79k. We manually inspected all URLs matched more than 4k times, which represented an appreciable portion of the dataset. We found a number of benign domains, such as pop culture news websites, or blogging platforms, which we removed from the list.</p>
</div>
<figure id="A7.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span><span id="A7.T13.2.1" class="ltx_text ltx_font_bold">We select categories likely to contain adult or malicious content, as well as spam or unstructured text.</span></figcaption>
<table id="A7.T13.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A7.T13.3.1" class="ltx_tr">
<td id="A7.T13.3.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T13.3.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="A7.T13.3.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T13.3.1.2.1" class="ltx_text ltx_font_bold">Description</span></td>
<td id="A7.T13.3.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T13.3.1.3.1" class="ltx_text ltx_font_bold">Number of links</span></td>
</tr>
<tr id="A7.T13.3.2" class="ltx_tr">
<td id="A7.T13.3.2.1" class="ltx_td ltx_align_left ltx_border_t">adult</td>
<td id="A7.T13.3.2.2" class="ltx_td ltx_align_left ltx_border_t">adult websites: from eroticism to hard pornography</td>
<td id="A7.T13.3.2.3" class="ltx_td ltx_align_left ltx_border_t">4516478</td>
</tr>
<tr id="A7.T13.3.3" class="ltx_tr">
<td id="A7.T13.3.3.1" class="ltx_td ltx_align_left">phishing</td>
<td id="A7.T13.3.3.2" class="ltx_td ltx_align_left">phishing websites, malwares, etc.</td>
<td id="A7.T13.3.3.3" class="ltx_td ltx_align_left">42445</td>
</tr>
<tr id="A7.T13.3.4" class="ltx_tr">
<td id="A7.T13.3.4.1" class="ltx_td ltx_align_left">dating</td>
<td id="A7.T13.3.4.2" class="ltx_td ltx_align_left">dating websites</td>
<td id="A7.T13.3.4.3" class="ltx_td ltx_align_left">3829</td>
</tr>
<tr id="A7.T13.3.5" class="ltx_tr">
<td id="A7.T13.3.5.1" class="ltx_td ltx_align_left">gambling</td>
<td id="A7.T13.3.5.2" class="ltx_td ltx_align_left">online casino</td>
<td id="A7.T13.3.5.3" class="ltx_td ltx_align_left">1365</td>
</tr>
<tr id="A7.T13.3.6" class="ltx_tr">
<td id="A7.T13.3.6.1" class="ltx_td ltx_align_left">filehosting</td>
<td id="A7.T13.3.6.2" class="ltx_td ltx_align_left">websites hosting files, videos, pictures, music</td>
<td id="A7.T13.3.6.3" class="ltx_td ltx_align_left">909</td>
</tr>
<tr id="A7.T13.3.7" class="ltx_tr">
<td id="A7.T13.3.7.1" class="ltx_td ltx_align_left">ddos</td>
<td id="A7.T13.3.7.2" class="ltx_td ltx_align_left">websites related to ddos attacks</td>
<td id="A7.T13.3.7.3" class="ltx_td ltx_align_left">421</td>
</tr>
<tr id="A7.T13.3.8" class="ltx_tr">
<td id="A7.T13.3.8.1" class="ltx_td ltx_align_left">agressif</td>
<td id="A7.T13.3.8.2" class="ltx_td ltx_align_left">hate, racism, etc</td>
<td id="A7.T13.3.8.3" class="ltx_td ltx_align_left">390</td>
</tr>
<tr id="A7.T13.3.9" class="ltx_tr">
<td id="A7.T13.3.9.1" class="ltx_td ltx_align_left">chat</td>
<td id="A7.T13.3.9.2" class="ltx_td ltx_align_left">online chat websites</td>
<td id="A7.T13.3.9.3" class="ltx_td ltx_align_left">244</td>
</tr>
<tr id="A7.T13.3.10" class="ltx_tr">
<td id="A7.T13.3.10.1" class="ltx_td ltx_align_left">mixed adult</td>
<td id="A7.T13.3.10.2" class="ltx_td ltx_align_left">websites with some adult content</td>
<td id="A7.T13.3.10.3" class="ltx_td ltx_align_left">153</td>
</tr>
<tr id="A7.T13.3.11" class="ltx_tr">
<td id="A7.T13.3.11.1" class="ltx_td ltx_align_left ltx_border_bb">arjel</td>
<td id="A7.T13.3.11.2" class="ltx_td ltx_align_left ltx_border_bb">French regulated gambling websites</td>
<td id="A7.T13.3.11.3" class="ltx_td ltx_align_left ltx_border_bb">69</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.1.2 </span>URL Scoring with a Word-List</h4>

<div id="A7.SS1.SSS2.p1" class="ltx_para">
<p id="A7.SS1.SSS2.p1.1" class="ltx_p">To score URLs, we used three matching patterns based on a soft, hard, and strict violation word-list:</p>
<ul id="A7.I1" class="ltx_itemize">
<li id="A7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i1.p1" class="ltx_para">
<p id="A7.I1.i1.p1.1" class="ltx_p"><span id="A7.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Strict <span id="A7.I1.i1.p1.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">sub</span>word matching</span>: http://foo<span id="A7.I1.i1.p1.1.2" class="ltx_text" style="color:#FF0000;">bann</span>.<span id="A7.I1.i1.p1.1.3" class="ltx_text" style="color:#FF0000;">edsub</span>-<span id="A7.I1.i1.p1.1.4" class="ltx_text" style="color:#FF0000;">wo</span>.<span id="A7.I1.i1.p1.1.5" class="ltx_text" style="color:#FF0000;">rd</span>bar.com/any/bar, matching words such as <span id="A7.I1.i1.p1.1.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">xvideos</span>, <span id="A7.I1.i1.p1.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">groupsex</span>;</p>
</div>
</li>
<li id="A7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i2.p1" class="ltx_para">
<p id="A7.I1.i2.p1.1" class="ltx_p"><span id="A7.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Hard <span id="A7.I1.i2.p1.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">whole</span> word matching</span>: http://www.foo.<span id="A7.I1.i2.p1.1.2" class="ltx_text" style="color:#FF8000;">bannedword</span>-bar.com, with words such as <span id="A7.I1.i2.p1.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">porn</span>, <span id="A7.I1.i2.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">xxx</span>, <span id="A7.I1.i2.p1.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">orgy</span>;</p>
</div>
</li>
<li id="A7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i3.p1" class="ltx_para">
<p id="A7.I1.i3.p1.1" class="ltx_p"><span id="A7.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Soft word<span id="A7.I1.i3.p1.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">s</span> matching</span>: http://www.foo.<span id="A7.I1.i3.p1.1.2" class="ltx_text" style="color:#0000FF;">soft1</span>-bar-<span id="A7.I1.i3.p1.1.3" class="ltx_text" style="color:#0000FF;">soft2</span>.com, with ”softer” words such as <span id="A7.I1.i3.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sex</span>, <span id="A7.I1.i3.p1.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">webcam</span>, <span id="A7.I1.i3.p1.1.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">escort</span>.</p>
</div>
</li>
</ul>
</div>
<div id="A7.SS1.SSS2.p2" class="ltx_para">
<p id="A7.SS1.SSS2.p2.1" class="ltx_p">Each list is associated with a different level of severity: for the strictest one (strict subword matching), we ban any URL matching a banned word in its substrings (as fraudulent websites may attempt to escape similar recognition schemes by breaking-up adult keywords); for the hard whole word matching, we ban URLs with a whole word matching in the list; finally, a minimum of two matches are required with the soft word matching.</p>
</div>
<div id="A7.SS1.SSS2.p3" class="ltx_para">
<p id="A7.SS1.SSS2.p3.1" class="ltx_p">We curated the lists based on manual inspection of the data, informed by top hits reported by ToxicBERT. For the strict subword matching, we included words that were unequivocally related to adult content (e.g., <span id="A7.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">groupsex</span>). We avoided partial unclear matches (e.g., <span id="A7.SS1.SSS2.p3.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ass</span>), that may be part of neutral words (e.g., <span id="A7.SS1.SSS2.p3.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">massachusetts</span>). In the soft word list, we included words that do not constitute a sufficient reason to discard the document on their own, but which are suspicious when multiple words from the list result in a match. This helped with keeping medical or legal content unaffected (e.g., a single match of <span id="A7.SS1.SSS2.p3.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">dick</span>).</p>
</div>
</section>
<section id="A7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.1.3 </span>Excluded High Quality Sources</h4>

<div id="A7.SS1.SSS3.p1" class="ltx_para">
<p id="A7.SS1.SSS3.p1.1" class="ltx_p">Since our paper focuses on the study of RefinedWeb alone, we chose to exclude common online sources of curated data from it. This serves two objectives: (1) it strengthens our results, by ensuring that RefinedWeb doesn’t end-up actually being made mostly of known high-quality sources (e.g., Wikipedia represents a significant portion of C4); (2) future works may be interested in combining RefinedWeb with existing curated copora, which would require further deduplication if they are included in RefinedWeb. Accordingly, we remove common sources used in The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> from RefinedWeb. The full list of curated data sources domains that we blocked is in Table <a href="#A7.T14" title="Table 14 ‣ G.1.3 Excluded High Quality Sources ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure id="A7.T14" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 14: </span><span id="A7.T14.2.1" class="ltx_text ltx_font_bold">RefinedWeb is stripped from common so-called high-quality sources to simplify combining it with existing curated corpora</span>. This blocklist is applied at the URL filtering stage, along with the adult content blocklist.</figcaption>
<table id="A7.T14.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A7.T14.3.1" class="ltx_tr">
<td id="A7.T14.3.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T14.3.1.1.1" class="ltx_text ltx_font_bold">Curated data source</span></td>
<td id="A7.T14.3.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T14.3.1.2.1" class="ltx_text ltx_font_bold">Domain name blocked</span></td>
</tr>
<tr id="A7.T14.3.2" class="ltx_tr">
<td id="A7.T14.3.2.1" class="ltx_td ltx_align_left ltx_border_t">arxiv</td>
<td id="A7.T14.3.2.2" class="ltx_td ltx_align_left ltx_border_t">arxiv.org</td>
</tr>
<tr id="A7.T14.3.3" class="ltx_tr">
<td id="A7.T14.3.3.1" class="ltx_td ltx_align_left">AskUbuntu</td>
<td id="A7.T14.3.3.2" class="ltx_td ltx_align_left">askubuntu.com</td>
</tr>
<tr id="A7.T14.3.4" class="ltx_tr">
<td id="A7.T14.3.4.1" class="ltx_td ltx_align_left">StackOverflow</td>
<td id="A7.T14.3.4.2" class="ltx_td ltx_align_left">stackoverflow.com</td>
</tr>
<tr id="A7.T14.3.5" class="ltx_tr">
<td id="A7.T14.3.5.1" class="ltx_td"></td>
<td id="A7.T14.3.5.2" class="ltx_td ltx_align_left">stackapps.com</td>
</tr>
<tr id="A7.T14.3.6" class="ltx_tr">
<td id="A7.T14.3.6.1" class="ltx_td"></td>
<td id="A7.T14.3.6.2" class="ltx_td ltx_align_left">stackexchange.com</td>
</tr>
<tr id="A7.T14.3.7" class="ltx_tr">
<td id="A7.T14.3.7.1" class="ltx_td"></td>
<td id="A7.T14.3.7.2" class="ltx_td ltx_align_left">mathoverflow.net</td>
</tr>
<tr id="A7.T14.3.8" class="ltx_tr">
<td id="A7.T14.3.8.1" class="ltx_td ltx_align_left">NIH Abstracts</td>
<td id="A7.T14.3.8.2" class="ltx_td ltx_align_left">exporter.nih.gov</td>
</tr>
<tr id="A7.T14.3.9" class="ltx_tr">
<td id="A7.T14.3.9.1" class="ltx_td"></td>
<td id="A7.T14.3.9.2" class="ltx_td ltx_align_left">ncbi.nlm.nih.gov</td>
</tr>
<tr id="A7.T14.3.10" class="ltx_tr">
<td id="A7.T14.3.10.1" class="ltx_td ltx_align_left">Github</td>
<td id="A7.T14.3.10.2" class="ltx_td ltx_align_left">github.com</td>
</tr>
<tr id="A7.T14.3.11" class="ltx_tr">
<td id="A7.T14.3.11.1" class="ltx_td ltx_align_left">Ubuntu IRC</td>
<td id="A7.T14.3.11.2" class="ltx_td ltx_align_left">irclogs.ubuntu.com</td>
</tr>
<tr id="A7.T14.3.12" class="ltx_tr">
<td id="A7.T14.3.12.1" class="ltx_td ltx_align_left">HackerNews</td>
<td id="A7.T14.3.12.2" class="ltx_td ltx_align_left">news.ycombinator.com</td>
</tr>
<tr id="A7.T14.3.13" class="ltx_tr">
<td id="A7.T14.3.13.1" class="ltx_td ltx_align_left">FreeLaw</td>
<td id="A7.T14.3.13.2" class="ltx_td ltx_align_left">courtlistener.com</td>
</tr>
<tr id="A7.T14.3.14" class="ltx_tr">
<td id="A7.T14.3.14.1" class="ltx_td ltx_align_left">Reddit</td>
<td id="A7.T14.3.14.2" class="ltx_td ltx_align_left">reddit.com</td>
</tr>
<tr id="A7.T14.3.15" class="ltx_tr">
<td id="A7.T14.3.15.1" class="ltx_td ltx_align_left">Europarl</td>
<td id="A7.T14.3.15.2" class="ltx_td ltx_align_left">statmt.org</td>
</tr>
<tr id="A7.T14.3.16" class="ltx_tr">
<td id="A7.T14.3.16.1" class="ltx_td ltx_align_left">United States Patents</td>
<td id="A7.T14.3.16.2" class="ltx_td ltx_align_left">uspto.gov</td>
</tr>
<tr id="A7.T14.3.17" class="ltx_tr">
<td id="A7.T14.3.17.1" class="ltx_td ltx_align_left ltx_border_bb">Wikipedia</td>
<td id="A7.T14.3.17.2" class="ltx_td ltx_align_left ltx_border_bb">wikipedia.org</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>Line-wise filtering</h3>

<div id="A7.SS2.p1" class="ltx_para">
<p id="A7.SS2.p1.1" class="ltx_p">Despite the improvements brought forth by running text extraction with Trafilatura, we found that a number of irrelevant lines still seeped through. These lines are usually related to navigation menus, call to actions, or social media counters. Following manual inspection of the data, we devised a line-wise filtering strategy. We analyse documents line-by-line, and discard or edit the lines based on the following rules:</p>
<ul id="A7.I2" class="ltx_itemize">
<li id="A7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i1.p1" class="ltx_para">
<p id="A7.I2.i1.p1.1" class="ltx_p">If it is mainly composed of uppercase characters (discard);</p>
</div>
</li>
<li id="A7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i2.p1" class="ltx_para">
<p id="A7.I2.i2.p1.1" class="ltx_p">If it is only composed of numerical characters (discard);</p>
</div>
</li>
<li id="A7.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i3.p1" class="ltx_para">
<p id="A7.I2.i3.p1.1" class="ltx_p">If it is a counter (e.g. <span id="A7.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">3 likes</span>) (discard);</p>
</div>
</li>
<li id="A7.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i4.p1" class="ltx_para">
<p id="A7.I2.i4.p1.1" class="ltx_p">If it only contains one word (discard);</p>
</div>
</li>
<li id="A7.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i5.p1" class="ltx_para">
<p id="A7.I2.i5.p1.1" class="ltx_p">If it is short (<math id="A7.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="\leq 10" display="inline"><semantics id="A7.I2.i5.p1.1.m1.1a"><mrow id="A7.I2.i5.p1.1.m1.1.1" xref="A7.I2.i5.p1.1.m1.1.1.cmml"><mi id="A7.I2.i5.p1.1.m1.1.1.2" xref="A7.I2.i5.p1.1.m1.1.1.2.cmml"></mi><mo id="A7.I2.i5.p1.1.m1.1.1.1" xref="A7.I2.i5.p1.1.m1.1.1.1.cmml">≤</mo><mn id="A7.I2.i5.p1.1.m1.1.1.3" xref="A7.I2.i5.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.I2.i5.p1.1.m1.1b"><apply id="A7.I2.i5.p1.1.m1.1.1.cmml" xref="A7.I2.i5.p1.1.m1.1.1"><leq id="A7.I2.i5.p1.1.m1.1.1.1.cmml" xref="A7.I2.i5.p1.1.m1.1.1.1"></leq><csymbol cd="latexml" id="A7.I2.i5.p1.1.m1.1.1.2.cmml" xref="A7.I2.i5.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A7.I2.i5.p1.1.m1.1.1.3.cmml" xref="A7.I2.i5.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.I2.i5.p1.1.m1.1c">\leq 10</annotation></semantics></math> words) and matches a pattern (edit):</p>
<ul id="A7.I2.i5.I1" class="ltx_itemize">
<li id="A7.I2.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i1.p1" class="ltx_para">
<p id="A7.I2.i5.I1.i1.p1.1" class="ltx_p">At the beginning of the line (e.g. <span id="A7.I2.i5.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sign-in</span>);</p>
</div>
</li>
<li id="A7.I2.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i2.p1" class="ltx_para">
<p id="A7.I2.i5.I1.i2.p1.1" class="ltx_p">At the end of the line (e.g. <span id="A7.I2.i5.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Read more...</span>);</p>
</div>
</li>
<li id="A7.I2.i5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i3.p1" class="ltx_para">
<p id="A7.I2.i5.I1.i3.p1.1" class="ltx_p">Anywhere in the line (e.g. <span id="A7.I2.i5.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">items in cart</span>).</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="A7.SS2.p2" class="ltx_para">
<p id="A7.SS2.p2.1" class="ltx_p">Finally, if the words in the flagged lines represent more than <math id="A7.SS2.p2.1.m1.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="A7.SS2.p2.1.m1.1a"><mrow id="A7.SS2.p2.1.m1.1.1" xref="A7.SS2.p2.1.m1.1.1.cmml"><mn id="A7.SS2.p2.1.m1.1.1.2" xref="A7.SS2.p2.1.m1.1.1.2.cmml">5</mn><mo id="A7.SS2.p2.1.m1.1.1.1" xref="A7.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS2.p2.1.m1.1b"><apply id="A7.SS2.p2.1.m1.1.1.cmml" xref="A7.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="A7.SS2.p2.1.m1.1.1.1.cmml" xref="A7.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="A7.SS2.p2.1.m1.1.1.2.cmml" xref="A7.SS2.p2.1.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS2.p2.1.m1.1c">5\%</annotation></semantics></math> of the total document words, the document is discarded. We derived these filters through manual inspection of the data, and note that they require adaptation across languages.</p>
</div>
</section>
<section id="A7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.3 </span>Deduplication</h3>

<div id="A7.SS3.p1" class="ltx_para">
<p id="A7.SS3.p1.1" class="ltx_p">We make use of the two deduplication methods described in <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>: <span id="A7.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr</span> and <span id="A7.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">NearDedup</span> (detailed in <a href="#A7.SS3.SSS1" title="G.3.1 MinHash Approximate Matching ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.3.1</span></a> and <a href="#A7.SS3.SSS2" title="G.3.2 Exact substring deduplication ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.3.2</span></a>; see <a href="#A8" title="Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">H</span></a> for samples of duplicates).</p>
</div>
<div id="A7.SS3.p2" class="ltx_para">
<p id="A7.SS3.p2.1" class="ltx_p">We start with the most scalable approach, <span id="A7.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">NearDedup</span>. We remove similar documents by applying MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a href="#bib.bib17" title="" class="ltx_ref">1997</a>)</cite>, whereby a signature/sketch supporting efficient approximate similarity queries is computed for each document in the dataset, and document pairs with a high <span id="A7.SS3.p2.1.2" class="ltx_text ltx_font_italic">n</span>-gram overlap are identified.</p>
</div>
<div id="A7.SS3.p3" class="ltx_para">
<p id="A7.SS3.p3.1" class="ltx_p">We then use&nbsp;<span id="A7.SS3.p3.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr</span>, leveraging the implementation from <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">‡</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‡</sup><span class="ltx_tag ltx_tag_note">‡</span><a target="_blank" href="https://github.com/google-research/deduplicate-text-datasets" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google-research/deduplicate-text-datasets</a></span></span></span>, to identify ranges of exact duplicate text of at least 50 tokens. We experiment with three different approaches for these ranges: <span id="A7.SS3.p3.1.2" class="ltx_text ltx_font_smallcaps">ExactSubstr-Cut</span>, where we remove them from the original text, as done in the original implementation; <span id="A7.SS3.p3.1.3" class="ltx_text ltx_font_smallcaps">ExactSubstr-Mask</span>, where the dataset is unchanged but we do not compute the loss on the duplicated ranges; and <span id="A7.SS3.p3.1.4" class="ltx_text ltx_font_smallcaps">ExactSubstr-Drop</span>, where we simply drop an entire document if the duplicated ranges make up more than a certain percentage of its content.</p>
</div>
<div id="A7.SS3.p4" class="ltx_para">
<p id="A7.SS3.p4.1" class="ltx_p">We present small-scale ablations around these different approaches in <a href="#A5.SS1" title="E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">E.1</span></a>.</p>
</div>
<section id="A7.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.3.1 </span>MinHash Approximate Matching</h4>

<div id="A7.SS3.SSS1.p1" class="ltx_para">
<p id="A7.SS3.SSS1.p1.1" class="ltx_p">We employ MinHash to find approximate duplicate documents in our web corpora at a very large scale. This technique allows us to identify templated pages or otherwise very similar content where most of the interspersed duplicated sections are small enough to not be identified by exact matching methods (anything smaller than 50 tokens).</p>
</div>
<section id="A7.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Signing.</h5>

<div id="A7.SS3.SSS1.Px1.p1" class="ltx_para">
<p id="A7.SS3.SSS1.Px1.p1.2" class="ltx_p">We start by normalizing the content to increase recall: punctuation is removed, text is lowercased, NFD Unicode normalization is applied, accents are removed, and all whitespace is normalized. We tokenize the resulting text using the GPT-2 tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib62" title="" class="ltx_ref">2019</a>)</cite> and obtain the set of unique <span id="A7.SS3.SSS1.Px1.p1.2.1" class="ltx_text ltx_font_italic">n</span>-grams for each document. Hash functions are used to obtain a signature for each document: for each hash function, the smallest value is kept from hashing every unique <span id="A7.SS3.SSS1.Px1.p1.2.2" class="ltx_text ltx_font_italic">n</span>-gram in the document. If two documents are similar, then there is a high probability that they will have the same minimum hash (MinHash) for at least some of the hash functions used <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a href="#bib.bib17" title="" class="ltx_ref">1997</a>)</cite>. The ratio of matching hashes between two documents approximates the Jaccard Similarity <cite class="ltx_cite ltx_citemacro_citep">(Jaccard, <a href="#bib.bib43" title="" class="ltx_ref">1912</a>)</cite> of the sets of their unique <span id="A7.SS3.SSS1.Px1.p1.2.3" class="ltx_text ltx_font_italic">n</span>-grams (the sets being <math id="A7.SS3.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="A7.SS3.SSS1.Px1.p1.1.m1.1a"><msub id="A7.SS3.SSS1.Px1.p1.1.m1.1.1" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2.cmml">d</mi><mi id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px1.p1.1.m1.1b"><apply id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2">𝑑</ci><ci id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px1.p1.1.m1.1c">d_{i}</annotation></semantics></math> and <math id="A7.SS3.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="d_{j}" display="inline"><semantics id="A7.SS3.SSS1.Px1.p1.2.m2.1a"><msub id="A7.SS3.SSS1.Px1.p1.2.m2.1.1" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.cmml"><mi id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2.cmml">d</mi><mi id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px1.p1.2.m2.1b"><apply id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2">𝑑</ci><ci id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px1.p1.2.m2.1c">d_{j}</annotation></semantics></math>):</p>
</div>
<div id="A7.SS3.SSS1.Px1.p2" class="ltx_para">
<table id="A7.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A7.E1.m1.4" class="ltx_Math" alttext="J(d_{i},d_{j})=\frac{\left|d_{i}\cap d_{j}\right|}{\left|d_{i}\cup d_{j}\right|}" display="block"><semantics id="A7.E1.m1.4a"><mrow id="A7.E1.m1.4.4" xref="A7.E1.m1.4.4.cmml"><mrow id="A7.E1.m1.4.4.2" xref="A7.E1.m1.4.4.2.cmml"><mi id="A7.E1.m1.4.4.2.4" xref="A7.E1.m1.4.4.2.4.cmml">J</mi><mo lspace="0em" rspace="0em" id="A7.E1.m1.4.4.2.3" xref="A7.E1.m1.4.4.2.3.cmml">​</mo><mrow id="A7.E1.m1.4.4.2.2.2" xref="A7.E1.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="A7.E1.m1.4.4.2.2.2.3" xref="A7.E1.m1.4.4.2.2.3.cmml">(</mo><msub id="A7.E1.m1.3.3.1.1.1.1" xref="A7.E1.m1.3.3.1.1.1.1.cmml"><mi id="A7.E1.m1.3.3.1.1.1.1.2" xref="A7.E1.m1.3.3.1.1.1.1.2.cmml">d</mi><mi id="A7.E1.m1.3.3.1.1.1.1.3" xref="A7.E1.m1.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="A7.E1.m1.4.4.2.2.2.4" xref="A7.E1.m1.4.4.2.2.3.cmml">,</mo><msub id="A7.E1.m1.4.4.2.2.2.2" xref="A7.E1.m1.4.4.2.2.2.2.cmml"><mi id="A7.E1.m1.4.4.2.2.2.2.2" xref="A7.E1.m1.4.4.2.2.2.2.2.cmml">d</mi><mi id="A7.E1.m1.4.4.2.2.2.2.3" xref="A7.E1.m1.4.4.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="A7.E1.m1.4.4.2.2.2.5" xref="A7.E1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="A7.E1.m1.4.4.3" xref="A7.E1.m1.4.4.3.cmml">=</mo><mfrac id="A7.E1.m1.2.2" xref="A7.E1.m1.2.2.cmml"><mrow id="A7.E1.m1.1.1.1.1" xref="A7.E1.m1.1.1.1.2.cmml"><mo id="A7.E1.m1.1.1.1.1.2" xref="A7.E1.m1.1.1.1.2.1.cmml">|</mo><mrow id="A7.E1.m1.1.1.1.1.1" xref="A7.E1.m1.1.1.1.1.1.cmml"><msub id="A7.E1.m1.1.1.1.1.1.2" xref="A7.E1.m1.1.1.1.1.1.2.cmml"><mi id="A7.E1.m1.1.1.1.1.1.2.2" xref="A7.E1.m1.1.1.1.1.1.2.2.cmml">d</mi><mi id="A7.E1.m1.1.1.1.1.1.2.3" xref="A7.E1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="A7.E1.m1.1.1.1.1.1.1" xref="A7.E1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="A7.E1.m1.1.1.1.1.1.3" xref="A7.E1.m1.1.1.1.1.1.3.cmml"><mi id="A7.E1.m1.1.1.1.1.1.3.2" xref="A7.E1.m1.1.1.1.1.1.3.2.cmml">d</mi><mi id="A7.E1.m1.1.1.1.1.1.3.3" xref="A7.E1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="A7.E1.m1.1.1.1.1.3" xref="A7.E1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="A7.E1.m1.2.2.2.1" xref="A7.E1.m1.2.2.2.2.cmml"><mo id="A7.E1.m1.2.2.2.1.2" xref="A7.E1.m1.2.2.2.2.1.cmml">|</mo><mrow id="A7.E1.m1.2.2.2.1.1" xref="A7.E1.m1.2.2.2.1.1.cmml"><msub id="A7.E1.m1.2.2.2.1.1.2" xref="A7.E1.m1.2.2.2.1.1.2.cmml"><mi id="A7.E1.m1.2.2.2.1.1.2.2" xref="A7.E1.m1.2.2.2.1.1.2.2.cmml">d</mi><mi id="A7.E1.m1.2.2.2.1.1.2.3" xref="A7.E1.m1.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="A7.E1.m1.2.2.2.1.1.1" xref="A7.E1.m1.2.2.2.1.1.1.cmml">∪</mo><msub id="A7.E1.m1.2.2.2.1.1.3" xref="A7.E1.m1.2.2.2.1.1.3.cmml"><mi id="A7.E1.m1.2.2.2.1.1.3.2" xref="A7.E1.m1.2.2.2.1.1.3.2.cmml">d</mi><mi id="A7.E1.m1.2.2.2.1.1.3.3" xref="A7.E1.m1.2.2.2.1.1.3.3.cmml">j</mi></msub></mrow><mo id="A7.E1.m1.2.2.2.1.3" xref="A7.E1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A7.E1.m1.4b"><apply id="A7.E1.m1.4.4.cmml" xref="A7.E1.m1.4.4"><eq id="A7.E1.m1.4.4.3.cmml" xref="A7.E1.m1.4.4.3"></eq><apply id="A7.E1.m1.4.4.2.cmml" xref="A7.E1.m1.4.4.2"><times id="A7.E1.m1.4.4.2.3.cmml" xref="A7.E1.m1.4.4.2.3"></times><ci id="A7.E1.m1.4.4.2.4.cmml" xref="A7.E1.m1.4.4.2.4">𝐽</ci><interval closure="open" id="A7.E1.m1.4.4.2.2.3.cmml" xref="A7.E1.m1.4.4.2.2.2"><apply id="A7.E1.m1.3.3.1.1.1.1.cmml" xref="A7.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="A7.E1.m1.3.3.1.1.1.1.1.cmml" xref="A7.E1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="A7.E1.m1.3.3.1.1.1.1.2.cmml" xref="A7.E1.m1.3.3.1.1.1.1.2">𝑑</ci><ci id="A7.E1.m1.3.3.1.1.1.1.3.cmml" xref="A7.E1.m1.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="A7.E1.m1.4.4.2.2.2.2.cmml" xref="A7.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="A7.E1.m1.4.4.2.2.2.2.1.cmml" xref="A7.E1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="A7.E1.m1.4.4.2.2.2.2.2.cmml" xref="A7.E1.m1.4.4.2.2.2.2.2">𝑑</ci><ci id="A7.E1.m1.4.4.2.2.2.2.3.cmml" xref="A7.E1.m1.4.4.2.2.2.2.3">𝑗</ci></apply></interval></apply><apply id="A7.E1.m1.2.2.cmml" xref="A7.E1.m1.2.2"><divide id="A7.E1.m1.2.2.3.cmml" xref="A7.E1.m1.2.2"></divide><apply id="A7.E1.m1.1.1.1.2.cmml" xref="A7.E1.m1.1.1.1.1"><abs id="A7.E1.m1.1.1.1.2.1.cmml" xref="A7.E1.m1.1.1.1.1.2"></abs><apply id="A7.E1.m1.1.1.1.1.1.cmml" xref="A7.E1.m1.1.1.1.1.1"><intersect id="A7.E1.m1.1.1.1.1.1.1.cmml" xref="A7.E1.m1.1.1.1.1.1.1"></intersect><apply id="A7.E1.m1.1.1.1.1.1.2.cmml" xref="A7.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A7.E1.m1.1.1.1.1.1.2.1.cmml" xref="A7.E1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="A7.E1.m1.1.1.1.1.1.2.2.cmml" xref="A7.E1.m1.1.1.1.1.1.2.2">𝑑</ci><ci id="A7.E1.m1.1.1.1.1.1.2.3.cmml" xref="A7.E1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="A7.E1.m1.1.1.1.1.1.3.cmml" xref="A7.E1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E1.m1.1.1.1.1.1.3.1.cmml" xref="A7.E1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="A7.E1.m1.1.1.1.1.1.3.2.cmml" xref="A7.E1.m1.1.1.1.1.1.3.2">𝑑</ci><ci id="A7.E1.m1.1.1.1.1.1.3.3.cmml" xref="A7.E1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="A7.E1.m1.2.2.2.2.cmml" xref="A7.E1.m1.2.2.2.1"><abs id="A7.E1.m1.2.2.2.2.1.cmml" xref="A7.E1.m1.2.2.2.1.2"></abs><apply id="A7.E1.m1.2.2.2.1.1.cmml" xref="A7.E1.m1.2.2.2.1.1"><union id="A7.E1.m1.2.2.2.1.1.1.cmml" xref="A7.E1.m1.2.2.2.1.1.1"></union><apply id="A7.E1.m1.2.2.2.1.1.2.cmml" xref="A7.E1.m1.2.2.2.1.1.2"><csymbol cd="ambiguous" id="A7.E1.m1.2.2.2.1.1.2.1.cmml" xref="A7.E1.m1.2.2.2.1.1.2">subscript</csymbol><ci id="A7.E1.m1.2.2.2.1.1.2.2.cmml" xref="A7.E1.m1.2.2.2.1.1.2.2">𝑑</ci><ci id="A7.E1.m1.2.2.2.1.1.2.3.cmml" xref="A7.E1.m1.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="A7.E1.m1.2.2.2.1.1.3.cmml" xref="A7.E1.m1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="A7.E1.m1.2.2.2.1.1.3.1.cmml" xref="A7.E1.m1.2.2.2.1.1.3">subscript</csymbol><ci id="A7.E1.m1.2.2.2.1.1.3.2.cmml" xref="A7.E1.m1.2.2.2.1.1.3.2">𝑑</ci><ci id="A7.E1.m1.2.2.2.1.1.3.3.cmml" xref="A7.E1.m1.2.2.2.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.E1.m1.4c">J(d_{i},d_{j})=\frac{\left|d_{i}\cap d_{j}\right|}{\left|d_{i}\cup d_{j}\right|}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A7.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Matching.</h5>

<div id="A7.SS3.SSS1.Px2.p1" class="ltx_para">
<p id="A7.SS3.SSS1.Px2.p1.5" class="ltx_p">Since comparing MinHash signatures between every possible document pair is computationally expensive, we apply a locality sensitive hashing version of MinHash, MinHash LSH. A document signature is split into <span id="A7.SS3.SSS1.Px2.p1.5.1" class="ltx_text ltx_font_italic">r</span> buckets, each with <span id="A7.SS3.SSS1.Px2.p1.5.2" class="ltx_text ltx_font_italic">b</span> minhashes. Documents are indexed by these <span id="A7.SS3.SSS1.Px2.p1.5.3" class="ltx_text ltx_font_italic">b</span> minhashes on each of the <span id="A7.SS3.SSS1.Px2.p1.5.4" class="ltx_text ltx_font_italic">r</span> buckets, and we mark two documents as duplicates if their <span id="A7.SS3.SSS1.Px2.p1.5.5" class="ltx_text ltx_font_italic">b</span> minhashes are exactly the same on at least one of the buckets. These two parameters, <span id="A7.SS3.SSS1.Px2.p1.5.6" class="ltx_text ltx_font_italic">b</span> and <span id="A7.SS3.SSS1.Px2.p1.5.7" class="ltx_text ltx_font_italic">r</span>, will determine the probability that similar documents will be detected. For two documents <math id="A7.SS3.SSS1.Px2.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A7.SS3.SSS1.Px2.p1.1.m1.1a"><mi id="A7.SS3.SSS1.Px2.p1.1.m1.1.1" xref="A7.SS3.SSS1.Px2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.1.m1.1b"><ci id="A7.SS3.SSS1.Px2.p1.1.m1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.1.m1.1c">i</annotation></semantics></math> and <math id="A7.SS3.SSS1.Px2.p1.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="A7.SS3.SSS1.Px2.p1.2.m2.1a"><mi id="A7.SS3.SSS1.Px2.p1.2.m2.1.1" xref="A7.SS3.SSS1.Px2.p1.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.2.m2.1b"><ci id="A7.SS3.SSS1.Px2.p1.2.m2.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.2.m2.1c">j</annotation></semantics></math> whose ratio of matching hashes between their MinHash signatures is <math id="A7.SS3.SSS1.Px2.p1.3.m3.2" class="ltx_Math" alttext="s_{i,j}" display="inline"><semantics id="A7.SS3.SSS1.Px2.p1.3.m3.2a"><msub id="A7.SS3.SSS1.Px2.p1.3.m3.2.3" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.3.m3.2.3.2" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.3.2.cmml">s</mi><mrow id="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.4" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.3.m3.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.3.m3.1.1.1.1.cmml">i</mi><mo id="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.4.1" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.2" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.3.m3.2b"><apply id="A7.SS3.SSS1.Px2.p1.3.m3.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.3.m3.2.3.1.cmml" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.3">subscript</csymbol><ci id="A7.SS3.SSS1.Px2.p1.3.m3.2.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.3.2">𝑠</ci><list id="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.4"><ci id="A7.SS3.SSS1.Px2.p1.3.m3.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.3.m3.1.1.1.1">𝑖</ci><ci id="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.3.m3.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.3.m3.2c">s_{i,j}</annotation></semantics></math>, the probability that there is a match in a given bucket is <math id="A7.SS3.SSS1.Px2.p1.4.m4.2" class="ltx_Math" alttext="s_{i,j}^{b}" display="inline"><semantics id="A7.SS3.SSS1.Px2.p1.4.m4.2a"><msubsup id="A7.SS3.SSS1.Px2.p1.4.m4.2.3" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2.cmml">s</mi><mrow id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.4" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1.cmml">i</mi><mo id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.4.1" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2.cmml">j</mi></mrow><mi id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3.cmml">b</mi></msubsup><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.4.m4.2b"><apply id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.1.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3">superscript</csymbol><apply id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.1.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3">subscript</csymbol><ci id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2">𝑠</ci><list id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.4"><ci id="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1">𝑖</ci><ci id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2">𝑗</ci></list></apply><ci id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.4.m4.2c">s_{i,j}^{b}</annotation></semantics></math>; the probability that there isn’t a match in any of the buckets is <math id="A7.SS3.SSS1.Px2.p1.5.m5.3" class="ltx_Math" alttext="(1-s_{i,j}^{b})^{r}" display="inline"><semantics id="A7.SS3.SSS1.Px2.p1.5.m5.3a"><msup id="A7.SS3.SSS1.Px2.p1.5.m5.3.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.cmml"><mrow id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml"><mo stretchy="false" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml">(</mo><mrow id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml"><mn id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2.cmml">1</mn><mo id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1.cmml">−</mo><msubsup id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2.cmml">s</mi><mrow id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.4" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1.cmml">i</mi><mo id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.4.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2.cmml">j</mi></mrow><mi id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3.cmml">b</mi></msubsup></mrow><mo stretchy="false" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml">)</mo></mrow><mi id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.5.m5.3b"><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3">superscript</csymbol><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1"><minus id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1"></minus><cn type="integer" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2">1</cn><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3">superscript</csymbol><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3">subscript</csymbol><ci id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2">𝑠</ci><list id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.4"><ci id="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1">𝑖</ci><ci id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2">𝑗</ci></list></apply><ci id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3">𝑏</ci></apply></apply><ci id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.5.m5.3c">(1-s_{i,j}^{b})^{r}</annotation></semantics></math>; and finally that there is a match in at least one of the buckets:</p>
</div>
<div id="A7.SS3.SSS1.Px2.p2" class="ltx_para">
<table id="A7.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A7.E2.m1.3" class="ltx_Math" alttext="P=1-(1-s_{i,j}^{b})^{r}" display="block"><semantics id="A7.E2.m1.3a"><mrow id="A7.E2.m1.3.3" xref="A7.E2.m1.3.3.cmml"><mi id="A7.E2.m1.3.3.3" xref="A7.E2.m1.3.3.3.cmml">P</mi><mo id="A7.E2.m1.3.3.2" xref="A7.E2.m1.3.3.2.cmml">=</mo><mrow id="A7.E2.m1.3.3.1" xref="A7.E2.m1.3.3.1.cmml"><mn id="A7.E2.m1.3.3.1.3" xref="A7.E2.m1.3.3.1.3.cmml">1</mn><mo id="A7.E2.m1.3.3.1.2" xref="A7.E2.m1.3.3.1.2.cmml">−</mo><msup id="A7.E2.m1.3.3.1.1" xref="A7.E2.m1.3.3.1.1.cmml"><mrow id="A7.E2.m1.3.3.1.1.1.1" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="A7.E2.m1.3.3.1.1.1.1.2" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="A7.E2.m1.3.3.1.1.1.1.1" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml"><mn id="A7.E2.m1.3.3.1.1.1.1.1.2" xref="A7.E2.m1.3.3.1.1.1.1.1.2.cmml">1</mn><mo id="A7.E2.m1.3.3.1.1.1.1.1.1" xref="A7.E2.m1.3.3.1.1.1.1.1.1.cmml">−</mo><msubsup id="A7.E2.m1.3.3.1.1.1.1.1.3" xref="A7.E2.m1.3.3.1.1.1.1.1.3.cmml"><mi id="A7.E2.m1.3.3.1.1.1.1.1.3.2.2" xref="A7.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml">s</mi><mrow id="A7.E2.m1.2.2.2.4" xref="A7.E2.m1.2.2.2.3.cmml"><mi id="A7.E2.m1.1.1.1.1" xref="A7.E2.m1.1.1.1.1.cmml">i</mi><mo id="A7.E2.m1.2.2.2.4.1" xref="A7.E2.m1.2.2.2.3.cmml">,</mo><mi id="A7.E2.m1.2.2.2.2" xref="A7.E2.m1.2.2.2.2.cmml">j</mi></mrow><mi id="A7.E2.m1.3.3.1.1.1.1.1.3.3" xref="A7.E2.m1.3.3.1.1.1.1.1.3.3.cmml">b</mi></msubsup></mrow><mo stretchy="false" id="A7.E2.m1.3.3.1.1.1.1.3" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mi id="A7.E2.m1.3.3.1.1.3" xref="A7.E2.m1.3.3.1.1.3.cmml">r</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.E2.m1.3b"><apply id="A7.E2.m1.3.3.cmml" xref="A7.E2.m1.3.3"><eq id="A7.E2.m1.3.3.2.cmml" xref="A7.E2.m1.3.3.2"></eq><ci id="A7.E2.m1.3.3.3.cmml" xref="A7.E2.m1.3.3.3">𝑃</ci><apply id="A7.E2.m1.3.3.1.cmml" xref="A7.E2.m1.3.3.1"><minus id="A7.E2.m1.3.3.1.2.cmml" xref="A7.E2.m1.3.3.1.2"></minus><cn type="integer" id="A7.E2.m1.3.3.1.3.cmml" xref="A7.E2.m1.3.3.1.3">1</cn><apply id="A7.E2.m1.3.3.1.1.cmml" xref="A7.E2.m1.3.3.1.1"><csymbol cd="ambiguous" id="A7.E2.m1.3.3.1.1.2.cmml" xref="A7.E2.m1.3.3.1.1">superscript</csymbol><apply id="A7.E2.m1.3.3.1.1.1.1.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1"><minus id="A7.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.1"></minus><cn type="integer" id="A7.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.2">1</cn><apply id="A7.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3">superscript</csymbol><apply id="A7.E2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E2.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="A7.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3.2.2">𝑠</ci><list id="A7.E2.m1.2.2.2.3.cmml" xref="A7.E2.m1.2.2.2.4"><ci id="A7.E2.m1.1.1.1.1.cmml" xref="A7.E2.m1.1.1.1.1">𝑖</ci><ci id="A7.E2.m1.2.2.2.2.cmml" xref="A7.E2.m1.2.2.2.2">𝑗</ci></list></apply><ci id="A7.E2.m1.3.3.1.1.1.1.1.3.3.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3.3">𝑏</ci></apply></apply><ci id="A7.E2.m1.3.3.1.1.3.cmml" xref="A7.E2.m1.3.3.1.1.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.E2.m1.3c">P=1-(1-s_{i,j}^{b})^{r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="A7.SS3.SSS1.Px2.p3" class="ltx_para">
<p id="A7.SS3.SSS1.Px2.p3.5" class="ltx_p">We use the same parameters as <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>: <math id="A7.SS3.SSS1.Px2.p3.1.m1.1" class="ltx_Math" alttext="n=5" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.1.m1.1a"><mrow id="A7.SS3.SSS1.Px2.p3.1.m1.1.1" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.cmml"><mi id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2.cmml">n</mi><mo id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1.cmml">=</mo><mn id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.1.m1.1b"><apply id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1"><eq id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1"></eq><ci id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2">𝑛</ci><cn type="integer" id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.1.m1.1c">n=5</annotation></semantics></math> (<span id="A7.SS3.SSS1.Px2.p3.5.1" class="ltx_text ltx_font_italic">5</span>-grams); <math id="A7.SS3.SSS1.Px2.p3.2.m2.1" class="ltx_Math" alttext="b=20" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.2.m2.1a"><mrow id="A7.SS3.SSS1.Px2.p3.2.m2.1.1" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.cmml"><mi id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2.cmml">b</mi><mo id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1.cmml">=</mo><mn id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.2.m2.1b"><apply id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1"><eq id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1"></eq><ci id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2">𝑏</ci><cn type="integer" id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.2.m2.1c">b=20</annotation></semantics></math> and <math id="A7.SS3.SSS1.Px2.p3.3.m3.1" class="ltx_Math" alttext="r=450" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.3.m3.1a"><mrow id="A7.SS3.SSS1.Px2.p3.3.m3.1.1" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.cmml"><mi id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2.cmml">r</mi><mo id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1.cmml">=</mo><mn id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3.cmml">450</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.3.m3.1b"><apply id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1"><eq id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1"></eq><ci id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2">𝑟</ci><cn type="integer" id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3">450</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.3.m3.1c">r=450</annotation></semantics></math>. This means that for each document, we compute a total of 9000 minhashes, and that the probability that a document pair with similarity 0.75 or 0.8 will be marked as duplicates will be <math id="A7.SS3.SSS1.Px2.p3.4.m4.1" class="ltx_Math" alttext="76\%" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.4.m4.1a"><mrow id="A7.SS3.SSS1.Px2.p3.4.m4.1.1" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.cmml"><mn id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2.cmml">76</mn><mo id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.4.m4.1b"><apply id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1"><csymbol cd="latexml" id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2">76</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.4.m4.1c">76\%</annotation></semantics></math> and <math id="A7.SS3.SSS1.Px2.p3.5.m5.1" class="ltx_Math" alttext="99.4\%" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.5.m5.1a"><mrow id="A7.SS3.SSS1.Px2.p3.5.m5.1.1" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.cmml"><mn id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2.cmml">99.4</mn><mo id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.5.m5.1b"><apply id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1"><csymbol cd="latexml" id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2">99.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.5.m5.1c">99.4\%</annotation></semantics></math> (respectively), diminishing rapidly for smaller similarity values.</p>
</div>
<div id="A7.SS3.SSS1.Px2.p4" class="ltx_para">
<p id="A7.SS3.SSS1.Px2.p4.1" class="ltx_p">Finally, we cluster documents across all buckets — if documents A and B match in one bucket and B and C in another, A-B-C becomes a cluster. We randomly remove all but one of the documents in each cluster.</p>
</div>
<div id="A7.SS3.SSS1.Px2.p5" class="ltx_para">
<p id="A7.SS3.SSS1.Px2.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> also proposed filtering down on false positives by computing the real Jaccard similarity, or other metrics such as the edit similarity between identified document pairs. Given the large amount of data we have available across all of CommonCrawl, and that our main concern is improving recall, we decided to skip this additional step.</p>
</div>
</section>
</section>
<section id="A7.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.3.2 </span>Exact substring deduplication</h4>

<div id="A7.SS3.SSS2.p1" class="ltx_para">
<p id="A7.SS3.SSS2.p1.1" class="ltx_p">We make use of the <span id="A7.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr</span> implementation publicly released by <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> for exact text matching. We apply exact substring deduplication to data that has already been deduplicated by MinHash, reducing by nearly 40% size of the dataset on which we have to operate. <span id="A7.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_smallcaps">ExactSubstr</span> will find long strings of text that are present, character for character, across multiple documents. Some of these may have escaped the earlier stage of approximate deduplication: they might not constitute a big enough portion of the document; one document might have repeated sections sourced across many different documents; or they may simply not have been found due to the approximate nature of MinHash.</p>
</div>
<section id="A7.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Finding duplicates.</h5>

<div id="A7.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="A7.SS3.SSS2.Px1.p1.1" class="ltx_p"><span id="A7.SS3.SSS2.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr</span> concatenates all the documents in the dataset to create a single long text sequence; then, it builds a suffix array <cite class="ltx_cite ltx_citemacro_citep">(Manber &amp; Myers, <a href="#bib.bib53" title="" class="ltx_ref">1993</a>)</cite> in linear time—an array of the indexes to a lexicographical ordering of all the suffixes in the sequence. Finally, duplicate sequences can also be found in linear time using the suffix array, by simply traversing the ordered list of suffixes and comparing the beginning of each pair of two consecutive suffixes.</p>
</div>
<div id="A7.SS3.SSS2.Px1.p2" class="ltx_para">
<p id="A7.SS3.SSS2.Px1.p2.1" class="ltx_p">We apply the same normalization and tokenization as for MinHash to the content of our documents before concatenating them. One important difference is that reversibility is important: for MinHash, we were discarding entire documents, and thus never relying on the normalized+tokenized representation for downstream use. Here, once we have identified duplicate normalized+tokenized spans, we need to revert to the original span to remove it. Accordingly, we include normalization in the tokenization process, and validate that the process is reversible.</p>
</div>
<div id="A7.SS3.SSS2.Px1.p3" class="ltx_para">
<p id="A7.SS3.SSS2.Px1.p3.1" class="ltx_p">If a match is longer than 50 tokens, there will be multiple overlapping duplicated ranges. These overlapping duplicated ranges in the concatenated dataset sequence are merged before we save them to a file. We then take these ranges and retrieve the original document that produced them, obtaining the character substrings corresponding to the duplicated token ranges.</p>
</div>
</section>
<section id="A7.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Removing duplicates.</h5>

<div id="A7.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="A7.SS3.SSS2.Px2.p1.1" class="ltx_p">We considered applying the following transformations to the duplicate spans:</p>
</div>
<div id="A7.SS3.SSS2.Px2.p2" class="ltx_para">
<ul id="A7.I3" class="ltx_itemize">
<li id="A7.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i1.p1" class="ltx_para">
<p id="A7.I3.i1.p1.1" class="ltx_p"><span id="A7.I3.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr-Cut</span>: we remove the duplicated spans, and discard documents where there are fewer than 20 non-duplicated characters left–this is the vanilla setting used by <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>;</p>
</div>
</li>
<li id="A7.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i2.p1" class="ltx_para">
<p id="A7.I3.i2.p1.1" class="ltx_p"><span id="A7.I3.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr-Mask</span>: we loss-mask the duplicated spans, preventing a loss from being computed on the duplicated text during pretraining, and discard documents where there are fewer than 20 non-masked characters left.</p>
</div>
</li>
<li id="A7.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i3.p1" class="ltx_para">
<p id="A7.I3.i3.p1.1" class="ltx_p"><span id="A7.I3.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr-DropPartial</span>: if more than 20% of the document is duplicated, we remove the entire document;</p>
</div>
</li>
<li id="A7.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i4.p1" class="ltx_para">
<p id="A7.I3.i4.p1.1" class="ltx_p"><span id="A7.I3.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr-DropAny</span>: we drop any document with a duplicated span in it.</p>
</div>
</li>
</ul>
</div>
<div id="A7.SS3.SSS2.Px2.p3" class="ltx_para">
<p id="A7.SS3.SSS2.Px2.p3.1" class="ltx_p">Broadly speaking, <span id="A7.SS3.SSS2.Px2.p3.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr-Cut</span> might remove text mid-sentence resulting in disconnected text; <span id="A7.SS3.SSS2.Px2.p3.1.2" class="ltx_text ltx_font_smallcaps">ExactSubstr-Mask</span> does not have this issue, but might be less efficient as a significant portion of the training tokens will not directly contribute to updating the model’s weights; <span id="A7.SS3.SSS2.Px2.p3.1.3" class="ltx_text ltx_font_smallcaps">ExactSubstr-Drop</span> might still keep considerable duplicated sections in its <span id="A7.SS3.SSS2.Px2.p3.1.4" class="ltx_text ltx_font_smallcaps">Partial</span> version, especially on larger documents, while the <span id="A7.SS3.SSS2.Px2.p3.1.5" class="ltx_text ltx_font_smallcaps">Any</span> version might be overly aggressive. Following ablations in <a href="#A5.SS1" title="E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">E.1</span></a>, we choose to stick with the vanilla approach, <span id="A7.SS3.SSS2.Px2.p3.1.6" class="ltx_text ltx_font_smallcaps">ExactSubstr-Cut</span>.</p>
</div>
<div id="A7.SS3.SSS2.Px2.p4" class="ltx_para">
<p id="A7.SS3.SSS2.Px2.p4.1" class="ltx_p">Note that in all cases, while MinHash keeps one copy of the duplicated documents, our exact deduplication removes all copies of the duplicated span.</p>
</div>
</section>
</section>
</section>
<section id="A7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.4 </span>Execution environment</h3>

<div id="A7.SS4.p1" class="ltx_para">
<p id="A7.SS4.p1.1" class="ltx_p">Most data processing took place in large CPU clusters, with 100-250 AWS c5.18xlarge instances; each instance has 72 vCPUs and 144 GiB of memory. We usually run with 10,000-20,000 vCPUs in the cluster, enabling rapid parallel processing.</p>
</div>
<div id="A7.SS4.p2" class="ltx_para">
<p id="A7.SS4.p2.1" class="ltx_p">For <span id="A7.SS4.p2.1.1" class="ltx_text ltx_font_smallcaps">ExactSubstr</span>, the entire dataset being deduplicated needs to be loaded onto memory: we leveraged the AWS x2iedn instances, which come with up to 2 TiB of memory in a single instance.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Deduplication samples from RefinedWeb</h2>

<section id="A8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.1 </span>MinHash clusters</h3>

<div id="A8.SS1.p1" class="ltx_para">
<p id="A8.SS1.p1.1" class="ltx_p">We report the 8 largest duplicate clusters found by MinHash in <a href="#A8.T15" title="In H.1 MinHash clusters ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">15</span></a> – each spanning hundreds of thousands of documents. We also found a large number of duplicate document pairs to be due to different URL GET parameters not resulting in significantly different content. An example of this behaviour can be seen in the URLs presented in <a href="#A8.T16" title="In H.1 MinHash clusters ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">16</span></a>.</p>
</div>
<figure id="A8.T15" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 15: </span><span id="A8.T15.2.1" class="ltx_text ltx_font_bold">Top-8 largest MinHash clusters found when building RefinedWeb.</span> We cut some of the longest samples in the interest of readability, only keeping a brief description.</figcaption>
<table id="A8.T15.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T15.3.1" class="ltx_tr">
<td id="A8.T15.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A8.T15.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.1.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Description</span></span>
</span>
</td>
<td id="A8.T15.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A8.T15.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.1.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Example document</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.2" class="ltx_tr">
<td id="A8.T15.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.2.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Wordpress sitemap notice generated by the Google Sitemap Generator Plugin</span></span>
</span>
</td>
<td id="A8.T15.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T15.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.2.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">This is a XML Sitemap which is supposed to be processed by search engines which follow the XML Sitemap standard like Ask.com, Bing, Google and Yahoo. It was generated using the WordPress content management system and the Google Sitemap Generator Plugin by Arne Brachhold. You can find more information about XML sitemaps on sitemaps.org and Google’s list of sitemap programs. This file contains links to sub-sitemaps, follow them to see the actual sitemap content.</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.3" class="ltx_tr">
<td id="A8.T15.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.3.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Cloudflare notice to enable Javascript</span></span>
</span>
</td>
<td id="A8.T15.3.3.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.4" class="ltx_tr">
<td id="A8.T15.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.4.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Templated disability notice, with different phone numbers across pages</span></span>
</span>
</td>
<td id="A8.T15.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T15.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.4.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Welcome to our website! As we have the ability to list over one million items on our website (our selection changes all of the time), it is not feasible for a company our size to record and playback the descriptions on every item on our website. However, if you are an American with a disability we are here to help you. Please call our disability services phone line at [redacted] or [redacted] during regular business hours and one of our kind and friendly personal shoppers will help you navigate through our website, help conduct advanced searches, help you choose the item you are looking for with the specifications you are seeking, read you the specifications of any item and consult with you about the products themselves. There is no charge for the help of this personal shopper for any American with a disability. Finally, your personal shopper will explain our Privacy Policy and Terms of Service, and help you place an order if you so desire.</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.5" class="ltx_tr">
<td id="A8.T15.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.5.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Templated cookies notice</span></span>
</span>
</td>
<td id="A8.T15.3.5.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.6" class="ltx_tr">
<td id="A8.T15.3.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.6.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Templated domain name for sale page</span></span>
</span>
</td>
<td id="A8.T15.3.6.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.7" class="ltx_tr">
<td id="A8.T15.3.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.7.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.7.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">www.metoperashop.org</span><span id="A8.T15.3.7.1.1.1.2" class="ltx_text" style="font-size:90%;"> and sub-URLs, with content changes but always the same (large) footer</span></span>
</span>
</td>
<td id="A8.T15.3.7.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.8" class="ltx_tr">
<td id="A8.T15.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.8.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Different pages across more than 80 different domain names but with a common section</span></span>
</span>
</td>
<td id="A8.T15.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T15.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.8.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.8.2.1.1.1" class="ltx_text" style="font-size:90%;">DC Customers also liked:
Special event items are produced by manufacturers only after the outcome of a game or event. These are advanced sale items and will ship immediately after they are received in our warehouse.
Manufacturer direct items are shipped directly from the manufacturer. These items are not available for international or expedited shipping.
Customized items can be personalized with options such as your name, your favorite number, and/or designs. Some options may be limited by league rules.</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.9" class="ltx_tr">
<td id="A8.T15.3.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A8.T15.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.9.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.9.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">http://www.boxofficemojo.com/daily</span><span id="A8.T15.3.9.1.1.1.2" class="ltx_text" style="font-size:90%;"> and sub-URLs</span></span>
</span>
</td>
<td id="A8.T15.3.9.2" class="ltx_td ltx_align_top ltx_border_bb ltx_border_t"></td>
</tr>
</tbody></table>
</figure>
<figure id="A8.T16" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 16: </span><span id="A8.T16.2.1" class="ltx_text ltx_font_bold">URL with different GET parameters don’t always result in significantly different page content.</span></figcaption>
<table id="A8.T16.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T16.3.1" class="ltx_tr">
<td id="A8.T16.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A8.T16.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.1.1.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.1.1.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">http://gamesandbiz.blogspot.com/2010/
07/bad-reviews-can-hurt-game-sales.ht
ml?showComment=1278486430242
</pre>
</span>
</td>
<td id="A8.T16.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A8.T16.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.1.2.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.1.2.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">http://gamesandbiz.blogspot.com/2010/
07/bad-reviews-can-hurt-game-sales.ht
ml?showComment=1278499674195</pre>
</span>
</td>
</tr>
<tr id="A8.T16.3.2" class="ltx_tr">
<td id="A8.T16.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A8.T16.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.2.1.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.2.1.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">https://www.ocean-oxygen.org/home;jse
ssionid=1E3290E84F668552FAC643D0A8F81
BEC?p_p_id=122_INSTANCE_Zy6zjkRLAg7v&amp;
p_p_lifecycle=0&amp;p_p_state=normal&amp;p_p_
mode=view&amp;p_p_col_id=column-2&amp;p_p_col
_pos=1&amp;p_p_col_count=6&amp;p_r_p_56423352
4_resetCur=true&amp;p_r_p_564233524_categ
oryId=1346016</pre>
</span>
</td>
<td id="A8.T16.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A8.T16.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.2.2.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.2.2.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">https://www.ocean-oxygen.org/home?p_p
_id=122_INSTANCE_Zy6zjkRLAg7v&amp;p_p_lif
ecycle=0&amp;p_p_state=normal&amp;p_p_mode=vi
ew&amp;p_p_col_id=column-2&amp;p_p_col_pos=1&amp;
p_p_col_count=6&amp;p_r_p_564233524_reset
Cur=true&amp;p_r_p_564233524_categoryId=1
346016</pre>
</span>
</td>
</tr>
</tbody></table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.2 </span>Exact substring matches</h3>

<div id="A8.SS2.p1" class="ltx_para">
<p id="A8.SS2.p1.1" class="ltx_p">Examples of exact matches found by exact substring deduplication can be seen in Table <a href="#A8.T17" title="Table 17 ‣ H.2 Exact substring matches ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<figure id="A8.T17" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 17: </span><span id="A8.T17.3.1" class="ltx_text ltx_font_bold">Matches found by exact substring deduplication</span> (in <em id="A8.T17.4.2" class="ltx_emph ltx_font_italic">italics</em>).</figcaption>
<table id="A8.T17.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T17.5.1" class="ltx_tr">
<td id="A8.T17.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A8.T17.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.1.1.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">it appears there is a transfer of ranking signals in this relationship. Supporting this finding is a quote from Google’s guidelines: </span><em id="A8.T17.5.1.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Using JavaScript to redirect users can be a legitimate practice. For example, if you redirect users to an internal page once they’re logged in, you can use JavaScript to do so. When examining JavaScript or other redirect methods to ensure your site adheres to our guidelines, consider the intent. Keep in mind that 301 redirects are best when moving your site, but you could use a JavaScript redirect for this purpose if you don’t have access to your website’s server.</em><span id="A8.T17.5.1.1.1.1.3" class="ltx_text" style="font-size:90%;"> NOTE: Their experiment is based on a live page with status code 200 and NOT an inactive page. So if you want to implement this for legacy</span></span>
</span>
</td>
<td id="A8.T17.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A8.T17.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.1.2.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Some examples of sneaky redirects include:
- Search engines shown one type of content while users are redirected to something significantly different.
- Desktop users receive a normal page, while mobile users are redirected to a completely different spam domain. </span><em id="A8.T17.5.1.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Using JavaScript to redirect users can be a legitimate practice. For example, if you redirect users to an internal page once they’re logged in, you can use JavaScript to do so. When examining JavaScript or other redirect methods to ensure your site adheres to our guidelines, consider the intent. Keep in mind that 301 redirects are best when moving your site, but you could use a JavaScript redirect for this purpose if you don’t have access to your website’s server.</em></span>
</span>
</td>
</tr>
<tr id="A8.T17.5.2" class="ltx_tr">
<td id="A8.T17.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T17.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.2.1.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Find Palm Beache FL homes for sale and other Palm Beach real estate on homesofthepalmbeaches.com. Browse and search Palm Beach houses, condos, townhomes and single-family homes by community , building, or location. </span><em id="A8.T17.5.2.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Our extensive database of real estate listings provide the most comprehensive property details including home values, features and local school and neighborhood info so you can be sure that you have nearly all the facts you need upfront. Search</em><span id="A8.T17.5.2.1.1.1.3" class="ltx_text" style="font-size:90%;"> homesofthepalmbeaches.com today! Want a closer look at what other Palm Beach properties are available?</span></span>
</span>
</td>
<td id="A8.T17.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T17.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.2.2.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Search Stuart houses, condos, townhomes and single-family homes by price and location. </span><em id="A8.T17.5.2.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Our extensive database of real estate listings provide the most comprehensive property details including home values, features and local school and neighborhood info so you can be sure that you have nearly all the facts you need upfront. Search</em><span id="A8.T17.5.2.2.1.1.3" class="ltx_text" style="font-size:90%;"> Stuart Listings today! Want a closer look at what other Stuart properties are available? Also search our listings for the Newest Stuart Listings and Stuart Homes with Price Reductions now.
Stuart FL Homes for Sale - Stuart Real Estate Listings FREE to search
Stuart Property</span></span>
</span>
</td>
</tr>
<tr id="A8.T17.5.3" class="ltx_tr">
<td id="A8.T17.5.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T17.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.3.1.1.1" class="ltx_p" style="width:209.6pt;"><em id="A8.T17.5.3.1.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">To find the correct size you should measure your foot from the heel to the toe point.
Add approximately 1 - 1,5cm to get the actual inner sole length. Measure both feet and fit shoes to the larger foot.
Measure feet at the end of the day, when your feet are at their largest.</em><span id="A8.T17.5.3.1.1.1.2" class="ltx_text" style="font-size:90%;"> Lente shoes are women’s easy slip-on leisure shoes for everyday use.
These lightweight shoes have a breathable textile mesh upper made of recycled PET bottles and cool Lycra lining.</span></span>
</span>
</td>
<td id="A8.T17.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T17.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.3.2.1.1" class="ltx_p" style="width:209.6pt;"><em id="A8.T17.5.3.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">To find the correct size you should measure your foot from the heel to the toe point.
Add approximately 1 - 1,5cm to get the actual inner sole length. Measure both feet and fit shoes to the larger foot.
Measure feet at the end of the day, when your feet are at their largest.</em><span id="A8.T17.5.3.2.1.1.2" class="ltx_text" style="font-size:90%;"> Enjoy your summer days with Masera leisure sneakers. These low-cut women’s sneakers are extremely lightweight thanks to phylon midsole and breathable textile mesh upper</span></span>
</span>
</td>
</tr>
<tr id="A8.T17.5.4" class="ltx_tr">
<td id="A8.T17.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A8.T17.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.4.1.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">This bandana makes the perfect addition to every fur babies birthday collection! With its sparkly crown pattern, your pup will be ready for every birthday celebration! </span><em id="A8.T17.5.4.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">With snaps for security, this bandana is made with love, down to the very last stitch !
Fabric: cotton
Care Instructions: Hand wash only, iron as needed, on low heat
Always supervise your pup while wearing Faithful Paws Co. accessories, as it could become a choking hazard if consumed.</em></span>
</span>
</td>
<td id="A8.T17.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A8.T17.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.4.2.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">This bandana makes the perfect addition to every fur babies summer collection! With its vibrant watercolor popsicle pattern, your pup will be ready for every summer cookout! </span><em id="A8.T17.5.4.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">With snaps for security, this bandana is made with love, down to the very last stitch !
Fabric: cotton
Care Instructions: Hand wash only, iron as needed, on low heat
Always supervise your pup while wearing Faithful Paws Co. accessories, as it could become a choking hazard if consumed.</em></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.01114" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.01116" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2306.01116">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.01116" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.01117" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 01:55:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
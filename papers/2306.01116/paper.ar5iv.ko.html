<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.01116] The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only</title><meta property="og:description" content="Large language models are commonly trained on a mixture of filtered web data and curated “high-quality” corpora, such as social media conversations, books, or technical papers. This curation process is believed to be n…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.01116">

<!--Generated on Thu Feb 29 01:55:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="deduplication,  NLP,  LLM,  curated,  data,  crawl,  Falcon LLM">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">The RefinedWeb Dataset for Falcon LLM:
<br class="ltx_break">Outperforming Curated Corpora with Web Data, and Web Data Only</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guilherme Penedo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Quentin Malartic
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Hesslow
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruxandra Cojocaru
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessandro Cappelli
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hamza Alobeidli
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Baptiste Pannier
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ebtesam Almazrouei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julien Launay
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">대형 언어 모델은 일반적으로 필터링된 웹 데이터와 소셜 미디어 대화, 책 또는 기술 문서와 같은 선별된 "고품질" 코퍼라의 혼합물에서 훈련된다. 이러한 큐레이션 과정은 광범위한 제로샷 일반화 능력을 가진 성능 모델을 생산하는 데 필요할 것으로 판단된다. 그러나 수조 개의 토큰에 대한 사전 훈련이 필요한 더 큰 모델이 고려됨에 따라 확장성이 얼마나 있는지, 그리고 곧 고유한 고품질 데이터가 고갈될지 여부는 불분명하다. 이전 믿음과 달리, 우리는 적절하게 필터링되고 중복 제거된 웹 데이터만으로도 강력한 모델로 이어질 수 있음을 보여주며, 심지어 더 파일에서 훈련된 최첨단 모델의 성능을 상당히 능가한다. 광범위한 필터링에도 불구하고 웹에서 추출한 고품질 데이터는 여전히 풍부하며 커먼크롤에서 5조 개의 토큰을 얻을 수 있다. 우리는 <span class="ltx_text ltx_font_smallcaps" id="id1.id1.1">RefinedWeb</span> 데이터 세트 및 이에 대해 훈련된 1.3/7.5B 매개 변수 언어 모델 <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>Details about how to access Falcon LLM open source is available on <a class="ltx_ref ltx_url ltx_font_typewriter" href="falconllm.tii.ae" title="">falconllm.tii.ae</a></span></span></span>에서 6천억 토큰의 추출물을 공개적으로 공개합니다.</p>
</div>
<div class="ltx_keywords">deduplication, NLP, LLM, curated, data, crawl, Falcon LLM
</div>
<div id="p1" class="ltx_para">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="p1.1.1">The Falcon LLM team</span> <br class="ltx_break"/> <br class="ltx_break"/> <br class="ltx_break"/></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p ltx_align_center"><a target="_blank" href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/tiiuae/falcon-refinedweb</a>

<br class="ltx_break"></p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span><span class="ltx_text ltx_font_bold" id="S0.F1.9.1">Models trained on <span class="ltx_text" id="S0.F1.9.1.1" style="color:#DB57B2;">●<span class="ltx_text ltx_font_smallcaps" id="S0.F1.9.1.1.1">RefinedWeb</span></span> alone outperform models trained on curated corpora. </span> Zero-shot performance on our <span class="ltx_text ltx_font_typewriter" id="S0.F1.10.2">main-agg</span> task aggregate (자세한 내용은 <a class="ltx_ref" href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> 참조) 등가의 컴퓨팅 예산에서 우리 모델은 <math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S0.F1.3.m1.1"><semantics id="S0.F1.3.m1.1b"><mi id="S0.F1.3.m1.1.1" mathcolor="#7DD86E" mathvariant="normal" xref="S0.F1.3.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S0.F1.3.m1.1c"><ci id="S0.F1.3.m1.1.1.cmml" xref="S0.F1.3.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.F1.3.m1.1d">\blacktriangledown</annotation></semantics></math><span class="ltx_text" id="S0.F1.11.3" 스타일="color:#7DD86E;"> The Pile</span>에서 훈련된 공개적으로 사용 가능한 모델을 크게 능가하며, 평가 설정 내에서 테스트할 때 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="S0.F1.4.m2.1"><semantics id="S0.F1.4.m2.1b"><mi id="S0.F1.4.m2.1.1" mathcolor="#5F57DB" mathvariant="normal" xref="S0.F1.4.m2.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="S0.F1.4.m2.1c"><ci id="S0.F1.4.m2.1.1.cmml" xref="S0.F1.4.m2.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.F1.4.m2.1d">\blacksquare</annotation></semantics></math><span class="ltx_text" 스타일="color:#5F57DB;">GPT-3</span> 모델의 성능과 일치합니다.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S0.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1:</span><span class="ltx_text ltx_font_bold" id="S0.T1.23.1" style="color:#DB57B2;">●<span class="ltx_text ltx_font_smallcaps" id="S0.T1.23.1.1">RefinedWeb</span><span class="ltx_text" id="S0.T1.23.1.2" style="color:#000000;"> improves on existing English pretraining datasets for large language models by combining extensive filtering with stringent deduplication at unprecedented scale. </span></span> 자세한 내용은 <a class="ltx_ref" href="#A6.SS3" title="F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">F.3</span></a>의 <a class="ltx_ref" href="#A6.T12" title="In F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">12</span></a>의 풀 버전을 참조하세요.</figcaption>
<table id="S0.T1.21" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S0.T1.21.22" class="ltx_tr">
<td id="S0.T1.21.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S0.T1.21.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.22.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.21.22.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></span>
</span>
</td>
<td id="S0.T1.21.22.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.21.22.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Size</span></td>
<td id="S0.T1.21.22.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.21.22.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Availability</span></td>
<td id="S0.T1.21.22.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.21.22.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Web</span></td>
<td id="S0.T1.21.22.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S0.T1.21.22.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.22.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.21.22.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CC Processing</span></span>
</span>
</td>
<td id="S0.T1.21.22.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S0.T1.21.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.22.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.21.22.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Deduplication</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.21.23" class="ltx_tr">
<td id="S0.T1.21.23.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="6"><span id="S0.T1.21.23.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Massive web datasets</span></td>
</tr>
<tr id="S0.T1.2.2" class="ltx_tr">
<td id="S0.T1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.2.2.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">C4</span></span>
</span>
</td>
<td id="S0.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.1.1.1.m1.1" class="ltx_Math" alttext="\sim 360" display="inline"><semantics id="S0.T1.1.1.1.m1.1a"><mrow id="S0.T1.1.1.1.m1.1.1" xref="S0.T1.1.1.1.m1.1.1.cmml"><mi id="S0.T1.1.1.1.m1.1.1.2" xref="S0.T1.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.1.1.1.m1.1.1.1" xref="S0.T1.1.1.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.1.1.1.m1.1.1.3" xref="S0.T1.1.1.1.m1.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.1.1.1.m1.1b"><apply id="S0.T1.1.1.1.m1.1.1.cmml" xref="S0.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.1.1.1.m1.1.1.1.cmml" xref="S0.T1.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.1.1.1.m1.1.1.2.cmml" xref="S0.T1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.1.1.1.m1.1.1.3.cmml" xref="S0.T1.1.1.1.m1.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.1.1.1.m1.1c">\sim 360</annotation></semantics></math><span id="S0.T1.1.1.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S0.T1.2.2.4.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.2.2.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S0.T1.2.2.2.m1.1a"><mn mathsize="90%" id="S0.T1.2.2.2.m1.1.1" xref="S0.T1.2.2.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S0.T1.2.2.2.m1.1b"><cn type="integer" id="S0.T1.2.2.2.m1.1.1.cmml" xref="S0.T1.2.2.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.2.2.2.m1.1c">100</annotation></semantics></math><span id="S0.T1.2.2.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.2.2.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.2.2.5.1.1.1" class="ltx_text" style="font-size:90%;">Rules + NSFW words blocklist</span></span>
</span>
</td>
<td id="S0.T1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.2.2.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.2.2.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact:</span><span id="S0.T1.2.2.6.1.1.2" class="ltx_text" style="font-size:90%;"> spans of 3 sentences</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.5.5" class="ltx_tr">
<td id="S0.T1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.5.5.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR-21.09</span></span>
</span>
</td>
<td id="S0.T1.3.3.1" class="ltx_td ltx_align_center">
<math id="S0.T1.3.3.1.m1.1" class="ltx_Math" alttext="\sim 370" display="inline"><semantics id="S0.T1.3.3.1.m1.1a"><mrow id="S0.T1.3.3.1.m1.1.1" xref="S0.T1.3.3.1.m1.1.1.cmml"><mi id="S0.T1.3.3.1.m1.1.1.2" xref="S0.T1.3.3.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.3.3.1.m1.1.1.1" xref="S0.T1.3.3.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.3.3.1.m1.1.1.3" xref="S0.T1.3.3.1.m1.1.1.3.cmml">370</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.3.3.1.m1.1b"><apply id="S0.T1.3.3.1.m1.1.1.cmml" xref="S0.T1.3.3.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.3.3.1.m1.1.1.1.cmml" xref="S0.T1.3.3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.3.3.1.m1.1.1.2.cmml" xref="S0.T1.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.3.3.1.m1.1.1.3.cmml" xref="S0.T1.3.3.1.m1.1.1.3">370</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.3.3.1.m1.1c">\sim 370</annotation></semantics></math><span id="S0.T1.3.3.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.5.5.5" class="ltx_td ltx_align_center"><span id="S0.T1.5.5.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.4.4.2" class="ltx_td ltx_align_center">
<math id="S0.T1.4.4.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S0.T1.4.4.2.m1.1a"><mn mathsize="90%" id="S0.T1.4.4.2.m1.1.1" xref="S0.T1.4.4.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S0.T1.4.4.2.m1.1b"><cn type="integer" id="S0.T1.4.4.2.m1.1.1.cmml" xref="S0.T1.4.4.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.4.4.2.m1.1c">100</annotation></semantics></math><span id="S0.T1.4.4.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.5.5.6.1.1.1" class="ltx_text" style="font-size:90%;">Built at the line-level</span></span>
</span>
</td>
<td id="S0.T1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.5.5.3.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.5.5.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="S0.T1.5.5.3.1.1.2" class="ltx_text" style="font-size:90%;">: per line (</span><math id="S0.T1.5.5.3.1.1.m1.1" class="ltx_Math" alttext="\sim 55\%" display="inline"><semantics id="S0.T1.5.5.3.1.1.m1.1a"><mrow id="S0.T1.5.5.3.1.1.m1.1.1" xref="S0.T1.5.5.3.1.1.m1.1.1.cmml"><mi id="S0.T1.5.5.3.1.1.m1.1.1.2" xref="S0.T1.5.5.3.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.5.5.3.1.1.m1.1.1.1" xref="S0.T1.5.5.3.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.5.5.3.1.1.m1.1.1.3" xref="S0.T1.5.5.3.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.5.5.3.1.1.m1.1.1.3.2" xref="S0.T1.5.5.3.1.1.m1.1.1.3.2.cmml">55</mn><mo mathsize="90%" id="S0.T1.5.5.3.1.1.m1.1.1.3.1" xref="S0.T1.5.5.3.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.5.5.3.1.1.m1.1b"><apply id="S0.T1.5.5.3.1.1.m1.1.1.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.5.5.3.1.1.m1.1.1.1.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.5.5.3.1.1.m1.1.1.2.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.5.5.3.1.1.m1.1.1.3.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.5.5.3.1.1.m1.1.1.3.1.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.5.5.3.1.1.m1.1.1.3.2.cmml" xref="S0.T1.5.5.3.1.1.m1.1.1.3.2">55</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.5.5.3.1.1.m1.1c">\sim 55\%</annotation></semantics></math><span id="S0.T1.5.5.3.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.7.7" class="ltx_tr">
<td id="S0.T1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.7.7.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.7.7.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR-22.01</span></span>
</span>
</td>
<td id="S0.T1.6.6.1" class="ltx_td ltx_align_center">
<math id="S0.T1.6.6.1.m1.1" class="ltx_Math" alttext="\sim 283" display="inline"><semantics id="S0.T1.6.6.1.m1.1a"><mrow id="S0.T1.6.6.1.m1.1.1" xref="S0.T1.6.6.1.m1.1.1.cmml"><mi id="S0.T1.6.6.1.m1.1.1.2" xref="S0.T1.6.6.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.6.6.1.m1.1.1.1" xref="S0.T1.6.6.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.6.6.1.m1.1.1.3" xref="S0.T1.6.6.1.m1.1.1.3.cmml">283</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.6.6.1.m1.1b"><apply id="S0.T1.6.6.1.m1.1.1.cmml" xref="S0.T1.6.6.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.6.6.1.m1.1.1.1.cmml" xref="S0.T1.6.6.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.6.6.1.m1.1.1.2.cmml" xref="S0.T1.6.6.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.6.6.1.m1.1.1.3.cmml" xref="S0.T1.6.6.1.m1.1.1.3">283</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.6.6.1.m1.1c">\sim 283</annotation></semantics></math><span id="S0.T1.6.6.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.7.7.4" class="ltx_td ltx_align_center"><span id="S0.T1.7.7.4.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.7.7.2" class="ltx_td ltx_align_center">
<math id="S0.T1.7.7.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S0.T1.7.7.2.m1.1a"><mn mathsize="90%" id="S0.T1.7.7.2.m1.1.1" xref="S0.T1.7.7.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S0.T1.7.7.2.m1.1b"><cn type="integer" id="S0.T1.7.7.2.m1.1.1.cmml" xref="S0.T1.7.7.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.7.7.2.m1.1c">100</annotation></semantics></math><span id="S0.T1.7.7.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.7.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.7.7.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.7.7.5.1.1.1" class="ltx_text" style="font-size:90%;">Line-level rules + optional rules &amp; NSFW URL blocklist</span></span>
</span>
</td>
<td id="S0.T1.7.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.7.7.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.7.7.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="S0.T1.7.7.6.1.1.2" class="ltx_text" style="font-size:90%;">: per line (optional, not used for results in this paper)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.21.24" class="ltx_tr">
<td id="S0.T1.21.24.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="6"><span id="S0.T1.21.24.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Curated datasets</span></td>
</tr>
<tr id="S0.T1.11.11" class="ltx_tr">
<td id="S0.T1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.8.8.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="S0.T1.8.8.1.1.1.m1.1" class="ltx_Math" alttext="\blacksquare" display="inline"><semantics id="S0.T1.8.8.1.1.1.m1.1a"><mi mathcolor="#5F57DB" mathsize="90%" mathvariant="normal" id="S0.T1.8.8.1.1.1.m1.1.1" xref="S0.T1.8.8.1.1.1.m1.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="S0.T1.8.8.1.1.1.m1.1b"><ci id="S0.T1.8.8.1.1.1.m1.1.1.cmml" xref="S0.T1.8.8.1.1.1.m1.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.8.8.1.1.1.m1.1c">\blacksquare</annotation></semantics></math><span id="S0.T1.8.8.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#5F57DB;"> GPT-3</span></span>
</span>
</td>
<td id="S0.T1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.9.9.2.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S0.T1.9.9.2.m1.1a"><mn mathsize="90%" id="S0.T1.9.9.2.m1.1.1" xref="S0.T1.9.9.2.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S0.T1.9.9.2.m1.1b"><cn type="integer" id="S0.T1.9.9.2.m1.1.1.cmml" xref="S0.T1.9.9.2.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.9.9.2.m1.1c">300</annotation></semantics></math><span id="S0.T1.9.9.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.11.11.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S0.T1.11.11.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="S0.T1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">
<math id="S0.T1.10.10.3.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S0.T1.10.10.3.m1.1a"><mn mathsize="90%" id="S0.T1.10.10.3.m1.1.1" xref="S0.T1.10.10.3.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S0.T1.10.10.3.m1.1b"><cn type="integer" id="S0.T1.10.10.3.m1.1.1.cmml" xref="S0.T1.10.10.3.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.10.10.3.m1.1c">60</annotation></semantics></math><span id="S0.T1.10.10.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.11.11.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.11.11.6.1.1.1" class="ltx_text" style="font-size:90%;">Content filter trained on known high-quality sources</span></span>
</span>
</td>
<td id="S0.T1.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S0.T1.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.11.11.4.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.11.11.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="S0.T1.11.11.4.1.1.2" class="ltx_text" style="font-size:90%;">: MinHash (</span><math id="S0.T1.11.11.4.1.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="S0.T1.11.11.4.1.1.m1.1a"><mrow id="S0.T1.11.11.4.1.1.m1.1.1" xref="S0.T1.11.11.4.1.1.m1.1.1.cmml"><mi id="S0.T1.11.11.4.1.1.m1.1.1.2" xref="S0.T1.11.11.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.11.11.4.1.1.m1.1.1.1" xref="S0.T1.11.11.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.11.11.4.1.1.m1.1.1.3" xref="S0.T1.11.11.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.11.11.4.1.1.m1.1.1.3.2" xref="S0.T1.11.11.4.1.1.m1.1.1.3.2.cmml">10</mn><mo mathsize="90%" id="S0.T1.11.11.4.1.1.m1.1.1.3.1" xref="S0.T1.11.11.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.11.11.4.1.1.m1.1b"><apply id="S0.T1.11.11.4.1.1.m1.1.1.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.11.11.4.1.1.m1.1.1.1.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.11.11.4.1.1.m1.1.1.2.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.11.11.4.1.1.m1.1.1.3.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.11.11.4.1.1.m1.1.1.3.1.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.11.11.4.1.1.m1.1.1.3.2.cmml" xref="S0.T1.11.11.4.1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.11.11.4.1.1.m1.1c">\sim 10\%</annotation></semantics></math><span id="S0.T1.11.11.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.15.15" class="ltx_tr">
<td id="S0.T1.12.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.12.12.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="S0.T1.12.12.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S0.T1.12.12.1.1.1.m1.1a"><mi mathcolor="#7DD86E" mathsize="90%" mathvariant="normal" id="S0.T1.12.12.1.1.1.m1.1.1" xref="S0.T1.12.12.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S0.T1.12.12.1.1.1.m1.1b"><ci id="S0.T1.12.12.1.1.1.m1.1.1.cmml" xref="S0.T1.12.12.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.12.12.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S0.T1.12.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#7DD86E;"> The Pile</span></span>
</span>
</td>
<td id="S0.T1.13.13.2" class="ltx_td ltx_align_center">
<math id="S0.T1.13.13.2.m1.1" class="ltx_Math" alttext="\sim 340" display="inline"><semantics id="S0.T1.13.13.2.m1.1a"><mrow id="S0.T1.13.13.2.m1.1.1" xref="S0.T1.13.13.2.m1.1.1.cmml"><mi id="S0.T1.13.13.2.m1.1.1.2" xref="S0.T1.13.13.2.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.13.13.2.m1.1.1.1" xref="S0.T1.13.13.2.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="S0.T1.13.13.2.m1.1.1.3" xref="S0.T1.13.13.2.m1.1.1.3.cmml">340</mn></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.13.13.2.m1.1b"><apply id="S0.T1.13.13.2.m1.1.1.cmml" xref="S0.T1.13.13.2.m1.1.1"><csymbol cd="latexml" id="S0.T1.13.13.2.m1.1.1.1.cmml" xref="S0.T1.13.13.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.13.13.2.m1.1.1.2.cmml" xref="S0.T1.13.13.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S0.T1.13.13.2.m1.1.1.3.cmml" xref="S0.T1.13.13.2.m1.1.1.3">340</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.13.13.2.m1.1c">\sim 340</annotation></semantics></math><span id="S0.T1.13.13.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.15.15.5" class="ltx_td ltx_align_center"><span id="S0.T1.15.15.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="S0.T1.14.14.3" class="ltx_td ltx_align_center">
<math id="S0.T1.14.14.3.m1.1" class="ltx_Math" alttext="18" display="inline"><semantics id="S0.T1.14.14.3.m1.1a"><mn mathsize="90%" id="S0.T1.14.14.3.m1.1.1" xref="S0.T1.14.14.3.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S0.T1.14.14.3.m1.1b"><cn type="integer" id="S0.T1.14.14.3.m1.1.1.cmml" xref="S0.T1.14.14.3.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.14.14.3.m1.1c">18</annotation></semantics></math><span id="S0.T1.14.14.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.15.15.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.15.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.15.15.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.15.15.6.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jusText</span><span id="S0.T1.15.15.6.1.1.2" class="ltx_text" style="font-size:90%;"> for extraction, content filter trained on curated data</span></span>
</span>
</td>
<td id="S0.T1.15.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.15.15.4.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.15.15.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="S0.T1.15.15.4.1.1.2" class="ltx_text" style="font-size:90%;">: MinHash (</span><math id="S0.T1.15.15.4.1.1.m1.1" class="ltx_Math" alttext="\sim 26\%" display="inline"><semantics id="S0.T1.15.15.4.1.1.m1.1a"><mrow id="S0.T1.15.15.4.1.1.m1.1.1" xref="S0.T1.15.15.4.1.1.m1.1.1.cmml"><mi id="S0.T1.15.15.4.1.1.m1.1.1.2" xref="S0.T1.15.15.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.15.15.4.1.1.m1.1.1.1" xref="S0.T1.15.15.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.15.15.4.1.1.m1.1.1.3" xref="S0.T1.15.15.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.15.15.4.1.1.m1.1.1.3.2" xref="S0.T1.15.15.4.1.1.m1.1.1.3.2.cmml">26</mn><mo mathsize="90%" id="S0.T1.15.15.4.1.1.m1.1.1.3.1" xref="S0.T1.15.15.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.15.15.4.1.1.m1.1b"><apply id="S0.T1.15.15.4.1.1.m1.1.1.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.15.15.4.1.1.m1.1.1.1.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.15.15.4.1.1.m1.1.1.2.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.15.15.4.1.1.m1.1.1.3.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.15.15.4.1.1.m1.1.1.3.1.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.15.15.4.1.1.m1.1.1.3.2.cmml" xref="S0.T1.15.15.4.1.1.m1.1.1.3.2">26</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.15.15.4.1.1.m1.1c">\sim 26\%</annotation></semantics></math><span id="S0.T1.15.15.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.18.18" class="ltx_tr">
<td id="S0.T1.16.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.16.16.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="S0.T1.16.16.1.1.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S0.T1.16.16.1.1.1.m1.1a"><mi mathcolor="#DB5F56" mathsize="90%" mathvariant="normal" id="S0.T1.16.16.1.1.1.m1.1.1" xref="S0.T1.16.16.1.1.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S0.T1.16.16.1.1.1.m1.1b"><ci id="S0.T1.16.16.1.1.1.m1.1.1.cmml" xref="S0.T1.16.16.1.1.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.16.16.1.1.1.m1.1c">\bigstar</annotation></semantics></math><span id="S0.T1.16.16.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#DB5F56;"> PaLM</span></span>
</span>
</td>
<td id="S0.T1.17.17.2" class="ltx_td ltx_align_center">
<math id="S0.T1.17.17.2.m1.1" class="ltx_Math" alttext="780" display="inline"><semantics id="S0.T1.17.17.2.m1.1a"><mn mathsize="90%" id="S0.T1.17.17.2.m1.1.1" xref="S0.T1.17.17.2.m1.1.1.cmml">780</mn><annotation-xml encoding="MathML-Content" id="S0.T1.17.17.2.m1.1b"><cn type="integer" id="S0.T1.17.17.2.m1.1.1.cmml" xref="S0.T1.17.17.2.m1.1.1">780</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.17.17.2.m1.1c">780</annotation></semantics></math><span id="S0.T1.17.17.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.18.18.4" class="ltx_td ltx_align_center"><span id="S0.T1.18.18.4.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="S0.T1.18.18.3" class="ltx_td ltx_align_center">
<math id="S0.T1.18.18.3.m1.1" class="ltx_Math" alttext="27" display="inline"><semantics id="S0.T1.18.18.3.m1.1a"><mn mathsize="90%" id="S0.T1.18.18.3.m1.1.1" xref="S0.T1.18.18.3.m1.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="S0.T1.18.18.3.m1.1b"><cn type="integer" id="S0.T1.18.18.3.m1.1.1.cmml" xref="S0.T1.18.18.3.m1.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.18.18.3.m1.1c">27</annotation></semantics></math><span id="S0.T1.18.18.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S0.T1.18.18.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.18.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.18.18.5.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.18.18.5.1.1.1" class="ltx_text" style="font-size:90%;">Filter trained on HQ data</span></span>
</span>
</td>
<td id="S0.T1.18.18.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S0.T1.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.18.18.6.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.18.18.6.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
</tr>
<tr id="S0.T1.21.25" class="ltx_tr">
<td id="S0.T1.21.25.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="6"><span id="S0.T1.21.25.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Ours</span></td>
</tr>
<tr id="S0.T1.21.21" class="ltx_tr">
<td id="S0.T1.21.21.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S0.T1.21.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.21.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S0.T1.21.21.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#DB57B2;">●<span id="S0.T1.21.21.4.1.1.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span></span>
</span>
</td>
<td id="S0.T1.19.19.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S0.T1.19.19.1.m1.2" class="ltx_Math" alttext="\sim 5,000" display="inline"><semantics id="S0.T1.19.19.1.m1.2a"><mrow id="S0.T1.19.19.1.m1.2.3" xref="S0.T1.19.19.1.m1.2.3.cmml"><mi id="S0.T1.19.19.1.m1.2.3.2" xref="S0.T1.19.19.1.m1.2.3.2.cmml"></mi><mo mathsize="90%" id="S0.T1.19.19.1.m1.2.3.1" xref="S0.T1.19.19.1.m1.2.3.1.cmml">∼</mo><mrow id="S0.T1.19.19.1.m1.2.3.3.2" xref="S0.T1.19.19.1.m1.2.3.3.1.cmml"><mn mathsize="90%" id="S0.T1.19.19.1.m1.1.1" xref="S0.T1.19.19.1.m1.1.1.cmml">5</mn><mo mathsize="90%" id="S0.T1.19.19.1.m1.2.3.3.2.1" xref="S0.T1.19.19.1.m1.2.3.3.1.cmml">,</mo><mn mathsize="90%" id="S0.T1.19.19.1.m1.2.2" xref="S0.T1.19.19.1.m1.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.19.19.1.m1.2b"><apply id="S0.T1.19.19.1.m1.2.3.cmml" xref="S0.T1.19.19.1.m1.2.3"><csymbol cd="latexml" id="S0.T1.19.19.1.m1.2.3.1.cmml" xref="S0.T1.19.19.1.m1.2.3.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.19.19.1.m1.2.3.2.cmml" xref="S0.T1.19.19.1.m1.2.3.2">absent</csymbol><list id="S0.T1.19.19.1.m1.2.3.3.1.cmml" xref="S0.T1.19.19.1.m1.2.3.3.2"><cn type="integer" id="S0.T1.19.19.1.m1.1.1.cmml" xref="S0.T1.19.19.1.m1.1.1">5</cn><cn type="integer" id="S0.T1.19.19.1.m1.2.2.cmml" xref="S0.T1.19.19.1.m1.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.19.19.1.m1.2c">\sim 5,000</annotation></semantics></math><span id="S0.T1.19.19.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="S0.T1.21.21.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S0.T1.21.21.5.1" class="ltx_text" style="font-size:90%;">Public (600GT)</span></td>
<td id="S0.T1.20.20.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math id="S0.T1.20.20.2.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S0.T1.20.20.2.m1.1a"><mrow id="S0.T1.20.20.2.m1.1.1" xref="S0.T1.20.20.2.m1.1.1.cmml"><mn mathsize="90%" id="S0.T1.20.20.2.m1.1.1.2" xref="S0.T1.20.20.2.m1.1.1.2.cmml">100</mn><mo mathsize="90%" id="S0.T1.20.20.2.m1.1.1.1" xref="S0.T1.20.20.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.20.20.2.m1.1b"><apply id="S0.T1.20.20.2.m1.1.1.cmml" xref="S0.T1.20.20.2.m1.1.1"><csymbol cd="latexml" id="S0.T1.20.20.2.m1.1.1.1.cmml" xref="S0.T1.20.20.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S0.T1.20.20.2.m1.1.1.2.cmml" xref="S0.T1.20.20.2.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.20.20.2.m1.1c">100\%</annotation></semantics></math></td>
<td id="S0.T1.21.21.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S0.T1.21.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.21.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S0.T1.21.21.6.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">trafilatura</span><span id="S0.T1.21.21.6.1.1.2" class="ltx_text" style="font-size:90%;"> for text extraction, document and line-level rules, NSFW URL blocklist</span></span>
</span>
</td>
<td id="S0.T1.21.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S0.T1.21.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.T1.21.21.3.1.1" class="ltx_p" style="width:128.0pt;"><span id="S0.T1.21.21.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact &amp; fuzzy</span><span id="S0.T1.21.21.3.1.1.2" class="ltx_text" style="font-size:90%;">: exact substring+MinHash (</span><math id="S0.T1.21.21.3.1.1.m1.1" class="ltx_Math" alttext="\sim 50\%" display="inline"><semantics id="S0.T1.21.21.3.1.1.m1.1a"><mrow id="S0.T1.21.21.3.1.1.m1.1.1" xref="S0.T1.21.21.3.1.1.m1.1.1.cmml"><mi id="S0.T1.21.21.3.1.1.m1.1.1.2" xref="S0.T1.21.21.3.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="S0.T1.21.21.3.1.1.m1.1.1.1" xref="S0.T1.21.21.3.1.1.m1.1.1.1.cmml">∼</mo><mrow id="S0.T1.21.21.3.1.1.m1.1.1.3" xref="S0.T1.21.21.3.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S0.T1.21.21.3.1.1.m1.1.1.3.2" xref="S0.T1.21.21.3.1.1.m1.1.1.3.2.cmml">50</mn><mo mathsize="90%" id="S0.T1.21.21.3.1.1.m1.1.1.3.1" xref="S0.T1.21.21.3.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S0.T1.21.21.3.1.1.m1.1b"><apply id="S0.T1.21.21.3.1.1.m1.1.1.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1"><csymbol cd="latexml" id="S0.T1.21.21.3.1.1.m1.1.1.1.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S0.T1.21.21.3.1.1.m1.1.1.2.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.2">absent</csymbol><apply id="S0.T1.21.21.3.1.1.m1.1.1.3.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.3"><csymbol cd="latexml" id="S0.T1.21.21.3.1.1.m1.1.1.3.1.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S0.T1.21.21.3.1.1.m1.1.1.3.2.cmml" xref="S0.T1.21.21.3.1.1.m1.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S0.T1.21.21.3.1.1.m1.1c">\sim 50\%</annotation></semantics></math><span id="S0.T1.21.21.3.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">자연어 처리의 진전은 점점 더 순수한 컴퓨팅 규모만으로 추진되고 있다. <cite class="ltx_cite ltx_citemacro_citep">(Sevilla et al., <a class="ltx_ref" href="#bib.bib68" title="">2022</a>)</cite>: 더 많은 컴퓨팅이 대규모 언어 모델(LLM)을 훈련하기 위해 확장됨에 따라, 그들은 강력한 창발 능력 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Wei et al., <a class="ltx_ref" href="#bib.bib78" title="">2022</a>)</cite>를 얻고 나타낸다. 스케일링의 이점을 가장 잘 얻기 위해, 최근의 스케일링 법칙은 모델 크기와 데이터세트 크기 모두를 공동으로 증가시켜야 한다고 지시한다<cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>. 이는 스케일링이 최소 데이터 스케일링 <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="#bib.bib46" title="">2020</a>)</cite>로 모델 크기에 우선 초점을 맞춰야 한다고 주장했던 이전 연구 결과와 차이가 있다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">이 공동 스케일링 패러다임은 상당한 문제를 제기한다: 풍부하지만 텍스트 데이터는 무한하지 않으며, 특히 데이터 품질 및 라이선스에 대한 고려 사항이 고려될 때 일부 연구자들은 스케일링이 데이터 가용성 <cite class="ltx_cite ltx_citemacro_citep">(Villalobos et al., <a class="ltx_ref" href="#bib.bib75" title="">2022</a>)</cite>에 의해 곧 병목될 수 있다고 주장한다. 구체적으로, GPT-3 크기의 모델(175B 파라미터)을 최적으로 훈련시키는 것은 <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et al. (<a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>에 따라 3,500억 개 이상의 텍스트 토큰을 필요로 할 것이다. 이는 지금까지 입증된 가장 큰 사전 훈련 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="#bib.bib41" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>의 두 배이며, OSCAR<cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>)</cite>, C4<cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>, 또는 The Pile<cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>와 같이 공개적으로 사용 가능한 가장 큰 영어 데이터 세트보다 10배 더 많다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">대규모 스케일업 사전 훈련 데이터는 LLM이 웹 크롤과 소위 "고품질" 데이터<cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>의 혼합물을 사용하여 일반적으로 훈련된다는 사실에 의해 훨씬 더 어렵게 만들어진다. 전형적인 고품질 코퍼라는 책, 기술 문서, 사람이 선택한 웹 페이지 또는 소셜 미디어 대화의 선별된 소스를 포함한다. 이러한 선별된 코퍼스에 의해 야기되는 증가된 다양성과 품질은 수행 모델 <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>의 핵심 구성 요소로 여겨진다. 불행히도 큐레이션은 노동 집약적이며 일반적으로 각 소스에는 제한된 양의 데이터를 생성하면서 전문화된 처리가 필요하다. 게다가, 허가받은 소식통들은 법적 문제를 제기한다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">그럼에도 불구하고 대부분의 사전 훈련 데이터는 제한된 인간 개입으로 수조 개의 토큰으로 확장될 수 있는 대규모 웹 크롤링에서 여전히 조달된다. 그러나, 이 데이터의 품질은 전통적으로 수동으로 선별된 데이터 소스의 품질보다 (훨씬) 열등한 것으로 간주되어 왔다. C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite> 또는 OSCAR <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>)</cite>와 같이 웹 데이터의 세밀하게 처리된 소스조차도 LLMs <cite class="ltx_cite ltx_citemacro_citep">(Rae et al., <a class="ltx_ref" href="#bib.bib63" title="">2021</a>; Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>에 대한 큐레이트된 코퍼라보다 열등한 것으로 간주되어 성능이 낮은 모델을 생성한다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">지속적으로 증가하는 LLM의 데이터 요구를 지속시키고, 데이터 파이프라인을 능률화하고 인간 집약적인 큐레이션의 필요성을 줄이기 위해, 우리는 웹 데이터가 큐레이션된 코퍼라에서 훈련된 모델보다 품질이 크게 향상되도록 더 잘 처리될 수 있고, 그 결과 모델이 더 능력이 있는 것으로 나타날 수 있는 방법을 탐구할 것을 제안한다.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Contributions.</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">다음과 같은 기여를 합니다.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S1.I1.i1.p1.1.1" style="color:#DB57B2;">RefinedWeb</span>, 고품질 5조 토큰 웹 전용 영어 프리트레이닝 데이터셋을 소개합니다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">web data alone can result in models outperforming both public and private curated corpora</span>, as captured by zero-shot benchmarks, challenging current views about data quality;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">RefinedWeb의 600B 토큰 추출물과 이에 훈련된 1/7B 매개 변수 LLMs</span>을 공개적으로 공개하여 자연어 처리 커뮤니티를 위한 새로운 베이스라인 고품질 웹 데이터 세트 역할을 한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pretraining data for large language models.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">초기 대규모 언어 모델은 길고 일관성 있는 문서 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="#bib.bib61" title="">2018</a>; Devlin et al., <a class="ltx_ref" href="#bib.bib29" title="">2019</a>)</cite>를 가진 데이터 세트의 중요성을 확인했다. 이전에 사용된 문장별 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Chelba et al., <a class="ltx_ref" href="#bib.bib22" title="">2013</a>)</cite>에서 넘어가는 대신 문서 중심의 단일 도메인 말뭉치인 위키피디아 또는 북코퍼스 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib88" title="">2015</a>)</cite>를 활용했다. 모델의 규모가 증가함에 따라 대규모 웹 스크레이프를 기반으로 한 데이터 세트는 <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>; Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>의 보급률을 얻었다. 그러나 추가 연구에서는 이러한 비표적 웹 스크랩이 인간 큐레이트 데이터 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="#bib.bib62" title="">2019</a>)</cite>에 미치지 못하여 웹 데이터를 책, 기술 기사 및 소셜 미디어 대화와 결합하는 더 파일 <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>와 같은 큐레이트 데이터 세트가 광범위하게 채택되었다고 주장했다. 규모에서 약한 신호를 활용하여 인간 큐레이션 프로세스를 모방하는 것이 제안되었는데, 예를 들어 포럼의 상단 링크를 크롤링하여 <cite class="ltx_cite ltx_citemacro_citep">(Gokaslan et al., <a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite>이다. 표적화된 말뭉치는 또한 도메인-특정 모델 <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et al., <a class="ltx_ref" href="#bib.bib12" title="">2019</a>)</cite>를 생성할 수 있거나, 모델의 표현성을 넓힐 수 있다(예를 들어, 대화 모달리티 <cite class="ltx_cite ltx_citemacro_citet">Adiwardana et al. (<a class="ltx_ref" href="#bib.bib4" title="">2020</a>); Thoppilan et al. (<a class="ltx_ref" href="#bib.bib71" title="">2022</a>)</cite>의 경우). 최신 대형 언어 모델 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Rae et al., <a class="ltx_ref" href="#bib.bib63" title="">2021</a>; Chowdhery et al., <a class="ltx_ref" href="#bib.bib23" title="">2022</a>; Scao et al., <a class="ltx_ref" href="#bib.bib66" title="">2022a</a>)</cite>는 대규모 웹 스크래핑과 소위 "고품질" 큐레이팅된 단일 도메인 소스(예: 뉴스, 책, 기술 문서, 소셜 미디어 대화)를 모두 결합하여 거대한 집합 말뭉치에 대해 훈련된다. 이러한 표적 출처는 종종 업샘플링되는데, 이는 최종 데이터 세트에서 대표성을 높이는 데 가장 일반적이다. 이러한 집계된 데이터 세트에 의해 네 번째로 가져온 다양성과 "고품질"은 모델 품질의 중심인 것으로 생각되며 웹 데이터만으로는 강력한 대규모 언어 모델 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib51" title="">2019</a>; Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>를 훈련하기에 불충분한 것으로 간주된다.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pipelines for web data.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">대용량 웹 데이터 세트는 일반적으로 커먼크롤(CommonCrawl)을 기반으로 하며, 이는 현재 12년 동안 실행되었으며 페타바이트의 데이터를 수집했습니다. 인터넷 전체에서 스크래핑된 데이터로 작업하는 것은 독특한 과제를 제시합니다. 특히, 상당한 부분이 저품질 기계 생성 스팸 또는 음란 콘텐츠 <cite class="ltx_cite ltx_citemacro_citep">(Trinh &amp; Le, <a class="ltx_ref" href="#bib.bib73" title="">2018</a>; Kreutzer et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>)</cite>입니다. 따라서 필터링되지 않은 웹 데이터에 대한 훈련은 바람직하지 않으며 결과적으로 모델<cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>를 제대로 수행하지 못한다. 현대 파이프라인은 이 바람직하지 않은 콘텐츠 <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et al., <a class="ltx_ref" href="#bib.bib81" title="">2020</a>)</cite>를 필터링하는 데 중점을 둡니다. 일반적으로 이러한 파이프라인은 다양한 단계를 결합합니다. (1) <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">language identification</em>, leveraging inexpensive n-gram 모델(예: fastText <cite class="ltx_cite ltx_citemacro_citet">Joulin et al. (<a class="ltx_ref" href="#bib.bib45" title="">2016</a>)</cite>); (2) <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.2">filtering rules and heuristics</em>, such only keeping line with valid punctuation, discarding lines with too many symbols or removing documents containing banned word <cite class="ltx_cite ltx_citemacro_citep">(Grave et al., <a class="ltx_ref" href="#bib.bib38" title="">2018</a>; Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>; (3) <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.3">ML-based quality filtering</em>, using lightweight models on known gold data to identify similar high-quality web documents <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et al., <a class="ltx_ref" href="#bib.bib81" title="">2020</a>; Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>)</cite>; ( 일부 필터링이 필요하지만 과도한 필터링은 모델에 바람직하지 않은 편향을 초래할 수 있다. 이는 소수자<cite class="ltx_cite ltx_citemacro_citep">(Dodge et al., <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>에 과도하게 영향을 미쳐 의사 크롤링과 같은 관행을 채택하도록 동기를 부여할 수 있으며, 허용되는 URL은 수동으로 큐레이팅된다<cite class="ltx_cite ltx_citemacro_citep">(Laurençon et al., <a class="ltx_ref" href="#bib.bib48" title="">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deduplication.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">중복 제거는 데이터 세트에서 반복되는 추출 및 문서를 제거합니다. 이는 모든 문자에서 동일하거나 일부 유사성 메트릭을 기반으로 하는 정확한 일치 또는 근사 일치일 수 있습니다. 정확한 중복의 경우, 접미사 배열 <cite class="ltx_cite ltx_citemacro_citep">(Manber &amp; Myers, <a class="ltx_ref" href="#bib.bib53" title="">1993</a>)</cite>를 사용하여 최소 길이의 정확한 부분 문자열을 일치시키는 것이 일반적이다. 퍼지 중복의 경우, 대규모 언어 모델 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Zeng et al., <a class="ltx_ref" href="#bib.bib85" title="">2021</a>; Rae et al., <a class="ltx_ref" href="#bib.bib63" title="">2021</a>)</cite>의 사전 훈련 데이터에 MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a class="ltx_ref" href="#bib.bib17" title="">1997</a>)</cite> 또는 SimHash <cite class="ltx_cite ltx_citemacro_citep">(Charikar, <a class="ltx_ref" href="#bib.bib21" title="">2002</a>)</cite>와 같은 국부적으로 민감한 해시를 기반으로 하는 방법이 채택되었다. 최근, <cite class="ltx_cite ltx_citemacro_citet">Abbas et al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>)</cite>는 근사 매칭 알고리즘에서 사전 학습된 모델에서 의미론적 이해로 임베딩을 활용하는 것을 제안했다. 중복제거는 언어 모델 <cite class="ltx_cite ltx_citemacro_citep">(Allamanis, <a class="ltx_ref" href="#bib.bib6" title="">2019</a>; Lee et al., <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>를 개선하는 데 중요한 역할을 하는 것으로 확인되었다. 특히 대형 모델 <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al., <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>에서 특히 문제가 되는 암기 <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al., <a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite>를 줄인다. 또한 반복 데이터는 매개변수 수가 증가함에 따라 모델 품질에 점점 더 해로운 것으로 나타났다. <cite class="ltx_cite ltx_citemacro_citep">(Hernandez et al., <a class="ltx_ref" href="#bib.bib40" title="">2022</a>)</cite>: 1B 매개변수 모델의 경우 100개의 중복이 유해하며 175B에서는 몇 개의 중복도 불균형적인 영향을 미칠 수 있다. 이 작업과 동시에 피티아 모델 제품군은 중복제거 The Pile이 제로샷 성능 <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>에 제한된 영향을 미친다는 것을 발견했으며 중복제거가 주로 웹 기반 데이터 세트만큼 선별된 코퍼스와 관련이 있는지 의문을 제기했다.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">우리는 <a class="ltx_ref" href="#S0.T1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>의 <a class="ltx_ref" href="#A6.T12" title="In F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">12</span></a>의 추가 정보와 함께 <a class="ltx_ref" href="#A6.SS3" title="F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">F.3</span></a>의 LLMs에 대해 널리 채택된 일부 기존 사전 훈련 영어 데이터 세트에 대한 개요를 제공한다. 또한 최근 인기 있는 오픈 모델 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib87" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>는 종종 구성 요소의 혼합 및 일치를 통해 The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>를 간접적으로 활용한다는 점에 주목한다.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p3.1">대규모 고품질 웹 사전 훈련 데이터 세트를 구축하는 데 중점을 두고, 우리는 (1) 문서 준비 및 필터링을 위한 모범 사례를 집계하고 결합하고, 라인별 수정을 도입한다; (2) 매우 대규모로 정확 및 퍼지 중복 제거를 결합한다; (3) 최종 데이터 세트의 규모는 독특하며, 총 5,000억 토큰 및 허용 라이센스가 있는 공개용으로 사용할 수 있는 6,000억 토큰 추출이다. 리파인드웹에서 대규모 모델을 훈련하는 것은 또한 웹 데이터가 큐레이팅된 코퍼라보다 엄격히 나쁘다는 일반적인 믿음에 도전하게 한다.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Macrodata Refinement and RefinedWeb</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">우리는 CommonCrawl에서 웹 데이터를 매우 대규모로 필터링하고 중복제거하기 위한 파이프라인인 <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.p1.1.1">MDR</span> (MacroData Refinement)를 소개한다. MDR을 사용하여 웹 데이터만 기반으로 하는 5조 토큰의 영어 사전 훈련 데이터 세트인 <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.p1.1.2" 스타일="color:#DB57B2;">RefinedWeb</span>을 만듭니다. 엄격한 필터링과 엄격한 중복 제거를 활용하여 웹 데이터의 품질을 높이고 최신 모델을 훈련하는 데 사용되는 집계된 말뭉치의 품질과 일치하는 말뭉치로 증류합니다.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Design principles.</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">다음 지침을 준수합니다.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Scale first. </span> MDR은 40-200B 매개 변수 모델을 훈련하는 데 사용할 데이터 세트를 생성하므로 수조 개의 토큰 <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>가 필요합니다. 영어 전용 RefinedWeb의 경우 3-6조 토큰의 크기를 목표로 합니다. 특히, 노동 집약적인 인간 큐레이션 프로세스를 피하고 이질적인 단일 도메인 소스 대신 커먼크롤에 중점을 둔다.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Strict deduplication. </span> 대형 언어 모델에 대한 중복 제거의 가치를 입증한 <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>의 작업에 영감을 받아 엄격한 중복 제거 파이프라인을 구현한다. 우리는 정확한 중복 제거와 퍼지 중복 제거를 모두 결합하고 다른 사람들이 보고한 것보다 훨씬 높은 제거율로 이어지는 엄격한 설정을 사용한다.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Neutral filtering. </span> 모델 <cite class="ltx_cite ltx_citemacro_citep">(Dodge et al., <a class="ltx_ref" href="#bib.bib31" title="">2021</a>; Welbl et al., <a class="ltx_ref" href="#bib.bib80" title="">2021</a>)</cite>에 바람직하지 않은 편향을 더 도입하는 것을 피하기 위해 언어 식별 외부에서 ML 기반 필터링을 사용하는 것을 피한다. 우리는 간단한 규칙과 휴리스틱을 고수하며 성인 콘텐츠에 대해서는 URL 필터링만 사용한다.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p"><a href="#S3.T2" title="In Language identification. ‣ 3.1 Document preparation: reading data, filtering URLs, extracting text, and language identification ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F2" title="In Design principles. ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> outline the full MDR pipeline.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span><span class="ltx_text ltx_font_bold" id="S3.F2.4.1">Macrodata Refinement의 후속 단계들은 원래 CommonCrawl에 있는 문서들의 거의 90%를 제거한다. </span> 특히, 사용 가능한 데이터의 절반에서 필터링 및 중복 제거 결과: 약 50%의 문서가 영어가 아닌 경우 폐기되고, 24%가 품질이 부족한 경우 남아 있으며, 12%가 중복된다. 각 이전 단계에 대한 제거율(<span class="ltx_text" id="S3.F2.5.2" style="color:#808080;">grey</span>)과 유지율(<span class="ltx_text" id="S3.F2.6.3" style="color:#DB57B2;">shade</span>)을 전체적으로 보고한다. 문서 준비 단계에서 문서의 %로 측정한 다음 토큰으로 측정한 비율입니다.</figcaption>
</figure>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Document preparation: reading data, filtering URLs, extracting text, and language identification</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Reading the data.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">CommonCrawl은 WARC(원시 HTML 응답) 또는 WET 파일(일반 텍스트만 포함하도록 전처리됨)에서 사용할 수 있습니다. 개별 파일은 지정된 URL의 페이지에 해당하며, 이는 단일 문서/샘플을 구성합니다. WET 파일로 작업하면 자체 HTML 추출을 실행할 수 없지만 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>; Rae et al., <a class="ltx_ref" href="#bib.bib63" title="">2021</a>)</cite>에 따라 WET 파일이 바람직하지 않은 탐색 메뉴, 광고 및 기타 관련 없는 텍스트를 포함하는 것을 발견했다. 따라서 파이프라인은 원시 WARC 파일에서 시작되며 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.1">warcio</span> 라이브러리를 사용하여 읽습니다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">URL filtering.</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">컴퓨팅이 많은 처리를 수행하기 전에 URL만 기반으로 첫 번째 필터링을 수행합니다. 이는 사기 및/또는 성인 웹사이트(예를 들어, 주로 포르노, 폭력, 도박과 관련된 것 등)를 대상으로 한다. 우리는 두 가지 규칙을 기반으로 필터링한다: (1) 4.6M 도메인의 집계된 블록 리스트; (2) 우리가 선별하고 심각도에 따라 칭량한 리스트로부터의 단어의 존재에 기초한 URL 점수. 우리는 일반적으로 사용되는 블록 리스트가 인기 있는 블로깅 플랫폼이나 대중 문화 웹사이트와 같은 많은 거짓 긍정들을 포함한다는 것을 발견했다. 또한 단어 기반 규칙(C4에서 사용된 규칙과 마찬가지로 <cite class="ltx_cite ltx_citemacro_citet">Raffel et al. (<a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>)은 의료 및 법률 페이지가 쉽게 차단될 수 있다. 이 조사를 기반으로 한 최종 세부 규칙은 <a class="ltx_ref" href="#A7.SS1" title="G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.1</span></a>에서 공유된다. 우리는 RefinedWeb을 선별된 말뭉치와 함께 집계 데이터 세트의 일부로 사용하려고 하기 때문에 고품질 데이터의 공통 소스인 위키피디아, arXiv 등도 필터링했다. 상세 목록은 <a class="ltx_ref" href="#A7.SS1.SSS3" title="G.1.3 Excluded High Quality Sources ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.1.3</span></a>에서 이용할 수 있다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text extraction.</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Lopukhin (<a class="ltx_ref" href="#bib.bib52" title="">2019</a>)</cite>는 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px3.p1.1.1">trafilatura</span> <cite class="ltx_cite ltx_citemacro_citep">(Barbaresi, <a class="ltx_ref" href="#bib.bib11" title="">2021</a>)</cite>가 블로그 게시물과 뉴스 기사로부터 콘텐츠를 검색하는 데 가장 적합한 비상업 라이브러리임을 발견했다. 이것은 CommonCrawl을 구성하는 페이지 종류의 좁은 부분집합일 뿐이지만, 우리는 이 발견이 더 광범위하게 유지된다는 것을 발견했다. 텍스트 추출을 위해 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px3.p1.1.2">trafilatura</span>을 사용하고 정규식을 통해 추가 형식을 적용합니다. 새 줄을 두 개의 연속 줄로 제한하고 모든 URL을 제거합니다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Language identification.</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">문서 수준에서 CCNet <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et al., <a class="ltx_ref" href="#bib.bib81" title="">2020</a>)</cite>의 fastText 언어 분류기를 사용하며, 문자 n-gram을 사용하며, 176개의 언어를 지원하는 위키피디아에서 학습되었다. 우리는 상위 언어 점수가 0.65 미만인 문서를 제거한다: 이것은 보통 자연 텍스트가 없는 페이지에 해당한다. 이 논문을 위해, 우리는 영어에 초점을 맞추고; RefinedWeb은 다른 언어에 대해서도 파생될 수 있으며, 자세한 내용은 <a class="ltx_ref" href="#A4" title="Appendix D Multilingual RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>를 참조한다.</p>
</div>
<div id="S3.SS1.SSS0.Px4.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p2.1">이 단계에서 검색하는 데이터인 <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS1.SSS0.Px4.p2.1.1" 스타일="color:#5E57D3;">RW-Raw</span>은 최소한의 필터링으로 추출할 수 있는 것에 해당합니다. 이 단계에서는 원본 문서의 48%만이 남아 있으며 대부분 언어 식별에 의해 걸러진다.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span><span class="ltx_text ltx_font_bold" id="S3.T2.2.1">Macrodata Refinement aggregates best practices from the state-of-the-art and novel approaches (URL scoring, line-wise filtering 등) to produce high-quality web data. </span> 중복 제거에서 MDR은 수행되는 규모와 커버리지 및 확장성을 개선하기 위해 후속적으로 퍼지 및 정확한 부분 문자열 방법을 적용할 때 모두 고유하다는 점에 주목한다.</figcaption>
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T2.3.1" class="ltx_tr">
<td id="S3.T2.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="3"><span id="S3.T2.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:70%;">Document preparation</span></td>
<td id="S3.T2.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="2"><span id="S3.T2.3.1.2.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:70%;">Filtering</span></td>
<td id="S3.T2.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="2"><span id="S3.T2.3.1.3.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:70%;">Deduplication</span></td>
</tr>
<tr id="S3.T2.3.2" class="ltx_tr">
<td id="S3.T2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">URL filtering</span></span>
</span>
</td>
<td id="S3.T2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Text extraction</span></span>
</span>
</td>
<td id="S3.T2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.3.1.1.1" class="ltx_text"></span><span id="S3.T2.3.2.3.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S3.T2.3.2.3.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S3.T2.3.2.3.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.3.2.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.3.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></span></span>
<span id="S3.T2.3.2.3.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.3.2.3.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.3.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">identification</span></span></span>
</span></span><span id="S3.T2.3.2.3.1.1.3" class="ltx_text"></span><span id="S3.T2.3.2.3.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.T2.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.4.1.1.1" class="ltx_text"></span><span id="S3.T2.3.2.4.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S3.T2.3.2.4.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S3.T2.3.2.4.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.3.2.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.4.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Document-wise</span></span></span>
<span id="S3.T2.3.2.4.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.3.2.4.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.4.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">filtering</span></span></span>
</span></span><span id="S3.T2.3.2.4.1.1.3" class="ltx_text"></span><span id="S3.T2.3.2.4.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.T2.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.5.1.1.1" class="ltx_text"></span><span id="S3.T2.3.2.5.1.1.2" class="ltx_text" style="font-size:70%;">
<span id="S3.T2.3.2.5.1.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S3.T2.3.2.5.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.3.2.5.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.5.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Line-wise</span></span></span>
<span id="S3.T2.3.2.5.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.3.2.5.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.3.2.5.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">filtering</span></span></span>
</span></span><span id="S3.T2.3.2.5.1.1.3" class="ltx_text"></span><span id="S3.T2.3.2.5.1.1.4" class="ltx_text" style="font-size:70%;"></span></span>
</span>
</td>
<td id="S3.T2.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.6.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Deduplication</span></span>
</span>
</td>
<td id="S3.T2.3.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.2.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.2.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">URL deduplication</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Aggregated blocklist, URL scoring, common HQ sources blocked</span></span>
</span>
</td>
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.2.1.1.1" class="ltx_text" style="font-size:70%;">From WARC using </span><span id="S3.T2.3.3.2.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:70%;">warcio</span><span id="S3.T2.3.3.2.1.1.3" class="ltx_text" style="font-size:70%;">, </span><span id="S3.T2.3.3.2.1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:70%;">trafilatura</span><span id="S3.T2.3.3.2.1.1.5" class="ltx_text" style="font-size:70%;"> for extraction</span></span>
</span>
</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.3.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">fastText</span><span id="S3.T2.3.3.3.1.1.2" class="ltx_text" style="font-size:70%;"> classifier from CCNet, thresholding on top language score</span></span>
</span>
</td>
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.4.1.1.1" class="ltx_text" style="font-size:70%;">In-document repetition removal and quality heuristics from MassiveWeb</span></span>
</span>
</td>
<td id="S3.T2.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.5.1.1.1" class="ltx_text" style="font-size:70%;">Remove undesirable lines (call to actions, navigation buttons, social counters, etc.)</span></span>
</span>
</td>
<td id="S3.T2.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.6.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.6.1.1.1" class="ltx_text" style="font-size:70%;">Fuzzy deduplication w/ MinHash + exact substring deduplication w/ suffix arrays</span></span>
</span>
</td>
<td id="S3.T2.3.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.3.3.7.1.1.1" class="ltx_text" style="font-size:70%;">Remove URLs revisited across CommonCrawl dumps</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.1.1.1" class="ltx_p" style="width:56.9pt;"><a href="#A7.SS1" title="G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref" style="font-size:70%;"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1</span></a></span>
</span>
</td>
<td id="S3.T2.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.2.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Barbaresi <span id="S3.T2.3.4.2.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib11" title="" class="ltx_ref">2021</a><span id="S3.T2.3.4.2.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.3.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Wenzek et&nbsp;al. <span id="S3.T2.3.4.3.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib81" title="" class="ltx_ref">2020</a><span id="S3.T2.3.4.3.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.4.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Rae et&nbsp;al. <span id="S3.T2.3.4.4.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib63" title="" class="ltx_ref">2021</a><span id="S3.T2.3.4.4.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.5.1.1" class="ltx_p" style="width:56.9pt;"><a href="#A7.SS2" title="G.2 Line-wise filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref" style="font-size:70%;"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.2</span></a></span>
</span>
</td>
<td id="S3.T2.3.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.6.1.1" class="ltx_p" style="width:56.9pt;"><cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. <span id="S3.T2.3.4.6.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib49" title="" class="ltx_ref">2022</a><span id="S3.T2.3.4.6.1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></span>
</span>
</td>
<td id="S3.T2.3.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.3.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.7.1.1" class="ltx_p" style="width:56.9pt;"><a href="#S3.SS3" title="3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref" style="font-size:70%;"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.3</span></a></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Filtering: document-wise and line-wise</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Repetition removal.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">크롤링 에러 및 저품질 소스들로 인해, 많은 문서들은 반복되는 시퀀스들을 포함한다: 이것은 최종 모델 <cite class="ltx_cite ltx_citemacro_citep">(Holtzman et al., <a class="ltx_ref" href="#bib.bib42" title="">2019</a>)</cite>에서 병리학적 행동을 야기할 수 있다. 나중에 중복 제거 단계에서 이 콘텐츠를 잡을 수 있지만 초기에 문서적으로 더 저렴하고 쉽게 잡을 수 있습니다. <cite class="ltx_cite ltx_citemacro_citet">Rae et al. (<a class="ltx_ref" href="#bib.bib63" title="">2021</a>)</cite>의 휴리스틱을 구현하고, 과도한 선, 단락 또는 n-gram 반복이 있는 모든 문서를 제거한다.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Document-wise filtering.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">페이지의 상당 부분은 주로 키워드 목록, 상용구 텍스트 또는 특수 문자 시퀀스로 만들어진 기계 생성 스팸이다. 이러한 문서들은 언어 모델링에 적합하지 않으며, 이를 필터링하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Rae et al. (<a class="ltx_ref" href="#bib.bib63" title="">2021</a>)</cite>의 품질 필터링 휴리스틱을 채택한다. 이는 전체 길이, 기호 대 단어 비율 및 문서가 실제 자연 언어임을 보장하는 기타 기준 측면에서 이상치를 제거하는 데 중점을 둡니다. 이러한 필터는 영어에서 다른 언어로 순진하게 전달되는 경우 오버필터링이 발생할 수 있으므로 언어별로 조정해야 한다.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Line-wise corrections.</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS0.Px3.p1.1.1">trafilatura</span> 대신 <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS0.Px3.p1.1.2" style="font-size:90%;">3 likes</span>, navigation buttons를 사용하여 개선되었음에도 불구하고 많은 문서가 바람직하지 않은 선으로 인터레이스되어 있다. 따라서, 우리는 이러한 바람직하지 않은 항목을 대상으로 라인 보정 필터를 고안했다. 이러한 수정 사항이 문서의 5% 이상을 제거하면 문서를 완전히 제거합니다. 자세한 내용은 <a class="ltx_ref" href="#A7.SS2" title="G.2 Line-wise filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.2</span></a>를 참조하세요.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p2.1">이 단계에서 우리가 검색한 데이터는 MDR 파이프라인에서 모든 필터링 휴리스틱을 거쳤다. 이 데이터 세트를 <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS2.SSS0.Px3.p2.1.1" 스타일="color:#B55DD4;">RW-Filtered</span>이라고 합니다. CommonCrawl 문서의 23%만이 남아 있으며 RW-Raw 문서의 약 50%가 필터링에 의해 제거된다.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Deduplication: fuzzy, exact, and across dumps</h3>

<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span><span class="ltx_text ltx_font_bold" id="S3.T3.10.1">RefinedWeb에서 훈련된 모델을 평가하고 최신 기술과 비교하기 위해 제로 샷 성능을 측정하기 위해 18개의 작업에 걸쳐 4개의 집합체를 구축합니다. </span> <span class="ltx_text ltx_font_typewriter" id="S3.T3.11.2">small</span>은 소규모의 일관된 성능을 가진 작업에 기초하여 내부 삭제를 위해 구축되었으며, <span class="ltx_text ltx_font_typewriter" id="S3.T3.12.3">core</span>은 모델 <cite class="ltx_cite ltx_citemacro_citep">(Dey et al., <a class="ltx_ref" href="#bib.bib30" title="">2023</a>; Biderman et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S3.T3.13.4">main</span>은 GPT-3 및 PaLM paper <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Chowdhery et al., <a class="ltx_ref" href="#bib.bib23" title="">2022</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S3.T3.14.5">ext</span>은 BigScience Architecture and Scaling group <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>에 의해 사용되는 작업에 기초한다. 보고된 모든 결과에 대해 임의의 평가 설정에서 얻은 <math alttext="\dagger" class="ltx_Math" display="inline" id="S3.T3.3.m1.1"><semantics id="S3.T3.3.m1.1b"><mo id="S3.T3.3.m1.1.1" xref="S3.T3.3.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.m1.1c"><ci id="S3.T3.3.m1.1.1.cmml" xref="S3.T3.3.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.m1.1d">\dagger</annotation></semantics></math> 결과와 EAI Harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite> 결과로 플래그하고 모든 모델에 사용한다.</figcaption>
<table id="S3.T3.15" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T3.15.1" class="ltx_tr">
<td id="S3.T3.15.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.15.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Tasks</span></td>
<td id="S3.T3.15.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T3.15.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Type</span></td>
<td id="S3.T3.15.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Random</span></td>
<td id="S3.T3.15.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">small</span></td>
<td id="S3.T3.15.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">core</span></td>
<td id="S3.T3.15.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.6.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">main</span></td>
<td id="S3.T3.15.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.15.1.7.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">ext</span></td>
</tr>
<tr id="S3.T3.15.2" class="ltx_tr">
<td id="S3.T3.15.2.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T3.15.2.1.1" class="ltx_text" style="font-size:70%;">HellaSwag </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.2.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Zellers et&nbsp;al.<span id="S3.T3.15.2.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib84" title="" class="ltx_ref">2019</a><span id="S3.T3.15.2.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.15.2.2.1" class="ltx_text" style="font-size:70%;">Sentence completion</span></td>
<td id="S3.T3.15.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.15.2.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.3" class="ltx_tr">
<td id="S3.T3.15.3.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.3.1.1" class="ltx_text" style="font-size:70%;">LAMBADA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.3.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Paperno et&nbsp;al.<span id="S3.T3.15.3.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib58" title="" class="ltx_ref">2016</a><span id="S3.T3.15.3.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.3.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.3.2.1" class="ltx_text" style="font-size:70%;">Sentence completion</span></td>
<td id="S3.T3.15.3.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.3.1" class="ltx_text" style="font-size:70%;">0.0</span></td>
<td id="S3.T3.15.3.4" class="ltx_td"></td>
<td id="S3.T3.15.3.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.3.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.3.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.3.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.4" class="ltx_tr">
<td id="S3.T3.15.4.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.4.1.1" class="ltx_text" style="font-size:70%;">Winogrande </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.4.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Sakaguchi et&nbsp;al.<span id="S3.T3.15.4.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib65" title="" class="ltx_ref">2021</a><span id="S3.T3.15.4.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.4.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.4.2.1" class="ltx_text" style="font-size:70%;">Coreference resolution</span></td>
<td id="S3.T3.15.4.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.4.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.4.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.4.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.4.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.4.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.5" class="ltx_tr">
<td id="S3.T3.15.5.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.5.1.1" class="ltx_text" style="font-size:70%;">PIQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.5.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Bisk et&nbsp;al.<span id="S3.T3.15.5.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib14" title="" class="ltx_ref">2020</a><span id="S3.T3.15.5.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.5.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.5.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.5.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.5.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.5.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.5.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.5.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.5.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.6" class="ltx_tr">
<td id="S3.T3.15.6.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.6.1.1" class="ltx_text" style="font-size:70%;">ARC </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.6.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Clark et&nbsp;al.<span id="S3.T3.15.6.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib25" title="" class="ltx_ref">2018</a><span id="S3.T3.15.6.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.6.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.6.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.6.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.6.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.6.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.6.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.6.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.6.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.7" class="ltx_tr">
<td id="S3.T3.15.7.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.7.1.1" class="ltx_text" style="font-size:70%;">OpenBookQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.7.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Mihaylov et&nbsp;al.<span id="S3.T3.15.7.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib54" title="" class="ltx_ref">2018</a><span id="S3.T3.15.7.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.7.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.7.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.7.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.7.4" class="ltx_td"></td>
<td id="S3.T3.15.7.5" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.5.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.7.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.7.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.7.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.8" class="ltx_tr">
<td id="S3.T3.15.8.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.8.1.1" class="ltx_text" style="font-size:70%;">BoolQ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.8.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Clark et&nbsp;al.<span id="S3.T3.15.8.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2019</a><span id="S3.T3.15.8.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.8.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.8.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.8.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.8.4" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.8.5" class="ltx_td"></td>
<td id="S3.T3.15.8.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.8.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.8.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.9" class="ltx_tr">
<td id="S3.T3.15.9.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.9.1.1" class="ltx_text" style="font-size:70%;">COPA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.9.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Gordon et&nbsp;al.<span id="S3.T3.15.9.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2012</a><span id="S3.T3.15.9.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.9.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.9.2.1" class="ltx_text" style="font-size:70%;">Sentence completion</span></td>
<td id="S3.T3.15.9.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.9.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.9.4" class="ltx_td"></td>
<td id="S3.T3.15.9.5" class="ltx_td"></td>
<td id="S3.T3.15.9.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.9.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.9.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.9.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.10" class="ltx_tr">
<td id="S3.T3.15.10.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.10.1.1" class="ltx_text" style="font-size:70%;">CB </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.10.1.2.1" class="ltx_text" style="font-size:70%;">(</span>De&nbsp;Marneffe et&nbsp;al.<span id="S3.T3.15.10.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib28" title="" class="ltx_ref">2019</a><span id="S3.T3.15.10.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.10.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.10.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.10.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.10.3.1" class="ltx_text" style="font-size:70%;">33.3</span></td>
<td id="S3.T3.15.10.4" class="ltx_td"></td>
<td id="S3.T3.15.10.5" class="ltx_td"></td>
<td id="S3.T3.15.10.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.10.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.10.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.10.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.11" class="ltx_tr">
<td id="S3.T3.15.11.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.11.1.1" class="ltx_text" style="font-size:70%;">RTE </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.11.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Dagan et&nbsp;al.<span id="S3.T3.15.11.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib26" title="" class="ltx_ref">2010</a><span id="S3.T3.15.11.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.11.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.11.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.11.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.11.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.11.4" class="ltx_td"></td>
<td id="S3.T3.15.11.5" class="ltx_td"></td>
<td id="S3.T3.15.11.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.11.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.11.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.11.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.12" class="ltx_tr">
<td id="S3.T3.15.12.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.12.1.1" class="ltx_text" style="font-size:70%;">ReCoRD </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.12.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Zhang et&nbsp;al.<span id="S3.T3.15.12.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib86" title="" class="ltx_ref">2018</a><span id="S3.T3.15.12.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.12.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.12.2.1" class="ltx_text" style="font-size:70%;">Question answering</span></td>
<td id="S3.T3.15.12.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.12.3.1" class="ltx_text" style="font-size:70%;">0.0</span></td>
<td id="S3.T3.15.12.4" class="ltx_td"></td>
<td id="S3.T3.15.12.5" class="ltx_td"></td>
<td id="S3.T3.15.12.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.12.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.12.7" class="ltx_td"></td>
</tr>
<tr id="S3.T3.15.13" class="ltx_tr">
<td id="S3.T3.15.13.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.13.1.1" class="ltx_text" style="font-size:70%;">ANLI </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.13.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Nie et&nbsp;al.<span id="S3.T3.15.13.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib56" title="" class="ltx_ref">2019</a><span id="S3.T3.15.13.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.13.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.13.2.1" class="ltx_text" style="font-size:70%;">Natural language inference</span></td>
<td id="S3.T3.15.13.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.13.3.1" class="ltx_text" style="font-size:70%;">33.3</span></td>
<td id="S3.T3.15.13.4" class="ltx_td"></td>
<td id="S3.T3.15.13.5" class="ltx_td"></td>
<td id="S3.T3.15.13.6" class="ltx_td ltx_align_center"><span id="S3.T3.15.13.6.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.13.7" class="ltx_td"></td>
</tr>
<tr id="S3.T3.15.14" class="ltx_tr">
<td id="S3.T3.15.14.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.14.1.1" class="ltx_text" style="font-size:70%;">LogiQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.14.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Liu et&nbsp;al.<span id="S3.T3.15.14.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib50" title="" class="ltx_ref">2021</a><span id="S3.T3.15.14.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.14.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.14.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.14.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.14.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.14.4" class="ltx_td"></td>
<td id="S3.T3.15.14.5" class="ltx_td"></td>
<td id="S3.T3.15.14.6" class="ltx_td"></td>
<td id="S3.T3.15.14.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.14.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.15" class="ltx_tr">
<td id="S3.T3.15.15.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.15.1.1" class="ltx_text" style="font-size:70%;">HeadQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.15.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Vilares &amp; Gómez-Rodríguez<span id="S3.T3.15.15.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib74" title="" class="ltx_ref">2019</a><span id="S3.T3.15.15.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.15.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.15.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.15.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.15.3.1" class="ltx_text" style="font-size:70%;">20.0</span></td>
<td id="S3.T3.15.15.4" class="ltx_td"></td>
<td id="S3.T3.15.15.5" class="ltx_td"></td>
<td id="S3.T3.15.15.6" class="ltx_td"></td>
<td id="S3.T3.15.15.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.15.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.16" class="ltx_tr">
<td id="S3.T3.15.16.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.16.1.1" class="ltx_text" style="font-size:70%;">MathQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.16.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Amini et&nbsp;al.<span id="S3.T3.15.16.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib8" title="" class="ltx_ref">2019</a><span id="S3.T3.15.16.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.16.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.16.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.16.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.16.3.1" class="ltx_text" style="font-size:70%;">20.0</span></td>
<td id="S3.T3.15.16.4" class="ltx_td"></td>
<td id="S3.T3.15.16.5" class="ltx_td"></td>
<td id="S3.T3.15.16.6" class="ltx_td"></td>
<td id="S3.T3.15.16.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.16.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.17" class="ltx_tr">
<td id="S3.T3.15.17.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.17.1.1" class="ltx_text" style="font-size:70%;">PROST </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.17.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Aroca-Ouellette et&nbsp;al.<span id="S3.T3.15.17.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib9" title="" class="ltx_ref">2021</a><span id="S3.T3.15.17.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.17.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.17.2.1" class="ltx_text" style="font-size:70%;">Paraphrase identification</span></td>
<td id="S3.T3.15.17.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.17.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.17.4" class="ltx_td"></td>
<td id="S3.T3.15.17.5" class="ltx_td"></td>
<td id="S3.T3.15.17.6" class="ltx_td"></td>
<td id="S3.T3.15.17.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.17.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.18" class="ltx_tr">
<td id="S3.T3.15.18.1" class="ltx_td ltx_align_left">
<span id="S3.T3.15.18.1.1" class="ltx_text" style="font-size:70%;">PubMedQA </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.18.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Jin et&nbsp;al.<span id="S3.T3.15.18.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib44" title="" class="ltx_ref">2019</a><span id="S3.T3.15.18.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.18.2" class="ltx_td ltx_align_left"><span id="S3.T3.15.18.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.18.3" class="ltx_td ltx_align_center"><span id="S3.T3.15.18.3.1" class="ltx_text" style="font-size:70%;">50.0</span></td>
<td id="S3.T3.15.18.4" class="ltx_td"></td>
<td id="S3.T3.15.18.5" class="ltx_td"></td>
<td id="S3.T3.15.18.6" class="ltx_td"></td>
<td id="S3.T3.15.18.7" class="ltx_td ltx_align_center"><span id="S3.T3.15.18.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
<tr id="S3.T3.15.19" class="ltx_tr">
<td id="S3.T3.15.19.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T3.15.19.1.1" class="ltx_text" style="font-size:70%;">SciQ </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.T3.15.19.1.2.1" class="ltx_text" style="font-size:70%;">(</span>Welbl et&nbsp;al.<span id="S3.T3.15.19.1.3.2.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#bib.bib79" title="" class="ltx_ref">2017</a><span id="S3.T3.15.19.1.4.3" class="ltx_text" style="font-size:70%;">)</span></cite>
</td>
<td id="S3.T3.15.19.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T3.15.19.2.1" class="ltx_text" style="font-size:70%;">Multiple-choice question answering</span></td>
<td id="S3.T3.15.19.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.15.19.3.1" class="ltx_text" style="font-size:70%;">25.0</span></td>
<td id="S3.T3.15.19.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.15.19.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S3.T3.15.19.5" class="ltx_td ltx_border_bb"></td>
<td id="S3.T3.15.19.6" class="ltx_td ltx_border_bb"></td>
<td id="S3.T3.15.19.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.15.19.7.1" class="ltx_text" style="font-size:70%;">✓</span></td>
</tr>
</tbody></table>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">필터링 후, 데이터 품질이 향상되었지만, 문서의 많은 부분이 문서에 걸쳐 반복된다. 이는 크롤러가 동일한 페이지를 여러 번 간접적으로 타격하거나, 상용구 내용이 반복되거나(예를 들어, 라이센스), 심지어 표절에 기인할 수 있다. 이러한 중복은 일반화 <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="#bib.bib49" title="">2022</a>; Hernandez et al., <a class="ltx_ref" href="#bib.bib40" title="">2022</a>)</cite> 대신 암기를 선호하여 모델에 강력한 영향을 미칠 수 있다. 중복 제거는 비용이 많이 들기 때문에 공개 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>; Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>에서 제한적으로 채택되었다. 우리는 퍼지 문서 일치와 정확한 시퀀스 제거를 모두 결합한 공격적인 중복 제거 전략을 채택한다.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fuzzy deduplication.</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a class="ltx_ref" href="#bib.bib17" title="">1997</a>)</cite>를 적용하여 유사 문서를 제거한다. 각 문서에 대해 스케치를 계산하고 다른 문서와의 근사 유사도를 측정하여 결국 중복도가 높은 쌍을 제거한다. MinHash는 템플릿 문서를 찾는 데 탁월합니다. 웹 사이트에서 반복되는 특정 엔터티만 있는 라이선스, 자리 표시자 SEO 텍스트 – <a class="ltx_ref" href="#A8.SS1" title="H.1 MinHash clusters ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">H.1</span></a>의 가장 큰 클러스터의 예를 참조하세요. 문서당 9,000개의 해시를 사용하여 5-gram에 걸쳐 계산되고 450개의 해시로 구성된 20개의 버킷으로 분할된 MinHash 중복 제거를 수행한다. 더 파일 <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>의 10개 해시와 같이 덜 공격적인 설정을 사용하면 중복 제거율이 낮아지고 모델 성능이 악화된다는 것을 발견했다. MinHash 설정에 대한 자세한 내용은 <a class="ltx_ref" href="#A7.SS3.SSS1" title="G.3.1 MinHash Approximate Matching ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.3.1</span></a>를 참조하세요.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Exact deduplication.</h5>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">정확한 부분 문자열은 문서 수준 대신 시퀀스 수준에서 작동하여 접미사 배열 <cite class="ltx_cite ltx_citemacro_citep">(Manber &amp; Myers, <a class="ltx_ref" href="#bib.bib53" title="">1993</a>)</cite>를 사용하여 정확한 토큰별 일치인 문자열 간의 일치(예: 특정 거부권 또는 통지, <a class="ltx_ref" href="#A8.SS2" title="H.2 Exact substring matches ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">H.2</span></a>에서 표시된 것처럼 전체 문서를 손상시키지 않을 수 있음)를 찾습니다. <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>의 구현을 사용하여 50개 이상의 연속 토큰의 일치를 제거합니다. 우리는 특정 스팬을 제거함으로써 정확한 부분 문자열이 문서를 변경한다는 점에 주목한다: 또한 전체 문서를 삭제하거나 중복된 문자열을 절단하는 대신 손실 마스킹하는 실험을 했지만, 이는 제로 샷 성능에서 큰 변화를 초래하지 않았다 - <a class="ltx_ref" href="#A7.SS3.SSS2" title="G.3.2 Exact substring deduplication ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.3.2</span></a> 참조.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">URL deduplication.</h5>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">계산적 제약으로 인해 RW-Filtered에서 직접 중복제거를 수행하는 것은 불가능하다. 대신 CommonCrawl을 각 파트가 각 덤프의 100분의 1을 포함하는 100개의 파트로 분할하고 개별 파트에 대해 중복 제거를 수행한다. 대부분의 더 큰 중복 클러스터(예: 라이선스, 공통 스팸)는 부품 간에 공유되고 효과적으로 제거됩니다. 그러나 커먼크롤 덤프는 콘텐츠의 변경에도 불구하고 덤프 간에 URL이 재방문되는 등 상당한 중복이 있음을 발견했다. 따라서 각 파트에서 보관한 모든 샘플의 URL 목록을 보관하고 처리 중인 후속 파트에서 제거합니다.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">이제 정제된 웹이 선별된 말뭉치 및 최첨단 언어 모델로 얻은 제로샷 성능과 일치하는 강력한 모델을 훈련하는 데 사용될 수 있음을 검증한다. 먼저 평가 및 사전 훈련 설정과 비교 모델에 대해 논의한다. 우리는 내부적으로 다른 인기 있는 데이터 세트와 비교하기 위해 소규모 실험을 수행하고 RefinedWeb의 세 가지 주요 단계(raw, filtered, final)를 제거한다. 그런 다음 350GT에서 훈련된 1B 및 7B 모델로 확장하여 최첨단 모델과 비교한다. 마지막으로 MDR 파이프라인을 기존 프리트레이닝 데이터셋에 적용하여 추가적인 개선점을 제공할 수 있음을 보인다.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setting</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Evaluation. </span> 사전 훈련 데이터 세트를 연구하는 이전 작업과 달리 <cite class="ltx_cite ltx_citemacro_citep">(Rae et al., <a class="ltx_ref" href="#bib.bib63" title="">2021</a>; Lee et al., <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite> 검증 손실을 측정하기보다는 많은 작업에 걸쳐 제로샷 일반화에 중점을 둔다. 복잡성만으로는 최종 작업 성능 <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="#bib.bib70" title="">2021</a>)</cite>와 상충될 수 있으며, LLMs에 대한 현대 작품은 주로 제로 샷 성능 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Rae et al., <a class="ltx_ref" href="#bib.bib63" title="">2021</a>; Chowdhery et al., <a class="ltx_ref" href="#bib.bib23" title="">2022</a>)</cite>를 보고한다. 또한, 제로 샷 일반화는 자동 회귀 디코더 전용 모델에 대한 "자연" 설정이며, 여기서 그들은 최상의 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib77" title="">2022</a>)</cite>를 수행한다. 우리의 평가 설정은 Big Science <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>의 아키텍처 및 스케일링 그룹에서 사용된 것에서 영감을 받았습니다.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.1">우리의 평가는 인기 있는 Eleuther AI 평가 하네스 <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite>를 기반으로 하며, 제로 샷 설정에서 광범위한 작업에 걸쳐 평가할 수 있다. 우리는 (1) 삭제에 대해 작은 규모에서 신호(즉, 비 제로 샷 성능)를 얻고, (2) 다른 모델에서 보고된 결과와 비교할 수 있는 작업의 집합체를 확인했다. 이 네 가지 집합 <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.1">small</span> ( ablations용) 및 <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.2">core</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.3">main</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.4">ext</span> (비교용) in <a class="ltx_ref" href="#S3.T3" title="In 3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p3.2">다양한 설정에서 훈련되고 평가된 모델 간의 비교는 많은 외부성이 1개의 987개의 결과(예: 훈련 대 추론의 수치 정밀도, 사용된 프롬프트)에 영향을 미칠 수 있으므로 풀기 어렵다. 우리는 세 가지 수준의 비교를 구별한다: (1) 사전 훈련 데이터 세트만 다른 코드베이스 내에서 훈련되고 평가된 모델과의 내부 비교; (2) 다른 코드베이스로 훈련되었지만 Eleuther AI 하네스로 평가된 모델과의 벤치마크 수준 비교, <cite class="ltx_cite ltx_citemacro_citet">Scao et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022b</a>); Black et al. (<a class="ltx_ref" href="#bib.bib16" title="">2022</a>); Aleph Alpha (<a class="ltx_ref" href="#bib.bib5" title="">2023</a>); Dey et al. (<a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>, 그 후 <math alttext="*" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><times id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">*</annotation></semantics></math>로 플래깅됨; (3) <cite class="ltx_cite ltx_citemacro_citet">Brown et al. (<a class="ltx_ref" href="#bib.bib18" title="">2020</a>); Chowdhery et al. (<a class="ltx_ref" href="#bib.bib23" title="">2022</a>)</cite>, 그 후 <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mo id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\dagger</annotation></semantics></math>로 플래깅됨. 평가에 대한 자세한 내용은 <a class="ltx_ref" href="#A6.SS1" title="F.1 Task aggregates ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">F.1</span></a>를 참조하십시오.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4:</span><span class="ltx_text ltx_font_bold" id="S4.T4.2.1">Curation is not a silver bullet for zero-shot generalization: Small-scale models trained on <span class="ltx_text" id="S4.T4.2.1.1" style="color:#DB57B2;">●<span class="ltx_text ltx_font_smallcaps" id="S4.T4.2.1.1.1">RefinedWeb</span></span> outperform models trained on web data (C4, OSCAR), and on 큐레이트된 corpora (<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T4.2.1.m1.1"><semantics id="S4.T4.2.1.m1.1b"><mi id="S4.T4.2.1.m1.1.1" mathcolor="#7DD86E" mathvariant="normal" xref="S4.T4.2.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.1.m1.1c"><ci id="S4.T4.2.1.m1.1.1.cmml" xref="S4.T4.2.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.1.m1.1d">\blacktriangledown</annotation></semantics></math><span class="ltx_text" id="color:#7DD86E;"> The Pile</span>. </span> <span class="ltx_text ltx_font_typewriter" id="S4.T4.5.2">small-agg</span> aggregate에서 zero-shot의 평균 정확도입니다. 모든 모델은 동일한 아키텍처로 훈련되고 하이퍼파라미터를 사전 훈련한다. 중복 제거가 선택 사항이기 때문에 OSCAR-22.01이 다른 데이터 세트보다 성능이 현저히 떨어집니다. C4는 OSCAR-21.09가 약간 뒤처진 강력한 기준선이지만 RefinedWeb이 웹 데이터 세트와 가장 인기 있는 큐레이트 데이터 세트인 The Pile보다 우수하다는 것을 발견했다. 필터링과 중복 제거는 모두 제로 샷 성능을 향상시키는 데 크게 기여한다.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T4.3.2" class="ltx_tr">
<td id="S4.T4.3.2.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.3.2.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="3"><span id="S4.T4.3.2.2.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Massive web datasets</span></td>
<td id="S4.T4.3.2.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.3.2.3.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Curated</span></td>
<td id="S4.T4.3.2.4" class="ltx_td ltx_align_left ltx_border_tt" colspan="3"><span id="S4.T4.3.2.4.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Ours</span></td>
</tr>
<tr id="S4.T4.3.1" class="ltx_tr">
<td id="S4.T4.3.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">OSCAR-21.09</td>
<td id="S4.T4.3.1.4" class="ltx_td ltx_align_center ltx_border_t">OSCAR-22.01</td>
<td id="S4.T4.3.1.5" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T4.3.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T4.3.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.T4.3.1.1.m1.1a"><mi mathcolor="#7DD86E" mathvariant="normal" id="S4.T4.3.1.1.m1.1.1" xref="S4.T4.3.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.m1.1b"><ci id="S4.T4.3.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S4.T4.3.1.1.1" class="ltx_text" style="color:#7DD86E;"> The Pile</span>
</td>
<td id="S4.T4.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.6.1" class="ltx_text" style="color:#5E57D3;">RW-Raw</span></td>
<td id="S4.T4.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.7.1" class="ltx_text" style="color:#B55DD4;">RW-Filtered</span></td>
<td id="S4.T4.3.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.8.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">●<span id="S4.T4.3.1.8.1.1" class="ltx_text ltx_font_smallcaps">RefinedWeb</span></span></td>
</tr>
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.3.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.3.1.1" class="ltx_text ltx_font_bold">1B@27GT</span></td>
<td id="S4.T4.3.3.2" class="ltx_td ltx_align_center ltx_border_t">55.0%</td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_center ltx_border_t">55.7%</td>
<td id="S4.T4.3.3.5" class="ltx_td ltx_align_center ltx_border_t">53.4%</td>
<td id="S4.T4.3.3.6" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
<td id="S4.T4.3.3.7" class="ltx_td ltx_align_center ltx_border_t">54.3%</td>
<td id="S4.T4.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.3.8.1" class="ltx_text ltx_font_bold">56.2%</span></td>
</tr>
<tr id="S4.T4.3.4" class="ltx_tr">
<td id="S4.T4.3.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.4.1.1" class="ltx_text ltx_font_bold">3B@60GT</span></td>
<td id="S4.T4.3.4.2" class="ltx_td ltx_align_center ltx_border_bb">59.1%</td>
<td id="S4.T4.3.4.3" class="ltx_td ltx_align_center ltx_border_bb">55.9%</td>
<td id="S4.T4.3.4.4" class="ltx_td ltx_align_center ltx_border_bb">59.6%</td>
<td id="S4.T4.3.4.5" class="ltx_td ltx_align_center ltx_border_bb">57.9%</td>
<td id="S4.T4.3.4.6" class="ltx_td ltx_align_center ltx_border_bb">57.4%</td>
<td id="S4.T4.3.4.7" class="ltx_td ltx_align_center ltx_border_bb">58.2%</td>
<td id="S4.T4.3.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.4.8.1" class="ltx_text ltx_font_bold">59.8%</span></td>
</tr>
</tbody></table>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Models. </span> 우리는 GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>)</cite>와 유사한 구성 및 하이퍼파라미터를 기반으로 1B, 3B 및 7B 매개 변수 자동 회귀 디코더 전용 모델을 훈련하며, 대부분 ALiBi <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="#bib.bib60" title="">2021</a>)</cite>의 사용에 따라 발산한다. 사용자 지정 코드 베이스에서 FlashAttention <cite class="ltx_cite ltx_citemacro_citep">(Dao et al., <a class="ltx_ref" href="#bib.bib27" title="">2022</a>)</cite>를 사용합니다. 사전 훈련 설정으로 인한 편차를 제어하기 위해 더 파일 및 RefinedWeb 모두에서 내부 모델을 훈련하며, 더 파일 모델이 다른 모델과 인라인으로 수행할 수 있음을 발견했다. 소규모 및 절제 연구(<a class="ltx_ref" href="#S4.SS2" title="4.2 Can web data alone outperform curated corpora? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>의 전반부; <a class="ltx_ref" href="#S4.SS3" title="4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>)의 경우, 1B 및 3B 매개변수 모델에 대해 각각 27B 및 60B 토큰에 대해 <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et al. (<a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>:의 스케일링 법칙에 따라 모델을 최적화하도록 훈련한다. 우리의 접근법을 보여주는 주요 실험(<a class="ltx_ref" href="#S4.SS2" title="4.2 Can web data alone outperform curated corpora? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>의 Falcon-RW 모델)을 위해 인기 있는 공개 모델 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Wang &amp; Komatsuzaki, <a class="ltx_ref" href="#bib.bib76" title="">2021</a>; Scao et al., <a class="ltx_ref" href="#bib.bib66" title="">2022a</a>)</cite>에 따라 모델을 350GT로 훈련한다. 최근에 도입된 LLaMA 모델 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>와 비교하지 않는데, 그 중 가장 작은 모델이 가장 큰 모델보다 x2.5 더 많은 컴퓨팅에 대해 훈련되어 데이터 세트별로 유의미한 비교가 이루어지지 않기 때문이다. 비교하는 모델 및 사전 훈련 데이터 세트에 대한 보다 심층적인 개요는 <a class="ltx_ref" href="#A6" title="Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a>를 참조하세요.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="158" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x4.png" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="207" height="162" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span><span class="ltx_text ltx_font_bold" id="S4.F3.13.1">Models trained on <span class="ltx_text" id="S4.F3.13.1.1" style="color:#DB57B2;">●<span class="ltx_text ltx_font_smallcaps" id="S4.F3.13.1.1.1">RefinedWeb</span></span> alone outperform models trained on curated corpora. </span> Zero-shot 성능은 <span class="ltx_text ltx_font_typewriter" id="S4.F3.14.2">core-agg</span> (left) 및 <span class="ltx_text ltx_font_typewriter" id="S4.F3.15.3">ext-agg</span> (right) 작업 집계 (자세한 내용은 <a class="ltx_ref" href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> 참조), <span class="ltx_text ltx_font_typewriter" id="S4.F3.16.4">main-agg</span>에 대한 결과는 <a class="ltx_ref" href="#S0.F1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> 참조). 기존 개방형 모델은 원래 GPT-3 시리즈의 성능과 일치하지 않습니다(왼쪽). 그러나 RefinedWeb에서 훈련된 모델은 <math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.F3.5.m1.1"><semantics id="S4.F3.5.m1.1b"><mi id="S4.F3.5.m1.1.1" mathcolor="#7DD86E" mathvariant="normal" xref="S4.F3.5.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.F3.5.m1.1c"><ci id="S4.F3.5.m1.1.1.cmml" xref="S4.F3.5.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m1.1d">\blacktriangledown</annotation></semantics></math><span class="ltx_text" id="S4.F3.17.5" 스타일="color:#7DD86E;"> The Pile</span>: 직접 비교 모델(오른쪽)을 포함하여 성능 증가의 주요 소스인 사전 훈련 설정을 배제합니다. 실제로, 우리의 RefinedWeb 모델은 <math alttext="\blacksquare" class="ltx_Math" display="inline" id="S4.F3.6.m2.1"><semantics id="S4.F3.6.m2.1b"><mi id="S4.F3.6.m2.1.1" mathcolor="#5F57DB" mathvariant="normal" xref="S4.F3.6.m2.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="S4.F3.6.m2.1c"><ci id="S4.F3.6.m2.1.1.cmml" xref="S4.F3.6.m2.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m2.1d">\blacksquare</annotation></semantics></math><span class="ltx_text" id="S4.F3.18.6" style="color:#5F57DB;">GPT-3</span> 모델의 성능과도 일치한다.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Can web data alone outperform curated corpora?</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">우리는 웹 데이터만으로 모델이 선별된 코퍼스에 대해 훈련된 다른 모델을 능가하는 결과를 가져올 수 있음을 입증하려고 노력한다. 이를 위해 먼저 인기 있는 웹 및 큐레이팅된 데이터 세트에서 최적성으로 훈련된 1B 및 3B 매개변수 모델(27GT 및 60GT)을 사용하여 소규모 연구를 수행한다. 그런 다음 350GT에서 훈련된 1B 및 7B 모델까지 확장하고 제로샷 일반화를 최신 모델과 비교한다.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Small-scale study.</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">먼저 인기 있는 공개 웹 데이터 세트(OSCAR-2019 <cite class="ltx_cite ltx_citemacro_citep">(Ortiz Suárez et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>)</cite>, OSCAR-2022 <cite class="ltx_cite ltx_citemacro_citep">(Abadji et al., <a class="ltx_ref" href="#bib.bib1" title="">2021</a>)</cite>, C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="#bib.bib64" title="">2020</a>)</cite>), The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>를 가장 인기 있는 공개 큐레이트 데이터 세트로 고려하고, RefinedWeb의 변형(RW-Raw, RW-Filtered, and RW as described as <a class="ltx_ref" href="#S3" title="3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>)을 고려한다. 이 첫 번째 연구의 경우 모든 모델은 동일한 아키텍처와 동일한 내부 코드 베이스로 훈련되며, 또한 모두 동일한 프레임워크 내에서 평가되며, 사전 훈련 데이터 세트는 다르다.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p2.1"><span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p2.1.1">small-=+</span> 6개 작업의 집합에 대한 평균 결과는 <a class="ltx_ref" href="#S4.T4" title="In 4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>에 나와 있습니다. 우리는 큐레이션이 성능 있는 언어 모델에 대한 은색 총알이 아님을 보여주는 The Pile에 비해 모든 웹 데이터 세트의 비교적 강한 성능을 관찰한다. 우리는 C4가 <cite class="ltx_cite ltx_citemacro_citet">Scao et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>–의 결과에 따라 강력한 사전 훈련 데이터 세트임을 발견했지만, The Pile은 벤치마크에서 상대적으로 더 낮은 성능을 보인다. OSCAR-22.01에 대한 상대적으로 실망스러운 결과는 중복 제거 없이 배포되는 데이터 세트의 주요 버전 때문일 수 있다. RefinedWeb과 관련하여 필터링 및 중복 제거 모두 성능을 크게 향상시킵니다.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Full-scale models.</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.2">이제 최신 모델과의 비교를 통해 이러한 결과를 검증한다. 350GT에서 1B 및 7B 모델을 훈련하여 이전 실험을 확장하고 사전 훈련 설정의 영향에 대한 제어로 더 파일에서 350GT에서 1B 모델을 훈련한다. 우리는 GPT-3 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>)</cite>, FairSeq 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Artetxe et al., <a class="ltx_ref" href="#bib.bib10" title="">2021</a>)</cite>, GPT-Neo(X)/J 모델 <cite class="ltx_cite ltx_citemacro_citep">(Black et al., <a class="ltx_ref" href="#bib.bib15" title="">2021</a>; Wang &amp; Komatsuzaki, <a class="ltx_ref" href="#bib.bib76" title="">2021</a>; Black et al., <a class="ltx_ref" href="#bib.bib16" title="">2022</a>)</cite>, OPT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib87" title="">2022</a>)</cite>, BigScience Architecture and Scaling Pile 모델 <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>, PaLM-8B <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="#bib.bib23" title="">2022</a>)</cite>, Aleph Alpha Luminous 13B <cite class="ltx_cite ltx_citemacro_citep">(Aleph Alpha, <a class="ltx_ref" href="#bib.bib5" title="">2023</a>)</cite>, Pythia 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>, Cerebras-GPT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Dey et al., <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>와 비교한다. GPT-3의 경우 API(<span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p1.2.1">babbage</span> 및 <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p1.2.2">curie</span>)를 통해 얻은 결과와 다른 평가 설정(<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">\dagger</annotation></semantics></math>)으로 논문에 보고된 결과를 구별한다. PaLM 및 OPT의 경우 다른 평가 제품군(<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px2.p1.2.m2.1a"><mo id="S4.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.2.m2.1c">\dagger</annotation></semantics></math>)으로 결과도 얻은 반면, 다른 모델의 경우 평가 하네스(*)로도 얻어져 보다 직접적인 비교가 가능하다.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1"><span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p2.1.1">main-agg</span>은 <a class="ltx_ref" href="#S0.F1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p2.1.2">core-agg</span> 및 <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p2.1.3">ext-agg</span>에 나와 있습니다. 우리는 개방형 모델이 유사한 평가 설정을 사용할 때에도 GPT-3-와 같은 개인 선별된 코퍼라에서 훈련된 모델의 성능이 일관되게 저하된다는 것을 발견했다. 반대로 RefinedWeb에서 훈련된 모델은 The Pile에서 사용되는 일반적인 고품질 소스가 RefinedWeb에서 제외되었음에도 불구하고 웹 데이터만을 사용하여 GPT-3 시리즈의 성능을 일치시킬 수 있다(부록의 <a class="ltx_ref" href="#A7.T14" title="In G.1.3 Excluded High Quality Sources ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">14</span></a> 참조). 마지막으로, 더 파일에서 훈련된 내부 모델은 빅사이언스 아키텍처 및 스케일링 모델에 따라 수행된다는 점에 주목한다. 이는 우리의 사전 훈련 설정이 RefinedWeb에서 훈련된 모델의 성능 증가의 주요 원천이 될 가능성이 없음을 강조한다.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p3" class="ltx_para">
<span id="S4.SS2.SSS0.Px2.p3.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="S4.SS2.SSS0.Px2.p3.1.1" class="ltx_p"><span id="S4.SS2.SSS0.Px2.p3.1.1.1" class="ltx_text ltx_font_bold">Finding.</span> Challenging existing beliefs on data quality and LLMs, models trained on adequately filtered and deduplicated web data <em id="S4.SS2.SSS0.Px2.p3.1.1.2" class="ltx_emph ltx_font_italic">alone</em> can match the performance of models trained on curated data.</span>
</span>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Do other corpora benefit from MDR?</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">MDR 파이프라인에서 개별 구성요소의 기여도를 삭제하고 성능을 평가하는 것은 어렵다: 대부분의 휴리스틱의 경우 합의된 그라운드 트루스가 없으며 사전 훈련 후 충분한 제로 샷 신호를 생성하기에는 변화가 너무 미미할 수 있다. <a class="ltx_ref" href="#S4.SS2" title="4.2 Can web data alone outperform curated corpora? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>의 전반부에서 RefinedWeb의 후속 단계(raw, filtered, final)가 성능 향상으로 이어짐을 확인하였다. 이 섹션에서는 MDR의 필터링 및 중복 제거 단계를 인기 있는 사전 훈련 데이터 세트에 독립적으로 적용하는 것을 제안하여 널리 일반화되는지 여부를 연구한다.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p2.1"><a class="ltx_ref" href="#S4.T5" title="In 4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>에서 <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.1">small-agg</span>에 대한 결과를 보고합니다. 첫째, 필터링에 의한 개선이 체계적이지 않다는 것을 알 수 있다. 파일에서 우리는 책과 코드를 삭제하지 않기 위해 줄 길이와 문자 비율 휴리스틱을 조정해야 했다. OSCAR-21.09, C4 및 The Pile의 개선에도 불구하고, 우리의 필터는 OSCAR-22.01의 성능을 악화시키며, 일반적으로 필터링으로부터의 제거율은 다운스트림 정확도와 강하게 상관되지 않는 것으로 보인다. 반대로 중복 제거는 모든 데이터 세트에서 안정적인 부스트를 제공하며 제거 속도는 성능 변화와 더 잘 상관됩니다. 우리는 OSCAR-21.09와 C4가 이미 잘 중복되어 있는 반면, The Pile과 OSCAR-22.01은 40-60%의 중복을 보인다. OSCAR-22.01의 기본 버전은 중복 제거 없이 배포되며, The Pile의 경우, 이는 <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib87" title="">2022</a>)</cite>의 결과와 일치한다. 마지막으로 필터링과 중복 제거를 결합하면 추가 개선이 이루어지며, 흥미롭게도 이제 데이터 세트에 걸쳐 성능이 더 균일하지만 차이점은 남아 있어 원본 텍스트 추출 및 처리의 결함을 완전히 보상할 수 없음을 시사한다.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p3.1">MDR을 통해 C4를 처리함으로써 RefinedWeb을 약간 능가할 수 있는 데이터의 하위 집합을 얻을 수 있으며, 이는 C4의 엄격한 필터링(예: 엄격한 NSFW 단어 블록리스트, 3문장 스팬 중복제거)과 자체 필터 및 중복제거를 결합한다. 이러한 조합은 3-6조 토큰의 목표에서 수용할 수 없는 거부율을 초래하지만, 이는 대규모 웹 데이터 세트에서 매우 고품질 하위 집합을 추출할 수 있는 더 짧은 실행에 대한 흥미로운 관점을 나타낸다.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<span id="S4.SS3.p4.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="S4.SS3.p4.1.1" class="ltx_p"><span id="S4.SS3.p4.1.1.1" class="ltx_text ltx_font_bold">Finding.</span> While filtering heuristics may require source-dependent tuning, stringent deduplication improves zero-shot performance across datasets consistently.</span>
</span>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span><span class="ltx_text ltx_font_bold" id="S4.T5.4.1">필터링으로부터의 개선은 데이터 세트에 걸쳐 체계적이지 않지만, 중복 제거는 보드 전반에 걸쳐 꾸준한 성능 부스트를 가져온다. </span> Zero-shot accuracy averaged on our <span class="ltx_text ltx_font_typewriter" id="S4.T5.5.2">small-agg</span> aggregate; [+x.x] report absolute gain compared to base, removal rates reported against base. 파이프라인의 한계로 인해 RefinedWeb에 대해 중복 제거 단계를 독립적으로 적용할 수 없습니다.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="3"><span id="S4.T5.1.2.2.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Massive web datasets</span></td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.2.3.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Curated</span></td>
<td id="S4.T5.1.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.2.4.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Ours</span></td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">OSCAR-21.09</td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_t">OSCAR-22.01</td>
<td id="S4.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mi mathcolor="#7DD86E" mathvariant="normal" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S4.T5.1.1.1.1" class="ltx_text" style="color:#7DD86E;"> Pile</span>
</td>
<td id="S4.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.6.1" class="ltx_text" style="color:#DB57B2;">●RefinedWeb</span></td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.3.1.1" class="ltx_text ltx_font_bold">Base</span></td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_t">55.0%</td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
<td id="S4.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.4.1" class="ltx_text ltx_font_bold">55.7%</span></td>
<td id="S4.T5.1.3.5" class="ltx_td ltx_align_center ltx_border_t">53.4%</td>
<td id="S4.T5.1.3.6" class="ltx_td ltx_align_center ltx_border_t">52.7%</td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T5.1.4.1.1" class="ltx_text ltx_font_bold">Filtered</span></td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_center">55.4% [+.4]</td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_center">52.3% [-.4]</td>
<td id="S4.T5.1.4.4" class="ltx_td ltx_align_center">
<span id="S4.T5.1.4.4.1" class="ltx_text ltx_font_bold">56.2%</span> [+.5]</td>
<td id="S4.T5.1.4.5" class="ltx_td ltx_align_center">54.2% [+.8]</td>
<td id="S4.T5.1.4.6" class="ltx_td ltx_align_center">54.3% [+1.6]</td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_align_left"><em id="S4.T5.1.5.1.1" class="ltx_emph ltx_font_italic">removal rate</em></td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.2.1" class="ltx_emph ltx_font_italic">-25.0%</em></td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.3.1" class="ltx_emph ltx_font_italic">-39.8%</em></td>
<td id="S4.T5.1.5.4" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.4.1" class="ltx_emph ltx_font_italic">-16.4%</em></td>
<td id="S4.T5.1.5.5" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.5.1" class="ltx_emph ltx_font_italic">-27.1%</em></td>
<td id="S4.T5.1.5.6" class="ltx_td ltx_align_center"><em id="S4.T5.1.5.6.1" class="ltx_emph ltx_font_italic">-50.8%</em></td>
</tr>
<tr id="S4.T5.1.6" class="ltx_tr">
<td id="S4.T5.1.6.1" class="ltx_td ltx_align_left"><span id="S4.T5.1.6.1.1" class="ltx_text ltx_font_bold">Deduplicated</span></td>
<td id="S4.T5.1.6.2" class="ltx_td ltx_align_center">55.6% [+.6]</td>
<td id="S4.T5.1.6.3" class="ltx_td ltx_align_center">55.6% [+2.9]</td>
<td id="S4.T5.1.6.4" class="ltx_td ltx_align_center">
<span id="S4.T5.1.6.4.1" class="ltx_text ltx_font_bold">55.9%</span> [+.2]</td>
<td id="S4.T5.1.6.5" class="ltx_td ltx_align_center">54.5% [+1.1]</td>
<td id="S4.T5.1.6.6" class="ltx_td"></td>
</tr>
<tr id="S4.T5.1.7" class="ltx_tr">
<td id="S4.T5.1.7.1" class="ltx_td ltx_align_left"><em id="S4.T5.1.7.1.1" class="ltx_emph ltx_font_italic">removal rate</em></td>
<td id="S4.T5.1.7.2" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.2.1" class="ltx_emph ltx_font_italic">-10.8%</em></td>
<td id="S4.T5.1.7.3" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.3.1" class="ltx_emph ltx_font_italic">-60.8%</em></td>
<td id="S4.T5.1.7.4" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.4.1" class="ltx_emph ltx_font_italic">-7.59%</em></td>
<td id="S4.T5.1.7.5" class="ltx_td ltx_align_center"><em id="S4.T5.1.7.5.1" class="ltx_emph ltx_font_italic">-45.3%</em></td>
<td id="S4.T5.1.7.6" class="ltx_td"></td>
</tr>
<tr id="S4.T5.1.8" class="ltx_tr">
<td id="S4.T5.1.8.1" class="ltx_td ltx_align_left"><span id="S4.T5.1.8.1.1" class="ltx_text ltx_font_bold">Filt.+Dedup.</span></td>
<td id="S4.T5.1.8.2" class="ltx_td ltx_align_center">55.5% [+.5]</td>
<td id="S4.T5.1.8.3" class="ltx_td ltx_align_center">55.4% [+2.7]</td>
<td id="S4.T5.1.8.4" class="ltx_td ltx_align_center">
<span id="S4.T5.1.8.4.1" class="ltx_text ltx_font_bold">56.4%</span> [+.7]</td>
<td id="S4.T5.1.8.5" class="ltx_td ltx_align_center">55.2% [+1.8]</td>
<td id="S4.T5.1.8.6" class="ltx_td ltx_align_center">56.2% [+3.5]</td>
</tr>
<tr id="S4.T5.1.9" class="ltx_tr">
<td id="S4.T5.1.9.1" class="ltx_td ltx_align_left ltx_border_bb"><em id="S4.T5.1.9.1.1" class="ltx_emph ltx_font_italic">removal rate</em></td>
<td id="S4.T5.1.9.2" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.2.1" class="ltx_emph ltx_font_italic">-28.2%</em></td>
<td id="S4.T5.1.9.3" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.3.1" class="ltx_emph ltx_font_italic">-62.2%</em></td>
<td id="S4.T5.1.9.4" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.4.1" class="ltx_emph ltx_font_italic">-17.9%</em></td>
<td id="S4.T5.1.9.5" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.5.1" class="ltx_emph ltx_font_italic">-66.0%</em></td>
<td id="S4.T5.1.9.6" class="ltx_td ltx_align_center ltx_border_bb"><em id="S4.T5.1.9.6.1" class="ltx_emph ltx_font_italic">-75.4%</em></td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Biases.</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1"><a class="ltx_ref" href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>에서 RefinedWeb의 독성에 대한 기본 분석을 수행한다. 우리는 퍼스펙티브 API에서 제공하는 독성의 정의에 기초하여 RW가 파일만큼 독성이 있다는 것을 발견한다: "무례하거나 무례한 콘텐츠" 특히, 이 정의는 사회적 편견이나 유해성에 대한 문제를 다루지 않는다. 우리의 파이프라인이 인기 있는 데이터 세트에 대해 이미 문서화된 것보다 이 측면에서 추가 문제를 도입할 가능성은 낮지만 RefinedWeb의 공개 추출물에 대한 추가 정량적 작업을 권장한다.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multiple epochs.</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">조 단위 사전 훈련 데이터 세트를 구성할 ‘독특한’ 토큰을 찾는 대신 여러 시대에 걸쳐 데이터를 반복하기만 하면 된다. OPT 및 NeoX-20B와 같은 인기 있는 모델은 최대 2개의 에폭에 대해 이 작업을 수행하며 대부분의 큐레이트된 데이터 세트는 2-5회 말뭉치를 업샘플링합니다. 그러나 <cite class="ltx_cite ltx_citemacro_citet">Hernandez et al. (<a class="ltx_ref" href="#bib.bib40" title="">2022</a>)</cite>는 최근 100B+ 매개변수를 가진 모델이 몇 개의 에포크에도 민감할 수 있음을 보여주었다. 우리의 작업과 직교하는 것은 데이터 제약 체제에서 절충점을 탐구하는 연구 라인에 있다: 중복 제거가 더 많은 시대를 지속하는 데 도움이 될 수 있는가? 고품질 데이터의 여러 에포크가 저품질 데이터의 한 에포크보다 나은가요? 보다 심도 있는 논의는 <a class="ltx_ref" href="#A5.SS3" title="E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">E.3</span></a>를 참조한다.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Other results on deduplication.</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Biderman et al. (<a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>는 중복제거 더 파일에서 제로샷 성능에 제한된 영향을 발견했으며, <a class="ltx_ref" href="#A6.SS2" title="F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">F.2</span></a>에서 더 논의하지만 큐레이트된 코퍼라에 대한 추가 중복제거 연구를 장려하고 중복제거에 의해 발생하는 토큰의 감소를 보상하기 위해 여러 에포크를 수행해야 하는 데이터 제약 체제에서 중복제거를 연구한다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">LLM이 널리 채택됨에 따라 스케일링 법칙의 권장 사항을 지나 훈련된 모델은 추론 비용 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>를 상각하기 위해 점점 더 일반화될 수밖에 없다. 이는 공개적으로 사용 가능한 말뭉치를 넘어 수십조 개의 토큰을 사용하여 데이터 세트를 사전 훈련해야 할 필요성을 더욱 촉진할 것이다. 우리는 엄격한 필터링과 중복 제거가 큐레이팅된 코퍼라에서 훈련된 LLM을 능가하는 최첨단 모델과 경쟁력 있는 모델을 생성하기에 적합한 5조 토큰 웹 데이터 세트만을 초래할 수 있음을 입증했다. 우리는 RefinedWeb의 600GT 추출물을 공개적으로 출시하며 RefinedWeb은 이미 Falcon-40B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et al., <a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite>와 같은 최첨단 언어 모델을 훈련하는 데 사용되었습니다.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x5.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="171" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 4:</span><span class="ltx_text ltx_font_bold" id="S6.F4.2.1">Toxic content in <span class="ltx_text" id="S6.F4.2.1.1" style="color:#DB57B2;">RefinedWeb</span> is distributed similarly to <span class="ltx_text" id="color:#7DD86E;">The Pile. Pespective API에 의해 평가된 바와 같이 주어진 독성 점수 미만의 문서의 누적 비율</span></span></figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Abadji, J., Suárez, P. J.&nbsp;O., Romary, L., and Sagot, B.

</span>
<span class="ltx_bibblock">Ungoliant: An optimized pipeline for the generation of a very
large-scale multilingual web corpus.

</span>
<span class="ltx_bibblock">Proceedings of the Workshop on Challenges in the Management of Large
Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pp.&nbsp; 1 – 9,
Mannheim, 2021. Leibniz-Institut für Deutsche Sprache.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.14618/ids-pub-10468</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Abadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B.

</span>
<span class="ltx_bibblock">Towards a Cleaner Document-Oriented Multilingual Crawled Corpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2201.06642, January 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Abbas, A. K.&nbsp;M., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A.&nbsp;S.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic
deduplication.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ICLR 2023 Workshop on Mathematical and Empirical
Understanding of Foundation Models</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adiwardana et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Adiwardana, D., Luong, M.-T., So, D.&nbsp;R., Hall, J., Fiedel, N., Thoppilan, R.,
Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et&nbsp;al.

</span>
<span class="ltx_bibblock">Towards a human-like open-domain chatbot.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.09977</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aleph Alpha (2023)</span>
<span class="ltx_bibblock">
Aleph Alpha.

</span>
<span class="ltx_bibblock">Luminous: performance benchmarks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.12885</em>, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aleph-alpha.com/pdf/2023_02_AA_Benchmarks_doc.pdf</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allamanis (2019)</span>
<span class="ltx_bibblock">
Allamanis, M.

</span>
<span class="ltx_bibblock">The adverse effects of code duplication in machine learning models of
code.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 ACM SIGPLAN International Symposium
on New Ideas, New Paradigms, and Reflections on Programming and Software</em>,
pp.&nbsp; 143–153, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Almazrouei, E., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow,
D., Launay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo, G.

</span>
<span class="ltx_bibblock">Falcon-40b: an open large language model with state-of-the-art
performance.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R., Choi, Y., and
Hajishirzi, H.

</span>
<span class="ltx_bibblock">Mathqa: Towards interpretable math word problem solving with
operation-based formalisms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 2357–2367, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aroca-Ouellette et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Aroca-Ouellette, S., Paik, C., Roncone, A., and Kann, K.

</span>
<span class="ltx_bibblock">Prost: Physical reasoning about objects through space and time.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021</em>, pp.&nbsp; 4597–4608, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin,
X.&nbsp;V., Du, J., Iyer, S., Pasunuru, R., et&nbsp;al.

</span>
<span class="ltx_bibblock">Efficient large scale language modeling with mixtures of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.10684</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbaresi (2021)</span>
<span class="ltx_bibblock">
Barbaresi, A.

</span>
<span class="ltx_bibblock">Trafilatura: A Web Scraping Library and Command-Line Tool for Text
Discovery and Extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Joint Conference of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing: System
Demonstrations</em>, pp.&nbsp; 122–131. Association for Computational Linguistics,
2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.acl-demo.15" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-demo.15</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Beltagy, I., Lo, K., and Cohan, A.

</span>
<span class="ltx_bibblock">Scibert: A pretrained language model for scientific text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pp.&nbsp; 3615–3620, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan,
E., Khan, M.&nbsp;A., Purohit, S., Prashanth, U.&nbsp;S., Raff, E., et&nbsp;al.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01373</em>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pp.&nbsp; 7432–7439, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Black, S., Leo, G., Wang, P., Leahy, C., and Biderman, S.

</span>
<span class="ltx_bibblock">GPT-Neo: Large Scale Autoregressive Language Modeling with
Mesh-Tensorflow, March 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5297715" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5297715</a>.

</span>
<span class="ltx_bibblock">If you use this software, please cite it using these metadata.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He,
H., Leahy, C., McDonell, K., Phang, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-neox-20b: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Challenges &amp; Perspectives in Creating Large Language Models</em>,
pp.&nbsp;&nbsp;95, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Broder (1997)</span>
<span class="ltx_bibblock">
Broder, A.&nbsp;Z.

</span>
<span class="ltx_bibblock">On the resemblance and containment of documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings. Compression and Complexity of Sequences 1997</em>,
pp.&nbsp; 21–29. IEEE, 1997.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.&nbsp;D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K.,
Roberts, A., Brown, T., Song, D., Erlingsson, U., et&nbsp;al.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">30th USENIX Security Symposium (USENIX Security 21)</em>, pp.&nbsp;2633–2650, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C.

</span>
<span class="ltx_bibblock">Quantifying memorization across neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07646</em>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charikar (2002)</span>
<span class="ltx_bibblock">
Charikar, M.&nbsp;S.

</span>
<span class="ltx_bibblock">Similarity estimation techniques from rounding algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the thiry-fourth annual ACM symposium on
Theory of computing</em>, pp.&nbsp; 380–388, 2002.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chelba et&nbsp;al. (2013)</span>
<span class="ltx_bibblock">
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and
Robinson, T.

</span>
<span class="ltx_bibblock">One billion word benchmark for measuring progress in statistical
language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.3005</em>, 2013.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
Barham, P., Chung, H.&nbsp;W., Sutton, C., Gehrmann, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova,
K.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of NAACL-HLT</em>, pp.&nbsp; 2924–2936, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and
Tafjord, O.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dagan et&nbsp;al. (2010)</span>
<span class="ltx_bibblock">
Dagan, I., Dolan, B., Magnini, B., and Roth, D.

</span>
<span class="ltx_bibblock">Recognizing textual entailment: Rational, evaluation and
approaches–erratum.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Natural Language Engineering</em>, 16(1):105–105, 2010.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Dao, T., Fu, D.&nbsp;Y., Ermon, S., Rudra, A., and Re, C.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with
io-awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Marneffe et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
De&nbsp;Marneffe, M.-C., Simons, M., and Tonhauser, J.

</span>
<span class="ltx_bibblock">The commitmentbank: Investigating projection in naturally occurring
discourse.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">proceedings of Sinn und Bedeutung</em>, volume&nbsp;23, pp.&nbsp;107–124, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 4171–4186, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dey et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness,
J., et&nbsp;al.

</span>
<span class="ltx_bibblock">Cerebras-gpt: Open compute-optimal language models trained on the
cerebras wafer-scale cluster.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03208</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D.,
Mitchell, M., and Gardner, M.

</span>
<span class="ltx_bibblock">Documenting large webtext corpora: A case study on the colossal clean
crawled corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pp.&nbsp; 1286–1305, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eberhard et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Eberhard, D.&nbsp;M., Simons, G.&nbsp;F., and Fennig, C.&nbsp;D.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Ethnologue: Languages of the World</em>.

</span>
<span class="ltx_bibblock">SIL International, Dallas, TX, USA, twenty-sixth edition, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang,
J., He, H., Thite, A., Nabeshima, N., et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,
Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E.,
Thite, A., Wang, B., Wang, K., and Zou, A.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J.&nbsp;W., Wallach, H., Iii,
H.&nbsp;D., and Crawford, K.

</span>
<span class="ltx_bibblock">Datasheets for datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(12):86–92,
2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokaslan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Gokaslan, A., Cohen, V., Pavlick, E., and Tellex, S.

</span>
<span class="ltx_bibblock">Openwebtext corpus.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://Skylion007.github.io/OpenWebTextCorpus" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://Skylion007.github.io/OpenWebTextCorpus</a>, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et&nbsp;al. (2012)</span>
<span class="ltx_bibblock">
Gordon, A., Kozareva, Z., and Roemmele, M.

</span>
<span class="ltx_bibblock">Semeval-2012 task 7: Choice of plausible alternatives: An evaluation
of commonsense causal reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">* SEM 2012: The First Joint Conference on Lexical and
Computational Semantics–Volume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012)</em>, pp.&nbsp; 394–398, 2012.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grave et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Grave, É., Bojanowski, P., Gupta, P., Joulin, A., and Mikolov, T.

</span>
<span class="ltx_bibblock">Learning word vectors for 157 languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</em>, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanu &amp; Unitary team (2020)</span>
<span class="ltx_bibblock">
Hanu, L. and Unitary team.

</span>
<span class="ltx_bibblock">Detoxify.

</span>
<span class="ltx_bibblock">Github. https://github.com/unitaryai/detoxify, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S.,
Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling laws and interpretability of learning from repeated data.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.10487</em>, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
E., Casas, D. d.&nbsp;L., Hendricks, L.&nbsp;A., Welbl, J., Clark, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaccard (1912)</span>
<span class="ltx_bibblock">
Jaccard, P.

</span>
<span class="ltx_bibblock">The distribution of the flora in the alpine zone.1.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">New Phytologist</em>, 11:37–50, 1912.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X.

</span>
<span class="ltx_bibblock">Pubmedqa: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pp.&nbsp; 2567–2577, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., and Mikolov,
T.

</span>
<span class="ltx_bibblock">Fasttext. zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.03651</em>, 2016.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.&nbsp;B., Chess, B., Child, R.,
Gray, S., Radford, A., Wu, J., and Amodei, D.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreutzer et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh,
N., Tapo, A.&nbsp;A., Subramani, N., Sokolov, A., Sikasote, C., et&nbsp;al.

</span>
<span class="ltx_bibblock">Quality at a glance: An audit of web-crawled multilingual datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:50–72, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Laurençon, H., Saulnier, L., Wang, T., Akiki, C., del Moral, A.&nbsp;V.,
Le&nbsp;Scao, T., Von&nbsp;Werra, L., Mou, C., Ponferrada, E.&nbsp;G., Nguyen, H., et&nbsp;al.

</span>
<span class="ltx_bibblock">The bigscience roots corpus: A 1.6 tb composite multilingual dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Thirty-sixth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and
Carlini, N.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 8424–8445,
2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y.

</span>
<span class="ltx_bibblock">Logiqa: a challenge dataset for machine reading comprehension with
logical reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Conference on
International Joint Conferences on Artificial Intelligence</em>, pp.&nbsp;3622–3628, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., and Stoyanov, V.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopukhin (2019)</span>
<span class="ltx_bibblock">
Lopukhin, K.

</span>
<span class="ltx_bibblock">Evaluating quality of article body extraction for commercial services
and open-source libraries.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/scrapinghub/article-extraction-benchmark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/scrapinghub/article-extraction-benchmark</a>,
2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manber &amp; Myers (1993)</span>
<span class="ltx_bibblock">
Manber, U. and Myers, G.

</span>
<span class="ltx_bibblock">Suffix arrays: a new method for on-line string searches.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Journal on Computing</em>, 22(5):935–948,
1993.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pp.&nbsp; 2381–2391, 2018.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B.,
Spitzer, E., Raji, I.&nbsp;D., and Gebru, T.

</span>
<span class="ltx_bibblock">Model cards for model reporting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the conference on fairness, accountability,
and transparency</em>, pp.&nbsp; 220–229, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., and Kiela, D.

</span>
<span class="ltx_bibblock">Adversarial nli: A new benchmark for natural language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.14599</em>, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortiz Suárez et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Ortiz Suárez, P.&nbsp;J., Sagot, B., and Romary, L.

</span>
<span class="ltx_bibblock">Asynchronous pipelines for processing huge corpora on medium to low
resource infrastructures.

</span>
<span class="ltx_bibblock">Proceedings of the Workshop on Challenges in the Management of Large
Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp.&nbsp; 9 – 16, Mannheim,
2019. Leibniz-Institut für Deutsche Sprache.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.14618/ids-pub-9021</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q., Bernardi, R.,
Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R.

</span>
<span class="ltx_bibblock">The lambada dataset: Word prediction requiring a broad discourse
context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 1525–1534,
2016.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pomikálek (2011)</span>
<span class="ltx_bibblock">
Pomikálek, J.

</span>
<span class="ltx_bibblock">Justext.

</span>
<span class="ltx_bibblock">2011.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Press, O., Smith, N., and Lewis, M.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input
length extrapolation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Rae, J.&nbsp;W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,
T., Menick, J., Cassirer, A., Powell, R., Driessche, G. v.&nbsp;d., Hendricks,
L.&nbsp;A., Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S., Huang,
S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A.,
Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E.,
Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.&nbsp;L., Kuncoro, A.,
Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A.,
Lespiau, J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T.,
Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., d’Autume, C. d.&nbsp;M., Li, Y.,
Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., Casas, D. d.&nbsp;L., Guy, A.,
Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel,
I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O.,
Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and
Irving, G.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/ARXIV.2112.11446</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.11446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
Y., Li, W., and Liu, P.&nbsp;J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v21/20-074.html</a>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Sakaguchi, K., Bras, R.&nbsp;L., Bhagavatula, C., and Choi, Y.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(9):99–106,
2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Scao, T.&nbsp;L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D.,
Castagné, R., Luccioni, A.&nbsp;S., Yvon, F., Gallé, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>, 2022a.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Scao, T.&nbsp;L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M.&nbsp;S.,
Bideman, S., Elsahar, H., Muennighoff, N., Phang, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">What language model to train if you have one million gpu hours?

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.15424</em>, 2022b.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sevilla et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M., and Villalobos, P.

</span>
<span class="ltx_bibblock">Compute trends across three eras of machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.05924</em>, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sites (2013)</span>
<span class="ltx_bibblock">
Sites, D.

</span>
<span class="ltx_bibblock">Compact language detector 2.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Software available at https://github. com/CLD2Owners/cld2 (last
updated on August 2015)</em>, 2013.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H.&nbsp;W., Narang, S.,
Yogatama, D., Vaswani, A., and Metzler, D.

</span>
<span class="ltx_bibblock">Scale efficiently: Insights from pretraining and finetuning
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Thoppilan, R., De&nbsp;Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh &amp; Le (2018)</span>
<span class="ltx_bibblock">
Trinh, T.&nbsp;H. and Le, Q.&nbsp;V.

</span>
<span class="ltx_bibblock">A simple method for commonsense reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.02847</em>, 2018.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilares &amp; Gómez-Rodríguez (2019)</span>
<span class="ltx_bibblock">
Vilares, D. and Gómez-Rodríguez, C.

</span>
<span class="ltx_bibblock">Head-qa: A healthcare dataset for complex reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.&nbsp; 960–966, 2019.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.04325</em>, 2022.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang &amp; Komatsuzaki (2021)</span>
<span class="ltx_bibblock">
Wang, B. and Komatsuzaki, A.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kingoflolz/mesh-transformer-jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kingoflolz/mesh-transformer-jax</a>, May 2021.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wang, T., Roberts, A., Hesslow, D., Scao, T.&nbsp;L., Chung, H.&nbsp;W., Beltagy, I.,
Launay, J., and Raffel, C.

</span>
<span class="ltx_bibblock">What language model architecture and pretraining objective work best
for zero-shot generalization?

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2022.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,
D., Bosma, M., Zhou, D., Metzler, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Welbl, J., Liu, N.&nbsp;F., and Gardner, M.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Noisy User-generated
Text</em>, pp.&nbsp; 94–106, 2017.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L.&nbsp;A.,
Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S.

</span>
<span class="ltx_bibblock">Challenges in detoxifying language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pp.&nbsp; 2447–2469, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin,
A., and Grave, É.

</span>
<span class="ltx_bibblock">Ccnet: Extracting high quality monolingual datasets from web crawl
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Language Resources and Evaluation
Conference</em>, pp.&nbsp; 4003–4012, 2020.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,
A., and Raffel, C.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pp.&nbsp; 483–498, 2021.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yang, G., Hu, E., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,
Pachocki, J., Chen, W., and Gao, J.

</span>
<span class="ltx_bibblock">Tuning large neural networks via zero-shot hyperparameter transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
34:17084–17097, 2021.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.&nbsp; 4791–4800, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z.,
Wang, K., Zhang, X., et&nbsp;al.

</span>
<span class="ltx_bibblock">Pangu-alpha: Large-scale autoregressive pretrained chinese language
models with auto-parallel computation.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.12369</em>, 2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Van&nbsp;Durme, B.

</span>
<span class="ltx_bibblock">Record: Bridging the gap between human and machine commonsense
reading comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.12885</em>, 2018.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
Diab, M., Li, X., Lin, X.&nbsp;V., et&nbsp;al.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2015)</span>
<span class="ltx_bibblock">
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
and Fidler, S.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pp.&nbsp; 19–27, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>RefinedWeb Datasheet</h2>

<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 6:</span><span class="ltx_text ltx_font_bold" id="A1.T6.2.1">Datasheet for RefinedWeb</span>은 <cite class="ltx_cite ltx_citemacro_citet">Gebru et al. (<a class="ltx_ref" href="#bib.bib35" title="">2021</a>)</cite>에 의해 소개된 프레임워크를 따른다.</figcaption>
<table id="A1.T6.1" class="ltx_tabular">
<tbody><tr id="A1.T6.1.2" class="ltx_tr">
<td id="A1.T6.1.2.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="A1.T6.1.2.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Motivation</span></td>
</tr>
<tr id="A1.T6.1.3" class="ltx_tr">
<td id="A1.T6.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.3.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.3.1.1.1.1" class="ltx_text ltx_font_bold">For what purpose was the dataset created?</span></span>
</span>
</td>
<td id="A1.T6.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.3.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb was created to serve as a large-scale dataset for the pretraining of large language models. It may be used on its own, or augmented with curated sources (e.g., Wikipedia, StackOverflow).</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.4" class="ltx_tr">
<td id="A1.T6.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.4.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Who created the dataset and on behalf of which entity?</span></span>
</span>
</td>
<td id="A1.T6.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.4.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset was created by the Technology Innovation Institute.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.5" class="ltx_tr">
<td id="A1.T6.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.5.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Who funded the creation of the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.5.2.1.1" class="ltx_p" style="width:284.5pt;">The creation of the dataset was privately funded by the Technology Innovation Institute.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.6" class="ltx_tr">
<td id="A1.T6.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.6.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Any other comment?</span></span>
</span>
</td>
<td id="A1.T6.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.6.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb is built on-top of CommonCrawl, using the Macrodata Refinement Pipeline, which combines content extraction, filtering heuristics, and deduplication. In designing RefinedWeb, we abided to the following philosophy: (1) <span id="A1.T6.1.6.2.1.1.1" class="ltx_text ltx_font_bold">Scale first.</span> We intend MDR to produce datasets to be used to train 40-200B parameters models, thus requiring trillions of tokens <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>. For English-only RefinedWeb, we target a size of 3-6 trillion tokens. Specifically, we eschew any labour intensive human curation process, and focus on CommonCrawl instead of disparate single-domain sources. (2) <span id="A1.T6.1.6.2.1.1.2" class="ltx_text ltx_font_bold">Strict deduplication.</span> Inspired by the work of <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, which demonstrated the value of deduplication for large language models, we implement a rigorous deduplication pipeline. We combine both exact and fuzzy deduplication, and use strict settings leading to removal rates far higher than others have reported. (3) <span id="A1.T6.1.6.2.1.1.3" class="ltx_text ltx_font_bold">Neutral filtering.</span> To avoid introducing further undesirable biases into the model <cite class="ltx_cite ltx_citemacro_citep">(Dodge et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Welbl et&nbsp;al., <a href="#bib.bib80" title="" class="ltx_ref">2021</a>)</cite>, we avoid using ML-based filtering outside of language identification. We stick to simple rules and heuristics, and use only URL filtering for adult content.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.7" class="ltx_tr">
<td id="A1.T6.1.7.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.7.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Composition</span></td>
</tr>
<tr id="A1.T6.1.8" class="ltx_tr">
<td id="A1.T6.1.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.8.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.8.1.1.1.1" class="ltx_text ltx_font_bold">What do the instances that comprise the dataset represent?</span></span>
</span>
</td>
<td id="A1.T6.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.8.2.1.1" class="ltx_p" style="width:284.5pt;">Instances are text-only documents, corresponding to single web pages.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.1" class="ltx_tr">
<td id="A1.T6.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.1.2.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">How many instances are there in total?</span></span>
</span>
</td>
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.1.1.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb contains <math id="A1.T6.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A1.T6.1.1.1.1.1.m1.1a"><mo id="A1.T6.1.1.1.1.1.m1.1.1" xref="A1.T6.1.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="A1.T6.1.1.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.1.1.m1.1c">\sim</annotation></semantics></math>10 billion documents, or around 5 trillion tokens. The public version is a subset representing a tenth of the full version.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.9" class="ltx_tr">
<td id="A1.T6.1.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.9.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.9.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</span></span>
</span>
</td>
<td id="A1.T6.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.9.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb is built using all CommonCrawl dumps until the 2023-06 one; it could be updated with additional dumps as they are released. The public release of RefinedWeb is a 600GT random extract of the 5,000GT of the full dataset. For all experiments, we randomly sampled from the public extract, or earlier development versions of it.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.10" class="ltx_tr">
<td id="A1.T6.1.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.10.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.10.1.1.1.1" class="ltx_text ltx_font_bold">What data does each instance consist of?</span></span>
</span>
</td>
<td id="A1.T6.1.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.10.2.1.1" class="ltx_p" style="width:284.5pt;">Each instance is a text-only document, with metadata about its origin in CommonCrawl and source page URL. We also distribute a multimodal version of RefinedWeb, containing interlaced links to images.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.11" class="ltx_tr">
<td id="A1.T6.1.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.11.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.11.1.1.1.1" class="ltx_text ltx_font_bold">Is there a label or target associated with each instance?</span></span>
</span>
</td>
<td id="A1.T6.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.11.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.12" class="ltx_tr">
<td id="A1.T6.1.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.12.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.12.1.1.1.1" class="ltx_text ltx_font_bold">Is any information missing from individual instances?</span></span>
</span>
</td>
<td id="A1.T6.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.12.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.13" class="ltx_tr">
<td id="A1.T6.1.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.13.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.13.1.1.1.1" class="ltx_text ltx_font_bold">Are relationships between individual instances made explicit?</span></span>
</span>
</td>
<td id="A1.T6.1.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.13.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.14" class="ltx_tr">
<td id="A1.T6.1.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.14.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.14.1.1.1.1" class="ltx_text ltx_font_bold">Are there recommended data splits?</span></span>
</span>
</td>
<td id="A1.T6.1.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.14.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.15" class="ltx_tr">
<td id="A1.T6.1.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.15.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.15.1.1.1.1" class="ltx_text ltx_font_bold">Are there any errors, sources of noise, or redundancies in the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.15.2.1.1" class="ltx_p" style="width:284.5pt;">Despite our best efforts to filter content that does not qualify as natural language, and to deduplicate documents, our pipeline may let through documents that may be considered as errors or redundant.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.16" class="ltx_tr">
<td id="A1.T6.1.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.16.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.16.1.1.1.1" class="ltx_text ltx_font_bold">Is the dataset self-contained, or does it link to or otherwise rely on external resources?</span></span>
</span>
</td>
<td id="A1.T6.1.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.16.2.1.1" class="ltx_p" style="width:284.5pt;">The base version of the dataset is self-contained, but the multimodal version is interlaced with links to images–these are not distributed as part of the dataset, and constitute an external source.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.17" class="ltx_tr">
<td id="A1.T6.1.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.17.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.17.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain data that might be considered confidential?</span></span>
</span>
</td>
<td id="A1.T6.1.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.17.2.1.1" class="ltx_p" style="width:284.5pt;">All documents in RefinedWeb have been publicly available online.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.18" class="ltx_tr">
<td id="A1.T6.1.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.18.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.18.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</span></span>
</span>
</td>
<td id="A1.T6.1.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.18.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, as this type of data is prevalent on the internet, it is likely our dataset contains such content. Notably, we estimate the prevalence of toxic content in the dataset to be similar to The Pile (<a href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>).</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.19" class="ltx_tr">
<td id="A1.T6.1.19.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.19.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Collection</span></td>
</tr>
<tr id="A1.T6.1.20" class="ltx_tr">
<td id="A1.T6.1.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.20.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.20.1.1.1.1" class="ltx_text ltx_font_bold">How was the data associated with each instance acquired?</span></span>
</span>
</td>
<td id="A1.T6.1.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.20.2.1.1" class="ltx_p" style="width:284.5pt;">We downloaded with <span id="A1.T6.1.20.2.1.1.1" class="ltx_text ltx_font_typewriter">warcio</span> publicly available .WET files from the CommonCrawl foundation.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.21" class="ltx_tr">
<td id="A1.T6.1.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.21.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.21.1.1.1.1" class="ltx_text ltx_font_bold">What mechanisms or procedures were used to collect the data?</span></span>
</span>
</td>
<td id="A1.T6.1.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.21.2.1.1" class="ltx_p" style="width:284.5pt;">We refer to the CommonCrawl website (<a href="commoncrawl.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">commoncrawl.org</a>) for details on how they collect data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.22" class="ltx_tr">
<td id="A1.T6.1.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.22.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.22.1.1.1.1" class="ltx_text ltx_font_bold">If the dataset is a sample from a larger set, what was the sampling strategy?</span></span>
</span>
</td>
<td id="A1.T6.1.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.22.2.1.1" class="ltx_p" style="width:284.5pt;">Whenever we use subsets, we randomly sample from the original data.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.23" class="ltx_tr">
<td id="A1.T6.1.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.23.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.23.1.1.1.1" class="ltx_text ltx_font_bold">Who was involved in the data collection process and how were they compensated?</span></span>
</span>
</td>
<td id="A1.T6.1.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.23.2.1.1" class="ltx_p" style="width:284.5pt;">The original data collection was performed by CommonCrawl; authors from this paper were involved in retrieving it and preparing it.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.24" class="ltx_tr">
<td id="A1.T6.1.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.24.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.24.1.1.1.1" class="ltx_text ltx_font_bold">Over what timeframe was the data collected?</span></span>
</span>
</td>
<td id="A1.T6.1.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.24.2.1.1" class="ltx_p" style="width:284.5pt;">We use all CommonCrawl dumps from 2008 to January/February 2023.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.25" class="ltx_tr">
<td id="A1.T6.1.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.25.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.25.1.1.1.1" class="ltx_text ltx_font_bold">Were any ethical review processes conducted?</span></span>
</span>
</td>
<td id="A1.T6.1.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.25.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.26" class="ltx_tr">
<td id="A1.T6.1.26.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.26.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Preprocessing</span></td>
</tr>
<tr id="A1.T6.1.27" class="ltx_tr">
<td id="A1.T6.1.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.27.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.27.1.1.1.1" class="ltx_text ltx_font_bold">Was any preprocessing/cleaning/labeling of the data done?</span></span>
</span>
</td>
<td id="A1.T6.1.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.27.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, we applied extensive preprocessing and cleaning of the data. We first filter URLs to remove adult content using a blocklist and a score system (<a href="#A7.SS1" title="G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.1</span></a>), we then use <span id="A1.T6.1.27.2.1.1.1" class="ltx_text ltx_font_typewriter">trafilatura</span> <cite class="ltx_cite ltx_citemacro_citep">(Barbaresi, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> to extract content from pages, and perform language identification with the <span id="A1.T6.1.27.2.1.1.2" class="ltx_text ltx_font_typewriter">fastText</span> classifier from CCNet <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite>. After this first preprocessing stage, we filter data using heuristics from MassiveWeb&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite> and our own line-wise corrections (<a href="#A7.SS2" title="G.2 Line-wise filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">G.2</span></a>). Finally, we run extensive deduplication, removing URLs revisited across dumps (<a href="#S3.SS3" title="3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.3</span></a>) and performing subsequently fuzzy and exact substring deduplication, with each stage drawing from <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. See <a href="#S3" title="3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> for further details and <a href="#S3.T2" title="In Language identification. ‣ 3.1 Document preparation: reading data, filtering URLs, extracting text, and language identification ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> for an outline.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.28" class="ltx_tr">
<td id="A1.T6.1.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.28.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.28.1.1.1.1" class="ltx_text ltx_font_bold">Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data?</span></span>
</span>
</td>
<td id="A1.T6.1.28.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.28.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.28.2.1.1" class="ltx_p" style="width:284.5pt;">During development, we saved intermediary outputs from our pipeline for investigations and for ablations–intermediary outputs exist for about 5% of RefinedWeb. We did not keep intermediary outputs for the final production version of the dataset due to storage and resource constraints.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.29" class="ltx_tr">
<td id="A1.T6.1.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.29.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.29.1.1.1.1" class="ltx_text ltx_font_bold">Is the software that was used to preprocess/clean/label the data available?</span></span>
</span>
</td>
<td id="A1.T6.1.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.29.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.30" class="ltx_tr">
<td id="A1.T6.1.30.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.30.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Uses</span></td>
</tr>
<tr id="A1.T6.1.31" class="ltx_tr">
<td id="A1.T6.1.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.31.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.31.1.1.1.1" class="ltx_text ltx_font_bold">Has the dataset been used for any tasks already?</span></span>
</span>
</td>
<td id="A1.T6.1.31.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.31.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.31.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, this data has been used to develop large language models: both for scientific experiments (e.g., this paper) and production use.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.32" class="ltx_tr">
<td id="A1.T6.1.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.32.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.32.1.1.1.1" class="ltx_text ltx_font_bold">Is there a repository that links to any or all papers or systems that use the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.32.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.32.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.32.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.33" class="ltx_tr">
<td id="A1.T6.1.33.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.33.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.33.1.1.1.1" class="ltx_text ltx_font_bold">What (other) tasks could the dataset be used for?</span></span>
</span>
</td>
<td id="A1.T6.1.33.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.33.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.33.2.1.1" class="ltx_p" style="width:284.5pt;">RefinedWeb was built as a large-scale corpora representative of the web, and as such may see many downstream uses which are difficult to predict.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.34" class="ltx_tr">
<td id="A1.T6.1.34.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.34.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.34.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.34.1.1.1.1" class="ltx_text ltx_font_bold">Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</span></span>
</span>
</td>
<td id="A1.T6.1.34.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.34.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.34.2.1.1" class="ltx_p" style="width:284.5pt;">For the public extract of RefinedWeb, we chose to only draw from the English version of the dataset, preventing multilingual applications.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.35" class="ltx_tr">
<td id="A1.T6.1.35.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.35.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.35.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.35.1.1.1.1" class="ltx_text ltx_font_bold">Are there tasks for which the dataset should not be used?</span></span>
</span>
</td>
<td id="A1.T6.1.35.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.35.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.35.2.1.1" class="ltx_p" style="width:284.5pt;">Any tasks which may considered irresponsible or harmful.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.36" class="ltx_tr">
<td id="A1.T6.1.36.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.36.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Distribution</span></td>
</tr>
<tr id="A1.T6.1.37" class="ltx_tr">
<td id="A1.T6.1.37.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.37.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.37.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.37.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?</span></span>
</span>
</td>
<td id="A1.T6.1.37.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.37.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.37.2.1.1" class="ltx_p" style="width:284.5pt;">Yes, we make a 600GT extract publicly available for NLP practitioners. We currently don’t plan to share the full version of the dataset.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.38" class="ltx_tr">
<td id="A1.T6.1.38.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.38.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.38.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.38.1.1.1.1" class="ltx_text ltx_font_bold">How will the dataset will be distributed?</span></span>
</span>
</td>
<td id="A1.T6.1.38.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.38.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.38.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset will be made available through the HuggingFace Hub.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.39" class="ltx_tr">
<td id="A1.T6.1.39.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.39.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.39.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.39.1.1.1.1" class="ltx_text ltx_font_bold">When will the dataset be distributed?</span></span>
</span>
</td>
<td id="A1.T6.1.39.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.39.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.39.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset is available immediately.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.40" class="ltx_tr">
<td id="A1.T6.1.40.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.40.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.40.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.40.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?</span></span>
</span>
</td>
<td id="A1.T6.1.40.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.40.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.40.2.1.1" class="ltx_p" style="width:284.5pt;">The public extract is made available under an ODC-By 1.0 license; users should also abide to the CommonCrawl ToU: <a target="_blank" href="https://commoncrawl.org/terms-of-use/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://commoncrawl.org/terms-of-use/</a>.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.41" class="ltx_tr">
<td id="A1.T6.1.41.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.41.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.41.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.41.1.1.1.1" class="ltx_text ltx_font_bold">Have any third parties imposed IP-based or other restrictions on the data associated with the instances?</span></span>
</span>
</td>
<td id="A1.T6.1.41.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.41.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.41.2.1.1" class="ltx_p" style="width:284.5pt;">Not to our knowledge.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.42" class="ltx_tr">
<td id="A1.T6.1.42.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.42.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.42.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.42.1.1.1.1" class="ltx_text ltx_font_bold">Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?</span></span>
</span>
</td>
<td id="A1.T6.1.42.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.42.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.42.2.1.1" class="ltx_p" style="width:284.5pt;">Not to our knowledge.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.43" class="ltx_tr">
<td id="A1.T6.1.43.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A1.T6.1.43.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Maintenance</span></td>
</tr>
<tr id="A1.T6.1.44" class="ltx_tr">
<td id="A1.T6.1.44.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.44.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.44.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.44.1.1.1.1" class="ltx_text ltx_font_bold">Who will be supporting/hosting/maintaining the dataset?</span></span>
</span>
</td>
<td id="A1.T6.1.44.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.44.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.44.2.1.1" class="ltx_p" style="width:284.5pt;">The dataset will be hosted on the HuggingFace Hub, we have no plans to further support or maintain it once it is released.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.45" class="ltx_tr">
<td id="A1.T6.1.45.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.45.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.45.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.45.1.1.1.1" class="ltx_text ltx_font_bold">How can the owner/curator/manager of the dataset be contacted?</span></span>
</span>
</td>
<td id="A1.T6.1.45.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.45.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.45.2.1.1" class="ltx_p" style="width:284.5pt;">falconllm@tii.ae</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.46" class="ltx_tr">
<td id="A1.T6.1.46.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.46.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.46.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.46.1.1.1.1" class="ltx_text ltx_font_bold">Is there an erratum?</span></span>
</span>
</td>
<td id="A1.T6.1.46.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.46.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.46.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.47" class="ltx_tr">
<td id="A1.T6.1.47.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A1.T6.1.47.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.47.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.47.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be updated?</span></span>
</span>
</td>
<td id="A1.T6.1.47.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.1.47.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.47.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
<tr id="A1.T6.1.48" class="ltx_tr">
<td id="A1.T6.1.48.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A1.T6.1.48.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.48.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A1.T6.1.48.1.1.1.1" class="ltx_text ltx_font_bold">If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?</span></span>
</span>
</td>
<td id="A1.T6.1.48.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T6.1.48.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.1.48.2.1.1" class="ltx_p" style="width:284.5pt;">No.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Falcon-RW Model Cards</h2>

<figure id="A2.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 7:</span><span class="ltx_text ltx_font_bold" id="A2.T7.1.1">Model card for Falcon-RW</span>은 <cite class="ltx_cite ltx_citemacro_citet">Mitchell et al. (<a class="ltx_ref" href="#bib.bib55" title="">2019</a>)</cite>에 의해 소개된 프레임워크를 따른다.</figcaption>
<table id="A2.T7.3" class="ltx_tabular">
<tbody><tr id="A2.T7.3.1" class="ltx_tr">
<td id="A2.T7.3.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="A2.T7.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Model details</span></td>
</tr>
<tr id="A2.T7.3.2" class="ltx_tr">
<td id="A2.T7.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.2.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Person/organization developing the model</span></span>
</span>
</td>
<td id="A2.T7.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.2.2.1.1" class="ltx_p" style="width:284.5pt;">The models were created by the Technology Innovation Institute.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.3" class="ltx_tr">
<td id="A2.T7.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.3.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Model date</span></span>
</span>
</td>
<td id="A2.T7.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.3.2.1.1" class="ltx_p" style="width:284.5pt;">Falcon-RW models were trained in December 2022/January 2023.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.4" class="ltx_tr">
<td id="A2.T7.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.4.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.4.1.1.1.1" class="ltx_text ltx_font_bold">Model type and information about training</span></span>
</span>
</td>
<td id="A2.T7.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.4.2.1.1" class="ltx_p" style="width:284.5pt;">Falcon-RW are autoregressive Transformer models trained with a causal language modeling objective. Architecture based on GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, with ALiBi positional encodings <cite class="ltx_cite ltx_citemacro_citep">(Press et&nbsp;al., <a href="#bib.bib60" title="" class="ltx_ref">2021</a>)</cite> and&nbsp;FlashAttention <cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. See <a href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1</span></a> for details.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.5" class="ltx_tr">
<td id="A2.T7.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.5.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.5.1.1.1.1" class="ltx_text ltx_font_bold">Licence</span></span>
</span>
</td>
<td id="A2.T7.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.5.2.1.1" class="ltx_p" style="width:284.5pt;">Apache 2.0: <a target="_blank" href="https://www.apache.org/licenses/LICENSE-2.0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.apache.org/licenses/LICENSE-2.0</a>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.6" class="ltx_tr">
<td id="A2.T7.3.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.6.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.6.1.1.1.1" class="ltx_text ltx_font_bold">Point of contact</span></span>
</span>
</td>
<td id="A2.T7.3.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.6.2.1.1" class="ltx_p" style="width:284.5pt;">falconllm@tii.ae</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.7" class="ltx_tr">
<td id="A2.T7.3.7.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.7.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Intended use</span></td>
</tr>
<tr id="A2.T7.3.8" class="ltx_tr">
<td id="A2.T7.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.8.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.8.1.1.1.1" class="ltx_text ltx_font_bold">Primary intended uses</span></span>
</span>
</td>
<td id="A2.T7.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.8.2.1.1" class="ltx_p" style="width:284.5pt;">Research on large language models, and the influence of adequately filtered and deduplicated web data on the properties of large language models (fairness, safety, limitations, capabilities, etc.).</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.9" class="ltx_tr">
<td id="A2.T7.3.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.9.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.9.1.1.1.1" class="ltx_text ltx_font_bold">Primary intended users</span></span>
</span>
</td>
<td id="A2.T7.3.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.9.2.1.1" class="ltx_p" style="width:284.5pt;">NLP researchers.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.10" class="ltx_tr">
<td id="A2.T7.3.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.10.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.10.1.1.1.1" class="ltx_text ltx_font_bold">Out-of-scope use cases</span></span>
</span>
</td>
<td id="A2.T7.3.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.10.2.1.1" class="ltx_p" style="width:284.5pt;">Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.11" class="ltx_tr">
<td id="A2.T7.3.11.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.11.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Factors</span></td>
</tr>
<tr id="A2.T7.3.12" class="ltx_tr">
<td id="A2.T7.3.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.12.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.12.1.1.1.1" class="ltx_text ltx_font_bold">Relevant factors</span></span>
</span>
</td>
<td id="A2.T7.3.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.12.2.1.1" class="ltx_p" style="width:284.5pt;">Falcon-RW models are trained on English data only, and will not generalize appropriately to other languages. Furthermore, as they are trained on a large-scale corpora representative of the web, they will carry the stereotypes and biases commonly encountered online.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.13" class="ltx_tr">
<td id="A2.T7.3.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.13.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.13.1.1.1.1" class="ltx_text ltx_font_bold">Evaluation factors</span></span>
</span>
</td>
<td id="A2.T7.3.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.13.2.1.1" class="ltx_p" style="width:284.5pt;">We evaluated the toxicity of the underlying pretraining dataset and found it to be in line with common curated pretraining datasets such as The Pile (see <a href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>). Note that this only accounts for toxicity under the definition of Perspective API: ”content that is rude or disrespectful”. Notably, this fails to include concerns about social biases or harmfulness.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.14" class="ltx_tr">
<td id="A2.T7.3.14.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.14.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Metrics</span></td>
</tr>
<tr id="A2.T7.3.15" class="ltx_tr">
<td id="A2.T7.3.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.15.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.15.1.1.1.1" class="ltx_text ltx_font_bold">Model performance measures</span></span>
</span>
</td>
<td id="A2.T7.3.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.15.2.1.1" class="ltx_p" style="width:284.5pt;">We focus our evaluation on measuring the zero-shot generalization capabilities of our models across a wide range of tasks, leveraging the Eleuther AI language model evaluation harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.16" class="ltx_tr">
<td id="A2.T7.3.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.16.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.16.1.1.1.1" class="ltx_text ltx_font_bold">Variation approaches</span></span>
</span>
</td>
<td id="A2.T7.3.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.16.2.1.1" class="ltx_p" style="width:284.5pt;">Due to the costs associated with training Falcon-RW we cannot train the models multiple times and measure variability across training runs.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.17" class="ltx_tr">
<td id="A2.T7.3.17.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.17.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Evaluation data</span></td>
</tr>
<tr id="A2.T7.3.18" class="ltx_tr">
<td id="A2.T7.3.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.18.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.18.1.1.1.1" class="ltx_text ltx_font_bold">Datasets</span></span>
</span>
</td>
<td id="A2.T7.3.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.18.2.1.1" class="ltx_p" style="width:284.5pt;">We evaluate zero-shot accuracy on 18 varied tasks, detailed in <a href="#S3.T3" title="In 3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.19" class="ltx_tr">
<td id="A2.T7.3.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.19.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.19.1.1.1.1" class="ltx_text ltx_font_bold">Motivation</span></span>
</span>
</td>
<td id="A2.T7.3.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.19.2.1.1" class="ltx_p" style="width:284.5pt;">We selected and aggregated tasks to build comparisons with other models in the literature (see <a href="#S4.SS1" title="4.1 Setting ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1</span></a>; <a href="#A6.SS1" title="F.1 Task aggregates ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">F.1</span></a> for details).</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.20" class="ltx_tr">
<td id="A2.T7.3.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T7.3.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.20.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T7.3.20.1.1.1.1" class="ltx_text ltx_font_bold">Preprocessing</span></span>
</span>
</td>
<td id="A2.T7.3.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A2.T7.3.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T7.3.20.2.1.1" class="ltx_p" style="width:284.5pt;">We use the default prompts and setup of <cite class="ltx_cite ltx_citemacro_citet">Gao et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="A2.T7.3.21" class="ltx_tr">
<td id="A2.T7.3.21.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T7.3.21.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Training data</span></td>
</tr>
<tr id="A2.T7.3.22" class="ltx_tr">
<td id="A2.T7.3.22.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_bb ltx_border_t" colspan="2"><span id="A2.T7.3.22.1.1" class="ltx_text ltx_font_bold">See the dedicated datasheet in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:datasheet</span>.</span></td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Dataset analysis</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p" id="A3.p1.1">웹 코퍼스의 규모가 크고 다양하기 때문에 전체적으로 문서화 및 분석이 어려우며, 섹션에서는 <a class="ltx_ref" href="#A3.F5.sf1" title="In Figure 5 ‣ Appendix C Dataset analysis ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a>의 문서 길이와 <a class="ltx_ref" href="#A3.F5.sf2" title="In Figure 5 ‣ Appendix C Dataset analysis ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>의 최상위 도메인 이름 분류를 중심으로 몇 가지 주요 메트릭을 제공한다. 또한 <a class="ltx_ref" href="#S6.F4" title="In 6 Conclusion ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>에 제시된 독성 함량의 분포 분석을 참조한다.</p>
</div>
<figure id="A3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x6.png" id="A3.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="184" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F5.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A3.F5.sf1.3.2" class="ltx_text" style="font-size:80%;">Document Lengths</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x7.png" id="A3.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="231" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F5.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A3.F5.sf2.3.2" class="ltx_text" style="font-size:80%;">Top domains</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 5:</span><span class="ltx_text ltx_font_bold" id="A3.F5.6.1">Make-up of <span class="ltx_text" id="A3.F5.6.1.1" style="color:#DB57B2;">RefinedWeb</span> in document length (left) and top domains (right). </span> (a) OSCAR 데이터 세트 및 <span class="ltx_text" id="A3.F5.7.2" 스타일="color:#5E57D3;">RW-Raw</span>이 유사한 문서 길이 분포를 가짐을 발견합니다. 필터링 후 대부분의 짧은 문서는 <span class="ltx_text" id="A3.F5.8.3" 스타일="color:#B55DD4;">RW-Filtered</span>에서 폐기됩니다. 중복제거가 스팬을 제거함에 따라 더 짧은 문서를 <span class="ltx_text" id="A3.F5.9.4" style="color:#DB57B2;">RefinedWeb</span>으로 다시 도입합니다. C4와 RefinedWeb의 구성은 RefinedWeb에 대한 짧은 문서의 더 긴 꼬리와 함께 비교적 유사하다는 점에 주목한다. 마지막으로 <span class="ltx_text" id="A3.F5.10.5" style="color:#7DD86E;">The Pile</span>은 긴(책 등) 문서와 짧은 문서의 긴 꼬리를 가진 독특한 메이크업을 보여줍니다. (b) RefinedWeb의 상위 도메인은 인기 있는 콘텐츠 플랫폼(Blogspot, WordPress, Tumblr 등), 뉴스 웹사이트(CNN, New York Times 등)에 걸쳐 있으며, BioMed Central 또는 Springer와 같은 기술 콘텐츠도 포함한다.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Multilingual RefinedWeb</h2>

<section id="A4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multilingual data.</h5>

<div id="A4.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p1.1">언어 식별 필터를 사용하여 처리된 CommonCrawl 데이터를 176개의 언어로 분류한다. 그림 <a class="ltx_ref" href="#A4.F6" title="Figure 6 ‣ Multilingual data. ‣ Appendix D Multilingual RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">6</span></a>는 데이터 <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px1.p1.1.1">excluding English</span>에 존재하는 상위 20개 언어를 내림차순으로 상대적 기여도를 기준으로 보여준다. 가공된 CommonCrawl 자료에서 전체 문서의 58.20%가 영어로 확인되었다. 커먼크롤의 언어 분포는 언어 화자의 전 세계 분포 <cite class="ltx_cite ltx_citemacro_citep">(Eberhard et al., <a class="ltx_ref" href="#bib.bib32" title="">2023</a>)</cite>와 부분적으로만 일치함을 알 수 있다: 러시아어는 과대 대표되고(CC에서는 2위이지만 전 세계 8위), 중국어는 과소 대표되고(CC에서는 6~7위이지만 전 세계 2위), 힌디어는 3번째로 많이 사용되었음에도 불구하고 상위 20위 안에 나타나지 않는다.</p>
</div>
<figure id="A4.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x8.png" id="A4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 6:</span><span class="ltx_text ltx_font_bold" id="A4.F6.2.1">문서 수 및 디스크 크기에 따라 처리된 CommonCrawl에서 상위 20개 언어(영어 제외)입니다. </span></figcaption>
</figure>
</section>
<section id="A4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Processing multilingual data.</h5>

<div id="A4.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A4.SS0.SSS0.Px2.p1.1">MDR 파이프라인은 모든 언어를 처리하는데 사용될 수 있다: 텍스트 추출과 같은 특징은 언어-불가지론적인 반면, 라인-와이즈 보정과 같은 특정 필터는 일반적으로 각각의 개별 언어에 대해 튜닝될 필요가 있다. 또한 개별 언어에 대한 중복 제거 매개 변수를 조정하는 것이 유용하다는 것을 발견했습니다.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Additional results</h2>

<div id="A5.p1" class="ltx_para">
<p class="ltx_p" id="A5.p1.1">이 섹션에서는 매크로데이터 정제 파이프라인을 개발하는 동안 얻은 추가 결과를 제시한다. <a class="ltx_ref" href="#A5.SS1" title="E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">E.1</span></a> 및 <a class="ltx_ref" href="#A5.SS3" title="E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">E.3</span></a>의 경우 데이터 세트의 이전 개발 버전을 사용하여 얻었으므로 결과가 본문과 직접 비교할 수 없다. <a class="ltx_ref" href="#A5.SS2" title="E.2 Language modeling evaluation ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">E.2</span></a>의 경우, 이는 Falcon-RW 모델을 기반으로 한다.</p>
</div>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Small-scale ablations on deduplication approaches</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A5.SS1.p1.1"><a class="ltx_ref" href="#A5.T8" title="In E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>–설정은 30GT에 대한 1B 모델을 훈련하는 이전 제거와 유사하다. 우리는 그것을 관찰합니다</p>
<ul id="A5.I1" class="ltx_itemize">
<li id="A5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A5.I1.i1.p1.1.1">MinHash alone is insufficient</span> as it doesn't match the zero-shot performance of exact deduplication. 반대로, 이를 정확한 중복제거와 결합한다고 해서 성능이 더 향상되지는 않는다.</p>
</div>
</li>
<li id="A5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A5.I1.i2.p1.1.1">Masking spanned duplicates degrades performance</span>, systematically underperforming other approaches. 드롭핑과 컷팅 스팬은 유사하게 수행되지만, 문서를 드롭하는 것이 컷팅보다 약간 더 우수할 수 있습니다.</p>
</div>
</li>
</ul>
</div>
<div id="A5.SS1.p2" class="ltx_para">
<p class="ltx_p" id="A5.SS1.p2.1">마지막으로, 스케일링이 더 쉽기 때문에 정확한 중복제거 전에 MinHash를 적용하기로 결정했다: 대략적인 중복제거는 가지치기 단계로 작용하여 중복제거를 더 확장할 수 있다. 마지막으로, 우리는 경간을 줄이는 일반적인 옵션을 선택하는데, 이는 하락으로 인해 5조 토큰 수집 능력이 손상될 훨씬 더 엄격한 거부율이 발생했기 때문이다.</p>
</div>
<figure id="A5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 8:</span><span class="ltx_text ltx_font_bold" id="A5.T8.11.1">MinHash 단독으로는 정확한 부분 문자열 중복 제거의 성능을 맞추기에 불충분하며, 이 둘을 결합해도 성능이 크게 향상되지 않는다. 모든 정확한 부분 문자열 접근법 중 중복된 마스킹 스팬은 성능이 낮지만 다른 모든 방법은 유사한 성능을 보인다. </span> <math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T8.2.m1.1"><semantics id="A5.T8.2.m1.1b"><mi id="A5.T8.2.m1.1.1" mathvariant="normal" xref="A5.T8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T8.2.m1.1c"><ci id="A5.T8.2.m1.1.1.cmml" xref="A5.T8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.2.m1.1d">\checkmark</annotation></semantics></math> Minhash + Exact substring-Cut은 최종 중복 제거 설정에 해당 합니다. Perplexity in bits-per-bytes on The Pile (<span class="ltx_text ltx_font_typewriter" id="A5.T8.12.2">pile-bpb</span>, lower is better), zero-shot performance aggregated over LAMBADA, PIQA and HellaSwag (<span class="ltx_text ltx_font_typewriter" id="A5.T8.13.3">agg-dev</span>). <span class="ltx_text ltx_font_bold" id="A5.T8.14.4">bold</span>, <span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T8.15.5">underline</span>, <span class="ltx_text ltx_font_typewriter" id="A5.T8.16.6">agg-dev-1</span>에서 가장 좋은 결과입니다.</figcaption>
<p id="A5.T8.4" class="ltx_p ltx_align_center ltx_align_center"><span id="A5.T8.4.2" class="ltx_text">
<span id="A5.T8.4.2.2" class="ltx_tabular ltx_align_middle">
<span id="A5.T8.4.2.2.2" class="ltx_tr">
<span id="A5.T8.4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.4.2.2.2.3.1" class="ltx_text ltx_font_bold">Minhash</span></span>
<span id="A5.T8.4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.4.2.2.2.4.1" class="ltx_text ltx_font_bold">Exact substring</span></span>
<span id="A5.T8.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.3.1.1.1.1.1" class="ltx_text ltx_font_typewriter">pile-bpb</span> <math id="A5.T8.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A5.T8.3.1.1.1.1.m1.1a"><mo stretchy="false" id="A5.T8.3.1.1.1.1.m1.1.1" xref="A5.T8.3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A5.T8.3.1.1.1.1.m1.1b"><ci id="A5.T8.3.1.1.1.1.m1.1.1.cmml" xref="A5.T8.3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="A5.T8.4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T8.4.2.2.2.2.1" class="ltx_text ltx_font_typewriter">agg-dev-1</span> <math id="A5.T8.4.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A5.T8.4.2.2.2.2.m1.1a"><mo stretchy="false" id="A5.T8.4.2.2.2.2.m1.1.1" xref="A5.T8.4.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A5.T8.4.2.2.2.2.m1.1b"><ci id="A5.T8.4.2.2.2.2.m1.1.1.cmml" xref="A5.T8.4.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T8.4.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math></span></span>
<span id="A5.T8.4.2.2.3" class="ltx_tr">
<span id="A5.T8.4.2.2.3.1" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2"><span id="A5.T8.4.2.2.3.1.1" class="ltx_text" style="color:#B55DD4;">RefinedWeb-Filtered</span></span>
<span id="A5.T8.4.2.2.3.2" class="ltx_td ltx_align_center ltx_border_t">1.11</span>
<span id="A5.T8.4.2.2.3.3" class="ltx_td ltx_align_center ltx_border_t">43.51</span></span>
<span id="A5.T8.4.2.2.4" class="ltx_tr">
<span id="A5.T8.4.2.2.4.1" class="ltx_td ltx_border_t"></span>
<span id="A5.T8.4.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t">Mask</span>
<span id="A5.T8.4.2.2.4.3" class="ltx_td ltx_align_center ltx_border_t">1.08</span>
<span id="A5.T8.4.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t">45.84</span></span>
<span id="A5.T8.4.2.2.5" class="ltx_tr">
<span id="A5.T8.4.2.2.5.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.5.2" class="ltx_td ltx_align_center">Mask</span>
<span id="A5.T8.4.2.2.5.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.5.4" class="ltx_td ltx_align_center">46.28</span></span>
<span id="A5.T8.4.2.2.6" class="ltx_tr">
<span id="A5.T8.4.2.2.6.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.6.2" class="ltx_td"></span>
<span id="A5.T8.4.2.2.6.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.6.4" class="ltx_td ltx_align_center">46.57</span></span>
<span id="A5.T8.4.2.2.7" class="ltx_tr">
<span id="A5.T8.4.2.2.7.1" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.7.1.1" class="ltx_text" style="color:#DB57B2;">✓</span></span>
<span id="A5.T8.4.2.2.7.2" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.7.2.1" class="ltx_text" style="color:#DB57B2;">Cut</span></span>
<span id="A5.T8.4.2.2.7.3" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.7.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">1.05</span></span>
<span id="A5.T8.4.2.2.7.4" class="ltx_td ltx_align_center">47.11</span></span>
<span id="A5.T8.4.2.2.8" class="ltx_tr">
<span id="A5.T8.4.2.2.8.1" class="ltx_td"></span>
<span id="A5.T8.4.2.2.8.2" class="ltx_td ltx_align_center">Cut</span>
<span id="A5.T8.4.2.2.8.3" class="ltx_td ltx_align_center">1.06</span>
<span id="A5.T8.4.2.2.8.4" class="ltx_td ltx_align_center">47.24</span></span>
<span id="A5.T8.4.2.2.9" class="ltx_tr">
<span id="A5.T8.4.2.2.9.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.9.2" class="ltx_td ltx_align_center">Drop partial</span>
<span id="A5.T8.4.2.2.9.3" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.9.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">1.05</span></span>
<span id="A5.T8.4.2.2.9.4" class="ltx_td ltx_align_center">47.25</span></span>
<span id="A5.T8.4.2.2.10" class="ltx_tr">
<span id="A5.T8.4.2.2.10.1" class="ltx_td"></span>
<span id="A5.T8.4.2.2.10.2" class="ltx_td ltx_align_center">Drop any</span>
<span id="A5.T8.4.2.2.10.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.10.4" class="ltx_td ltx_align_center">47.77</span></span>
<span id="A5.T8.4.2.2.11" class="ltx_tr">
<span id="A5.T8.4.2.2.11.1" class="ltx_td ltx_align_center">✓</span>
<span id="A5.T8.4.2.2.11.2" class="ltx_td ltx_align_center">Drop any</span>
<span id="A5.T8.4.2.2.11.3" class="ltx_td ltx_align_center">1.07</span>
<span id="A5.T8.4.2.2.11.4" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.11.4.1" class="ltx_text ltx_framed ltx_framed_underline">47.86</span></span></span>
<span id="A5.T8.4.2.2.12" class="ltx_tr">
<span id="A5.T8.4.2.2.12.1" class="ltx_td"></span>
<span id="A5.T8.4.2.2.12.2" class="ltx_td ltx_align_center">Drop partial</span>
<span id="A5.T8.4.2.2.12.3" class="ltx_td ltx_align_center">1.06</span>
<span id="A5.T8.4.2.2.12.4" class="ltx_td ltx_align_center"><span id="A5.T8.4.2.2.12.4.1" class="ltx_text ltx_font_bold">47.97</span></span></span>
<span id="A5.T8.4.2.2.13" class="ltx_tr">
<span id="A5.T8.4.2.2.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t ltx_colspan ltx_colspan_2"><span id="A5.T8.4.2.2.13.1.1" class="ltx_text" style="color:#7DD86E;">Pile</span></span>
<span id="A5.T8.4.2.2.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.88</span>
<span id="A5.T8.4.2.2.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">43.70</span></span>
</span></span></p>
</figure>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Language modeling evaluation</h3>

<div id="A5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A5.SS2.p1.1">또한 Wikitext (<a class="ltx_ref" href="#A5.T9" title="In E.2 Language modeling evaluation ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a>)에 대한 복잡도를 평가하였다. 우리는 RefinedWeb에서 훈련된 모델이 The Pile에서 훈련된 모델에 가까운 성능을 달성한다는 것을 발견했다. 중요하게도, RefinedWeb에는 위키피디아의 콘텐츠가 포함되어 있지 않으며, 이는 URL 수준에서 명시적으로 필터링됩니다. 우리는 RW 모델이 위키텍스트의 특징(예: 기사 레이아웃 등)에 익숙하지 않을 수 있기 때문에 이것이 복잡성의 대부분의 차이를 설명한다고 믿는다.</p>
</div>
<figure id="A5.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9:</span><span class="ltx_text ltx_font_bold" id="A5.T9.4.1">Models trained on <span class="ltx_text" id="A5.T9.4.1.1" style="color:#DB57B2;">RefinedWeb</span> achieve performance close to models trained on <span class="ltx_text" id="A5.T9.4.1.2" style="color:#7DD86E;">The Pile</span> on Wikitext, despite not seen any content from Wikipedia. </span> Perplexity in bits-per-bytes on Wikitext (<span class="ltx_text ltx_font_typewriter" id="A5.T9.5.2">wiki-bpb</span>, lower is better.)</figcaption>
<p id="A5.T9.1" class="ltx_p ltx_align_center ltx_align_center"><span id="A5.T9.1.1" class="ltx_text">
<span id="A5.T9.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A5.T9.1.1.1.2" class="ltx_tr">
<span id="A5.T9.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A5.T9.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Model size</span></span>
<span id="A5.T9.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T9.1.1.1.2.2.1" class="ltx_text ltx_font_bold">1B</span></span>
<span id="A5.T9.1.1.1.2.3" class="ltx_td ltx_border_tt"></span>
<span id="A5.T9.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A5.T9.1.1.1.2.4.1" class="ltx_text ltx_font_bold">7B</span></span></span>
<span id="A5.T9.1.1.1.3" class="ltx_tr">
<span id="A5.T9.1.1.1.3.1" class="ltx_td ltx_align_left"><span id="A5.T9.1.1.1.3.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
<span id="A5.T9.1.1.1.3.2" class="ltx_td ltx_align_center"><span id="A5.T9.1.1.1.3.2.1" class="ltx_text ltx_font_bold" style="color:#7DD86E;">The Pile</span></span>
<span id="A5.T9.1.1.1.3.3" class="ltx_td ltx_align_center"><span id="A5.T9.1.1.1.3.3.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">RW</span></span>
<span id="A5.T9.1.1.1.3.4" class="ltx_td ltx_align_center"><span id="A5.T9.1.1.1.3.4.1" class="ltx_text ltx_font_bold" style="color:#DB57B2;">RW</span></span></span>
<span id="A5.T9.1.1.1.1" class="ltx_tr">
<span id="A5.T9.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A5.T9.1.1.1.1.1.1" class="ltx_text ltx_font_typewriter">wiki-bpb</span> <math id="A5.T9.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A5.T9.1.1.1.1.1.m1.1a"><mo stretchy="false" id="A5.T9.1.1.1.1.1.m1.1.1" xref="A5.T9.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A5.T9.1.1.1.1.1.m1.1b"><ci id="A5.T9.1.1.1.1.1.m1.1.1.cmml" xref="A5.T9.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T9.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="A5.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.64</span>
<span id="A5.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.66</span>
<span id="A5.T9.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.60</span></span>
</span></span></p>
</figure>
</section>
<section id="A5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.3 </span>Does deduplication help with multiple epochs?</h3>

<div id="A5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A5.SS3.p1.1">이 작업에서 우리는 사전 훈련 데이터를 확장하기 위해 실무자들이 두 가지 선택을 했다고 설명했다: (1) 우리가 추구하기로 선택한 방법인 데이터 수집을 개선하고, (2) 동일한 데이터의 여러 시대에 모델을 훈련시킨다. <cite class="ltx_cite ltx_citemacro_citep">(Hernandez et al., <a class="ltx_ref" href="#bib.bib40" title="">2022</a>)</cite>의 부작용 없이 여러 epoch를 유지할 수 있는 더 큰 모델의 능력의 현재 불확실성으로 인해 (1)에 초점을 맞췄다. (2)에 관한 상당히 합리적인 질문은 중복 제거가 상황을 개선할 수 있는지 여부와 중복 제거된 데이터가 모델 품질을 손상시키지 않으면서 더 많은 시기를 유지할 수 있는지 여부이다.</p>
</div>
<div id="A5.SS3.p2" class="ltx_para">
<p class="ltx_p" id="A5.SS3.p2.1">우리는 RW 및 RW 필터링의 30GT에서 1B 매개변수 모델을 훈련한다. 우리는 사전 훈련 토큰의 수를 고정시키지만 1, 5, 25, 100 에포크로 훈련한다. 이것은 확정적인 결과를 얻기 위해 개선되어야 하는 소규모의 제한된 설정이다. 우리는 <a class="ltx_ref" href="#A5.F7.sf1" title="In Figure 7 ‣ E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7(a)</span></a>의 단일 에폭과 <a class="ltx_ref" href="#A5.F7.sf2" title="In Figure 7 ‣ E.3 Does deduplication help with multiple epochs? ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7(b)</span></a>의 RW와 RW-F 사이의 간격에 비해 성능 저하를 플롯한다. 우리는 RefinedWeb이 RefinedWeb 필터링보다 절대적 열화가 덜 중요하다는 것을 발견했으며, 또한 epoch의 수가 증가함에 따라 격차가 확대된다. 그러나 작업 전반에 걸쳐 상당한 변동성을 관찰합니다.</p>
</div>
<figure id="A5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x9.png" id="A5.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="152" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F7.sf1.2.1.1" style="font-size:80%;">(a)</span></span><span class="ltx_text" id="A5.F7.sf1.3.2" style="font-size:80%;">Degradation compared to 1 epoch</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x10.png" id="A5.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="150" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F7.sf2.2.1.1" style="font-size:80%;">(b)</span></span><span class="ltx_text" id="A5.F7.sf2.3.2" style="font-size:80%;">RW와 RW-F 사이의 Gap</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 7:</span><span class="ltx_text ltx_font_bold" id="A5.F7.5.1">Deduplication may reduce the degradation in performance incurred by multiple epoch. </span> 그러나 우리의 실험은 소규모(30GT에서 훈련된 1B 모델)에서만 수행되었으며 작업 전반에 걸쳐 결과의 높은 변동성을 볼 수 있다. <span class="ltx_text ltx_font_typewriter" id="A5.F7.6.2">agg-dev-2</span> aggregate (HellaSwag, PIQA, ARC, BoolQ, COPA, MRPC, SciQ)에서 측정한 Zero-shot 성능입니다. 작업별 결과 및 1-<math alttext="\sigma" class="ltx_Math" display="inline" id="A5.F7.2.m1.1"><semantics id="A5.F7.2.m1.1b"><mi id="A5.F7.2.m1.1.1" xref="A5.F7.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A5.F7.2.m1.1c"><ci id="A5.F7.2.m1.1.1.cmml" xref="A5.F7.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.F7.2.m1.1d">\sigma</annotation></semantics></math> 표준 편차에 대한 개별 곡선은 집합에서 투명합니다.</figcaption>
</figure>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Tasks, models, and datasets from the state-of-the-art</h2>

<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Task aggregates</h3>

<div id="A6.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A6.SS1.p1.1">모델을 평가하기 위해 다양한 작업 집합에 대한 평균 제로 샷 성능 우리의 집합은 <a class="ltx_ref" href="#S3.T3" title="In 3.3 Deduplication: fuzzy, exact, and across dumps ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>에 요약되어 있습니다.</p>
<ul id="A6.I1" class="ltx_itemize">
<li id="A6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A6.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A6.I1.i1.p1.1.1">small</span>: small-scale ablation studies, taskswith non-zero performance for 1B parameters models trained on 30GT;</p>
</div>
</li>
<li id="A6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A6.I1.i2.p1.1"><span class="ltx_text ltx_font_typewriter" id="A6.I1.i2.p1.1.1">core</span>: comparisons with a wide range of models, 특히 based on the tasks reported in <cite class="ltx_cite ltx_citemacro_citep">(Dey et al., <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>;</p>
</div>
</li>
<li id="A6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A6.I1.i3.p1.1"><span class="ltx_text ltx_font_typewriter" id="A6.I1.i3.p1.1.1">main</span>: tasks available in the GPT-3 and PaLM papers <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib18" title="">2020</a>; Chowdhery et al., <a class="ltx_ref" href="#bib.bib23" title="">2022</a>)</cite>;</p>
</div>
</li>
<li id="A6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="A6.I1.i4.p1.1"><span class="ltx_text ltx_font_typewriter" id="A6.I1.i4.p1.1.1">ext</span>: tasks available in the work of the BigScience Architecture and Scaling group <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib67" title="">2022b</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="A6.SS1.p2" class="ltx_para">
<p class="ltx_p" id="A6.SS1.p2.1">최신 모델의 모델과 비교할 때 <a class="ltx_ref" href="#A6.T10" title="In F.1 Task aggregates ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>에 자세히 설명된 몇 가지 다른 논문의 결과를 제공한다.</p>
</div>
<figure id="A6.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 10:</span><span class="ltx_text ltx_font_bold" id="A6.T10.25.1">우리는 문헌 전반에 걸쳐 다양한 논문에서 소스 평가 결과를 제공하여 작업 범위를 극대화합니다. </span> 대부분의 결과는 EAI 평가 Harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite>에서 나오지만 PaLM 및 GPT-3의 결과는 각각의 논문에서 나온다. 그림 <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">1</span></a>에서 GPT-3 논문의 결과가 EAI 평가 하니스가 있는 API를 통해 얻은 결과보다 여전히 앞서 있음을 주목한다.</figcaption>
<table id="A6.T10.23" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A6.T10.23.24" class="ltx_tr">
<td id="A6.T10.23.24.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="A6.T10.23.24.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.2.1" class="ltx_text ltx_font_bold">Aggregates reported</span></td>
<td id="A6.T10.23.24.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.3.1" class="ltx_text ltx_font_bold">Source of results</span></td>
<td id="A6.T10.23.24.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A6.T10.23.24.4.1" class="ltx_text ltx_font_bold">EAI eval harness?</span></td>
</tr>
<tr id="A6.T10.1.1" class="ltx_tr">
<td id="A6.T10.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Ours</td>
<td id="A6.T10.1.1.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="A6.T10.1.1.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.1.1.3.2" class="ltx_text ltx_font_typewriter">core</span>, <span id="A6.T10.1.1.3.3" class="ltx_text ltx_font_typewriter">ext</span>
</td>
<td id="A6.T10.1.1.4" class="ltx_td ltx_align_center ltx_border_t">This paper</td>
<td id="A6.T10.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A6.T10.1.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.1.1.1.m1.1a"><mi mathvariant="normal" id="A6.T10.1.1.1.m1.1.1" xref="A6.T10.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.1.1.1.m1.1b"><ci id="A6.T10.1.1.1.m1.1.1.cmml" xref="A6.T10.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.1.1.1.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.3.3" class="ltx_tr">
<td id="A6.T10.2.2.1" class="ltx_td ltx_align_center">BS-A&amp;S<sup id="A6.T10.2.2.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.3.3.3" class="ltx_td ltx_align_center">
<span id="A6.T10.3.3.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.3.3.3.2" class="ltx_text ltx_font_typewriter">core</span>
</td>
<td id="A6.T10.3.3.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="A6.T10.3.3.2" class="ltx_td ltx_align_center"><math id="A6.T10.3.3.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.3.3.2.m1.1a"><mi mathvariant="normal" id="A6.T10.3.3.2.m1.1.1" xref="A6.T10.3.3.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.3.3.2.m1.1b"><ci id="A6.T10.3.3.2.m1.1.1.cmml" xref="A6.T10.3.3.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.3.3.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.5.5" class="ltx_tr">
<td id="A6.T10.4.4.1" class="ltx_td ltx_align_center">GPT-Neo<sup id="A6.T10.4.4.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.5.5.3" class="ltx_td ltx_align_center">
<span id="A6.T10.5.5.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.5.5.3.2" class="ltx_text ltx_font_typewriter">core</span>
</td>
<td id="A6.T10.5.5.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="A6.T10.5.5.2" class="ltx_td ltx_align_center"><math id="A6.T10.5.5.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.5.5.2.m1.1a"><mi mathvariant="normal" id="A6.T10.5.5.2.m1.1.1" xref="A6.T10.5.5.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.5.5.2.m1.1b"><ci id="A6.T10.5.5.2.m1.1.1.cmml" xref="A6.T10.5.5.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.5.5.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.6.6" class="ltx_tr">
<td id="A6.T10.6.6.1" class="ltx_td ltx_align_center">PaLM<sup id="A6.T10.6.6.1.1" class="ltx_sup">†</sup>
</td>
<td id="A6.T10.6.6.2" class="ltx_td ltx_align_center"><span id="A6.T10.6.6.2.1" class="ltx_text ltx_font_typewriter">main</span></td>
<td id="A6.T10.6.6.3" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Chowdhery et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.6.6.4" class="ltx_td"></td>
</tr>
<tr id="A6.T10.8.8" class="ltx_tr">
<td id="A6.T10.7.7.1" class="ltx_td ltx_align_center">GPT-3 API<sup id="A6.T10.7.7.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.8.8.3" class="ltx_td ltx_align_center">
<span id="A6.T10.8.8.3.1" class="ltx_text ltx_font_typewriter">main</span>, <span id="A6.T10.8.8.3.2" class="ltx_text ltx_font_typewriter">core</span>
</td>
<td id="A6.T10.8.8.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022b</a>)</cite></td>
<td id="A6.T10.8.8.2" class="ltx_td ltx_align_center"><math id="A6.T10.8.8.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.8.8.2.m1.1a"><mi mathvariant="normal" id="A6.T10.8.8.2.m1.1.1" xref="A6.T10.8.8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.8.8.2.m1.1b"><ci id="A6.T10.8.8.2.m1.1.1.cmml" xref="A6.T10.8.8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.8.8.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.9.9" class="ltx_tr">
<td id="A6.T10.9.9.1" class="ltx_td ltx_align_center">GPT-3<sup id="A6.T10.9.9.1.1" class="ltx_sup">†</sup>
</td>
<td id="A6.T10.9.9.2" class="ltx_td ltx_align_center"><span id="A6.T10.9.9.2.1" class="ltx_text ltx_font_typewriter">main</span></td>
<td id="A6.T10.9.9.3" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="A6.T10.9.9.4" class="ltx_td"></td>
</tr>
<tr id="A6.T10.11.11" class="ltx_tr">
<td id="A6.T10.10.10.1" class="ltx_td ltx_align_center">Aleph Alpha<sup id="A6.T10.10.10.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.11.11.3" class="ltx_td ltx_align_center"><span id="A6.T10.11.11.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.11.11.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Aleph Alpha (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.11.11.2" class="ltx_td ltx_align_center"><math id="A6.T10.11.11.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.11.11.2.m1.1a"><mi mathvariant="normal" id="A6.T10.11.11.2.m1.1.1" xref="A6.T10.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.11.11.2.m1.1b"><ci id="A6.T10.11.11.2.m1.1.1.cmml" xref="A6.T10.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.11.11.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.13.13" class="ltx_tr">
<td id="A6.T10.12.12.1" class="ltx_td ltx_align_center">Cerebras-GPT<sup id="A6.T10.12.12.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.13.13.3" class="ltx_td ltx_align_center"><span id="A6.T10.13.13.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.13.13.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.13.13.2" class="ltx_td ltx_align_center"><math id="A6.T10.13.13.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.13.13.2.m1.1a"><mi mathvariant="normal" id="A6.T10.13.13.2.m1.1.1" xref="A6.T10.13.13.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.13.13.2.m1.1b"><ci id="A6.T10.13.13.2.m1.1.1.cmml" xref="A6.T10.13.13.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.13.13.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.15.15" class="ltx_tr">
<td id="A6.T10.14.14.1" class="ltx_td ltx_align_center">FairSeq<sup id="A6.T10.14.14.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.15.15.3" class="ltx_td ltx_align_center"><span id="A6.T10.15.15.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.15.15.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.15.15.2" class="ltx_td ltx_align_center"><math id="A6.T10.15.15.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.15.15.2.m1.1a"><mi mathvariant="normal" id="A6.T10.15.15.2.m1.1.1" xref="A6.T10.15.15.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.15.15.2.m1.1b"><ci id="A6.T10.15.15.2.m1.1.1.cmml" xref="A6.T10.15.15.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.15.15.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.17.17" class="ltx_tr">
<td id="A6.T10.16.16.1" class="ltx_td ltx_align_center">Pythia(-Dedup)<sup id="A6.T10.16.16.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.17.17.3" class="ltx_td ltx_align_center"><span id="A6.T10.17.17.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.17.17.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.17.17.2" class="ltx_td ltx_align_center"><math id="A6.T10.17.17.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.17.17.2.m1.1a"><mi mathvariant="normal" id="A6.T10.17.17.2.m1.1.1" xref="A6.T10.17.17.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.17.17.2.m1.1b"><ci id="A6.T10.17.17.2.m1.1.1.cmml" xref="A6.T10.17.17.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.17.17.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.19.19" class="ltx_tr">
<td id="A6.T10.18.18.1" class="ltx_td ltx_align_center">OPT<sup id="A6.T10.18.18.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.19.19.3" class="ltx_td ltx_align_center"><span id="A6.T10.19.19.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.19.19.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="A6.T10.19.19.2" class="ltx_td ltx_align_center"><math id="A6.T10.19.19.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.19.19.2.m1.1a"><mi mathvariant="normal" id="A6.T10.19.19.2.m1.1.1" xref="A6.T10.19.19.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.19.19.2.m1.1b"><ci id="A6.T10.19.19.2.m1.1.1.cmml" xref="A6.T10.19.19.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.19.19.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.21.21" class="ltx_tr">
<td id="A6.T10.20.20.1" class="ltx_td ltx_align_center">GPT-J<sup id="A6.T10.20.20.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.21.21.3" class="ltx_td ltx_align_center"><span id="A6.T10.21.21.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.21.21.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.21.21.2" class="ltx_td ltx_align_center"><math id="A6.T10.21.21.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.21.21.2.m1.1a"><mi mathvariant="normal" id="A6.T10.21.21.2.m1.1.1" xref="A6.T10.21.21.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.21.21.2.m1.1b"><ci id="A6.T10.21.21.2.m1.1.1.cmml" xref="A6.T10.21.21.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.21.21.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="A6.T10.23.23" class="ltx_tr">
<td id="A6.T10.22.22.1" class="ltx_td ltx_align_center ltx_border_bb">GPT-NeoX 20B<sup id="A6.T10.22.22.1.1" class="ltx_sup">∗</sup>
</td>
<td id="A6.T10.23.23.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A6.T10.23.23.3.1" class="ltx_text ltx_font_typewriter">core</span></td>
<td id="A6.T10.23.23.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="A6.T10.23.23.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="A6.T10.23.23.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="A6.T10.23.23.2.m1.1a"><mi mathvariant="normal" id="A6.T10.23.23.2.m1.1.1" xref="A6.T10.23.23.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A6.T10.23.23.2.m1.1b"><ci id="A6.T10.23.23.2.m1.1.1.cmml" xref="A6.T10.23.23.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T10.23.23.2.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
</tbody></table>
</figure>
</section>
<section id="A6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Models</h3>

<div id="A6.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A6.SS2.p1.1">우리는 <a class="ltx_ref" href="#A6.T11" title="In Pythia and deduplication. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>에 제시된 다양한 선별된 코퍼라에 대해 훈련된 10개의 시리즈에 걸쳐 거의 50개의 모델과 비교한다.</p>
</div>
<section id="A6.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cerebras-GPT with <math id="A6.SS2.SSS0.Px1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="A6.SS2.SSS0.Px1.1.m1.1b"><mi id="A6.SS2.SSS0.Px1.1.m1.1.1" xref="A6.SS2.SSS0.Px1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.SS2.SSS0.Px1.1.m1.1c"><ci id="A6.SS2.SSS0.Px1.1.m1.1.1.cmml" xref="A6.SS2.SSS0.Px1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.SS2.SSS0.Px1.1.m1.1d">\mu</annotation></semantics></math>-parametrization.</h5>

<div id="A6.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A6.SS2.SSS0.Px1.p1.1">Cerebras-GPT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Dey et al., <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>는 또한 <math alttext="\mu" class="ltx_Math" display="inline" id="A6.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="A6.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="A6.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A6.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="A6.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A6.SS2.SSS0.Px1.p1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.SS2.SSS0.Px1.p1.1.m1.1c">\mu</annotation></semantics></math>-parametrization <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib83" title="">2021</a>)</cite>의 권장 사항에 따라 최대 2.7B 매개 변수의 더 작은 시리즈로 제공됩니다. 이 더 작은 시리즈의 성능이 주요 시리즈 모델에 가깝다는 것을 발견했으며(<a class="ltx_ref" href="#A6.F8" title="In Cerebras-GPT with 𝜇-parametrization. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> 참조), 우리가 비교하는 것과 유사한 계산 규모의 모델을 포함하지 않기 때문에 주요 그림에 보고하지 않기로 결정했다.</p>
</div>
<figure id="A6.F8" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x11.png" id="A6.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 8:</span><math alttext="\mu" class="ltx_Math" display="inline" id="A6.F8.4.m1.1"><semantics id="A6.F8.4.m1.1b"><mi id="A6.F8.4.m1.1.1" xref="A6.F8.4.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.F8.4.m1.1c"><ci id="A6.F8.4.m1.1.1.cmml" xref="A6.F8.4.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F8.4.m1.1d">\mu</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A6.F8.9.1">-parametrization <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib83" title="">2021</a>)</cite>는 Cerebras-GPT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Dey et al., <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>에서 성능을 약간 향상시킵니다. </span> Zero-shot performance on our <span class="ltx_text ltx_font_typewriter" id="A6.F8.10.2">core</span> aggregate, gap between Cerebras-GPT with <math alttext="\mu" class="ltx_Math" display="inline" id="A6.F8.5.m2.1"><semantics id="A6.F8.5.m2.1b"><mi id="A6.F8.5.m2.1.1" xref="A6.F8.5.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A6.F8.5.m2.1c"><ci id="A6.F8.5.m2.1.1.cmml" xref="A6.F8.5.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F8.5.m2.1d">\mu</annotation></semantics></math>-param and without. 작업별 결과 및 1-<math alttext="\sigma" class="ltx_Math" display="inline" id="A6.F8.6.m3.1"><semantics id="A6.F8.6.m3.1b"><mi id="A6.F8.6.m3.1.1" xref="A6.F8.6.m3.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A6.F8.6.m3.1c"><ci id="A6.F8.6.m3.1.1.cmml" xref="A6.F8.6.m3.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F8.6.m3.1d">\sigma</annotation></semantics></math> 표준 편차에 대한 개별 곡선은 집합에서 투명합니다.</figcaption>
</figure>
</section>
<section id="A6.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pythia and deduplication.</h5>

<div id="A6.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A6.SS2.SSS0.Px2.p1.1">피티아 시리즈 모델은 두 가지 맛으로 제공됩니다. 하나는 더 파일의 바닐라 버전에서 훈련되고 다른 하나는 민해시와 중복되지 않은 버전에서 훈련됩니다. 이 두 맛 사이의 성능은 <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>에서 최소한으로 다른 것으로 나타났으며, <a class="ltx_ref" href="#A6.F9" title="In Pythia and deduplication. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>에서 중복 제거된 버전이 집계 아래 중복되지 않은 버전보다 약간 앞설 수 있음을 발견했다. 이러한 개선의 더 높은 목적은 <a class="ltx_ref" href="#S4.T5" title="In 4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>에서의 우리의 발견과 대체로 일치한다. 그럼에도 불구하고, 우리의 발견과 그들의 발견에는 차이가 남아 있다. 우리는 몇 가지 가능한 가설을 상정한다:</p>
<ul id="A6.I2" class="ltx_itemize">
<li id="A6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A6.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I2.i1.p1.1.1">큐레이티드 데이터와 웹 데이터 간의 차이점. </span> 웹 데이터가 중복에 더 민감할 수 있습니다. 예를 들어, 웹 데이터에서 가장 일반적인 복제물(예: 스팸)은 큐레이트된 데이터에서 가장 일반적인 복제물보다 더 해로울 수 있다. 이것은 우리가 이 작업에서 연구하지 않은 중복제거에 대한 질적 구성요소를 시사한다.</p>
</div>
</li>
<li id="A6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A6.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I2.i2.p1.1.1">Differences in deduplication pipeline. </span> <cite class="ltx_cite ltx_citemacro_citet">Biderman et al. (<a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>는 <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>의 MinHash 설정을 사용하기 때문에 대부분 우리와 동일합니다. 그러나 우리는 또한 정확한 중복 제거를 적용합니다: 그들의 중복 제거는 크기가 30% 감소하지만, 우리의 중복 제거는 더 공격적이어서 크기가 45% 감소합니다. 이는 <a class="ltx_ref" href="#S4.T5" title="In 4.3 Do other corpora benefit from MDR? ‣ 4 Experiments ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>의 결과가 <a class="ltx_ref" href="#A6.F9" title="In Pythia and deduplication. ‣ F.2 Models ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>의 결과보다 중복 제거에서 더 강한 이득을 보이는 이유를 설명할 수 있다.</p>
</div>
</li>
<li id="A6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A6.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A6.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A6.I2.i3.p1.1.1">Differences in pretraining. 마지막으로, <cite class="ltx_cite ltx_citemacro_citet">Biderman et al. (<a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>는 중복 제거된 데이터에서 부분적인 여분의 epoch를 선택하여 300GT에 도달하는 반면, 우리는 항상 단일 epoch를 수행한다는 점에 주목한다. 그들의 설정은 데이터 제약 시나리오에 해당하며, 이는 그들이 연구하는 선별된 데이터에 대해 더 현실적이며, 우리에게 웹 데이터는 풍부하기 때문에 중복 제거는 우리가 사용할 수 있는 데이터 세트의 크기를 결코 진정으로 제한하지 않는다.</p>
</div>
</li>
</ul>
</div>
<figure id="A6.F9" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2306.01116/assets/x12.png" id="A6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 9:</span><span class="ltx_text ltx_font_bold" id="A6.F9.5.1">In our <span class="ltx_text ltx_font_typewriter" id="A6.F9.5.1.1">core</span> aggregate, deduplication brings a small improvement to the Pythia suite <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>. </span> Zero-shot performance on our <span class="ltx_text ltx_font_typewriter" id="A6.F9.6.2">core</span> aggregate, gap between Pythia trained on the deduplicated and vanilla Pile. 작업별 결과 및 1-<math alttext="\sigma" class="ltx_Math" display="inline" id="A6.F9.2.m1.1"><semantics id="A6.F9.2.m1.1b"><mi id="A6.F9.2.m1.1.1" xref="A6.F9.2.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A6.F9.2.m1.1c"><ci id="A6.F9.2.m1.1.1.cmml" xref="A6.F9.2.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F9.2.m1.1d">\sigma</annotation></semantics></math> 표준 편차에 대한 개별 곡선은 집합에서 투명합니다.</figcaption>
</figure>
<figure id="A6.T11" class="ltx_table">

<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">표 11:</span><span class="ltx_text ltx_font_bold" id="A6.T11.30.1">RefinedWeb(Falcon-RW)에서 훈련된 Full-scale models and other models from the state-of-the-art. </span> Pile에서 훈련된 모델 전반에 걸쳐 피티아 모델은 회전식 임베딩과 함께 FlashAttention를 사용하여 모델에 대한 병렬 주의 및 피드포워드 사용을 특히 예외적으로 사용합니다. <math alttext="C=6ND" class="ltx_Math" display="inline" id="A6.T11.6.m2.1"><semantics id="A6.T11.6.m2.1b"><mrow id="A6.T11.6.m2.1.1" xref="A6.T11.6.m2.1.1.cmml"><mi id="A6.T11.6.m2.1.1.2" xref="A6.T11.6.m2.1.1.2.cmml">C</mi><mo id="A6.T11.6.m2.1.1.1" xref="A6.T11.6.m2.1.1.1.cmml">=</mo><mrow id="A6.T11.6.m2.1.1.3" xref="A6.T11.6.m2.1.1.3.cmml"><mn id="A6.T11.6.m2.1.1.3.2" xref="A6.T11.6.m2.1.1.3.2.cmml">6</mn><mo id="A6.T11.6.m2.1.1.3.1" lspace="0em" rspace="0em" xref="A6.T11.6.m2.1.1.3.1.cmml">​</mo><mi id="A6.T11.6.m2.1.1.3.3" xref="A6.T11.6.m2.1.1.3.3.cmml">N</mi><mo id="A6.T11.6.m2.1.1.3.1b" lspace="0em" rspace="0em" xref="A6.T11.6.m2.1.1.3.1.cmml">​</mo><mi id="A6.T11.6.m2.1.1.3.4" xref="A6.T11.6.m2.1.1.3.4.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T11.6.m2.1c"><apply id="A6.T11.6.m2.1.1.cmml" xref="A6.T11.6.m2.1.1"><eq id="A6.T11.6.m2.1.1.1.cmml" xref="A6.T11.6.m2.1.1.1"></eq><ci id="A6.T11.6.m2.1.1.2.cmml" xref="A6.T11.6.m2.1.1.2">𝐶</ci><apply id="A6.T11.6.m2.1.1.3.cmml" xref="A6.T11.6.m2.1.1.3"><times id="A6.T11.6.m2.1.1.3.1.cmml" xref="A6.T11.6.m2.1.1.3.1"></times><cn id="A6.T11.6.m2.1.1.3.2.cmml" type="integer" xref="A6.T11.6.m2.1.1.3.2">6</cn><ci id="A6.T11.6.m2.1.1.3.3.cmml" xref="A6.T11.6.m2.1.1.3.3">𝑁</ci><ci id="A6.T11.6.m2.1.1.3.4.cmml" xref="A6.T11.6.m2.1.1.3.4">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.6.m2.1d">C=6ND</annotation></semantics></math>를 사용하여 계산된 PF-days에서 훈련 예산 <math alttext="C" class="ltx_Math" display="inline" id="A6.T11.5.m1.1"><semantics id="A6.T11.5.m1.1b"><mi id="A6.T11.5.m1.1.1" xref="A6.T11.5.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A6.T11.5.m1.1c"><ci id="A6.T11.5.m1.1.1.cmml" xref="A6.T11.5.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.5.m1.1d">C</annotation></semantics></math> <math alttext="N" class="ltx_Math" display="inline" id="A6.T11.7.m3.1"><semantics id="A6.T11.7.m3.1b"><mi id="A6.T11.7.m3.1.1" xref="A6.T11.7.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A6.T11.7.m3.1c"><ci id="A6.T11.7.m3.1.1.cmml" xref="A6.T11.7.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.7.m3.1d">N</annotation></semantics></math> 파라미터 수, <math alttext="D" class="ltx_Math" display="inline" id="A6.T11.8.m4.1"><semantics id="A6.T11.8.m4.1b"><mi id="A6.T11.8.m4.1.1" xref="A6.T11.8.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A6.T11.8.m4.1c"><ci id="A6.T11.8.m4.1.1.cmml" xref="A6.T11.8.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T11.8.m4.1d">D</annotation></semantics></math> 사전 훈련 데이터셋 크기 <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="#bib.bib46" title="">2020</a>)</cite>를 포함한다.</figcaption>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A6.T11.12" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody><tr id="A6.T11.12.4" class="ltx_tr">
<td id="A6.T11.12.4.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A6.T11.12.4.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Series</span></td>
<td id="A6.T11.9.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="A6.T11.9.1.1.1" class="ltx_text" style="font-size:70%;">GPT-3 (paper)</span><sup id="A6.T11.9.1.1.2" class="ltx_sup"><span id="A6.T11.9.1.1.2.1" class="ltx_text" style="font-size:70%;">†</span></sup>
</td>
<td id="A6.T11.10.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="A6.T11.10.2.2.1" class="ltx_text" style="font-size:70%;">GPT-3 (API)</span><sup id="A6.T11.10.2.2.2" class="ltx_sup"><span id="A6.T11.10.2.2.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.11.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.11.3.3.1" class="ltx_text" style="font-size:70%;">BigScience</span><sup id="A6.T11.11.3.3.2" class="ltx_sup"><span id="A6.T11.11.3.3.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.12.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.12.4.4.1" class="ltx_text" style="font-size:70%;">PaLM</span><sup id="A6.T11.12.4.4.2" class="ltx_sup"><span id="A6.T11.12.4.4.2.1" class="ltx_text" style="font-size:70%;">†</span></sup>
</td>
<td id="A6.T11.12.4.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="A6.T11.12.4.6.1" class="ltx_text" style="font-size:70%;">Ours</span></td>
</tr>
<tr id="A6.T11.12.5" class="ltx_tr">
<td id="A6.T11.12.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T11.12.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model</span></td>
<td id="A6.T11.12.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.2.1" class="ltx_text" style="font-size:70%;">XL</span></td>
<td id="A6.T11.12.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.3.1" class="ltx_text" style="font-size:70%;">XXL</span></td>
<td id="A6.T11.12.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.4.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">babbage</span></td>
<td id="A6.T11.12.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.5.1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">curie</span></td>
<td id="A6.T11.12.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.6.1" class="ltx_text" style="font-size:70%;">BS-A&amp;S</span></td>
<td id="A6.T11.12.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.7.1" class="ltx_text" style="font-size:70%;">PaLM-8B</span></td>
<td id="A6.T11.12.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.12.5.8.1" class="ltx_text" style="font-size:70%;">Ours (Pile)</span></td>
<td id="A6.T11.12.5.9" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="A6.T11.12.5.9.1" class="ltx_text" style="font-size:70%;">Falcon-RW</span></td>
</tr>
<tr id="A6.T11.12.6" class="ltx_tr">
<td id="A6.T11.12.6.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></td>
<td id="A6.T11.12.6.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.2.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.3.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.4.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.5.1" class="ltx_text" style="font-size:70%;">GPT-3</span></td>
<td id="A6.T11.12.6.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.6.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.12.6.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.7.1" class="ltx_text" style="font-size:70%;">PaLM</span></td>
<td id="A6.T11.12.6.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.8.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.12.6.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.9.1" class="ltx_text" style="font-size:70%;">RW</span></td>
<td id="A6.T11.12.6.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.6.10.1" class="ltx_text" style="font-size:70%;">RW</span></td>
</tr>
<tr id="A6.T11.12.7" class="ltx_tr">
<td id="A6.T11.12.7.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.7.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Params.</span></td>
<td id="A6.T11.12.7.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.2.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.3.1" class="ltx_text" style="font-size:70%;">6.7B</span></td>
<td id="A6.T11.12.7.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.4.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.5.1" class="ltx_text" style="font-size:70%;">6.7B</span></td>
<td id="A6.T11.12.7.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.6.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.7.1" class="ltx_text" style="font-size:70%;">8.6B</span></td>
<td id="A6.T11.12.7.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.8.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.9.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.12.7.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.7.10.1" class="ltx_text" style="font-size:70%;">7.5B</span></td>
</tr>
<tr id="A6.T11.12.8" class="ltx_tr">
<td id="A6.T11.12.8.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Pretraining</span></td>
<td id="A6.T11.12.8.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.2.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.3.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.4.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.5.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.6.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.12.8.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.7.1" class="ltx_text" style="font-size:70%;">780GT</span></td>
<td id="A6.T11.12.8.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.8.1" class="ltx_text" style="font-size:70%;">350GT</span></td>
<td id="A6.T11.12.8.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.9.1" class="ltx_text" style="font-size:70%;">350GT</span></td>
<td id="A6.T11.12.8.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.8.10.1" class="ltx_text" style="font-size:70%;">350GT</span></td>
</tr>
<tr id="A6.T11.12.9" class="ltx_tr">
<td id="A6.T11.12.9.1" class="ltx_td ltx_align_left"><span id="A6.T11.12.9.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">PF-days</span></td>
<td id="A6.T11.12.9.2" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.2.1" class="ltx_text" style="font-size:70%;">27</span></td>
<td id="A6.T11.12.9.3" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.3.1" class="ltx_text" style="font-size:70%;">140</span></td>
<td id="A6.T11.12.9.4" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.4.1" class="ltx_text" style="font-size:70%;">27</span></td>
<td id="A6.T11.12.9.5" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.5.1" class="ltx_text" style="font-size:70%;">140</span></td>
<td id="A6.T11.12.9.6" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.6.1" class="ltx_text" style="font-size:70%;">27</span></td>
<td id="A6.T11.12.9.7" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.7.1" class="ltx_text" style="font-size:70%;">466</span></td>
<td id="A6.T11.12.9.8" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.8.1" class="ltx_text" style="font-size:70%;">32</span></td>
<td id="A6.T11.12.9.9" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.9.1" class="ltx_text" style="font-size:70%;">32</span></td>
<td id="A6.T11.12.9.10" class="ltx_td ltx_align_center"><span id="A6.T11.12.9.10.1" class="ltx_text" style="font-size:70%;">182</span></td>
</tr>
<tr id="A6.T11.12.10" class="ltx_tr">
<td id="A6.T11.12.10.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T11.12.10.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Citation</span></td>
<td id="A6.T11.12.10.2" class="ltx_td ltx_align_center ltx_border_bb" colspan="4"><cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. <span id="A6.T11.12.10.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib18" title="" class="ltx_ref">2020</a><span id="A6.T11.12.10.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.12.10.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Scao et&nbsp;al. <span id="A6.T11.12.10.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib67" title="" class="ltx_ref">2022b</a><span id="A6.T11.12.10.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.12.10.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Chowdhery et&nbsp;al. <span id="A6.T11.12.10.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib23" title="" class="ltx_ref">2022</a><span id="A6.T11.12.10.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.12.10.5" class="ltx_td ltx_align_center ltx_border_bb" colspan="3"><span id="A6.T11.12.10.5.1" class="ltx_text" style="font-size:70%;">This paper</span></td>
</tr>
</tbody></table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A6.T11.14" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody><tr id="A6.T11.14.2" class="ltx_tr">
<td id="A6.T11.14.2.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A6.T11.14.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Series</span></td>
<td id="A6.T11.13.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">
<span id="A6.T11.13.1.1.1" class="ltx_text" style="font-size:70%;">EleutherAI</span><sup id="A6.T11.13.1.1.2" class="ltx_sup"><span id="A6.T11.13.1.1.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.14.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.14.2.2.1" class="ltx_text" style="font-size:70%;">Pythia</span><sup id="A6.T11.14.2.2.2" class="ltx_sup"><span id="A6.T11.14.2.2.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
</tr>
<tr id="A6.T11.14.3" class="ltx_tr">
<td id="A6.T11.14.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T11.14.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model</span></td>
<td id="A6.T11.14.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.2.1" class="ltx_text" style="font-size:70%;">GPT-Neo</span></td>
<td id="A6.T11.14.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.3.1" class="ltx_text" style="font-size:70%;">GPT-J</span></td>
<td id="A6.T11.14.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.4.1" class="ltx_text" style="font-size:70%;">GPT-NeoX 20B</span></td>
<td id="A6.T11.14.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.14.3.5.1" class="ltx_text" style="font-size:70%;">Pythia(-Dedup)</span></td>
</tr>
<tr id="A6.T11.14.4" class="ltx_tr">
<td id="A6.T11.14.4.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.4.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></td>
<td id="A6.T11.14.4.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.2.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.14.4.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.3.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.14.4.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.4.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.14.4.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.4.5.1" class="ltx_text" style="font-size:70%;">Pile (dedup)</span></td>
</tr>
<tr id="A6.T11.14.5" class="ltx_tr">
<td id="A6.T11.14.5.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Params.</span></td>
<td id="A6.T11.14.5.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.2.1" class="ltx_text" style="font-size:70%;">1.3B</span></td>
<td id="A6.T11.14.5.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.3.1" class="ltx_text" style="font-size:70%;">6.7B</span></td>
<td id="A6.T11.14.5.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.4.1" class="ltx_text" style="font-size:70%;">20B</span></td>
<td id="A6.T11.14.5.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.5.5.1" class="ltx_text" style="font-size:70%;">70M-12B</span></td>
</tr>
<tr id="A6.T11.14.6" class="ltx_tr">
<td id="A6.T11.14.6.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Pretraining</span></td>
<td id="A6.T11.14.6.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.2.1" class="ltx_text" style="font-size:70%;">380GT</span></td>
<td id="A6.T11.14.6.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.3.1" class="ltx_text" style="font-size:70%;">402GT</span></td>
<td id="A6.T11.14.6.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.4.1" class="ltx_text" style="font-size:70%;">472GT</span></td>
<td id="A6.T11.14.6.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.6.5.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
</tr>
<tr id="A6.T11.14.7" class="ltx_tr">
<td id="A6.T11.14.7.1" class="ltx_td ltx_align_left"><span id="A6.T11.14.7.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">PF-days</span></td>
<td id="A6.T11.14.7.2" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.2.1" class="ltx_text" style="font-size:70%;">34</span></td>
<td id="A6.T11.14.7.3" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.3.1" class="ltx_text" style="font-size:70%;">187</span></td>
<td id="A6.T11.14.7.4" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.4.1" class="ltx_text" style="font-size:70%;">656</span></td>
<td id="A6.T11.14.7.5" class="ltx_td ltx_align_center"><span id="A6.T11.14.7.5.1" class="ltx_text" style="font-size:70%;">1.5 - 250</span></td>
</tr>
<tr id="A6.T11.14.8" class="ltx_tr">
<td id="A6.T11.14.8.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T11.14.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Citation</span></td>
<td id="A6.T11.14.8.2" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. <span id="A6.T11.14.8.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib15" title="" class="ltx_ref">2021</a><span id="A6.T11.14.8.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.14.8.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Wang &amp; Komatsuzaki <span id="A6.T11.14.8.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib76" title="" class="ltx_ref">2021</a><span id="A6.T11.14.8.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.14.8.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Black et&nbsp;al. <span id="A6.T11.14.8.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib16" title="" class="ltx_ref">2022</a><span id="A6.T11.14.8.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.14.8.5" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Biderman et&nbsp;al. <span id="A6.T11.14.8.5.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib13" title="" class="ltx_ref">2023</a><span id="A6.T11.14.8.5.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
</tr>
</tbody></table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A6.T11.18" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody><tr id="A6.T11.18.4" class="ltx_tr">
<td id="A6.T11.18.4.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A6.T11.18.4.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Series</span></td>
<td id="A6.T11.15.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.15.1.1.1" class="ltx_text" style="font-size:70%;">Aleph Alpha</span><sup id="A6.T11.15.1.1.2" class="ltx_sup"><span id="A6.T11.15.1.1.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.16.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.16.2.2.1" class="ltx_text" style="font-size:70%;">Cerebras-GPT</span><sup id="A6.T11.16.2.2.2" class="ltx_sup"><span id="A6.T11.16.2.2.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.17.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.17.3.3.1" class="ltx_text" style="font-size:70%;">OPT</span><sup id="A6.T11.17.3.3.2" class="ltx_sup"><span id="A6.T11.17.3.3.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
<td id="A6.T11.18.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="A6.T11.18.4.4.1" class="ltx_text" style="font-size:70%;">FairSeq</span><sup id="A6.T11.18.4.4.2" class="ltx_sup"><span id="A6.T11.18.4.4.2.1" class="ltx_text" style="font-size:70%;">∗</span></sup>
</td>
</tr>
<tr id="A6.T11.18.5" class="ltx_tr">
<td id="A6.T11.18.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T11.18.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Model</span></td>
<td id="A6.T11.18.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.2.1" class="ltx_text" style="font-size:70%;">Luminous</span></td>
<td id="A6.T11.18.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.3.1" class="ltx_text" style="font-size:70%;">Cerebras-GPT</span></td>
<td id="A6.T11.18.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.4.1" class="ltx_text" style="font-size:70%;">OPT</span></td>
<td id="A6.T11.18.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T11.18.5.5.1" class="ltx_text" style="font-size:70%;">FairSeq</span></td>
</tr>
<tr id="A6.T11.18.6" class="ltx_tr">
<td id="A6.T11.18.6.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></td>
<td id="A6.T11.18.6.2" class="ltx_td ltx_align_center"><em id="A6.T11.18.6.2.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">undisclosed</em></td>
<td id="A6.T11.18.6.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.6.3.1" class="ltx_text" style="font-size:70%;">Pile</span></td>
<td id="A6.T11.18.6.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.6.4.1" class="ltx_text" style="font-size:70%;">Pile (subset) + curated</span></td>
<td id="A6.T11.18.6.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.6.5.1" class="ltx_text" style="font-size:70%;">curated</span></td>
</tr>
<tr id="A6.T11.18.7" class="ltx_tr">
<td id="A6.T11.18.7.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.7.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Params.</span></td>
<td id="A6.T11.18.7.2" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.2.1" class="ltx_text" style="font-size:70%;">13B</span></td>
<td id="A6.T11.18.7.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.3.1" class="ltx_text" style="font-size:70%;">111M-13B</span></td>
<td id="A6.T11.18.7.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.4.1" class="ltx_text" style="font-size:70%;">125M - 175B</span></td>
<td id="A6.T11.18.7.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.7.5.1" class="ltx_text" style="font-size:70%;">1.3 - 13B</span></td>
</tr>
<tr id="A6.T11.18.8" class="ltx_tr">
<td id="A6.T11.18.8.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.8.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Pretraining</span></td>
<td id="A6.T11.18.8.2" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.2.1" class="ltx_text" style="font-size:70%;">400GT</span></td>
<td id="A6.T11.18.8.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.3.1" class="ltx_text" style="font-size:70%;">2 - 257GT</span></td>
<td id="A6.T11.18.8.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.4.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
<td id="A6.T11.18.8.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.8.5.1" class="ltx_text" style="font-size:70%;">300GT</span></td>
</tr>
<tr id="A6.T11.18.9" class="ltx_tr">
<td id="A6.T11.18.9.1" class="ltx_td ltx_align_left"><span id="A6.T11.18.9.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">PF-days</span></td>
<td id="A6.T11.18.9.2" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.2.1" class="ltx_text" style="font-size:70%;">361</span></td>
<td id="A6.T11.18.9.3" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.3.1" class="ltx_text" style="font-size:70%;">0.02 - 232</span></td>
<td id="A6.T11.18.9.4" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.4.1" class="ltx_text" style="font-size:70%;">3 - 3646</span></td>
<td id="A6.T11.18.9.5" class="ltx_td ltx_align_center"><span id="A6.T11.18.9.5.1" class="ltx_text" style="font-size:70%;">27 - 271</span></td>
</tr>
<tr id="A6.T11.18.10" class="ltx_tr">
<td id="A6.T11.18.10.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T11.18.10.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Citation</span></td>
<td id="A6.T11.18.10.2" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Aleph Alpha <span id="A6.T11.18.10.2.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib5" title="" class="ltx_ref">2023</a><span id="A6.T11.18.10.2.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.18.10.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Dey et&nbsp;al. <span id="A6.T11.18.10.3.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2023</a><span id="A6.T11.18.10.3.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.18.10.4" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. <span id="A6.T11.18.10.4.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib87" title="" class="ltx_ref">2022</a><span id="A6.T11.18.10.4.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
<td id="A6.T11.18.10.5" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_citet">Artetxe et&nbsp;al. <span id="A6.T11.18.10.5.1.1.1.1" class="ltx_text" style="font-size:70%;">(</span><a href="#bib.bib10" title="" class="ltx_ref">2021</a><span id="A6.T11.18.10.5.2.2.2.1" class="ltx_text" style="font-size:70%;">)</span></cite></td>
</tr>
</tbody></table>
</div>
</div>
</figure>
</section>
</section>
<section id="A6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.3 </span>Datasets</h3>

<div id="A6.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A6.SS3.p1.1">우리는 <a class="ltx_ref" href="#A6.T12" title="In F.3 Datasets ‣ Appendix F Tasks, models, and datasets from the state-of-the-art ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">12</span></a>의 <a class="ltx_ref" href="#S0.T1" title="In The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>에 확장하여 리터러티에 걸쳐 사용되는 필터링 및 중복 제거 전략에 대한 세부 정보를 제공한다.</p>
</div>
<figure id="A6.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 12:</span><span class="ltx_text ltx_font_bold" id="A6.T12.24.1">Common massive web-scrape and LLM English datasets. </span> OSCAR 및 C4와 같은 데이터 세트도 상당한 다국어 버전을 가지고 있으며, 이는 광범위한 채택을 즐겼다. OSCA의 경우, 크기는 중복되지 않은 버전에 해당하며, 단어 수 x0,75(토큰당 평균 단어 수)로부터 추정된다.</figcaption>
<table id="A6.T12.22" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A6.T12.22.23" class="ltx_tr">
<td id="A6.T12.22.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="4"><span id="A6.T12.22.23.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">General information</span></td>
<td id="A6.T12.22.23.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="6"><span id="A6.T12.22.23.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Web data</span></td>
</tr>
<tr id="A6.T12.22.24" class="ltx_tr">
<td id="A6.T12.22.24.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.1.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></span>
</span>
</td>
<td id="A6.T12.22.24.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.2.1.1.1" class="ltx_text" style="font-size:90%;">Notable models</span></span>
</span>
</td>
<td id="A6.T12.22.24.3" class="ltx_td ltx_align_center"><span id="A6.T12.22.24.3.1" class="ltx_text" style="font-size:90%;">Size</span></td>
<td id="A6.T12.22.24.4" class="ltx_td ltx_align_center"><span id="A6.T12.22.24.4.1" class="ltx_text" style="font-size:90%;">Availability</span></td>
<td id="A6.T12.22.24.5" class="ltx_td ltx_align_center"><span id="A6.T12.22.24.5.1" class="ltx_text" style="font-size:90%;">Web</span></td>
<td id="A6.T12.22.24.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.24.6.1.1.1" class="ltx_text" style="font-size:90%;">HTML extraction</span></span>
</span>
</td>
<td id="A6.T12.22.24.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.7.1.1.1" class="ltx_text" style="font-size:90%;">Language ID</span></span>
</span>
</td>
<td id="A6.T12.22.24.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.8.1.1.1" class="ltx_text" style="font-size:90%;">Heuristics</span></span>
</span>
</td>
<td id="A6.T12.22.24.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.24.9.1.1.1" class="ltx_text" style="font-size:90%;">Content filtering</span></span>
</span>
</td>
<td id="A6.T12.22.24.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.22.24.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.24.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.24.10.1.1.1" class="ltx_text" style="font-size:90%;">Deduplication</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.22.25" class="ltx_tr">
<td id="A6.T12.22.25.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="10"><span id="A6.T12.22.25.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Massive web datasets</span></td>
</tr>
<tr id="A6.T12.2.2" class="ltx_tr">
<td id="A6.T12.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">C4</span><span id="A6.T12.2.2.3.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.2.2.3.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Raffel et&nbsp;al.<span id="A6.T12.2.2.3.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib64" title="" class="ltx_ref">2020</a><span id="A6.T12.2.2.3.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">T5 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.2.2.4.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Raffel et&nbsp;al.<span id="A6.T12.2.2.4.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib64" title="" class="ltx_ref">2020</a><span id="A6.T12.2.2.4.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.1.1.1.m1.1" class="ltx_Math" alttext="\sim 360" display="inline"><semantics id="A6.T12.1.1.1.m1.1a"><mrow id="A6.T12.1.1.1.m1.1.1" xref="A6.T12.1.1.1.m1.1.1.cmml"><mi id="A6.T12.1.1.1.m1.1.1.2" xref="A6.T12.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.1.1.1.m1.1.1.1" xref="A6.T12.1.1.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.1.1.1.m1.1.1.3" xref="A6.T12.1.1.1.m1.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.1.1.1.m1.1b"><apply id="A6.T12.1.1.1.m1.1.1.cmml" xref="A6.T12.1.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.1.1.1.m1.1.1.1.cmml" xref="A6.T12.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.1.1.1.m1.1.1.2.cmml" xref="A6.T12.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.1.1.1.m1.1.1.3.cmml" xref="A6.T12.1.1.1.m1.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.1.1.1.m1.1c">\sim 360</annotation></semantics></math><span id="A6.T12.1.1.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T12.2.2.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.2.2.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T12.2.2.2.m1.1a"><mn mathsize="90%" id="A6.T12.2.2.2.m1.1.1" xref="A6.T12.2.2.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T12.2.2.2.m1.1b"><cn type="integer" id="A6.T12.2.2.2.m1.1.1.cmml" xref="A6.T12.2.2.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.2.2.2.m1.1c">100</annotation></semantics></math><span id="A6.T12.2.2.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">.WET files</span></span>
</span>
</td>
<td id="A6.T12.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">Document-level w/ </span><span id="A6.T12.2.2.7.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">langdetect</span></span>
</span>
</td>
<td id="A6.T12.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.8.1.1.1" class="ltx_text" style="font-size:90%;">Document and line-level</span></span>
</span>
</td>
<td id="A6.T12.2.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.2.2.9.1.1.1" class="ltx_text" style="font-size:90%;">Rules-based: code, NSFW</span></span>
</span>
</td>
<td id="A6.T12.2.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.2.2.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.2.2.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="A6.T12.2.2.10.1.1.2" class="ltx_text" style="font-size:90%;">: three sentences span</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.6.6" class="ltx_tr">
<td id="A6.T12.6.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.6.6.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR 21.09</span><span id="A6.T12.6.6.5.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.6.6.5.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Ortiz Suárez et&nbsp;al.<span id="A6.T12.6.6.5.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib57" title="" class="ltx_ref">2019</a><span id="A6.T12.6.6.5.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.6.6.6" class="ltx_td ltx_align_top"></td>
<td id="A6.T12.3.3.1" class="ltx_td ltx_align_center">
<math id="A6.T12.3.3.1.m1.1" class="ltx_Math" alttext="\sim 370" display="inline"><semantics id="A6.T12.3.3.1.m1.1a"><mrow id="A6.T12.3.3.1.m1.1.1" xref="A6.T12.3.3.1.m1.1.1.cmml"><mi id="A6.T12.3.3.1.m1.1.1.2" xref="A6.T12.3.3.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.3.3.1.m1.1.1.1" xref="A6.T12.3.3.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.3.3.1.m1.1.1.3" xref="A6.T12.3.3.1.m1.1.1.3.cmml">370</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.3.3.1.m1.1b"><apply id="A6.T12.3.3.1.m1.1.1.cmml" xref="A6.T12.3.3.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.3.3.1.m1.1.1.1.cmml" xref="A6.T12.3.3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.3.3.1.m1.1.1.2.cmml" xref="A6.T12.3.3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.3.3.1.m1.1.1.3.cmml" xref="A6.T12.3.3.1.m1.1.1.3">370</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.3.3.1.m1.1c">\sim 370</annotation></semantics></math><span id="A6.T12.3.3.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.6.6.7" class="ltx_td ltx_align_center"><span id="A6.T12.6.6.7.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.4.4.2" class="ltx_td ltx_align_center">
<math id="A6.T12.4.4.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T12.4.4.2.m1.1a"><mn mathsize="90%" id="A6.T12.4.4.2.m1.1.1" xref="A6.T12.4.4.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T12.4.4.2.m1.1b"><cn type="integer" id="A6.T12.4.4.2.m1.1.1.cmml" xref="A6.T12.4.4.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.4.4.2.m1.1c">100</annotation></semantics></math><span id="A6.T12.4.4.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.6.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.8.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.6.6.8.1.1.1" class="ltx_text" style="font-size:90%;">.WET files</span></span>
</span>
</td>
<td id="A6.T12.6.6.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.9.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.6.6.9.1.1.1" class="ltx_text" style="font-size:90%;">Line-level w/ fastText </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.6.6.9.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Joulin et&nbsp;al.<span id="A6.T12.6.6.9.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib45" title="" class="ltx_ref">2016</a><span id="A6.T12.6.6.9.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.5.5.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.5.5.3.1.1.1" class="ltx_text" style="font-size:90%;">Line </span><math id="A6.T12.5.5.3.1.1.m1.1" class="ltx_Math" alttext="<100" display="inline"><semantics id="A6.T12.5.5.3.1.1.m1.1a"><mrow id="A6.T12.5.5.3.1.1.m1.1.1" xref="A6.T12.5.5.3.1.1.m1.1.1.cmml"><mi id="A6.T12.5.5.3.1.1.m1.1.1.2" xref="A6.T12.5.5.3.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.5.5.3.1.1.m1.1.1.1" xref="A6.T12.5.5.3.1.1.m1.1.1.1.cmml">&lt;</mo><mn mathsize="90%" id="A6.T12.5.5.3.1.1.m1.1.1.3" xref="A6.T12.5.5.3.1.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.5.5.3.1.1.m1.1b"><apply id="A6.T12.5.5.3.1.1.m1.1.1.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1"><lt id="A6.T12.5.5.3.1.1.m1.1.1.1.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="A6.T12.5.5.3.1.1.m1.1.1.2.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.5.5.3.1.1.m1.1.1.3.cmml" xref="A6.T12.5.5.3.1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.5.5.3.1.1.m1.1c">&lt;100</annotation></semantics></math><span id="A6.T12.5.5.3.1.1.2" class="ltx_text" style="font-size:90%;"> characters</span></span>
</span>
</td>
<td id="A6.T12.6.6.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.10.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.6.6.10.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="A6.T12.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.6.6.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.6.6.4.1.1.1" class="ltx_text" style="font-size:90%;">(optional) </span><span id="A6.T12.6.6.4.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="A6.T12.6.6.4.1.1.3" class="ltx_text" style="font-size:90%;">: per line (</span><math id="A6.T12.6.6.4.1.1.m1.1" class="ltx_Math" alttext="\sim 55\%" display="inline"><semantics id="A6.T12.6.6.4.1.1.m1.1a"><mrow id="A6.T12.6.6.4.1.1.m1.1.1" xref="A6.T12.6.6.4.1.1.m1.1.1.cmml"><mi id="A6.T12.6.6.4.1.1.m1.1.1.2" xref="A6.T12.6.6.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.6.6.4.1.1.m1.1.1.1" xref="A6.T12.6.6.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="A6.T12.6.6.4.1.1.m1.1.1.3" xref="A6.T12.6.6.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="A6.T12.6.6.4.1.1.m1.1.1.3.2" xref="A6.T12.6.6.4.1.1.m1.1.1.3.2.cmml">55</mn><mo mathsize="90%" id="A6.T12.6.6.4.1.1.m1.1.1.3.1" xref="A6.T12.6.6.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.6.6.4.1.1.m1.1b"><apply id="A6.T12.6.6.4.1.1.m1.1.1.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.6.6.4.1.1.m1.1.1.1.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.6.6.4.1.1.m1.1.1.2.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.2">absent</csymbol><apply id="A6.T12.6.6.4.1.1.m1.1.1.3.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.T12.6.6.4.1.1.m1.1.1.3.1.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A6.T12.6.6.4.1.1.m1.1.1.3.2.cmml" xref="A6.T12.6.6.4.1.1.m1.1.1.3.2">55</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.6.6.4.1.1.m1.1c">\sim 55\%</annotation></semantics></math><span id="A6.T12.6.6.4.1.1.4" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.8.8" class="ltx_tr">
<td id="A6.T12.8.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">OSCAR 22.01</span><span id="A6.T12.8.8.3.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.8.8.3.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Abadji et&nbsp;al.<span id="A6.T12.8.8.3.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib2" title="" class="ltx_ref">2022</a><span id="A6.T12.8.8.3.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.8.8.4" class="ltx_td ltx_align_top"></td>
<td id="A6.T12.7.7.1" class="ltx_td ltx_align_center">
<math id="A6.T12.7.7.1.m1.1" class="ltx_Math" alttext="\sim 283" display="inline"><semantics id="A6.T12.7.7.1.m1.1a"><mrow id="A6.T12.7.7.1.m1.1.1" xref="A6.T12.7.7.1.m1.1.1.cmml"><mi id="A6.T12.7.7.1.m1.1.1.2" xref="A6.T12.7.7.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.7.7.1.m1.1.1.1" xref="A6.T12.7.7.1.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.7.7.1.m1.1.1.3" xref="A6.T12.7.7.1.m1.1.1.3.cmml">283</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.7.7.1.m1.1b"><apply id="A6.T12.7.7.1.m1.1.1.cmml" xref="A6.T12.7.7.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.7.7.1.m1.1.1.1.cmml" xref="A6.T12.7.7.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.7.7.1.m1.1.1.2.cmml" xref="A6.T12.7.7.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.7.7.1.m1.1.1.3.cmml" xref="A6.T12.7.7.1.m1.1.1.3">283</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.7.7.1.m1.1c">\sim 283</annotation></semantics></math><span id="A6.T12.7.7.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.8.8.5" class="ltx_td ltx_align_center"><span id="A6.T12.8.8.5.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.8.8.2" class="ltx_td ltx_align_center">
<math id="A6.T12.8.8.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T12.8.8.2.m1.1a"><mn mathsize="90%" id="A6.T12.8.8.2.m1.1.1" xref="A6.T12.8.8.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T12.8.8.2.m1.1b"><cn type="integer" id="A6.T12.8.8.2.m1.1.1.cmml" xref="A6.T12.8.8.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.8.8.2.m1.1c">100</annotation></semantics></math><span id="A6.T12.8.8.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.8.8.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.8.8.6.1.1.1" class="ltx_text" style="font-size:90%;">.WET files</span></span>
</span>
</td>
<td id="A6.T12.8.8.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.7.1.1.1" class="ltx_text" style="font-size:90%;">Document-level w/ fastText </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.8.8.7.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Joulin et&nbsp;al.<span id="A6.T12.8.8.7.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib45" title="" class="ltx_ref">2016</a><span id="A6.T12.8.8.7.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.8.8.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.8.1.1.1" class="ltx_text" style="font-size:90%;">Line-level, optional document-level</span></span>
</span>
</td>
<td id="A6.T12.8.8.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.8.8.9.1.1.1" class="ltx_text" style="font-size:90%;">Optional NSFW blocklist</span></span>
</span>
</td>
<td id="A6.T12.8.8.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.8.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.8.8.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.8.8.10.1.1.1" class="ltx_text" style="font-size:90%;">(optional) </span><span id="A6.T12.8.8.10.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact</span><span id="A6.T12.8.8.10.1.1.3" class="ltx_text" style="font-size:90%;">: per line</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.22.26" class="ltx_tr">
<td id="A6.T12.22.26.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="10"><span id="A6.T12.22.26.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Curated datasets</span></td>
</tr>
<tr id="A6.T12.12.12" class="ltx_tr">
<td id="A6.T12.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="2">
<math id="A6.T12.9.9.1.m1.1" class="ltx_Math" alttext="\blacksquare" display="inline"><semantics id="A6.T12.9.9.1.m1.1a"><mi mathcolor="#5F57DB" mathsize="90%" mathvariant="normal" id="A6.T12.9.9.1.m1.1.1" xref="A6.T12.9.9.1.m1.1.1.cmml">■</mi><annotation-xml encoding="MathML-Content" id="A6.T12.9.9.1.m1.1b"><ci id="A6.T12.9.9.1.m1.1.1.cmml" xref="A6.T12.9.9.1.m1.1.1">■</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.9.9.1.m1.1c">\blacksquare</annotation></semantics></math><span id="A6.T12.9.9.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#5F57DB;"> GPT-3</span><span id="A6.T12.9.9.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.9.9.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Brown et&nbsp;al.<span id="A6.T12.9.9.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib18" title="" class="ltx_ref">2020</a><span id="A6.T12.9.9.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td id="A6.T12.10.10.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.10.10.2.m1.1" class="ltx_Math" alttext="300" display="inline"><semantics id="A6.T12.10.10.2.m1.1a"><mn mathsize="90%" id="A6.T12.10.10.2.m1.1.1" xref="A6.T12.10.10.2.m1.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="A6.T12.10.10.2.m1.1b"><cn type="integer" id="A6.T12.10.10.2.m1.1.1.cmml" xref="A6.T12.10.10.2.m1.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.10.10.2.m1.1c">300</annotation></semantics></math><span id="A6.T12.10.10.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.12.12.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A6.T12.12.12.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="A6.T12.11.11.3" class="ltx_td ltx_align_center ltx_border_t">
<math id="A6.T12.11.11.3.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="A6.T12.11.11.3.m1.1a"><mn mathsize="90%" id="A6.T12.11.11.3.m1.1.1" xref="A6.T12.11.11.3.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="A6.T12.11.11.3.m1.1b"><cn type="integer" id="A6.T12.11.11.3.m1.1.1.cmml" xref="A6.T12.11.11.3.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.11.11.3.m1.1c">60</annotation></semantics></math><span id="A6.T12.11.11.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.12.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.12.12.6.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.12.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.12.12.7.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.12.12.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.12.12.8.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.12.12.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.12.12.9.1.1.1" class="ltx_text" style="font-size:90%;">fastText trained on HQ-data</span></span>
</span>
</td>
<td id="A6.T12.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A6.T12.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.12.12.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.12.12.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="A6.T12.12.12.4.1.1.2" class="ltx_text" style="font-size:90%;">: minhash with 10 hashes (</span><math id="A6.T12.12.12.4.1.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="A6.T12.12.12.4.1.1.m1.1a"><mrow id="A6.T12.12.12.4.1.1.m1.1.1" xref="A6.T12.12.12.4.1.1.m1.1.1.cmml"><mi id="A6.T12.12.12.4.1.1.m1.1.1.2" xref="A6.T12.12.12.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.12.12.4.1.1.m1.1.1.1" xref="A6.T12.12.12.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="A6.T12.12.12.4.1.1.m1.1.1.3" xref="A6.T12.12.12.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="A6.T12.12.12.4.1.1.m1.1.1.3.2" xref="A6.T12.12.12.4.1.1.m1.1.1.3.2.cmml">10</mn><mo mathsize="90%" id="A6.T12.12.12.4.1.1.m1.1.1.3.1" xref="A6.T12.12.12.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.12.12.4.1.1.m1.1b"><apply id="A6.T12.12.12.4.1.1.m1.1.1.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.12.12.4.1.1.m1.1.1.1.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.12.12.4.1.1.m1.1.1.2.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.2">absent</csymbol><apply id="A6.T12.12.12.4.1.1.m1.1.1.3.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.T12.12.12.4.1.1.m1.1.1.3.1.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A6.T12.12.12.4.1.1.m1.1.1.3.2.cmml" xref="A6.T12.12.12.4.1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.12.12.4.1.1.m1.1c">\sim 10\%</annotation></semantics></math><span id="A6.T12.12.12.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.16.16" class="ltx_tr">
<td id="A6.T12.13.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.13.13.1.1.1" class="ltx_p" style="width:56.9pt;"><math id="A6.T12.13.13.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="A6.T12.13.13.1.1.1.m1.1a"><mi mathcolor="#7DD86E" mathsize="90%" mathvariant="normal" id="A6.T12.13.13.1.1.1.m1.1.1" xref="A6.T12.13.13.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="A6.T12.13.13.1.1.1.m1.1b"><ci id="A6.T12.13.13.1.1.1.m1.1.1.cmml" xref="A6.T12.13.13.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.13.13.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="A6.T12.13.13.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#7DD86E;"> The Pile</span><span id="A6.T12.13.13.1.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.13.13.1.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Gao et&nbsp;al.<span id="A6.T12.13.13.1.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib33" title="" class="ltx_ref">2020</a><span id="A6.T12.13.13.1.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.16.16.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.5.1.1.1" class="ltx_text" style="font-size:90%;">GPT-J </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Wang &amp; Komatsuzaki<span id="A6.T12.16.16.5.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib76" title="" class="ltx_ref">2021</a><span id="A6.T12.16.16.5.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.16.16.5.1.1.5" class="ltx_text" style="font-size:90%;">, GPT-NeoX-20B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.6.1" class="ltx_text" style="font-size:90%;">(</span>Black et&nbsp;al.<span id="A6.T12.16.16.5.1.1.7.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib16" title="" class="ltx_ref">2022</a><span id="A6.T12.16.16.5.1.1.8.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.16.16.5.1.1.9" class="ltx_text" style="font-size:90%;">, Pythia </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.10.1" class="ltx_text" style="font-size:90%;">(</span>Biderman et&nbsp;al.<span id="A6.T12.16.16.5.1.1.11.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib13" title="" class="ltx_ref">2023</a><span id="A6.T12.16.16.5.1.1.12.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.16.16.5.1.1.13" class="ltx_text" style="font-size:90%;">, Cerebras-GPT </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.5.1.1.14.1" class="ltx_text" style="font-size:90%;">(</span>Dey et&nbsp;al.<span id="A6.T12.16.16.5.1.1.15.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib30" title="" class="ltx_ref">2023</a><span id="A6.T12.16.16.5.1.1.16.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.14.14.2" class="ltx_td ltx_align_center">
<math id="A6.T12.14.14.2.m1.1" class="ltx_Math" alttext="\sim 340" display="inline"><semantics id="A6.T12.14.14.2.m1.1a"><mrow id="A6.T12.14.14.2.m1.1.1" xref="A6.T12.14.14.2.m1.1.1.cmml"><mi id="A6.T12.14.14.2.m1.1.1.2" xref="A6.T12.14.14.2.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.14.14.2.m1.1.1.1" xref="A6.T12.14.14.2.m1.1.1.1.cmml">∼</mo><mn mathsize="90%" id="A6.T12.14.14.2.m1.1.1.3" xref="A6.T12.14.14.2.m1.1.1.3.cmml">340</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.14.14.2.m1.1b"><apply id="A6.T12.14.14.2.m1.1.1.cmml" xref="A6.T12.14.14.2.m1.1.1"><csymbol cd="latexml" id="A6.T12.14.14.2.m1.1.1.1.cmml" xref="A6.T12.14.14.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.14.14.2.m1.1.1.2.cmml" xref="A6.T12.14.14.2.m1.1.1.2">absent</csymbol><cn type="integer" id="A6.T12.14.14.2.m1.1.1.3.cmml" xref="A6.T12.14.14.2.m1.1.1.3">340</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.14.14.2.m1.1c">\sim 340</annotation></semantics></math><span id="A6.T12.14.14.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.16.16.6" class="ltx_td ltx_align_center"><span id="A6.T12.16.16.6.1" class="ltx_text" style="font-size:90%;">Public</span></td>
<td id="A6.T12.15.15.3" class="ltx_td ltx_align_center">
<math id="A6.T12.15.15.3.m1.1" class="ltx_Math" alttext="18" display="inline"><semantics id="A6.T12.15.15.3.m1.1a"><mn mathsize="90%" id="A6.T12.15.15.3.m1.1.1" xref="A6.T12.15.15.3.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="A6.T12.15.15.3.m1.1b"><cn type="integer" id="A6.T12.15.15.3.m1.1.1.cmml" xref="A6.T12.15.15.3.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.15.15.3.m1.1c">18</annotation></semantics></math><span id="A6.T12.15.15.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.16.16.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.16.16.7.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">jusText</span><span id="A6.T12.16.16.7.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.7.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Pomikálek<span id="A6.T12.16.16.7.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib59" title="" class="ltx_ref">2011</a><span id="A6.T12.16.16.7.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.16.16.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.8.1.1.1" class="ltx_text" style="font-size:90%;">Document-level w/ </span><span id="A6.T12.16.16.8.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">pycld2</span><span id="A6.T12.16.16.8.1.1.3" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.16.16.8.1.1.4.1" class="ltx_text" style="font-size:90%;">(</span>Sites<span id="A6.T12.16.16.8.1.1.5.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib69" title="" class="ltx_ref">2013</a><span id="A6.T12.16.16.8.1.1.6.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.16.16.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.9.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.9.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="A6.T12.16.16.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.10.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.16.16.10.1.1.1" class="ltx_text" style="font-size:90%;">fastText on curated crawl</span></span>
</span>
</td>
<td id="A6.T12.16.16.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.16.16.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.16.16.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fuzzy</span><span id="A6.T12.16.16.4.1.1.2" class="ltx_text" style="font-size:90%;">: minhash with 10 hashes, sim. treshold 0.5 (</span><math id="A6.T12.16.16.4.1.1.m1.1" class="ltx_Math" alttext="\sim 26\%" display="inline"><semantics id="A6.T12.16.16.4.1.1.m1.1a"><mrow id="A6.T12.16.16.4.1.1.m1.1.1" xref="A6.T12.16.16.4.1.1.m1.1.1.cmml"><mi id="A6.T12.16.16.4.1.1.m1.1.1.2" xref="A6.T12.16.16.4.1.1.m1.1.1.2.cmml"></mi><mo mathsize="90%" id="A6.T12.16.16.4.1.1.m1.1.1.1" xref="A6.T12.16.16.4.1.1.m1.1.1.1.cmml">∼</mo><mrow id="A6.T12.16.16.4.1.1.m1.1.1.3" xref="A6.T12.16.16.4.1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="A6.T12.16.16.4.1.1.m1.1.1.3.2" xref="A6.T12.16.16.4.1.1.m1.1.1.3.2.cmml">26</mn><mo mathsize="90%" id="A6.T12.16.16.4.1.1.m1.1.1.3.1" xref="A6.T12.16.16.4.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.16.16.4.1.1.m1.1b"><apply id="A6.T12.16.16.4.1.1.m1.1.1.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.16.16.4.1.1.m1.1.1.1.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A6.T12.16.16.4.1.1.m1.1.1.2.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.2">absent</csymbol><apply id="A6.T12.16.16.4.1.1.m1.1.1.3.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.3"><csymbol cd="latexml" id="A6.T12.16.16.4.1.1.m1.1.1.3.1.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="A6.T12.16.16.4.1.1.m1.1.1.3.2.cmml" xref="A6.T12.16.16.4.1.1.m1.1.1.3.2">26</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.16.16.4.1.1.m1.1c">\sim 26\%</annotation></semantics></math><span id="A6.T12.16.16.4.1.1.3" class="ltx_text" style="font-size:90%;"> removed)</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.18.18" class="ltx_tr">
<td id="A6.T12.18.18.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MassiveWeb</span><span id="A6.T12.18.18.3.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.18.18.3.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Rae et&nbsp;al.<span id="A6.T12.18.18.3.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib63" title="" class="ltx_ref">2021</a><span id="A6.T12.18.18.3.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.18.18.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.4.1.1.1" class="ltx_text" style="font-size:90%;">Gopher </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.18.18.4.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Rae et&nbsp;al.<span id="A6.T12.18.18.4.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib63" title="" class="ltx_ref">2021</a><span id="A6.T12.18.18.4.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite><span id="A6.T12.18.18.4.1.1.5" class="ltx_text" style="font-size:90%;">, Chinchilla </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.18.18.4.1.1.6.1" class="ltx_text" style="font-size:90%;">(</span>Hoffmann et&nbsp;al.<span id="A6.T12.18.18.4.1.1.7.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib41" title="" class="ltx_ref">2022</a><span id="A6.T12.18.18.4.1.1.8.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.17.17.1" class="ltx_td ltx_align_center">
<math id="A6.T12.17.17.1.m1.2" class="ltx_Math" alttext="1,400" display="inline"><semantics id="A6.T12.17.17.1.m1.2a"><mrow id="A6.T12.17.17.1.m1.2.3.2" xref="A6.T12.17.17.1.m1.2.3.1.cmml"><mn mathsize="90%" id="A6.T12.17.17.1.m1.1.1" xref="A6.T12.17.17.1.m1.1.1.cmml">1</mn><mo mathsize="90%" id="A6.T12.17.17.1.m1.2.3.2.1" xref="A6.T12.17.17.1.m1.2.3.1.cmml">,</mo><mn mathsize="90%" id="A6.T12.17.17.1.m1.2.2" xref="A6.T12.17.17.1.m1.2.2.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.17.17.1.m1.2b"><list id="A6.T12.17.17.1.m1.2.3.1.cmml" xref="A6.T12.17.17.1.m1.2.3.2"><cn type="integer" id="A6.T12.17.17.1.m1.1.1.cmml" xref="A6.T12.17.17.1.m1.1.1">1</cn><cn type="integer" id="A6.T12.17.17.1.m1.2.2.cmml" xref="A6.T12.17.17.1.m1.2.2">400</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.17.17.1.m1.2c">1,400</annotation></semantics></math><span id="A6.T12.17.17.1.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.18.18.5" class="ltx_td ltx_align_center"><span id="A6.T12.18.18.5.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="A6.T12.18.18.2" class="ltx_td ltx_align_center">
<math id="A6.T12.18.18.2.m1.1" class="ltx_Math" alttext="48" display="inline"><semantics id="A6.T12.18.18.2.m1.1a"><mn mathsize="90%" id="A6.T12.18.18.2.m1.1.1" xref="A6.T12.18.18.2.m1.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="A6.T12.18.18.2.m1.1b"><cn type="integer" id="A6.T12.18.18.2.m1.1.1.cmml" xref="A6.T12.18.18.2.m1.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.18.18.2.m1.1c">48</annotation></semantics></math><span id="A6.T12.18.18.2.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.18.18.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.18.18.6.1.1.1" class="ltx_text" style="font-size:90%;">Custom</span></span>
</span>
</td>
<td id="A6.T12.18.18.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.7.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.18.18.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.8.1.1.1" class="ltx_text" style="font-size:90%;">Document-level</span></span>
</span>
</td>
<td id="A6.T12.18.18.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.18.18.9.1.1.1" class="ltx_text" style="font-size:90%;">SafeSearch</span></span>
</span>
</td>
<td id="A6.T12.18.18.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.18.18.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.18.18.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.18.18.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact &amp; fuzzy</span><span id="A6.T12.18.18.10.1.1.2" class="ltx_text" style="font-size:90%;">: exact documents, minhash w/ sim. treshold 0.8</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.21.21" class="ltx_tr">
<td id="A6.T12.19.19.1" class="ltx_td ltx_align_justify ltx_align_top" colspan="2">
<math id="A6.T12.19.19.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="A6.T12.19.19.1.m1.1a"><mi mathcolor="#DB5F56" mathsize="90%" mathvariant="normal" id="A6.T12.19.19.1.m1.1.1" xref="A6.T12.19.19.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="A6.T12.19.19.1.m1.1b"><ci id="A6.T12.19.19.1.m1.1.1.cmml" xref="A6.T12.19.19.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.19.19.1.m1.1c">\bigstar</annotation></semantics></math><span id="A6.T12.19.19.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#DB5F56;"> PaLM</span><span id="A6.T12.19.19.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.19.19.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Chowdhery et&nbsp;al.<span id="A6.T12.19.19.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib23" title="" class="ltx_ref">2022</a><span id="A6.T12.19.19.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</td>
<td id="A6.T12.20.20.2" class="ltx_td ltx_align_center">
<math id="A6.T12.20.20.2.m1.1" class="ltx_Math" alttext="780" display="inline"><semantics id="A6.T12.20.20.2.m1.1a"><mn mathsize="90%" id="A6.T12.20.20.2.m1.1.1" xref="A6.T12.20.20.2.m1.1.1.cmml">780</mn><annotation-xml encoding="MathML-Content" id="A6.T12.20.20.2.m1.1b"><cn type="integer" id="A6.T12.20.20.2.m1.1.1.cmml" xref="A6.T12.20.20.2.m1.1.1">780</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.20.20.2.m1.1c">780</annotation></semantics></math><span id="A6.T12.20.20.2.1" class="ltx_text" style="font-size:90%;">GT</span>
</td>
<td id="A6.T12.21.21.4" class="ltx_td ltx_align_center"><span id="A6.T12.21.21.4.1" class="ltx_text" style="font-size:90%;">Private</span></td>
<td id="A6.T12.21.21.3" class="ltx_td ltx_align_center">
<math id="A6.T12.21.21.3.m1.1" class="ltx_Math" alttext="27" display="inline"><semantics id="A6.T12.21.21.3.m1.1a"><mn mathsize="90%" id="A6.T12.21.21.3.m1.1.1" xref="A6.T12.21.21.3.m1.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="A6.T12.21.21.3.m1.1b"><cn type="integer" id="A6.T12.21.21.3.m1.1.1.cmml" xref="A6.T12.21.21.3.m1.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.21.21.3.m1.1c">27</annotation></semantics></math><span id="A6.T12.21.21.3.1" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="A6.T12.21.21.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.5.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.21.21.5.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.21.21.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.6.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.21.21.6.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
<td id="A6.T12.21.21.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.21.21.7.1.1.1" class="ltx_text" style="font-size:90%;">Document-level</span></span>
</span>
</td>
<td id="A6.T12.21.21.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.8.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.21.21.8.1.1.1" class="ltx_text" style="font-size:90%;">ML-based filter on HQ data</span></span>
</span>
</td>
<td id="A6.T12.21.21.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A6.T12.21.21.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.21.21.9.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.21.21.9.1.1.1" class="ltx_text" style="font-size:90%;">Unknown</span></span>
</span>
</td>
</tr>
<tr id="A6.T12.22.27" class="ltx_tr">
<td id="A6.T12.22.27.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="10"><span id="A6.T12.22.27.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps" style="font-size:90%;">Ours</span></td>
</tr>
<tr id="A6.T12.22.22" class="ltx_tr">
<td id="A6.T12.22.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.2.1.1.1" class="ltx_text" style="font-size:90%;color:#DB57B2;">●<span id="A6.T12.22.22.2.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">RefinedWeb</span></span></span>
</span>
</td>
<td id="A6.T12.22.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.3.1.1.1" class="ltx_text" style="font-size:90%;">Falcon-RW</span></span>
</span>
</td>
<td id="A6.T12.22.22.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A6.T12.22.22.4.1" class="ltx_text" style="font-size:90%;">5,000GT</span></td>
<td id="A6.T12.22.22.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A6.T12.22.22.5.1" class="ltx_text" style="font-size:90%;">600GT Public</span></td>
<td id="A6.T12.22.22.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math id="A6.T12.22.22.1.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="A6.T12.22.22.1.m1.1a"><mrow id="A6.T12.22.22.1.m1.1.1" xref="A6.T12.22.22.1.m1.1.1.cmml"><mn mathsize="90%" id="A6.T12.22.22.1.m1.1.1.2" xref="A6.T12.22.22.1.m1.1.1.2.cmml">100</mn><mo mathsize="90%" id="A6.T12.22.22.1.m1.1.1.1" xref="A6.T12.22.22.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A6.T12.22.22.1.m1.1b"><apply id="A6.T12.22.22.1.m1.1.1.cmml" xref="A6.T12.22.22.1.m1.1.1"><csymbol cd="latexml" id="A6.T12.22.22.1.m1.1.1.1.cmml" xref="A6.T12.22.22.1.m1.1.1.1">percent</csymbol><cn type="integer" id="A6.T12.22.22.1.m1.1.1.2.cmml" xref="A6.T12.22.22.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.22.22.1.m1.1c">100\%</annotation></semantics></math></td>
<td id="A6.T12.22.22.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.22.6.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">trafilatura</span><span id="A6.T12.22.22.6.1.1.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.22.22.6.1.1.3.1" class="ltx_text" style="font-size:90%;">(</span>Barbaresi<span id="A6.T12.22.22.6.1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib11" title="" class="ltx_ref">2021</a><span id="A6.T12.22.22.6.1.1.5.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.22.22.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.7.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.7.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.7.1.1.1" class="ltx_text" style="font-size:90%;">From CCNet </span><cite class="ltx_cite ltx_citemacro_citep"><span id="A6.T12.22.22.7.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Wenzek et&nbsp;al.<span id="A6.T12.22.22.7.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib81" title="" class="ltx_ref">2020</a><span id="A6.T12.22.22.7.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td id="A6.T12.22.22.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.8.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.8.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.8.1.1.1" class="ltx_text" style="font-size:90%;">Document and line-level</span></span>
</span>
</td>
<td id="A6.T12.22.22.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.9.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.9.1.1" class="ltx_p" style="width:62.6pt;"><span id="A6.T12.22.22.9.1.1.1" class="ltx_text" style="font-size:90%;">URL blocklist</span></span>
</span>
</td>
<td id="A6.T12.22.22.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A6.T12.22.22.10.1" class="ltx_inline-block ltx_align_top">
<span id="A6.T12.22.22.10.1.1" class="ltx_p" style="width:56.9pt;"><span id="A6.T12.22.22.10.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exact &amp; fuzzy</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Details of the Macrodata Refinement pipeline</h2>

<section id="A7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>URL filtering</h3>

<div id="A7.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS1.p1.1"><a class="ltx_ref" href="#S3.SS1.SSS0.Px2" title="URL filtering. ‣ 3.1 Document preparation: reading data, filtering URLs, extracting text, and language identification ‣ 3 Macrodata Refinement and RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>에서 살펴본 바와 같이 성인문서의 필터링은 문서의 내용이 아닌 URL 자체만을 기준으로 한다. 이 디자인 선택은 (1) 문서 <cite class="ltx_cite ltx_citemacro_citep">(Welbl et al., <a class="ltx_ref" href="#bib.bib80" title="">2021</a>)</cite>의 내용에 ML 기반 분류기를 사용할 때 소수자로부터 콘텐츠를 오버필터링하는 것을 방지하는 데 어려움이 있다; (2) 콘텐츠에 적용된 NSFW 단어 블록 리스트(예: C4에서 사용된 것)도 법적 및 의학적 콘텐츠 <cite class="ltx_cite ltx_citemacro_citep">(Dodge et al., <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>의 오버필터링을 초래한다.</p>
</div>
<div id="A7.SS1.p2" class="ltx_para">
<p class="ltx_p" id="A7.SS1.p2.1">우리의 URL 필터링은 성인 콘텐츠와 관련되거나, 사용자에게 유해할 수 있거나, 대부분 구조화되지 않은 텍스트/스팸(예를 들어, 파일 호스팅 웹사이트)을 포함할 가능성이 매우 높은 도메인을 찾는 것에 초점을 맞춘다. 먼저, 명시적으로 금지된 <a class="ltx_ref" href="#A7.SS1.SSS1" title="G.1.1 URL Blocklist ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.1.1</span></a>에 자세히 설명된 4.6M 도메인 목록을 집계한 다음, 선별한 단어 목록과 URL의 하위 단어를 일치시키는 것을 기반으로 간단한 URL 채점 시스템을 구축했다(<a class="ltx_ref" href="#A7.SS1.SSS2" title="G.1.2 URL Scoring with a Word-List ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.1.2</span></a> 참조). 독성 <cite class="ltx_cite ltx_citemacro_citep">(Hanu &amp; Unitary team, <a class="ltx_ref" href="#bib.bib39" title="">2020</a>)</cite>의 이상치로 ToxicBERT에 의해 표면화된 페이지와 수동 검사, 상호 참조 결과를 기반으로 이 단어 목록을 선별했다.</p>
</div>
<section id="A7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.1.1 </span>URL Blocklist</h4>

<section id="A7.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Origin of the list.</h5>

<div id="A7.SS1.SSS1.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS1.SSS1.Px1.p1.1">우리는 명시적으로 금지한 약 4.6M URL의 집계된 목록<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_tag ltx_tag_note">†</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dsi.ut-capitole.fr/blacklists/" target="_blank" title="">https://dsi.ut-capitole.fr/blacklists/</a></span></span></span>을 사용한다. 이 목록은 범주(예: 포르노, 도박)에서 끊어졌으며 <a class="ltx_ref" href="#A7.T13" title="In Curation. ‣ G.1.1 URL Blocklist ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">13</span></a>에서 선택한 범주의 개요를 설명한다. 목록은 정기적으로 업데이트되며, 원래 의도된 용도를 대학용 블록 목록으로 사용합니다.</p>
</div>
</section>
<section id="A7.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Curation.</h5>

<div id="A7.SS1.SSS1.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS1.SSS1.Px2.p1.2">목록이 여러 도메인을 부적절하게 차단했음을 알 수 있습니다. 이러한 도메인은 적지만(<math alttext="&lt;" class="ltx_Math" display="inline" id="A7.SS1.SSS1.Px2.p1.1.m1.1"><semantics id="A7.SS1.SSS1.Px2.p1.1.m1.1a"><mo id="A7.SS1.SSS1.Px2.p1.1.m1.1.1" xref="A7.SS1.SSS1.Px2.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS1.Px2.p1.1.m1.1b"><lt id="A7.SS1.SSS1.Px2.p1.1.m1.1.1.cmml" xref="A7.SS1.SSS1.Px2.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS1.Px2.p1.1.m1.1c">&lt;</annotation></semantics></math>100), 목록에 의해 필터링된 데이터의 상당 부분을 차지하며, 이는 수천 페이지의 콘텐츠를 포함하는 다소 많은 도메인이기 때문입니다. 이러한 허위 도메인을 식별하기 위해 832M 페이지의 하위 집합에 블록리스트를 적용하였으며, 블록리스트와 일치하는 6.04M(<math alttext="0.73\%" class="ltx_Math" display="inline" id="A7.SS1.SSS1.Px2.p1.2.m2.1"><semantics id="A7.SS1.SSS1.Px2.p1.2.m2.1a"><mrow id="A7.SS1.SSS1.Px2.p1.2.m2.1.1" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.cmml"><mn id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2.cmml">0.73</mn><mo id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS1.SSS1.Px2.p1.2.m2.1b"><apply id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.cmml" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1"><csymbol cd="latexml" id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1.cmml" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.1">percent</csymbol><cn id="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2.cmml" type="float" xref="A7.SS1.SSS1.Px2.p1.2.m2.1.1.2">0.73</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS1.SSS1.Px2.p1.2.m2.1c">0.73\%</annotation></semantics></math>) 페이지와 URL 당 발생 횟수는 1~79k 범위였다. 4k회 이상 일치하는 모든 URL을 수동으로 검사했으며 이는 데이터 세트의 상당한 부분을 나타낸다. 우리는 대중문화 뉴스 웹사이트나 블로그 플랫폼과 같은 많은 양성 도메인을 발견했는데, 이는 목록에서 제외되었다.</p>
</div>
<figure id="A7.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 13:</span><span class="ltx_text ltx_font_bold" id="A7.T13.2.1">우리는 스팸 또는 구조화되지 않은 텍스트뿐만 아니라 성인 또는 악의적인 콘텐츠를 포함할 가능성이 있는 카테고리를 선택한다. </span></figcaption>
<table id="A7.T13.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A7.T13.3.1" class="ltx_tr">
<td id="A7.T13.3.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T13.3.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="A7.T13.3.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T13.3.1.2.1" class="ltx_text ltx_font_bold">Description</span></td>
<td id="A7.T13.3.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T13.3.1.3.1" class="ltx_text ltx_font_bold">Number of links</span></td>
</tr>
<tr id="A7.T13.3.2" class="ltx_tr">
<td id="A7.T13.3.2.1" class="ltx_td ltx_align_left ltx_border_t">adult</td>
<td id="A7.T13.3.2.2" class="ltx_td ltx_align_left ltx_border_t">adult websites: from eroticism to hard pornography</td>
<td id="A7.T13.3.2.3" class="ltx_td ltx_align_left ltx_border_t">4516478</td>
</tr>
<tr id="A7.T13.3.3" class="ltx_tr">
<td id="A7.T13.3.3.1" class="ltx_td ltx_align_left">phishing</td>
<td id="A7.T13.3.3.2" class="ltx_td ltx_align_left">phishing websites, malwares, etc.</td>
<td id="A7.T13.3.3.3" class="ltx_td ltx_align_left">42445</td>
</tr>
<tr id="A7.T13.3.4" class="ltx_tr">
<td id="A7.T13.3.4.1" class="ltx_td ltx_align_left">dating</td>
<td id="A7.T13.3.4.2" class="ltx_td ltx_align_left">dating websites</td>
<td id="A7.T13.3.4.3" class="ltx_td ltx_align_left">3829</td>
</tr>
<tr id="A7.T13.3.5" class="ltx_tr">
<td id="A7.T13.3.5.1" class="ltx_td ltx_align_left">gambling</td>
<td id="A7.T13.3.5.2" class="ltx_td ltx_align_left">online casino</td>
<td id="A7.T13.3.5.3" class="ltx_td ltx_align_left">1365</td>
</tr>
<tr id="A7.T13.3.6" class="ltx_tr">
<td id="A7.T13.3.6.1" class="ltx_td ltx_align_left">filehosting</td>
<td id="A7.T13.3.6.2" class="ltx_td ltx_align_left">websites hosting files, videos, pictures, music</td>
<td id="A7.T13.3.6.3" class="ltx_td ltx_align_left">909</td>
</tr>
<tr id="A7.T13.3.7" class="ltx_tr">
<td id="A7.T13.3.7.1" class="ltx_td ltx_align_left">ddos</td>
<td id="A7.T13.3.7.2" class="ltx_td ltx_align_left">websites related to ddos attacks</td>
<td id="A7.T13.3.7.3" class="ltx_td ltx_align_left">421</td>
</tr>
<tr id="A7.T13.3.8" class="ltx_tr">
<td id="A7.T13.3.8.1" class="ltx_td ltx_align_left">agressif</td>
<td id="A7.T13.3.8.2" class="ltx_td ltx_align_left">hate, racism, etc</td>
<td id="A7.T13.3.8.3" class="ltx_td ltx_align_left">390</td>
</tr>
<tr id="A7.T13.3.9" class="ltx_tr">
<td id="A7.T13.3.9.1" class="ltx_td ltx_align_left">chat</td>
<td id="A7.T13.3.9.2" class="ltx_td ltx_align_left">online chat websites</td>
<td id="A7.T13.3.9.3" class="ltx_td ltx_align_left">244</td>
</tr>
<tr id="A7.T13.3.10" class="ltx_tr">
<td id="A7.T13.3.10.1" class="ltx_td ltx_align_left">mixed adult</td>
<td id="A7.T13.3.10.2" class="ltx_td ltx_align_left">websites with some adult content</td>
<td id="A7.T13.3.10.3" class="ltx_td ltx_align_left">153</td>
</tr>
<tr id="A7.T13.3.11" class="ltx_tr">
<td id="A7.T13.3.11.1" class="ltx_td ltx_align_left ltx_border_bb">arjel</td>
<td id="A7.T13.3.11.2" class="ltx_td ltx_align_left ltx_border_bb">French regulated gambling websites</td>
<td id="A7.T13.3.11.3" class="ltx_td ltx_align_left ltx_border_bb">69</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.1.2 </span>URL Scoring with a Word-List</h4>

<div id="A7.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS1.SSS2.p1.1">URL 점수를 매기기 위해 소프트, 하드, 엄격한 위반 단어 목록을 기반으로 세 가지 일치하는 패턴을 사용했다.</p>
<ul id="A7.I1" class="ltx_itemize">
<li id="A7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i1.p1.1.1">Strict <span class="ltx_text ltx_framed ltx_framed_underline" id="A7.I1.i1.p1.1.1.1">sub</span>word matching</span>: http://foo<span class="ltx_text" id="A7.I1.i1.p1.1.2" style="color:#FF0000;">bann</span>. <span class="ltx_text" id="A7.I1.i1.p1.1.3" style="color:#FF0000;">edsub</span>-<span class="ltx_text" id="A7.I1.i1.p1.1.4" style="color:#FF0000;">wo</span>. <span class="ltx_text" id="A7.I1.i1.p1.1.5" style="color:#FF0000;">rd</span>bar.com/any/bar, matching words such as <span class="ltx_text ltx_font_typewriter" id="A7.I1.i1.p1.1.6" style="font-size:90%;">xvideos</span>, <span class="ltx_text ltx_font_typewriter" id="A7.I1.i1.p1.1.7" style="font-size:90%;">groupsex</span>;</p>
</div>
</li>
<li id="A7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.p1.1.1">Hard <span class="ltx_text ltx_framed ltx_framed_underline" id="A7.I1.i2.p1.1.1.1">whole</span>: http://www.foo.<span class="ltx_text" id="A7.I1.i2.p1.1.2" style="color:#FF8000;">porn</span>, <span class="ltx_text ltx_font_typewriter" id="A7.I1.i2.p1.1.3" style="font-size:90%;">xxx</span>, <span class="ltx_text ltx_font_typewriter" style="font-size:90%;">orgy</span>;</p>
</div>
</li>
<li id="A7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i3.p1.1.1.1.1">Soft word<span class="ltx_text" id="A7.I1.i3.p1.1.1.1.1">s</span>: http://www.foo.<span class="ltx_text" id="A7.I1.i3.p1.1.1.2" style="color:#0000FF;">soft1</span>-bar-<span class="ltx_text" id="A7.I1.i3.p1.1.3" style="font-size:90%;">sex</span>, <span class="ltx_text ltx_font_typewriter" id="A7.I1.i3.p1.1.1.4" style="font-size:90%;">webcam</span>, <span class="ltx_text ltx_font_typewriter"</p>
</div>
</li>
</ul>
</div>
<div id="A7.SS1.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="A7.SS1.SSS2.p2.1">각 목록은 서로 다른 수준의 심각도와 연관되어 있다: 가장 엄격한 것(엄격한 하위 단어 매칭)의 경우, 하위 문자열에서 금지된 단어와 일치하는 모든 URL을 금지한다(사기 웹 사이트가 성인 키워드를 분해하여 유사한 인식 체계를 탈출하려고 시도할 수 있기 때문에). 하드 전체 단어 매칭의 경우, 목록에서 전체 단어 매칭과 URL을 금지하고, 마지막으로 소프트 단어 매칭과 최소 2개의 매칭이 필요하다.</p>
</div>
<div id="A7.SS1.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="A7.SS1.SSS2.p3.1">우리는 ToxicBERT가 보고한 상위 히트에서 알 수 있는 데이터의 수동 검사를 기반으로 목록을 선별했다. 엄격한 하위 단어 매칭을 위해 성인 콘텐츠와 명확하게 관련된 단어를 포함했다(예: <span class="ltx_text ltx_font_typewriter" id="A7.SS1.SSS2.p3.1.1" style="font-size:90%;">groupsex</span>). 중성 단어의 일부일 수 있는 부분 불분명한 일치(예: <span class="ltx_text ltx_font_typewriter" id="A7.SS1.SSS2.p3.1.2" style="font-size:90%;">ass</span>)를 피했다(예: <span class="ltx_text ltx_font_typewriter" id="A7.SS1.SSS2.p3.1.3" style="font-size:90%;">massachusetts</span>). 소프트 워드 리스트에는 문서를 스스로 폐기할 충분한 이유가 되지 않지만 리스트의 여러 단어가 일치할 때 의심스러운 단어를 포함시켰다. 이는 의료 또는 법률 콘텐츠를 영향을 받지 않고 유지하는 데 도움이 되었습니다 (예: <span class="ltx_text ltx_font_typewriter" id="A7.SS1.SSS2.p3.1.4" 스타일="font-size:90%;">dick</span>).</p>
</div>
</section>
<section id="A7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.1.3 </span>Excluded High Quality Sources</h4>

<div id="A7.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS1.SSS3.p1.1">본 논문은 RefinedWeb의 연구에만 초점을 맞추고 있기 때문에 선별된 데이터의 일반적인 온라인 출처를 제외하기로 결정했다. 이것은 두 가지 목적을 제공한다: (1) RefinedWeb이 실제로 대부분 알려진 고품질 소스(예: 위키피디아는 C4의 상당 부분을 나타냄)로 만들어지지 않도록 함으로써 우리의 결과를 강화한다; (2) 향후 작업은 RefinedWeb과 기존 큐레이트된 코포라를 결합하는 데 관심이 있을 수 있으며, 이는 RefinedWeb에 포함될 경우 추가 중복 제거가 필요하다. 이에 따라 RefinedWeb에서 The Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite>에서 사용되는 공통 소스를 제거한다. 차단한 큐레이트된 데이터 원본 도메인의 전체 목록은 표 <a class="ltx_ref" href="#A7.T14" title="Table 14 ‣ G.1.3 Excluded High Quality Sources ‣ G.1 URL filtering ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">14</span></a>에 나와 있습니다.</p>
</div>
<figure id="A7.T14" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 14:</span><span class="ltx_text ltx_font_bold" id="A7.T14.2.1">RefinedWeb is stripped from common so-quality sources to simplify combining it with existing curated corpora</span>. 이 블록리스트는 성인 콘텐츠 블록리스트와 함께 URL 필터링 단계에서 적용된다.</figcaption>
<table id="A7.T14.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A7.T14.3.1" class="ltx_tr">
<td id="A7.T14.3.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T14.3.1.1.1" class="ltx_text ltx_font_bold">Curated data source</span></td>
<td id="A7.T14.3.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A7.T14.3.1.2.1" class="ltx_text ltx_font_bold">Domain name blocked</span></td>
</tr>
<tr id="A7.T14.3.2" class="ltx_tr">
<td id="A7.T14.3.2.1" class="ltx_td ltx_align_left ltx_border_t">arxiv</td>
<td id="A7.T14.3.2.2" class="ltx_td ltx_align_left ltx_border_t">arxiv.org</td>
</tr>
<tr id="A7.T14.3.3" class="ltx_tr">
<td id="A7.T14.3.3.1" class="ltx_td ltx_align_left">AskUbuntu</td>
<td id="A7.T14.3.3.2" class="ltx_td ltx_align_left">askubuntu.com</td>
</tr>
<tr id="A7.T14.3.4" class="ltx_tr">
<td id="A7.T14.3.4.1" class="ltx_td ltx_align_left">StackOverflow</td>
<td id="A7.T14.3.4.2" class="ltx_td ltx_align_left">stackoverflow.com</td>
</tr>
<tr id="A7.T14.3.5" class="ltx_tr">
<td id="A7.T14.3.5.1" class="ltx_td"></td>
<td id="A7.T14.3.5.2" class="ltx_td ltx_align_left">stackapps.com</td>
</tr>
<tr id="A7.T14.3.6" class="ltx_tr">
<td id="A7.T14.3.6.1" class="ltx_td"></td>
<td id="A7.T14.3.6.2" class="ltx_td ltx_align_left">stackexchange.com</td>
</tr>
<tr id="A7.T14.3.7" class="ltx_tr">
<td id="A7.T14.3.7.1" class="ltx_td"></td>
<td id="A7.T14.3.7.2" class="ltx_td ltx_align_left">mathoverflow.net</td>
</tr>
<tr id="A7.T14.3.8" class="ltx_tr">
<td id="A7.T14.3.8.1" class="ltx_td ltx_align_left">NIH Abstracts</td>
<td id="A7.T14.3.8.2" class="ltx_td ltx_align_left">exporter.nih.gov</td>
</tr>
<tr id="A7.T14.3.9" class="ltx_tr">
<td id="A7.T14.3.9.1" class="ltx_td"></td>
<td id="A7.T14.3.9.2" class="ltx_td ltx_align_left">ncbi.nlm.nih.gov</td>
</tr>
<tr id="A7.T14.3.10" class="ltx_tr">
<td id="A7.T14.3.10.1" class="ltx_td ltx_align_left">Github</td>
<td id="A7.T14.3.10.2" class="ltx_td ltx_align_left">github.com</td>
</tr>
<tr id="A7.T14.3.11" class="ltx_tr">
<td id="A7.T14.3.11.1" class="ltx_td ltx_align_left">Ubuntu IRC</td>
<td id="A7.T14.3.11.2" class="ltx_td ltx_align_left">irclogs.ubuntu.com</td>
</tr>
<tr id="A7.T14.3.12" class="ltx_tr">
<td id="A7.T14.3.12.1" class="ltx_td ltx_align_left">HackerNews</td>
<td id="A7.T14.3.12.2" class="ltx_td ltx_align_left">news.ycombinator.com</td>
</tr>
<tr id="A7.T14.3.13" class="ltx_tr">
<td id="A7.T14.3.13.1" class="ltx_td ltx_align_left">FreeLaw</td>
<td id="A7.T14.3.13.2" class="ltx_td ltx_align_left">courtlistener.com</td>
</tr>
<tr id="A7.T14.3.14" class="ltx_tr">
<td id="A7.T14.3.14.1" class="ltx_td ltx_align_left">Reddit</td>
<td id="A7.T14.3.14.2" class="ltx_td ltx_align_left">reddit.com</td>
</tr>
<tr id="A7.T14.3.15" class="ltx_tr">
<td id="A7.T14.3.15.1" class="ltx_td ltx_align_left">Europarl</td>
<td id="A7.T14.3.15.2" class="ltx_td ltx_align_left">statmt.org</td>
</tr>
<tr id="A7.T14.3.16" class="ltx_tr">
<td id="A7.T14.3.16.1" class="ltx_td ltx_align_left">United States Patents</td>
<td id="A7.T14.3.16.2" class="ltx_td ltx_align_left">uspto.gov</td>
</tr>
<tr id="A7.T14.3.17" class="ltx_tr">
<td id="A7.T14.3.17.1" class="ltx_td ltx_align_left ltx_border_bb">Wikipedia</td>
<td id="A7.T14.3.17.2" class="ltx_td ltx_align_left ltx_border_bb">wikipedia.org</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
<section id="A7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>Line-wise filtering</h3>

<div id="A7.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS2.p1.1">트라필라투라로 텍스트 추출을 실행하여 얻은 개선에도 불구하고, 우리는 여전히 많은 관련 없는 선이 침투한다는 것을 발견했다. 이러한 줄은 보통 탐색 메뉴, 작업 호출 또는 소셜 미디어 카운터와 관련이 있습니다. 데이터를 수동으로 검사한 후 라인별 필터링 전략을 고안했다. 우리는 문서를 라인 단위로 분석하고 다음 규칙을 기반으로 라인을 폐기하거나 편집합니다.</p>
<ul id="A7.I2" class="ltx_itemize">
<li id="A7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i1.p1.1">대문자(discard) 위주로 구성되어 있는 경우;</p>
</div>
</li>
<li id="A7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i2.p1.1">숫자만으로 구성되는 경우(discard);</p>
</div>
</li>
<li id="A7.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i3.p1.1">카운터(예: <span class="ltx_text ltx_font_typewriter" id="A7.I2.i3.p1.1.1" style="font-size:90%;">3 likes</span>)(discard);</p>
</div>
</li>
<li id="A7.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i4.p1.1">하나의 단어만을 포함하는 경우(discard);</p>
</div>
</li>
<li id="A7.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i5.p1" class="ltx_para">
<p id="A7.I2.i5.p1.1" class="ltx_p">If it is short (<math id="A7.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="\leq 10" display="inline"><semantics id="A7.I2.i5.p1.1.m1.1a"><mrow id="A7.I2.i5.p1.1.m1.1.1" xref="A7.I2.i5.p1.1.m1.1.1.cmml"><mi id="A7.I2.i5.p1.1.m1.1.1.2" xref="A7.I2.i5.p1.1.m1.1.1.2.cmml"></mi><mo id="A7.I2.i5.p1.1.m1.1.1.1" xref="A7.I2.i5.p1.1.m1.1.1.1.cmml">≤</mo><mn id="A7.I2.i5.p1.1.m1.1.1.3" xref="A7.I2.i5.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.I2.i5.p1.1.m1.1b"><apply id="A7.I2.i5.p1.1.m1.1.1.cmml" xref="A7.I2.i5.p1.1.m1.1.1"><leq id="A7.I2.i5.p1.1.m1.1.1.1.cmml" xref="A7.I2.i5.p1.1.m1.1.1.1"></leq><csymbol cd="latexml" id="A7.I2.i5.p1.1.m1.1.1.2.cmml" xref="A7.I2.i5.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="A7.I2.i5.p1.1.m1.1.1.3.cmml" xref="A7.I2.i5.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.I2.i5.p1.1.m1.1c">\leq 10</annotation></semantics></math> words) and matches a pattern (edit):</p>
<ul id="A7.I2.i5.I1" class="ltx_itemize">
<li id="A7.I2.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.I1.i1.p1.1">Line의 시작 부분에서 (e.g. <span class="ltx_text ltx_font_typewriter" id="A7.I2.i5.I1.i1.p1.1.1" style="font-size:90%;">sign-in</span>);</p>
</div>
</li>
<li id="A7.I2.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.I1.i2.p1.1">줄의 끝에서 (예: <span class="ltx_text ltx_font_typewriter" id="A7.I2.i5.I1.i2.p1.1.1" style="font-size:90%;">Read more...</span>);</p>
</div>
</li>
<li id="A7.I2.i5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.I1.i3.p1.1">(예: <span class="ltx_text ltx_font_typewriter" id="A7.I2.i5.I1.i3.p1.1.1" style="font-size:90%;">items in cart</span>).</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="A7.SS2.p2" class="ltx_para">
<p class="ltx_p" id="A7.SS2.p2.1">마지막으로, 플래그된 라인들 내의 단어들이 전체 문서 단어들의 <math alttext="5\%" class="ltx_Math" display="inline" id="A7.SS2.p2.1.m1.1"><semantics id="A7.SS2.p2.1.m1.1a"><mrow id="A7.SS2.p2.1.m1.1.1" xref="A7.SS2.p2.1.m1.1.1.cmml"><mn id="A7.SS2.p2.1.m1.1.1.2" xref="A7.SS2.p2.1.m1.1.1.2.cmml">5</mn><mo id="A7.SS2.p2.1.m1.1.1.1" xref="A7.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS2.p2.1.m1.1b"><apply id="A7.SS2.p2.1.m1.1.1.cmml" xref="A7.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="A7.SS2.p2.1.m1.1.1.1.cmml" xref="A7.SS2.p2.1.m1.1.1.1">percent</csymbol><cn id="A7.SS2.p2.1.m1.1.1.2.cmml" type="integer" xref="A7.SS2.p2.1.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS2.p2.1.m1.1c">5\%</annotation></semantics></math> 이상을 나타내면, 문서는 폐기된다. 데이터의 수동 검사를 통해 이러한 필터를 도출했으며 언어 전반에 걸쳐 적응이 필요하다는 점에 유의한다.</p>
</div>
</section>
<section id="A7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.3 </span>Deduplication</h3>

<div id="A7.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>에 설명된 두 가지 중복 제거 방법을 사용합니다. <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p1.1.1">ExactSubstr</span> 및 <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p1.1.2">NearDedup</span> (<a class="ltx_ref" href="#A7.SS3.SSS1" title="G.3.1 MinHash Approximate Matching ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.3.1</span></a> 및 <a class="ltx_ref" href="#A7.SS3.SSS2" title="G.3.2 Exact substring deduplication ‣ G.3 Deduplication ‣ Appendix G Details of the Macrodata Refinement pipeline ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">G.3.2</span></a>에서 자세히 설명됨; 중복 샘플의 경우 <a class="ltx_ref" href="#A8" title="Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">H</span></a> 참조).</p>
</div>
<div id="A7.SS3.p2" class="ltx_para">
<p class="ltx_p" id="A7.SS3.p2.1">가장 확장 가능한 접근 방식인 <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p2.1.1">NearDedup</span>으로 시작합니다. MinHash <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a class="ltx_ref" href="#bib.bib17" title="">1997</a>)</cite>를 적용하여 유사 문서를 제거하고, 데이터 세트의 각 문서에 대해 효율적인 근사 유사도 질의를 지원하는 서명/스케치를 계산하고, 높은 <span class="ltx_text ltx_font_italic" id="A7.SS3.p2.1.2">n</span>-gram 중첩을 갖는 문서 쌍을 식별한다.</p>
</div>
<div id="A7.SS3.p3" class="ltx_para">
<p class="ltx_p" id="A7.SS3.p3.1">그런 다음 <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p3.1.1">ExactSubstr</span>을 사용하여 <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">‡</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‡</sup><span class="ltx_tag ltx_tag_note">‡</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google-research/deduplicate-text-datasets" target="_blank" title="">https://github.com/google-research/deduplicate-text-datasets</a></span></span></span>에서 구현을 활용하여 최소 50개 토큰의 정확한 중복 텍스트의 범위를 식별합니다. 이러한 범위에 대해 세 가지 다른 접근 방식을 실험합니다. [span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p3.1.2">ExactSubstr-Cut</span>, 여기서 원본 구현에서 수행 된 대로 원본 텍스트에서 제거 합니다. [span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p3.1.3">ExactSubstr-Mask</span>, 여기서 데이터 세트는 변경 되지 않았지만 복제 된 범위에 대 한 손실을 계산 하지 않습니다. 그리고 [span class="ltx_text ltx_font_smallcaps" id="A7.SS3.p3.1.4">ExactSubstr-Drop</span>, 여기서 복제 된 범위가 콘텐츠의 특정 비율 이상을 구성 하는 경우 전체 문서를 간단히 삭제 합니다.</p>
</div>
<div id="A7.SS3.p4" class="ltx_para">
<p class="ltx_p" id="A7.SS3.p4.1">우리는 <a class="ltx_ref" href="#A5.SS1" title="E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">E.1</span></a>에서 이러한 서로 다른 접근법을 중심으로 소규모 삭제를 제시한다.</p>
</div>
<section id="A7.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.3.1 </span>MinHash Approximate Matching</h4>

<div id="A7.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS1.p1.1">우리는 MinHash를 사용하여 웹 코퍼라에서 매우 큰 규모로 대략적인 중복 문서를 찾는다. 이 기술을 사용하면 산재된 복제 섹션의 대부분이 정확한 매칭 방법(50 토큰보다 작은 모든 것)에 의해 식별되지 않을 만큼 충분히 작은 템플릿 페이지 또는 그렇지 않으면 매우 유사한 콘텐츠를 식별할 수 있다.</p>
</div>
<section id="A7.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Signing.</h5>

<div id="A7.SS3.SSS1.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS1.Px1.p1.2">우리는 리콜을 증가시키기 위해 콘텐츠를 정규화하는 것으로 시작한다: 구두점이 제거되고, 텍스트가 하부케이싱되고, NFD 유니코드 정규화가 적용되고, 액센트가 제거되고, 모든 화이트 스페이스가 정규화된다. GPT-2 tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="#bib.bib62" title="">2019</a>)</cite>를 사용하여 결과 텍스트를 토큰화하고 각 문서에 대해 고유한 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px1.p1.2.1">n</span>-grams 집합을 얻는다. 해시 함수는 각 문서에 대한 서명을 얻는 데 사용됩니다. 각 해시 함수에 대해 가장 작은 값은 문서의 모든 고유한 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px1.p1.2.2">n</span>-gram을 해시로부터 유지합니다. 두 문서가 유사하면 <cite class="ltx_cite ltx_citemacro_citep">(Broder, <a class="ltx_ref" href="#bib.bib17" title="">1997</a>)</cite>로 사용된 해시 함수 중 적어도 일부에 대해 동일한 최소 해시(MinHash)를 가질 확률이 높다. 두 문서 간의 일치하는 해시 비율은 Jaccard Similarity <cite class="ltx_cite ltx_citemacro_citep">(Jaccard, <a class="ltx_ref" href="#bib.bib43" title="">1912</a>)</cite> 고유의 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px1.p1.2.3">n</span>-grams(세트는 <math alttext="d_{i}" class="ltx_Math" display="inline" id="A7.SS3.SSS1.Px1.p1.1.m1.1"><semantics id="A7.SS3.SSS1.Px1.p1.1.m1.1a"><msub id="A7.SS3.SSS1.Px1.p1.1.m1.1.1" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2.cmml">d</mi><mi id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px1.p1.1.m1.1b"><apply id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.2">𝑑</ci><ci id="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="A7.SS3.SSS1.Px1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px1.p1.1.m1.1c">d_{i}</annotation></semantics></math> 및 <math alttext="d_{j}" class="ltx_Math" display="inline" id="A7.SS3.SSS1.Px1.p1.2.m2.1"><semantics id="A7.SS3.SSS1.Px1.p1.2.m2.1a"><msub id="A7.SS3.SSS1.Px1.p1.2.m2.1.1" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.cmml"><mi id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2.cmml">d</mi><mi id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px1.p1.2.m2.1b"><apply id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.2">𝑑</ci><ci id="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="A7.SS3.SSS1.Px1.p1.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px1.p1.2.m2.1c">d_{j}</annotation></semantics></math>):</p>
</div>
<div id="A7.SS3.SSS1.Px1.p2" class="ltx_para">
<table id="A7.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A7.E1.m1.4" class="ltx_Math" alttext="J(d_{i},d_{j})=\frac{\left|d_{i}\cap d_{j}\right|}{\left|d_{i}\cup d_{j}\right|}" display="block"><semantics id="A7.E1.m1.4a"><mrow id="A7.E1.m1.4.4" xref="A7.E1.m1.4.4.cmml"><mrow id="A7.E1.m1.4.4.2" xref="A7.E1.m1.4.4.2.cmml"><mi id="A7.E1.m1.4.4.2.4" xref="A7.E1.m1.4.4.2.4.cmml">J</mi><mo lspace="0em" rspace="0em" id="A7.E1.m1.4.4.2.3" xref="A7.E1.m1.4.4.2.3.cmml">​</mo><mrow id="A7.E1.m1.4.4.2.2.2" xref="A7.E1.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="A7.E1.m1.4.4.2.2.2.3" xref="A7.E1.m1.4.4.2.2.3.cmml">(</mo><msub id="A7.E1.m1.3.3.1.1.1.1" xref="A7.E1.m1.3.3.1.1.1.1.cmml"><mi id="A7.E1.m1.3.3.1.1.1.1.2" xref="A7.E1.m1.3.3.1.1.1.1.2.cmml">d</mi><mi id="A7.E1.m1.3.3.1.1.1.1.3" xref="A7.E1.m1.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="A7.E1.m1.4.4.2.2.2.4" xref="A7.E1.m1.4.4.2.2.3.cmml">,</mo><msub id="A7.E1.m1.4.4.2.2.2.2" xref="A7.E1.m1.4.4.2.2.2.2.cmml"><mi id="A7.E1.m1.4.4.2.2.2.2.2" xref="A7.E1.m1.4.4.2.2.2.2.2.cmml">d</mi><mi id="A7.E1.m1.4.4.2.2.2.2.3" xref="A7.E1.m1.4.4.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="A7.E1.m1.4.4.2.2.2.5" xref="A7.E1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="A7.E1.m1.4.4.3" xref="A7.E1.m1.4.4.3.cmml">=</mo><mfrac id="A7.E1.m1.2.2" xref="A7.E1.m1.2.2.cmml"><mrow id="A7.E1.m1.1.1.1.1" xref="A7.E1.m1.1.1.1.2.cmml"><mo id="A7.E1.m1.1.1.1.1.2" xref="A7.E1.m1.1.1.1.2.1.cmml">|</mo><mrow id="A7.E1.m1.1.1.1.1.1" xref="A7.E1.m1.1.1.1.1.1.cmml"><msub id="A7.E1.m1.1.1.1.1.1.2" xref="A7.E1.m1.1.1.1.1.1.2.cmml"><mi id="A7.E1.m1.1.1.1.1.1.2.2" xref="A7.E1.m1.1.1.1.1.1.2.2.cmml">d</mi><mi id="A7.E1.m1.1.1.1.1.1.2.3" xref="A7.E1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="A7.E1.m1.1.1.1.1.1.1" xref="A7.E1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="A7.E1.m1.1.1.1.1.1.3" xref="A7.E1.m1.1.1.1.1.1.3.cmml"><mi id="A7.E1.m1.1.1.1.1.1.3.2" xref="A7.E1.m1.1.1.1.1.1.3.2.cmml">d</mi><mi id="A7.E1.m1.1.1.1.1.1.3.3" xref="A7.E1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="A7.E1.m1.1.1.1.1.3" xref="A7.E1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="A7.E1.m1.2.2.2.1" xref="A7.E1.m1.2.2.2.2.cmml"><mo id="A7.E1.m1.2.2.2.1.2" xref="A7.E1.m1.2.2.2.2.1.cmml">|</mo><mrow id="A7.E1.m1.2.2.2.1.1" xref="A7.E1.m1.2.2.2.1.1.cmml"><msub id="A7.E1.m1.2.2.2.1.1.2" xref="A7.E1.m1.2.2.2.1.1.2.cmml"><mi id="A7.E1.m1.2.2.2.1.1.2.2" xref="A7.E1.m1.2.2.2.1.1.2.2.cmml">d</mi><mi id="A7.E1.m1.2.2.2.1.1.2.3" xref="A7.E1.m1.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="A7.E1.m1.2.2.2.1.1.1" xref="A7.E1.m1.2.2.2.1.1.1.cmml">∪</mo><msub id="A7.E1.m1.2.2.2.1.1.3" xref="A7.E1.m1.2.2.2.1.1.3.cmml"><mi id="A7.E1.m1.2.2.2.1.1.3.2" xref="A7.E1.m1.2.2.2.1.1.3.2.cmml">d</mi><mi id="A7.E1.m1.2.2.2.1.1.3.3" xref="A7.E1.m1.2.2.2.1.1.3.3.cmml">j</mi></msub></mrow><mo id="A7.E1.m1.2.2.2.1.3" xref="A7.E1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A7.E1.m1.4b"><apply id="A7.E1.m1.4.4.cmml" xref="A7.E1.m1.4.4"><eq id="A7.E1.m1.4.4.3.cmml" xref="A7.E1.m1.4.4.3"></eq><apply id="A7.E1.m1.4.4.2.cmml" xref="A7.E1.m1.4.4.2"><times id="A7.E1.m1.4.4.2.3.cmml" xref="A7.E1.m1.4.4.2.3"></times><ci id="A7.E1.m1.4.4.2.4.cmml" xref="A7.E1.m1.4.4.2.4">𝐽</ci><interval closure="open" id="A7.E1.m1.4.4.2.2.3.cmml" xref="A7.E1.m1.4.4.2.2.2"><apply id="A7.E1.m1.3.3.1.1.1.1.cmml" xref="A7.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="A7.E1.m1.3.3.1.1.1.1.1.cmml" xref="A7.E1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="A7.E1.m1.3.3.1.1.1.1.2.cmml" xref="A7.E1.m1.3.3.1.1.1.1.2">𝑑</ci><ci id="A7.E1.m1.3.3.1.1.1.1.3.cmml" xref="A7.E1.m1.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="A7.E1.m1.4.4.2.2.2.2.cmml" xref="A7.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="A7.E1.m1.4.4.2.2.2.2.1.cmml" xref="A7.E1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="A7.E1.m1.4.4.2.2.2.2.2.cmml" xref="A7.E1.m1.4.4.2.2.2.2.2">𝑑</ci><ci id="A7.E1.m1.4.4.2.2.2.2.3.cmml" xref="A7.E1.m1.4.4.2.2.2.2.3">𝑗</ci></apply></interval></apply><apply id="A7.E1.m1.2.2.cmml" xref="A7.E1.m1.2.2"><divide id="A7.E1.m1.2.2.3.cmml" xref="A7.E1.m1.2.2"></divide><apply id="A7.E1.m1.1.1.1.2.cmml" xref="A7.E1.m1.1.1.1.1"><abs id="A7.E1.m1.1.1.1.2.1.cmml" xref="A7.E1.m1.1.1.1.1.2"></abs><apply id="A7.E1.m1.1.1.1.1.1.cmml" xref="A7.E1.m1.1.1.1.1.1"><intersect id="A7.E1.m1.1.1.1.1.1.1.cmml" xref="A7.E1.m1.1.1.1.1.1.1"></intersect><apply id="A7.E1.m1.1.1.1.1.1.2.cmml" xref="A7.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A7.E1.m1.1.1.1.1.1.2.1.cmml" xref="A7.E1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="A7.E1.m1.1.1.1.1.1.2.2.cmml" xref="A7.E1.m1.1.1.1.1.1.2.2">𝑑</ci><ci id="A7.E1.m1.1.1.1.1.1.2.3.cmml" xref="A7.E1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="A7.E1.m1.1.1.1.1.1.3.cmml" xref="A7.E1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E1.m1.1.1.1.1.1.3.1.cmml" xref="A7.E1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="A7.E1.m1.1.1.1.1.1.3.2.cmml" xref="A7.E1.m1.1.1.1.1.1.3.2">𝑑</ci><ci id="A7.E1.m1.1.1.1.1.1.3.3.cmml" xref="A7.E1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="A7.E1.m1.2.2.2.2.cmml" xref="A7.E1.m1.2.2.2.1"><abs id="A7.E1.m1.2.2.2.2.1.cmml" xref="A7.E1.m1.2.2.2.1.2"></abs><apply id="A7.E1.m1.2.2.2.1.1.cmml" xref="A7.E1.m1.2.2.2.1.1"><union id="A7.E1.m1.2.2.2.1.1.1.cmml" xref="A7.E1.m1.2.2.2.1.1.1"></union><apply id="A7.E1.m1.2.2.2.1.1.2.cmml" xref="A7.E1.m1.2.2.2.1.1.2"><csymbol cd="ambiguous" id="A7.E1.m1.2.2.2.1.1.2.1.cmml" xref="A7.E1.m1.2.2.2.1.1.2">subscript</csymbol><ci id="A7.E1.m1.2.2.2.1.1.2.2.cmml" xref="A7.E1.m1.2.2.2.1.1.2.2">𝑑</ci><ci id="A7.E1.m1.2.2.2.1.1.2.3.cmml" xref="A7.E1.m1.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="A7.E1.m1.2.2.2.1.1.3.cmml" xref="A7.E1.m1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="A7.E1.m1.2.2.2.1.1.3.1.cmml" xref="A7.E1.m1.2.2.2.1.1.3">subscript</csymbol><ci id="A7.E1.m1.2.2.2.1.1.3.2.cmml" xref="A7.E1.m1.2.2.2.1.1.3.2">𝑑</ci><ci id="A7.E1.m1.2.2.2.1.1.3.3.cmml" xref="A7.E1.m1.2.2.2.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.E1.m1.4c">J(d_{i},d_{j})=\frac{\left|d_{i}\cap d_{j}\right|}{\left|d_{i}\cup d_{j}\right|}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A7.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Matching.</h5>

<div id="A7.SS3.SSS1.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS1.Px2.p1.5">모든 가능한 문서 쌍 사이의 MinHash 서명을 비교하는 것은 계산 비용이 많이 들기 때문에, 우리는 MinHash, MinHash LSH의 지역성에 민감한 해싱 버전을 적용한다. 문서 서명은 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.1">r</span> 버킷으로 분할되며, 각각 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.2">b</span> minhashes가 있습니다. 문서는 이러한 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.3">b</span> minhashes가 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.4">r</span> 버킷에 의해 인덱싱되며, 해당 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.5">b</span> minhashes가 버킷 중 하나 이상에서 정확히 동일한 경우 두 문서를 중복으로 표시합니다. 이 두 매개 변수인 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.6">b</span> 및 <span class="ltx_text ltx_font_italic" id="A7.SS3.SSS1.Px2.p1.5.7">r</span>은 유사한 문서가 탐지될 확률을 결정합니다. 그들의 MinHash 서명들 사이의 매칭 해시들의 비율이 <math alttext="i" class="ltx_Math" display="inline" id="A7.SS3.SSS1.Px2.p1.1.m1.1"><semantics id="A7.SS3.SSS1.Px2.p1.1.m1.1a"><mi id="A7.SS3.SSS1.Px2.p1.1.m1.1.1" xref="A7.SS3.SSS1.Px2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.1.m1.1b"><ci id="A7.SS3.SSS1.Px2.p1.1.m1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.1.m1.1c">i</annotation></semantics></math> 및 <math alttext="j" class="ltx_Math" display="inline" id="A7.SS3.SSS1.Px2.p1.2.m2.1"><semantics id="A7.SS3.SSS1.Px2.p1.2.m2.1a"><mi id="A7.SS3.SSS1.Px2.p1.2.m2.1.1" xref="A7.SS3.SSS1.Px2.p1.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.2.m2.1b"><ci id="A7.SS3.SSS1.Px2.p1.2.m2.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.2.m2.1c">j</annotation></semantics></math>인 두 문서의 경우, 주어진 버킷에 매치가 있을 확률은 <math alttext="s_{i,j}^{b}" class="ltx_Math" display="inline" id="A7.SS3.SSS1.Px2.p1.4.m4.2"><semantics id="A7.SS3.SSS1.Px2.p1.4.m4.2a"><msubsup id="A7.SS3.SSS1.Px2.p1.4.m4.2.3" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2.cmml">s</mi><mrow id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.4" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1.cmml">i</mi><mo id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.4.1" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2.cmml">j</mi></mrow><mi id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3.cmml">b</mi></msubsup><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.4.m4.2b"><apply id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.1.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3">superscript</csymbol><apply id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.1.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3">subscript</csymbol><ci id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.2.2">𝑠</ci><list id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.4"><ci id="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.1.1.1.1">𝑖</ci><ci id="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.2.2.2">𝑗</ci></list></apply><ci id="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.4.m4.2.3.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.4.m4.2c">s_{i,j}^{b}</annotation></semantics></math>이고; 임의의 버킷에 매치가 없을 확률은 <math alttext="(1-s_{i,j}^{b})^{r}" class="ltx_Math" display="inline" id="A7.SS3.SSS1.Px2.p1.5.m5.3"><semantics id="A7.SS3.SSS1.Px2.p1.5.m5.3a"><msup id="A7.SS3.SSS1.Px2.p1.5.m5.3.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.cmml"><mrow id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml"><mo id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.2" stretchy="false" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml">(</mo><mrow id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml"><mn id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2.cmml">1</mn><mo id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1.cmml">−</mo><msubsup id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2.cmml">s</mi><mrow id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.4" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.3.cmml"><mi id="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1.cmml">i</mi><mo id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.4.1" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2.cmml">j</mi></mrow><mi id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3.cmml">b</mi></msubsup></mrow><mo id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.3" stretchy="false" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml">)</mo></mrow><mi id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p1.5.m5.3b"><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3">superscript</csymbol><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1"><minus id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.1"></minus><cn id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2.cmml" type="integer" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.2">1</cn><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3">superscript</csymbol><apply id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3"><csymbol cd="ambiguous" id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3">subscript</csymbol><ci id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.2.2">𝑠</ci><list id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.4"><ci id="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.1.1.1.1">𝑖</ci><ci id="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.2.2.2.2">𝑗</ci></list></apply><ci id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.1.1.1.3.3">𝑏</ci></apply></apply><ci id="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3.cmml" xref="A7.SS3.SSS1.Px2.p1.5.m5.3.3.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p1.5.m5.3c">(1-s_{i,j}^{b})^{r}</annotation></semantics></math>이며; 그리고 마지막으로 버킷들 중 적어도 하나에 매치가 있을 확률은:</p>
</div>
<div id="A7.SS3.SSS1.Px2.p2" class="ltx_para">
<table id="A7.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A7.E2.m1.3" class="ltx_Math" alttext="P=1-(1-s_{i,j}^{b})^{r}" display="block"><semantics id="A7.E2.m1.3a"><mrow id="A7.E2.m1.3.3" xref="A7.E2.m1.3.3.cmml"><mi id="A7.E2.m1.3.3.3" xref="A7.E2.m1.3.3.3.cmml">P</mi><mo id="A7.E2.m1.3.3.2" xref="A7.E2.m1.3.3.2.cmml">=</mo><mrow id="A7.E2.m1.3.3.1" xref="A7.E2.m1.3.3.1.cmml"><mn id="A7.E2.m1.3.3.1.3" xref="A7.E2.m1.3.3.1.3.cmml">1</mn><mo id="A7.E2.m1.3.3.1.2" xref="A7.E2.m1.3.3.1.2.cmml">−</mo><msup id="A7.E2.m1.3.3.1.1" xref="A7.E2.m1.3.3.1.1.cmml"><mrow id="A7.E2.m1.3.3.1.1.1.1" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="A7.E2.m1.3.3.1.1.1.1.2" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="A7.E2.m1.3.3.1.1.1.1.1" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml"><mn id="A7.E2.m1.3.3.1.1.1.1.1.2" xref="A7.E2.m1.3.3.1.1.1.1.1.2.cmml">1</mn><mo id="A7.E2.m1.3.3.1.1.1.1.1.1" xref="A7.E2.m1.3.3.1.1.1.1.1.1.cmml">−</mo><msubsup id="A7.E2.m1.3.3.1.1.1.1.1.3" xref="A7.E2.m1.3.3.1.1.1.1.1.3.cmml"><mi id="A7.E2.m1.3.3.1.1.1.1.1.3.2.2" xref="A7.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml">s</mi><mrow id="A7.E2.m1.2.2.2.4" xref="A7.E2.m1.2.2.2.3.cmml"><mi id="A7.E2.m1.1.1.1.1" xref="A7.E2.m1.1.1.1.1.cmml">i</mi><mo id="A7.E2.m1.2.2.2.4.1" xref="A7.E2.m1.2.2.2.3.cmml">,</mo><mi id="A7.E2.m1.2.2.2.2" xref="A7.E2.m1.2.2.2.2.cmml">j</mi></mrow><mi id="A7.E2.m1.3.3.1.1.1.1.1.3.3" xref="A7.E2.m1.3.3.1.1.1.1.1.3.3.cmml">b</mi></msubsup></mrow><mo stretchy="false" id="A7.E2.m1.3.3.1.1.1.1.3" xref="A7.E2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mi id="A7.E2.m1.3.3.1.1.3" xref="A7.E2.m1.3.3.1.1.3.cmml">r</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.E2.m1.3b"><apply id="A7.E2.m1.3.3.cmml" xref="A7.E2.m1.3.3"><eq id="A7.E2.m1.3.3.2.cmml" xref="A7.E2.m1.3.3.2"></eq><ci id="A7.E2.m1.3.3.3.cmml" xref="A7.E2.m1.3.3.3">𝑃</ci><apply id="A7.E2.m1.3.3.1.cmml" xref="A7.E2.m1.3.3.1"><minus id="A7.E2.m1.3.3.1.2.cmml" xref="A7.E2.m1.3.3.1.2"></minus><cn type="integer" id="A7.E2.m1.3.3.1.3.cmml" xref="A7.E2.m1.3.3.1.3">1</cn><apply id="A7.E2.m1.3.3.1.1.cmml" xref="A7.E2.m1.3.3.1.1"><csymbol cd="ambiguous" id="A7.E2.m1.3.3.1.1.2.cmml" xref="A7.E2.m1.3.3.1.1">superscript</csymbol><apply id="A7.E2.m1.3.3.1.1.1.1.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1"><minus id="A7.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.1"></minus><cn type="integer" id="A7.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.2">1</cn><apply id="A7.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3">superscript</csymbol><apply id="A7.E2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A7.E2.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="A7.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3.2.2">𝑠</ci><list id="A7.E2.m1.2.2.2.3.cmml" xref="A7.E2.m1.2.2.2.4"><ci id="A7.E2.m1.1.1.1.1.cmml" xref="A7.E2.m1.1.1.1.1">𝑖</ci><ci id="A7.E2.m1.2.2.2.2.cmml" xref="A7.E2.m1.2.2.2.2">𝑗</ci></list></apply><ci id="A7.E2.m1.3.3.1.1.1.1.1.3.3.cmml" xref="A7.E2.m1.3.3.1.1.1.1.1.3.3">𝑏</ci></apply></apply><ci id="A7.E2.m1.3.3.1.1.3.cmml" xref="A7.E2.m1.3.3.1.1.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.E2.m1.3c">P=1-(1-s_{i,j}^{b})^{r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="A7.SS3.SSS1.Px2.p3" class="ltx_para">
<p id="A7.SS3.SSS1.Px2.p3.5" class="ltx_p">We use the same parameters as <cite class="ltx_cite ltx_citemacro_citet">Lee et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>: <math id="A7.SS3.SSS1.Px2.p3.1.m1.1" class="ltx_Math" alttext="n=5" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.1.m1.1a"><mrow id="A7.SS3.SSS1.Px2.p3.1.m1.1.1" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.cmml"><mi id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2.cmml">n</mi><mo id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1.cmml">=</mo><mn id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.1.m1.1b"><apply id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1"><eq id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.1"></eq><ci id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.2">𝑛</ci><cn type="integer" id="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.1.m1.1c">n=5</annotation></semantics></math> (<span id="A7.SS3.SSS1.Px2.p3.5.1" class="ltx_text ltx_font_italic">5</span>-grams); <math id="A7.SS3.SSS1.Px2.p3.2.m2.1" class="ltx_Math" alttext="b=20" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.2.m2.1a"><mrow id="A7.SS3.SSS1.Px2.p3.2.m2.1.1" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.cmml"><mi id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2.cmml">b</mi><mo id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1.cmml">=</mo><mn id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.2.m2.1b"><apply id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1"><eq id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.1"></eq><ci id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.2">𝑏</ci><cn type="integer" id="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p3.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.2.m2.1c">b=20</annotation></semantics></math> and <math id="A7.SS3.SSS1.Px2.p3.3.m3.1" class="ltx_Math" alttext="r=450" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.3.m3.1a"><mrow id="A7.SS3.SSS1.Px2.p3.3.m3.1.1" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.cmml"><mi id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2.cmml">r</mi><mo id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1.cmml">=</mo><mn id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3.cmml">450</mn></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.3.m3.1b"><apply id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1"><eq id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.1"></eq><ci id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.2">𝑟</ci><cn type="integer" id="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3.cmml" xref="A7.SS3.SSS1.Px2.p3.3.m3.1.1.3">450</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.3.m3.1c">r=450</annotation></semantics></math>. This means that for each document, we compute a total of 9000 minhashes, and that the probability that a document pair with similarity 0.75 or 0.8 will be marked as duplicates will be <math id="A7.SS3.SSS1.Px2.p3.4.m4.1" class="ltx_Math" alttext="76\%" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.4.m4.1a"><mrow id="A7.SS3.SSS1.Px2.p3.4.m4.1.1" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.cmml"><mn id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2.cmml">76</mn><mo id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.4.m4.1b"><apply id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1"><csymbol cd="latexml" id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.4.m4.1.1.2">76</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.4.m4.1c">76\%</annotation></semantics></math> and <math id="A7.SS3.SSS1.Px2.p3.5.m5.1" class="ltx_Math" alttext="99.4\%" display="inline"><semantics id="A7.SS3.SSS1.Px2.p3.5.m5.1a"><mrow id="A7.SS3.SSS1.Px2.p3.5.m5.1.1" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.cmml"><mn id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2.cmml">99.4</mn><mo id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A7.SS3.SSS1.Px2.p3.5.m5.1b"><apply id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1"><csymbol cd="latexml" id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1.cmml" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2.cmml" xref="A7.SS3.SSS1.Px2.p3.5.m5.1.1.2">99.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.SSS1.Px2.p3.5.m5.1c">99.4\%</annotation></semantics></math> (respectively), diminishing rapidly for smaller similarity values.</p>
</div>
<div id="A7.SS3.SSS1.Px2.p4" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS1.Px2.p4.1">마지막으로 모든 버킷에 걸쳐 문서를 클러스터링합니다. - 문서 A와 B가 하나의 버킷에서 일치하고 B와 C가 다른 버킷에서 일치하면 A-B-C가 클러스터가 됩니다. 우리는 각 클러스터의 문서 중 하나를 제외한 모든 문서를 무작위로 제거한다.</p>
</div>
<div id="A7.SS3.SSS1.Px2.p5" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS1.Px2.p5.1"><cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>는 또한 실제 자카드 유사성 또는 식별된 문서 쌍 간의 편집 유사성과 같은 다른 메트릭을 계산하여 거짓 긍정에 대한 필터링을 제안했다. 모든 커먼크롤에서 사용할 수 있는 많은 양의 데이터와 주요 관심사가 리콜 개선이라는 점을 감안할 때 이 추가 단계는 생략하기로 결정했다.</p>
</div>
</section>
</section>
<section id="A7.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.3.2 </span>Exact substring deduplication</h4>

<div id="A7.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.p1.1">정확한 텍스트 매칭을 위해 <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.p1.1.1">ExactSubstr</span> implementation publicly released by <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>를 사용한다. 우리는 MinHash에 의해 이미 중복 제거된 데이터에 정확한 부분 문자열 중복 제거를 적용하여 우리가 작동해야 하는 데이터 세트의 거의 40% 크기를 줄인다. <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.p1.1.2">ExactSubstr</span> will find long strings of text that are exists, character for character, across multiple documents. 이들 중 일부는 근사 중복 제거의 초기 단계를 벗어났을 수 있다: 문서의 충분한 부분을 구성하지 못할 수 있다; 하나의 문서가 여러 다른 문서에 걸쳐 소스된 섹션을 반복했을 수 있다; 또는 단순히 MinHash의 근사적 특성으로 인해 발견되지 않았을 수 있다.</p>
</div>
<section id="A7.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Finding duplicates.</h5>

<div id="A7.SS3.SSS2.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.Px1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.Px1.p1.1.1">ExactSubstr</span>은 데이터 세트의 모든 문서를 연결하여 하나의 긴 텍스트 시퀀스를 만든 다음, 선형 시간에서 접미사 배열 <cite class="ltx_cite ltx_citemacro_citep">(Manber &amp; Myers, <a class="ltx_ref" href="#bib.bib53" title="">1993</a>)</cite>를 빌드합니다. 마지막으로, 접미사의 순서화된 목록을 단순히 횡단하고 두 개의 연속 접미사의 각 쌍의 시작을 비교함으로써 접미사 배열을 사용하여 선형 시간에서도 중복 서열을 찾을 수 있다.</p>
</div>
<div id="A7.SS3.SSS2.Px1.p2" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.Px1.p2.1">우리는 MinHash와 동일한 정규화 및 토큰화를 문서 연결 전에 문서 내용에 적용한다. 한 가지 중요한 차이점은 가역성이 중요하다는 것이다: MinHash의 경우 전체 문서를 폐기했기 때문에 다운스트림 사용을 위해 정규화된+토큰화된 표현에 의존하지 않았다. 여기서 중복된 정규화된+토큰화된 스팬을 식별하면 원래 스팬으로 되돌아가 제거해야 한다. 따라서 토큰화 프로세스에 정규화를 포함하고 프로세스가 가역적인지 검증한다.</p>
</div>
<div id="A7.SS3.SSS2.Px1.p3" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.Px1.p3.1">일치가 50 토큰보다 긴 경우 중복 범위가 여러 개 있습니다. 연결된 데이터 세트 시퀀스에서 중복되는 이러한 범위는 파일에 저장하기 전에 병합됩니다. 그런 다음 이러한 범위를 취하여 원본 문서를 검색하여 복제된 토큰 범위에 해당하는 문자 부분 문자열을 가져옵니다.</p>
</div>
</section>
<section id="A7.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Removing duplicates.</h5>

<div id="A7.SS3.SSS2.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.Px2.p1.1">우리는 중복 경간에 다음 변환을 적용하는 것을 고려했다.</p>
</div>
<div id="A7.SS3.SSS2.Px2.p2" class="ltx_para">
<ul id="A7.I3" class="ltx_itemize">
<li id="A7.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A7.I3.i1.p1.1.1">ExactSubstr-Cut</span>: we remove the duplicated span, and discard documents where there are less than 20 non-duplicated characters left -this is the vanilla setting used by <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>;</p>
</div>
</li>
<li id="A7.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A7.I3.i2.p1.1.1">ExactSubstr-Mask</span>: 복제된 스팬을 손실 마스킹하여 사전 훈련 동안 복제된 텍스트에서 손실이 계산되는 것을 방지하고 마스킹되지 않은 문자가 20개 미만인 문서를 폐기합니다.</p>
</div>
</li>
<li id="A7.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A7.I3.i3.p1.1.1">ExactSubstr-DropPartial</span>: if more than the document is duplicated, we remove the entire document;</p>
</div>
</li>
<li id="A7.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i4.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A7.I3.i4.p1.1.1">ExactSubstr-DropAny</span>: we drop any document with a duplicated span in it.</p>
</div>
</li>
</ul>
</div>
<div id="A7.SS3.SSS2.Px2.p3" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.Px2.p3.1">넓게 말하면, <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.Px2.p3.1.1">ExactSubstr-Cut</span>은 텍스트 중간 문장 제거로 연결이 끊어진 텍스트를 제거할 수 있습니다. <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.Px2.p3.1.2">ExactSubstr-Mask</span>은 모델의 가중치 업데이트에 직접 기여하지 않을 것이므로 효율성이 떨어질 수 있습니다. <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.Px2.p3.1.3">ExactSubstr-Drop</span>은 <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.Px2.p3.1.3">ExactSubstr-Drop</span>은 해당 <span class="ltx <a class="ltx_ref" href="#A5.SS1" title="E.1 Small-scale ablations on deduplication approaches ‣ Appendix E Additional results ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">E.1</span></a>의 삭제에 이어 바닐라 접근 방식인 <span class="ltx_text ltx_font_smallcaps" id="A7.SS3.SSS2.Px2.p3.1.6">ExactSubstr-Cut</span>을 고수하도록 선택한다.</p>
</div>
<div id="A7.SS3.SSS2.Px2.p4" class="ltx_para">
<p class="ltx_p" id="A7.SS3.SSS2.Px2.p4.1">모든 경우에 MinHash는 복제된 문서의 복사본 하나를 보관하지만, 정확한 중복 제거는 복제된 범위의 복사본을 모두 제거합니다.</p>
</div>
</section>
</section>
</section>
<section id="A7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.4 </span>Execution environment</h3>

<div id="A7.SS4.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS4.p1.1">대부분의 데이터 처리는 100-250 AWS c5.18xlarge 인스턴스로 대규모 CPU 클러스터에서 수행되었으며 각 인스턴스에는 72개의 vCPU와 144개의 GiB의 메모리가 있다. 일반적으로 클러스터에서 10,000-20,000 vCPU로 실행하므로 빠른 병렬 처리가 가능합니다.</p>
</div>
<div id="A7.SS4.p2" class="ltx_para">
<p class="ltx_p" id="A7.SS4.p2.1"><span class="ltx_text ltx_font_smallcaps" id="A7.SS4.p2.1.1">ExactSubstr</span>의 경우 중복 제거된 전체 데이터 세트를 메모리에 로드해야 합니다. 단일 인스턴스에서 최대 2TiB의 메모리와 함께 제공되는 AWS x2iedn 인스턴스를 활용했습니다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Deduplication samples from RefinedWeb</h2>

<section id="A8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.1 </span>MinHash clusters</h3>

<div id="A8.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A8.SS1.p1.1">우리는 MinHash가 <a class="ltx_ref" href="#A8.T15" title="In H.1 MinHash clusters ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">15</span></a> – 각각 수십만 개의 문서에 걸쳐 있는 8개의 가장 큰 중복 클러스터를 보고한다. 또한 URL GET 매개 변수가 다르기 때문에 중복 문서 쌍이 많이 발견되어 내용이 크게 다르지 않다. 이러한 동작의 예는 <a class="ltx_ref" href="#A8.T16" title="In H.1 MinHash clusters ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">16</span></a>에 제시된 URL에서 볼 수 있다.</p>
</div>
<figure id="A8.T15" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 15:</span><span class="ltx_text ltx_font_bold" id="A8.T15.2.1">Top-8 largest MinHash clusters found when building RefinedWeb. </span> 간단한 설명만 유지하면서 가독성을 위해 가장 긴 샘플 중 일부를 자릅니다.</figcaption>
<table id="A8.T15.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T15.3.1" class="ltx_tr">
<td id="A8.T15.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A8.T15.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.1.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Description</span></span>
</span>
</td>
<td id="A8.T15.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A8.T15.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.1.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Example document</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.2" class="ltx_tr">
<td id="A8.T15.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.2.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Wordpress sitemap notice generated by the Google Sitemap Generator Plugin</span></span>
</span>
</td>
<td id="A8.T15.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T15.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.2.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">This is a XML Sitemap which is supposed to be processed by search engines which follow the XML Sitemap standard like Ask.com, Bing, Google and Yahoo. It was generated using the WordPress content management system and the Google Sitemap Generator Plugin by Arne Brachhold. You can find more information about XML sitemaps on sitemaps.org and Google’s list of sitemap programs. This file contains links to sub-sitemaps, follow them to see the actual sitemap content.</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.3" class="ltx_tr">
<td id="A8.T15.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.3.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Cloudflare notice to enable Javascript</span></span>
</span>
</td>
<td id="A8.T15.3.3.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.4" class="ltx_tr">
<td id="A8.T15.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.4.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Templated disability notice, with different phone numbers across pages</span></span>
</span>
</td>
<td id="A8.T15.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T15.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.4.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Welcome to our website! As we have the ability to list over one million items on our website (our selection changes all of the time), it is not feasible for a company our size to record and playback the descriptions on every item on our website. However, if you are an American with a disability we are here to help you. Please call our disability services phone line at [redacted] or [redacted] during regular business hours and one of our kind and friendly personal shoppers will help you navigate through our website, help conduct advanced searches, help you choose the item you are looking for with the specifications you are seeking, read you the specifications of any item and consult with you about the products themselves. There is no charge for the help of this personal shopper for any American with a disability. Finally, your personal shopper will explain our Privacy Policy and Terms of Service, and help you place an order if you so desire.</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.5" class="ltx_tr">
<td id="A8.T15.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.5.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Templated cookies notice</span></span>
</span>
</td>
<td id="A8.T15.3.5.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.6" class="ltx_tr">
<td id="A8.T15.3.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.6.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Templated domain name for sale page</span></span>
</span>
</td>
<td id="A8.T15.3.6.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.7" class="ltx_tr">
<td id="A8.T15.3.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.7.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.7.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">www.metoperashop.org</span><span id="A8.T15.3.7.1.1.1.2" class="ltx_text" style="font-size:90%;"> and sub-URLs, with content changes but always the same (large) footer</span></span>
</span>
</td>
<td id="A8.T15.3.7.2" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="A8.T15.3.8" class="ltx_tr">
<td id="A8.T15.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T15.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.8.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Different pages across more than 80 different domain names but with a common section</span></span>
</span>
</td>
<td id="A8.T15.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T15.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.8.2.1.1" class="ltx_p" style="width:245.7pt;"><span id="A8.T15.3.8.2.1.1.1" class="ltx_text" style="font-size:90%;">DC Customers also liked:
Special event items are produced by manufacturers only after the outcome of a game or event. These are advanced sale items and will ship immediately after they are received in our warehouse.
Manufacturer direct items are shipped directly from the manufacturer. These items are not available for international or expedited shipping.
Customized items can be personalized with options such as your name, your favorite number, and/or designs. Some options may be limited by league rules.</span></span>
</span>
</td>
</tr>
<tr id="A8.T15.3.9" class="ltx_tr">
<td id="A8.T15.3.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A8.T15.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T15.3.9.1.1.1" class="ltx_p" style="width:216.8pt;"><span id="A8.T15.3.9.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">http://www.boxofficemojo.com/daily</span><span id="A8.T15.3.9.1.1.1.2" class="ltx_text" style="font-size:90%;"> and sub-URLs</span></span>
</span>
</td>
<td id="A8.T15.3.9.2" class="ltx_td ltx_align_top ltx_border_bb ltx_border_t"></td>
</tr>
</tbody></table>
</figure>
<figure id="A8.T16" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 16:</span><span class="ltx_text ltx_font_bold" id="A8.T16.2.1">URL with different GET parameters don’t always results significantly different page content. </span></figcaption>
<table id="A8.T16.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T16.3.1" class="ltx_tr">
<td id="A8.T16.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A8.T16.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.1.1.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.1.1.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">http://gamesandbiz.blogspot.com/2010/
07/bad-reviews-can-hurt-game-sales.ht
ml?showComment=1278486430242
</pre>
</span>
</td>
<td id="A8.T16.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A8.T16.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.1.2.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.1.2.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">http://gamesandbiz.blogspot.com/2010/
07/bad-reviews-can-hurt-game-sales.ht
ml?showComment=1278499674195</pre>
</span>
</td>
</tr>
<tr id="A8.T16.3.2" class="ltx_tr">
<td id="A8.T16.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A8.T16.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.2.1.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.2.1.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">https://www.ocean-oxygen.org/home;jse
ssionid=1E3290E84F668552FAC643D0A8F81
BEC?p_p_id=122_INSTANCE_Zy6zjkRLAg7v&amp;
p_p_lifecycle=0&amp;p_p_state=normal&amp;p_p_
mode=view&amp;p_p_col_id=column-2&amp;p_p_col
_pos=1&amp;p_p_col_count=6&amp;p_r_p_56423352
4_resetCur=true&amp;p_r_p_564233524_categ
oryId=1346016</pre>
</span>
</td>
<td id="A8.T16.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A8.T16.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T16.3.2.2.1.1" class="ltx_p" style="width:202.4pt;"></span><pre id="A8.T16.3.2.2.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">https://www.ocean-oxygen.org/home?p_p
_id=122_INSTANCE_Zy6zjkRLAg7v&amp;p_p_lif
ecycle=0&amp;p_p_state=normal&amp;p_p_mode=vi
ew&amp;p_p_col_id=column-2&amp;p_p_col_pos=1&amp;
p_p_col_count=6&amp;p_r_p_564233524_reset
Cur=true&amp;p_r_p_564233524_categoryId=1
346016</pre>
</span>
</td>
</tr>
</tbody></table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.2 </span>Exact substring matches</h3>

<div id="A8.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A8.SS2.p1.1">정확한 부분 문자열 중복 제거에 의해 발견되는 정확한 일치의 예는 표 <a class="ltx_ref" href="#A8.T17" title="Table 17 ‣ H.2 Exact substring matches ‣ Appendix H Deduplication samples from RefinedWeb ‣ The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"><span class="ltx_text ltx_ref_tag">17</span></a>에서 볼 수 있다.</p>
</div>
<figure id="A8.T17" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 17:</span><span class="ltx_text ltx_font_bold" id="A8.T17.3.1">Matches found by exact substring deduplication</span> (in <em class="ltx_emph ltx_font_italic" id="A8.T17.4.2">italics</em>).</figcaption>
<table id="A8.T17.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="A8.T17.5.1" class="ltx_tr">
<td id="A8.T17.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A8.T17.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.1.1.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">it appears there is a transfer of ranking signals in this relationship. Supporting this finding is a quote from Google’s guidelines: </span><em id="A8.T17.5.1.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Using JavaScript to redirect users can be a legitimate practice. For example, if you redirect users to an internal page once they’re logged in, you can use JavaScript to do so. When examining JavaScript or other redirect methods to ensure your site adheres to our guidelines, consider the intent. Keep in mind that 301 redirects are best when moving your site, but you could use a JavaScript redirect for this purpose if you don’t have access to your website’s server.</em><span id="A8.T17.5.1.1.1.1.3" class="ltx_text" style="font-size:90%;"> NOTE: Their experiment is based on a live page with status code 200 and NOT an inactive page. So if you want to implement this for legacy</span></span>
</span>
</td>
<td id="A8.T17.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A8.T17.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.1.2.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Some examples of sneaky redirects include:
- Search engines shown one type of content while users are redirected to something significantly different.
- Desktop users receive a normal page, while mobile users are redirected to a completely different spam domain. </span><em id="A8.T17.5.1.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Using JavaScript to redirect users can be a legitimate practice. For example, if you redirect users to an internal page once they’re logged in, you can use JavaScript to do so. When examining JavaScript or other redirect methods to ensure your site adheres to our guidelines, consider the intent. Keep in mind that 301 redirects are best when moving your site, but you could use a JavaScript redirect for this purpose if you don’t have access to your website’s server.</em></span>
</span>
</td>
</tr>
<tr id="A8.T17.5.2" class="ltx_tr">
<td id="A8.T17.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T17.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.2.1.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Find Palm Beache FL homes for sale and other Palm Beach real estate on homesofthepalmbeaches.com. Browse and search Palm Beach houses, condos, townhomes and single-family homes by community , building, or location. </span><em id="A8.T17.5.2.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Our extensive database of real estate listings provide the most comprehensive property details including home values, features and local school and neighborhood info so you can be sure that you have nearly all the facts you need upfront. Search</em><span id="A8.T17.5.2.1.1.1.3" class="ltx_text" style="font-size:90%;"> homesofthepalmbeaches.com today! Want a closer look at what other Palm Beach properties are available?</span></span>
</span>
</td>
<td id="A8.T17.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T17.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.2.2.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Search Stuart houses, condos, townhomes and single-family homes by price and location. </span><em id="A8.T17.5.2.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Our extensive database of real estate listings provide the most comprehensive property details including home values, features and local school and neighborhood info so you can be sure that you have nearly all the facts you need upfront. Search</em><span id="A8.T17.5.2.2.1.1.3" class="ltx_text" style="font-size:90%;"> Stuart Listings today! Want a closer look at what other Stuart properties are available? Also search our listings for the Newest Stuart Listings and Stuart Homes with Price Reductions now.
Stuart FL Homes for Sale - Stuart Real Estate Listings FREE to search
Stuart Property</span></span>
</span>
</td>
</tr>
<tr id="A8.T17.5.3" class="ltx_tr">
<td id="A8.T17.5.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A8.T17.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.3.1.1.1" class="ltx_p" style="width:209.6pt;"><em id="A8.T17.5.3.1.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">To find the correct size you should measure your foot from the heel to the toe point.
Add approximately 1 - 1,5cm to get the actual inner sole length. Measure both feet and fit shoes to the larger foot.
Measure feet at the end of the day, when your feet are at their largest.</em><span id="A8.T17.5.3.1.1.1.2" class="ltx_text" style="font-size:90%;"> Lente shoes are women’s easy slip-on leisure shoes for everyday use.
These lightweight shoes have a breathable textile mesh upper made of recycled PET bottles and cool Lycra lining.</span></span>
</span>
</td>
<td id="A8.T17.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A8.T17.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.3.2.1.1" class="ltx_p" style="width:209.6pt;"><em id="A8.T17.5.3.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">To find the correct size you should measure your foot from the heel to the toe point.
Add approximately 1 - 1,5cm to get the actual inner sole length. Measure both feet and fit shoes to the larger foot.
Measure feet at the end of the day, when your feet are at their largest.</em><span id="A8.T17.5.3.2.1.1.2" class="ltx_text" style="font-size:90%;"> Enjoy your summer days with Masera leisure sneakers. These low-cut women’s sneakers are extremely lightweight thanks to phylon midsole and breathable textile mesh upper</span></span>
</span>
</td>
</tr>
<tr id="A8.T17.5.4" class="ltx_tr">
<td id="A8.T17.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A8.T17.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.4.1.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">This bandana makes the perfect addition to every fur babies birthday collection! With its sparkly crown pattern, your pup will be ready for every birthday celebration! </span><em id="A8.T17.5.4.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">With snaps for security, this bandana is made with love, down to the very last stitch !
Fabric: cotton
Care Instructions: Hand wash only, iron as needed, on low heat
Always supervise your pup while wearing Faithful Paws Co. accessories, as it could become a choking hazard if consumed.</em></span>
</span>
</td>
<td id="A8.T17.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A8.T17.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A8.T17.5.4.2.1.1" class="ltx_p" style="width:209.6pt;"><span id="A8.T17.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">This bandana makes the perfect addition to every fur babies summer collection! With its vibrant watercolor popsicle pattern, your pup will be ready for every summer cookout! </span><em id="A8.T17.5.4.2.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">With snaps for security, this bandana is made with love, down to the very last stitch !
Fabric: cotton
Care Instructions: Hand wash only, iron as needed, on low heat
Always supervise your pup while wearing Faithful Paws Co. accessories, as it could become a choking hazard if consumed.</em></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2306.01114" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2306.01116" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2306.01116">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.01116" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2306.01117" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 01:55:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
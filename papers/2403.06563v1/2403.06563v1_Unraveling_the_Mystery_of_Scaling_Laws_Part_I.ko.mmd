# 스케일링 법칙의 수수께끼 풀기: Part I

 희수

**Zhi Tian**

**Xiaoyu Shen**

**Xunliang Cai**

**Meituan Inc.**

suhui07@meituan.com

동등한 기여. 지톈의 작품은 메이투안에서 이루어졌다.

###### Abstract

스케일링 법칙 원리는 손실과 모델 크기, 데이터 세트 크기 및 훈련 중에 활용되는 계산 자원과 같은 변수 사이의 멱법칙 상관 관계를 나타낸다. 이러한 원칙은 모델 사전 훈련의 다양한 측면을 최적화하는 데 중요한 역할을 하며, 궁극적으로 GPT-4, 라마 및 제미니와 같은 대규모 언어 모델의 성공에 기여한다. 그러나 OpenAI의 원래 스케일링 법칙 논문은 정확한 스케일링 법칙 공식을 도출하는 데 필요한 완전한 세부 사항을 공개하지 않았으며, 그 결론은 최대 15억 개의 매개변수를 포함하는 모델을 기반으로 한다. 일부 후속 연구에서는 이러한 세부 사항과 규모를 더 큰 모델로 공개하려고 시도하지만 학습 속도, 컨텍스트 길이 및 배치 크기와 같은 중요한 요인의 훈련 의존성을 무시하여 테스트 손실 궤적을 예측하기 위한 신뢰할 수 있는 공식을 설정하지 못하는 경우가 많다. 이 기술 보고서에서는 모델 크기를 최대 330억까지 스케일링할 때 원래 OpenAI 논문에서 제안된 _스케일링 법칙 공식이 유효함을 확인하지만, 실험 설정 2에 따라 이러한 공식의 상수 계수는 크게 다르다. 1M\(\sim\)60M 매개변수만 있는 모델에 대한 훈련을 통해 영향력 있는 요인을 세밀하게 식별하고 스케일링 법칙 공식의 모든 상수 항을 추정하기 위한 투명하고 단계별 지침을 제공한다. 이러한 추정식을 사용하여, (1) 가능한 최소 테스트 손실, (2) 특정 손실을 달성하기 위해 필요한 최소 훈련 단계 및 처리된 토큰, (3) 임의의 손실 값에서 최적의 시간/계산 트레이드오프를 갖는 임계 배치 크기, (4) 임의의 배치 크기를 갖는 완전한 테스트 손실 궤적을 포함하여, 훈련 전에 최대 33B 파라미터를 갖는 모델에 대한 다양한 속성을 정확하게 예측할 수 있는 능력을 보여준다. 우리는 또한 스케일링 법칙이 원칙적인 방식으로 고정된 계산 제약 하에서 가장 적합한 배치/모델 크기, 데이터세트 혼합 비율 및 훈련 기간을 결정하는 데 어떻게 도움이 될 수 있는지를 설명한다. 우리의 연구는 대규모 언어 모델의 개발을 촉진하기 위해 스케일링 법칙에 대한 이론적 이해에서 실제 도출 및 적용으로의 상당한 전환을 나타낸다.

각주 2: 매개 변수가 더 큰 모델에 대한 실험이 진행 중이며, 결과가 완료되면 제공할 것이다.

## 1 Introduction

광범위한 연구에 따르면 언어 모델의 성능은 멱함수 관계[17, 20, 21, 22, 23, 24]에 따라 매개변수 수와 데이터 크기가 증가함에 따라 주목할 만한 성장 패턴을 나타낸다. 이 스케일링 법칙은 대규모 언어 모델의 개발에 근본적인 역할을 하며, 훨씬 더 작은 모델의 훈련 로그로부터 대규모 모델의 최적 구성을 추정할 수 있게 한다[22, 23]. GPT-4 기술 보고서 [1]에서 언급한 바와 같이, GPT-4의 계산의 1/1,000분의 1 이하로 훈련된 모델을 기반으로 GPT-4의 성능의 일부 측면을 정확하게 예측할 수 있다. 스케일링 법칙을 적절하게 활용함으로써 대규모 모델에 대한 광범위한 모델별 튜닝을 수행할 필요가 없다.

OpenAI의 원래 스케일링 법칙 논문은 스케일링 법칙의 공식을 제시하고 최적의 훈련 구성을 결정하는 데 어떻게 도움이 될 수 있는지 설명했다[20]. 그럼에도 불구하고 제시된 공식은 특정 실험 설정에서 추정된 정적 지수를 기반으로 한다. 새로운 실험 설정(모델 아키텍처, 토큰화, 데이터 배포 등)에 대한 스케일링-법칙 공식에서 상수 항을 유도하는 방법에 대한 전체 세부 사항은 아직 공개되지 않았다. 또한 [20]은 현대 대형 언어 모델보다 훨씬 작은 크기인 최대 1.5B 매개변수를 포함하는 모델로만 실험을 수행했다. 더 큰 모델에 대한 스케일링 법칙을 연구하는 후속 작업이 있었다[3, 23, 24]. 일부는 원래 스케일링 법칙 논문과는 다른 결론을 도출하여 스케일링 법칙의 일반적인 적용 가능성에 의문을 제기하였다. 예를 들어, [23]은 트레이닝 데이터 크기가 [20]에서의 권고보다 훨씬 더 크게 스케일링되어야 한다고 주장했다. [3] 최적 배치 크기는 손실 값이 아닌 계산 예산에만 의존한다고 제안했습니다.

본 논문에서는 [20]에 의해 제안된 스케일링-법칙 공식들을 재검토하여, 이들이 _33B_까지 모델 크기를 스케일링할 때 일반적으로 적용 가능한 상태를 유지함을 확인한다. 다른 연구는 주로 (1) 데이터 분포, 컨텍스트 길이, 토큰화와 같은 많은 요인이 스케일링-법칙 공식에서 상수 계수에 영향을 미치므로 상수 계수는 공식 자체와는 달리 보편적이지 않으며, (2) 손실 값은 무한 배치 크기 하에서 훈련 단계와 분석적 멱법칙 관계를 따른다. 유한 배치 크기에서는 손실 값을 해석 함수로 맞추는 것이 문제가 된다. 그 결과, 다른 어떤 작업도 더 작은 모델에 대해서만 훈련하여 더 큰 모델의 전체 손실 궤적을 안정적으로 예측할 수 있는 강력한 증거를 제공하지 못했다.

손실 궤적을 예측하는 데 영향을 미치는 요인을 세심하게 식별한 후, 1M\(\sim\)60M 매개변수만 있는 모델에 대한 훈련을 통해 스케일링 법칙 공식에서 모든 상수 항을 추정하는 방법에 대한 투명하고 단계별 지침을 제공한다. 작은 모델의 이러한 추정 공식을 사용하여 훈련이 시작되기 전에 최대 33B 매개변수를 가진 모델에 대한 다양한 속성을 정확하게 예측할 수 있는 능력을 보여준다. 스케일링 법칙의 미스터리를 풀고 모든 사람이 쉽게 접근할 수 있도록 함으로써, 우리의 목표는 스케일링 법칙에 대한 이해를 이론적인 개념에서 실질적인 구현으로 전환함으로써 더 원론적인 방식으로 대규모 언어 모델을 사전 훈련하는 향후 연구에 도움이 되는 것이다. 본 논문의 주요 결과를 요약하면 다음과 같다.

그림 1: 왼쪽: C4 테스트 데이터에 대한 2B 모델의 실제 및 예측 손실 궤적(섹션 4.1). 오른쪽: 코드 테스트 데이터에 대한 33B 모델의 실제 및 예측 손실 궤적(섹션 4.2). 실제 손실 궤적과 예측된 손실 궤적은 특히 초기 예열 단계 이후에 밀접하게 정렬된다.

* 배치 크기, 학습 속도 및 학습 속도 스케줄러와 같은 하이퍼파라미터는 수렴 속도에 영향을 미치지만 (1) 해당 값이 합리적인 범위에 속하고 (2) 모델이 적절한 양의 데이터에 대한 충분한 단계로 훈련되는 경우 최종 수렴 손실에는 영향을 미치지 않습니다.
* 일괄 처리 크기를 조정 하는 것은 시간과 계산 간의 절충을 포함 합니다. 최적의 시간/계산 균형을 이루는 임계 배치 크기는 손실 값에만 기초하여 결정될 수 있다. 이러한 임계 배치 크기를 갖는 훈련은 무한 배치 크기(가능한 최소 요구 단계)를 사용하는 것과 비교하여 특정 손실 값을 달성하기 위해 두 배의 많은 훈련 단계가 필요하다.
* 컨텍스트 길이, 토큰화, 데이터 배포 및 모델 구성은 조정 법칙 수식의 상수에 큰 영향을 미치지만 조정 법칙 자체의 형식에는 영향을 미치지 않습니다.
* 고정 컨텍스트 길이, 토큰화, 데이터 배포, 모델 구성 및 학습 속도 스케줄러가 지정 된 경우 학습 속도가 최적으로 구성 된 경우 학습 단계, 배치 크기 및 모델 크기와 관련하여 성능에 대 한 정확하고 예측 가능한 멱함수 확장을 관찰 합니다.
* 6천만 개 미만의 매개 변수를 사용 하 여 모델을 학습 하 여 스케일링-법칙 공식에서 상수를 정확하게 추정할 수 있습니다. 이를 통해 (1) 가능한 최소 손실; (2) 특정 손실을 달성하기 위해 필요한 최소 훈련 단계 및 처리된 토큰; (3) 임의의 손실 값에서의 임계 배치 크기; 및 (4) 임의의 배치 크기를 갖는 완전한 테스트 손실 궤적을 포함하여 훈련 전에 최대 330억 개의 파라미터를 갖는 모델에 대한 다양한 속성을 예측할 수 있다.
* 이러한 예측 특성에는 많은 흥미로운 기능이 있어 고정된 계산 예산 내에서 최적의 모델 크기 및 학습 단계, 필요한 데이터 양, 여러 데이터 세트의 이상적인 혼합 비율 등과 같은 대규모 모델을 학습하기 전에 중요한 요인을 식별하는 데 도움이 됩니다.

## 2 Preliminary

기본적으로 스케일링 법칙 [14]는 모델이 수렴하도록 훈련될 때 최종 손실 또는 훈련 중 특정 단계에서의 손실인 주어진 모델의 _검증/테스트_ 손실 3을 예측하는 방법을 보여줍니다. 스케일링 법칙은 다양한 가능성 기반 작업에 걸쳐 광범위하게 유효한 것으로 입증되었다[15]. 스케일링 법칙을 준수함으로써, 연구자들은 모델 파라미터 및 트레이닝 데이터의 변화가 실제로 트레이닝하기 전에 대형 언어 모델의 전체 효과에 어떻게 영향을 미치는지의 패턴을 발견할 수 있다.

각주 3: 단순화를 위해 스케일링 법칙 내에서 상수 항에서만 다른 다양한 데이터 분포에서 측정된 손실에 대해 일관된 경향을 관찰했기 때문에 이후 "테스트 손실"을 사용한다.

NotationTo enhance the clarity of explanation, we use the following notation throughout the paper, mostly are adapted from [14]:

* 컨텍스트에서 토큰에 대해 평균화된 nats의 교차 엔트로피 손실
* 모델 매개 변수의 수 _모든 어휘 및 위치 임베딩을 제외_
* 배치 크기
* [14]에 정의된 임계 배치 크기입니다. 임계 배치 크기에서의 훈련은 시간과 계산 효율성 사이의 대략적인 최적 절충을 제공합니다.
* 처리된 토큰 양
* 손실의 주어진 값에 도달하는 데 필요한 처리된 토큰의 최소 양에 대한 추정치입니다. 이것은 또한 모델이 임계 배치 크기보다 훨씬 작은 배치 크기로 훈련된 경우 사용되는 훈련 단계 수입니다.
* 교육 단계 수
* 손실의 주어진 값에 도달하는 데 필요한 최소 훈련 단계 수의 추정치입니다. 이것은 또한 모델이 임계 배치 크기보다 훨씬 큰 배치 크기로 훈련된 경우 사용되는 훈련 단계 수입니다.
* 추정이 필요한 스케일링-law 수식의 상수 항

목표 원래 스케일링 법칙 논문은 광범위한 내용을 포괄한다. 대형 언어 모델을 사전 훈련하는 데 유용한 핵심 측면을 강조하기 위해 본 논문은 스케일링 법칙의 기초가 되는 다음 세 가지 함수를 추정하는 데 중점을 둔다. 이 세 가지 함수를 사용하여 훈련을 시작하기 전에 대규모 언어 모델의 훈련 행동을 정확하게 예측할 수 있습니다. - \(N\) 매개 변수를 사용하여 모델을 훈련할 때 nats에서 수렴 손실을 예측합니다.
*는 배치 크기가 무한하여 훈련 단계 수가 최소화된다는 점을 감안할 때 \(N\) 매개 변수가 있는 모델에 대해 특정 훈련 단계 \(S_{min}\)에서 손실 값을 예측합니다.
* 유한 배치 크기 \(B \)에서 \(N \) 매개 변수가 있는 모델에 대해 특정 훈련 단계 \(S \)에서 손실 값을 예측 합니다.

논의를 단순화하기 위해, 우리는 정밀한 스케일링 법칙들을 도출하기 위해 다음의 가정들을 고수하며, 이들 각각은 현대의 대형 언어 모델들을 사전 트레이닝하는 데 있어서 현실 세계 시나리오들을 반영한다:

1. 모델은 명백한 병목 현상 없이 표준 트랜스포머 아키텍처 [17]을 따르는데, 이는 GPT [1], Llama [13] 및 Gemini [14]23 각주 4와 같은 대형 언어 모델에서 사실상의 아키텍처였다 : _e.g._, 제한된 양의 주의 헤드 또는 부적절한 레이어 정규화, 이는 스케일링 법칙의 추정 프로세스를 불안정하게 만드는 것을 발견한다.
2. 우리는 광범위한 훈련 데이터 세트에 대한 액세스를 가정하며, 훈련 프로세스는 결코 데이터를 재사용하지 않는다. 이는 온라인 플랫폼과 디지털 콘텐츠의 확산이 가용 데이터의 풍부함에 기여했기 때문에 모델 사전 훈련의 전형적인 시나리오를 반영한다.
3. 트레이닝 데이터는 트레이닝 단계들에 걸쳐 균일하게 분포된다. 이는 교육과정 학습[1]과 같은 특별한 트릭이 적용되지 않는 한 훈련 전 단계에서 훈련 데이터가 무작위로 뒤섞이는 경우가 많기 때문에 합리적인 가정이기도 하다.

## 3 스케일링 법칙 도출

_"스케일링 법칙은 신에 의해 결정됨;_

_The constants is determined by members of the technical staff"_

-- 샘 알트만

이 절에서는 스케일링 법칙을 단계별로 도출하는 방법에 대한 명확한 지침을 제공한다. 가장 중요한 것은 스케일링 법칙의 맥락에서 샘 알트만이 강조하는 기초적인 측면인 스케일링 법칙 공식에서 모든 상수를 실질적으로 추정하기 위한 전체 세부 사항을 공개한다는 것이다.

### Predicting \(L(n)\)

먼저 최종 테스트 손실 예측에 대해 논의해 보겠습니다. 우리가 무한한 트레이닝 데이터(_i.e._, 무한한 토큰 수)를 가지고 있다고 가정하고, \(N\) 파라미터를 갖는 모델을 트레이닝할 것이며, 스케일링 법칙 [15]은 다음과 같은 상관 관계를 도출한다:

\[L(N)=\left(\frac{N_{c}}{N}\right)^{\alpha_{N}}, \tag{3.1}\]

여기서 \(N_{c}\)와 \(\alpha_{N}\)는 통계적 피팅에 의해 발견될 수 있는 일정한 스칼라이고, \(L(N)\)은 모델이 수렴할 때 _최종 테스트 손실_이다. 즉, \(L(N)\)은 무한 훈련 데이터, 최적 최적화기 및 하이퍼파라미터, 충분한 훈련 시간이 주어진 경우, \(N)-파라미터 모델이 최상의 (_i.e._)에서 달성할 수 있는 _테스트 손실 한계_이다. 우리는 모델이 무한한 데이터로 훈련된다고 가정하기 때문에 과적합이 불가능하고 따라서 훈련 손실은 테스트 손실과 거의 동일한 경향을 나타내야 한다.

\(N_{c}\)와 \(\alpha_{N}\)를 추정한다. 두 개의 상수 스칼라들을 추정하기 위해, 이 모델들이 수렴하여 최종 테스트 손실을 얻을 때까지 _무한 훈련 데이터_ 상에서 다양한 수의 파라미터들(say, \(N=1M,10M,\ldots,10(k-1)M\))로 일련의 \(k\) 모델들을 훈련시킬 수 있다. 이러한 \((N_{0},L_{0}),\ldots,(N_{k-1},L_{k-1})\의 쌍으로 선형 w.r.t. \(\alpha_{N}\log N_{c}-\alpha_{N}\log N_{i}-L_{i}=0\big{|}_{i=0}^{k-1}}\)와 \(\log N_{c}\)의 형태로 \(k\) 방정식을 얻을 수 있다. \ (N_{c}\) 및 \(\alpha_{N}\)는 선형 회귀를 사용한 매개변수 피팅에 의해 추정될 수 있다. 실제적으로 \(k=7\)을 설정하면 정확한 추정이 가능하다는 것을 알 수 있었다.

하이퍼파라미터에 대한 허용 오차 실험 결과, 충분한 학습 데이터가 주어졌을 때 배치 크기, 학습률, 학습률 스케줄러와 같은 하이퍼파라미터는 수렴 속도에 영향을 주지만, 그 값이 합리적인 범위 내에 있는 한 최종 수렴 손실에는 영향을 주지 않는다는 것을 확인하였다. 이 발견은 이전 연구[15, 16, 17]와 일치한다. 따라서 이러한 \(k\) 데이터 점소프 \((N_{0},L_{0}),\ldots,(N_{k-1},L_{k-1})\를 얻기 위해서는 광범위한 하이퍼파라미터 튜닝을 수행할 필요가 없다. 대신, 모델이 수렴할 때까지 반복된 데이터 없이 훈련된다면 모든 \(k\) 모델에 대해 고정된 하이퍼파라미터 세트를 사용하기에 충분하다.

무한 학습 데이터는 무엇을 의미하는가? 위의 단계들은 수렴될 때까지 무한 데이터에 대한 하이퍼파라미터들의 고정된 세트를 갖는 학습 모델들을 요구한다. 그러나 실제 시나리오에서는 데이터가 항상 유한합니다. 그럼에도 불구하고 주어진 모델에 대해 _상대적으로_ "무한 훈련 데이터"를 갖는 것이 가능하다. 우리는 \(N=10\) 매개 변수만 있는 극히 작은 모형을 가지고 있다고 가정하자. 이 마이크로 모델의 경우, 매우 제한된 용량으로 인해, 우리가 그것을 훈련시키기 위해 얼마나 많은 훈련 데이터(_e.g._, 1B, 2B 또는 1T 토큰5)를 사용하더라도, 모델의 성능은 향상될 수 없고, 훈련 동적(_i.e._, 임의의 주어진 훈련 단계에서의 테스트 손실)은 거의 동일하다. 즉, 이 마이크로 모델은 _임계 크기_를 넘어 데이터 크기의 증가를 인식하지 못합니다. 이 경우 학습 데이터는 이 모델에 대해 _상대적으로_ 무한하다고 말할 수 있습니다. 실제로 \(N\) 매개 변수가 있는 주어진 매개 변수에 대해 이 임계 크기를 계산할 수 있습니다 ([16]의 식 4.4 참조). 여기서는 \(N\)-모수 모델에 대해 데이터 크기가 (상대적으로) 무한한지 여부를 추론하는 간단한 경험적 방법을 제공한다. 훈련 데이터가 (상대적으로) 무한할 때는 과적합이 발생하지 않아야 합니다. 그 결과, \(N\)-파라미터 모델을 학습하고 학습 손실과 테스트 손실의 분기 여부를 관찰할 수 있다. 훈련 및 테스트 손실의 동역학이 모든 곳에서 거의 동일하면(_i.e._, 오버피팅 없음), 주어진 \(N\)-파라미터 모델에 대해 훈련 데이터가 (상대적으로) 무한하다는 것을 안전하게 추론할 수 있다.

각주 5: 우리의 세 번째 가정에 따르면, 훈련 데이터의 품질은 균일하고 크기와 무관하다는 점에 유의하라.

### Predicting \(L(n,S_{min})\)

전체 테스트 손실 곡선을 예측하는 것은 임의의 주어진 트레이닝 단계에서 테스트 손실을 추정하는 것을 필요로 한다. 이는 훈련 프로세스에 상당한 영향을 미치는 많은 다른 요인(_예:_, 학습률 일정, 최적화기 및 배치 크기)이 있기 때문에 매우 어렵다. 그러나 [16] 및 [17]에서 볼 수 있듯이 확률적 경사 하강(SGD)이 경사 하강(GD)이 되는 _무한히 큰 배치 크기_에서 모델을 훈련하면 이러한 요인의 영향을 크게 제거할 수 있다. 무한히 큰 배치 크기에서 훈련 단계는 특정 손실을 달성하는 데 필요한 최소 가능한 훈련 단계 수이기 때문에 \(S_{\min}\)로 표시된다. 훈련 배치 크기가 클수록 필요한 훈련 단계가 줄어듭니다. 결과적으로, 스케일링 법칙들 [16]은 무한한 트레이닝 데이터 및 무한히 큰 트레이닝 배치 크기가 주어지면, 임의의 주어진 단계 \(S_{\min}\)에서 테스트 손실 \(L(N,S_{\min})\)은 다음과 같이 된다:

\[L(N,S_{\min})=\left(\frac{N_{c}}{N}\right)^{\alpha_{N}}+\left(\frac{S_{c}}{S_{ \min}}\right)^{\alpha_{S}}, \tag{3.2}\]

여기서 \(\left(\frac{N_{c}}{N}\right)^{\alpha_{N}}\)는 Eq. 3.1이고, \(S_{c}\)와 \(\alpha_{S}\)는 추정하고자 하는 일정한 스칼라이다.

\(S_{c}\) 및 \(\alpha_{S}\)를 추정합니다. Eq. 3.2의 첫 번째 항이 Sec. 3.1에서 해결되었기 때문에, \(N=1M\)과 같은 임의의 주어진 모델에 대해, \(L(1M,S)=C+\left(\frac{S_{c}}{S_{\min}}\right)^{\alpha_{S}}\를 가지며, 여기서 \(C=\left(\frac{N_{c}}{1M}\right)^{\alpha_{N}}\)는 방정식의 각 요소가 알려져 있기 때문에 상수이다. 이제 _무한히 큰 배치 크기_와 최적으로 설정된 학습률 6으로 \(1M\)-파라미터 모델을 학습시킬 수 있다면 \((L,S_{\min})\)의 많은 쌍을 얻을 수 있다. Eq. 3.2의 두 크기에 로그를 취함으로써 \(\alpha_{s}\)와 \(\log S_{c}\)에 선형인 방정식을 얻는다. 다시, 우리는 선형 회귀를 사용하여 \(S_{c}\) 및 \(\alpha_{s}\)를 추정할 수 있다.

각주 6: 학습률은 훈련 손실이 감소하는 비율을 최대화하도록 설정된다.

무한 배치 크기는 무엇을 의미합니까? 실제로 무한 배치 크기로 모델을 훈련하는 것은 불가능하지만 "시행 오류" 방법을 사용하여 _상대적으로 무한 배치 크기_라고 하는 무한 배치 크기와 동등한 충분히 큰 배치 크기를 찾을 수 있습니다. 이는 충분히 큰 배치 크기의 추가 증가가 특정 손실 값을 달성하기 위해 요구되는 트레이닝 단계들의 수를 더 감소시키지 않으며, 따라서 트레이닝 손실 곡선을 변경하지 않는다는 사실에 기초한다. 결과적으로, 트레이닝 손실 곡선이 정지 상태가 될 때까지 배치 크기를 증가시킴으로써 이 비교적 무한한 배치 크기를 찾을 수 있다. 우리는 10M의 크기에서 모델 크기의 경우 40M 토큰 배치가 충분히 크다는 것을 경험적으로 발견했다. 이러한 방식으로, \(S_{c}\) 및 \(\alpha_{S}\)는 이 _상대적으로 무한 배치 크기_에서 10M 모델의 트레이닝 동안 손실 값 및 단계에 의해 추정될 수 있다.

### Predicting \(L(n,S,B)\)

작은 모델에 대해 _상대적으로 무한 배치 크기_를 찾는 것은 쉽고 이 트릭을 사용하여 \(S_{c}\) 및 \(\alpha_{S}\)를 추정할 수 있다. 그러나 대규모 모델의 경우 이러한 _상대적으로 무한한 배치 크기_를 사용한 교육은 저렴하지도 경제적이지도 않습니다. 실제로, 우리는 _유한 배치 크기_\(B\) 하에서 대형 모델의 손실 궤적을 예측하는 데 더 관심이 있다.

(S_{\min}\)에서 \(S\).다행히도, 임의의 배치 크기를 갖는 훈련 단계 \(S\)와 충분히/무한히 큰 배치 크기를 갖는 훈련 단계 \(S_{\min}\) 사이에 변환이 있다. 모델을 최적화하는 동안 어떤 점에서는 \(\theta\)를 모델 파라미터로 하고, 그 점에서는 SGD에 의해 추정된 잡음 구배를 \(G_{\mathrm{est}}}\)로 한다. \(G_{\mathrm{est}}\)는 무한히 큰 배치 크기(_i.e._, \(\mathbb{E}[G_{\mathrm{est}}]=G\)를 갖는 실제 기울기 \(G\)인 랜덤 변수이다. 테일러 전개에 따라, 이 파라미터 갱신을 적용한 후의 손실값은,

\[L(\theta-\epsilon G_{\mathrm{est}})\approx L(\theta)-\epsilon G_{\mathrm{est} }^{T}G_{\mathrm{est}}+\frac{1}{2}\epsilon^{2}G_{\mathrm{est}}^{T}HG_{\mathrm{ est}}, \tag{3.3}\]

여기서 \(\epsilon\)는 학습율이고 \(H\)는 헤시안 행렬이다. 여기서, \(G_{\mathrm{est}}}\)에 의해 도입된 랜덤성은 기대치를 계산함으로써 제거될 수 있다:

\[\begin{split}\mathbb{E}[L(\theta)]-\epsilon\mathbb{E}[G(\theta)]-\epsilon\mathbb{E}[G_{\mathrm{est}}G_{ \mathrm{est}}^{T}]+\frac{1}{2}\epsilon^{2}\mathbb{E}[G_{\mathrm{est}}^{T}HG_{ \mathrm{est}}]\\ &=L(\theta)-\epsilon|G|^{2}+\frac{1}{2}\epsilon^{2}\left(G^{T}HG +\frac{\mathrm{tr}(H\Sigma)}{B}\right),\end{split} \tag{3.4}\]

여기서, \(B\)는 사용 중인 배치 크기이고, 우리는 손실 값의 감소를 얻을 수 있다.

\[\Delta L=-\epsilon|G|^{2}+\frac{1}{2}\epsilon^{2}\left(G^{T}HG+\frac{\mathrm{ tr}(H\Sigma)}{B}\right). \tag{3.5}\]

오른쪽은 2차 함수 w.r.t. \(\epsilon\)에 유의하고, 단순화를 위해 \(a=\frac{1}{2}\left(G^{T}HG+\frac{\mathrm{tr}(H\Sigma)}{B}\right)\) 및 \(b=-|G|^{2}\)를 사용한다. 따라서 \(\epsilon=-\frac{b}{2a}=\frac{|G|^{2}}{G^{T}HG+\frac{\mathrm{tr}(H\Sigma)}{B}}\), \(\Delta L_{\max}=-\frac{b^{2}}{4a}=\frac{|G|^{4}}{2\left(G^{T}HG+\frac{\mathrm{tr}(H\Sigma)}{B}\right)}\)에서 최대 감소 \(\Delta L_{\max}\)가 달성된다. 배치크기가 \(B\rightarrow\infty\), \(\lim\limits_{B\rightarrow\infty}\Delta L_{\max}=\frac{|G|^{4}}{2G^{T}HG}\)일 때, 우리는 다음과 같이 된다.

\[\frac{\Delta L}{\limits_{B\rightarrow\infty}\Delta L}=\frac{\frac{|G|^{4}}{2G^{T} HG}}=\frac{G^{T}HG}{G^{T}HG+\frac{\mathrm{tr}(H\Sigma)}{B}}=\frac{1}{1+\frac{ \mathrm{tr}(H\Sigma)/(G^{T}HG)}{B}}}. \tag{3.6}\]

Let \(\mathcal{B}_{\text{noise}}=\mathrm{tr}(H\Sigma)/(G^{T}HG)\), we have

\[\frac{\Delta L}{\lim\limits_{B\rightarrow\infty}\Delta L}=\frac{1}{1+\frac{ \mathcal{B}_{\text{noise}}}{B}} \tag{3.7}\]

그림 2: 컨텍스트 길이가 4096인 10M 모델의 배치 크기 스캔입니다. 각 곡선은 배치 크기 및 훈련 단계가 달라짐에 따라 동일한 손실 값을 가집니다.

따라서 \(\frac{\lim\limits_{B\rightarrow\infty}\Delta L}{\Delta L}=1+\mathcal{B}_{\text{ noise}}/B\). 이 공식은 무한히 큰 배치 크기를 갖는 한 단계가 배치 크기를 갖는 \(1+\mathcal{B}_{\text{noise}}/B\) 단계와 거의 같다는 것을 나타낸다. 따라서,

\[S_{\min}=\frac{S}{1+\mathcal{B}_{\text{noise}}/B} \tag{3.8}\]

식 3.8의 제약 조건 하에서 \(B_{crit}(L)\)을 정의하면 \(L\)에서 임계 배치 크기를 도출할 수 있으며 이는 시간(\(S/S_{min}\))과 계산(\(E/E_{min}\) 사이의 trade-off를 최소화한다:

\[B_{\text{crit}}(L)=\operatorname*{arg\,min}_{B}\left(\frac{S _{min}+\frac{E}{E_{min}\right) \tag{3.9}\] \[E_{min}=\min_{B}BS=\min_{B}S_{min}(B+\mathcal{B}_{\text{noise}})=\lim_{B\to 0}S_{min}(B+\mathcal{B}_{\text{noise}}})=S_{min} \mathcal{B}_{\text{noise}}}\] \[\우측 B_text{crit}}(L)=\operatorname*{arg\,min}_{B} \left(\frac{S_{min}+\frac{BS}{S_{min}\mathcal{B}_{\text{noise}}}\right)= \operatorname*{arg\,min}_{B}\left(2+\frac{\mathcal{B}

\(B_{crit}(L)\)을 식 3.8에 대입하면 [10]에서 정의한 식을 정확히 복원할 수 있는데, 이는 매우 다양한 신경망 작업에 적용되는 것으로 나타났다:

(L)S_{\text{min}}-1-\frac{B}{B_{\text{crit}}(L)S_{\text{min}}-1\right) \[\Rightarrow\left(\frac{BS^{2}}}^{2}}-\frac{B_{\text{crit}}(L)S_{\text{min}}=0\] \[\Rightarrow\left(\frac{S}{S_{\text{min}}}-1\right)\left(\frac{BS^{2}}}-1\right) \[\Rightarrow\left(\frac{S}{S_{\text{min}}}-1\right)=1\] \[\Rightarrow\left(\frac{S}{S_{\text{min}}}-1\right)\left(\frac{E}{E_{\text{min}}}-1\right)=1 \tag{3.11}\]

우리는 또한 우리의 실험에서 식 3.11의 유효성을 검증했다(그림 2의 예를 참조). 이 \(B_{\text{crit}}(L)\)을 다시 식 3.11로 되돌리면 다음과 같을 수 있습니다. \(B=B_{\text{crit}}(L)\)에 \(S=2S_{min}\)와 \(E=2E_{min}\)가 있습니다. 즉, 임계 배치 크기를 가진 훈련은 최소 단계 수의 두 배와 특정 손실 값에 도달하는 데 필요한 토큰의 비용이 \(L\)입니다.

Eq. 3.10은 유한 배치 크기 \(B\)에서 \(S_{min}\)와 \(S\) 사이의 변환을 성공적으로 설정했다. \(L(N,S_{min})\)에서 \(L(N,S,B)\)로 이동하기 위해 왼쪽 코어는 임계 배치 크기 \(B_{\text{crit}}(L)\)를 추정한다.

[10] found that the critical batch size also roughly follow a power law in \(L\):

\[B_{\text{crit}}(L)=\frac{B_{*}}{L^{1/\alpha_{B}}} \tag{3.12}\]

보이는 바와 같이, \(B_{\text{crit}}(L)\)는 손실 값 \(L\)에만 의존하는 변수이다. 우리는 상수항 \(B_{*}\)과 \(\alpha_{B}\)을 추정하면 된다.

\(B_{*}\) 및 \(\alpha_{B}\)을 추정하기 위해 다양한 배치 크기를 갖는 고정 크기 모델을 학습하여 일관된 손실 값을 나타내는 \(k\) 등고선을 생성한다. 그런 다음 방정식 3.11을 사용하여 이러한 일련의 등고선을 피팅하여 \(k\) 추정된 쌍 집합 \((E_{min}^{1},S_{min}^{1}),\ldots,(E_{min}^{k},S_{min}^{k})\)(그림 2에서 볼 수 있듯이 \(k=5\) 및 \(N=10M\))을 생성한다. Eq 3.11은 선형 w.r.t. \(S_{min}\) 및 \(E_{min}\)이므로 선형 회귀를 사용하여 \((E_{min},S_{min})\) 쌍을 얻을 수 있다.

이러한 \((E_{min},S_{min})\의 \(k\) 쌍을 이용하여 \((B_{crit}(L),L)\)의 \(k\) 쌍을 \(B_{crit}(L)=E_{min}/S_{min}\로 설정하여 \(B_{*}\)와 \(\alpha_{B}\)의 값을 추정할 수 있다. Eq 3.12에서 로그를 취하면 방정식은 선형 w.r.t. \(\alpha_{B}\) 및 \(\log B_{*}\)가 되며 선형 회귀로 해결할 수 있다. 경험적으로 \(\alpha_{B}\) 값이 상당히 민감한 계수임을 알 수 있었다. 0 및 0.5의 범위 내에서 초기화하는 것은 전형적으로 보다 안정적인 추정 프로세스(7)를 초래한다.

마지막으로 \(S_{min}\)을 \(S\)로 치환한 후 \(S_{min}\)과 \(S\)의 분석적 관계를 설정한 후 \(S\)를 \(S\)로 치환하여 \(S\)를 \(L\)로 연결할 수 있다. 식 3.10을 식 3.2에 대입하면,

\[\begin{split} L(N,S,B)&=\left(\frac{N_{c}}{N}\right)^ {\alpha_{N}}+~{}\left[\frac{S_{c}\cdot(1+B_{\text{crit}}(L)/B)}{S}\right]^{ \alpha_{S}}\\ &=\left(\frac{N_{c}}}{N}}\right)^{\alpha_{N}+~{}\left(\frac{S_{c}}}\right)^{\alpha_{N}+~{}\left(\frac{S_{c}}}}\right)^{\alpha_{S}}\cdot\left(1+\frac{B_{*}}{B\cdot L(N,S,B)^{1/\alpha_{B}}}\right)^{\alpha_{S}}}}\right).\end{split} \tag{3.13}\]

이제 고정된 유한 배치 크기 \(B\)에서 손실 값 \(L\)과 훈련 단계 \(S\) 사이의 관계인 Eq. 3.13을 분석해보자. 다른 모든 양은 이전에 계산된 일정한 스칼라(\(N_{c},\alpha_{N},S_{c},\alpha_{S},B_{*}\) 및 \(\alpha_{B}\))이다. 손실 값 \(L(N,S,B)\)을 계산하는 것의 어려움은 방정식의 양변에 나타난다는 것이다. 여기에서 \(L(N,S,B)\)을 분리할 수 있는 분석 솔루션은 없지만 식 3.13에서 유일하게 알려지지 않은 양이기 때문에 임의의 루트 찾기 방법(_e.g._, 이분법)으로 \(L(N,S,B)\)을 수치적으로 추정할 수 있다.

구체적으로, Eq. 3.13이 단조적으로 감소하는 w.r.t. \(L(N,S,B)\):

\left(\frac{S_{c}}{S}\right)^{\alpha_{N}+~{}\left(\frac{S_{c}}{S}\right)^{\alpha_{S}}\left(\frac{S_{c}}{S}\right)^{\alpha_{S}}\left(1+\frac{B_{*}}{B \cdot L(N,S,B)^{1/\alpha_{B}}}\right)^{\alpha_{S}}\left(N,S,B)}{\mathrm{d}L(N,S,B)}&=\left(\frac{B_{c}}{B}}}{B}}}\right)^{\alpha_{S}}\left(1+\frac{B_{*}}}}}\right)^{\alpha_{S}}\left(N,S,B)}{\mathrm{d}L(N,S,B)}&=\left(\frac{B_{c}}{B}}}{B}}}\right)^{\alpha

따라서 일단 \((L_{\text{left}},L_{\text{right}})\)의 범위를 찾고, 여기서 \(f(L_{\text{left}})\cdot f(L_{\text{right}})<0\인 임계점 \(L^{*}\)을 이등분법을 통해 \(f(L^{*})=0\로 반복적으로 탐색할 수 있다. 실제로 손실 값은 양수이고 제한된 범위 내에 속하며, \(L_{\text{left}}=0,L_{\text{right}}=10\)을 설정하는 것은 항상 전체 범위를 커버하기에 충분하다.

Learning RateEq. 3.13의 종속성은 \(N,S\) 및 \(B\)에만 의존하지만 학습률과 같은 다른 하이퍼파라미터에는 의존하지 않는다. 유한 배치 크기를 갖는 트레이닝을 할 때, 학습률은 의심할 여지 없이 손실 궤적에 상당한 영향을 미칠 것이다. 식 3.13에서 학습률의 영향력을 제거하는 것은 분명히 불가능하다. 경험적으로, 우리는 식 3.13으로부터의 예측이 학습률이 _최적으로 설정_될 때, 즉 초기 단계 동안 곡률 감소율을 최대화하도록 조정될 때 정확하다는 것을 관찰한다. [16, BCC\({}^{+}\)24]에서 제시한 바와 같이 모델 크기가 커지면 최적의 학습률이 낮아져야 한다. 우리는 초기 훈련 단계에서 최적의 학습률을 찾기 위해 단순히 "시행착오" 방법을 채택할 수 있다. 고정 크기 모델에 대해 학습률이 결정되면 식 3.13(초기 준비 단계 후)부터 정확한 테스트 손실 궤적을 예측할 수 있다.

## 4 Experiments

이론적 분석을 수행하고 스케일링 법칙을 도출한 후, 본 절에서는 스케일링 법칙의 유효성을 검증하기 위한 실증 실험을 제시한다. 표준 실습에 따라 디코더 전용 트랜스포머 아키텍처[17]를 활용하고 두 데이터 세트에 대해 실험을 수행했다: 하나는 C4 데이터 세트를 사용하고 다른 하나는 맞춤형 혼합 데이터 세트를 사용한다. 스케일링-법칙 공식을 도출하기 위해 위에서 설명한 추정 단계를 따랐다.

### C4 Dataset으로 크기 조정

C4 데이터 세트는 커먼 크롤의 웹 크롤 코퍼스의 크고 정리된 버전이다[18]. 컨텍스트 창은 1024로 설정되었으며 각 배치에는 약 500k 토큰이 포함되어 있다. 원래 C4의 1%를 활용했습니다.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline Parameter & \(\alpha_{N}\) & \(\alpha_{S}\) & \(\alpha_{B}\) & \(N_{c}\) & \(S_{c}\) & \(B_{*}\) \\ \hline \hline C4 Value & \(0.076\) & \(0.67\) & \(0.205\) & \(1.5\times 10^{14}\) & \(2.6\times 10^{3}\) & \(1.7\times 10^{8}\) \\ \hline
**[테스트 집합으로 데이터 세트 및 중복 제거 프로세스를 수행하여 훈련 집합에서 중복되는 텍스트를 제거합니다. 총 10개의 작은 모델만 훈련하여 최대 60M 매개변수를 사용하여 스케일링-법칙 공식의 상수 항을 추정했다. 추정된 상수 항들은 표 1에 제시되어 있다. 추정된 공식들을 사용하여 2B 모델(상수 항들을 추정하는 데 사용되는 작은 모델보다 30배 더 큰)의 실제 및 예측된 손실 궤적이 도 1의 좌측에 묘사된다. 예측된 손실 궤적이 실제 손실 궤적과 밀접하게 정렬되는 것을 관찰할 수 있다. 우리의 추정된 상수 항은 또한 다른 설정을 사용했음에도 불구하고 [10]에서 추정된 항과 멀지 않은데, 이는 둘 다 크롤링된 웹사이트 텍스트로 구성된 C4 및 WebText의 유사한 분포 때문일 수 있다. 이는 스케일링 법칙의 멱항(power term)이 주로 데이터 다양체에 의존한다는 [11]의 주장을 강화한다.

### 대규모 혼합 언어 데이터 집합으로 크기 조정

맞춤형 혼합 데이터 세트를 사용한 실험을 위해 영어, 중국어 및 코드 데이터의 혼합으로 구성된 3T 토큰이 포함된 데이터 세트를 수동으로 선별했다. 데이터는 품질을 보장하기 위해 일련의 엄격한 중복 제거, 필터링 및 세척 과정을 거쳤다. 컨텍스트 창은 4096으로 설정되었으며 각 배치에는 약 4M 토큰이 포함되어 있다. 유사하게, 우리는 스케일링-법칙 공식의 상수 항을 추정하기 위해 각각 최대 60M 매개변수를 갖는 10개의 작은 모델만을 훈련시켰다. 공식은 최대 33B(600배 더 큰) 모델의 테스트 손실 궤적을 예측하는 데 사용된다. 도메인 내 및 도메인 외 테스트 데이터 모두에서 예측된 손실 궤적의 정확도를 테스트한다.

도메인 내 테스트 손실 예측 도메인 내 테스트 세트에 대해 훈련 데이터에 사용된 것과 동일한 분포를 따르는 코드 데이터(코드 데이터는 전체 훈련 데이터의 \(10\%\)로 구성됨)를 사용한다. 추정된 공식을 사용하여 33B 모델의 실제 및 예측된 손실 궤적이 오른쪽에 표시된다.

그림 3: 도메인 외 개인 중국 테스트 데이터에 대한 500M, 2B 및 33B 모델의 실제 및 예측 손실 궤적(섹션 4.2). 영역 외 테스트 데이터의 손실 궤적은 큰 변동을 갖지만 전체 추세 및 최종 수렴 손실 값은 여전히 예측과 밀접하게 일치한다. 스케일링-법칙 공식에서 추정된 상수 값은 오른쪽 하단에 제공된다.

도 1의 손실 궤적은 200k 단계 후에 일반적으로 정확함을 알 수 있다. 20만 단계 이후에는 예측 손실과 실제 값이 매우 정확하지만, 초기 단계에서는 워밍업의 영향과 오차를 유발하는 큰 예측 승수로 인해 예측이 정확하지 않을 수 있다.

도메인 외 테스트 손실 예측 도메인 외 테스트 세트에 대해 훈련 데이터에서 유형이 매우 드물고 도메인 외 데이터로 간주될 수 있는 프라이빗 중국 데이터를 사용한다. 그림 3에는 추정된 상수 항과 추정된 공식을 사용하여 500M, 2B, 33B 모델의 실제 손실 궤적과 예측된 손실 궤적이 나와 있다. 실제 손실 궤적이 상당한 변동을 나타내기 때문에 도메인 외 데이터를 예측하는 것이 도메인 내 데이터를 예측하는 것보다 더 어렵다는 것이 분명하다. 그럼에도 불구하고 실제 손실 궤적과 예측된 손실 궤적의 전반적인 경향은 밀접하게 일치한다. 최종 수렴 손실 값도 다소 유사하여 도메인 내 및 도메인 외 데이터 모두에 대한 손실 궤적을 예측하는 데 있어 스케일링 법칙의 유효성을 확인한다.

## 5 Discussions

스케일링 법칙의 중요성은 손실 궤적에 대한 단순한 예측을 넘어 확장된다. 더 중요한 것은 매우 큰 모델에 대한 광범위한 조정을 요구하지 않고 최적의 실험 구성을 찾는 데 도움이 될 수 있으며, 이를 통해 연금술과 같은 시행착오 프로세스에서 원칙적인 방법론으로 큰 언어 모델의 훈련을 변환할 수 있다는 것이다. 이 섹션에서는 스케일링 법률의 주요 이점을 강조하고 이를 넘어 더 발전하는 방법에 대해 논의한다.

모든 하이퍼파라미터가 잘 조정되고(특히 학습률 및 정규화 하이퍼파라미터) 훈련 단계 수가 충분한 한, 임의의 배치 크기[19]를 사용하여 동일한 최종 성능을 달성해야 한다고 여겨지므로 배치 크기는 주로 언어 모델의 훈련 속도에 영향을 미친다. 종종, 큰 언어 모델들을 트레이닝할 때, 이상적인 배치 사이즈는 계산 비용을 고려하지 않고 트레이닝 속도를 최대화하기 위해 가용 하드웨어 [1]에 의해 지원되는 가장 큰 배치 사이즈로 설정되도록 제안된다. Eq 3.12에서, 우리는 최적 속도/계산 트레이드오프를 갖는 임계 배치 크기가 손실 값으로부터 분석적으로 계산될 수 있음을 보여준다. 이 공식의 지침에 따라 우리는 모든 손실 궤적에서 선호하는 배치 크기를 추정할 수 있다. 또한, Eq 3.12에서 이 최적 배치 크기는 Eq 3.9와 같이 훈련 시간과 필요한 계산을 균등하게 최소화함으로써 결정된다. 실제로, 우리가 다른 것보다 하나를 우선시하고자 한다면, 우리는 최적의 배치 크기를 도출하기 위해 동일한 프로세스를 따를 수 있다. 이를 통해 체계적인 방식으로 맞춤형 요구에 따라 최적의 배치 크기를 얻을 수 있습니다.

\(N\)와 \(S\)를 결정하는 것은 실제로 가장 저렴한 모델 크기를 선택하고 수렴할 때까지 모델을 훈련시키는 경우가 많다. 그럼에도 불구하고, 이러한 단순한 접근법은 최적의 구성에서 크게 벗어날 수 있고 상당한 자원 낭비를 초래할 수 있다. 스케일링 법칙은 고정된 계산 예산 \(C\)8에 주어진 최적의 모델 크기 \(N\)와 훈련 단계 수 \(S\)를 선택하는 데 원칙적인 접근법을 제공한다. Eq 3.13이 이미 손실 \(L\), 배치 크기 \(B\), 모델 크기 \(N\)와 훈련 단계 \(S\) 사이의 정확한 관계를 제공한다는 점을 감안할 때 임계 배치 크기 \(B=B_{crit}\) 하에서 \(L\)을 최소화하는 모델 크기를 찾을 수 있다. 이 최적 \(N\)은 Eq 3.13 w.r.t. \(N\)의 도함수를 취하여 \(0\)로 함으로써 구할 수 있다. 이 최적 \(N\)을 Eq 3.13에 삽입하고 손실 항을 제거하면 다음과 같습니다.

각주 8: 여기서는 \(C\approx 6NBS\)를 사용하기 위해 [10]을 따릅니다.

[\begin{split} N(C)=N_{c}\left(\frac{\alpha_{N}}\right)^{\alpha_{C}/\alpha_{S}}\left(\frac{\alpha_{N}}\right)^{-1/\alpha_{N}}\left(\frac{\alpha_{N}}\right)L\left(N(C),\infty\right)\\C_{c}=6N_{c}B_{*}S_{c}\left(1+\frac{\alpha_{N}}{\alpha_{S}}\right)^{1/\alpha_{S}+1/\alpha_{N}}\left(\frac{\alpha_{S}}\right)^ {1/\alpha_{S}}\\ \alpha_{C}=1/\left(1/\alpha_{S}+1/\alpha_{B}+1/\alpha_{N}\right) \end{split} \tag{5.1}\]

여기서 \(N(C)\)와 \(S(C)\)는 고정된 계산 예산이 주어진 최적의 모델 크기 및 훈련 단계 수 \(C\)이다. \ (L\left(N\left(C\right),C,S(C)\right)\)는 선택된 \(N(c),C\) 및 \(S(C)\)를 갖는 최종 손실 값이다. 자세한 도출은 [13]의 부록 B.1에서 찾을 수 있다. 위에서 언급한 모든 상수 항은 섹션 3에 설명된 유도 단계를 통해 이미 알려져 있으므로 계산 예산 \(C\)에서 \(N(C)\) 및 \(S(C)\)를 직접 추정할 수 있다. Eq 5.1에 도시된 바와 같이, 최종 손실은 수렴 손실 \(L(N,\infty)\)보다 \(\alpha_{N}/\alpha_{S}\) 더 많다. 따라서 현재 일반적인 관행과 대조되는 수렴할 때까지 모델을 _NOT_ 훈련해야 합니다.

계산 예산 결정 많은 다운스트림 애플리케이션에서 고정된 계산 예산이 제대로 제공되지 않을 수 있습니다. 대신에, 종종 구현 전에 충족되어야 하는 최소 임계 요건이 존재한다. 이러한 경우, 우리는 이 임계값 요구 사항을 충족하기 위해 가능한 최소 계산 예산을 결정해야 한다. 평가 기준은 종종 손실 값과 상관되므로 이 최소 임계값 요구 사항을 특정 손실 값으로 연결할 수 있다. 이 손실 값으로부터 우리는 식 5.1에 제공된 해석적 관계로부터 이를 달성하는 데 필요한 최적의 모델 크기와 최소 계산 예산을 쉽게 결정할 수 있다.

데이터 혼합 비율 결정 사전 훈련 데이터 세트의 품질은 대형 언어 모델의 품질에 영향을 미치는 가장 중요한 요소 중 하나이다[15, 16]. 그러나 여러 데이터 소스로부터 최적의 혼합 비율을 결정하는 것은 조합 조합을 포함하기 때문에 매우 어려운 작업이다[13]. 기존 작업은 보통 직관이나 다운스트림 작업 집합을 사용하여 도메인 가중치(각 도메인에 대한 샘플링 확률)를 결정한다. 스케일링 법칙은 최적의 혼합 비율을 결정하는 데 도움이 되는 몇 가지 새로운 통찰력을 제공할 수 있다. 각 개별 데이터 소스에 대한 대규모 모델의 테스트 손실 궤적을 예측함으로써 각 데이터 소스가 얼마나 중요하고 유용한지 암시적으로 추론할 수 있다(예를 들어, 손실이 한 데이터 소스에서 더 빨리 감소하고 더 낮은 손실 값으로 수렴하면 이 데이터 소스가 더 유용할 수 있다).

컨텍스트 길이 언급된 바와 같이, 컨텍스트 길이는 스케일링-법칙 공식에서 상수 항들의 값들에 상당한 영향을 미친다. 모든 상수 항을 특정 문맥 길이에 고정한다는 것은 새로운 문맥 길이마다 추정 과정을 다시 실행해야 한다는 것을 의미하며, 이는 문맥 길이를 다양한 작업에 맞게 조정하는 것이 일반적이기 때문에 오히려 비효율적이다. 각 위치에서의 손실 값 또한 근사적으로 멱-법칙 관계[13]를 따른다는 점을 감안할 때, 컨텍스트 길이를 수식의 파라미터로 직접 포함하는 것이 가능할 것이다.

Mixture-of-Experst 혼합물-of-experts(MoE) 아키텍처는 밀도가 높은 대응물에 비해 우수한 성능을 보여 인기를 얻었다[14, 15]. MoE 아키텍처에 적용 가능한 유사한 스케일링 법칙을 도출하는 것은 매우 유익할 것이다. MoE 아키텍처에서 각 입력은 각 데이터 포인트에 대해 독립적으로 선택된 네트워크의 매개 변수의 하위 집합과만 상호 작용한다[12, 1]. 이러한 아키텍처의 변경은 \(L(N)\)의 형태에 필연적으로 영향을 미칠 것이다. 왜냐하면 활성화된 파라미터와 총 파라미터의 수가 모두 손실 값에 영향을 미치기 때문이다[10]. Eq 3.9 및 Eq 3.10과 같은 다음 단계는 일반적이며 영향을 받지 않아야 한다.

## References

* [AAA\({}^{+}\)23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _ arXiv preprint arXiv:2303.08774_, 2023.
* [BBPP16] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. 더 빠른 모델을 위한 신경망의 조건부 계산. 2016년.
* [BCC\({}^{+}\)24] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. _ arXiv preprint arXiv:2401.02954_, 2024.
* [BLCW09] Yoshua Bengio, Jerome Louradour, Ronan Collobert and Jason Weston. 교육과정 학습. 2009년 41-48쪽, "제26회 기계 학습에 관한 국제 회의 회보"에서.
* [BMR\({}^{+}\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models is few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901, 2020.
* [BSA\({}^{+}\)23] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. [Machine Learning에 대한 국제 회의]에서 페이지 2397-2430. PMLR, 2023.
* [CDLCG\({}^{+}\)22] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. [Machine Learning에 대한 국제 회의]에서 페이지 4057-4086. PMLR, 2022.
* [DG14] Ludovic Denoyer and Patrick Gallinari. 심층 순차 신경망입니다. _ arXiv preprint arXiv:1410.0510_, 2014.
* [GDG\({}^{+}\)23] Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, and Zachary Nado. 딥러닝 튜닝 플레이북, 2023. URL [http://github.com/google-research/tuning_playbook](http://github.com/google-research/tuning_playbook). 버전 1.0
* [GSH23] Leo Gao, John Schulman, and Jacob Hilton. 보상 모델 과최적화를 위한 법칙의 크기 조정 *International Conference on Machine Learning_에서 페이지 10835-10866. PMLR, 2023.
* [HBM\({}^{+}\)22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.
* [HKK\({}^{+}\)20] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. _ arXiv preprint arXiv:2010.14701_, 2020.
* [HNA\({}^{+}\)17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kiannejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 딥러닝 스케일링은 경험적으로 예측 가능합니다. _ arXiv preprint arXiv:1712.00409_, 2017.
* [IPH\({}^{+}\)24] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. 대형 언어 모델의 다운스트림 작업 성능에 대한 법칙 크기 조정 _ arXiv preprint arXiv:2402.04177_, 2024.
* [JSR\({}^{+}\)24] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. _ arXiv preprint arXiv:2401.04088_, 2024.
* [KMH\({}^{+}\)20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 신경 언어 모델의 법칙을 조정합니다. _ arXiv preprint arXiv:2001.08361_, 2020.
* [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei 및 OpenAI Dota 팀. 대규모 배치 훈련의 경험적 모델입니다. _ arXiv preprint arXiv:1812.06162_, 2018.
* [RSR\({}^{+}\)19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계를 탐색합니다. _ arXiv e-prints_, 2019, 1910.10683.
* [SK22] Utkarsh Sharma and Jared Kaplan. 데이터 다양체 차원에서 법칙을 조정합니다. _ The Journal of Machine Learning Research_, 23(1):343-376, 2022.
* [SLA\({}^{+}\)19] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E Dahl. 데이터 병렬성이 신경망 학습에 미치는 영향을 측정합니다. _ Journal of Machine Learning Research_, 20(112):1-49, 2019.
* [SZY\({}^{+}\)22] Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, and Jie Zhou. Welm: 중국어를 위한 잘 읽은 사전 훈련된 언어 모델입니다. _ arXiv preprint arXiv:2209.10372_, 2022.
* [TAB\({}^{+}\)23] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.
* [TDR\({}^{+}\)22] Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 효율적으로 확장: 사전 훈련 및 미세 조정 변압기의 통찰력을 제공합니다. 2022년 _International Conference on Learning Representations_ 에서.

* [TLI\({}^{+}\)23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.
* [VSP\({}^{+}\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 주목해 주십시오. _ 신경 정보 처리 시스템의 발전_, 30, 2017.
* [XPD\({}^{+}\)24] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. 도레미: 데이터 혼합을 최적화하면 언어 모델 사전 훈련이 빨라집니다. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.
* [ZKHB22] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby 및 Lucas Beyer. 스케일링 비전 트랜스포머. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_에서, 페이지 12104-12113, 2022.

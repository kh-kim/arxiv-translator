{
    "2403.03507v1": {
        "paper_id": "2403.03507v1",
        "abs_url": "https://arxiv.org/abs/2403.03507v1",
        "pdf_url": "https://arxiv.org/pdf/2403.03507v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2403.03507v1_GaLore_Memory-Efficient_LLM_Training_by_Gradient_Low-Rank_Projection.pdf",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Jiawei Zhao",
            "Zhenyu Zhang",
            "Beidi Chen",
            "Zhangyang Wang",
            "Anima Anandkumar",
            "Yuandong Tian"
        ],
        "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.",
        "comments": "",
        "official_code_urls": [
            "https://github.com/jiaweizzhao/galore"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/galore-memory-efficient-llm-training-by",
        "bibtex": "@misc{zhao2024galore,\n      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, \n      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},\n      year={2024},\n      eprint={2403.03507},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}"
    }
}
# LLaMA Beyond English: 언어능력 전이에 관한 실증적 연구

Jun Zhao

이 작가들은 동등하게 기여했다.

Zhihao Zhang1

Luhui Gao

Qi Zhang1

Tao Gui

Xuanjing Huang

푸단대학교 컴퓨터학부

{zhaoj19,zhangzhihao19,qz,tgui}@fudan.edu.cn

각주 1 : 교신저자

각주 2 : 교신저자

###### Abstract

최근, 챗GPT에 의해 예시되는 대형 언어 모델(LLM)에서 다양한 복잡한 작업에 걸쳐 놀라운 숙련도를 보여주는 상당한 발전이 목격되었다. 그러나, 많은 주류 LLM들(예를 들어, LLaMA)은 영어-우세 코퍼스에 대해 사전 트레이닝되어, 다른 비영어 언어들에서의 그들의 성능을 제한한다. 본 논문에서는 언어 생성 및 명령어 수행 능력을 비영어 언어로 효과적으로 전달하는 방법에 초점을 맞춘다. 이 질문에 답하기 위해 우리는 1440 GPU 시간이 누적된 LLaMA를 기반으로 광범위한 경험적 조사를 수행한다. 어휘 확장, 추가 사전 훈련, 명령어 튜닝과 같은 핵심 요소가 전이에 미치는 영향을 분석한다. 모델의 지식 수준을 정확하게 평가하기 위해 널리 사용되는 표준화된 테스트 벤치마크인 C-Eval, MMLU, AGI-Eval 및 GAOKAO-Bench를 사용한다. 또한 17개 범주의 수업과제로 구성된 벤치마크인 LLM-Eval을 기반으로 정확성, 유창성, 정보성, 논리적 일관성, 무해성 등의 측면을 고려하여 모델의 응답 품질에 대한 종합적인 평가를 수행한다. 평가 결과, 지식 정렬 및 응답 품질 측면에서 사전 학습 데이터의 \(1\%\) 미만으로 최신 전달 모델과 유사한 성능을 달성할 수 있음을 보여준다. 또한, 13개의 저자원 언어에 걸친 실험 결과도 유사한 경향을 보인다. 우리는 실험에 의해 밝혀진 결론이 커뮤니티가 비영어 LLM을 개발하는 데 도움이 될 것으로 기대한다.

## Introduction

수십 년 동안 자연 언어 처리(NLP)의 연구자들은 지능의 기본 원리를 탐구해 왔다[1]. 최근 대형 언어 모델(LLM)의 발전은 희망의 빛을 드러낸 것으로 보인다. 모델 크기와 훈련 데이터의 전례 없는 규모에서 이점을 얻으려면 ChatGPT[14], PaLM[15], LLaMA[16] 등과 같은 많은 LLM이 추론[17], 계획[18], 경험[19]에서 인간 수준을 능가하거나 능가하는 강력한 능력이 나타났다. 이러한 일반적인 능력들은 또한 LLM들이 전체 UBE(Uniform Bar Examination)[14]를 성공적으로 완료하거나 자연 언어 명령어들에 기초한 코딩(StabilityAI 2023)과 같은 복잡한 실세계 태스크들을 다룰 수 있는 기초를 제공한다.

많은 잘 알려진 LLM은 여러 언어의 다양한 혼합 코퍼스에 대한 사전 훈련 덕분에 다양한 언어에서 입력을 이해하고 응답을 생성할 수 있다. 그러나 언어 자원의 불균형적인 분포로 인해 모든 언어에 대한 광범위한 훈련 데이터를 수집하는 것은 거의 불가능하다[10]. 대표적인 LLM BLOOM[13]을 예로 들면, 46개의 자연어에 대해 사전 훈련되었다. 그러나 이 숫자는 현재 사용 중인 대략적인 \(7,000\) 언어 중 \(0.66\%\)에 불과하다. 더욱이, 이들 46개 언어의 말뭉치 내에서, 고자원 영어 텍스트가 저자원 Chitumbka 언어보다 280만 배 더 많은 극심한 불균형이 존재한다. 이것은 고립된 사례가 아니다. 또 다른 널리 논의된 언어 모델인 LLaMA가 있다.

도 1: (왼쪽에 묘사된 바와 같이) 주로 영어가 우세한 코퍼스에서 트레이닝되는 사전 트레이닝된 LLaMA 모델들은 본질적으로 비영어 언어들을 다루는 데 능숙하지 않다. 우리는 어휘 확장, 추가 사전 훈련 및 명령 튜닝의 필요성과 그들이 능력 전달에 어느 정도 영향을 미치는지 조사하는 것을 목표로 한다. 이 탐사를 통해 우리는 LLaMA의 언어 능력을 (오른쪽에 예시된 바와 같이) 비영어 언어로 효율적으로 이전할 수 있으며, 그 과정에서 비용을 최소화할 수 있다.

라틴어와 키릴어 스크립트를 사용하는 20개 관련 언어의 제한된 데이터로 보충된 주로 영어가 우세한 코퍼스에 대해 사전 교육을 받았다. 결과적으로 LLaMA는 충분한 훈련을 받지 않은 비영어 언어와 관련된 맥락에서 열등한 성능을 보인다. 일부 연구자들은 관심 있는 특정 언어에 대한 대규모 데이터를 수집하고 LLM[20]을 재교육한다. 그러나, 이는 필연적으로 높은 계산 및 데이터 수집 비용을 초래하여, 낮은 자원 언어에 적합하지 않다. Cui, Yang, Yao[21]는 원래 어휘를 확장하고 LoRA[1]의 30B 중국어 토큰으로 LLaMA를 추가로 사전 훈련하여 유망한 결과를 보고한다. 그럼에도 불구하고, 이송 과정에 대한 세밀한 체계적인 조사는 여전히 부족하다.

이 연구에서 우리는 LLM에서 언어 능력 전달에 대한 포괄적인 이해도를 얻기 위한 조치를 취한다. 그림 1과 같이 LLaMA를 기반으로 몇 가지 주요 측면을 경험적으로 조사한다.

(1) **전이에 대한 어휘 확장의 영향** 원본 어휘에 대해 0.5억 중국 토큰을 추가 사전 훈련 하는 것이 확장 된 어휘에 대 한 성능보다 훨씬 더 우수 합니다. 이는 어휘 확장이 수백억 개 정도의 소규모 점진적 사전 훈련에 적합한 선택이 아닐 수 있음을 시사한다.

(2) **효과적인 전송에 필요한 교육 척도** 1,000억 토큰 이하의 중국어 사전 교육이 LLaMA의 지식 수준을 크게 향상시키기에 충분하지 않다는 것을 발견했습니다. 그러나 LLaMA의 응답 품질(즉, 언어 생성 능력)을 향상시키기 위해서는 대규모 추가 사전 훈련보다는 수십만 개의 명령 데이터만 필요하다.

(3) **이전 훈련이 원래 영어 능력에 미치는 영향**. 이전 훈련을 위한 중국 말뭉치에 대한 배타적 의존은 LLaMA의 원래 영어 능력을 현저하게 손상시키며, 다국어 공동 훈련을 통해 우려 사항이 효과적으로 완화된다는 것을 발견했습니다.

앞서 언급한 연구 결과는 LLaMA의 언어 생성 및 지침에 따른 기능을 최소한의 비용으로 비영어 언어로 이전할 수 있게 한다. 표준화된 4개의 테스트 벤치마크(C-Eval, GAOKAOBench, MMLU, AGI-Eval)와 명령어 평가 벤치마크인 LLM-Eval의 평가 결과를 바탕으로 훈련 데이터를 \(1\%\) 미만으로 사용하면서 최신 개방형 중국어 LLaMA와 비교 가능한 지식 수준과 응답 품질을 달성하였다. 또한, 다른 13개의 저자원 언어에 대한 확장 실험도 유사한 경향을 보인다. 본 논문에서는 비영어 LLM을 구성하는 데 있어 커뮤니티에 도움과 지침을 제공하기 위해 실험 결과와 분석을 목표로 한다.

## 배경 및 개요

이 하위 섹션에서는 먼저 지침에 따른 LLM을 개발하기 위한 필수 단계를 제시한다. 그 후, 우리는 이 모델을 비영어 언어로 외삽하는 일반적인 관행을 검토하고 모델 외삽을 위해 수행된 경험적 연구의 개요를 제공한다.

### 1단계: 언어 능력 및 지식 획득 사전 교육

LLM에 대한 기초 능력의 중요한 소스로서, 프리트레이닝은 프리픽스 시퀀스들에 기초하여 다음 토큰을 예측하는 것을 목표로 한다. 형식적으로 큰 말뭉치 \(\mathcal{D}\)가 주어지면 다음과 같은 손실을 최소화하는 것이 훈련 목표입니다.

\[\mathcal{L}_{pretrain}=\sum_{x\in\mathcal{D}}\sum_{i}\log p_{\theta}(x_{i}|x_{1},...,x_{i-1}), \tag{1}\]

여기서, \(x=\{x_{1},...,x_{n}\}\)은 입력 토큰 시퀀스를 나타낸다.

수십억에서 수조 개의 토큰에 이르는 방대한 텍스트 데이터에 대한 사전 훈련을 통해 LLM은 복잡한 언어 구조, 의미 및 문맥 관계를 포착할 수 있어 강력한 언어 생성 능력을 획득할 수 있다. 또한 이러한 LLM은 개념, 사실 및 이들 사이의 연결을 이해하는 방법을 학습하여 세계 지식에 대한 광범위한 이해로 이어진다.

### 2단계: 인간 의도와 정렬하기 위한 명령어 튜닝

지시 튜닝(SFT)은 지시를 따르는 LLM의 능력을 더욱 향상시키는 것을 목표로 한다. 학습 데이터는 많은 명령어-응답 쌍으로 구성된다. 모델은 단지 선행 텍스트로부터 계속되는 것이 아니라 명령어에 정확하게 응답하는 것을 배울 필요가 있다. 형식적으로 명령 데이터 집합 \(\mathcal{D}^{\prime}=\{(I,Y)\}\)이 주어지면 \(I\)은 작업 명령을 나타내고 \(Y\)은 원하는 응답을 나타내므로 명령 튜닝의 학습 목표는 다음과 같은 손실을 최소화하는 것입니다.

\[\mathcal{L}_{ins}=-\log p_{\theta}(Y|I), \tag{2}\]

다양한 지시 작업을 조정함으로써 모델은 인간의 지시를 더 잘 이해하고 따를 수 있으며 보이지 않는 지시로 일반화할 수 있다.

### LLM을 비영어 언어로 외삽

LLM은 사전 훈련 및 명령어 튜닝을 통해 언어 생성 및 명령어 추종 능력을 획득한다. 그러나 영어는 자연어 처리 분야에서 지배적인 위치를 차지하고 있으며, 다양한 도메인의 텍스트 데이터 수집이 가장 풍부하다. 영어가 우세한 코퍼라에서 훈련된 LMM은 다른 비영어 언어에서 열등한 성능을 보인다. LLM을 비영어 언어로 외삽하는 것은 매우 가치 있는 연구 과제를 제기한다. 일반적인 외삽 접근법은 다음 세 단계로 구성된다 : (1) 목표 언어의 토큰을 추가하기 위해 어휘를 확장하고, 따라서 그 언어에 대한 인코딩 표현성을 향상시킨다. (2) LLM들의 언어 생성 능력들을 타겟 언어로 전달하기 위한 추가 사전 트레이닝. 이 단계에 필요한 훈련 규모는 일반적으로 수십억 개의 토큰으로, 처음부터 훈련에 필요한 수조 개의 토큰보다 훨씬 적다. (3) LLM의 명령어-추종 능력을 전달하기 위해 타겟 언어로 SFT를 수행하는 단계.

본 논문은 어휘 확장 전후, 다양한 사전 훈련 및 SFT 척도에서 LLM의 성능 차이를 비교하는 앞서 언급한 세 단계에 대한 포괄적인 경험적 연구를 수행한다. 효과적인 전이를 위해 어휘 확장의 필요성과 필요한 훈련 척도를 분석한다.

## Experimental Setup

본 논문은 비영어권 언어로 언어 생성 및 명령어 수행 능력을 효과적으로 전달하는 방법을 탐구하는 것을 목적으로 한다. 중국어로 이용 가능한 풍부한 언어적 자원을 감안할 때 포괄적이고 심층적인 경험적 연구가 수행될 수 있다. 따라서 실험 및 분석은 중국어를 시작으로 시작되며 관찰된 현상은 10개 이상의 저자원 언어에서 추가로 검증된다. 이 섹션에서는 실험에 사용된 데이터 세트, 모델 및 평가 방법론을 제시한다.

### Models

불필요한 대규모 반복 사전 훈련을 피하기 위해 다양한 규모의 중국 말뭉치에 대해 훈련된 오픈 소스 모델을 사용했다. 이 중 LLaMA와 LLaMA2는 명시적인 중국어 사전 훈련을 거치지 않고 검문소의 역할을 하는 반면, 중국어 LLaMA와 중국어 LLaMA2는 300억 토큰의 중국어 사전 훈련을 통해 검문소로 취급된다. 그 규모는 오픈 차이나 LLaMA의 1,000억 토큰에 달한다. 분석 및 비교를 위해 이러한 모델의 성능을 참조로 사용한다.

**LLaMA**[12]: LLaMA는 공개적으로 사용 가능한 영어 지배 코퍼스에서 훈련 된 메타 AI에 의해 개발 된 일련의 기초 모델입니다. 코퍼스에는 CommonCrawl, C4, Github 코드, 위키피디아, 북스, ArXiv 논문 등이 포함되어 있으며, 약 1조 4천억 토큰에 달한다. 이러한 출처 중 위키피디아는 다국어 텍스트로 구성되어 전체 말뭉치의 4.5%를 기여한다. 라틴어 또는 키릴어 스크립트를 사용하는 20개 언어를 포함합니다. LLaMA는 그 크기의 기초 모델에 대해 최신의 결과를 달성한다. 예를 들어, 130억 개의 파라미터만을 갖는 LLaMA-13B는 많은 NLP 벤치마크에서 훨씬 더 큰 175B 파라미터 GPT-3보다 성능이 우수하다. 우리는 실험에서 LLaMA-7B와 LLaMA-13B를 고려한다.

**LLaMA2**[12]: LLaMA2는 LLaMA의 향상된 업그레이드 버전입니다. 전작에 비해 받은 업그레이드에는 보다 강력한 데이터 정리 프로세스, 40% 크기의 증가를 자랑하는 공개적으로 사용 가능한 사전 훈련 데이터의 새로운 혼합, 이해력 향상을 위한 이중 컨텍스트 길이, 추론의 효율성을 위한 그룹화된 쿼리 주의 구현이 포함된다. 이러한 개선은 고급 언어 이해 작업을 처리하는 데 더 강력한 도구가 됩니다. 우리는 실험에서 LLaMA2-7B를 고려한다.

**중국어 LLaMA**[12]: 중국어 LLaMA는 중국 텍스트를 이해하고 생성하는 기능을 향상시키도록 설계된 원본 LLaMA의 확장입니다. 이 목표는 SentencePiece를 사용하여 개발된 중국어 토큰나이저를 통합함으로써 달성된다. 이 토큰화기는 어휘의 크기가 \(49,953\)으로 한자를 보다 효과적으로 처리할 수 있다. 또한, 모델 트레이닝 동안 메모리 소모를 감소시키기 위해 파라미터-효율적인 미세 조정 기법[13]을 채용한다. 실험에서는 약 300억 개의 중국어 토큰에 해당하는 약 120GB 크기의 코퍼스에서 학습되는 중국어 LLaMA 7B 플러스를 고려한다.

**Chinese LLaMA2**[12]: Chinese LLaMA2는 Chinese LLaMA의 고급 반복입니다. 중국어 LLaMA와 동일한 말뭉치 및 학습 데이터를 활용하되, LLaMA2의 기본 모델을 활용한다. 또한, 새로운 버전의 어휘 구축과 코드 구현도 최적화되었다. 우리의 실험에서 우리는 300억 개의 중국 토큰에 대해 사전 훈련된 중국 LLaMA2 7B를 고려한다.

**Open Chinese LLaMA**[10]: Open Chinese LLaMA는 원본 LLaMA의 대규모 확장 버전입니다. LLaMA의 중국어 텍스트 처리 능력을 향상시키기 위해 오픈 중국어 LLaMA는 1,000억 개의 토큰을 포함하는 코퍼스에 대한 추가 사전 훈련을 거친다. 코퍼스는 인터넷에서 수집된 텍스트와 원본 LLAMA 모델이 사용하는 영어 및 코드 데이터의 하위 집합으로 구성된다.

### Datasets

LLaMA의 언어 능력을 관심 영어가 아닌 언어로 전달하기 위해 BELLE와 Bactrain-X라는 두 가지 교육 데이터 세트를 훈련에 활용한다. 전자는 중국어와 관련된 실험에 사용되고, 후자는 다른 언어와 관련된 실험에 사용된다.

**BELLE**[11]: BELLE는 150만 명령어 추적 예제를 포함하는 Lianjia Tech에서 개발한 대규모 중국 명령어 튜닝 데이터 세트입니다. 복제되고 품질이 낮은 데이터를 제거하여 최종적으로 95만 개의 예를 유지했다.

**Bactrain-X**[12]: Bactrian-X는 다국어 명령 튜닝을 용이하게 하기 위해 52개 언어에 걸쳐 명령 및 응답을 포함 합니다. Alpaca-52k [10] 및 Dolly-15k [12] 데이터 세트의 67K 영어 지침을 51개 언어로 번역한 다음 ChatGPT로 응답을 생성하여 생성한다. 모델의 역량을 객관적이고 종합적으로 평가하기 위해 응답 품질과 지식 수준의 두 가지 관점에서 평가를 수행한다. 전자의 경우 LLM-Eval 벤치마크를 사용하여 다양한 저자원 언어로 번역하여 다국어 평가를 지원한다. 후자의 경우 널리 채택된 표준화된 테스트 벤치마크인 C-Eval, MMLU, AGI-Eval 및 GAOKAO-Bench를 사용한다.

**LLM-Eval**[13]: LLM-Eval은 명령어 추적 평가를 위해 수동으로 구성된 벤치마크입니다. 사실적 질문 응답, 읽기 이해, 프레임 생성, 단락 재작성, 요약, 수학 문제 해결, 추론, 시 생성, 프로그래밍 등 17개 대분류 453개의 수업 과제가 있다.

**C-Eval**[13]: C-Eval은 중학교에서 전문 시험에 이르기까지 52개 과목에 걸쳐 13948개의 시험 문제가 있는 중국어 평가 모음입니다. 여기에는 STEM, 인문학, 사회 과학 및 기타 주제가 포함됩니다. C-Eval HARD는 고급 추론이 필요한 8개의 도전적인 수학 및 과학 과목의 하위 집합이다.

**MMLU**(Hendrycks et al.2020): MMLU는 STEM, 인문학 및 사회 과학을 포함한 57개 다양한 주제에 걸쳐 지식을 학습하고 적용하는 LLM의 능력을 측정합니다. 이 시험은 초등부터 고급 전문가까지 다양한 난이도를 다루고 있습니다.

**AGI-Eval**(Zhong et al.2023): AGIEval은 대학 입시, 로스쿨 입학 시험 및 전문 자격 시험을 포함하여 수백만 명이 수행한 표준화 테스트의 질문을 사용합니다. 영어와 중국어로 모두 19개의 과제가 있습니다.

**Gaokao-Bench**(Zhang et al.2023b): GAOKAO-Bench는 2010-2022년 중국 대학 입시(Gaokao)의 2811개 시험 문제를 모든 과목에 적용합니다. 그것은 수학, 중국어, 영어, 물리학 등에 걸쳐 1781개의 객관식, 218개의 빈칸 채우기, 812개의 개방형 질문을 가지고 있다.

### Evaluation Protocol

LLM-Eval의 경우 Zhang et al.2023a의 관행을 따랐으며 정확성, 유창성, 정보성, 논리성 및 무해성의 5가지 채점 항목을 통해 모델의 응답 품질을 평가했다. 각 측면에 대한 점수는 0에서 3까지의 범위이다. 우리는 부록에 표시된 프롬프트를 사용하여 자동화된 평가를 위해 지침, 모델 응답 및 참조 답변을 GPT-4에 제출한다. Zhang et al.2023a에 의해 보고된 결과에 기초하여, 이 평가 방법은 인간 평가와의 높은 정합성을 입증한다.

4개의 표준화된 테스트 벤치마크에 대해 모델 응답에 대한 정확도 메트릭을 계산한다. 또한 AGI-Eval 및 GAOKAO-Bench에 대해 제로 샷 설정을 사용하고 C-Eval 및 MMLU에 대해 5 샷 설정을 사용하는 일반적인 관행을 따른다.

## Main Results

### 어휘 확장이 전송에 미치는 영향

특정 언어로 LLM의 능력을 향상시키는 것을 목표로 할 때 어휘 확장은 직관적으로 합리적인 접근법이다. 본 절에서는 LLM-Eval 벤치마크를 통해 어휘 확장의 영향을 평가하고 실험 결과를 표 1에 제시하였다. 먼저 인터넷에서 100만 개의 중국어 문장(약 5억 토큰)을 수집하고 어휘 확장 없이 원본 LLaMA를 추가로 사전 훈련했다. 놀랍게도, 이 모델이 1K, 5K 및 950K 명령어 튜닝 설정에 걸쳐 어휘 확장 중국어 LLaMA를 훨씬 능가한다는 것을 발견했다. 이 발견은 중국 LLaMA가 우리의 0.5억 토큰보다 훨씬 더 많은 300억 토큰에 대한 추가 중국 사전 교육을 받았다는 점을 감안할 때 생각할 수 없는 것이다. 또한 950K 설정 내에서 원본 LLaMA에 대한 어휘를 확장하고 동일한 0.5억 토큰으로 학습하여 학습 데이터 불일치의 영향을 완화한 결과를 포함한다. 결과는 일관되게 유지됩니다. 이는 수백억 토큰의 훈련 규모 내에서 어휘 확장이 유리한 선택이 아님을 나타낸다. 다른 문헌 [23]에서 보고된 바와 같이, 우리는 더 큰 규모의 사전 훈련(예: 수조 개의 토큰)을 포함하는 설정에서 어휘 확장의 효과를 부정하지 않지만, 이는 이미 단순한 언어 전달보다 재훈련에 더 기울어진다.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline  & **Method** & ACC. & F. & INFO. & LC. & H. & AVG. \\ \hline \multirow{8}{*}{1k SFT} & LLaMA [21] & 0.482 & 1.194 & 0.858 & 0.614 & 2.970 & 1.224 \\  & LLaMA with \(10K\) pretrain & 0.482 & 1.441 & 0.829 & 0.712 & 2.963 & 1.285 \\  & LLaMA with \(100K\) pretrain & 0.587 & 1.952 & 0.881 & 0.991 & 2.973 & 1.477 \\  & LLaMA with \(1M\) pretrain & 0.735 & 2.071 & 1.002 & 1.046 & 2.957 & 1.562 \\  & Chinese LLaMA [21] & 0.509 & 1.205 & 0.811 & 0.726 & 2.970 & 1.244 \\  & Open Chinese LLaMA [21] & 1.406 & 2.584 & 1.685 & 1.877 & 2.989 & 2.108 \\ \hline \hline \multirow{8}{*}{5k SFT} & LLaMA [21] & 0.450 & 1.279 & 0.767 & 0.612 & 3.000 & 1.199 \\  & LLaMA with \(10K\) pretrain & 0.411 & 1.372 & 0.814 & 0.612 & 2.961 & 1.258 \\  & LLaMA with \(100K\) pretrain & 0.488 & 1.922 & 0.876 & 0.977 & 3.000 & 1.493 \\  & LLaMA with \(1M\) pretrain & 0.682 & 2.085 & 1.039 & 1.008 & 2.969 & 1.623 \\  & Chinese LLaMA [21] & 0.581 & 1.341 & 0.899 & 0.783 & 2.992 & 1.432 \\  & Open Chinese LLaMA [21] & 1.295 & 2.481 & 1.667 & 1.884 & 2.969 & 2.245 \\ \hline \hline \multirow{8}{*}{950k SFT} & LLaMA [21] & 1.783 & 2.767 & 2.142 & 2.212 & 2.993 & 2.379 \\  & LLaMA with \(1M\) pretrain & 1.812 & 2.799 & 2.080 & 2.303 & 3.000 & 2.399 \\  & LLaMA-EXT with \(1M\) pretrain & 1.591 & 2.726 & 1.918 & 2.164 & 2.998 & 2.279 \\ \cline{1-1}  & Chinese LLaMA [21] & 1.808 & 2.795 & 2.112 & 2.313 & 3.000 & 2.406 \\ \cline{1-1}  & Open Chinese LLaMA [21] & 1.890 & 2.858 & 2.189 & 2.390 & 2.993 & 2.464 \\ \cline{1-1}  & LLaMA2 [21] & 1.868 & 2.822 & 2.171 & 2.379 & 3.000 & 2.448 \\ \cline{1-1}  & Chinese LLaMA2 [21] & 1.701 & 2.838 & 2.011 & 2.251 & 3.000 & 2.360 \\ \hline \hline \end{tabular}
\end{table}
표 1: 추가 프리트레이닝 및 명령어 튜닝(SFT)의 상이한 스케일을 갖는 응답 품질. ACC., F., LC., H., INFO., 및 AVG. 각각 정확성, 유창성, 논리적 일관성, 무해성, 정보성 및 평균을 나타낸다. 약 100만 개의 샘플이 약 5억 개의 토큰을 차지합니다. 중국 LLaMA와 중국 오픈 LLaMA에 대한 사전 훈련 규모는 각각 300억 토큰과 1000억 토큰이다.

### 효과적인 전송을 위해 필요한 학습 크기입니다.

훈련 척도는 사전 훈련 척도와 명령어 튜닝 척도 모두로 구성된 LLM 능력의 전달 가능성에 영향을 미치는 또 다른 중요한 요인을 구성한다. 실험 결과는 표 1에 나타나 있다. LLaMA(10K, 100K, 및 1M 추가 프리트레이닝을 포함함) 및 Open Chinese LLaMA의 예를 들어, 추가 Chinese 프리트레이닝의 규모는 0에서 1000억 토큰으로 점진적으로 증가한다. 1K 및 5K 명령어 튜닝 설정에서 추가 사전 훈련의 규모가 증가함에 따라 응답 품질이 점진적으로 향상되는 것을 관찰했다. 그러나 명령어 튜닝 데이터 스케일이 950K로 증가할 때 모델 간에 응답 품질에 큰 차이가 없음을 발견했다. 결과적으로, 우리는 더 많은 사전 훈련이 인간의 지시와 모델의 정렬을 가속화할 수 있지만 수백억 개의 훈련 규모만으로는 모델이 더 많은 양의 세계 지식을 파악할 수 없다고 가정한다. 이는 유사한 반응 수준에서 수렴으로 이어진다. 즉, 응답 품질의 향상은 주로 지식 수준의 향상보다는 언어 생성 능력의 향상에서 비롯된다.

그러나 각주 1: 중국-LLaMA는 어휘 확장이라는 추가 요인으로 인해 예외로 간주된다.

이 관점을 검증하기 위해 널리 사용되는 4개의 표준화된 테스트 벤치마크에 대한 모델의 지식 수준을 평가했다. 그림 2에서 볼 수 있듯이 LLaMA 7B, 중국 LLaMA 7B 및 오픈 중국 LLaMA 7B는 C-eval, goakao-bench 및 agi-eval에서 비교적으로 수행되며, 이는 추가 중국 사전 훈련에 의해 유도된 유의미한 차이가 없음을 나타낸다. 중국어에 대한 추가 사전 교육이 부족함에도 불구하고 LLaMA2-7B와 LLaMA-13B 모두 C-eval, MMLU 및 AGI-Eval에서 Open Chinese LLaMA를 능가하여 조 단위의 사전 교육과 더 큰 모델 크기가 실제로 모델 지식 수준을 향상시키는 효과적인 경로 역할을 할 수 있음을 시사한다.

### 원본 영어 기능에 대 한 방법

중국어 능력 향상이 기존 영어 능력에 영향을 미치는지도 관심의 대상이다. 이 질문을 해결하기 위해 인터넷에서 200,000개의 중국어 샘플을 추가로 수집하고 정제된 웹 데이터 세트[20]에서 무작위로 200,000개의 영어 샘플을 추출했다. 이 샘플들을 사용하여 표 2에 묘사된 바와 같이 서로 다른 규모의 말뭉치에서 훈련된 LLaMA 모델의 영어 복잡도와 중국어 복잡도를 평가한다. 우리의 연구 결과는 추가 사전 훈련 척도가 증가함에 따라 모델의 복잡도는 중국어에서는 꾸준히 감소하지만 영어에서는 특히 증가한다는 것을 보여준다. 이는 하나의 중국어 코퍼스를 통해서만 모델의 역량을 높이는 것은 원래의 영어 능력을 희생시키는 대가를 치르게 된다는 것을 시사한다.

또한 개방형 중국어 LLaMA에 대한 당혹도 평가를 수행하여 중국어와 영어 당혹도가 모두 낮은 것으로 나타났다. 이 결과는 훈련 데이터가 중국어와 영어 콘텐츠를 모두 통합하여 영어 당혹도의 현저한 상승 없이 중국어 당혹도를 감소시킬 수 있다는 점을 감안할 때 놀라운 일이 아니다. 전반적으로, 중국어 코퍼스에 대한 배타적 의존은 LLaMA의 원래 영어 능력을 현저하게 손상시키며, 우려는 다국어 공동 훈련을 통해 효과적으로 완화되었다.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & L(0) & L(10k) & L(100k) & L(1M) & Open \\ \hline
**Chinese** & 10.151 & 8.697 & 6.634 & 5.249 & 3.924 \\
**English** & 14.691 & 15.625 & 29.553 & 198.840 & 15.045 \\ \hline \hline \end{tabular}
\end{table}
표 2: 상이한 추가 사전 훈련 척도를 갖는 모델 복잡성. L은 LLaMA를 나타내며 괄호 안의 숫자는 추가 사전 훈련 샘플의 양을 나타낸다. Open은 Open Chinese LLaMA를 의미한다.

그림 2: 4개의 벤치마크에 대한 지식 수준 평가 결과.

### 분석을 여러 언어로 확장

이전 섹션에서 우리의 실험은 중국어에 초점을 맞춘다. 다른 비영어 언어에서 유사한 결론이 도출될 수 있는지 조사하기 위해 13개의 저자원 언어로 실험을 확장한다. 평가 일관성을 보장하기 위해 LLM-Eval 벤치마크를 이러한 13개 언어로 번역하고 동일한 평가 메트릭을 사용한다. 표 3과 같이 SFT 데이터의 증가에 따라 모든 저자원 언어에 대한 응답 품질이 크게 향상되었다. 이들 언어 중 아랍어, 인도네시아어, 베트남어가 가장 우수한 성능을 보였다. 13개 언어 모두 자원이 부족함에도 불구하고, 이 세 언어는 더 자주 사용된다[23]. 결과적으로 LLaMA는 (영어에 비해 전체 발생이 적지만) 더 자주 만나 모델이 이러한 언어의 지침을 신속하게 이해할 수 있다. 이것은 이전 섹션에서 도출된 결론과 일치한다.

앞 절에서는 어휘의 확장이 언어 전달 가능성에 부정적인 영향을 미치는 것을 관찰하였다. 그럴듯한 가설은 어휘 확장이 방해할 수 있는 LLM 내의 교차 언어 의미 정렬의 존재이다. 이 정렬 가설을 검증하기 위해 1k 지침 데이터 세트로 LLaMA를 미세 조정하고 모델의 출력을 조사한다. 흥미로운 사실은 우리는 코드 전환 샘플의 특정 비율을 관찰했다. 그림 3에 묘사된 바와 같이, 이러한 샘플의 모델 응답은 여러 언어의 토큰으로 구성되며 의미론적으로 일관성이 있다. 우리는 중국어가 목표 언어일 때 전송 과정뿐만 아니라 다른 13개의 저자원 언어가 목표 언어일 때도 코드 전환이 발생하는 것을 관찰했다. 그림 4와 같이 코드 전환이 있는 표본의 비율은 대략 \(2\%\)에서 \(5\%\) 사이이다. 이는 LLaMA가 사전 훈련 과정에서 개념 간의 교차 언어 정렬 관계를 학습했을 수 있음을 나타낸다.

그림 3: 코드 전환 사례 연구 빨간색 배경이 있는 텍스트는 비영어 대상 언어(중국어)를 나타냅니다. 시안 배경이 있는 텍스트는 모델의 출력에서 코드 전환 언어를 나타내며, 이는 영어, 일본어, 러시아어 또는 기타 언어일 수 있습니다.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Language**} & \multicolumn{8}{c}{1k SFT} & \multicolumn{8}{c}{65k SFT} \\ \cline{2-13}  & ACC. & F. & INFO. & LC. & H. & AVG. & ACC. & F. & INFO. & LC. & H. & AVG. \\ \hline Arbic & 0.188 & 1.061 & 0.191 & 0.254 & 3.000 & 0.939 & 1.268 & 2.499 & 1.529 & 1.607 & 3.000 & 1.981 \\ Bengali & 0.046 & 0.492 & 0.050 & 0.041 & 3.000 & 0.726 & 0.959 & 2.257 & 1.156 & 1.189 & 3.000 & 1.712 \\ Gujarati & 0.061 & 0.426 & 0.052 & 0.063 & 2.998 & 0.720 & 0.683 & 1.795 & 0.875 & 0.790 & 2.995 & 1.428 \\ Hindi & 0.131 & 1.064 & 0.147 & 0.162 & 3.000 & 0.901 & 1.014 & 2.342 & 1.238 & 1.240 & 2.998 & 1.766 \\ Indonesian & 0.398 & 1.266 & 0.544 & 0.438 & 2.995 & 1.128 & 1.659 & 2.751 & 0.226 & 2.012 & 3.000 & 2.290 \\ Malayalam & 0.101 & 0.621 & 0.103 & 0.103 & 3.000 & 0.786 & 0.906 & 2.427 & 1.182 & 1.197 & 3.000 & 1.742 \\ Marathi & 0.095 & 0.781 & 0.107 & 0.117 & 2.998 & 0.820 & 1.038 & 2.476 & 1.288 & 1.364 & 2.998 & 1.833 \\ Nepali & 0.151 & 0.991 & 0.177 & 0.146 & 2.986 & 0.890 & 0.969 & 2.417 & 1.236 & 1.285 & 3.000 & 1.781 \\ Swahili & 0.083 & 0.712 & 0.090 & 0.086 & 2.998 & 0.794 & 1.569 & 2.707 & 1.955 & 1.907 & 3.000 & 2.228 \\ Tamil & 0.140 & 0.914 & 0.176 & 0.174 & 2.998 & 0.880 & 0.960 & 2.457 & 1.198 & 1.257 & 2.998 & 1.774 \\ Telugu & 0.054 & 0.560 & 0.057 & 0.090 & 3.000 & 0.752 & 0.539 & 1.735 & 0.674 & 0.712 & 3.000 & 1.332 \\ Urdu & 0.057 & 0.573 & 0.052 & 0.071 & 3.000 & 0.751 & 1.038 & 2.443 & 1.285 & 1.335 & 3.000 & 1.820 \\ Vietnamese & 0.105 & 0.623 & 0.126 & 0.117 & 3.000 & 0.794 & 1.361 & 2.595 & 1.665 & 1.710 & 3.000 & 2.066 \\ \hline \hline Average & 0.124 & 0.776 & 0.144 & 0.143 & 2.998 & 0.837 & 1.074 & 2.377 & 1.331 & 1.354 & 2.999 & 1.827 \\ \hline \hline \end{tabular}
\end{table}
표 3: LLM-Eval에 대한 13개의 저자원 언어에 대한 모델 응답 품질의 평가 결과. ACC., F., LC., H., INFO., 및 AVG. 각각 정확성, 유창성, 논리적 일관성, 무해성, 정보성 및 평균을 나타낸다.

## Related Work

### LLMs의 리소스 간격

LLM의 주요 과제 중 하나는 주로 영어 코퍼스에서 사전 훈련되고 다른 언어의 데이터에 대한 접근이 제한적이기 때문에 자원 격차이다. 영어는 다양한 도메인으로부터의 가장 원시 텍스트 데이터를 갖는 극히 높은 리소스 언어로서 NLP의 분야를 지배하며, Joshi 등(2020) 분야에 표현된 세계의 7000개 이상의 언어 중 몇 개를 남긴다. 이렇게 하면 언어 모델이 다른 언어를 처리할 수 있는 능력에 차이가 발생합니다. 이전 연구 결과에 따르면 LLM은 특히 저자원 언어인 Nguyen et al.(2023), Zhu et al.(2023), Huang et al.(2023)에서 비영어 텍스트를 이해하고 생성하는 데 어려움을 겪고 있다. 자원 격차를 해결하기 위해 연구자와 실무자에 의해 몇 가지 솔루션이 제안되거나 구현되었다. 하나의 가능한 해결책은 다양한 언어 및 분야로부터 이용가능한 데이터의 양을 증가시키고, LLMs Lin et al.(2022); Chen et al.(2022); Cahyawijaya et al.(2023)을 사전 트레이닝하고 평가하기 위해 접근가능하게 하는 것이다. 그러나 이러한 접근 방식은 상당한 계산 비용을 초래하고 자원 격차가 지속된다. 대안적으로, mBERT Devlin 등(2019) 및 XLM-R Conneau 등(2020)과 같이 동시에 상이한 언어로부터의 텍스트에 대해 훈련된 다국어 언어 모델이 갭을 효과적으로 브리지하기 위해 도입되었다.

### Cross-Lingual Transfer

다국어 언어 모델은 Wu and Dredze (2019); Pires et al. (2019); Winata et al. (2021). 이는 한 언어의 지도 데이터에서 언어 능력을 획득하고 추가 학습 데이터 없이 또는 거의 없이 다른 언어에 적용할 수 있음을 의미한다. 강한 교차 언어 성능의 이면에 있는 메커니즘은 연구자들에 의해 조사되었다. 다국어 언어 모델은 임의의 언어 Artetxe 등(2020); Chi 등(2020); Conneau 등(2020)에 적용 가능한 보편적인 규칙을 추론한 것으로 나타났다. mBERT Devlin 등(2019)과 같은 다국어 다국어 언어 모델이 여러 언어에 걸쳐 공유된 하위 단어 어휘 및 공동 사전 훈련에 의존한다는 공통 가설과 달리, Pires 등(2019); Cao 등(2020); Wu 및 Dredze(2019), 연구자들은 모델에 대한 새로운 이해를 개발했으며, 모델의 보편적 의미 추상화 Artetxe 등(2020); Chi 등(2020)을 학습할 수 있는 능력을 강조했다. 언어 간 성능에 영향을 미치는 요인에 대해 연구자들은 전달 가능성을 Conneau et al. (2020); Dufter and Schutze (2020); Wu et al. (2022) 및 언어 거리 Conneau et al. (2020); Eronen et al. (2023)과 연관시켰다. 여기에서 우리는 다른 측면의 결과를 제시하는 새로운 LLaMA 기반 실험과 함께 언어 모델의 교차 언어 전달 가능성을 추가로 조사한다.

### Code-Switching

코드 전환은 단일 발화 내에서 다국어 화자가 언어 사이를 전환하는 현상이다. 코드 전환 작업에 대한 다국어 언어 모델의 성능에 대한 이전 연구에서는 혼합된 결과를 보여주었다. 일부 연구에서는 특정 코드 전환 시나리오에 대해 미세 조정된 사전 훈련된 모델이 영어-스페인어 및 영어-힌디 카누자 등(2020)과 같은 특정 언어 쌍에 대해 최첨단 성능을 달성할 수 있다고 제안한 반면, 다른 연구에서는 메타 임베딩을 사용하면 더 적은 매개변수 Winata 등(2019); Winata 등(2019, 2021)으로 더 나은 결과를 얻을 수 있다는 것을 발견했다. 또 다른 연구 라인에서는 다국어 언어 모델인 Jiang et al. (2020); Tan and Joty (2021); Krishnan et al. (2021)의 성능을 향상시키기 위해 코드 전환 기반 방법이 제시되었다.

## Conclusions

본 논문에서는 언어 생성 및 명령어 수행 능력을 비영어 언어로 효과적으로 전달하는 방법에 초점을 맞춘다. 구체적으로 효과적인 전이를 위해 어휘 확장의 필요성과 필요한 훈련 척도를 분석하기 위해 포괄적인 실증 연구를 수행한다. 이를 통해 어휘 확장이 필요하며, 추가 사전 학습 데이터를 \(1\%\) 미만으로 최신 모델과 유사한 전송 성능을 얻을 수 있음을 알 수 있다. 또한 전이 훈련 중 코드 전환 사례를 관찰하여 교차 언어 정렬이 모델 내에 내재화되었을 수 있음을 시사한다. 13개의 저자원 언어에 대한 확장 실험에서 유사한 결과가 관찰된다. 우리의 분석 및 결과는 비영어 LLM을 개발하는 데 있어 지역사회에 도움과 지침을 제공한다.

그림 4: 언어 간 코드 전환율입니다.

[MISSING_PAGE_FAIL:8]

Krishnan, J.; Anastasopoulos, A.; Purohit, H.; and Rangwala, H. 2021. Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling. arXiv:2103.07792
* Li 등(2023) Li, H.; Koto, F.; Wu, M.; Aji, A. F., and Baldwin, T. 2023. Bactrian-X : Low-Rank Adaptation을 갖는 다국어 복제 가능 명령어-추종 모델. 2305.15011
* Lin 등(2022) Lin, X. V.; Mihaylov, T.; Artetxe, M.; 왕태 진석 Simig, D.; Ott, M.; 금속, N.; 보세일 두, J.; 파수누루, R.; 슬레이퍼 Koura, P. S.; Chaudhary, V.; O'Horo, B.; Wang, J.; Zettlemoyer, L.; Kozareva, Z. Diab, M.; Stoyanov, V. 및 Li, X. 2022. Few-shot Learning with Multilingual Language Models. arXiv:2112.10668.
* Nguyen et al. (2023) Nguyen, X. -P.; Aljunied, S. M.; Joty, S.; 및 Bing, L. 2023. Democratizing LLMs for Low-Resource Languages with Linguistically-Diverse Prompts. 2306.11372
* OpenAI(2022) OpenAI. 2022. ChatGPT를 소개한다.
* OpenLMLab (2023) OpenLMLab. 2023. Open-Chinese-LLaMA.
* Penedo et al.(2023) Penedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cappelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. arXiv:2306.01116.
* Pires et al.(2019) Pires, T.; Schlinger, E. and Garrette, D. 2019. How multi languageual BERT? _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 4996-5001. Florence, Italy: Association for Computational Linguistics.
* Ranta and Goutte (2021) Ranta, A.; and Goutte, C. 2021. Linguistic Diversity in Natural Language Processing. _ Traitement Automatique des Langues_, 62(3): 7-11.
* Scao 등(2023) Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilic, S.; Hesslow, D.; and Castagne, R. 2023. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv:2211.05100.
* StabilityAI(2023) StabilityAI. 2023. Annonuncing StableCode.
* Tan and Joty (2021) Tan, S. Joty, S. 2021. Code-Mixing on Sesame Street: Dawn of the Adversarial Polydots. arXiv:2103.09593.
* Taori 등(2023) Taori, R.; 굴라자니, I.; 장, T.; Dubois, Y. Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca: A Strong, Replicable Instruction-Following Model.
* Team (2023a) Team, I. 2023a. Interlm: 점진적으로 향상된 기능을 갖춘 다국어 언어 모델입니다.
* Team (2023b) Team, I. 2023b. InternLM: Progressively Enhanced Capabilities를 가진 다국어 언어 모델. [https://github.com/InternLM/InternLM-techreport] (https://github.com/InternLM/InternLM-techreport).
* Touvron et al. (2023a) Touvron, H.; Lavril, T.; 이자카드, G.; 마티넷, X.; 라코 -A.; 라크루아, T.; Roziere, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: 개방적이고 효율적인 기초 언어 모델. ARXiv:2302.13971
* Touvron 등(2023b) Touvron, H.; Martin, L.; 스톤, K.; Albert, P. and Almahairi, A. 2023b. 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. 2307.09288
* Winata et al. (2021a) Winata, G. I.; Cahyawijaya, S.; Liu, Z.; 린, Z.; Madotto, A. and Fung, P. 2021a. 코드 전환에서 다국어 모델이 효과적인가요? 2103.13309
* Winata, Lin, and Fung (2019) Winata, G. I.; Lin, Z.; and Fung, P. 2019. Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition. _Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)_, 181-186. Florence, Italy: Association for Computational Linguistics.
* Winata et al. (2019) Winata, G. I.; Lin, Z.; 신재진 and Fung, P. 2019. Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 3541-3547. Hong Kong, China: Association for Computational Linguistics.
* Winata et al. (2021b) Winata, G. I.; Madotto, A.; Lin, Z.; 류, R.; Yosinski, J. and Fung, P. 2021b. 언어 모델은 적은 수의 다국어 학습자입니다. _Proceedings of the 1st Workshop on Multilingual Representation Learning_, 1-15. Punta Cana, Dominican Republic: Association for Computational Linguistics.
* Wu and Dredze (2019) Wu, S.; 및 Dredze, M. 2019. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 833-844. Hong Kong, China: Association for Computational Linguistics.
* Wu, Papadimitriou, and Tamkin(2022) Wu, Z.; Papadimitriou, I. and Tamkin, A. 2022. OOLOong: Investigating What Makes crosslingual Transfer with Controlled Studies. arXiv:2202.12312.
* Zhang et al.(2023a) Zhang, M.; Zhang, Q.; 장용; Gui, T. 2023a. LLIMEVAL-1 중국어 대언어 모델 평가 1단계
* Zhang 등(2023b) Zhang, X.; Li, C.; Zong, Y.; 잉, Z.; He, L. 그리고 Qiu, X. 2023b. GAOKAO 벤치마크에서 대용량 언어 모델의 성능 평가 2305.12474
* Zhong et al.(2023) Zhong, W.; 추이 곽영 양영 루, S. 왕영 Saied, A.; Chen, W.; Duan, N. 2023. AGIEval: 기초 모델 평가를 위한 인간 중심 벤치마크. 2304.06364
* Zhu 등(2023) Zhu, W.; 류, H.; 동, Q.; 황상욱 공룡 Chen, J.; 및 Li, L. 2023. Large Language Model을 이용한 다국어 기계번역: 경험적 결과 및 분석. arXiv:2304.04675.

## Appendix

**LLM-Eval 프롬프트**

특정 교과와 관련된 질문의 경우 0/1/2/3의 척도를 사용하여 다음 LLM 출력을 채점한다.

정확도: 정확한 정보로 정답이 정확하고 정확한지 여부를 평가합니다. 질 높은 답변은 사실적으로 신뢰할 수 있어야 합니다.

유창성: 일관된 표현과 명확한 표현으로 대답이 인간의 언어 규칙을 얼마나 잘 준수하는지 평가합니다. 질 높은 답은 난해하거나 구문 분석하기 어려운 문장 없이 이해하기 쉬워야 한다.

정보성: 답변이 실용적인 가치의 관련 정보를 충분히 제공하면 판단합니다. 질 높은 답변은 질문자에게 유용하고 적절한 정보를 제공해야 한다.

논리성: 답안이 논리적으로 건전하고 관점과 논증을 제시하는 데 정확한지 검토한다. 질 높은 답은 논리적인 원리를 따라야 하며 명확한 추론과 일관성을 보여야 한다.

무해함: 대답이 윤리적 규범을 준수하면서 비윤리적 또는 유해한 내용을 회피하는지 여부를 평가합니다. 질 높은 답변은 유해하거나 비도덕적인 정보의 전파를 피하면서 윤리적 원칙을 지켜야 한다.

참고: 모형이 응답을 제공하지 않는 경우 "무해성"을 제외한 모든 점수는 0이어야 합니다.

질문은: 질문 LLM 응답은: 응답

이 질문에 대한 참조 답변은 다음과 같습니다. 참조 답변 LLM 응답의 "정확성", "유창성", "정보성", "논리성", "무해성"에 대해 인식된 점수를 0/1/2/3의 척도로 할당하는 다음 형식으로 답변을 제공하십시오.

"Accuracy": LLM response의 정확도(integer),

"fluency": LLM response's fluency(integer),

"Informativeness": LLM response's informativeness (integer),

"Logicality": LLM response's logicality (integer),

"Harmlessness": LLM 응답의 무해성(정수)에 대한 점수입니다.

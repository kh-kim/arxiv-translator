# Physics of Language Models: Part 3.3,

지식용량 척도법

Zeyuan Allen-Zhu

zeyuanallenzhu@meta.com

메타/FAIR 연구실

Yuanzhi Li

Yuanzhi.Li@mbzuai.ac.ae

모하메드 빈 자예드 AI대학

2024년 3월 14일 메타 내부 검토를 위해 제출되었습니다. 많은 도움이 되는 대화들에 대해 린샤오와 장유첸에게 감사드립니다. W&B의 이안 클라크, 구랍 데, 안몰 만, 맥스 파이퍼뿐만 아니라 메타 페어 넥스트시스의 루카 베르톤치니, 랴오 후, 케일럽 호, 아포스토로스 코콜리스, 슈보 센굽타, 메타 클라우드 재단의 헨리 에스텔라, 윌 존슨, 리즈완 하쉬미, 루카스 노아에게 특별한 감사를 표하고 싶습니다.

Introduction

큰 언어 모델의 스케일링 법칙은 연구의 중추적인 영역으로 남아 있으며, 작은 언어 모델에 대한 실험을 통해 매우 큰 모델의 성능에 대한 예측을 가능하게 한다. 훈련 시간 측면에서, 확립된 스케일링 법칙 [1, 13, 14, 16, 21]은 최적의 훈련 플롭 대 모델 크기에 대해 논의한다. 그러나 최근 연구[12, 24, 25]는 이러한 법률에 도전하여 훨씬 더 많은 플롭으로 더 작은 모델을 훈련하면 우수한 결과를 얻을 수 있음을 보여준다. 이러한 법칙은 특정 크기의 모델을 훈련하는 데 얼마나 많은 시간/데이터가 필요한지 이야기하지만 또 다른 근본적인 질문은 _충분한 훈련을 가정할 때 모델이 달성할 수 있는 궁극적인 성능은 무엇입니까_ 이다. 대규모 모델에서 알려진 새로운 행동에도 불구하고[8, 34], 모델 크기가 적절하게 훈련되었을 때 용량에 미치는 영향에 대한 원리적이고 정량적인 분석이 부족하다.1

각주 1: 벤치마크 태스크에서 사전 훈련된 모델이 어떻게 수행되는지를 비교하는 풍부한 문헌이 있다. 대부분의 비교는 서로 다른 데이터에 대해 훈련된 서로 다른 모델 패밀리에 대한 것입니다. LLaMA-70B가 미스트랄-7B보다 나은 경우 이득은 사전 훈련 데이터의 선택 또는 아키텍처 차이 또는 모델의 실제 크기에서 비롯됩니까? LLaMA-70B 점수는 세계 지식 벤치마크에서 63.6%인 반면 LLaMA-7B 점수는 48.9%[33]인 것과 같은 일부 비교는 동일한 아키텍처 사이에 있으며, 이는 모델 크기를 10배 증가시키면 용량이 130% = 63.6/48.9로 증가한다는 것을 의미합니까? 따라서 통제된 환경에서 스케일링 법칙을 연구하기 위해 보다 원칙적인 프레임워크를 사용하는 것이 매우 중요하다.

과모수화에 대한 전통적인 이론은 충분히 훈련된 모델에서 모델 크기를 스케일링하는 것이 훈련 데이터의 암기를 향상시킬 수 있고[6], 일반화 오류를 개선할 수 있으며[15, 27, 28], 복잡한 목표 함수를 더 잘 적합시킬 수 있음을 시사한다[5, 23]. 그러나 이러한 결과는 종종 큰 상수 또는 다항식 요인을 간과하여 실제 결과와 상당한 불일치를 초래한다.

본 논문에서는 모델 크기 대 지식 저장 용량에 관한 매우 정확한 스케일링 법칙을 조사하기 위한 원칙적인 프레임워크를 소개한다. 더 큰 언어 모델이 더 많은 지식을 저장할 수 있다는 것은 직관적이지만, 총 지식 척도는 모델의 크기에 선형적으로 영향을 미치는가? 이 크기 조정의 **정확한 상수** 는 무엇인가요? 이 상수를 이해하는 것은 지식 저장소에서 변압기 모델의 효율성을 평가하고 다양한 요인(예: 아키텍처, 양자화, 훈련 기간 등)이 이 용량에 어떻게 영향을 미치는지 평가하는 데 중요하다.

지식은 인간 지능의 중추적인 구성 요소는 아니더라도 우리의 광범위한 역사에 걸쳐 축적되어 있다. GPT-4와 같은 대규모 언어 모델은 정교한 논리뿐만 아니라 뛰어난 지식 기반으로도 유명하다. GPT-4가 1T 이상의 매개 변수를 가지고 있다는 소문에도 불구하고 모든 인간의 지식을 저장할 필요가 있습니까?_ 10B 모델이 고품질 데이터로 충분히 훈련되면 GPT-4의 지식 용량과 일치할 수 있습니까? 우리 논문은 이러한 질문들을 해결하기 위해 노력한다.

**지식 조각** "인간 지식의 한 조각"을 정확하게 정의하는 것은 어렵습니다. 이 논문은 제한적이지만 충분히 흥미로운 영역에 초점을 맞추어 진전을 이루는 것을 목표로 한다. 우리는 지식의 _조각_ 을 (이름, 속성, 값) 튜플(예: (Anya Forger, 생일, 10/2/1996)로 정의합니다. 그리고 세계 지식 벤치마크의 많은 데이터는 다음과 같이 조각으로 분해할 수 있습니다.2

각주 2: 예에는 트리비아QA[20]의 (아프리카, 최대 국가, 수단) 및 (It Happened One Night, director, Frank Capra) 또는 (Teton Dam, collapse date, 06/05/1976) 및 (USA, Capital, Washington D.C.) 내츄럴퀘스트[22]의 예가 포함된다.

지식 베이스에서 (이름, 속성, 값) 튜플을 무작위로 생성하고 영어 설명으로 변환하여 _합성_ 지식 전용 데이터 세트를 생성합니다. 우리는 이러한 텍스트에 대해 무작위 초기화의 표준 자동 회귀 목표를 사용하여 언어 모델(예: GPT-2, LLaMA, 미스트랄)을 사전 훈련하고 학습된 지식을 "추정"한다. 지식 조각의 수와 모델 크기를 변화시킴으로써, 우리는 지식 용량 스케일링 법칙을 개요화한다.

이와는 대조적으로, 실제 지식을 정량화하는 것은 어렵습니다. 예를 들어, LLaMA-70B가 벤치마크에서 LLaMA-7B보다 30% 더 우수하다면, 10배 모델 스케일링이 반드시 용량을 30%만 증가시키는 것을 의미하지는 않습니다(각주 1 참조). 또한 합성 설정을 사용하면 다양한 하이퍼파라미터를 조정할 수 있습니다.

[MISSING_PAGE_FAIL:3]

* 결과 5: 1000 노출 설정에서 2bit/param 용량 비율은 **범용 규칙**: MLP 계층이 없는 경우에도 모든 모델이 이 비율을 밀접하게 달성합니다.
* 결과 6: 100회의 노출에서 일부 아치는 한계를 나타내며, 특히 LLaMA/Mistral의 용량 비율은 가장 잘 조정된 학습 비율 후에도 GPT2보다 1.3배 낮다.
* 결과 7: 추가 제어된 실험은 "게이트 MLP" 사용이 지식 저장소에서 LLaMA/Mistral 아키텍처의 성능 저하로 이어진다는 것을 나타냅니다.

_비고 1.4_: **프레임워크는 모델을 비교할 수 있는 원칙적인 놀이터를 제공합니다.* * 손실/복잡성을 기반으로 하는 기존 비교와 대조되며, 이는 논쟁의 여지가 있는 결론을 생성할 수 있습니다.6 제어된 데이터도 모델 간에 더 큰 차이를 보여줍니다.7 각주 6: 모델은 더 간단한 데이터에 대해 _훨씬 더 나은_ 을 수행하지만 복잡한 데이터에 대해서는 약간 더 열악하거나 추론 작업에 탁월하지만 지식 스토리지에서는 그렇지 않음으로써 더 나은 복잡성을 달성할 수 있습니다. 우리의 결과는 더 미묘한 견해를 제공한다: GatedMLP는 자주 접하는 지식(1000개 노출)에는 영향을 미치지 않지만 적당히 드문 지식(100개 노출)에는 영향을 미친다.
* 섹션 8: 양자화가 모델 용량에 영향을 미치는 방법. 우리는 기본 스케일링 법칙에서 int8 또는 int4로 모델을 양자화하기 위해 GPTQ[10]을 적용했다. 놀랍게도,
* 결과 8: int8로 양자화하는 것은 모델 용량을 손상시키지 않습니다(2bit/param의 경계에 있는 모델에 대해서도). 그러나 int4로 양자화하는 것은 용량을 0.7bit/param으로 줄입니다.

_Remark 1.5_: int8은 8비트이므로, LLMs는 지식을 저장하기 위한 이론적 한계치의 1/4을 초과할 수 있으며, 따라서 지식은 모든 계층에 걸쳐 모델 내부에 매우 컴팩트하게 저장되어야 한다.

_Remark 1.6_: 충분한 트레이닝 후에 2bit/param이 획득되기 때문에, 트레이닝을 더 길게 _하면 모델 용량을 더 향상시키지 못할 수 있지만, _양자화는 할 수 있다_. 이 논문에서 다루지 않았지만, 우리의 프레임워크는 또한 다양한 양자화 방법을 비교하기 위한 원칙적인 놀이터를 제공한다.
* 섹션 9: 희소성(MoE)이 모델 용량에 영향을 미치는 방법. Mixture-of-experts (MoE) 모델은 조밀한 모델보다 더 빠른 추론을 제공하지만, 종종 동일한 총 파라미터 카운트(유효한 파라미터가 아님)를 갖는 조밀한 모델들의 성능을 저하시킨다. 우리는 이러한 성능 저하가 지식 저장 능력의 부족으로 인한 것이 아닐 가능성이 있음을 보여준다.
* 결과 9: MoE 모델은 32명의 전문가가 있더라도 추론 중 전체 매개 변수의 8.8%만 사용하더라도 기본 크기 조정 법칙에 비해 용량이 1.3배만 감소합니다.
* 섹션 10: 정크 지식이 모델 용량에 영향을 미치는 방법. 모든 사전 훈련 데이터가 똑같이 유용한 것은 아니다. 인터넷 데이터의 대부분은 학습 언어 모델에 대한 귀중한 지식이 부족한 반면[24], 위키피디아와 같은 지식이 풍부한 출처는 학습 토큰의 작은 부분만을 나타낸다. 유용한 데이터와 "정크" 데이터를 모두 사용하여 통제된 실험을 수행하여 모델 용량에 미치는 영향을 탐구한다.
* 결과 10+11: 정크 데이터는 모델 용량을 크게 줄입니다. 예를 들어 "유용한/정크" 훈련 토큰의 비율이 1:7인 경우 유용한 지식이 100번 노출된 경우에도 유용한 지식의 용량은 20_x만큼 손실됩니다.8 각주 8: 손실 계수는 300/600/1000번의 유용한 지식 노출을 통해 3x/1.5x/1.3x로 향상되며, 이는 100번의 노출에 대해서만 정크 없이 훈련을 수행하는 결과 4와 비교됩니다.
결과 12] _효과적인 완화_ 는 모든 유용한 지식에 특수 토큰을 준비 하는 것입니다. 이것은 모든 위키피디아 문단의 시작에 wikipedia.org와 같은 도메인 이름을 추가하는 것과 유사하며, 모델은 _자동으로_ 귀중한 도메인에 대한 사전 지식 없이 고품질 데이터를 식별합니다. 상기 예에서, 손실 계수는 20x에서 2x로 개선된다.

전반적으로 지식 용량 확장 법칙을 연구하는 우리의 접근법은 실제 벤치마크에 대해 인터넷 데이터에서 훈련된 언어 모델을 평가하는 기존 방법에 비해 유연하고 **보다 정확한 놀이터** 를 제공한다. 이 정확도는 부분적으로 실제 벤치마크 결과의 유효성을 손상시킬 수 있는 벤치마크 오염에 대한 우려를 제거하는 데이터 세트의 합성 특성 때문이다. 본 논문에서는 다양한 모델 아키텍처와 지식 유형에 대한 철저한 비교를 수행했다. 다양한 양자화 방법을 탐구하지는 않았지만 이는 향후 연구를 위한 유망한 방향을 나타낸다. 또한 정크 데이터의 영향을 조사하고 완화 전략을 제안했습니다. 우리는 이 원칙적인 탐색에서 얻은 통찰력이 실무자들이 모델 선택, 훈련 데이터 준비 및 LLMs에 대한 추가 이론적 연구에 대한 정보에 입각한 결정을 내리는 데 도움이 될 수 있다고 믿는다.

## 2 Preliminaries

본 논문에서 지식이란 (이름, 속성, 값) \(=(n,a,v)\)의 세 문자열로 이루어진 튜플이다. 예를 들어, \(n=\) "Anya", \(a=\) "birthday", \(v=\) "Oct 2, 1996).

### 지식(이론 설정)

지식 집합의 복잡도는 지식 조각의 수뿐만 아니라 값 문자열의 길이 \(v\), 어휘의 다양성 및 기타 요인에 의해 결정된다. 예를 들어 속성 \(a=\) "여권 번호"인 경우 값 \(v\)에는 \(a=\) "성별"에 비해 더 많은 지식 비트가 포함되어 있습니다. 속성 \(a=\) "생년월일"이면 값 \(v\)는 \(3\)_chunks_: \((10,2,1996)\)로 구성될 수 있습니다.

이러한 예들을 고려하여, 우리는 지식의 복잡성에 영향을 미칠 수 있는 하이퍼파라미터들의 세트를 제안한다:

1. \(N\) -- (구분) 이름 수 \(n\), \(\mathcal{N}\)로 표시됨.
2. \(K\) -- the number of attributes \(a\), with \(\mathcal{A}\) representing the set of attributes. 단순화를 위해 \(|\mathcal{A}|=K\)가 고정되어 있다고 가정한다.
3. \(T\) --token 수 \(T\), 여기서 \(v\)의 모든 문자는 일부 \(|\mathcal{T}|=T\)에 대해 \(\mathcal{T}\)에 속합니다. 예를 들어, \(T\)를 토큰화기에서 "vocab size"라고 생각할 수 있습니다.
4. \(C\) 및 \(L\) - 값에 대한 각 청크의 수와 길이: 각 값 \(v\in(\mathcal{T}^{L})^{C}\)은 \(v=(v_{1},v_{2},\cdots,v_{C})\), 여기서 \(v_{i}\in\mathcal{T}^{L}\)로 나타낼 수 있다.
5. \(D\) -- 청크의 다양성: 각 지식 조각에 대해 \((n,a,v)\) 및 \(i\in[C]\)에 대해, 청크 \(v_{i}\)는 \(\mathcal{D}_{a}\subset\mathcal{T}^{L}\)에 속하고, 심성이 있는 일부 집합에 대해 \(D\stackrel{{\text{def}}}}{{=}}|\mathcal{D}_{a}|\ll T^{L}\).

_Remark 2.1_: 표기 단순화를 위해 속성 \(a\in\mathcal{A}\) 내의 모든 청크들이 동일한 다이버시티 집합 \(\mathcal{D}_{a}\)을 공유하고, 모든 청크들이 동일한 길이 등이라고 가정하였다. 이를 통해 각 하이퍼파라미터가 모델의 용량에 미치는 영향을 보다 쉽게 입증할 수 있다. 실제로, 상이한 속성들은 상이한 다양성 세트들 또는 값 길이들을 가질 수 있다 - 예를 들어, \(\mathcal{D}_{\text{passport}}\)는 \(\mathcal{D}_{\text{gender}}\)보다 훨씬 클 수 있다. 우리의 이론적 결과는 더 복잡한 표기법이긴 하지만 이러한 설정에 적용된다.

이론적 결과에서는 다음과 같이 정의된 데이터셋 \(\mathsf{bioD}(N,K,C,D,L,T)\)을 소개한다.

**정의 2.2** (bioD 데이터 생성): _집합 \(\mathcal{A}=\left\{\text{``ID 1 ''\ldots `ID K''}\right\}\) 및 후보 이름( \(N_{0}\stackrel{{\text{\tiny def}}}}{{=}}|\mathcal{N}_{0}|\gg N\)의 고정 집합 \(\mathcal{N}_{0}\)과 같은 \(K\) 속성의 고정 집합을 고려 합니다._

1. _생성_ \(N\) _이름_ \(\mathcal{N}_{0}\) _형식_ \(\mathcal{N}\)__ 에서 랜덤(대체 없이) 균일하게 이름 생성
2. _각 속성_ \(a\in\mathcal{A}\)_에 대해, generate_ \(D\) _distinct strings_ \(w_{1,a},\cdots,w_{D,a}\in\mathcal{T}^{L}\) _균일하게 랜덤(교체 없이)하여 다이버시티 집합_ \(\mathcal{D}_{a}\)_를 형성한다._
3. _각 이름_ \(n\in\mathcal{N}\) _ 및 속성_ \(a\in\mathcal{A}\)_ 에 대해 값_ \(v^{\star}(n,a)=(v_{1},v_{2},\cdots,v_{C})\) _를 무작위로 균등하게 샘플링하여 생성합니다._

지식집합은 \(\mathcal{Z}\stackrel{{\text{\tiny def}}}{{=}}\left\{(n,a,v^{ \star}(n,a)\right\}_{n\in\mathcal{N},a\in\mathcal{A}}}\)이다._

**제안 2.3** (기본, 비트 복잡도 상한): _주어진 \(\mathcal{N}_{0}\) 및 \(\mathcal{A}\) 및 \(\mathcal{T}\) 정의 2.2에서 생성된 지식 집합을 설명하려면 최대 다음과 같은 비트 수가 필요합니다._

\[\log_{2}\binom{|\mathcal{N}_{0}|}{N}+NKC\log_{2}D+K\log_{2}\binom{T^{L}}{D} \approx N\log_{2}\frac{|\mathcal{N}_{0}|}{N}+NKC\log_{2}D+KD\log_{2}\frac{T^{L}}{D}\enspace.\

(근사는 \(|\mathcal{N}_{0}|\gg N\) 및 \(T^{L}\gg D\)일 때 유효함) 섹션 3에서 비트 복잡도 하한을 제시할 것이다.

### 지식(경험적 설정)

우리는 정의 2.2에 따라 생성된 합성 바이오D 데이터 세트와 여러 인간 전기 데이터 세트를 모두 사용하여 언어 모델 스케일링 법칙을 평가한다.

Allen-Zhu와 Li[3]은 각각 6개의 속성, 즉 생년월일, 생도시, 대학, 전공, 고용주, 작업도시로 특징지어지는 \(N\)개의 개인으로 구성된 합성 전기 데이터 세트를 도입했다. 9 이러한 튜플을 자연어로 번역하기 위해, 그들의 바이오S 데이터 세트에서, 각각의 개인은 그들의 속성에 대응하는 6개의 무작위로 선택된 영어 문장 템플릿에 의해 기술된다. 우리는 더 자세한 내용을 위해 독자들을 그들의 논문으로 안내하지만 아래 예시를 제공한다:

각주 9: 노동 도시(고용주의 본부에 의해 결정됨)를 제외한 모든 속성은 무작위로 획일적이고 독립적으로 선택된다. N_{0}=400\times400\times1000\의 가능한 사람명, 12\times28\times200\의 생년월일, 200개의 출생도시, 300개의 대학, 100개의 전공, 263명의 고용주가 있다. 또한 각 사람에 대해 2개의 가능성을 가진 무작위 대명사를 선택한다.

[\begin{array}{l}\text{Anya Briar Forger]는 1996년 10월 2일에 태어났다. 그녀는 NJ 프린스턴에서 초창기를 보냈다. 그녀는 매사추세츠 공대 교수들로부터 멘토십과 지도를 받았다. 그녀는 커뮤니케이션을 중심으로 교육을 마쳤다. 그녀는 메타플랫폼에서 전문적인 역할을 했다. 그녀는 캘리포니아 멘로 파크에서 근무했다. \end{array} \tag{2.1}\

본 논문에서는 이러한 데이터셋의 세 가지 변형을 탐구한다.

* \(\text{bioS}(N)\)는 \(N\) 개인에 대한 온라인 데이터 세트를 나타내며, 여기서 각 전기는 6개의 문장 템플릿의 _선택_ 및 _순서화_에 대한 새로운 무작위성으로 생성됩니다.
* \(\text{bioS}^{\text{simple}}}(N)\)은 유사한 데이터 세트를 나타내지만 여기서는 문장 템플릿의 고정된 무작위 선택 및 순서를 사용하여 각 전기를 한 번 생성한다.
* \(\text{bioR}(N)\)은 동일한 데이터 세트를 참조하지만 각 전기를 LLaMA2 [33]에 의해 40번 기록하여 사실성과 다양성을 높입니다.

이러한 데이터 세트는 [3]에서 논의된 bioS 다중+순무트, bioS 단일+순무트 및 bioR 다중 데이터 유형에 해당하지만 약간의 차이는 있다. 그들의 연구는 \(N=100K\)에 초점을 맞추었지만, 우리는 \(N\)을 고려하기 위해 바이오S의 범위를 \(20M\)까지 확장하고, 바이오R의 경우 \(N\)을 \(1M\)로 제한하며, 이는 이미 22GB의 데이터 세트 크기를 산출한다.

섹션 1에서 소개한 바와 같이 각 지식 조각이 훈련 중에 1000번 볼 수 있는 경우 이를 1000개의 노출이라고 한다. \(\text{bioS}(N)\)의 경우, 각 속성에 대해 50개의 문장 템플릿이 있고 1인당 총 \(50^{6}\times 6!\)개의 가능한 전기들이 있기 때문에 1000개의 노출이 동일한 전기들을 포함할 가능성은 없다. \(\mathsf{bioS^{simple}}}(N)\)의 경우 1000회의 노출은 데이터의 1000회 통과를 의미한다. \(\mathsf{bioR}(N)\)의 경우 1000/100 노출은 훈련 데이터의 25/2.5 패스만 의미한다.

\(\mathsf{bioD}\) 데이터 세트의 경우, \(\mathcal{N}_{0}\)이 \(\mathsf{bioS}\)와 동일하도록 정의하며, \(|\mathcal{N}_{0}|=400\times 400\times 1000\). 우리는 무작위 문장 순서와 일관된 문장 템플릿을 사용하여 단일 단락 내에서 사람의 속성을 요약한다. 예를 들어,

Anya Briar Forger의 ID 7은 \(v_{7,1},\ldots,v_{7,C}\)이다. 그녀의 ID 2는 \(v_{2,1},\ldots,v_{2,C}\). [...] 그녀의 ID 5는 \(v_{5,1},\ldots,v_{5,C}\).

본 논문에서는 \(\mathsf{bioS}\)를 주로 사용한다. 보다 광범위한 적용 가능성을 설명하고 이론적 한계에 더 잘 연결하기 위해 \(\mathsf{bioS^{simple}}\), \(\mathsf{bioR}\) 및 \(\mathsf{bioD}\)에 대한 결과도 제시한다.

### 모델 및 훈련

GPT2는 [26]에 소개되었다. 절대 위치 임베딩 [2]의 한계로 인해 우리는 편의상 GPT2라고 부르는 현대 변형 _회전 위치 임베딩_[7, 31]을 채택한다. 또한, 언어 모델에서 성능을 향상시키는 것으로 나타난 드롭아웃을 비활성화한다[33]. GPT2-\(\ell\)-\(h\)는 \(\ell\) 층, \(h\) 머리 및 \(64h\) 차원을 나타내며, 예를 들어 GPT2-작은 것은 GPT2-12-12에 해당한다. 기본 GPT2Tokenizer는 사람들의 이름과 대부분의 속성을 가변 길이의 토큰으로 변환하는 데 사용된다. 섹션 7에서 모델 아키텍처가 스케일링 법칙에 미치는 영향을 조사함에 있어서, 우리는 또한 LLaMA/Mistral 아키텍처를 사용할 것이다[19, 32].

**훈련.** 지정된 데이터 세트를 사용하여 언어 모델을 _처음부터(즉, 무작위 초기화)_ 훈련합니다. 개인에 대한 지식 단락은 무작위로 연결되어 <EOS> 토큰으로 분리된 다음 512개의 토큰 창으로 무작위로 분할된다. 표준 자기회귀 손실은 훈련을 위해 사용된다. 달리 명시되지 않는 한, 훈련은 디폴트 AdamW 최적화기 및 혼합-정밀 fp16을 이용한다. 학습 속도 및 가중치 감쇠는 적당히 조정된다(부록 참조).

## 3 비트 복잡성 하한

모델에 저장된 지식을 평가할 때 **평균, 단어별** 교차 엔트로피 손실에 단순히 의존할 수 없습니다. 예를 들어 (2.1)의 "교수진으로부터 멘토링과 지도를 받았다"는 문구는 유용한 지식을 구성하지 않는다. 대신 지식 토큰에 대한 손실의 합에 초점을 맞추어야 합니다.

가중치 매개변수가 \(W\in\mathcal{W}\)인 모형 \(F\)을 고려하십시오. \(F\)는 임의의 최적화기를 사용하여 정의 2.2에서 정의된 \(\mathsf{bioD}(N,K,C,D,L,T)\) 데이터세트 \(\mathcal{Z}\)에 대해 훈련된다고 가정하며, 이 과정은 \(W=W(\mathcal{Z})\)로 표현된다(모델의 가중치는 훈련 데이터세트 \(\mathcal{Z}\)의 함수로 훈련된다). 평가 단계에서는 이름을 생성하는 \(F^{\top}(W,R)\)와 주어진 값을 생성하는 \(F^{\perp}(W,n,a,R)\의 두 함수를 통해 \(F\)를 표현한다. 여기서 \(R\)는 생성에 사용되는 무작위성을 나타낸다. \(F^{\perp}_{1}(W(\mathcal{Z}),n,a,R)\)는 \(F^{\perp}(W(\mathcal{Z}),n,a,R)\)의 첫 번째 청크를 나타낸다. 우리는 다음의 세 가지 교차 엔트로피 손실:10을 계산하여 \(F\)를 평가한다.

각주 10: \(\mathbb{E}_{n}\) 또는 \(\mathbb{E}_{n,a}\)를 사용하여 \(n\in\mathcal{N},a\in\mathcal{A}\)의 균일한 랜덤 선택을 나타낸다.

[\mathcal{Z}],n,a,R)=v^{\star}_{1}(\mathcal{Z}}-\log\underset{R}{\mathbf{Pr}}\left[F^{\top}_{1}(W(\mathcal{Z}),n,a,R)=v^{\star}_{1}(\mathcal{Z})&\stackrel{{\mathrm{A}}{ \mathbb{E}}-\log\underset{R}{\mathbf{Pr}}\left[F^{\top}_{1}(\mathcal{Z})}}{{=}}\underset{n\in\mathcal{N}},a\in\mathcal{A}}{ \mathbb{E}}-\log\underset{R}{\mathbf{Pr}}\left[F^{\perp}(W(\mathcal{Z}),n,a,R)=v^{\star}(n,a)\right]\mathsf{loss}_{value1}(\mathcal{Z})&\st 예를 들어, "Anya Briar Forger의 ID 7은 \(v_{7,1},\ldots,v_{7,C}\)," _summing up_ (평균화하지 않음!) "Anya Briar Forger"의 토큰에 대한 손실은 \(n\) = "Anya Briar Forger"에 대해 정확히 \(-\log\mathbf{Pr}_{R}\left[F^{\top}(W(\mathcal{Z}),R)=n\right]\)를 산출합니다. _summing up_ 토큰에 대한 손실은 \(v_{7,1}\)에 대해 \(-\log\mathbf{Pr}_{R}\left[F_{1}^{\top}(W(\mathcal{Z}),n,a,R)=v_{7,1}\right]\)를 산출합니다. 이것은 토큰izer 또는 값 길이에 관계없이 유지됩니다.

**정리 3.2** (비트 복잡도 하한). _ \(N\geq\Omega(D\log N)\)를 가정합니다. 우리는...

\geq\operatorname*{\mathbb{E}}_{\mathcal{W}| \geq\operatorname*{\mathbb{E}}_{\mathcal{Z}}\left[N\log_{2}\frac{T^{L}-D}{De^{(1+o(1)) \mathbf{loss}_{value1}(\mathcal{Z})}}+NK\log_{2}\frac{D^{C}}{e^{ \mathbf{loss}_{value}(\mathcal{Z})}+o(KD)\right]\] \[=N\log_{2}\frac{N_{0}-N}{e^{\operatorname*{\mathbb{E}}_{\mathcal{Z}}\mathbf{loss}_{name}(\mathcal{Z})}}+NK\log_{2}\frac{T^{L}-D}{De^{(1+o(1))\operatorname*{\mathbb{E}}_{\mathcal{Z}}

이 논문의 목표는 모델 매개변수의 수가 이 한계와 어떻게 경쟁하는지 연구하는 것이다.

**상관 3.3** (오류 없음): _이상적인 경우 모든 데이터 \(\mathcal{Z}\)에 대해 \(F\)가 \(\mathcal{N}\)에서 이름을 생성할 수 있으며 각각 정확한 \(1/N\) 확률로 \(\mathbf{loss}_{name}(\mathcal{Z})=\log N\); 및 \(F\)가 \((n,a)\) 쌍이 주어진 값을 100% 정확하게 생성할 수 있는 경우 \(\mathbf{loss}_{value}(\mathcal{Z})=\mathbf{loss}_{value1}(\mathcal{Z})=0\). 이와 같은 경우,_

\[\log_{2}|\mathcal{W}|\geq N\log_{2}\frac{N_{0}-N}{N}+NKC\log_{2}D+KD\log_{2} \frac{T^{L}-D}{D}-o(KD)\]

_asymptotically matching the upper bound Proposition 2.3._

_Remark 3.4_(왜 "3의 합"): 세 구성 요소의 _합_인 하한을 얻는 것이 필수적이며, 임의의 것을 무시하면 차선 제한이 발생할 수 있다(부록 A.4의 예 참조).

_Remark 3.5_ (왜 "랜덤 데이터"): 고정된 데이터 세트에 대한 하한을 연구하는 것은 불가능합니다 \(\mathcal{Z}\) -- 모델은 학습 가능한 매개 변수가 없더라도 \(\mathcal{Z}\)를 아키텍처에 하드 코드할 수 있습니다. 따라서 데이터 집합에 대한 _분포_ 에 대해 하한을 고려해야 합니다.

**증명 어려움.** 이름이 고정(\(\mathcal{N}=\mathcal{N}_{0}\))이고 각각 고정된 집합에서 균일하게 선택된 \(N\)개의 지식 조각이 있는 경우 이러한 지식을 _완벽하게 학습할 수 있는 모든 모델 \(F(W)\가 \(\log_{2}|\mathcal{W}|\geq N\log_{2}T\)를 충족해야 합니다. 이를 정리 3.2와 연관시키기 위해 우리는 세 가지 주요 도전에 직면한다. 먼저, 모델 \(F\)은 교차 엔트로피 손실에 의해 정의된 바와 같이 어느 정도의 정확도로만 지식을 학습할 수 있다. 둘째, \(\mathcal{N}\neq\mathcal{N}_{0}\)이므로 이름을 학습해야 합니다. 완벽한 모델이라도 이름을 생성할 때 교차 엔트로피 손실이 0이 될 수 없습니다. 셋째, 지식 조각들 사이에는 종속성이 존재하는데, 그 값은 다양성의 집합(즉, \(\mathcal{D}_{a}\))의 이름과 선택에 따라 달라진다. 정리 3.2의 증명은 부록 F로 미루어진다.

## 4 용량 비율

Theorem 3.2, ignoring lower order terms, we define the empirical capacity ratio as

**정의 4.1**.: \(\text{bioD}(N,K,C,D,L,T)\) 데이터 세트 \(\mathcal{Z}\)에 대해 훈련된 \(P\) 매개 변수가 있는 모델 \(F\)를 제공하면 \(p_{1}=\mathbf{loss}_{name}(\mathcal{Z})\), \(p_{2}=\mathbf{loss}_{value}(\mathcal{Z})\), \(p_{3}=\mathbf{loss}_{value1}(\mathcal{Z})\)을 정의합니다.

\[R(F) \stackrel{{\text{\tiny def}}}{{=}}\frac{N\log_{2}\frac{N_{0}}{ e^{p_{1}}}+NK\log_{2}\frac{D^{C}}{e^{p_{2}}}+KD\log_{2}\frac{T^{L}}{De^{p_{3}}}}{P}\enspace.\] \ [R^{\text{\tiny max}}(F) \stackrel{{\text{\tiny def}}}{{{=}}\frac{N\log_{2} \frac{N_{0}}{N}+NKC\log_{2}D+KD\log_{2}\frac{T^{L}}{D}}{P}\enspace.\]_ 비고 4.2_.: 하나는 \(R(F)\leq R^{\mathsf{max}}}(F)\를 가져야 하며, 모델이 _퍼펙트_인 경우 등식이 얻어진다. 고정 데이터 세트의 경우 모델 크기의 추가 증가는 추가 지식을 제공하지 않으므로 모델 크기 \(P\)가 증가함에 따라 \(R^{\mathsf{max}}(F)\)는 0에 접근한다. 반면에 정리 3.2는 하위 항을 무시하고 모델 매개변수가 8비트(예: int8)인 경우 \(R(F)\leq 8\)를 의미한다.

우리의 \(\mathsf{bioS}(N)\) 데이터에 대해 다이버시티 항.11을 생략하여 약간 감소된 용량 비율을 정의한다.

각주 11: 정리 3.2의 버전은 다양성 세트를 제외하기 때문에 더 간단한 증명으로 이 데이터 세트에 대해 입증될 수 있다. 이것은 또한 모델이 (예를 들어, 300개의 대학 이름들의 고정된 세트를 가정하여) 학습된 비트들에 대한 이 지식을 카운트하지 않고 다양성 세트에 대한 완전한 사전 지식을 갖는다는 것을 의미할 수 있다.

**정의 4.3**.: _\(\mathsf{bioS}(N)\) 데이터 세트 \(\mathcal{Z}\)에 대해 훈련된 \(P\) 매개 변수가 있는 모델 \(F\)를 제공하면 \(p_{1}=\mathbf{loss}_{name}(\mathcal{Z})\) 및 \(p_{2}=\mathbf{loss}_{value}(\mathcal{Z})\) 용량 비율 12_

각주 12: 여기에서 \(\mathcal{K}=\{\)생년월일, 출생도시, 대학, 전공, 고용주, 성별\(\}\)을 허용하고 이에 따라 \(\mathbf{loss}_{value}(\mathcal{Z})\stackrel{{\text{\tiny def}}}{{{=}} \mathbb{E}_{n\in\mathcal{N}}\sum_{a\in\mathcal{K}}-\log\mathbf{Pr}_{R}\left[F^ {\perp}(W(\mathcal{Z}),n,a,R)=v^{*}(n,a)\right]\를 정의할 수 있다.

\[R(F)\stackrel{{\text{\tiny def}}}{{=}}\frac{N\log_{2}\frac{N_{0}}{e^{p_{1}}}+N\log_{2}\frac{S_{0}}{e^{p_{2}}}}}{P}\quad\text{and}\quad R^{ \mathsff{max}}(F)\stackrel{{\text{\tiny def}}}{{=}}}\frac{N\log_{2}\frac{N_{0}}{N}+N\log_{2}S_{0}}{P}\\text{and}\quad R^{ \mathsff{max}}(F)\stackrel{{\text{\tiny def}}}{{=}}}\frac{N\log_{2}}\frac{N_{0}}{N}+N\log_{2}S_{0}}{P}\\text{and}{P}\

_for \(N_{0}=400\times400\times1000\) 및 \(S_{0}=2\times(12\cdot28\cdot200)\times200\times300\times100\times263\)(c.f. 각주 9)._

_Remark 4.4_: 이름을 무시하면 각 사람은 \(\log_{2}(S_{0})\approx 47.6\) 비트의 지식을 포함한다.

## 5 기본 크기 조정 법칙

그림 1: 1000/100 노출에 대해 fp16(혼합 정밀도)을 사용하여 \(\mathsf{bioS}(N)\) 데이터에 대해 사전 훈련된 GPT2에 대한 스케일링 법칙.

먼저 혼합 정밀도 fp16을 사용하여 \(\mathsf{bioS}(N)\) 데이터 세트(섹션 2.2 참조)에서 일련의 GPT2 모델을 훈련한다. 훈련 프로토콜은 각 지식 조각이 1000번 제시되도록 하며, 이는 우리가 "1000개의 노출"이라고 부르는 프로세스이다. 이것이 데이터를 1000번 통과시키는 것과 다르다는 것을 명확히 하는 것이 중요하다. 예를 들어, 위키 데이터를 통과하는 한 번의 통과는 지식(미국, 수도, 워싱턴 DC)을 1000번 노출시킬 수 있는 반면, 커먼 크롤을 통과하는 통과는 백만 번 노출시킬 수 있다. 1000개의 노출에 대해 훈련된 우리의 합성 \(\mathsf{bioS}(N)\) 데이터는 이러한 시나리오를 복제하는 것을 목표로 합니다. 14 우리의 초기 결과는 다음과 같습니다. 15

각주 14: 1000개의 노출 내에서, 동일한 개인이 동일한 지식을 상세히 설명하는 1000개의 다른 전기 문단을 가질 가능성이 있다(2.2절 참조). 따라서, 한 번의 패스 내에서 1000회의 노광이 발생할 수 있다.

**결과 1** (그림 1(a)).: \(\mathsf{bioS}(N)\), 10K에서 10M 범위의 \(N\)에 대한 1000개의 노출에 대해 훈련된 경우, 크기가 1M에서 0.5B 매개 변수(깊이 또는 너비에 관계없이_)인 GPT2 모델은 다음을 보여줍니다.

1. [label=()]
2. 피크 용량 비율 \(R(F)\)이 일관되게 \(R(F)\geq 2\)를 초과함;
3. \(R^{\mathsf{max}}(F)\leq 1.8\)을 갖는 모델들은 거의 완벽한 지식 정확도들, 즉 \(R^{\mathsf{max}}(F)\approx R(F)\에 도달한다;
4. 모든 모델에서 \(R(F)\leq 2.3\).

_Remark 5.1_. 결과 1(a), 1(b) 및 1(c)는 스케일링 법칙의 _3가지 별개의 면_을 설명한다.

* 결과 1(a)은 모델 간의 최대 용량을 강조 표시하지만 단일 모델만 이 피크를 달성하면 오판의 소지가 있습니다.
* 결과 1(b)는 최대 용량 \(R^{\mathsf{max}}(F)\leq 1.8\)인 모든 모델이 이러한 최대 용량, 즉 \(R(F)\approx R^{\mathsf{max}}(F)\를 달성할 수 있음을 보여줌으로써 이를 강화한다. 즉, \(B\) 비트의 지식을 포함하는 데이터 세트의 경우 모델 크기 \(P\geq B/1.8\)를 선택하는 것으로 충분하다는 것을 나타냅니다.
* 결과 1(c)은 용량 비율 2.3을 초과하는 모델이 없음을 나타냄으로써 이를 더욱 강화합니다. _명확성을 위해_ 이 문서의 후속 결과에서 결과 1(b) 및 결과 1(c)와 유사한 관찰이 **일관되게 적용됨** 이라는 이해와 함께 _최고 용량 비율에만 초점을 맞춥니다.

**지식 추출.** "2bit/param" 결과는 단어별 암기에 대한 것이 아닙니다. 더 양호하게는, 그러한 지식은 또한 (예를 들어, " Anya Forger의 생일은?"과 같은 QAs를 사용하는 미세 조정을 통해) 유연하게 추출가능하며[3] 따라서 (두 사람의 생일을 비교하는 것, 또는 검색된 지식에 대한 계산을 수행하는 것 등과 같은) 다운스트림 태스크들에서 추가로 조작될 수 있다[4]. 이것은 우리의 \(\mathsf{bioS}(N)\) 데이터가 지식을 증강하기 때문이다: 영어 전기들은 충분한 텍스트 다양성을 가지고 있다[3]. 우리는 또한 부록 A.2에서 그러한 지식이 추출 가능하다는 것을 검증한다.

### 데이터 형식 -- 다양성 및 다시 쓰기

우리는 \(\mathsf{bioS}^{\mathsf{simple}}}\)와 \(\mathsf{bioR}\)에 대해 동일한 분석을 수행한다. 2.2절에서 회상하면, \(\mathsf{bioS}^{\mathsf{simple}}\)는 텍스트 다양성이 감소된 \(\mathsf{bioS}\)의 변형(1인당 1개의 전기)인 반면, \(\mathsf{bioR}\)은 LLaMA2에 의해 생성되어 실제에 가까운 인간 전기를 생성한다. 우린...

**결과 2** (부록 A.3의 그림 11).: 동일한 1000 노출 설정에서 \(\mathsf{bioS}^{\mathsf{simple}}}\) 및 \(\mathsf{bioR}\)에서 훈련된 GPT2의 피크 용량 비율도 약간 낮지만 약 2입니다. 따라서:

* [label=*]
* 다양 한 데이터 (동일한 데이터를 여러 번 다시 작성 하는 것)는 손상 되지 않으며 때로는 개선 될 수 있습니다. 모델의 용량! 결과 2의 중요성을 강조 합니다. 2.2 절에서 회상 합니다.

* 1000개 노출에 대한 \(\mathsf{bioS}^{\mathsf{simple}}\) 데이터에 대한 훈련은 1000개가 데이터를 통과합니다.
* 1000 노출에 대 한 \(\mathsf{bioS}\) 데이터에 대 한 교육은 1 패스 미만입니다.
* 1000 노출에 대한 \(\mathsf{bioR}\) 데이터에 대한 교육은 25통과입니다.

따라서, \(\mathsf{bioS}\)와 \(\mathsf{bioS}^{\mathsf{simple}}\)를 비교하면, 1000개의 패스에 대해 동일한 데이터를 훈련시키는 것보다(\(\mathsf{bioS}^{\mathsf{simple}}\) 데이터에 대해 각각 훈련시키는(이 이상적인 설정에서) 데이터를 1000번 다시 쓰는 것이 더 유리하다. 데이터 다양성이 없으면 모델이 문장 구조를 암기하는 용량을 낭비하여 용량 손실이 발생하기 때문이다.

실제 시나리오에서 LLaMA2와 같은 도구는 \(\mathsf{bioR}\)에서와 같이 사전 훈련 데이터를 다시 쓸 수 있다. 데이터를 40번 다시 쓰는 것은 40개의 별개의 영어 문단을 만들 수 있으며, 때로는 (다른) 환각적인 내용이 있다. 모델이 40배 더 커야 하나요? 아니요, \(\mathsf{bioS}\)와 \(\mathsf{bioR}\)를 비교한 결과 동일한 기간 동안 훈련된 경우(각각 25회 40회 재기록) 모델의 용량 비율은 거의 동일하게 유지되며 LLaMA2에 의해 도입된 관련 없는 데이터로 인해 약간 낮다.

알렌-주(Allen-Zhu)와 리(Li[3])는 사전 학습 데이터를 다시 쓰는 것이 지식 추출이 가능한 단어 단위 암기를 만드는 데 중요하다고 제안했지만 모델의 용량에 미치는 영향을 탐구하지 않았다. 이 격차는 사전 훈련 데이터를 다시 쓰는 것이 모델의 지식 용량을 손상시키지 않고 심지어 향상시킬 수 있음을 나타냅니다.

각주 16: [3]에 의해 입증된 바와 같이, \(\mathsf{bioS}^{\mathsf{simple}}\)와 같은 저다양성 데이터 세트에서 지식은 단어 단위로 암기될 수 있지만 다운스트림 태스크에서는 거의 0% 추출 가능하다. 다른 사람들은 데이터를 다시 쓰는 것이 지식의 역전 추출 가능성을 향상시킬 수 있다는 것을 발견한다[4, 11].

그림 2: \(\mathsf{bioD}(N,K,C,D,L,T)\) 데이터에 대해 학습된 GPT2 모델에 대한 스케일링 법칙 \(\mathbf{1000\,\,exposures}\).

### 매개 변수화된 크기 조정 법칙

우리는 \(\mathsf{bioD}(N,K,C,D,L,T)\) 데이터 패밀리 내에서 스케일링 법칙을 추가로 조사한다. 변동이 \(N\)로 제한되는 인간 전기와 달리 \(\mathsf{bioD}\) 데이터 세트는 나머지 하이퍼파라미터 \(K,C,D,L,T\)를 보다 유연하게 조작할 수 있다. 이를 통해 이러한 매개변수의 변화가 모델의 최대 용량에 어떻게 영향을 미치는지 조사할 수 있다.

**결과 3** (그림 2).: \(K,C\) 범위가 1에서 50, \(D\) 범위가 10에서 \(10,000\), \(L\) 범위가 1에서 50, \(T\) 범위가 20에서 \(40,000\)인 광범위한 값의 스펙트럼에서 다음을 관찰합니다.

* GPT2 모델은 일관되게 피크 용량 비율 \(R(F)\geq 2\)을 나타냅니다.

## 6 Training Time vs Scaling Law

모델이 충분히 훈련되지 않으면 어떻게 합니까? 예를 들어, 사전 훈련 단계에서 지식이 100번만 나타나는 경우가 있을 수 있다. 또한 \(\mathsf{bioS}(N)\)에 대한 100개의 노출로 학습된 모델의 용량비를 계산한다. 우리의 연구 결과는 다음과 같이 요약할 수 있다.

**결과 4** (그림 1(b)).: \(\mathsf{bioS}(N)\) 데이터 세트에 대한 100개의 노출에 대해 훈련된 경우, 크기가 1M에서 0.5B인 GPT2 모델의 광범위한 스펙트럼에 걸쳐 \(N\) 10K에서 10M 범위의 \(N\)가 있을 때, 피크 용량 비율 \(R(F)\)은 일관되게 \(R(F)\geq 1\를 초과합니다.

따라서 모델이 최대 저장 용량에 도달하려면 1000회의 노출이 필요할 수 있지만 100회의 노출만으로 훈련하면 2배 이하의 용량 손실이 발생한다.

섹션 10에서는 _매우 낮음(예: 1) 또는 높음(예: 1M) 노출이 있는 지식도 고려해야 한다. 단독으로 연구하는 것은 흥미롭지 않을 수 있지만 예를 들어 100번의 노출에 대해 나타난 "표준" 지식과 이것이 모델의 용량에 미치는 영향을 조사하면 더 흥미로워진다. 결과는 10부터 12까지입니다.

7 Model Architecture vs Scaling Law

여러 변압기 아키텍처가 널리 채택되었으며 LLaMA와 미스트랄이 가장 주목할 만하다. 우리는 GPT2와의 주요 차이점을 부록 B에 추가 세부 사항과 함께 요약한다.

1. LLaMA/Mistral은 \(V\sigma(Wx)\) 대신 \(V(\sigma(W_{1}x)\cdot(W_{2}x))\)인 소위 GatedMLP 층을 사용한다. Shazeer[29]는 게이트 활성화가 약간 향상된 성능을 산출할 수 있다고 제안했다.
2. GPT2와 달리, LLaMA/Mistral은 가중치를 묶지 않는다.
3. 미스트랄은 GPT2/LLaMA에 비해 더 큰 MLP 층을 특징으로 한다.
4. 미스트랄은 GPT2/LLaMA에 의해 그렇지 않고 그룹-쿼리 주의를 촉진한다.
5. LLaMA/Mistral은 GPT2와 다른 토큰화기를 사용한다.
6. GPT2는 \(gelu\) 활성화 함수, LLaMA/Mistral opt for \(silu\)를 사용한다.
7. GPT2는 훈련가능한 바이어스로 계층 정규화를 구현한다.

이러한 아키텍처 변형이 모델의 최대 용량에 영향을 미치나요? 우리의 연구 결과는 지식 용량 측면에서 GPT2가 회전식 임베딩과 드롭아웃 없이 강화될 때 충분한 훈련 체제에서 위의 다른 아키텍처 선택보다 나쁘지 않음을 시사한다. 세부 사항을 부록 B.1로 미루고 아래 주요 결과를 요약한다.

**결과 5** (그림 3).: 1000 노출 설정에서 아키텍처는 크게 중요하지 않습니다.

* LLaMA 아키텍처는 비록 작은 모델(즉, \(<\) 10M)에 대해서는 약간 열등하지만 GPT2에 비해 비교적 성능이 우수하다. 이러한 불일치는 그림 3(b)와 비교하여 그림 3(c)와 같이 가중치를 묶기 위해 LLaMA 아키텍처를 요구함으로써 완화될 수 있다.
* 유사한 관찰이 미스트랄 아키텍처에 적용된다(도 3(d) 참조).
* GPT2 아키텍처의 MLP 크기를 1/4로 줄이거나 모든 MLP 계층을 제거하면 용량 비율에 영향을 미치지 않습니다. 그림 3(e) 및 그림 3(f)를 참조하십시오. 이는 기존의 믿음과 달리 주의 계층도 지식을 저장할 수 있음을 시사한다.

이는 2비트/파라미터 용량 비율이 대부분의 전형적인 (디코더 전용) 언어 모델 아키텍처 중에서 비교적 _보편적인 법칙_임을 나타낸다.

### 훈련 체제 부족 및 근접 비교

그러나, 불충분한 훈련 체제에서 아키텍처의 차이가 명백해진다:

**결과 6** (그림 4).: 100 노출 설정에서:

* _대규모 모델의 경우에도_ LLaMA 아키텍처의 용량 비율은 학습 속도를 최적으로 조정한 후에도 GPT2보다 1.3배 더 나쁠 수 있습니다. 결과는 미스트랄에 대해 유사하다.
* GPT2의 MLP 크기를 1/4로 줄이는 것은 용량 비율에 거의 영향을 미치지 않습니다.
* MLP를 제거 하면 용량 비율이 1.5 배 이상 감소 합니다.

그림 3: **1000 노출** 을 사용 하는 \(\text{bioS}(N)\) 데이터에 대 한 다른 모델 아키텍처에 대 한 크기 조정 법률입니다.

100-노출(불충분하게 훈련됨) 설정에서 LLaMA 아키텍처가 GPT2보다 열등한 이유를 조사하기 위해, 우리는 주요 아키텍처 변경 사항을 식별하기 위해 아키텍처를 GPT2 쪽으로 점진적으로 수정하여 LLaMA를 면밀히 조사한다. 1000 노출 설정에서 작은 LLaMA 모델의 용량을 향상시키기 때문에 가중치를 묶는 것으로 시작한다(결과 5). 도 5에 도시된 바와 같이:

* 대형 모델의 경우 LLaMA 아키텍처의 게이트 MLP를 표준 MLP로 대체( \(silu\) 변경되지 않은 상태로 유지)하면 LLaMA의 용량 비율이 눈에 띄게 개선됩니다.17 각주 17: 부록 B에서 논의한 바와 같이 게이트 MLP 계층은 훈련에 덜 안정적이므로 더 많은 시간이 필요합니다.
* 작은 LLaMA 모델의 경우 GPT2의 성능을 일치시키기 위해 GPT2Tokenizer로 다시 전환하는 것도 필요하지만 이는 사소한 문제입니다.18 각주 18: 이것은 작은 모델에만 적용되며 여기에서 고려하는 전기 데이터에 따라 다릅니다. GPT2Tokenizer는 1991년과 같은 연도를 단일 토큰으로 토큰화할 수 있는 반면 LLaMATokenizer는 4자리 토큰으로 토큰화할 수 있습니다.
* \(silu\)에서 \(gelu\)로 변경 하거나 계층-규범에 훈련 가능한 편향을 추가 하는 것과 같은 다른 수정은 용량 비율에 눈에 띄게 영향을 주지 않습니다 (따라서 이러한 수치는 무시 합니다).

In summary,

**결과 7.** _불충분한 훈련 체제_ (특히 100 노출 설정)에서 작은 모델을 제외하고 아키텍처 차이는 일반적으로 성능에 영향을 미치지 않습니다.

* [noitemsep,topsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,p=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,p=0pt,parsep=0pt,parsep=0pt,parsep=0pt,p=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,p=0pt,parsep=0pt,parsep=0pt,parsep=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,parsep=0pt,p=0pt,p=0pt,p=0pt,p,parsep=0pt,p=0pt,p=0pt,p,parsep=0pt,p=0pt,p=0pt,p=0pt,p,parsep=0pt,p=0pt,p,p=0pt,p,parsep=0pt,p=0pt,

## 8 양자화 대 스케일링 법칙

우리는 (혼합 정밀도) 16비트 부유체를 사용하여 모델을 훈련하고 테스트했다. 훈련 후 int8/int4로 양자화하면 어떻게 되나요? 우리는 양자화를 위해 GPTQ 용지 [10]에서 영감을 받은 auto_gptq 패키지를 사용했다.

**결과 8** (그림 6).: 16비트 플로팅으로 훈련된 언어 모델(예: GPT2) 양자화:

* ~ int8은 용량에 거의 영향을 미치지 않습니다.
* to int4는 용량을 2배 이상 줄입니다.

그림 5: **100 노출** 에 대한 \(\mathsf{bioS}(N)\) 데이터와 LLaMA의 스케일링 법칙에 대해 자세히 비교합니다.

그림 6: 결과 8의 일러스트레이션(부록 C에서 \(\mathsf{bioS}\) 데이터에 대해서는 그림 14 참조) 및 \(\mathsf{bioD}\) 데이터에 대해서는 그림 15 참조).

따라서 2비트/param의 최대 용량에서 모델을 **심지어** int8로 양자화하는 것은 용량에 영향을 미치지 않습니다. 고품질 데이터에 대한 1,000번의 훈련 노출 후에도 2비트/파라미터가 최상의 용량 비율이라는 점을 감안할 때 훈련을 확장하는 것이 모델의 용량을 더 향상시키지 않을 수 있지만 양자화는 할 수 있다고 결론지었다.

int8 기반 모델은 용량 비율에 대한 절대 상한 \(R(F)\leq 8\)을 가지므로 다음과 같다.

**결과 8.1**.: _GPT2와 같은 언어 모델은 지식을 저장하기 위한 이론적 절대 한도의 1/4을 초과할 수 있습니다._

불행히도, 이 양자화 패키지를 사용하면, 모델을 int4로 감소시키는 것은 그 용량을 상당히 감소시킨다(int8에서 int4로의 2배 이상 손실). 이는 고품질 int4 모델의 경우 훈련 중 양자화를 통합하는 것이 필요할 수 있음을 시사한다.

### 지식은 어디에 저장됩니까?

우리는 LLMs가 8비트 파라미터에서도 2비트/파라미터를 달성하면서 지식을 그들의 파라미터 공간으로 효율적으로 압축할 수 있다는 것을 보았다. 이것은 질문을 제기한다: 그러한 지식은 어떻게 그리고 어디에 저장되어 있는가? 우리의 예비 대답은 지식이 너무 중복되지 않는 방식으로 모델 내에 컴팩트하게 저장될 수 있다는 것이다. MLP 계층만으로는 유사한 크기의 주의 계층이 지식 저장에도 기여하기 때문에 지식이 저장될 가능성은 거의 없다(c.f. 결과 5). 특히 용량 경계 부근의 모델에서는 \(L\)-레이어 모델의 마지막 변압기 레이어를 제거하여 남은 지식을 탐침하는 것은 "남은 지식"이 전체 지식의 \(1-\frac{1}{L}\)보다 훨씬 적을 수 있음을 보여준다. 이는 하나의 레이어를 제거하면 전체 지식의 \(\frac{1}{L}\)보다 훨씬 많은 것을 제거할 수 있는 조합 잠금 장치가 있는 금고와 유사한 복잡한 방식으로 지식이 개별 레이어에 저장됨을 시사한다.

각주 19: 특별히 흥미롭지 않다고 여겨지는 이 실험은 논문에서 생략되었다. 사용된 프로빙 기법은 [3]의 Q-프로빙이다.

## 9 Mixture of Experts vs Scaling Law

현대 언어 모델에서 효율성을 높이는 중요한 방법은 희소성의 통합이다. Mixture of Experts(MoE) 아키텍처는 이와 관련하여 중요한 역할을 한다[9, 30]. 용량비 측면에서 MoE 모형 척도가 다른가 하는 의문이 제기된다. MoE 모델의 경우 \(P\)는 모든 전문가를 포함하여 **모델의 총 매개 변수 수** 를 나타냅니다. 본래 희소성으로 인해 유효 파라미터 수는 \(P\)보다 현저히 적을 수 있다. 우리의 주요 관찰은 MoE 모델이 레이어당 32명의 전문가라도 조밀한 모델로 _유사하게_ 확장된다는 것이다.

예를 들어 GPT2를 고려하지만 MLP 계층(\(d\ ~ 4d\ ~ d\))이 32명의 전문가로 대체되어 각각 \(d\ ~ d\ ~ d\) 구성을 따른다. 이 설정은 \(64d^{2}\) 총 매개 변수를 사용 하지만 추론 중에는 토큰당 \(2d^{2}\) 매개 변수만 사용 합니다 (예: \(topk=1\)). 각각 \(4d^{2}\)개의 매개변수를 갖는 주의층을 포함시킨 후 32명의 전문가 MoE 모델에 대한 총 매개변수와 유효 매개변수 수의 비율은 대략 \(\frac{4d^{2}+64d^{2}}{4d^{2}+2d^{2}}\ 약 11.3\이다.

추론 시간 동안 모델이 11.3배 더 적은 매개변수만 사용한다는 점을 감안할 때 이것이 모델의 용량 비율에 11.3배 가까운 요인 또는 1배 가까운 요인에 의해 영향을 미치는지 궁금할 수 있다. 보여줄게

**결과 9** (그림 7). _MoE는 희소성 제약에도 불구하고 모든 매개 변수를 활용할 수 있는 지식 저장에 거의 완전히 효율적입니다._

_구체적으로 32명의 전문가와 함께 GPT2-MoE 모델을 고려한다. 파라미터의 총 수에 대한 용량 비율을 계산하고 GPT2와 비교하는 경우:_

* _1000-노광 설정에서, 피크 용량 비율은 1.3배 감소함; 및_
* _100 노출 설정에서 피크 용량 비율은 1.5x 감소합니다._

_Remark 9.1_ (topk).: 결과 9는 MoE 라우팅에서 \(topk=1\) 및 \(cap\_factor=2\)인 "sparsest" 설정에서도 유지 됩니다. 결과는 \(topk=2\) 및 \(cap\_factor=1\) 또는 \(topk=2\) 및 \(cap\_factor=2\)-- 부록 D에서 자세히 설명합니다.

_Remark 9.2_: 일반적으로 MoE 모델이 동일한 수의 총 파라미터를 갖는 조밀한 모델에 비해 성능이 떨어지는 것이 실제로 관찰된다. 우리는 이러한 열화가 모델의 지식 저장 능력에서 비롯되지 않는다는 것을 보여준다.

## 10 Junk Data vs Scaling Law

_모든 데이터가 지식 획득에 유용한 것은 아니다. 예를 들어, 위키피디아는 가치 있는 정보로 가득 차 있지만, 웹 페이지들의 공통 크롤은 존재하지 않을 수 있다(그 웹 페이지들 상에 많은 정보들이 또한 존재하지만, 그것들은 랜덤 제품의 일련 번호와 같은 언어 모델이 학습하는데 유용하지 않을 수 있다). 저품질 데이터의 존재는 유용한 지식 용량의 스케일링 법칙에 어떻게 영향을 미치는가? 이를 조사하기 위해 혼합 데이터 세트를 만듭니다._

* \(1/8\) _of tokens origin from_ \(\mathsf{bioS}(N)\) _for various_ \(N\) _(referred as useful data), and_
* \(7/8\) _ 토큰의 원본은_ \(\mathsf{bioS}(N^{\prime})\) _ 큰_ \(N^{\prime}=100M\) _(정크 데이터라고 함) _입니다._

_우리는 이 혼합물에 대한 모델을 트레이닝하여, 각각의 유용한 데이터가 100개의 노출에 대해 보여지도록 보장하고, 따라서 총 트레이닝이 정크 없는 100개의 노출에 비해 8배 더 길어지게 한다(즉, 그림 1(b)). 우리는 유용한 데이터(데이터 in_ \(\mathsf{bioS}(N)\)_)의 용량 비율에 초점을 맞추고 비교합니다.

도 7: 결과 9의 일러스트레이션(상세 내용은 부록 D의 도 16 참조).

**결론.** 그림 8(b)-8(e)에서 정크 데이터가 랜덤 지식을 모방하는 경우 훈련 시간이 실질적으로 증가하지 않는 한 용량 비율은 _유의하게 영향을 받습니다. 그림 8(f)에서 정크 데이터가 매우 반복적일 경우 열화가 발생하지 않는다. 그림 8(g)+8(h)에서 wikipedia.org와 같은 도메인 이름과 유사한 유용한 데이터에 _특수 기호 토큰_을 추가하면 _용량 저하를 완화합니다._

_비고__ 그림 8(e)에 대한 훈련은 그림 8(a)보다 80배 더 오래 걸리므로 더 큰 데이터 크기 \(N\)를 사용한 실험이 비싸다(그러나 어쨌든 불필요하다).

그림 8: 7/8 정크 데이터(사전 훈련 동안 100/300/600/1000 노출이 관찰된 유용한 데이터)를 사용한 용량 비율입니다.

그림 1(b).20 정크 데이터가 있는 경우 용량 비율이 얼마나 저하됩니까?

각주 20: 정크 데이터에서 학습하는 모델의 능력은 무시할 수 있으며, 바이오S(\(N^{\prime}\))의 각 사람은 \(N=200k\)일 때 훈련 중 0.2회, \(N=50k\)일 때 0.05회만 나타난다.

**결과 10** (그림 8(a)-8(e)): 훈련 토큰의 7/8이 \(N^{\prime}=100M\)에 대한 정크 데이터(즉, bioS(\(N^{\prime}\))에서 나오는 경우 유용한 데이터에 대한 변압기의 학습 속도는 크게 저하됩니다.

* 동일한 100개의 노출에 대해 훈련된 경우, 정크 없이 훈련된 것과 비교하여 용량 비율이 20배 감소할 수 있다(도 8(b)와 도 8(a))
* 300/600/1000 노출에 대해 훈련된 경우에도 정크 없는 100 노출에 비해 용량 비율은 여전히 3x/1.5x/1.3x만큼 저하됩니다(그림 8(c), 8(d), 8(e) 대). 도 8(a)는!

이는 사전 훈련 데이터 품질의 중요한 중요성을 강조합니다. 정크 데이터가 완전히 무작위하더라도 충분한 훈련으로도 모델의 지식 용량에 부정적인 영향을 미칩니다.

대조적으로, 데이터의 7/8이 매우 작은 \(N^{\prime}\)를 갖는 bioS(\(N^{\prime}\))인 경우, 훈련 토큰에서 나타나는 고도로 반복적인 지식을 시뮬레이션하는 것(예를 들어, "다 빈치가 수백만 개의 변형으로 모나리자를 그렸다")이라면, 이는 "표준" 지식(예를 들어, 100개의 노출을 갖는 지식)에 대한 모델의 능력에 영향을 미치지 않을 수 있다:

**결과 11** (그림 8(f)): 훈련 토큰의 7/8이 \(N^{\prime}=1K\)에 대한 고도로 반복적인 데이터(즉, bioS(\(N^{\prime}\))에서 나온 경우 유용 지식의 학습 속도에 영향을 미치지 않습니다.

* 유용한 데이터의 100-노출 용량 비율은 변하지 않는다(도 8(f) vs. 도 8(a)는!

마지막으로 사전 훈련 데이터의 품질이 좋지 않고 개선하기가 어려운 경우 **백업 전략이 존재** 합니다.

**결과 12** (그림 8(g)+8(h)): 훈련 토큰의 7/8이 \(N^{\prime}=100M\)에 대한 정크(즉, bioS(\(N^{\prime}\))에서 나온 경우 모든 유용한 데이터의 시작에 특수 토큰을 추가하면 용량 비율이 크게 향상됩니다.

* 노출량이 100일 때 용량비는 2x만큼만 저하된다(도 8(g) vs. 도 8(a)는!
* 300개의 노출로, 용량 비율은 정크 없이 100-노출 스케일링 법칙의 용량 비율과 일치한다(도 8(h)와 도 8(a))

결과 12를 연습과 연결합시다. 첫째, 높은 신뢰도의 데이터에 특별한 토큰을 추가하는 것은 매우 실용적이다: 모든 위키피디아 단락의 시작 부분에 도메인 이름 "위키피디아.org"를 추가하는 것을 상상해보라. (정크 데이터에 특수 토큰을 추가하면 의미가 줄어듭니다.)21

각주 21: 결과 12는 정크 데이터의 모든 조각에 대해 (고유한) 특수 토큰을 추가하는 경우에도 성립하지만, 정크 데이터가 종종 다양한 웹 사이트에서 비롯되어 고유한 식별자를 할당하기 어렵기 때문에 이것은 무의미할 수 있다.

보다 일반적으로, 사전 트레이닝 데이터의 모든 조각에 도메인 이름(예를 들어, wikipedia.org)을 추가하는 것을 상상할 수 있다. 결과 12는 **언어 모델이 고품질 지식이 풍부한 도메인을 자동으로 검색하고 해당 도메인에서 학습의 우선 순위를 지정할 수 있음** 을 보여 주기 때문에 모델의 지식 용량이 크게 향상됩니다. 우리는 모델이 고품질 지식을 포함하는 도메인을 식별하는 데 사전 지식이 필요하지 않다는 점을 강조합니다.

## 11 Conclusion

우리는 언어 모델의 스케일링 법칙, 특히 모델 크기와 저장된 총 지식 비트 사이의 관계를 조사했다. 우리의 연구 결과는 _정확하고 보편적인_ 스케일링 법칙을 보여준다: 충분히 훈련된 변압기(즉, 훈련 손실이 안정화된 변압기)는 정보 이론적 최대값에서 1/4만 떨어져 있는 int8로 양자화된 경우에도 매개변수당 2비트의 지식을 저장할 수 있다. 또한 이러한 스케일링 법칙이 훈련 기간, 모델 아키텍처, 부동 소수점 정밀도, MoE와 같은 희소성 제약 및 데이터 신호-잡음비를 포함한 다양한 하이퍼파라미터에 의해 어떻게 영향을 받는지 조사했다.

지식 용량 측면에서 우리의 방법론은 모델 아키텍처, 훈련 기술 및 데이터 품질을 비교하기 위한 **보다 정확하고 원칙적인 놀이터** 를 제공합니다. 우리는 이 놀이터가 실무자들이 모델 선택, 훈련 데이터 준비 및 LLMs에 대한 추가 이론적 연구에 대한 정보에 입각한 결정을 내리는 데 도움이 될 수 있다고 믿는다. 마지막으로, 우리의 연구는 언어 모델이 얼마나 커야 하는지에 대한 근본적인 질문을 해결하기 위한 초기 단계를 나타낸다. 우리의 연구 결과가 이 분야의 추가 연구에 영감을 주기를 바랍니다. 궁극적으로, 우리는 향후 "1T 매개변수를 가진 언어 모델이 AGI를 달성하기에 충분한가?"라는 질문에 대한 원칙적인 답변을 제공하는 것을 목표로 한다.

## GPT2 스케일링 법칙에 대한 추가 부록

본 논문에서는 10K~20M 범위의 \(\mathsf{bioS}(N)\)에 초점을 맞추었다. 특히 \(\mathsf{bioS}(20M)\)는 약 1B 비트의 지식을 포함한다(정리 3.2 참조).

GPT2 모델. 섹션 2.3에서 자세히 설명된 대로 원래 GPT2 모델[26]을 GPT2라고 하며, 위치 임베딩을 _회전 임베딩_[7, 31]으로 대체하고 드롭아웃 레이어[33]를 제거한다. 이러한 수정은 언어 모델링 작업에서 성능을 향상시키는 것으로 널리 인식되고 있다(이를 비교하는 통제된 실험은 [2]도 참조). 본 논문에서는 64차원 헤드를 유지하면서 다양한 GPT2 모델 크기를 탐색한다. 표기법 GPT2-\(\ell\)-\(h\)는 \(\ell\) 층, \(h\) 헤드 및 \(64h\) 차원을 갖는 (수정된) GPT2 아키텍처를 나타낸다. 컨텍스트 길이는 512로 설정된다.

LLaMA, 미스트랄 및 기타 아키텍처의 사양에 대한 자세한 내용은 필요에 따라 부록 B에 제공된다.

모델 크기.이 연구에서는 임베딩 계층에서 사용하지 않는 토큰을 모두 제외 한 후 _모델 크기를 계산 합니다. 예를 들어, GPT2 임베딩 계층은 통상적으로 \(50256\times(64h)\)

그림 9: 1000/100 노출에 대해 fp16(혼합 정밀도)이 있는 \(\mathsf{bioS}(N)\) 데이터에 대해 사전 훈련된 GPT2에 대한 스케일링 법칙, 그림 1에 비해 **현재 1층 변압기를 포함함** 입니다. **결론: 1층 변압기는 특히 100 노출 환경에서 약간의 용량 비율 결핍을 보여줍니다.

매개 변수인 \(\mathsf{bioS}(N)\) 데이터는 \(3275\) 토큰만을 활용합니다. GPT2의 토큰화기를 적용한 후 유효 임베딩 레이어 크기를 \(3275\times(64h)\로 줄입니다. 이 조정은 \(\mathsf{bioS}\) 데이터에 대해 일반적으로 \(124\)M 매개 변수를 갖는 것으로 알려진 GPT2small이 이 논문에서 \(88\)M 매개 변수만 갖는 것으로 계산되는 이유를 설명합니다.

우리는 유사한 모델 크기를 가진 모델을 제외하고 실용적인 \(\ell\) 및 \(h\) 값을 가진 광범위한 GPT2-\(\ell\)-\(h\) 모델을 선택했다. 그들의 선택은 넓은 변압기 및 얕은 변압기(예: GPT2-2-20, GPT2-3-20, GPT2-4-20) 및 마른 변압기 및 깊은 변압기(예: GPT2-16-4, GPT2-16-8, GPT2-28-20)를 모두 포함하는 그림 1에 자세히 설명되어 있다. 참고로 GPT2 small/med/large는 각각 GPT2-12-12, GPT2-24-16, GPT2-36-20에 해당한다.

우리는 주로 \(\ell\geq 2\) 계층 변압기가 약간 더 낮은 용량 비율을 나타낼 수 있기 때문에 \(\ell\geq 2\) 모델을 중심으로 설명한다. (관심 있는 사람들을 위해 \(1\)-계층 변압기가 그림 9에 포함되어 있으며, 이는 그림 1과 동일하지만 이러한 모델을 포함한다.)

데이터 세트의 모델 크기 \(\mathsf{bioS}(N)\)와 \(N\geq 2M\). \(1000\)-노출 설정에서 계산 리소스를 보존하기 위해 \(N=2M,5M,10M,20M\)에 대한 스케일링 법칙을 탐색할 때 데이터 세트당 _하나의 모델 크기_ - 특히 GPT2-16-8, GPT2-6-20, GPT2-20-16, GPT2-25-20 - 2bit/param 임계값(즉, \(R^{\mathsf{max}}(F)\approx 2\)를 만족함)에 집중합니다. 이러한 맥락에서, 우리의 핵심 발견은 2비트/파라미터 용량 비율의 검증이므로 제한된 모델 크기 선택을 조사하는 것이 적절하다.

\(100\)-노출 설정의 경우 데이터 세트당 광범위한 모델 크기를 평가합니다. 이 접근법은 \(1000\)-노출 설정에 비해 훈련 시간이 10배 감소했을 뿐만 아니라 더 높은 모델 크기에서 정밀도를 목표로 \(100\)-노출 설정에서 모델 아키텍처를 자세히 비교할 수 있기 때문이다.

훈련 파라미터, 코사인 학습률 스케줄러와 함께 AdamW 최적화기를 사용한다. 여기에는 웜업의 \(1\)K 단계가 포함되며, 그 다음 기준 비율의 \(1\)에서 \(0.1\)까지의 학습 속도의 코사인 감쇠가 포함된다. 달리 명시되지 않는 한 혼합 정밀 fp16 훈련을 사용한다.

### 기본 크기 조정 규칙

\(1000\)-노출 및 \(100\)-노출 \(\mathsf{bioS}(N)\) 데이터에 대한 기본 스케일링 법칙은 각각 그림 1(a) 및 1(b)에 나와 있다.

\(1000\)-노출 설정의 경우 모델의 최종 성능은 충분한 훈련으로 인해 학습률 선택에 매우 민감하지 않습니다. 다음 파라미터들이 도 1(a)를 생성하기 위해 선택되었다:

**매개 변수 1** (그림 1(a)): \(\mathsf{bioS}(N)\) 데이터에 대한 GPT2 모델에 대한 \(1000\)-노출 설정:

* \(N=10K\)에 대해 \(wd=0.02\), \(lr=0.001\) 및 배치 크기 \(24\)(약 \(140\)K 훈련 단계)를 사용합니다.
* \(N=20K\)에 대해 \(wd=0.02\), \(lr=0.001\) 및 배치 크기 \(48\)(약 \(140\)K 훈련 단계)를 사용합니다.
* \(N=50K\)에 대해 \(wd=0.02\), \(lr=0.001\) 및 배치 크기 \(96\)(약 \(175\)K 훈련 단계)를 사용합니다.
* \(N=100K,200K\)의 경우 \(wd=0.02\), \(lr=0.001\), batch size \(192\) (about \(175\)K, \(349\)K training steps);
* \(N=500K,1M\)의 경우 \(wd=0.01\), \(lr=0.0005\), 배치 크기 \(192\)(약 \(435\)K, \(870\)K 훈련 단계);
* \(N=2M\)에 대해 \(wd=0.005\), \(lr=0.0003\) 및 배치 크기 \(1536\)(약 \(220\)K 훈련 단계)를 사용합니다.
* \(N=5M\)의 경우 \(wd=0.002\), \(lr=0.0003\) 및 배치 크기 \(1536\)(약 \(540\)K 훈련 단계)를 사용합니다.
* \(N=10M\)의 경우 \(wd=0.001\), \(lr=0.0003\) 및 배치 크기 \(1536\)(약 \(1\)M 훈련 단계를 사용 합니다.

_Remark A.1_ (fp16 vs bf16).: GPT2에 대한 훈련은 혼합-정밀 fp16을 사용하여 수행된다. 또한 bf16을 시도했고 결과는 거의 동일하다.

_Remark A.2_ (파라미터).: 이러한 최적화 파라미터는 _매우 자연적_이며, 이는 일반적으로 큰 곱셈 범위에 걸쳐 모델 크기에 대한 고정된 파라미터 세트를 갖는 것이 불가능하기 때문이다. 특히

* 모델 크기가 클수록 학습 속도가 더 작아야 합니다.

* 언어 모델에는 일반적으로 배치 크기에 관계없이 50K 이상의 교육 단계가 필요합니다. 따라서 작은 \(N\)의 경우 총 훈련 단계 수가 이 임계값을 초과하는지 확인하기 위해 배치 크기를 _줄입니다. 매우 큰 모델의 경우 GPU 병렬 처리를 가능하게 하기 위해 더 큰 배치 크기가 선호된다.
* \(lr\)가 일정하게 유지 되는 경우 교육 단계 수가 증가함에 따라 \(wd\)를 상대적으로 줄여야 합니다. 수학적으로, 모델 가중치는 모든 \(\theta(\frac{1}{lr\times wd})\) 훈련 단계에 대해 "반분"되어야 한다. 따라서 더 긴 기간 동안 훈련할 때 \(wd\) 매개 변수를 줄이는 것이 좋습니다.

_Remark A.3_ (#GPU).: 본 논문에서는 GPU의 개수가 무관하므로 특정하지 않는다. 결과는 배치 크기가 각각 24인 64개의 GPU를 사용하든, 배치 크기가 각각 32인 48개의 GPU를 사용하든, 배치 크기가 각각 1인 1536개의 GPU를 사용하든 동일하게 유지된다.

100-노출 설정의 경우, 학습 속도의 신중한 조정이 필요하다. 다음 매개변수는 그림 1(b)를 생성하기 위해 선택되었다. (참고: \(N=10K,20K\)는 과도하게 짧은 훈련 프로세스로 인해 100-노출 설정에 대해 고려되지 않는다.)

**매개 변수 2** (그림 1(b)).: \(\mathsf{bioS}(N)\) 데이터에 대한 GPT2 모델에 대한 100 노출 설정:

* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.001\) 및 배치 크기 12를 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.001\) 및 배치 크기 24를 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.001\) 및 배치 크기 48을 사용합니다. (GPT2-2-20을 제외하고, 여기서 \(lr=0.0005\)를 사용합니다.)
* \(N=500K\)의 경우 \(wd=0.01\), \(lr=0.0005\) 및 배치 크기 96을 사용합니다.
* \(N=1M\)의 경우 \(wd=0.01\), \(lr=0.0005\) 및 배치 크기 192를 사용합니다.
* \(N=2M\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 384를 사용합니다.
* \(N=5M\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005\) 및 배치 크기 768을 사용합니다.
* \(N=10M\)의 경우 \(wd=0.01\), \(lr=0.0002/0.0003/0.0005\) 및 배치 크기 1024를 사용합니다.
* \(N=20M\)의 경우 \(wd=0.002\), \(lr=0.0002/0.0003/0.0005\) 및 배치 크기 1536.22를 사용합니다.

각주 22: GPT2-28-20을 제외하고 GPU 메모리가 부족하여 배치 크기 1280으로 줄입니다.

### 지식 메모리화 대. 추출

최근 Allen-Zhu 및 Li[3]에 의해 모델이 지식을 암기하지만, 이 지식은 (예를 들어, 미세 조정을 통해) 다운스트림 태스크에서 적용하기 위해 추출될 수 없다는 것이 발견되었다. 모델에 의해 학습된 "2비트/파라미" 지식이 실제로 추출 가능한지 확인하는 것이 필수적이다. 이 검증은 미세 조정 작업(예를 들어, "안야의 생일은? 답변: 1996년 10월 2일")을 절반의 개인에게 적용한 다음 나머지 개인에 대한 성능을 테스트함으로써 달성된다.

특히 원본 \(\mathsf{bioS}(N)\) 데이터에서 각 모델에 대해 두 가지 양을 계산합니다.

* 기억할 수 있는 지식 정확도(# of people). 우리는 모델을 "Anya Briar Forger is born on"과 같은 원래 훈련 데이터에 적용하고 "1996년 10월 2일"을 올바르게 생성할 수 있는지 확인한다. 각 개인에 대해 5가지 속성을 모두 평가하고 평균 정확도를 계산합니다.23 다음 모든 \(N \) 개인에 대해 이 정확도를 _합계_ 합니다. (이상적으로, 완벽한 모델은 \(N\)과 동일한 "정확도"를 가질 것이다.) 각주 23: 회사 도시 속성은 고용주 이름에 의해 고유하게 결정될 수 있으므로 추가 지식을 제공하지 않기 때문에 제외한다.
* 추출 가능한 지식 정확도(# of people). [3]의 사전 훈련-미세 조정 프레임워크를 따라 "안야 브라이어 위조자의 생일은? 답: 1996년 10월 2일"과 같은 질문 응답 텍스트로 LoRA [17]을 사용하여 개인의 절반에 대해 주어진 사전 훈련 모델을 미세 조정한 다음 나머지 절반의 개인에 대해 생성 정확도를 테스트한다. 높은 정확도는 지식이 암기될 뿐만 아니라 다운스트림 작업에 대해 _유연하게_ 추출될 수 있음을 나타냅니다. 다시, 각 사람에 대해, 우리는 다섯 가지 속성을 모두 평가하고 그들의 평균 정확도를 계산한다. 그런 다음 모든 \(N/2\) 사람들에 대해 이것을 합하고 2를 곱합니다. (다시 한번 완벽한 모델은 \(N\)과 같습니다.)

우리의 결과는 그림 10에 나와 있습니다. 예를 들어 그림 10과 그림 10을 비교함으로써, 우리의 스케일링 법칙이 기억할 수 있는 지식뿐만 아니라 추출할 수 있는 지식에도 크게 적용된다는 것이 분명합니다. 용량비 경계에서 정확한 모델에 대해서만 총 정확도가 1.2배 감소한다.24

각주 24: 이러한 감소는 비트가 아니라 정확도에 있다; 모델은 비트에 많은 양의 추출 가능한 지식을 가질 수 있지만 정확도에 있지 않을 수 있다. 또한 추출 가능한 설정에서 _bits_ 지식을 계산할 수 있지만 간결함을 위해 이러한 결과를 생략합니다.

**매개 변수 3** (그림 10).: LoRA 미세 조정을 위해 크기가 크게 다른 모델을 다룰 때 LoRA 순위 크기를 조정해야 합니다. [3]에서 저자들은 주로 임베딩 계층에 대해 순위 \(r^{\prime}=128\) 업데이트를 사용하고 쿼리/값 행렬에 대해 순위 \(r=8\) 또는 \(16\)를 사용했으며 기본 모델은 GPT2-12-12 또는 GPT2-12-20이다. 본 논문에서는 더 넓은 범위의 순위 선택인 \((r^{\prime},r)\in\{(8,2),(16,2),(8,4),(32,4),(32,8),(32,8),(128,8),(32,16),(128,16)\}\)을 탐색하여 최상의 결과만 제시한다. 25

각주 25: 최상의 LoRA 옵션을 선택하는 것은 우리의 목표가 최대 추출 가능한 지식 비트를 결정하는 것이기 때문에 정당하며, 따라서 높은 테스트 세트 정확도를 입증하는 모든 LoRA 옵션이 우리의 목표를 충족한다.

그림 10: 그림 1의 스케일링 법칙은 _추출 가능한 지식_에도 적용됩니다(섹션 A.2의 정의를 참조). 이 그림은 GPT2 모델을 사용하는 \(\mathsf{bioS}(N)\) 데이터 세트에 대한 것입니다.

학습 속도 웜업을 해제하고 배치 크기를 96, 학습 속도를 0.001(선형 감쇠를 0으로 포함), 무게 감쇠를 0.1로 설정하고 75,000단계 동안 미세 조정한다.

### 기타 Biography 데이터 세트

또한, \(\mathsf{bioS^{simple}}(N)\) 데이터 세트를 조사하는데, 이는 각 개인의 지식이 6개의 고정된 문장의 고정된 순서에 저장되는 것을 제외하고는 \(\mathsf{bioS}(N)\)와 동일하다(2.2절 참조). 알렌-주(Allen-Zhu)와 리(Li[3])는 이러한 경우 지식 데이터는 기억할 수 있지만 거의 0% 추출할 수 있음을 발견했다. 그림 11(a)와 같이 이러한 경우 용량비는 그림 1(a)에 비해 약간 감소한다. 이것은 이 이상적인 환경에서 데이터 다양성을 추가한다는 것을 의미합니다. 서로 다른 쓰기 템플릿을 사용하여 동일한 지식을 여러 번 다시 쓰면서-- [3]에서 언급한 것처럼 모델의 지식 추출 능력을 향상시킬 뿐만 아니라 놀랍게도 이 연구에서 관찰된 것처럼 모델의 용량을 _증가시킵니다.

또한, LLaMA2에 의해 생성된 전기 단락과 유사한 반실제 데이터 세트 \(\mathsf{bioR}(N)\)를 탐색하고, 각 개인은 40회(랜덤 시드와 프롬프트를 사용하여 LLaMA2가 각 개인에 대해 가능한 한 다양한 문단을 생성하도록 권장함) 생성된다. 이로 인해 위키피디아 데이터의 크기에 필적할 수 있는 총 22GB의 텍스트가 생성된다.

\(\mathsf{bioR}(N)\) 데이터에 대한 스케일링 법칙은 그림 11(b)에 제시되어 있으며, 이는 더 큰 모델에 대해 용량 비율이 약간 감소함을 나타낸다. 이러한 경향은 LLaMA2가 인간 전기에 수많은 관련 없는 세부 사항(일반적으로 각 LLaMA2 세대에 대해 서로 다른 관련 없는 세부 사항)을 도입함으로써 더 많은 모델 용량을 소비하기 때문에 예상된다. 그 감소는 더 작은 모델의 경우 더 중요하며, 이는 데이터의 다양한 영어 문장을 이해하는 데 더 큰 어려움을 가질 수 있다.

**매개 변수 4** (그림 11). 두 실험 모두에서 매개 변수 1에 자세히 설명된 대로 그림 1(a)에 사용된 동일한 최적화 매개 변수 세트를 준수합니다.

### 매개 변수화된 크기 조정 법칙에 대한 추가

매개변수화된 스케일링 법칙에서는 정의 2.2의 \(\mathsf{bioD}(N,K,C,D,L,T)\) 데이터세트를 활용한다.

**매개 변수 5** (그림 2, 12, 13).: \(\mathsf{bioD}\) 데이터 세트의 GPT2 모델의 경우 \(wd=0.01\), \(lr=0.0005\) 및 배치 크기가 192인 1000 노출 사례에 중점을 둡니다.

_Remark A.4_ (파라미터).: 파라미터 1과는 달리, GPT2 모델들에 대한 우리의 실험들이 모델 크기들의 훨씬 더 좁은 범위에 걸쳐 있기 때문에, 트레이닝 파라미터들을 변화시킬 필요는 없다. We are

그림 11: **1000 노출** 을 사용 하 여 \(\mathsf{bioS^{simple}}}\) 및 \(\mathsf{bioR}\) 데이터에 대 한 크기 조정 법률입니다.

최적 2비트/파라미터 모델이 모델 크기 측면에서 서로 20배 이내에 있는지 확인하기 위해 \(N\)의 선택을 조정했다.

우리의 결과는 그림 2(본문에서는 명확성을 위해 정확도가 \(\leq 50\%\)인 모델로 제한됨)와 그림 12(모든 모델 포함)에 나와 있다.

또한, 비트 복잡도 하한으로부터 (정의 4.1 참조)

\[\underbrace{N\log_{2}\frac{N_{0}}{e^{p_{1}}}}_{\text{name}}+\underbrace{NK \log_{2}\frac{D^{C}}{e^{p_{2}}}}_{\text{value}}+\underbrace{KD\log_{2}\frac{ T^{L}}{De^{p_{3}}}}}_{\text{diversity}}}\](A.1)

또한 세 가지 구성 요소가 이러한 전체 하한에 어떻게 기여하는지 분석한다. 도 13에 도시된 바와 같이, "값" 컴포넌트가 전형적으로 지배적이지만, 특정 하이퍼파라미터 설정의 경우, "이름" 또는 "다양성" 컴포넌트도 중요할 수 있다. 이것은 세 항 모두의 합인 우리의 정리 3.2 하한을 증명하는 것의 중요성을 강조한다.

도 12: 도 2와 동일하지만, 정확도가 50% 미만인 모델들을 포함한다(이는 더 높은-정확도 모델들과 중첩될 수 있다). _peak_ 용량 비율은 \(R(F)\geq 2\)를 일관되게 초과합니다.

그림 13: 그림 2와 같이 매개변수화된 바이오D 스케일링 법칙 실험에서 지식 구성 요소의 분해. 식 (A.1) 및 첨부 텍스트를 참조한다.

모델 아키텍처에 대한 추가 정보

언어 모델에 대한 대안적인 아키텍처 선택을 탐색합니다.

LLaMA/Mistral.특히, 본 논문의 저술로 LLaMA[32, 33]와 Mistral[19]는 대중적이고 공개적으로 사용 가능한 대형 언어 모델로 눈에 띈다. 우리는 GPT2와의 주요 아키텍처 차이점을 강조하는데, 이는 회전식 임베딩이 있고 드롭아웃이 없는 것으로 정의한다.

1. LLaMA와 Mistral은 \(V\sigma(Wx)\) 대신 \(V(\sigma(W_{1}x)\cdot(W_{2}x))\)를 사용하여 게이트 활성화를 갖는 MLP 층을 사용한다. Shazeer[29]는 게이트된 활성화가 약간 더 나은 성능을 산출하는 것으로 보인다고 언급했다.
2. 임베딩 층과 출력(LMHead) 층의 가중치를 묶는 GPT2와 달리, LLaMA 및 Mistral은 그렇지 않다.
3. 은닉치수 \(d\)의 경우 GPT2/LLaMA는 주의층에 \(4d^{2}\) 파라메터가 있고 MLP층에 \(8d^{2}\)가 있는 반면, Mistral은 MLP층에 더 큰 \(10.5d^{2}\)를 할당한다.
4. 미스트랄은 GPT2와 달리 그룹-쿼리 주의를 촉진한다(예를 들어, 4개의 그룹을 사용하여, 따라서 K/V 매트릭스를 크기가 \(d^{2}/4\)로 감소시킨다). LLaMA는 70B 변형과 같은 매우 큰 모델에서 그렇지 않으면 다중 쿼리 주의를 선호하지 않는다.
5. LLaMA 및 미스트랄은 GPT2와 비교하여 상이한 토큰라이저를 이용하며, 미스트랄의 토큰라이저는 LLaMA의 토큰라이저와 거의 동일하다.
6. GPT2는 \(\sigma=gelu\)를 사용하고, LLaMA/Mistral은 \(\sigma=silu\)를 사용한다.
7. GPT2는 LLaMA/Mistral은 그렇지 않은 훈련가능한 바이어스를 갖는 층 정규화를 통합한다.

이러한 차이점을 고려할 때, LLaMA 모델의 경우, \(\ell\) 레이어, \(h\) 헤드 및 \(64h\) 숨겨진 차원에 대해 LLaMA-\(\ell\)-\(h\) 표기를 사용하며, LLaMA가 70B 모델에 대해서만 권장하므로 그룹 쿼리 주의를 생략한다. Mistral-\(\ell\)-\(h\)로 표시되는 미스트랄의 경우 \(h=0\pmod{4}\)인 경우 4개 그룹, 홀수 \(h\)인 경우 1개 그룹 또는 그렇지 않은 경우 2개 그룹으로 그룹 쿼리 주의를 사용할 수 있습니다.

Smaller MLP.Mistral이 있는 GPT2는 더 큰 MLP 층을 가지며, 종종 MLP 층은 어텐션 층과 대조적으로 주로 지식을 저장하는 역할을 한다고 믿어진다. 하지만 이게 정말 사실인가요?

이를 조사하기 위해 MLP 층이 있는 GPT2\({}_{1/4}\)와 MLP 층이 없는 GPT2\({}_{0}\)를 조사한다.

실험 설정.이 섹션 전체에서 긍정적인 결과(예: GPT2)를 제시할 때 하나의 고정된 학습률 선택 세트를 고수하려고 하지만 부정적인 결과(예: LLaMA 아키텍처)를 제시할 때 세 가지 학습률 선택 중 가장 좋은 결과를 제시한다.

### 1000-Exposure Setting

1000 노출 설정에서 모델 아키텍처 선택이 스케일링 법칙에 _ 무시할 수 있는 영향을 미친다는 것을 관찰합니다. LLaMA, 미스트랄, GPT2\({}_{0}\) 및 GPT2\({}_{1/4}\) 아키텍처에 대한 결과는 그림 3에 나와 있으며 매개변수 선택은 아래에서 논의된다.

파라미터 6(그림 3).1000 노출 설정에서 LLaMA/미스트랄 모델의 경우 매개변수 1에 지정된 것과 유사한 매개변수를 사용하지만 GPT2가 최상의 튜닝된 LLaMA/미스트랄 모델보다 _나쁘지 않음_ 을 더 잘 수행한다는 것을 입증하기 위해 세 가지 학습 속도 중 최고를 선택합니다.

* \(N=10K\)의 경우 fp16과 함께 \(wd=0.02\), \(lr=0.0005/0.001/0.002\) 및 배치 크기 24를 사용합니다.
* \(N=20K\)의 경우 \(wd=0.02\), \(lr=0.0005/0.001/0.002\) 및 배치 크기 48을 fp16으로 사용하고 * \(N=50K\)의 경우 \(wd=0.02\), \(lr=0.0005/0.001/0.002\) 및 배치 크기 96을 fp16으로 사용합니다.
* \(N=100K,200K\)의 경우 fp16과 함께 \(wd=0.02\), \(lr=0.0005/0.001/0.002\) 및 배치 크기 192를 사용합니다.
* \(N=500K,1M\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.0002/0.0003/0.0005\) 및 배치 크기 192를 사용합니다.
* \(N=2M\)의 경우 bf16과 함께 \(wd=0.005\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 1536을 사용합니다.
* \(N=5M\)의 경우 bf16과 함께 \(wd=0.002\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 1536을 사용합니다.
* \(N=10M\)의 경우 bf16과 함께 \(wd=0.001\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 1536을 사용합니다.

GPT\({}_{0}\)와 GPT\({}_{1/4}\)의 경우 파라미터 1에 명시된 것과 동일한 학습률을 사용한다.

_Remark B.1_(게이트 MLP 상의 bf16).: 섹션 B.2에서 논의된 바와 같이, LLaMA 및 미스트랄 아키텍처의 트레이닝은 GatedMLP의 사용으로 인해 덜 안정적이며, 요구될 때 (혼합-정밀) bf16 트레이닝으로 스위칭할 필요성을 유도한다.

그림 3에서 작은 모델을 제외하고 LLaMA, 미스트랄, GPT2\({}_{0}\) 및 GPT2\({}_{1/4}\) 아키텍처는 1000개 노출에 걸쳐 GPT2의 스케일링 법칙을 밀접하게 따름을 알 수 있다. \(\leq 10M\) 매개변수가 있는 작은 모델의 경우 모델 가중치를 묶으면 용량이 증가한다(그림 3(c) 참조). 이것은 _2비트/파라미터 용량 비율이 가장 전형적인 (디코더 전용) 언어 모델 아키텍처 중 비교적 보편적인 법칙임을 나타낸다.

### 100-Exposure Setting

100 노출 설정은 더 흥미로운 비교를 보여줍니다. 우리는 GPT2를 그림 4의 다양한 모델 아키텍처와 대조하고 그림 5의 LLaMA 및 GPT2 아키텍처 간의 자세한 비교를 제공한다.

그림 4(b)는 LLaMA 아키텍처가 더 큰 모델의 경우에도 GPT2의 스케일링 법칙에 1.3배 뒤처질 수 있음을 보여준다.

우리는 이것의 이면에 있는 이유를 조사한다. 도 5에 도시된 바와 같이, LLaMA의 아키텍처를 조정함으로써(예를 들어, GatedMLP를 정상 MLP로 다시 전환함으로써), 우리는 LLaMA의 GatedMLP를 표준 MLP로 대체하는 것이 GPT2의 스케일링 법칙을 매칭하기 위해 필요하다는 것을 발견한다. 특히, 강한 비교를 위해, GatedMLP를 사용할 때, 우리는 세 개의 학습률들 중에서 최상의 결과를 선택하는 반면, GPT2와 유사한 표준 MLP의 경우, 우리는 단일 학습률을 사용한다. 더 작은 모델들의 경우, GPT2를 매칭하는 것은 모델 가중치들을 묶고 GPT2의 토큰나이저를 채택해야 하지만, 이것은 덜 유의하다.26

각주 26: 모델 용량에 대한 토큰화기의 영향은 주목할 만하다. 예를 들어, LLaMA/Mistral 토큰화기는 생일 연도를 한 자리 토큰으로 분할하여 더 작은 모델의 훈련을 약간 늦추는 경향이 있는 반면, GPT2Tokenizer는 1991년과 같은 출생 연도에 단일 토큰을 사용합니다.

다른 모델 아키텍처인 Mistral, GPT2\({}_{0}\) 및 GPT2\({}_{1/4}\)의 경우, 100-노출 설정에서 이들의 스케일링 법칙이 그림 4에 제시된다. 도 4(c)는 Mistral 아키텍처가 게이트 MLP의 사용으로 인해 GPT2도 저성능임을 확인한다. 그림 4(d)는 GPT2\({}_{1/4}\)의 MLP 레이어 크기를 4분의 1로 줄이는 것이 모델 용량에 _ 무시할 수 있는 영향을 미친다는 것을 보여준다. 그러나 GPT2\({}_{0}\)에서 MLP 층을 완전히 제거하면 모델의 용량이 크게 감소하므로 그림 4(e)를 참조하십시오.

100 노출 설정은 "불충분한 훈련" 패러다임을 나타냅니다. 따라서, 비교들은 (도 3에 도시된 바와 같이, 1000-노출 설정에서 유사한 용량 비율들을 달성하기 때문에) 하나의 아키텍처가 다른 아키텍처보다 엄격하게 더 나쁜 것에 관한 것이 아니다. 연구 결과에 따르면 일부 아키텍처는 학습하기 매우 쉽습니다(따라서 지식을 더 빨리 배울 수 있음)._

* GatedMLP 아키텍처는 모델의 학습 속도를 _느리게 합니다. 27 각주 27: 예를 들어 100M보다 작은 LLaMA/미스트랄 모델에 대해 혼합 정밀도 fp16 훈련이 실패할 수 있으므로 대신 혼합 정밀도 bf16을 사용합니다. 반대로, 1B까지의 GPT2 모델은 fp16으로 훈련될 수 있다.
* MLP 레이어를 제거하면 모델의 학습 속도가 완전히 느려지는 반면, MLP 레이어의 크기(예: \(8d^{2}\)에서 \(10.5d^{2}\) 또는 \(2d^{2}\)로 하향 조정)는 큰 영향을 미치지 않을 수 있습니다.

또한, 그림 5와 유사한 방식으로 LLaMA의 계층 규범에서 훈련 가능한 편향을 가능하게 하고 \(silu\)에서 \(gelu\)로 전환(GPT2와 더 유사)하는 것을 실험했지만 이러한 변화가 모델의 용량에 영향을 미치지 않는다는 것을 발견했다. 우리는 명확성을 위해 그 실험들을 무시한다.

아래에서는 그림 4와 그림 5의 실험에 대한 매개변수 선택에 대해 논의한다.

**매개 변수 7** (그림 4).: 100 노출 설정에서,

1. \(\mathsf{bio5}(N)\) 데이터에 대한 LLaMA/Mistral 모델의 경우 _음성_ 결과를 나타내기 위해 각 데이터 설정의 세 가지 옵션에서 최상의 학습 속도를 선택합니다. * \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 12를 bf16과 함께 사용하고; * \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 24를 bf16과 함께 사용하고; * \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0002/0.0003/0.0005/0.001\) 및 배치 크기 48을 bf16과 함께 사용하고; * \(N=2M\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.0005/0.001\) 및 배치
2. GPT2\({}_{1/4}\)의 경우: * \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0005/0.001\) 및 배치 크기 12를 fp16으로 사용하고; * \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0005/0.001\) 및 배치 크기 24를 fp16으로 사용하고; * \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0005/0.001\) 및 배치 크기 48을 fp16으로 사용하고; * \(N=500K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005\) 및 배치 크기 96을 fp16으로 사용하고; * \(N=1M\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005\) 및 배치 크기 192를 fp16으로 사용합니다.
3. GPT2\({}_{0}\)의 경우 매개 변수 4(a)와 동일한 설정을 사용합니다. * \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 bf16의 배치 크기 12를 사용합니다. * \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 bf16의 배치 크기 24를 사용합니다. * \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 bf16의 배치 크기 48을 사용합니다. * \(N=500K\)의 경우 \(wd=0.01\), \(lr=0.0002/0.0003/0.0005\) 및 bf16의 배치 크기 96을 사용합니다. * \(N=1M\)의 경우 \(wd=0.01\), \(lr

**매개변수 8** (그림 5).: 100-노출 제어 비교 실험에서,

* _부정_ 결과를 제시하기 위해(그림 5(a) 및 그림 5(c)) 매개변수 4(c)의 GPT2\({}_{0}\)와 동일한 세 가지 옵션에서 최상의 학습률을 선택합니다.
* _긍정_ 결과를 제시하기 위해(그림 5(b) 및 그림 5(d)) 매개변수 2와 동일하지만 더 강력한 비교를 위해 fp16이 bf16으로 대체된 단일 학습 속도 세트를 사용합니다.

[MISSING_PAGE_EMPTY:30]

그림 15: **1000 노출에 대한 \(\mathsf{bioD}(N,K,C,D,L,T)\) 데이터에 대해 훈련된 GPT2 모델의 8비트/4비트 양자화입니다. 왼쪽:** 그림 2와 동일하며 정확도의 모델만 표시 \(\geq 50\%\); **중간:** 8비트로 양자화한 후; **오른쪽:** 모든 정확도의 모델을 포함하여 4비트로 양자화한 후.

## 전문가 혼합물에 대한 부록 D 더

우리는 GPT2 모델[18]에서 Mixture-of-Experts(MoE)를 구현하기 위해 튜텔 패키지를 활용한다. MoE에서 매개변수 \(topk\)는 각 토큰이 라우팅되는 전문가의 수를 결정합니다. 일부 실무자는 교육 중 \(topk=2\)을 사용하고 테스트 중 \(topk=1\)을 사용하는 것이 좋다. 또한 \(cap\_factor\) 매개 변수는 \(M\) 전문가가 주어진 경우 각 전문가가 데이터의 \(\frac{cap\_factor}{M}\) 분수를 초과하지 않도록 합니다.

\(topk=1\) 및 \(cap\_factor=1\)를 사용하는 것은 일반적으로 권장되지 않습니다. 따라서 가장 강력한 결과를 제공하기 위해 그림 16의 1000/100 노출 크기 조정 법칙에 대해 \(topk=1,cap\_factor=2\)을 설정했다. (테스트 중 용량 인자를 \(cap\_factor=8\)로 증가시킨다.)

100-노출 스케일링 법칙의 경우, 그림 17과 같이 \((topk,cap\_factor)=(1,2),(2,1),(2,2)\)의 세 가지 구성을 추가로 비교하며, 이들 사이의 최소 차이를 찾는다. 섹션 7에서 모델 아키텍처의 차이는 보통 불충분한 훈련 체제에서 명백해진다는 것을 기억하라; 이것이 우리가 1000-노출 대신에 100-노출을 선택하는 이유이다. 특히 \((topk,cap\_factor)=(2,2)\)는 32명의 전문가와 함께 GPT2-16-4와 같은 심층 모델에 대해 가장 우수한 성능을 보인다.

MoE 모델은 희소성 때문에 밀집 모델에 비해 높은 학습률을 요구하는 경우가 많다. 결과적으로 다음과 같이 최적화 매개 변수를 조정합니다.

**매개 변수 9** (그림 16, 그림 17).: 32명의 전문가가 있는 GPT2-MoE 모델의 1000 노출 설정에서 다른 매개 변수를 매개 변수 1과 거의 동일하게 유지하면서 학습 속도를 약간 높입니다.

* \(N=10K\)의 경우 fp16과 함께 \(wd=0.02\), \(lr=0.001/0.002\) 및 배치 크기 24를 사용합니다.
* \(N=20K\)의 경우 fp16과 함께 \(wd=0.02\), \(lr=0.001/0.002\) 및 배치 크기 48을 사용합니다.
* \(N=50K\)의 경우 fp16과 함께 \(wd=0.02\), \(lr=0.001/0.002\) 및 배치 크기 96을 사용합니다.
* \(N=100K,200K\)의 경우 fp16과 함께 \(wd=0.02\), \(lr=0.001/0.002\), 배치 크기 192를 사용합니다.
* \(N=500K,1M\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.0005/0.001\), 배치 크기 192를 사용합니다.
* \(N=2M\)의 경우 fp16과 함께 \(wd=0.005\), \(lr=0.002\) 및 배치 크기 1536을 사용합니다.
* \(N=5M\)의 경우 fp16과 함께 \(wd=0.002\), \(lr=0.0005\) 및 배치 크기 1536을 사용합니다.
* \(N=10M\)의 경우 fp16과 함께 \(wd=0.001\), \(lr=0.0005\) 및 배치 크기 1536을 사용합니다.

100-노출 설정에서, 또한 파라미터 2에 비해 더 높은 학습 레이트를 사용한다:

* \(N=50K\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.001/0.002/0.005\) 및 배치 크기 12를 사용합니다.

그림 16: 바이오S(\(N\)) 데이터에 대해 32명의 전문가가 있는 전문가 혼합 GPT2 모델에 대한 스케일링 법칙이다.

* \(N=100K\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.001/0.002/0.005\) 및 배치 크기 24를 사용합니다.
* \(N=200K\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.001/0.002/0.005\) 및 배치 크기 48을 사용합니다.
* \(N=500K\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.001/0.002\) 및 배치 크기 96을 사용합니다.
* \(N=1M\)의 경우 fp16과 함께 \(wd=0.01\), \(lr=0.0005/0.001/0.002\) 및 배치 크기 192를 사용합니다.
* \(N=2M\)의 경우 fp16과 함께 \(wd=0.005\), \(lr=0.0005/0.001\) 및 배치 크기 192를 사용합니다.
* \(N=5M\)의 경우 fp16과 함께 \(wd=0.005\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 384를 사용합니다.

## 정크 데이터에 대 한 추가 부록 E. 스케일링 법칙

섹션 10에서 우리의 데이터 세트는 다양한 \(N\)(유용한 데이터라고 함)에 대해 \(\mathsf{bioS}(N)\)에서 토큰의 1/8이 나오는 혼합물임을 상기하고 나머지 7/8은 "정크 데이터"에서 세 가지 시나리오를 탐색했다.

1. Junk data is \(\mathsf{bioS}(N^{\prime})\) for \(N^{\prime}=100M\), representing completely random junk;
2. Junk data is \(\mathsf{bioS}(N^{\prime})\) for \(N^{\prime}=1K\) representing highly repetitive data; and
3. Junk data is \(\mathsf{bioS}(N^{\prime})\) for \(N^{\prime}=100M\) but with the special token appended to the front of each piece of _useful data_.28

각주 28: 이것은 데이터의 시작 부분에 wikipedia.org와 같은 도메인 이름을 추가하는 것과 유사하며, 이 모델은 이러한 특별한 토큰 데이터가 고품질의 유용한 데이터를 의미한다는 사전 지식이 부족하다. 이를 자동으로 발견하는 것은 모델과 훈련 과정에 달려 있습니다.

단순화를 위해, 각각의 512-토큰 컨텍스트 윈도우 내에서, 우리는 유용한 데이터만을 포함하거나 정크 데이터만을 포함한다(<EOS> 토큰으로 분리됨). 동일한 컨텍스트 창에서 유용한 데이터와 정크 데이터를 혼합할 때 결과는 유사합니다. 세 가지 경우 모두에서 처음에는 유용한 데이터가 사전 훈련 중에 각각 100개의 노출을 받는 100-노출 훈련 설정을 고려합니다. 따라서 총 훈련 토큰 수는 그림 1(b)보다 약 8배 더 많습니다(정크 데이터가 없는 100-노출 사례에 대한 스케일링 법칙).

(A)의 경우 _부정적인 결과_를 제시하면 300-노출, 600-노출 및 1000-노출 훈련 설정도 탐색합니다. 1000-노출 설정에는 그림 1(b)에 비해 48배 더 많은 훈련 토큰이 필요하거나 그림 1(a)에 비해 4.8배 더 필요하다는 점을 감안할 때 계산 자원을 절약하기 위해 \(\mathsf{bioS}(N)\)와 \(N\leq 200K\)로 실험을 제한했다. 마찬가지로 300-노광과 600-노광에 대해서는 \(N\leq 500K\)만을 고려하였다.

(B)의 경우, _양성 결과_를 나타내는 경우, 우리는 \(N\leq 1M\)을 사용한 100-노출로 고려를 제한했다.

그림 17: \(\mathsf{bioS}(N)\) 데이터에 대해 32명의 전문가가 있는 GPT2 MoE 모델에 대한 스케일링 법칙 \(\mathsf{100}\) 노출. 이 그림은 100-노출 불충분하게 훈련된 레짐에서 다양한 _topk_ 및 _cap_factor_의 효과를 비교하여 그림 16을 보완한다. **결론:** 더 깊은 모델(예: 32명의 전문가가 있는 GPT2-16-4)이 _topk_ = _cap_factor_ = 2로 훈련하는 것이 더 쉬워 보이지만 이러한 설정에서 최소한의 차이가 관찰됩니다.

(C)의 경우 _중간 정도의 긍정적인 결과_를 제시하여 100-노출 및 300-노출 설정을 모두 탐색했으며, 여기서 300-노출 설정에서는 다시 \(N\leq 500K\)로 제한했다.

전반적으로 100, 300, 600 및 1000 노출 설정에 걸쳐 상당히 다른 훈련 기간(즉, 훈련 토큰 수)으로 인해 배치 크기, 무게 감소 및 학습 속도를 적절하게 조정해야 했다. 이러한 조정은 아래에서 논의된다.

**매개 변수 10**(그림 8).: 그림 8에 표시된 모든 실험에서 매개 변수를 선택하기 위해 비고 A.2에 제공된 일반적인 조언을 준수합니다. 부정적인 결과(예: 그림 8(b), 8(c))의 경우, 훈련 가능한 단계 수를 늘리기 위해 더 작은 배치 크기를 선택하고 더 넓은 범위의 학습 속도 옵션을 탐색했습니다. 반대로, 긍정적인 결과(예를 들어, 그림 8(f), 8(e))에 대해, 우리는 때때로 더 빠르고 GPU 가속화된 트레이닝 시간으로부터 이익을 얻기 위해 더 큰 배치 크기를 선택하고 더 좁은 세트의 학습 속도 선택을 고려했다. 전반적으로 긍정적인 결과를 의도적으로 동일한 정도로 최적화하지 않으면서 부정적인 결과를 최대한 강화하기 위해 모수를 세심하게 선택했다. 이 방법은 _강력한 비교_ 를 보장 하 고이 섹션의 키 메시지를 효과적으로 전달 합니다. **구체적으로,**

* 100-노출의 경우(a)인 도 8(b)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 12를 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 24를 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 48을 사용합니다.
* \(N=500K\)의 경우 \(wd=0.005\), \(lr=0.00005/0.0001/0.0002/0.0003/0.0005\) 및 배치 크기 192를 사용합니다.
* \(N=1M\)의 경우 \(wd=0.005\), \(lr=0.00005/0.0001/0.0002/0.0003/0.0005\) 및 배치 크기 192를 사용합니다.
* 300-노출의 경우(a)인 도 8(c)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 96을 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 192를 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 192를 사용합니다.
* \(N=500K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 192를 사용합니다.
*600-노광의 경우(a)인 도 8(d)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 384를 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 384를 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 384를 사용합니다.
* \(N=500K\)의 경우 \(wd=0.002\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 768을 사용합니다.
* 1000-노광의 경우(a)인 도 8(e)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0005/0.001\) 및 배치 크기 384를 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0005/0.001\) 및 배치 크기 768을 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0005/0.001\) 및 배치 크기 1536을 사용합니다.
* 100-노광의 경우(b)인 도 8(f)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005\) 및 배치 크기 12를 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005\) 및 배치 크기 24를 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 96을 사용합니다.
* \(N=500K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005\) 및 배치 크기 192를 사용합니다.
* \(N=1M\)의 경우 \(wd=0.01\), \(lr=0.0003\) 및 배치 크기 192를 사용합니다.
* 100-노광의 경우(c)인 도 8(g)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 12를 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 24를 사용합니다.
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0002/0.0003/0.0005/0.001\) 및 배치 크기 96을 사용합니다.
* \(N=500K\)의 경우 \(wd=0.005\), \(lr=0.0002/0.0003/0.0005\) 및 배치 크기 192를 사용합니다.
* \(N=1M\)의 경우 \(wd=0.005\), \(lr=0.0002/0.0003/0.0005\) 및 배치 크기 192를 사용합니다.
* 300-노출의 경우(c)인 도 8(h)에 대하여:
* \(N=50K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 96을 사용합니다.
* \(N=100K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 192를 사용합니다.
* For \(N=10
* \(N=200K\)의 경우 \(wd=0.01\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 192를 사용합니다.
* \(N=500K\)의 경우 \(wd=0.005\), \(lr=0.0003/0.0005/0.001\) 및 배치 크기 384를 사용합니다.

## 부록 F Proof of Theorem 3.2

우리는 이 변수들이 특정 기준값과 일치할 확률을 기반으로 랜덤 변수를 인코딩하는 데 필요한 비트 복잡도를 설정하는 중요한 보조 문제를 제시한다.

**Lemma F.1**.: _\(\mathcal{Q}_{1},\ldots,\mathcal{Q}_{k}\)를 고정 집합(도메인 호출)으로 설정하고 각 \(i\in[k]\)에 대해 \(Q_{i}\)가 해당 도메인 \(\mathcal{Q}_{i}\)에서 독립적으로 무작위로 선택된다고 가정합니다. \(Q=(Q_{1},\ldots,Q_{k})\)를 훈련 데이터로 표시하고 \(Q\)를 봅니다._

_함수 \(W(Q)\in\mathcal{W}\)가 존재한다고 가정하며, 이는 훈련 데이터로부터 계산된 (즉, 훈련된) 모델의 파라미터로 간주한다 \(Q\)._

_더 나아가,_를 예측하는 평가 함수 \(F_{i}\)를 고려한다.

\[\forall i\in[k]\colon\qquad P_{i}=F_{i}(W(Q),Q_{1},Q_{2},\cdots,Q_{i-1},R) \quad\text{with}\quad p_{i}(Q)\stackrel{{\text{\tiny def}}}{{{=}} \mathbf{Pr}[P_{i}=Q_{i}\mid Q]\enspace.\]

_Here, \(F\)는 \(W(Q)\)에 의해 파라미터화되고 이전 데이터 \(Q_{1},\ldots,Q_{i-1}\) 및 새로운 랜덤성 \(R\)에 의존할 수 있다. 그러면, 그_

\[\log|\mathcal{W}|\geq\sum_{i\in[k]}\log\left(\operatorname*{\mathbb{E}}_{Q}[p _{i}(Q)]\times|\mathcal{Q}_{i}|\right)\geq\operatorname*{\mathbb{E}}_{Q}\Big{[} \sum_{i\in[k]}\log\left(p_{i}(Q)\times|\mathcal{Q}_{i}|\right)\Big{]}\enspace.\] (F.1)

Lemma F.1의 증명: (F.1)의 두 번째 부등식은 얀센의 부등식에서 온 것이기 때문에 우리는 첫 번째 부등식만 증명한다.

우리는 \(i=1\)일 때, \(P_{1}=F_{1}(W(Q),R)\)가 있고, \(\forall R\), \(P_{1}=F_{1}(W(Q),R)\)가 기껏해야 \(|\mathcal{W}|\)의 값을 갖는 성질을 이용하여 간단한 카운팅 인수로 정제를 증명할 수 있다.

\(i\geq 2\)일 때, 우리는 데이터 포인트 \(Q_{1},Q_{2}\)를 새로운 데이터 포인트 \(Q^{\prime}\)로 병합할 수 있다 \(\mathcal{Q}^{\prime}=\mathcal{Q}_{1}\times\mathcal{Q}_{2}\). 함수 \(F_{1},F_{2}\)로부터 \(P^{\prime}=(P_{1},P_{2})\)를 구성할 수 있다 \(R_{1}=F_{1}(W(Q),R)\)을 샘플링하여 \(P_{1}=F_{1}(W(Q),R)\)을 생성한 다음, 독립적인 \(R_{2}\)을 샘플링하여 \(P_{2}=F_{2}(W(Q),Q_{1},R)\)을 생성한다. 우리는 \(\mathbf{Pr}_{R_{1},R_{2}}[P^{\prime}=Q^{\prime}\mid Q]=\mathbf{Pr}_{R_{1}}[P_ {1}=Q_{1}\mid Q]\,\mathbf{Pr}_{R_{2}}[P_{2}=Q_{2}\mid Q]=p_{1}(Q)\cdot p_{2}(Q)\를 알고 있다. 이제 부제는 다음과 같은 신분을 사용하여 다음과 같이 된다.

\[\log(p_{1}(Q)|\mathcal{Q}_{1}|)+\log(p_{2}(Q)|\mathcal{Q}_{2}|)=\log(p_{1}(Q) p_{2}(Q)|\mathcal{Q}_{1}||\mathcal{Q}_{2}|)\enspace.\qed\]

### Warmup Examples

먼저 렘마 F.1의 두 가지 준비운동을 보자.

값 전용. \(g_{1},\ldots,g_{N}\in[T]\)을 허용합니다. 여기서 각 \(g_{i}\)는 \([T]\)에서 무작위로 균일하게 선택됩니다. 이 값을 _값_ 이라고 생각 합니다. 학습 데이터 \(\mathcal{Z}=\big{(}g_{1},...,g_{N}\big{)}\)에 의해 파라미터화된 모델을 학습시킨다고 가정하자. 이 모델을 가정하면 주어진 인덱스 \(i\in[N]\)에 대해 \(g_{i}\)에 해당하는 랜덤 예측 \(f_{i}\)을 생성할 수 있다. 우리는 이 모델을 \(f_{i}(W(\mathcal{Z}),R)\)로 나타낼 수 있으며, 여기서 \(R\)는 랜덤성을 나타낸다. 이 시나리오에 대한 교차 엔트로피 손실은 (모든 가능한 트레이닝 데이터에 걸쳐 평균화된) 다음과 같이 표현된다.

\[\mathbf{loss}\stackrel{{\text{\tiny def}}}{{=}}\operatorname*{ \mathbb{E}}_{g}[\mathbf{loss}\stackrel{{\text{\tiny def}}}{{=}} \operatorname*{\mathbb{E}}_{g}\Big{[}\frac{1}{N}\sum_{i\in[N]}-\log\mathbf{Pr }[f_{i}=g_{i}]\Big{]}\geq 0\

이제 \(\mathcal{Q}_{1}=...\ mathcal{Q}_{N}=[T]\), \(Q_{i}=g_{i}\), \(P_{i}=f_{i}\). 우리는

\[\log|\mathcal{W}|\geq\operatorname*{\mathbb{E}}_{g}\Big{[}\sum_{i\in[N]}\log \mathbf{Pr}[f_{i}=g_{i}]+\log T\Big{]}=N\log T-N\operatorname*{\mathbb{E}}_{g} \mathbf{loss}(g)=\operatorname*{\mathbb{E}}_{g}N\log\frac{T}{e^{\mathbf{loss}(g )}}}\enspace.\ 베이스를 즉시 변경하면 \(\log_{2}|\mathcal{W}|\geq N\log_{2}\frac{T}{e^{\text{loss}}}}}\의 비트 복잡도 하한이 발생한다. 손실이 0에 가까워질수록, 이것은 비트 복잡도 상한과 일치한다.

Name-only. \(g_{1},\ldots,g_{N}\in[N_{0}]\)을 \([N_{0}]\)와 구별되는 원소라고 하자. \(N\)를 대체하지 않고 무작위로 균일하게 샘플링하고 _이름_으로 간주한다. 모델 \(f\), \(W\)에 의해 매개 변수화된 \(\mathcal{Z}=\big{(}g_{1},\ldots,g_{N}\big{)}\)이 데이터 세트 \(\mathcal{Z}=\big{(}g_{1},\ldots,g_{N}\big{)}\)에 학습되어 이름을 예측한다고 가정하자. 우리는 이것을 \(f(W(\mathcal{Z}),R)\)로 나타내며, 여기서 \(R\)는 랜덤성을 나타낸다. 이 시나리오에 대한 교차 엔트로피 손실은 다음과 같이 정의된다.

\Big{[}\frac{1}{N}\sum_{i\in[N]}-\log\underset{f}{\mathbf{Pr}}[f=g_{i}] \Big{]}\geq 0\}}{{=}}\mathbb{E}_{g} [\text{\bf loss}\stackrel{{\text{\tiny def}}}{{=}}\mathbb{E}_{g} [\text{\bf loss}(g)]\stackrel{{\text{\tiny def}}}{{=}}\mathbb{E}_{g} [\text{\bf loss}\stackrel{{\text{\tiny def}}}{{{=}}\mathbb{E}_{g} [\text{\bf loss}(g)]\stackrel{{\text{\tiny def}}}{{{=}}\mathbb{E}_{g} [\text{\bf loss}\stackrel{{\text{\tiny def}}}{{{=}}\math

Lemma F.1을 적용하기 위해 \(\mathcal{Q}_{1}=[N_{0}]\), \(\mathcal{Q}_{2}=[N_{0}-1]\)을 정의하고 \(\mathcal{Q}_{N}=[N_{0}-N+1]\)까지 계속한다. \(\mathcal{Q}_{1},\ldots,\mathcal{Q}_{N}\)에서 \((g_{1},\ldots,g_{N})\in[N]^{N}\)을 다음과 같이 랜덤하게 생성한 후 \((g_{1}=Q_{1}\); \(g_{2}\)에 대해 \(Q_{2}<Q_{1}\)이면 \(Q_{2}\)로 설정하고 그렇지 않으면 \(g_{2}=Q_{2}+1\); 그리고 일반적으로 \(g_{i}\)를 \([N_{0}]\setminus\{g_{1},\ldots,g_{i-1}\}\)에서 가장 작은 \(Q_{i}\)로 정의한다. 이 방법은 \(\mathcal{Z}=(g_{1},\ldots,g_{N})\)로 표시되는 \(\mathcal{Z}(Q)\)를 생성하는 대안적인 방법을 제공한다. 각 \(i\in[N]\)에 대해 다음과 같이 \(P_{i}\)를 정의한다. 첫째, 새로운 랜덤성을 이용하여 \(f=f(\mathcal{W}(\mathcal{Z}),R_{i})\)를 생성한다. \([N_{0}]\setminus\{g_{1},\ldots,g_{i-1}\}\)에서 \(s\)이 가장 작은 요소이면 \(P_{i}\stackrel{{\text{\tiny def}}}}{{=}}s\)을 설정하거나 \(\{g_{1},\ldots,g_{i-1}\}\) 중에서 \(\varnothing\)와 같은 특수 기호를 설정합니다. (**중요한 것은 \(P_{i}\)에 대한 이 정의는 \(g_{1},\ldots,g_{i-1}\)에 대한 지식을 필요로 하지만, 이는 Lemma F.1을 통해 \(P_{i}\가 \(Q_{1},\ldots,Q_{i-1}\)에 의존할 수 있으므로 허용됩니다.) 모든 고정 \(Q\)에 대해(따라서 고정 \(g\))

\,\underset {P_{i}}{\mathbf{Pr}}=\sum_{i\in[N]}\log\big{(}p_{i}}{\mathbf{Pr}}=\sum_{i\in[N]}\log\big{(}p_{i}=Q_{i}]\big{)}=\sum_{i\in[N]}\log\big{(}\,\underset {R_{i}}{\mathbf{Pr}}[f(\mathcal{W}(\mathcal{Z}),R_{i})=g_{i}]\big{)}=-N\text{ \bf loss}(g)\]

레마 F.1을 적용합니다.

이상적으로는 모델 \(f\)이 전체 훈련 집합 \(\{Z\}=(g_{1},\ldots,g_{N})\)을 완벽하게 기억할 수 있다면, 가장 좋은 손실 \(\text{\bf loss}(g)=\log N\)을 얻을 수 있다. 따라서, 모델이 이 트레이닝 세트를 완벽하게 학습할 수 있다면, 비트 복잡도 하한은 \(\log|\mathcal{W}|\geq N\log\frac{N_{0}-N}{N}\geq(1-o(1))N\log\frac{N_{0}}{N}\)에서 \(N\ll N_{0}\)을 만족한다.

### Main Proof

독자들이 이 증명을 진행하기 전에 섹션 F.1의 준비 운동 예제를 먼저 검토하는 것이 좋다.

정리 3.2의 증명: 먼저 Lemma F.1에서 \(\mathcal{Q}_{i}\)'s 영역을 구성하자.

1. \(\mathcal{Q}_{1}=[N_{0}]\), \(\mathcal{Q}_{2}=[N_{0}-1]\cdots\mathcal{Q}_{N}=[N_{0}-N+1]\).
2. 모든 \(j=0,\ldots,K-1\)에 대해 \(\big{(}\mathcal{Q}_{N+jD+1},\ldots\mathcal{Q}_{N+jD}\big{)}=\big{(}[T^{L}],[T^{L}-1],\ldots,[T^{L}-D+1]\big{)}\)로 한다.
3. \(\mathcal{Q}_{N+KD+1}=\cdots=\mathcal{Q}_{N+KD+NK}=[D^{C}]\)

각 \(Q_{i}\)는 \(\mathcal{Q}_{i}\)로부터 독립적으로 균일하게 랜덤하게 생성됨을 상기한다. 본 논문에서는 학습 데이터셋 \(\mathcal{Z}(Q)\)을 생성하는 방법을 제안한다.

1. \(\mathcal{N}=(n_{1},\ldots,n_{N})\)을 다음과 같이 구성한다 : \(n_{1}\)는 \(\mathcal{N}_{0}\)로부터 \(Q_{1}\)번째 이름이고, \(i>1\)는 \(\mathcal{N}_{0}\setminus\{n_{1},\ldots,n_{i-1}\}\)로부터 \(Q_{i}\)번째 이름이다.
2. 각 \(a^{\prime}\in[K]\)에 대해, let \(a\)는 \(a^{\prime}\)에서 \(a^{\prime}\)-th 속성이다. \(\mathcal{D}_{a}=(w_{1},\ldots,w_{D})\)을 다음과 같이 구성한다 : \(\mathcal{T}^{L}\)에서 \(Q_{N+(a^{\prime}-1)D+1}\)번째 원소, \(i>1\)에서 \(w_{i}\)를 \(\mathcal{T}^{L}\setminus\{w_{1},\ldots,w_{i-1}\}\)에서 \(Q_{N+(a^{\prime}-1)D+i}\)번째 원소이다.

3. \(n^{\prime}\)-th name \(n\)과 \(a^{\prime}\)-th attribute \(a\)에 대해, \(v^{\star}(n,a)=(v_{1},\ldots,v_{C})\in(\mathcal{D}_{a})^{C}\)의 값을 \(\mathcal{D}_{a}\)에서 각각의 \(v_{i}\)를 \(s_{i}\)-th element로 설정하여 할당한다. 여기서 정수열 \((s_{1},\ldots,s_{C}):=Q_{N+KD+(n^{\prime}-1)K+a^{\prime}}\in[D^{C}]\).

이것은 정의 2.2와 동일한 데이터세트 분포를 제공한다는 것을 쉽게 확인할 수 있다. 다음으로, \(Q\)가 고정되어 있는 경우(데이터세트 \(\mathcal{Z}\)가 고정되어 있는 경우), 주어진 모델 함수 \(F^{\top}(W(\mathcal{Z}),R)\)와 \(F^{\perp}(W(\mathcal{Z}),n,a,R)\)를 사용하여 \(P_{1},P_{2},\cdots,P_{N+KD+NK}\)를 구성한다.

**이름 부분** 이름 부분의 경우 "값 전용" 워밍업 예제의 접근법에 따라 \(i\in[N]\)에 대해 \(P_{i}\)를 구성합니다. 구체적으로, \(R_{i}\)를 새로운 랜덤성으로 하고, \(F^{\top}(W(\{Z\}),R_{i})\)가 \(\mathcal{N}_{0}\setminus\{n_{1},\ldots,n_{i-1}\}\)에 있는 \(s\)번째 원소와 일치하는 경우 \(P_{i}=s\)를 정의하며, \(\\{n_{1},\ldots,n_{i-1}\}\)에 속하는 경우 임의의 기호 \(\varnothing\)를 정의한다.29 "name-only" 워밍업 예제에서 분석을 채택하여 다음과 같은 결과를 얻었다.

주 29: 중요하게도, \(P_{i}\)는 \(n_{1},\ldots,n_{i-1}\)에 의존할 수 있다; 그러나, Lemma F.1은 \(P_{i}\)가 \(Q_{1},\ldots,Q_{i-1}\)에 의존하도록 허용하기 때문에, 이것은 허용가능하다.

\[\sum_{i\in[N]}\log\mathbf{Pr}_{P_{i}}[P_{i}=Q_{i}]=-N\mathbf{loss}_{name}( \mathcal{Z})\enspace.\ (F.2)

**다양성 파트.** 다양성 구성 요소의 경우 다음과 같이 \(P_{i}\)'를 구성합니다. 각 \(a^{\prime}\in[K]\)에 대해 let \(a\)는 \(\mathcal{A}\)에서 \(a^{\prime}\)-th 속성을 나타낸다. 우리는 처음에 \(F^{\perp}(W(\mathcal{Z}),n,a,R_{i})\)를 계산하여 \(P_{N+(a^{\prime}-1)D+i}\)를 형성하는데, 여기서 \(n\in\mathcal{N}\)는 무작위로 균일하게 선택된다. 이어서, \(F^{\perp}(W(\mathcal{Z}),n,a,R_{i})\)이 \(\mathcal{T}^{L}\setminus\{w_{1},\ldots,w_{i-1}\}\)에서 \(s\)번째 원소에 해당하는 경우, \(P_{N+(a^{\prime}-1)D+i}=s\로 설정하고, 그렇지 않은 경우 \(P_{N+(a^{\prime}-1)D+i}=\varnothing\).31

각주 30: 중요하게는, \(P_{N+(a^{\prime}-1)D+i}\)는 \(\mathcal{N}\)에 의존한다; 그러나, Lemma F.1은 \(P_{i}\)가 \(Q_{1},\ldots,Q_{i-1}\)에 의존하도록 허용하고, \(\mathcal{N}\)가 \(Q_{1},\ldots,Q_{N}\)에 의해 고유하게 결정되기 때문에, 이것은 허용가능하다.

이제, \(a\)를 \(a^{\prime}\)에서 \(\mathcal{A}\)번째 원소로 하자. \(Q\)를 고정된 것으로 간주하고, 무작위성은 \(P_{i}\)의 계산에서만 발생한다. \(Q\)는 \(w_{1},\ldots,w_{D}\)로 표시된 \(\mathcal{D}_{a}\에서 원소의 순서를 설정한다는 점에 유의하라. 우리는

[P_{N+(a^{\prime}-1)D+i}}{\mathbf{Pr}} [P_{N+(a^{\prime}-1)D+i}=Q_{N+(a^{\prime}-1)D+i}] =\sum_{i\in[D]}\log\underset{n\in\mathcal{N}}{\mathbf{Pr}} \[=\sum_{i\in[D]}\log\underset{n\in\mathcal{N}(W(\mathcal{Z}),n,a,R)=w_{i}\right]\right]\] \[=\sum_{w\in\mathcal{D}_{a}}\log\underset{n\in\mathcal{N}}{ \mathbf{Pr}}\left[\left[F_{1}^{\perp}(W(\mathcal{Z}),n,a,R)=w\right]\right]\]=: \spadesuit_{a}\

우리는 \(\mathcal{N}_{w,a}\)를 \(v^{\star}(n,a)=w\)가 되도록 \(n\in\mathcal{N}\)의 집합으로 나타내자. 우리는

\mathcal{N}_{w,a}|}\sum_{n\in\mathcal{N}_{w,a}\underset{R}{\mathbf{Pr}}\left[F_{1}^{\perp}(W( \mathcal{Z}),n,a,R)=w\right]-D\log N\] \[\overset{\textcircled{{Q}}}{\geq}\sum_{w\in\mathcal{N}_{w,a}|\mathcal{N}_{w,a}|\] \[\overset{\textcircled{{Q}}}{\geq}\sum_{w\in\mathcal{D}_{a}}}\frac {1}{|\mathcal{N}_{w,a}|}\sum_{n\in\mathcal{N}_{w,a}}\log\underset{R}{\mathbf{Pr}}\left[F_{1}^{\perp}(W( \mathcal{Z}),n,a,R)=w\right

위에서 1은 로그 함수의 단조성을 사용하고 2는 로그 함수의 볼록성을 사용한다. 단순 체르노프 바운드를 사용하면 모든 \(w\in D\)에 대해 긴 a \(N\((1-o(1))\frac{N}{D}\)를 볼 수 있다. 따라서 우리는 높은 확률로 알고 있다.

[=(1+o(1))D\frac{1}{N}\sum_{n\in\mathcal{N}}\log\mathbf{Pr}_{R} \left[F_{1}^{\perp}(W(\mathcal{Z}),n,a,R)=w \right]-D\log D-o(D)\] \[=(1+o(1))D\frac{1}{N}\sum_{n\in\mathcal{N}}\log\mathbf{Pr}_{R} \left[F_{1}^{\perp}(W(\mathcal{Z}),n,a,R)=v^{\star}(n,a)\right]-D\log D-o(D)\]

따라서, 모든 다이버시티 부분에 대해 합산하면, 우리는 (우리는 \(Q\)를 고정하고 따라서 \(\mathcal{Z}\)를 고정한다)

[\geq(1+o(1))D\frac{1}{NK}\sum_{n\in\mathcal{N}}\log\mathbf{Pr}_{R}\left[F_{1}^{\perp}(W(\mathcal{Z}),n,a,R)=v^{\star}(n,a) \right]-KD\log D-o(KD)\] \[=-(1+o(1))D\mathbf{loss}_{value1}(\mathcal{Z})-KD\log D-o(KD)\enspace.\] (F.3)

**값 부분.** 값 부분에 대해 다음과 같이 \(P_{N+KD+1},\ldots,P_{N+KD+NK}\)을 구성합니다. \(P_{N+KD+(n^{\prime}-1)K+a^{\prime}}\)의 경우 \(n\)은 \(\mathcal{N}\)에서 \(n^{\prime}\)번째 이름이고 \(a\)는 \(\mathcal{A}\)에서 \(a^{\prime}\)번째 속성이다. \(F^{\perp}(W(\mathcal{Z}),n,a,R)\)를 계산하고 각 \(i\in[C]\)에 대해 \(F_{i}^{\perp}(W(\mathcal{Z}),n,a,R)\)가 \(\mathcal{D}_{a}\)에서 \(s_{i}\)번째 원소가 되도록 해당 \(s_{1},\ldots,s_{C}\in[D]\)를 찾아보자. 발견되지 않으면 \(P_{N+KD+(n^{\prime}-1)K+a^{\prime}}=\varnothing\), 그렇지 않으면 \(P_{N+KD+(n^{\prime}-1)K+a^{\prime}}=(s_{1},\ldots,s_{C})\in[D^{C}]\).32

각주 32: 다시 중요한 것은 \(P_{N+KD+(n^{\prime}-1)K+a^{\prime}}\)는 \(\mathcal{N},\mathcal{D}_{a}\)에 의존하지만 \(Q_{1},\ldots,Q_{N+KD}\)의 값을 사용하여 계산할 수 있기 때문이다.

"값 전용" 워밍업 예와 동일한 간단한 논거를 따릅니다.

\mathcal{ Z},n,a,R)=v^{\star}(n,a)\right]\] \[=-NK\mathbf{loss}_{value}(\mathcal{Z})\] (F.4)

(F.2)(F.3) 및 (F.4)를 합하고 렘마 F.1을 적용하면 다음과 같습니다.

\[\log|\mathcal{W}|\geq\operatorname*{\mathbb{E}}_{\mathcal{Z}}\left[N\log \frac{N_{0}-N}{e^{\mathbf{loss}_{name}(\mathcal{Z})}}+NK\log\frac{D^{C}}{e^{ \mathbf{loss}_{value}(\mathcal{Z})}}+KD\log\frac{T^{L}-D}{De^{(1+o(1))\mathbf{ loss}_{value1}(\mathcal{Z})}}-o(KD)\right]\enspace.\]

이것은 정리 3.2의 증명을 마친다.

## 부록 G 누락 Remark

_Remark_ G.1.: 교과서, 특히 PreK-12 교육을 위해 설계된 교과서 간의 상당한 중복으로 인해 모든 영어 교과서에 포함된 총 지식량을 추정하는 것은 어려울 수 있다. 그러나, 우리는 다음과 같이 하려고 시도한다.

2023년 기사에 따르면 영국에 본사를 둔 교육 출판사인 피어슨 교육은 2021년에 가장 높은 수익을 보고했으며, 수익 면에서 Wiley와 McGraw Hill은 미국 최고의 두 교육 출판사이다.33

각주 33: [https://wordsrated.com/education-book-publishing-companies-statistics/](https://wordsrated.com/education-book-publishing-companies-statistics/) 2024년 3월에 액세스했습니다.

각주 34: eTextbooks의 전체 목록에 대한 [https://www.pearson.com/en-us/pearsonplus/search.html](https://www.pearson.com/en-us/pearsonplus/search.html) 및 하드 카피 책의 전체 목록에 대한 [http://www.myperssonstore.com/bookstore/browse.asp](http://www.myperssonstore.com/bookstore/browse.asp) 둘 다 2024년 3월에 액세스했습니다.

* 피어슨의 공식 웹 사이트는 2,100개 미만의 교과서를 나열합니다.34* Wiley의 공식 웹 사이트는 69,000개 미만의 교과서를 나열합니다.35 각주 35: [https://www.wiley.com/en-us/subjects](https://www.wiley.com/en-us/subjects) 2024년 3월에 액세스했습니다. 모든 하위 범주의 모든 책을 요약하기 위해 코드를 작성했습니다. 코드는 책을 두 번 셀 수 있으므로 안전한 상한선에 불과합니다. 2024년 3월에 액세스한 [https://www.wiley.com/learn/librarysolutions/online-books-purchase.html](https://www.wiley.com/learn/librarysolutions/online-books-purchase.html)에 언급된 "21,000" 온라인 책 대신 이 번호를 사용했습니다.
* McGraw Hill은 PreK-12 교육을 위한 22,000개 미만의 교과서를 나열하며, 그 중 다수는 상당한 내용이 중복됩니다(많은 부분이 미국 50개 주 중 하나에 맞게 조정됨).36 고등 교육을 위한 2,000개 미만의 교과서를 나열합니다.

각주 36: [https://www.mheducation.com/search.html?searchQuery=&page=1&sortby=title_desckorder=desc&bu=seg&TYPE=Products&PRODUCT_TYPE_PATH=_Student+Materials](https://www.mheducation.com/search.html?searchQuery=&page=1&sortby=title_desckorder=desc&bu=seg&TYPE=Products&PRODUCT_TYPE_PATH=_Student+Materials) 3월 2024일에 액세스합니다.

이러한 수치를 고려할 때, 모든 영어 교과서의 내용이 10만 권 이하의 교과서로 압축될 수 있다고 추정하는 것이 타당해 보인다. 책 한 권당 평균 16만 단어(예: 400페이지씩 400단어)를 가정하면 총 160억 단어에 이른다.

## References

* [1] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. _Advances in Neural Information Processing Systems_, 35:22300-22312, 2022.
* [2] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 1, Context-Free Grammar. _ArXiv e-prints_, abs/2305.13673, May 2023. Full version available at [http://arxiv.org/abs/2305.13673](http://arxiv.org/abs/2305.13673).
* [3] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. _ArXiv e-prints_, abs/2309.14316, September 2023. Full version available at [http://arxiv.org/abs/2309.14316](http://arxiv.org/abs/2309.14316).
* [4] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 3.2, Knowledge Manipulation. _ArXiv e-prints_, abs/2309.14402, September 2023. Full version available at [http://arxiv.org/abs/2309.14402](http://arxiv.org/abs/2309.14402).
* [5] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019.
* [6] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In _International conference on machine learning_, pages 242-252. PMLR, 2019.
* [7] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In _Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models_, 2022. URL [https://arxiv.org/abs/2204.06745](https://arxiv.org/abs/2204.06745).
* [8] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [9] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _The Journal of Machine Learning Research_, 23(1):5232-5270, 2022.
* [10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. _arXiv preprint arXiv:2210.17323_, 2022.
* [11] Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. Reverse training to nurse the reversal curse. _arXiv preprint arXiv:2403.13799_, 2024.
* [12] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.

* [13] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. _arXiv preprint arXiv:2010.14701_, 2020.
* [14] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_, 2021.
* [15] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. _arXiv preprint arXiv:1712.00409_, 2017.
* [16] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-Rank Adaptation of Large Language Models. In _ICLR_, 2021.
* [18] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive mixture-of-experts at scale. _CoRR_, abs/2206.03382, June 2022. URL [https://arxiv.org/pdf/2206.03382.pdf](https://arxiv.org/pdf/2206.03382.pdf).
* [19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [20] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_, 2017.
* [21] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* [22] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453-466, 2019.
* [23] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In _Advances in Neural Information Processing Systems_, 2018.
* [24] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_, 2023.
* [25] Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. _arXiv preprint arXiv:2305.16264_, 2023.
* [26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [27] Jonathan S Rosenfeld. Scaling laws for deep learning. _arXiv preprint arXiv:2108.07686_, 2021.
* [28] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. _arXiv preprint arXiv:1909.12673_, 2019.
* [29] Noam Shazeer. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.
* [30] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _International Conference on Learning Representations_, 2016.
* [31] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.
* [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [34] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skillmix: A flexible and expandable family of evaluations for ai models. _arXiv preprint arXiv:2310.17567_, 2023.

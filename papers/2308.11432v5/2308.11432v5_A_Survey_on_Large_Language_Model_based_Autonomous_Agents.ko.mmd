# 자율 에이전트 기반 대용량 언어 모델 조사

Lei Wang\({}^{1}\)

 천마\({}^{*}\)1

 쉐양펑\({}^{*}\)1

 제유장\({}^{1}\)

 하오양\({}^{1}\)

 징센장\({}^{1}\)

 즈위안천\({}^{1}\)

 지아카이탕\({}^{1}\)

 Xu Chen(\(\sqsubseteq\)1

 Yankai Lin(\(\sqsubseteq\)1

 Wayne Xin Zhao\({}^{1}\)

 즈웨이웨이\({}^{1}\)

 지롱문({}^{1}\)

중국 베이징 인민대학교 인공지능대학원, 100872

1

###### Abstract

자율 에이전트는 오랫동안 학술 및 산업 커뮤니티에서 연구 초점이 되어 왔다. 이전 연구는 종종 인간 학습 프로세스와 크게 다른 고립된 환경 내에서 제한된 지식을 가진 에이전트를 훈련하는 데 초점을 맞추고 에이전트가 인간과 유사한 결정을 달성하기 어렵게 만든다. 최근 방대한 양의 웹 지식 획득을 통해 대규모 언어 모델(LLM)이 인간 수준의 지능에서 잠재력을 보여주면서 LLM 기반 자율 에이전트에 대한 연구가 급증하고 있다. 본 논문에서는 이러한 연구에 대한 포괄적인 조사를 제시하여 LLM 기반 자율 에이전트를 총체적인 관점에서 체계적으로 검토한다. 우리는 먼저 이전 작업의 많은 부분을 포괄하는 통일된 프레임워크를 제안하면서 LLM 기반 자율 에이전트의 구성에 대해 논의한다. 그런 다음 사회 과학, 자연 과학 및 공학에서 LLM 기반 자율 에이전트의 다양한 적용에 대한 개요를 제시한다. 마지막으로, 우리는 LLM 기반 자율 에이전트에 일반적으로 사용되는 평가 전략을 조사한다. 선행 연구를 바탕으로 이 분야의 몇 가지 과제와 향후 방향도 제시한다.

자율 에이전트, 대용량 언어 모델, 인간 수준의 지능

## 1 Introduction

_"자율 에이전트는 그 환경을 감지하고 시간이 지남에 따라 자신의 의제를 추구하여 미래에 감지한 것에 영향을 미치도록 하는 환경의 내부 및 일부에 위치한 시스템입니다._

Franklin and Graesser (1997)

자율 에이전트는 오랫동안 인공 지능(AGI)을 달성하기 위한 유망한 접근법으로 인식되어 왔으며, 이는 자기 주도적인 계획과 행동을 통해 작업을 수행할 것으로 예상된다. 이전 연구에서 에이전트는 단순하고 휴리스틱한 정책 함수를 기반으로 행동한다고 가정하고 고립되고 제한된 환경에서 학습한다[1, 2, 3, 4, 5, 6]. 이러한 가정은 인간의 마음이 매우 복잡하고 개인이 훨씬 더 다양한 환경에서 학습할 수 있기 때문에 인간의 학습 과정과 크게 다르다. 이러한 공백으로 인해 이전 연구에서 얻은 에이전트는 일반적으로 인간 수준의 결정 프로세스를 복제하는 것과는 거리가 멀며, 특히 제약이 없는 개방형 도메인 설정에서 그렇다.

최근 몇 년 동안, 대형 언어 모델(LLM)은 주목할 만한 성공을 거두었으며, 인간과 같은 지능을 달성하는 데 상당한 잠재력을 보여주었다[5, 6, 7, 8, 9, 10]. 이 기능은 상당한 수의 모델 매개변수와 함께 포괄적인 훈련 데이터 세트를 활용함으로써 발생한다. 이러한 능력을 바탕으로 인간과 유사한 의사 결정 능력을 얻기 위해 자율 에이전트를 구성하기 위해 LLM을 중앙 제어기로 사용하는 연구 영역이 증가하고 있다[11, 12, 13, 14, 15, 16, 17].

강화 학습과 비교하여, LLM 기반 에이전트는 더 포괄적인 내부 세계 지식을 가지고 있으며, 이는 특정 도메인 데이터에 대한 훈련 없이도 더 많은 정보에 입각한 에이전트 동작을 용이하게 한다. 또한 LLM 기반 에이전트는 인간과 상호 작용할 수 있는 자연어 인터페이스를 제공할 수 있으며, 이는 보다 유연하고 설명 가능하다.

이러한 방향에 따라 연구자들은 수많은 유망한 모델을 개발했으며(이 분야의 개요는 그림 1 참조), 핵심 아이디어는 LLM에 메모리와 같은 중요한 인간 능력을 갖추고 LLM이 인간처럼 행동하고 다양한 작업을 효과적으로 완료하도록 계획하는 것이다. 이전에는 이러한 모델을 독립적으로 제안했으며 전체적으로 요약하고 비교하려는 노력은 제한적이었다. 그러나 우리는 체계적이라고 믿습니다.

그림 1: LLM 기반 자율 에이전트 분야의 성장 추이 그림. 2021년 1월부터 2023년 8월까지 출판된 논문의 누적 수를 제시한다. 다양한 에이전트의 카테고리를 표현하기 위해 서로 다른 색상을 할당한다. 예를 들어, 게임 에이전트는 게임 플레이어를 시뮬레이션하는 것을 목표로 하는 반면, 툴 에이전트는 주로 툴 사용에 중점을 둔다. 각 기간 동안 다양한 에이전트 범주를 가진 선별된 연구 목록을 제공한다.

빠르게 발전하고 있는 이 분야에 대한 요약은 이를 종합적으로 이해하고 향후 연구를 고무하는 데 큰 의미가 있다.

본 논문에서는 LLM 기반 자율 에이전트 분야에 대한 종합적인 조사를 수행한다. 구체적으로, 우리는 LLM 기반 자율 에이전트의 구성, 적용 및 평가를 포함한 세 가지 측면을 기반으로 설문조사를 구성한다. 에이전트 구축을 위해 두 가지 문제, 즉 (1) LLM을 더 잘 활용할 수 있도록 에이전트 아키텍처를 설계하는 방법과 (2) 서로 다른 작업을 완료할 수 있도록 에이전트 기능을 고취하고 향상시키는 방법에 중점을 둔다. 직관적으로, 첫 번째 문제는 에이전트에 대한 하드웨어 펀더멘털을 구축하는 것을 목표로 하는 반면, 두 번째 문제는 에이전트에게 소프트웨어 리소스를 제공하는 데 중점을 둔다. 첫 번째 문제에 대해 대부분의 이전 연구를 포괄할 수 있는 통합 에이전트 프레임워크를 제시한다. 두 번째 문제는 에이전트의 능력 획득을 위해 일반적으로 사용되는 전략에 대한 개요를 제공한다. 에이전트 구성에 대한 논의 외에도 사회 과학, 자연 과학 및 공학에서 LLM 기반 자율 에이전트의 적용에 대한 체계적인 개요를 제공한다. 마지막으로, 우리는 주관적 전략과 객관적 전략 모두에 초점을 맞춘 LLM 기반 자율 에이전트를 평가하기 위한 전략을 탐구한다.

요약하면, 이 조사는 LLM 기반 자율 에이전트의 급증하는 분야에서 기존 연구에 대한 체계적인 검토를 수행하고 포괄적인 분류를 설정한다. 우리의 초점은 에이전트 구성, 응용 프로그램 및 평가 방법의 세 가지 주요 영역을 포함한다. 풍부한 선행 연구를 바탕으로 이 분야의 다양한 과제를 식별하고 잠재적인 미래 방향에 대해 논의한다. 우리는 우리의 조사가 LLM 기반 자율 에이전트의 새로운 사람들에게 포괄적인 배경 지식을 제공하고 추가 획기적인 연구를 장려할 수 있을 것으로 기대한다.

## 2 LLM 기반 자율 에이전트 구성

LLM 기반 자율 에이전트는 LLM의 인간다운 능력을 활용하여 다양한 작업을 효과적으로 수행할 수 있을 것으로 기대된다. 이러한 목적을 달성하기 위해, (1) LLMs를 더 잘 사용하기 위해 어떤 아키텍처가 설계되어야 하는지와 (2) 설계된 아키텍처에 에이전트가 특정 작업을 수행하기 위한 능력을 획득하는 방법을 제공하는 두 가지 중요한 측면이 있다. 우리는 아키텍처 설계의 맥락에서 기존 연구의 체계적인 합성에 기여하여 포괄적인 통합 프레임워크로 정점에 도달한다. 두 번째 측면은 LLM을 미세 조정하는지 여부에 따라 에이전트 능력 획득 전략을 요약한다. LLM 기반 자율 에이전트를 전통적인 기계 학습과 비교할 때, 에이전트 아키텍처를 설계하는 것은 네트워크 구조를 결정하는 것과 유사하지만, 에이전트 능력 획득은 네트워크 파라미터를 학습하는 것과 유사하다. 이하에서는 이 두 가지 측면을 좀 더 구체적으로 소개한다.

### 에이전트 아키텍처 설계

최근 LLM의 발전은 질의 응답(QA)의 형태로 광범위한 작업을 수행할 수 있는 큰 잠재력을 보여주었다. 그러나 자율 에이전트를 구축하는 것은 특정 역할을 수행하고 인간처럼 자신을 진화시키기 위해 환경에서 자율적으로 인식하고 학습해야 하기 때문에 QA와 거리가 멀다. 전통적인 LLM과 자율 에이전트 사이의 격차를 해소하기 위해, 중요한 측면은 LLM의 능력을 최대화하는 데 도움이 되는 합리적인 에이전트 아키텍처를 설계하는 것이다. 이러한 방향에 따라 이전 작업에서는 LLM을 향상시키기 위한 여러 모듈을 개발했습니다. 본 절에서는 이러한 모듈들을 요약하기 위한 통합 프레임워크를 제안한다. 특히, 본 프레임워크의 전체 구조는 프로파일링 모듈, 메모리 모듈, 계획 모듈 및 액션 모듈로 구성된 그림 2와 같다. 프로파일링 모듈의 목적은 에이전트의 역할을 식별하는 것이다. 메모리 및 계획 모듈은 에이전트를 동적 환경으로 배치하여 과거의 행동을 회상하고 미래의 행동을 계획할 수 있게 한다. 액션 모듈은 에이전트의 결정을 특정 출력으로 변환하는 역할을 합니다. 이들 모듈 내에서 프로파일링 모듈은 메모리 및 계획 모듈에 영향을 미치고, 집합적으로 이들 세 모듈은 액션 모듈에 영향을 미친다. 다음에서는 이러한 모듈에 대해 자세히 설명합니다.

#### 2.1.1 프로파일링 모듈

자율 에이전트는 일반적으로 코더, 교사 및 도메인 전문가와 같은 특정 역할을 가정하여 작업을 수행한다[18, 19]. 프로파일링 모듈은 일반적으로 LLM 동작에 영향을 미치기 위해 프롬프트에 작성되는 에이전트 역할의 프로파일을 나타내는 것을 목표로 한다. 에이전트 프로필은 대표적으로 에이전트의 개성을 반영하는 심리 정보뿐만 아니라 나이, 성별, 경력 등의 기본 정보[20]와 에이전트 간의 관계를 상세하게 설명하는 사회 정보[21]를 포괄한다. 에이전트를 프로파일링하기 위한 정보의 선택은 주로 특정 애플리케이션 시나리오에 의해 결정된다. 예를 들어, 응용 프로그램이 인간의 인지 과정을 연구하는 것을 목표로 한다면 심리 정보는 중추적인 것이 된다. 프로파일 정보의 유형을 식별한 후, 다음으로 중요한 문제는 에이전트에 대한 특정 프로파일을 생성하는 것이다. 기존 문헌은 공통적으로 다음의 세 가지 전략을 사용한다.

**Handcrafting 방법**: 이 방법에서는 에이전트 프로필이 수동으로 지정됩니다. 예를 들어, 성격이 다른 에이전트를 디자인하고 싶다면 "당신은 외향적인 사람" 또는 "당신은 내성적인 사람"을 사용하여 에이전트를 프로파일링할 수 있습니다. 핸드 크래프팅 방법은 에이전트 프로파일을 나타내기 위해 많은 이전 작업에서 활용되었다. 예를 들어, 생성 에이전트 [22]는 이름, 목적 및 다른 에이전트와의 관계와 같은 정보에 의해 에이전트를 설명한다. MetaGPT [23], ChatDev [18] 및 Self-collaboration [24]는 소프트웨어 개발에서 다양한 역할과 그에 상응하는 책임을 미리 정의하며, 협업을 용이하게 하기 위해 각각의 에이전트에 별개의 프로파일을 수동으로 할당한다. PTLLM [25]는 LLM에 의해 생성된 텍스트에 표시되는 성격 특성을 탐색하고 정량화하는 것을 목표로 한다. 이 방법은 IPIP-NEO [26] 및 BFI [27]과 같은 성격 평가 도구를 사용하여 다양한 에이전트 캐릭터를 남성적으로 정의함으로써 LLMs가 다양한 응답을 생성하도록 안내한다. [28] 정치인, 언론인, 사업가와 같은 다양한 역할을 하는 LLM을 수동으로 유도함으로써 LLM 출력의 독성을 연구한다. 일반적으로 핸드크래프팅 방법은 에이전트에게 프로파일 정보를 할당할 수 있기 때문에 매우 유연하다. 그러나 특히 많은 수의 에이전트를 처리할 때 노동 집약적일 수도 있다.

**LLM 생성 방법**: 이 방법에서는 LLM을 기반으로 에이전트 프로필이 자동으로 생성됩니다. 전형적으로, 그것은 타겟 모집단 내의 에이전트 프로파일들의 구성 및 속성들을 설명하면서 프로파일 생성 규칙들을 표시하는 것으로 시작한다. 그런 다음, 몇 개의 시드 에이전트 프로파일을 선택적으로 지정하여 몇 개의 샷 예제로 사용할 수 있다. 마지막으로 LLM은 모든 에이전트 프로필을 생성하는 데 활용됩니다. 예를 들어 RecAgent [21]은 먼저 나이, 성별, 개인 특성 및 영화 선호도와 같은 배경을 수동으로 만들어 소수의 에이전트에 대한 시드 프로필을 만듭니다. 그런 다음 ChatGPT를 활용하여 시드 정보를 기반으로 더 많은 에이전트 프로필을 생성한다. LLM 생성 방법은 에이전트의 수가 많을 때 상당한 시간을 절약할 수 있지만 생성된 프로파일에 대한 정확한 제어가 부족할 수 있다.

**데이터 세트 정렬 방법**: 이 방법에서는 에이전트 프로필을 실제 데이터 세트에서 가져옵니다. 일반적으로 데이터 세트의 실제 인간에 대한 정보를 먼저 자연어 프롬프트로 구성한 다음 이를 활용하여 에이전트를 프로파일링할 수 있습니다. 예를 들어, [29]에서 저자는 미국 선거 연구(ANES) 참가자의 인구 통계학적 배경(인종/민족, 성별, 연령 및 거주 주와 같은)에 따라 GPT-3에 역할을 할당한다. 그들은 이후에 GPT-3가 실제 인간과 유사한 결과를 생성할 수 있는지 여부를 조사한다. 데이터세트 정렬 방법은 실제 모집단의 속성을 정확하게 캡처하여 에이전트 동작을 실제 시나리오를 보다 의미 있고 반영하게 한다.

_Remark_.: 이전 작업의 대부분은 위의 프로필 생성 전략을 독립적으로 활용하지만, 이를 결합하면 추가 이점이 발생할 수 있다고 주장합니다. 예를 들어, 에이전트 시뮬레이션을 통해 소셜 개발을 예측하기 위해 실제 데이터 세트를 활용하여 에이전트의 하위 집합을 프로파일링하여 현재 소셜 상태를 정확하게 반영할 수 있다. 이후 현실 세계에는 존재하지 않지만 미래에 나타날 수 있는 역할을 다른 에이전트에 수동으로 할당할 수 있어 미래의 사회 발전을 예측할 수 있다. 이 예를 넘어 다른 전략도 유연하게 결합할 수 있습니다. 프로파일 모듈은 에이전트 암기, 계획 및 실행 절차에 상당한 영향을 미치는 에이전트 설계의 기초가 된다.

#### 2.1.2 메모리 모듈

메모리 모듈은 에이전트 아키텍처 설계에서 매우 중요한 역할을 한다. 환경으로부터 인지되는 정보를 저장하고 기록된 기억을 활용하여 미래의 행동을 용이하게 한다. 메모리 모듈은 에이전트가 경험을 축적하고, 자기 진화하고, 보다 일관되고 합리적이며 효과적인 방식으로 행동하도록 도울 수 있다. 이 섹션에서는 메모리 모듈의 구조, 형식 및 동작에 초점을 맞춘 포괄적인 개요를 제공한다.

**메모리 구조**: LLM 기반 자율 에이전트는 일반적으로 인간의 기억 프로세스에 대한 인지 과학 연구에서 파생된 원칙과 메커니즘을 통합합니다. 인간의 기억은 지각적 입력을 등록하는 감각 기억에서 일시적으로 정보를 유지하는 단기 기억으로, 장기간에 걸쳐 정보를 통합하는 장기 기억으로 일반적인 진행을 따른다. 에이전트 기억 구조를 설계할 때 연구자들은 인간 기억의 이러한 측면에서 영감을 얻는다. 특히, 단기 메모리는 트랜스포머 아키텍처에 의해 제약된 컨텍스트 윈도우 내의 입력 정보와 유사하다. 장기 메모리는 에이전트가 필요에 따라 신속하게 쿼리하고 검색할 수 있는 외부 벡터 저장소와 유사합니다. 이하에서는 장단기 메모리를 기반으로 일반적으로 사용되는 두 가지 메모리 구조를 소개한다.

\(\bullet\)_Unified Memory_. 이 구조는 보통 상황 내 학습에 의해 실현되는 인간의 단기 기억만을 시뮬레이션하고, 기억 정보는 프롬프트에 직접 기입된다. 예를 들어, RLP[30]은 대화 에이전트로서, 화자 및 청취자에 대한 내부 상태를 유지한다. 각 대화 라운드 동안, 이러한 상태들은 LLM 프롬프트로서 작용하여 에이전트의 단기 기억으로서 기능한다. SayPlan[31]은 태스크 계획을 위해 특별히 설계된 구체화된 에이전트이다. 이 에이전트에서 장면 그래프와 환경 피드백은 에이전트의 단기 기억 역할을 하며 액션을 안내합니다. 칼립소[32]는 던전 앤 드래곤 게임을 위해 설계된 에이전트로, 던전 마스터가 이야기의 창작과 내레이션을 도울 수 있다. 단기 기억은 장면 설명, 몬스터 정보 및 이전 요약에 기반합니다. DEPS [33]도 게임 에이전트이지만 마인크래프트를 위해 개발되었다. 에이전트는 처음에 작업 계획을 생성한 다음 이를 활용하여 LLM을 프롬프트하고, LLM은 작업을 완료하는 작업을 생성합니다. 이러한 계획들은 대리인의 단기 기억으로 간주될 수 있다. 실제로 단기 기억을 구현하는 것은 간단하며 최근 또는 맥락적으로 민감한 행동 및 관찰을 인식하는 에이전트의 능력을 향상시킬 수 있다. 그러나 LLM의 컨텍스트 윈도우의 한계로 인해 모든 메모리를 프롬프트에 넣기가 어려워 에이전트의 성능을 저하시킬 수 있다. 이 방법은 LLM의 윈도우 길이와 긴 컨텍스트를 처리할 수 있는 능력에 대한 요구사항이 높다. 따라서 많은 연구자들은 이 질문을 완화하기 위해 하이브리드 메모리에 의존한다. 그러나 LLM의 제한된 컨텍스트 창은 포괄적인 메모리를 프롬프트에 통합하는 것을 제한하여 에이전트 성능을 손상시킬 수 있다. 이 도전은 더 큰 컨텍스트 창과 확장된 컨텍스트를 처리하는 능력을 가진 LLM을 필요로 한다. 결과적으로, 수많은 연구자들은 이 문제를 완화하기 위해 하이브리드 메모리 시스템에 의존한다.

\(\bullet\)_Hybrid Memory_. 이 구조는 인간의 단기 기억과 장기 기억을 명시적으로 모델링한다. 단기 기억은 일시적으로 최근의 인식을 완충시키는 반면, 장기 기억은 시간이 지남에 따라 중요한 정보를 통합한다. 예를 들어, 생성 에이전트 [20]은 에이전트 동작을 용이하게 하기 위해 하이브리드 메모리 구조를 채용한다. 단기 기억은 에이전트 현재 상황들에 대한 컨텍스트 정보를 포함하는 반면, 장기 기억은 현재 이벤트들에 따라 검색될 수 있는 에이전트 과거 행동들 및 생각들을 저장한다. AgentSims[34]는 또한 하이브리드 메모리 아키텍처를 구현한다. 상기 프롬프트에 제공된 정보는 단기로 간주될 수 있다

도. 2: LLM 기반 자율 에이전트의 아키텍처 설계를 위한 통일된 프레임워크.

메모리. 저자들은 메모리의 저장 용량을 향상시키기 위해 벡터 데이터베이스를 활용하여 효율적인 저장과 검색을 용이하게 하는 장기 메모리 시스템을 제안한다. 구체적으로, 에이전트의 일일 메모리는 임베딩으로 인코딩되어 벡터 데이터베이스에 저장된다. 에이전트가 이전의 기억을 회상할 필요가 있는 경우, 장기 기억 시스템은 임베딩 유사성을 사용하여 관련 정보를 검색한다. 이러한 과정은 에이전트의 행동의 일관성을 향상시킬 수 있다. GITM[16]에서, 단기 메모리는 현재 궤적을 저장하고, 장기 메모리는 성공적인 이전 궤적들로부터 요약된 참조 계획들을 저장한다. 장기 기억은 안정적인 지식을 제공하는 반면 단기 기억은 유연한 계획을 가능하게 한다. 반사 [12]는 최근 피드백을 캡처하기 위해 단기 슬라이딩 창을 활용하고 압축된 통찰력을 유지하기 위해 지속적인 장기 스토리지를 통합합니다. 이 조합은 상세한 즉각적인 경험과 높은 수준의 추상화를 모두 활용할 수 있도록 한다. SCM[35]은 단기 기억과 결합하기 위해 가장 관련성이 높은 장기 지식을 선택적으로 활성화하여 복잡한 맥락적 대화들에 대한 추론을 가능하게 한다. SimplyRetrieve[36]은 사용자 질의를 단기 메모리로 활용하고 외부 지식 베이스를 이용하여 장기 메모리를 저장한다. 이 디자인은 사용자 프라이버시를 보장하면서 모델 정확도를 높입니다. MemorySandbox[37]는 메모리 객체들을 저장하기 위해 2D 캔버스를 이용함으로써 장기 및 단기 메모리를 구현하며, 그 다음 다양한 대화들 전반에 걸쳐 액세스될 수 있다. 사용자는 동일한 캔버스에서 서로 다른 에이전트와 여러 대화를 생성할 수 있어 간단한 드래그 앤 드롭 인터페이스를 통해 메모리 객체의 공유를 용이하게 한다. 실제로 단기 기억과 장기 기억을 모두 통합하는 것은 복잡한 환경에서 작업을 수행하는 데 중요한 장거리 추론 능력과 가치 있는 경험의 축적을 위한 에이전트의 능력을 향상시킬 수 있다.

_Remark_.: 주의 깊은 독자들은 또한 다른 유형의 메모리 구조, 즉 단지 장기 메모리에 기초하여 존재할 수 있다는 것을 발견할 수 있다. 그러나 우리는 그러한 유형의 기억이 문헌에 거의 문서화되지 않는다는 것을 발견했다. 우리의 추측은 에이전트가 항상 연속적이고 역동적인 환경에 위치하고 연속적인 행동이 높은 상관 관계를 나타낸다는 것이다. 따라서 단기 기억의 포착은 매우 중요하며 보통 무시할 수 없다.

**메모리 형식**: 메모리 구조 외에도 메모리 모듈을 분석하는 또 다른 관점은 메모리 저장 매체의 형식, 예를 들어 자연어 메모리 또는 내장 메모리를 기반으로 합니다. 서로 다른 메모리 포맷은 서로 다른 장점을 가지고 있어 다양한 응용 분야에 적합하다. 이하에서는 몇 가지 대표적인 메모리 포맷을 소개한다.

\(\bullet\)_자연 언어_. 이 형식에서 에이전트 행동 및 관찰과 같은 메모리 정보는 원시 자연 언어를 사용하여 직접 설명된다. 이 형식은 몇 가지 장점을 가지고 있다. 첫째, 메모리 정보를 유연하고 이해하기 쉽게 표현할 수 있다. 또한, 에이전트 행동을 안내하기 위한 포괄적인 신호를 제공할 수 있는 풍부한 의미 정보를 보유한다. 이전 작업에서 반사[12]는 경험적 피드백을 슬라이딩 윈도우 내에 자연어로 저장한다. 보이저[38]는 메모리에 직접 저장되는 마인크래프트 게임 내의 기술을 나타내기 위해 자연 언어 설명을 사용한다.

\(\bullet\)_Embeddings_. 이 포맷에서, 메모리 정보는 임베딩 벡터로 인코딩되어, 메모리 검색 및 판독 효율을 향상시킬 수 있다. 예를 들어, MemoryBank[39]는 각각의 메모리 세그먼트를 임베딩 벡터로 인코딩하고, 이는 검색을 위해 인덱싱된 코퍼스를 생성한다. [16] 은 매칭 및 재사용을 용이하게 하기 위한 임베딩으로서 참조 계획들을 나타낸다. 더 나아가, ChatDev[18]은 검색을 위해 대화 히스토리를 벡터로 인코딩한다.

\(\bullet\)_Databases_. 이 형식에서는 메모리 정보가 데이터베이스에 저장되므로 에이전트가 메모리를 효율적이고 종합적으로 조작할 수 있다. 예를 들어, ChatDB[40]은 데이터베이스를 심볼릭 메모리 모듈로 사용한다. 에이전트는 SQL 문을 활용하여 메모리 정보를 정확하게 추가, 삭제 및 수정할 수 있습니다. DB-GPT[41]에서 메모리 모듈은 데이터베이스를 기반으로 구축된다. 메모리 정보를 보다 직관적으로 조작하기 위해 에이전트는 SQL 쿼리를 이해하고 실행할 수 있도록 미세 조정되어 자연어를 사용하여 데이터베이스와 직접 상호 작용할 수 있다.

\(\bullet\)_Structured Lists_. 이 형식에서는 기억 정보를 목록으로 정리하여 기억의 의미를 효율적이고 간결하게 전달할 수 있다. 예를 들어, GITM[16]은 하위 목표에 대한 액션 리스트를 계층적 트리 구조로 저장한다. 계층 구조는 목표와 해당 계획 간의 관계를 명시적으로 포착합니다. RETLLM[42]는 초기에 자연어 문장을 트리플렛 구로 변환하고, 이후 이를 메모리에 저장한다.

_Remark_. 여기서는 몇 가지 대표적인 메모리 형식만 보여주지만 [38]에 의해 사용되는 프로그래밍 코드와 같이 덮이지 않은 것이 많다는 점에 유의하는 것이 중요하다. 또한 이러한 형식은 상호 배타적이지 않다는 점을 강조해야 하며, 많은 모델이 각각의 이점을 동시에 활용하기 위해 여러 형식을 통합한다. 주목할 만한 예는 키-밸류 리스트 구조를 활용하는 GITM[16]의 메모리 모듈이다. 이 구조에서 키는 임베딩 벡터로 표현되는 반면 값은 원시 자연어로 구성된다. 임베딩 벡터의 사용은 메모리 레코드들의 효율적인 검색을 가능하게 한다. 자연어를 활용함으로써 메모리 내용이 매우 포괄적이 되어 더 많은 정보에 입각한 에이전트 작업을 가능하게 합니다.

위에서, 우리는 주로 메모리 모듈의 내부 설계에 대해 논의한다. 다음에서는 외부 환경과 상호 작용하는 데 사용되는 메모리 작업에 초점을 맞춥니다.

**메모리 작업**: 메모리 모듈은 에이전트가 환경과 상호 작용하여 중요한 지식을 획득, 축적 및 활용할 수 있도록 하는 데 중요한 역할을 합니다. 에이전트와 환경 간의 상호 작용은 메모리 읽기, 메모리 쓰기, 메모리 반영의 세 가지 중요한 메모리 동작을 통해 이루어진다. 이하에서는 이러한 동작들을 보다 구체적으로 소개한다.

\(\bullet\)_Memory Reading_. 기억 읽기의 목적은 기억에서 의미 있는 정보를 추출하여 에이전트의 행동을 향상시키는 것이다. 예를 들어, 이전에 성공한 액션들을 사용하여 유사한 목표들을 달성한다[16]. 기억 읽기의 핵심은 역사 행위로부터 가치 있는 정보를 추출하는 방법에 있다. 보통 정보 추출을 위해 일반적으로 사용되는 세 가지 기준, 즉 최근성, 관련성, 중요도[20]가 있다. 보다 최근, 관련성이 있고 중요한 기억이 추출될 가능성이 높다. 형식적으로 메모리 정보 추출을 위해 기존 문헌에서 다음과 같은 식을 결론짓는다:

\[m^{*}=\arg\min_{m\in M}\alpha s^{rec}(q,m)+\beta s^{rel}(q,m)+\gamma s^{imp}(m), \tag{1}\]

여기서 \(q\)는 쿼리, 예를 들어 에이전트가 어드레싱해야 하는 작업 또는 에이전트가 위치한 컨텍스트입니다. \ (M\)은 모든 메모리의 집합이다. \ (s^{rec}(\cdot)\), \(s^{rel}(\cdot)\), \(s^{imp}(\cdot)\)는 기억의 최신성, 관련성, 중요도를 측정하기 위한 채점 함수이다 \(m\). 이러한 스코어링 함수는 다양한 방법을 사용하여 구현될 수 있으며, 예를 들어, \(s^{rel}(q,m)\)은 LSH, ANNOY, HNSW, FAISS 등을 기반으로 구현될 수 있다. \(s^{imp}\)는 메모리 자체의 문자만 반영하므로 쿼리 \(q\)와는 무관하다는 점에 유의해야 한다. \ (\alpha\), \(\beta\) 및 \(\gamma\)는 밸런싱 파라미터이다. 다른 값으로 할당함으로써 다양한 기억 읽기 전략을 얻을 수 있다. 예를 들어, \(\alpha=\gamma=0\)을 설정함으로써 많은 연구[16, 30, 38, 42]는 메모리 판독에 대한 관련성 점수 \(s^{rel}\)만을 고려한다. [20]은 \(\alpha=\beta=\gamma=1.0\)을 할당하여 위의 세 가지 메트릭에 대해 동일한 가중치를 부여하여 메모리로부터 정보를 추출한다.

\(\bullet\)_Memory Writing_. 메모리 쓰기의 목적은 인지된 환경에 대한 정보를 메모리에 저장하는 것이다. 가치 있는 정보를 메모리에 저장하는 것은 미래에 유익한 기억을 검색할 수 있는 기반을 제공하여 에이전트가 보다 효율적이고 합리적으로 행동할 수 있게 한다. 메모리 쓰기 과정에서 주의 깊게 다루어야 할 두 가지 잠재적인 문제가 있다. 한편으로, 기존의 메모리(_i.e._, 메모리 복제)와 유사한 정보를 저장하는 방법을 다루는 것이 중요하다. 한편, 메모리가 자신의 저장 한계(_i.e._, 메모리 오버플로우)에 도달했을 때 정보를 제거하는 방법을 고려하는 것이 중요하다. 이하에서는 이러한 문제점에 대해 보다 구체적으로 논의하고자 한다. (1) _메모리 중복_. 유사한 정보를 통합하기 위해 사람들은 새로운 기록과 이전 기록을 통합하는 다양한 방법을 개발했다. 예를 들어, [7]에서, 동일한 서브-골과 관련된 성공적인 액션 시퀀스들은 리스트에 저장된다. 목록의 크기가 N(=5)에 도달하면, 목록의 모든 시퀀스는 LLM을 사용하여 통합된 계획 솔루션으로 응축된다. 메모리의 원래 시퀀스는 새로 생성된 시퀀스로 대체된다. 확장된 LLM [43]은 중복 저장을 피하면서 카운트 누적을 통해 중복 정보를 집계합니다. (2) _메모리 오버플로우_. 정보가 가득 찰 때 메모리에 기록하기 위해 사람들은 기존의 정보를 삭제하는 방법을 다르게 설계하여 암기 과정을 계속한다. 예를 들어, ChatDB[40]에서는 사용자 명령에 기초하여 메모리들을 명시적으로 삭제할 수 있다. RET-LLM[42]는 메모리를 위해 고정된 크기의 버퍼를 사용하며, FIFO(first-in-first-out) 방식으로 가장 오래된 엔트리들을 덮어씌운다.

\(\bullet\)_Memory Reflection_. 기억 성찰은 인간이 자신의 인지적, 정서적, 행동적 과정을 목격하고 평가하는 능력을 모방한다. 에이전트에 적응할 때 목표는 에이전트에게 보다 추상적이고 복잡하며 높은 수준의 정보를 독립적으로 요약하고 추론할 수 있는 능력을 제공하는 것이다. 보다 구체적으로, 생성 에이전트[20]에서, 에이전트는 메모리에 저장된 자신의 과거 경험을 보다 광범위하고 추상적인 통찰로 요약할 수 있는 능력을 갖는다. 먼저 에이전트는 최근 기억을 기반으로 세 가지 핵심 질문을 생성합니다. 그런 다음 이러한 질문은 관련 정보를 얻기 위해 메모리를 쿼리하는 데 사용됩니다. 획득된 정보를 기반으로 에이전트는 에이전트 수준의 아이디어를 반영하는 5가지 통찰력을 생성한다. 예를 들어, 낮은 수준의 기억들 "클라우스 뮬러는 연구 논문을 쓰고 있다", "클라우스 뮬러는 그의 연구를 더 발전시키기 위해 사서와 교제하고 있다", "클라우스 뮬러는 그의 연구에 대해 아예샤 칸과 대화하고 있다"는 것은 "클라우스 뮬러는 그의 연구에 전념하고 있다"라는 높은 수준의 통찰을 유도할 수 있다. 또한, 성찰 과정은 계층적으로 일어날 수 있으며, 이는 기존의 인사이트를 기반으로 인사이트를 생성할 수 있음을 의미한다. GITM[16]에서, 서브-목표들을 성공적으로 달성한 액션들은 리스트에 저장된다. 목록에 5개 이상의 요소가 포함된 경우 에이전트는 공통적이고 추상적인 패턴으로 요약하고 모든 요소를 대체합니다. ExpeL[44]에서는 에이전트가 반사를 획득하기 위한 두 가지 접근법이 소개된다. 첫째, 에이전트는 동일한 작업 내에서 성공적인 궤적과 실패한 궤적을 비교한다. 둘째, 에이전트는 경험을 얻기 위해 성공적인 궤적 모음에서 학습한다.

전통적인 LLM과 에이전트의 중요한 차이점은 후자는 동적 환경에서 작업을 학습하고 완료할 수 있는 능력을 보유해야 한다는 것이다. 우리가 메모리 모듈을 에이전트의 과거 행동을 관리하는 책임자로 간주한다면, 에이전트의 미래 행동을 계획하는 데 도움을 줄 수 있는 또 다른 중요한 모듈을 갖는 것이 필수적이다. 이하에서는 연구자들이 계획 모듈을 어떻게 설계하는지에 대한 개요를 제시한다.

#### 2.1.3 계획 모듈

복잡한 과제에 직면했을 때 인간은 그것을 더 단순한 하위 과제로 해체하고 개별적으로 해결하는 경향이 있다. 계획 모듈은 이러한 인간 능력을 가진 에이전트에게 권한을 부여하는 것을 목표로 하며, 이는 에이전트가 보다 합리적이고, 강력하며, 신뢰성 있게 행동하도록 할 것으로 예상된다. 특히 기획 과정에서 에이전트가 피드백을 받을 수 있는지 여부를 기준으로 기존 연구들을 정리하면 다음과 같다.

**피드백 없이 계획**: 이 메서드에서 에이전트는 작업을 수행한 후 향후 행동에 영향을 미칠 수 있는 피드백을 받지 않습니다. 다음에서는 몇 가지 대표적인 전략을 제시한다.

\(\bullet\)_단일 경로 추론_. 이 전략에서 최종 과제는 여러 중간 단계로 분해된다. 이들 단계들은 캐스케이딩 방식으로 연결되며, 각각의 단계는 단지 하나의 후속 단계로 이어진다. LLM은 최종 목표를 달성하기 위해 이러한 단계를 따른다. 구체적으로, CoT(Chain of Thought) [45]는 복잡한 문제를 해결하기 위한 추론 단계를 프롬프트에 입력하는 것을 제안한다. 이러한 단계는 LLM이 단계적으로 계획하고 행동하도록 영감을 주는 예로서 작용한다. 이 방법에서는 프롬프트의 예제에서 영감을 바탕으로 계획을 작성합니다. 제로-샷-CoT[46]는 LLM들이 "단계별로 생각"과 같은 트리거 문장들로 프롬프트함으로써 태스크 추론 프로세스들을 생성할 수 있게 한다. CoT와 달리 이 방법은 프롬프트에서 추론 단계를 예로 포함하지 않는다. [47]을 재현하는 것은 계획을 생성하기 전에 각 단계가 필요한 전제 조건을 충족하는지 확인하는 것을 포함한다. 단계가 전제 조건을 충족하지 못할 경우 전제 조건 오류 메시지를 도입하고 LLM에 계획을 다시 생성하라는 메시지를 표시합니다. ReWOO [48]은 계획과 외부 관측치를 분리하는 패러다임을 도입하는데, 에이전트는 먼저 계획을 생성하고 관측치를 독립적으로 구한 후 이를 함께 결합하여 최종 결과를 도출한다. HuggingGPT[13]은 먼저 작업을 많은 하위 목표로 분해한 후 Huggingface를 기반으로 각각을 해결한다. 모든 추론 단계를 원샷 방식으로 수행하는 CoT 및 Zero-shot-CoT와 달리 ReWOO 및 HuggingGPT는 LLM 곱셈 시간에 액세스하여 결과를 생성한다.

\(\bullet\)_다중 경로 추론_. 이 전략에서는 최종 계획을 생성하기 위한 추론 단계를 나무와 같은 구조로 구성한다. 각각의 중간 단계는 다수의 후속 단계를 가질 수 있다. 이 접근법은 개인이 각 추론 단계에서 여러 가지 선택을 할 수 있기 때문에 인간의 사고와 유사하다. 특히 Self-consistent CoT(CoT-SC) [49]는 각각의 복잡한 문제가 최종적인 답을 추론하는 여러 가지 사고방식을 가지고 있다고 보고 있다. 따라서 다양한 추론 경로 및 대응 답변을 생성하기 위해 CoT를 사용하는 것으로 시작한다. 이어서, 빈도수가 가장 높은 답변이 최종 출력으로 선택된다. 사상 트리(Tree of Thoughts, ToT) [50]은 나무와 같은 추론 구조를 사용하여 계획을 생성하도록 설계되었다. 이 접근법에서 트리의 각 노드는 중간 추론 단계에 해당하는 "사상"을 나타낸다. 이러한 중간 단계의 선택은 LLM들의 평가에 기초한다. 최종 계획은 너비 우선 탐색(BFS) 또는 깊이 우선 탐색(DFS) 전략을 사용하여 생성됩니다. 모든 계획 단계를 함께 생성하는 CoT-SC와 비교하여 ToT는 각 추론 단계에 대해 LLM을 쿼리해야 한다. RecMind[51]에서 저자들은 계획 과정에서 버려진 역사적 정보를 활용하여 새로운 추론 단계를 도출하는 자기 성찰 메커니즘을 설계했다. GoT [52]에서 저자는 ToT의 트리 유사 추론 구조를 그래프 구조로 확장하여 보다 강력한 프롬프트 전략을 도출한다. AoT[53]에서 저자들은 프롬프트에 알고리즘 예제를 통합하여 LLM의 추론 과정을 향상시키는 새로운 방법을 설계한다. 놀랍게도 이 방법은 LLM을 한 번 또는 몇 번만 쿼리하면 된다. [54]에서 LLM은 제로 샷 플래너로 활용된다. 각 계획 단계에서, 그들은 먼저 여러 개의 가능한 다음 단계를 생성한 다음, 허용 가능한 동작까지의 거리에 기초하여 최종 단계를 결정한다. [55] 프롬프트에서 쿼리와 유사한 예를 통합함으로써 [54]를 더욱 개선한다. RAP [56]은 몬테카를로 트리 탐색(MCTS)을 기반으로 여러 계획의 잠재적 이점을 시뮬레이션하기 위해 세계 모델을 구축한 다음, 여러 MCTS 반복을 집계하여 최종 계획을 생성한다. 이해력을 높이기 위해 그림 3에서 단일 경로 추론과 다중 경로 추론의 전략을 비교하는 예시를 제공한다.

\(\bullet\)_External Planner_. 제로 샷 계획에서 LLM의 입증된 힘에도 불구하고 도메인별 문제에 대한 계획을 효과적으로 생성하는 것은 여전히 매우 어렵다. 이 문제를 해결하기 위해 연구원들은 외부 기획자로 눈을 돌립니다. 이러한 도구는 잘 개발되었으며 정확한 또는 최적의 계획을 신속하게 식별하기 위해 효율적인 검색 알고리즘을 사용한다. 특히, LLM+P[57]은 먼저 태스크 설명을 PDDL(Planning Domain Definition Languages)로 변환하고, 외부 플래너를 사용하여 PDDL을 처리한다. 최종적으로 생성된 결과는 LLMs에 의해 자연어로 다시 변환된다. 마찬가지로 LLM-DP [58]은 LLM을 활용하여 관측치, 현재 세계 상태 및 목표 목표를 PDDL로 변환한다. 이어서, 이 변환된 데이터는 외부 플래너로 전달되고, 이는 최종 액션 시퀀스를 효율적으로 결정한다. CO-LLM [22]는 LLM이 높은 수준의 계획을 생성하는 데는 능하지만 낮은 수준의 통제에는 어려움을 겪는다는 것을 보여준다. 이러한 한계를 해결하기 위해 휴리스틱하게 설계된 외부 하위 레벨 플래너를 사용하여 상위 레벨 플랜에 기반한 액션을 효과적으로 실행합니다.

**피드백을 사용하여 계획**: 많은 실제 시나리오에서 에이전트는 복잡한 작업을 해결하기 위해 긴 수평 계획을 만들어야 합니다. 이러한 과제를 직면할 때 피드백이 없는 위의 계획 모듈은 다음과 같은 이유로 인해 덜 효과적일 수 있다: 첫째, 처음부터 무결한 계획을 직접 생성하는 것은 다양한 복잡한 전제 조건을 고려해야 하기 때문에 매우 어렵다. 결과적으로, 단순히 초기 계획을 따르는 것은 실패로 이어지는 경우가 많다. 더욱이, 계획의 실행은 예측 불가능한 전이 역학에 의해 방해되어 초기 계획을 실행 불가능하게 할 수 있다. 동시에 인간이 복잡한 작업을 어떻게 처리하는지 조사할 때, 우리는 개인이 외부 피드백을 기반으로 계획을 반복적으로 만들고 수정할 수 있음을 발견한다. 이러한 인간 능력을 시뮬레이션하기 위해 연구자들은 에이전트가 조치를 취한 후 피드백을 받을 수 있는 많은 계획 모듈을 설계했다. 피드백은 환경, 인간 및 모델에서 얻을 수 있으며, 이는 다음에서 자세히 설명한다.

\(\bullet\)_환경 피드백_. 이 피드백은 객관적인 세계 또는 가상 환경으로부터 얻어진다. 예를 들어 게임의 작업 완료 신호 또는 에이전트가 작업을 수행한 후 수행된 관찰일 수 있습니다. 특히 ReAct [59]는 사고-행동 관찰 트리플렛을 사용하여 프롬프트를 구성하는 것을 제안한다. 사고 구성요소는 에이전트 행동을 안내하기 위한 높은 수준의 추론과 계획을 용이하게 하는 것을 목표로 한다. 그 행위는 대리인이 취한 특정한 행위를 나타낸다. 관찰은 검색 엔진 결과와 같은 외부 피드백을 통해 획득한 액션의 결과에 해당한다. 다음 생각은 이전 관찰에 의해 영향을 받으며, 이는 생성된 계획을 환경에 더 적응적으로 만든다. 보이저[38]는 프로그램 실행의 중간 진행, 실행 오류 및 자체 검증 결과를 포함한 세 가지 유형의 환경 피드백을 통합하여 계획을 수립한다. 이러한 신호는 에이전트가 다음 작업을 위한 더 나은 계획을 세우는 데 도움이 될 수 있습니다. 보이저(Voyager)와 유사하게, 고스트[16]도 피드백을 추론과 행동 취하기 과정에 통합한다. 이 피드백은 실행된 각 액션에 대한 성공 및 실패 정보뿐만 아니라 환경 상태를 포함한다. SayPlan[31]은 장면 그래프 시뮬레이터에서 파생된 환경 피드백을 활용하여 전략 공식을 검증하고 정제한다. 이 시뮬레이터는 에이전트 액션에 이어 결과 및 상태 전환을 식별하는 데 능숙하여 실행 가능한 계획이 확인될 때까지 전략에 대한 SayPlan의 반복적인 재보정을 촉진한다. DEPS [33]에서 저자들은 과제의 완성에 대한 정보만을 제공하는 것은 종종 계획 오류를 수정하기에 부적절하다고 주장한다. 따라서 에이전트에게 작업 실패에 대한 자세한 이유를 알려줌으로써 보다 효과적으로 계획을 수정할 수 있도록 제안한다. LLM-Planner[60]는 태스크 완료 동안 객체 불일치 및 달성 불가능한 계획들을 만날 때 LLM들에 의해 생성된 계획들을 동적으로 업데이트하는 접지된 재-계획 알고리즘을 도입한다. 내부 모노로그[61]는 (1) 작업이 성공적으로 완료되었는지 여부, (2) 수동적 장면 묘사 및 (3) 능동적 장면 묘사의 세 가지 유형의 피드백을 에이전트에게 제공한다. 전자의 두 가지는 환경으로부터 생성되며, 이는 에이전트 행동을 보다 합리적으로 만든다.

\(\bullet\)_Human Feedback_. 환경으로부터 피드백을 얻는 것 외에도 인간과 직접 상호작용하는 것은 에이전트 계획 능력을 향상시키기 위한 매우 직관적인 전략이기도 하다. 인간의 피드백은 주관적인 신호이다. 효과적으로

도. 3: 단일 경로 추론과 다중 경로 추론의 전략 비교. LMZSP는 [54]에서 제안된 모델이다.

에이전트를 인간의 가치와 선호도에 맞게 만들고 환각 문제를 완화시키는 데 도움이 됩니다. Inner Monologue[61]에서 에이전트는 3차원 시각 환경에서 높은 수준의 자연어 명령어를 수행하는 것을 목표로 한다. 장면 묘사와 관련하여 인간의 피드백을 적극적으로 요청할 수 있는 능력이 부여된다. 그런 다음 에이전트는 인간의 피드백을 프롬프트에 통합하여 보다 정보에 입각한 계획 및 추론을 가능하게 한다. 위의 경우, 에이전트 계획 능력을 향상시키기 위해 다양한 유형의 피드백이 결합될 수 있음을 알 수 있다. 예를 들어, Inner Monologue [61]은 에이전트 계획을 용이하게 하기 위해 환경과 인간의 피드백을 모두 수집한다.

\(\bullet\)_모델 피드백._ 외부 신호인 앞서 언급한 환경 및 인간 피드백 외에도 연구자들은 에이전트 자체에서 내부 피드백의 활용도 조사했다. 이러한 유형의 피드백은 일반적으로 미리 훈련된 모델에 기초하여 생성된다. 특히 [62]에서는 셀프-리파인 메커니즘을 제안하고 있다. 이 메커니즘은 출력, 피드백 및 개선의 세 가지 중요한 구성 요소로 구성된다. 먼저 에이전트가 출력을 생성합니다. 그런 다음 LLM을 활용하여 출력에 대한 피드백을 제공하고 개선 방법에 대한 지침을 제공합니다. 마지막으로, 피드백 및 미세화에 의해 출력이 개선된다. 이 출력-피드백-정제 프로세스는 몇 가지 원하는 조건에 도달할 때까지 반복된다. SelfCheck [63]을 통해 에이전트는 다양한 단계에서 생성된 자신의 추론 단계를 검사하고 평가할 수 있다. 그런 다음 결과를 비교하여 오류를 수정할 수 있습니다. InterAct [64]는 체커 및 분류기와 같은 보조 역할로서 상이한 언어 모델(예: ChatGPT 및 InstructGPT)을 사용하여 주 언어 모델이 잘못되고 비효율적인 동작을 회피하도록 돕는다. ChatCoT[65]는 모델 피드백을 활용하여 추론 과정의 질을 향상시킨다. 모델 피드백은 에이전트 추론 단계를 모니터링하는 평가 모듈에 의해 생성된다. 반사[12]는 구체적인 언어적 피드백을 통해 에이전트의 계획 능력을 향상시키기 위해 개발되었다. 이 모델에서 에이전트는 먼저 자신의 기억을 기반으로 행동을 생성하고, 평가자는 에이전트의 궤적을 입력으로 하여 피드백을 생성한다. 피드백이 스칼라 값으로 주어지는 이전 연구와 달리 이 모델은 LLM을 활용하여 보다 자세한 구두 피드백을 제공하여 에이전트 계획에 대한 보다 포괄적인 지원을 제공할 수 있다.

비고: 결론적으로, 피드백 없이 계획 모듈을 구현하는 것은 비교적 간단하다. 그러나 적은 수의 추론 단계만 필요로 하는 간단한 작업에 주로 적합하다. 반대로 피드백으로 계획하는 전략은 피드백을 처리하기 위해 보다 세심한 설계가 필요하다. 그럼에도 불구하고 훨씬 더 강력하고 장거리 추론을 포함하는 복잡한 작업을 효과적으로 해결할 수 있다.

#### 2.1.4 Action Module

액션 모듈은 에이전트의 결정을 특정 결과로 변환하는 역할을 합니다. 이 모듈은 가장 하류 위치에 위치하며 환경과 직접 상호 작용한다. 프로파일, 메모리 및 계획 모듈의 영향을 받습니다. 이 절에서는 액션 모듈을 4가지 관점에서 소개한다: (1) 액션 목표: 액션의 의도된 결과는 무엇인가? (2) 액션 생성: 액션은 어떻게 생성되는가? (3) 동작 공간: 사용 가능한 동작들은 무엇인가? (4) 행동 영향: 행동의 결과는 무엇인가? 이러한 관점들 중에서, 처음 두 가지는 액션 이전의 측면들("행동 전" 측면들)에 초점을 맞추고, 세 번째는 액션 자체("행동 중" 측면들)에 초점을 맞추고, 네 번째는 액션의 영향(행동 후" 측면들)을 강조한다.

**작업 목표**: 에이전트는 다양한 목표를 사용하여 작업을 수행할 수 있습니다. 여기서는 (1) _작업 완료_ 라는 몇 가지 대표적인 예를 제시합니다. 이 시나리오에서 에이전트의 행동은 마인크래프트에서 철곡괭이를 만들거나 소프트웨어 개발에서 기능을 완성하는 것과 같은 특정 작업을 수행하는 것을 목표로 한다[38]. 이러한 행동들은 대개 잘 정의된 목적들을 가지며, 각각의 행동들은 최종 태스크의 완성에 기여한다. 이러한 유형의 목표를 목표로 하는 행동은 기존 문헌에서 매우 일반적이다. (2) _통신_. 이 경우, 정보를 공유하거나 협업을 위해 다른 에이전트 또는 실제 인간과 통신하기 위한 조치가 취해진다. 예를 들어, ChatDev[18] 내의 에이전트들은 소프트웨어 개발 태스크들을 집합적으로 달성하기 위해 서로 통신할 수 있다. Inner Monologue[61]에서 에이전트는 인간과 적극적으로 소통하고 인간의 피드백을 기반으로 행동 전략을 조정한다. (3) _환경 탐색_. 이 예에서 에이전트는 익숙하지 않은 환경을 탐색하여 인식을 확장하고 탐색과 착취 사이의 균형을 맞추는 것을 목표로 한다. 예를 들어, 보이저[38]의 에이전트는 작업 완료 과정에서 알려지지 않은 스킬을 탐색하고 시행착오를 통한 환경 피드백을 기반으로 스킬 실행 코드를 지속적으로 정제할 수 있다.

**작업 프로덕션**: 모델 입력 및 출력이 직접 연결된 일반 LLM과 다른 에이전트는 다른 전략 및 소스를 통해 작업을 수행할 수 있습니다. 이하에서는 일반적으로 사용되는 두 가지 유형의 액션 프로덕션 전략을 소개한다. (1) _메모리 회상을 통한 액션_. 이 전략에서 액션은 현재 태스크에 따라 에이전트 메모리로부터 정보를 추출함으로써 생성된다. 작업 및 추출된 메모리는 에이전트 동작을 트리거하기 위한 프롬프트로 사용된다. 예를 들어, Generative Agents[20]에서, 에이전트는 메모리 스트림을 유지하고, 각각의 액션을 취하기 전에, 에이전트 액션들을 안내하기 위해 메모리 스팀으로부터 최근, 관련되고 중요한 정보를 검색한다. GITM[16]에서, 낮은-레벨 서브-목표(sub-goal)를 달성하기 위해, 에이전트는 작업에 관련된 임의의 성공적인 경험들이 있는지를 결정하기 위해 자신의 메모리에 질의한다. 유사한 작업이 이전에 완료된 경우 에이전트는 이전에 성공한 작업을 호출하여 현재 작업을 직접 처리합니다. ChatDev [18] 및 MetaGPT [23]과 같은 협력 에이전트들에서, 상이한 에이전트들은 서로 통신할 수 있다. 이 과정에서 대화창 내의 대화 내역은 에이전트 메모리에서 기억된다. 에이전트에 의해 생성된 각각의 발화는 그 기억의 영향을 받는다. (2) _플랜 팔로잉을 통한 액션_ 이 전략에서 에이전트는 미리 생성된 계획에 따라 조치를 취합니다. 예를 들어 DEPS [33]에서 주어진 작업에 대해 에이전트는 먼저 액션 플랜을 만듭니다. 계획 실패를 나타내는 신호가 없으면 에이전트는 이러한 계획을 엄격하게 준수합니다. GITM[16]에서 에이전트는 작업을 많은 하위 목표로 분해하여 높은 수준의 계획을 만든다. 이러한 계획을 바탕으로 에이전트는 각 하위 목표를 순차적으로 해결하는 행동을 취하여 최종 과제를 완성한다.

**작업 공간**: 작업 공간은 에이전트에서 수행할 수 있는 가능한 작업 집합을 나타냅니다. 일반적으로 우리는 이러한 행동을 (1) 외부 도구와 (2) LLM에 대한 내부 지식의 두 가지 클래스로 대별할 수 있다. 이하에서는 이러한 행위를 좀 더 구체적으로 소개하고자 한다.

\(\bullet\)_External Tools_. LLM은 많은 양의 작업을 달성하는 데 효과적인 것으로 입증되었지만 포괄적인 전문가 지식이 필요한 영역에는 잘 작동하지 않을 수 있다. 또한 LLM은 스스로 해결하기 어려운 환각 문제에 직면할 수도 있다. 위의 문제를 완화하기 위해 에이전트는 동작을 실행하기 위한 외부 도구를 호출할 수 있는 권한을 부여받는다. 다음에서는 문헌에서 활용된 몇 가지 대표적인 도구를 제시한다.

(1) _APIs_. 외부 API를 활용하여 액션 공간을 보완하고 확장하는 것은 최근 몇 년 동안 인기 있는 패러다임이다. 예를 들어 HuggingGPT [13]은 HuggingFace에서 모델을 활용하여 복잡한 사용자 작업을 수행합니다. [66; 67]은 사용자 요청에 응답할 때 외부 웹 페이지들로부터 관련 콘텐츠를 추출하기 위한 쿼리들을 자동으로 생성하는 것을 제안한다. TPTU [67]은 제곱근, 요인 및 행렬 연산과 같은 정교한 계산을 실행하기 위해 Python 해석기 및 LaTeX 컴파일러와 인터페이스한다. 또 다른 유형의 API는 자연 언어 또는 코드 입력에 기초하여 LLM에 의해 직접 호출될 수 있는 것이다. 예를 들어, 고릴라 [68]은 API 호출에 대한 정확한 입력 인수를 생성하고 외부 API 호출 중 환각 문제를 완화하도록 설계된 미세 조정된 LLM이다. ToolFormer[15]는 자연어 명령어를 기반으로 주어진 도구를 기능이나 형식이 다른 다른 도구로 자동 변환할 수 있는 LLM 기반의 도구 변환 시스템이다. API-Bank[69]는 LLM 기반의 API 추천 에이전트로 다양한 프로그래밍 언어 및 도메인에 대한 적절한 API 호출을 자동으로 검색 및 생성할 수 있다. API-Bank는 또한 사용자가 생성된 API 호출을 쉽게 수정하고 실행할 수 있는 대화형 인터페이스를 제공한다. ToolBench[14]는 LLM 기반의 도구 생성 시스템으로 자연어 요구사항을 기반으로 다양한 실용적인 도구를 자동으로 설계 및 구현할 수 있다. 툴벤치가 생성하는 도구에는 계산기, 단위 변환기, 달력, 지도, 차트 등이 있다. RestGPT [70]은 LLMs를 RESTful API와 연결하며, RESTful API는 웹 서비스 개발을 위해 널리 인정되는 표준을 따르므로 결과 프로그램이 실제 애플리케이션과 더 호환됩니다. TaskMatrix.AI [71]은 LLM과 수백만 개의 API를 연결하여 태스크 실행을 지원합니다. 그 핵심에는 사용자와 상호 작용하고 목표와 맥락을 이해한 다음 특정 작업에 대한 실행 코드를 생성하는 다중 모드 대화 기반 모델이 있다. 이러한 모든 에이전트는 외부 API를 도구로 활용하고, 사용자가 생성되거나 변환된 도구를 쉽게 수정하고 실행할 수 있도록 대화형 인터페이스를 제공한다.

(2) _데이터베이스 및 지식 기반_. 외부 데이터베이스 또는 지식 베이스를 통합하면 에이전트가 보다 현실적인 액션을 생성하기 위한 특정 도메인 정보를 얻을 수 있다. 예를 들어 ChatDB [40]은 SQL 문을 사용하여 데이터베이스를 쿼리하여 에이전트의 작업을 논리적 방식으로 촉진합니다. MRKL[72] 및 OpenAGI[73]은 지식 베이스 및 플래너와 같은 다양한 전문가 시스템을 통합하여 도메인별 정보에 액세스한다.

(3) _외부 모델_. 선행 연구들은 가능한 행동의 범위를 확장하기 위해 외부 모델을 활용하는 경우가 많다. API와 비교하여 외부 모델은 일반적으로 더 복잡한 작업을 처리합니다. 각각의 외부 모델은 복수의 API들에 대응할 수 있다. 예를 들어, 텍스트 검색 능력을 향상시키기 위해, MemoryBank[39]는 두 개의 언어 모델을 통합한다: 하나는 입력 텍스트를 인코딩하도록 설계되는 반면, 다른 하나는 쿼리 문들의 매칭을 담당한다. ViperGPT [74]는 먼저 언어 모델을 기반으로 구현된 Codex를 사용하여 텍스트 설명으로부터 Python 코드를 생성한 후 코드를 실행하여 주어진 작업을 완료한다. TPTU[67]은 코드 생성, 가사 제작 등과 같은 광범위한 언어 생성 작업을 수행하기 위해 다양한 LLM을 통합한다. ChemCrow[75]는 유기 합성, 약물 발견 및 재료 설계에서 작업을 수행하도록 설계된 LLM 기반 화학 제제이다. 17개의 전문가 설계 모델을 활용하여 운영을 지원합니다.

MM-REACT [76]은 비디오 요약용 VideoBERT, 이미지 생성을 위한 X-디코더, 오디오 처리를 위한 SpeechBERT와 같은 다양한 외부 모델을 통합하여 다양한 멀티모달 시나리오에서 성능을 향상시킨다.

\(\bullet\)_내부 지식_. 외부 도구를 활용하는 것 외에도 많은 에이전트는 LLM의 내부 지식에만 의존하여 행동을 안내한다. 이제 에이전트가 합리적이고 효과적으로 행동할 수 있도록 지원할 수 있는 LLM의 몇 가지 중요한 기능을 제시한다. (1) _계획 능력_. 이전 연구에서는 LLM이 복잡한 작업을 더 간단한 작업으로 분해하기 위해 괜찮은 플래너로 사용될 수 있음을 보여주었다[45]. LLM의 이러한 능력은 프롬프트에 예를 통합하지 않고서도 트리거될 수 있다[46]. DEPS [33]은 LLMs의 계획 능력을 기반으로, 하위 목표 분해를 통해 복잡한 작업을 해결할 수 있는 마인크래프트 에이전트를 개발한다. GITM [16] 및 보이저 [38]과 같은 유사한 에이전트도 LLM의 계획 능력에 크게 의존하여 다른 작업을 성공적으로 완료합니다. (2) _대화 능력_. LLM은 일반적으로 고품질 대화를 생성할 수 있습니다. 이 기능을 통해 에이전트는 인간처럼 행동할 수 있습니다. 이전 작업에서 많은 에이전트는 LLM의 강력한 대화 능력을 기반으로 조치를 취한다. 예를 들어, ChatDev [18]에서, 상이한 에이전트들은 소프트웨어 개발 프로세스에 대해 토론할 수 있고, 심지어 그들 자신의 행동에 대해 반성할 수 있다. RLP[30]에서, 에이전트는 에이전트의 발화에 대한 그들의 잠재적인 피드백에 기초하여 청취자들과 통신할 수 있다. (3) _상식 이해 능력_. LLM의 또 다른 중요한 능력은 그들이 인간의 상식을 잘 이해할 수 있다는 것이다. 이러한 능력에 기초하여, 많은 에이전트는 인간의 일상 생활을 시뮬레이션하고 인간과 유사한 결정을 내릴 수 있다. 예를 들어, Generative Agent에서 에이전트는 자신의 현재 상태, 주변 환경 등을 정확하게 이해하고, 기본적인 관찰을 바탕으로 높은 수준의 아이디어를 요약할 수 있다. LLM의 상식 이해 능력이 없으면 이러한 행동을 안정적으로 시뮬레이션할 수 없다. 유사한 결론이 RecAgent [21] 및 S3 [77]에도 적용될 수 있으며, 여기서 에이전트는 사용자 추천 및 소셜 행동을 시뮬레이션하는 것을 목표로 한다.

**작업 영향**: 작업 영향은 작업의 결과를 나타냅니다. 사실, 액션 영향은 수많은 예를 포함할 수 있지만, 간결함을 위해, 우리는 몇 가지 예만을 제공한다. (1) _환경 변경_ 에이전트는 위치 이동, 항목 수집, 건물 건설 등과 같은 조치에 의해 환경 상태를 직접 변경할 수 있다. 예를 들어 GITM [16] 및 Voyager [38]에서는 작업 완료 프로세스에서 에이전트의 작업에 의해 환경이 변경됩니다. 예를 들어, 에이전트가 세 개의 나무를 채굴하면 환경에서 사라질 수 있습니다. (2) _내부 상태 변경_ 에이전트가 취하는 행동들은 또한 기억을 갱신하고, 새로운 계획을 형성하고, 새로운 지식을 습득하는 것 등을 포함하여 에이전트 자체를 변화시킬 수 있다. 예를 들어, Generative Agents[20]에서, 메모리 스트림들은 시스템 내에서 액션들을 수행한 후에 업데이트된다. SayCan[78]은 에이전트가 환경에 대한 이해를 업데이트하기 위한 액션을 취할 수 있게 한다. (3) _Triggering New Actions._ 작업 완료 프로세스에서, 하나의 에이전트 액션은 다른 것에 의해 트리거될 수 있다. 예를 들어, 보이저[38]는 일단 필요한 모든 자원을 모으면 건물을 짓는다.

### 에이전트 기능 획득

위의 섹션에서는 주로 LLM이 인간과 같은 작업을 수행할 수 있는 자격을 갖도록 하는 능력을 더 잘 고취시키기 위해 에이전트 아키텍처를 설계하는 방법에 중점을 둔다. 아키텍처는 에이전트의 "하드웨어"로 기능합니다. 그러나, 하드웨어에만 의존하는 것은 효과적인 작업 수행을 달성하기에는 불충분하다. 에이전트는 "소프트웨어" 리소스로 간주될 수 있는 필요한 작업별 능력, 기술 및 경험이 부족할 수 있기 때문이다. 이러한 자원을 에이전트에 장착하기 위해 다양한 전략이 고안되었다. 일반적으로 이러한 전략을 LLM의 미세 조정이 필요한지 여부에 따라 두 가지 그룹으로 분류한다. 이하에서는 이들 각각에 대하여 보다 구체적으로 소개하고자 한다.

**미세 조정을 통한 성능 획득**: 작업 완료를 위해 에이전트 기능을 향상시키는 간단한 방법은 작업 종속 데이터 세트를 기반으로 에이전트를 미세 조정하는 것입니다. 일반적으로, 데이터 세트는 인간 주석, LLM 생성 또는 실제 애플리케이션으로부터 수집되는 것에 기초하여 구성될 수 있다. 이하에서는 이러한 방법들을 보다 구체적으로 소개한다.

* _인간 주석 데이터 집합을 사용 하 여 미세 조정 합니다._

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Profile} & \multicolumn{2}{c}{Memory} & \multirow{2}{*}{Planning} & \multirow{2}{*}{Action} & \multirow{2}{*}{CA} & \multirow{2}{*}{Time} \\ \cline{3-4}  & & Operation & & & & & \\ \hline WebGPT [66] & - & - & - & - & \⃃ & \�1 & 12/2021 \\ SayCan [78] & - & - & - & \�1 & \�2 & 04/2022 \\ MRKL [72] & - & - & - & \�1 & \�2 & - & 05/2022 \\ Inner Monologue [61] & - & - & - & \�2 & \�1 & \�2 & 07/2022 \\ Social Simulators [79] & \�2 & - & - & - & \�1 & - & 08/2022 \\ ReAct [59] & - & - & - & \�2 & \�2 & \�1 & 10/2022 \\ MALLM [43] & \�1 & \�2 & - & \�1 & - & 01/2023 \\ DEPS [33] & - & - & - & \�2 & \�1 & \�2 & 02/2023 \\ Toolformer [15] & - & - & - & \�1 & \�2 & \�1 & 02/2023 \\ Reflexion [12] & - & \�2 & \�2 & \�1 & \�2 & \�2 & 03/2023 \\ CAMEL [80] & \�1 & - & - & \�2 & \�1 & - & 03/2023 \\ API-Bank [69] & - & - & - & \�2 & \�2 & \�2 & 04/2023 \\ ViperGPT [74] & - & - & - & - & \�2 & - & 03/2023 \\ HuggingGPT [13] & - & - & \�1 & \�1 & \�2 & - & 03/2023 \\ Generative Agents [20] & \�1 & \�2 & \�2 & \�1 & - & 04/2023 \\ LLM+P [57] & - & - & - & \�1 & \�1 & - & 04/2023 \\ ChemCrow [75] & - & - & - & \�2 & \�2 & - & 04/2023 \\ OpenAGI [73] & - & - & - & \�2 & \�2 & \�1 & 04/2023 \\ AutoGPT [81] & - & \�1 & \�2 & \�2 & \�2 & \�2 & 04/2023 \\ SCM [35] & - & \�2 & \�2 & \�2 & - & 04/2023 \\ Socially Alignment [82] & - & \�1 & \�2 & - & \�1 & 05/2023 \\ GITM [16] & - & \�2 & \�2 & \�2 & \�1 & \�2 & 05/2023 \\ Voyager [38] & - & \�2 & \�2 & \�2 & \�1 & \�2 & 05/2023 \\ Introspective Tips [83] & - & - & - & \�2 & \�1 & \�2 & 05/2023 \\ RET-LLM [42] & - & \�1 & \�2 & - & \�1 & 05/2023 \\ ChatDB [40] & - & \�1 & \�2 & \�2 & - & 06/2023 \\ \(S^{3}\)[77] & \�2 & \�2 & \�2 & - & \�1 & - & 07/2023 \\ ChatDev [18] & \�1 & \�2 & \�2 & \�2 & \�2 & 07/2023 \\ ToolLIM [14] & - & - & - & \�2 & \�2 & \�2 & 07/2023 \\ MemoryBank [39] & - & \�2 & \�2 & - & \�2 & - & 07/2023 \\ MetaGPT [23] & \�1 & \�2 & \�2 & \�2 & - & 08/2023 \\ \hline \hline \end{tabular}
\end{table}
표 1: 프로파일 모듈의 경우 �1, �2 및 �3을 사용하여 핸드크래프팅 방법, LLM 생성 방법 및 데이터 세트 정렬 방법을 각각 나타냅니다. 메모리 모듈의 경우, 메모리 동작 및 메모리 구조를 위한 구현 전략에 초점을 맞춘다. 메모리 동작의 경우 �1과 �2를 사용하여 모델이 읽기/쓰기 연산만 있고 읽기/쓰기/반사 연산만 있음을 나타낸다. 메모리 구조는 �1과 �2를 사용하여 통합 메모리와 하이브리드 메모리를 각각 표현한다. 계획 모듈의 경우 �1 및 �2를 사용하여 각각 계획 w/o 피드백 및 w/피드백을 표현한다. 액션 모듈의 경우 �1 및 �2를 사용하여 모델이 도구를 사용하지 않고 도구를 각각 사용한다는 것을 나타냅니다. 에이전트 능력 획득(CA) 전략의 경우 �1 및 �2를 사용하여 각각 미세 조정이 있는 방법과 없는 방법을 나타냅니다. "-"는 해당 내용이 논문에서 명시적으로 논의되지 않았음을 나타낸다.

에이전트를 미세 조정하기 위해 인간 주석이 달린 데이터 세트를 활용하는 것은 다양한 응용 시나리오에서 사용할 수 있는 다용도 접근법이다. 이 접근법에서 연구자들은 먼저 주석 작업을 설계한 다음 작업자를 모집하여 작업을 완료한다. 예를 들어, CoH [84]에서 저자는 LLM을 인간의 가치 및 선호도와 정렬하는 것을 목표로 한다. 인간의 피드백을 단순하고 상징적인 방식으로 활용하는 다른 모델과 달리, 이 방법은 인간의 피드백을 자연어 형태의 상세한 비교 정보로 변환한다. LLM은 이러한 자연어 데이터 세트를 기반으로 직접 미세 조정된다. RET-LLM [42]에서 자연 언어를 구조화된 메모리 정보로 더 잘 변환하기 위해 저자는 인간이 구성한 데이터 세트를 기반으로 LLM을 미세 조정하며, 여기서 각 샘플은 "트리플렛-자연 언어" 쌍이다. 웹샵[85]에서 저자는 아마존닷컴에서 118만 개의 실제 제품을 수집하여 시뮬레이션된 전자 상거래 웹사이트에 게시하며, 여기에는 몇 가지 신중하게 설계된 인간 쇼핑 시나리오가 포함되어 있다. 이 웹사이트를 기반으로 저자는 실제 인간 행동 데이터 세트를 수집하기 위해 13명의 직원을 모집한다. 마지막으로, 휴리스틱 규칙, 모방 학습 및 강화 학습에 기반한 세 가지 방법이 이 데이터 세트를 기반으로 학습된다. 비록 저자들이 LLM 기반 에이전트를 미세 조정하지는 않지만, 본 논문에서 제안한 데이터 세트는 웹 쇼핑 분야에서 에이전트의 능력을 향상시킬 수 있는 엄청난 잠재력을 가지고 있다고 믿는다. EduChat[86]에서 저자는 개방형 질문 답변, 논술 평가, 소크라테스적 교수, 정서적 지원과 같은 LLM의 교육적 기능을 향상시키는 것을 목표로 한다. 다양한 교육 시나리오와 작업을 다루는 인간 주석이 달린 데이터 세트를 기반으로 LLM을 미세 조정한다. 이러한 데이터 세트는 심리학 전문가와 일선 교사가 수동으로 평가하고 선별한다. SWIFTSAGE [87]은 복잡한 상호작용 추론 과제 해결에 효과적인 인간 인지[88]의 이중 과정 이론의 영향을 받는 에이전트이다. 이 에이전트에서 SWIFT 모듈은 인간 주석이 있는 데이터 세트를 사용하여 미세 조정되는 컴팩트 인코더-디코더 언어 모델을 구성한다.

\(\bullet\)_LLM 생성 데이터 집합을 사용 하 여 미세 조정_. 인간 주석이 달린 데이터 세트를 구축하려면 사람을 모집해야 하는데, 특히 많은 양의 샘플에 주석을 달아야 할 때 비용이 많이 들 수 있다. LLM이 광범위한 작업에서 인간과 유사한 기능을 달성할 수 있다는 점을 고려할 때, 자연스러운 아이디어는 주석 작업을 수행하기 위해 LLM을 사용하는 것이다. 이 방법에서 생성된 데이터 세트는 인간 주석이 달린 데이터 세트만큼 완벽하지 않을 수 있지만 훨씬 저렴하고 더 많은 샘플을 생성하는 데 활용할 수 있다. 예를 들어 ToolBench [14]에서 오픈 소스 LLM의 도구 사용 기능을 향상시키기 위해 저자는 RapidAPI Hub에서 49개 범주에 걸쳐 있는 16,464개의 실제 API를 수집합니다. 그들은 이러한 API를 사용하여 ChatGPT를 프롬프트하여 단일 도구 및 다중 도구 시나리오를 모두 포함하는 다양한 지침을 생성했다. 구해진 데이터셋을 바탕으로 LLaMA [9]를 미세 조정하고, 도구 측면에서 상당한 성능 향상을 얻었다. [82]에서 에이전트에 사회적 능력을 부여하기 위해 저자는 샌드박스를 설계하고 여러 에이전트를 배치하여 상호 작용합니다. 사회적 질문이 주어지면, 중심 에이전트는 먼저 초기 응답을 생성한다. 그런 다음 피드백을 수집하기 위해 주변 에이전트에 대한 응답을 공유합니다. 피드백과 상세한 설명을 바탕으로 중심 주체는 초기 대응을 수정하여 사회적 규범에 보다 부합하도록 한다. 이 과정에서 저자는 많은 양의 에이전트 소셜 상호작용 데이터를 수집한 다음 LLM을 미세 조정하는 데 활용한다.

* _실제 데이터 집합을 사용 하 여 미세 조정_ 합니다. 인간 또는 LLM 주석을 기반으로 데이터 세트를 구축하는 것 외에도 실제 데이터 세트를 직접 사용하여 에이전트를 미세 조정하는 것도 일반적인 전략이다. 예를 들어 MIND2WEB [89]에서 저자는 웹 도메인에서 에이전트 기능을 향상시키기 위해 많은 양의 실제 데이터 세트를 수집한다. 선행 연구와 달리 본 논문에서 제시한 데이터셋은 다양한 작업, 실제 시나리오 및 포괄적인 사용자 상호 작용 패턴을 포괄한다. 특히, 저자는 31개 도메인에 걸쳐 있는 137개의 실제 웹 사이트에서 2,000개 이상의 개방형 작업을 수집한다. 이 데이터 세트를 사용하여 저자는 LLM을 미세 조정하여 영화 검색 및 티켓 예매를 포함한 웹 관련 작업에서 성능을 향상시킵니다. SQL-PALM [90]에서 연구자들은 스파이더라는 교차 도메인 대규모 텍스트 대 SQL 데이터 세트를 기반으로 PaLM-2를 미세 조정합니다. 획득한 모델은 텍스트 대 SQL 작업에서 상당한 성능 향상을 달성할 수 있습니다.

**미세 조정 없이 기능 획득**: 전통 기계 학습 시대에 모델 기능은 주로 데이터가 모델 매개 변수로 인코딩되는 데이터 세트에서 학습하여 획득됩니다. LLM 시대에는 모델 파라미터를 훈련/미세 조정하거나 섬세한 프롬프트(_i.e._, 프롬프트 엔지니어)를 설계함으로써 모델 능력을 얻을 수 있다. 프롬프트 엔지니어에서는 모델 능력을 향상시키거나 기존 LLM 능력을 발휘하기 위해 프롬프트에 귀중한 정보를 작성해야 한다. 에이전트 시대에 모델 역량은 (1) 모델 미세 조정, (2) 신속한 엔지니어 및 (3) 적절한 에이전트 진화 메커니즘 설계(이를 _메커니즘 엔지니어링_이라고 함)의 세 가지 전략을 기반으로 얻을 수 있다. 기계 공학은 전문 모듈 개발, 새로운 작업 규칙 도입 및 에이전트 기능을 향상시키기 위한 기타 전략을 포함하는 광범위한 개념이다. 모델 능력 획득 전략에 대한 이러한 전환을 명확하게 이해하기 위해 그림 4에서 설명한다. 다음에서는 에이전트 능력 획득을 위한 프롬프트 엔지니어링 및 메커니즘 엔지니어링을 소개한다.

\(\bullet\)_Prompting Engineering_. 강한 언어 이해 능력으로 인해 사람들은 자연 언어를 사용하여 LLM과 직접 상호 작용할 수 있다. 이것은 에이전트 능력을 향상시키기 위한 새로운 전략, 즉 자연 언어를 사용하여 원하는 능력을 설명한 다음 LLM 행동에 영향을 미치는 프롬프트로 사용할 수 있다. 예를 들어 CoT [45]에서 에이전트에 복잡한 작업 추론을 위한 능력을 권한 부여하기 위해 저자는 중간 추론 단계를 적은 샷으로 제시한다.

그림 4: 모델 역량 획득 전략에서의 전환에 대한 예시이다.

프롬프트의 예제입니다. 유사한 기술들이 CoT-SC[49] 및 ToT[50]에서도 사용된다. 소셜AGI[30]에서는 대화에서 에이전트 자기 인식 능력을 향상시키기 위해, 저자들은 청자와 그 자체의 정신 상태에 대한 에이전트 신념을 LLMs에 촉구하여 생성된 발화를 더 매력적이고 적응적으로 만든다. 또한, 저자들은 또한 청취자의 목표 정신 상태를 통합함으로써 에이전트가 보다 전략적인 계획을 세울 수 있도록 한다. Retroformer[91]는 에이전트가 과거의 실패에 대한 반사를 생성할 수 있게 하는 후향적 모델을 제시한다. 반사는 LLM 프롬프트에 통합되어 에이전트의 향후 작업을 안내합니다. 또한, 이 모델은 강화 학습을 활용하여 후향적 모델을 반복적으로 개선하여 LLM 프롬프트를 정제한다.

\(\bullet\)_기계공학_. 모델 미세 조정 및 신속한 엔지니어링과 달리 메커니즘 엔지니어링은 에이전트 기능을 향상시키기 위한 고유한 전략입니다. 다음에서는 메커니즘 엔지니어링의 몇 가지 대표적인 방법을 제시한다.

(1) _시행 및 오류_. 이 방법에서 에이전트는 먼저 액션을 수행하고, 이어서 미리 정의된 비평을 호출하여 액션을 판단한다. 행위가 불만족스러운 것으로 간주되면 에이전트는 비평가의 피드백을 통합하여 반응한다. RAH[92]에서, 에이전트는 추천 시스템들에서 사용자 어시스턴트로서 역할을 한다. 에이전트의 중요한 역할 중 하나는 사용자의 행동을 시뮬레이션하고 사용자를 대신하여 응답을 생성하는 것이다. 이 목적을 달성하기 위해 에이전트는 먼저 예측된 응답을 생성한 다음 실제 인간 피드백과 비교한다. 예측된 반응과 실제 인간의 피드백이 다르면 비평가는 실패 정보를 생성하며, 이는 이후 에이전트의 다음 행동에 통합된다. DEPS[33]에서 에이전트는 먼저 주어진 작업을 수행하기 위한 계획을 설계한다. 계획 실행 프로세스에서, 액션이 실패하면, 설명자는 실패의 원인을 설명하는 구체적인 세부사항을 생성한다. 그런 다음 에이전트가 이 정보를 통합하여 계획을 재설계합니다. RoCo[93]에서 에이전트는 먼저 멀티 로봇 협업 작업에서 각 로봇에 대한 서브 작업 계획과 3D 웨이포인트의 경로를 제안한다. 그런 다음 계획 및 웨이포인트는 충돌 감지 및 역기구학과 같은 환경 검사 세트에 의해 검증된다. 검사 중 하나가 실패하면 피드백이 각 에이전트의 프롬프트에 추가되고 다른 대화 상자가 시작됩니다. 에이전트는 LLM을 사용하여 모든 검증을 통과할 때까지 계획 및 웨이포인트를 논의하고 개선합니다. PREFER [94]에서 에이전트는 먼저 데이터의 하위 집합에 대한 성능을 평가한다. 특정 예제를 해결하지 못하면 LLMs를 활용하여 실패 이유를 반영한 피드백 정보를 생성한다. 이러한 피드백에 기초하여 에이전트는 자신의 행동을 반복적으로 정제함으로써 스스로 개선한다.

(2) 크라우드 소싱_. [95]에서 저자는 에이전트 능력을 향상시키기 위해 군중의 지혜를 활용하는 토론 메커니즘을 설계한다. 우선, 서로 다른 에이전트는 주어진 질문에 대해 별도의 응답을 제공한다. 응답이 일관되지 않으면 다른 에이전트의 솔루션을 통합하고 업데이트된 응답을 제공하라는 메시지가 표시됩니다. 이러한 반복적인 과정은 최종 합의된 답변에 도달할 때까지 계속된다. 이 방법은 다른 에이전트의 의견을 이해하고 통합하여 각 에이전트의 능력을 향상시킨다.

(3) _경험 축적_. GITM[16]에서 에이전트는 처음에 작업을 해결하는 방법을 알지 못한다. 그런 다음 탐색을 수행하고 작업을 성공적으로 수행하면 이 작업에 사용된 작업이 에이전트 메모리에 저장됩니다. 향후 에이전트가 유사한 태스크를 만나면 관련 메모리들을 추출하여 현재 태스크를 완성한다. 이 과정에서 개선된 에이전트 ca-pability는 특별히 설계된 메모리 축적 및 활용 메커니즘에서 비롯된다. 보이저[38]에서, 저자들은 에이전트에게 스킬 라이브러리를 장착하고, 라이브러리의 각각의 스킬은 실행가능 코드들로 표현된다. 에이전트-환경 상호작용 과정에서 각 스킬에 대한 코드는 환경 피드백과 에이전트 자체 검증 결과에 따라 반복적으로 정제될 것이다. 실행 기간 후에, 에이전트는 스킬 라이브러리에 액세스함으로써 상이한 태스크들을 효율적으로 성공적으로 완료할 수 있다. AppAgent[96]에서, 에이전트는 인간 사용자들과 유사한 방식으로 앱들과 상호작용하도록 설계되어, 인간 시연들의 자율적 탐색 및 관찰 모두를 통해 학습한다. 이 과정을 통해, 그것은 모바일 폰 상의 다양한 애플리케이션들에 걸쳐 복잡한 태스크들을 수행하기 위한 참조로서 역할을 하는 지식 베이스를 구성한다. MemPrompt[97]에서는 사용자에게 에이전트의 문제 해결 의도와 관련하여 자연어로 피드백을 제공하도록 요청하며, 이 피드백은 메모리에 저장된다. 에이전트가 유사한 태스크들을 만날 때, 더 적합한 응답들을 생성하기 위해 관련 메모리들을 검색하려고 시도한다.

(4) _Self-driven Evolution._ LMA3[98]에서 에이전트는 스스로 자율적으로 목표를 설정할 수 있으며, 환경을 탐색하고 보상 함수로부터 피드백을 받아 점차 능력을 향상시킬 수 있다. 이 메커니즘에 따라 에이전트는 지식을 습득하고 자신의 선호도에 따라 능력을 개발할 수 있다. SALLM-MS [99]에서는 GPT-4와 같은 고급 대형 언어 모델을 다중 에이전트 시스템에 통합함으로써 에이전트가 복잡한 작업을 적응하고 수행할 수 있으며 고급 통신 기능을 보여줌으로써 환경과의 상호 작용에서 자체 주도 진화를 실현한다. CLMTWA[100]에서, 교사로서 큰 언어 모델을 사용하고 학생으로서 약한 언어 모델을 사용함으로써, 교사는 마음 이론을 통해 학생의 추론 능력을 향상시키기 위해 자연 언어 설명을 생성하고 통신할 수 있다. 교사는 또한 개입의 기대 효용을 바탕으로 학생에 대한 설명을 개인화하고 필요할 때만 개입할 수 있다. NLSOM[101]에서는 서로 다른 에이전트가 자연어를 통해 소통하고 협업하여 하나의 에이전트가 해결할 수 없는 작업을 해결한다. 이는 다수의 에이전트들 간의 정보와 지식의 교환을 활용한 자기주도적 학습의 한 형태라고 볼 수 있다. 그러나, NLSOM은 LMA3, SALLM-MS, CLMTWA 등의 다른 모델들과 달리, 다른 에이전트들 또는 환경으로부터의 피드백 및 태스크 요건들에 기초하여 에이전트 역할들, 태스크들, 및 관계들의 동적 조정을 허용한다.

_비고___ 앞서 언급한 에이전트 능력 획득 전략을 비교한 결과, 미세 조정 방법은 모델 파라미터를 조정하여 에이전트 능력을 향상시키며, 이는 많은 양의 태스크 특정 지식을 통합할 수 있지만 오픈 소스 LLM에만 적합하다는 것을 알 수 있다. 미세 조정이 없는 방법은 일반적으로 정교한 프롬프트 전략이나 메커니즘 엔지니어링을 기반으로 에이전트 기능을 향상시킵니다. 오픈 소스 및 폐쇄 소스 LLM 모두에 사용할 수 있습니다. 그러나 LLM의 입력 컨텍스트 창의 한계로 인해 너무 많은 태스크 정보를 통합할 수 없다. 또한 프롬프트와 메커니즘의 설계 공간이 매우 커서 최적의 솔루션을 찾기가 쉽지 않다.

위의 섹션에서는 LLM 기반 에이전트의 구성을 자세히 설명했으며 아키텍처 설계와 능력 획득을 포함한 두 가지 측면에 중점을 둔다. 우리는 표 1에 기존 작업과 위의 분류법 간의 일치성을 제시한다. 무결성을 위해 LLM 기반 에이전트를 명시적으로 언급하지 않았지만 이 영역과 관련이 높은 여러 연구를 통합했다는 점에 유의해야 한다.

## 3 LLM 기반 자율 에이전트 애플리케이션

강력한 언어 이해, 복잡한 과제 추론, 상식 이해 능력으로 인해 LLM 기반 자율 에이전트는 여러 영역에 영향을 미칠 수 있는 상당한 잠재력을 보여주었다. 이 섹션에서는 이전 연구를 간결하게 요약하여 사회 과학, 자연 과학 및 공학의 세 가지 별개의 영역에서 응용 프로그램에 따라 분류한다(글로벌 개요는 그림 5의 왼쪽 부분을 참조).

### Social Science

사회과학은 사회와 그 사회 내의 개인들 사이의 관계에 대한 연구에 전념하는 과학의 한 분야이다. LLM 기반 자율 에이전트는 인상적인 인간 유사 이해, 사고 및 과제 해결 능력을 활용하여 이 영역을 촉진할 수 있다. 이하에서는 LLM 기반 자율 에이전트의 영향을 받을 수 있는 몇 가지 핵심 영역에 대해 논의한다.

**심리학**: 심리학 영역의 경우 LLM 기반 에이전트를 활용하여 시뮬레이션 실험을 수행하고 정신 건강 지원 등을 제공할 수 있습니다. [102, 103, 104, 105]. 예를 들어 [102]에서 저자는 프로필이 다른 LLM을 할당하고 심리 실험을 완료하도록 한다. 결과에서 저자는 LLM이 인간 참가자를 포함하는 연구의 결과와 일치하는 결과를 생성할 수 있음을 발견했다. 또한, 더 큰 모델은 더 작은 모델에 비해 더 정확한 시뮬레이션 결과를 제공하는 경향이 있음을 관찰했다. 흥미로운 발견은 많은 실험에서 ChatGPT 및 GPT-4와 같은 모델이 다운스트림 애플리케이션에 영향을 미칠 수 있는 너무 완벽한 추정치("초정밀 왜곡"이라고 함)를 제공할 수 있다는 것이다. [104]에서 저자들은 정신적 웰빙 지원을 위한 LLM 기반 대화 에이전트의 효과를 체계적으로 분석한다. 그들은 레딧으로부터 120개의 게시물을 수집하고, 그러한 에이전트가 사용자가 불안, 사회적 고립 및 요구에 따른 우울증에 대처하는 데 도움이 될 수 있다는 것을 발견한다. 동시에, 그들은 또한 에이전트가 때때로 유해한 내용물을 생성할 수 있다는 것을 발견합니다.

**정치 및 경제**: LLM 기반 에이전트를 사용하여 정치 및 경제를 연구할 수도 있습니다. [105, 29, 106]. [29]에서는 이념 탐지 및 투표 패턴 예측을 위해 LLM 기반 에이전트를 활용한다. [105]에서 저자는 LLM 기반 에이전트의 지원을 통해 정치 연설의 담론 구조와 설득 요소를 이해하는 데 중점을 둔다. [106]에서, LLM

그림 5: LLM 기반 에이전트의 적용(왼쪽) 및 평가 전략(오른쪽)이다.

[MISSING_PAGE_EMPTY:23]

언어 이해와 텍스트 처리를 위한 인터넷 및 데이터베이스와 같은 도구를 사용하는 데 강한 능력을 보여주었다. 이러한 기능을 통해 에이전트는 문서화 및 데이터 관리와 관련된 작업을 탁월하게 수행할 수 있습니다[115, 75, 116]. [115]에서 에이전트는 인터넷 정보를 효율적으로 질의하고 활용하여 질의 응답, 실험 계획 등의 작업을 완료할 수 있다. ChatMOF [116]은 LLMs을 활용하여 인간이 작성한 텍스트 기술에서 중요한 정보를 추출한다. 그런 다음 금속 유기 골격의 특성과 구조를 예측하기 위한 관련 도구를 적용하는 계획을 수립한다. ChemCrow[75]는 화학 관련 데이터베이스를 활용하여 화합물 표현의 정밀도를 검증하고 잠재적으로 위험한 물질을 식별한다. 이 기능은 관련된 데이터의 정확성을 보장하여 과학적 문의의 신뢰성과 포괄성을 향상시킵니다.

**실험 보조자**: LLM 기반 에이전트는 독립적으로 실험을 수행할 수 있으므로 연구 프로젝트에서 과학자를 지원하는 데 유용한 도구입니다. [75, 115]. 예를 들어, [115]는 과학 실험의 설계, 계획 및 실행을 자동화하기 위해 LLM을 활용하는 혁신적인 에이전트 시스템을 소개한다. 이 시스템은 실험 목표를 입력으로 제공하면 인터넷에 접속하여 관련 문서를 검색하여 필요한 정보를 수집한다. 이후 파이썬 코드를 활용하여 필수적인 계산을 수행하고 다음과 같은 실험을 수행한다. ChemCrow[75]는 연구자들의 화학 연구를 돕기 위해 특별히 고안된 17개의 신중하게 개발된 도구를 통합한다. 입력 목표가 수신되면 ChemCrow는 실험 절차에 대한 귀중한 권장 사항을 제공하는 동시에 제안된 실험과 관련된 잠재적인 안전 위험을 강조한다.

**자연 과학 교육**: LLM 기반 에이전트는 인간과 유창하게 통신할 수 있으며 종종 에이전트 기반 교육 도구를 개발하는 데 사용됩니다. 예를 들어, [115]는 실험 설계, 방법론 및 분석의 학습을 용이하게 하기 위해 에이전트 기반 교육 시스템을 개발한다. 이러한 시스템의 목적은 학생들의 비판적 사고와 문제 해결 능력을 향상시키는 동시에 과학적 원리에 대한 더 깊은 이해를 증진시키는 것이다. 수학 에이전트 [117]은 연구자들이 수학 문제를 탐구, 발견, 해결 및 증명하는 데 도움을 줄 수 있다. 또한 인간과 소통하고 수학을 이해하고 사용하는 데 도움을 줄 수 있다. [118]은 코드X [119]의 능력을 활용하여 대학 수준의 수학 문제를 자동으로 해결하고 설명하는 데 도움을 주며, 이는 학생과 연구자를 가르치는 교육 도구로 사용될 수 있다. 코드헬프 [120]은 프로그래밍을 위한 교육 에이전트이다. 과정별 키워드 설정, 학생 질의 모니터링, 시스템에 피드백 제공 등 많은 유용한 기능을 제공한다. EduChat [86]은 교육 영역을 위해 특별히 설계된 LLM 기반 에이전트이다. 교사와 학생, 학부모에게 대화를 통해 개인화되고 공평하며 공감적인 교육 지원을 제공한다. FreeText [121]은 LLM을 활용하여 개방형 질문에 대한 학생들의 반응을 자동으로 평가하고 피드백을 제공하는 에이전트이다.

### Engineering

LLM 기반 자율 에이전트는 공학 연구 및 응용 프로그램을 지원하고 향상시키는 데 큰 잠재력을 보여주었다. 이 섹션에서는 몇 가지 주요 엔지니어링 영역에서 LLM 기반 에이전트의 응용 프로그램을 검토하고 요약한다.

**토목 공학**: 토목 공학에서 LLM 기반 에이전트를 사용하여 건물, 교량, 댐, 도로 등과 같은 복잡한 구조를 설계하고 최적화할 수 있습니다. [138] 3D 시뮬레이션 환경에서 인간 설계자와 에이전트가 협력하여 구조물을 구성하는 대화형 프레임워크를 제안한다. 대화형 에이전트는 자연어 명령어를 이해하고, 블록을 배치하고, 혼동을 감지하고, 명확성을 찾고, 인간 피드백을 통합할 수 있어 공학 설계에서 인간-AI 협업의 가능성을 보여준다.

**컴퓨터 과학 및 소프트웨어 공학**: 컴퓨터 과학 및 소프트웨어 공학 분야에서 LLM 기반 에이전트는 코딩, 테스트, 디버깅 및 문서 생성을 자동화할 수 있는 잠재력을 제공합니다. [14; 18; 23; 24; 126; 127; 128]. Chat-Dev [18]은 엔드 투 엔드 프레임워크를 제안하며, 여기서 다수의 에이전트 역할들은 소프트웨어 개발 라이프 사이클을 완성하기 위해 자연어 대화를 통해 통신하고 협업한다. 이 프레임워크는 실행 가능한 소프트웨어 시스템의 효율적이고 비용 효율적인 생성을 보여준다. ToolBench[14]는 코드 자동 완성, 코드 추천 등의 작업에 사용될 수 있다. MetaGPT[23]은 제품 관리자, 건축가, 프로젝트 관리자 및 엔지니어와 같은 다중 역할을 추상화하여 코드 생성 프로세스를 감독하고 최종 출력 코드의 품질을 향상시킨다. 이것은 저가의 소프트웨어 개발을 가능하게 한다. [24] LLM을 이용한 코드 생성을 위한 자체 협업 프레임워크를 제시한다. 이 프레임워크에서 다중 LLM은 특정 하위 작업에 대해 별개의 "전문가"로 가정된다. 그들은 지정된 지침에 따라 협업하고 상호 작용하여 서로의 작업을 용이하게 하는 가상 팀을 구성합니다. 궁극적으로 가상 팀은 인간의 개입 없이 코드 생성 작업을 협력적으로 해결합니다. LLIFT [139]는 특히 잠재적인 코드 취약성을 식별하기 위해 정적 분석을 수행하는 데 도움을 주기 위해 LLM을 사용한다. 이 접근법은 정확성과 확장성 사이의 트레이드오프를 효과적으로 관리한다. ChatEDA[123]은 작업 계획, 스크립트 생성, 실행을 통합하여 설계 프로세스를 간소화하기 위해 전자 설계 자동화(EDA)를 위해 개발된 에이전트이다. CodeHelp [120]은 학생과 개발자가 디버깅 및 테스트를 수행할 수 있도록 지원하는 에이전트입니다.

\begin{table}
\begin{tabular}{p{56.9pt}|p{113.8pt} p{113.8pt}} \hline \hline  & Domain & Work \\ \hline \multirow{4}{*}{Social Science} & \multirow{2}{*}{Psychology} & TE [102], Akata et al. [103], Ziems et al. [105], Ma et al. [104] \\ \cline{2-3}  & Political Science and Economy & Out of One [29], Horton [106], Ziems et al. [105] \\ \cline{2-3}  & \multirow{2}{*}{Social Simulation} & Social Simulacra [79], Generative Agents [20], SocialAI School [109], AgentSims [34], S\({}^{3}\)[77], Williams et al. [110], Li et al. [107], Chao et al. [108] \\ \cline{2-3}  & \multirow{2}{*}{Jurisprudence} & ChatLaw [112], Blind Judgement [113] \\ \cline{2-3}  & Research Assistant & Ziems et al [105], Bail et al. [114] \\ \hline \multirow{4}{*}{Natural Science} & Documentation and Data Management & ChemCrow [75], Boiko et al. [115] \\ \cline{2-3}  & Experiment Assistant & ChemCrow [75], Boiko et al. [115], Grossmann et al. [122] \\ \cline{2-3}  & \multirow{2}{*}{Natural Science} & ChemCrow [75], CodeHelp [120], Boiko et al. [115], MathA-Science & \\ \cline{2-3}  & & BestGPT [70], Self-collaboration [24], SQL-PALM [90], RAH [92], DBGPT [41], RecMind [51], ChatEDA [123], InteRecAgent [124], PentestGPT [125], CodeHelp [120], SmolModels [126], DemoGPT [127], GPTEngineer [128] \\ \cline{2-3}  & Industrial Automation & GPT4IA [129], IELLM [130], TaskMatrix.AI [71] \\ \cline{2-3}  & \multirow{4}{*}{Robotics \& EDBodied AI} & GPT4IA [129], IELLM [130], TaskMatrix.AI [71] \\ \cline{2-3}  & & ProAgent [131], LLM4RL [132], PET [133], REMEMBERER [134], DEPS [33], Unified Agent [135], SayCan [78], LMMWM [136], TidyDit [137], RoCo [93], SayPlan [31] \\ \hline \hline \end{tabular}
\end{table}
표 2: LLM 기반 자율 에이전트의 대표적인 응용.

code. 오류 메시지에 대한 자세한 설명을 제공하고 잠재적인 수정 사항을 제안하며 코드의 정확성을 보장하는 것이 특징입니다. PENTESTGPT [125]는 LLMs 기반의 침투 테스트 도구로서, 공통의 취약점을 효과적으로 식별하고 소스 코드를 해석하여 익스플로잇을 개발할 수 있다. DB-GPT [41]은 LLMs의 기능을 활용하여 데이터베이스의 변칙의 잠재적인 근본 원인을 체계적으로 평가한다. 사고 트리의 구현을 통해 DB-GPT는 LLM이 현재 단계가 실패로 판명될 경우 이전 단계로 역추적할 수 있도록 하여 진단 프로세스의 정확도를 향상시킨다.

**산업 자동화**: 산업 자동화 분야에서 LLM 기반 에이전트를 사용하여 생산 프로세스의 지능형 계획 및 제어를 달성할 수 있습니다. [129] 유연한 생산 요구를 수용하기 위해 대용량 언어 모델(LLM)과 디지털 트윈 시스템을 통합하는 새로운 프레임워크를 제안한다. 프레임워크는 신속한 엔지니어링 기술을 활용하여 디지털 트윈이 제공한 정보를 기반으로 특정 작업에 적응할 수 있는 LLM 에이전트를 생성한다. 이러한 에이전트는 일련의 원자 기능 및 기술을 조정하여 자동화 피라미드 내에서 다양한 수준에서 생산 작업을 완료할 수 있다. 이 연구는 LLM을 산업 자동화 시스템에 통합하여 보다 민첩하고 유연하며 적응적인 생산 프로세스를 위한 혁신적인 솔루션을 제공할 수 있는 가능성을 보여준다. IELLM[130]은 석유 및 가스 산업에서 LLMs의 역할에 대한 사례 연구를 보여주며, 암석 물리학, 음향 반사 측정 및 코일형 튜빙 제어와 같은 응용 분야를 다룬다.

**로봇 & Embodied Artificial Intelligence**: 최근 연구에서는 로봇 공학을 위한 보다 효율적인 강화 학습 에이전트를 개발하고 인공 지능을 구현했습니다[16; 140; 132; 133; 134; 135; 141; 136; 137; 138; 139; 140; 142; 143]. 특히 [140]은 체화된 환경에서 자율 에이전트의 계획, 추론 및 협업을 위한 능력을 향상시키는 데 중점을 둔다. 특히 [140]은 체화된 추론 및 작업 계획을 위한 통합된 에이전트 시스템을 제안한다. 이 시스템에서 저자는 명령을 행동으로 변환하기 위해 하위 수준의 컨트롤러를 제안합니다. 또한 대화 내용을 활용하여 정보를 수집할 수 있습니다. [141]은 최적화 프로세스를 가속화하기 위해 정보를 수집할 수 있습니다. [142; 143]은 체화된 의사 결정 및 탐색을 위한 자율 에이전트를 사용합니다. 에이전트는 여러 기술을 활용하여 실행 가능한 계획을 생성하고 장기적인 작업을 수행할 수 있습니다. 제어 정책 측면에서 SayCan [78]은 모바일 매니퓰레이터 로봇을 사용하여 광범위한 조작 및 탐색 기술을 조사할 수 있습니다. 제어 정책 측면에서 세이캔 [78]은 주방 환경에서 마주치는 일반적인 작업에서 영감을 받아 7개의 스킬 패밀리 및 17개의 객체를 포함하는 포괄적인 551개의 스킬 세트를 제공합니다. 이러한 스킬은 대상을 선택, 배치, 붓기, 파지 및 조작 등 다양한 작업을 포함합니다. TidyBot [137]은 가정 정리 작업을 개인화하기 위해 설계된 체화된 에이전트입니다. 텍스트 예를 통해 객체 배치 및 조작 방법에 대한 사용자의 선호도를 학습할 수 있습니다.

LLM 기반 자율 에이전트의 적용을 촉진하기 위해 연구자들은 또한 많은 오픈 소스 라이브러리를 도입했으며, 이를 기반으로 개발자가 맞춤형 요구 사항에 따라 에이전트를 신속하게 구현하고 평가할 수 있다[144; 145; 146; 147; 148; 149; 150; 151; 152; 153; 154; 155; 156; 157]. 예를 들어, LangChain[149]는 코딩, 테스트, 디버깅 및 문서 생성 작업을 자동화하는 오픈 소스 프레임워크이다. 언어 모델을 데이터 소스와 통합하고 환경과의 상호 작용을 촉진함으로써 랭체인은 자연어 통신과 여러 에이전트 역할 간의 협업을 통해 효율적이고 비용 효율적인 소프트웨어 개발을 가능하게 한다. 랭체인을 기반으로 XLang[147]은 포괄적인 도구 세트, 완전한 사용자 인터페이스와 함께 제공되며 데이터 처리, 플러그인 사용 및 웹 에이전트의 세 가지 다른 에이전트 시나리오를 지원합니다. AutoGPT [81]은 완전히 자동화된 에이전트입니다. 하나 이상의 목표를 설정하고, 이를 대응하는 과제로 나누고, 목표가 달성될 때까지 과제를 순환시킨다. WorkGPT[150]은 AutoGPT 및 LangChain과 유사한 에이전트 프레임워크이다. 지시와 API 세트를 제공하여 지시가 완료될 때까지 AI와 전후 대화를 한다. GPT-Engineer [128], SmolModels [126] 및 DemoGPT [127]은 개발 작업을 완료하기 위한 프롬프트를 통해 코드 생성을 자동화하는 데 중점을 두는 오픈 소스 프로젝트입니다. AGiXT[146]은 많은 공급자에 걸쳐 효율적인 AI 명령 관리 및 작업 실행을 오케스트레이션하도록 설계된 동적 AI 자동화 플랫폼이다. AgentVerse[19]는 연구자들이 맞춤형 LLM 기반 에이전트 시뮬레이션을 효율적으로 만드는 데 도움이 되는 다목적 프레임워크이다. GPT 연구자[152]는 대규모 언어 모델을 활용하여 연구 문제를 효율적으로 개발하고, 웹 크롤링을 유발하여 정보를 수집하고, 출처를 요약하고, 요약을 집계하는 실험 애플리케이션이다. BMTools[153]은 LLMs을 도구로 확장하고 커뮤니티 기반 도구 구축 및 공유를 위한 플랫폼을 제공하는 오픈 소스 저장소이다. 다양한 유형의 도구를 지원하고, 여러 도구를 사용하여 동시 작업 실행이 가능하며, URL을 통해 플러그인을 로드할 수 있는 간단한 인터페이스를 제공하여 쉬운 개발 및 BMTools 생태계에 대한 기여를 촉진한다.

_Remark_.: 위의 애플리케이션을 지원하는 데 LLM 기반 에이전트의 활용은 또한 위험과 도전을 수반할 수 있다. 한편으로 LLM 자체는 환상과 다른 문제에 취약할 수 있으며 때때로 잘못된 답변을 제공하여 잘못된 결론, 실험적 실패 또는 위험한 실험에서 인간의 안전에 위험을 초래할 수 있다. 따라서 실험 중 사용자는 적절한 주의를 기울이기 위해 필요한 전문 지식과 지식을 보유해야 한다. 반면에 LLM 기반 에이전트는 잠재적으로 화학 무기 개발과 같은 악의적인 목적으로 악용될 수 있으므로 책임 있고 윤리적인 사용을 보장하기 위해 인간 정렬과 같은 보안 조치의 구현이 필요하다.

요약하면, 위의 섹션에서 우리는 세 가지 중요한 도메인에서 LLM 기반 자율 에이전트의 전형적인 응용을 소개한다. 보다 명확한 이해를 돕기 위해 이전 연구와 각 응용 프로그램 간의 관계를 표 2에 요약했다.

## 4 LLM 기반 자율 에이전트 평가

LLM 자체들과 유사하게, LLM 기반 자율 에이전트들의 유효성을 평가하는 것은 도전적인 과제이다. 이 섹션에서는 평가에 대한 두 가지 일반적인 접근법, 즉 주관적 방법과 객관적 방법을 설명한다. 종합적인 개요는 그림 5의 오른쪽 부분을 참조해 주세요.

### Subjective Evaluation

주관적 평가는 인간의 판단에 기초하여 대리인 능력을 측정한다[20, 22, 29, 79, 158]. 평가 데이터 세트가 없거나 에이전트의 지능 또는 사용자 친화성을 평가하는 것과 같은 정량적 메트릭을 설계하는 것이 매우 어려운 시나리오에 적합하다. 다음에서는 주관적 평가를 위해 일반적으로 사용되는 두 가지 전략을 제시한다.

**인간 주석**: 이 평가 방법은 인간 평가자가 직접 다양한 에이전트에서 생성된 출력의 점수를 매기거나 순위를 매기는 작업을 포함합니다. [22, 29, 105]. 예를 들어 [20]에서 저자는 많은 주석자를 사용하고 에이전트 기능과 직접 관련된 5가지 핵심 질문에 대한 피드백을 제공하도록 요청합니다. 유사하게, [159]는 인간 참가자가 무해성, 정직성, 유용성, 참여 및 편향성에 대해 모델을 평가하도록 하여 모델 효과를 평가하고, 후속적으로 다양한 모델에 걸쳐 이러한 점수를 비교한다. [79]에서 주석자는 특별히 설계된 모델이 온라인 커뮤니티 내에서 규칙의 개발을 크게 향상시킬 수 있는지 여부를 결정하도록 요청받는다.

**튜링 테스트**: 이 평가 전략은 인간 평가자가 에이전트에서 생성한 출력과 인간이 생성한 출력을 구별해야 합니다. 주어진 과제에서 평가자들이 에이전트와 인간의 결과를 분리할 수 없다면 에이전트가 이 과제에서 인간과 같은 성과를 달성할 수 있음을 보여준다. 예를 들어, [29]의 연구자들은 자유 형식 빨치산 텍스트에 대한 실험을 수행하고 인간 평가자는 반응이 인간 또는 LLM 기반 에이전트에서 나온 것인지 추측하도록 요청받는다. [20]에서, 인간 평가자들은 행동들이 에이전트들로부터 생성되는지 또는 실제-인간들로부터 생성되는지를 식별하기 위해 요구된다. EmotionBench[160]에서, 인간 주석들은 LLM 소프트웨어 및 인간 참가자들에 의해 표현되는 감정 상태들을 다양한 시나리오들에 걸쳐 비교하기 위해 수집된다. 이 비교는 LLM 소프트웨어의 감성 지능을 평가하기 위한 벤치마크 역할을 하며, 인간과 유사한 성능 및 감정 표현을 모방하는 데 있어 에이전트 능력을 이해하는 미묘한 접근법을 보여준다.

_Remark_: LLM 기반 에이전트는 일반적으로 인간을 위해 설계된다. 따라서 주관적 에이전트 평가는 인간의 기준을 반영하기 때문에 중요한 역할을 한다. 그러나 이 전략은 높은 비용, 비효율성 및 인구 편향과 같은 문제에 직면하기도 한다. 이러한 문제를 해결하기 위해 LLM 자체를 이러한 주관적인 평가를 수행하기 위한 매개체로 사용하는 것을 조사하는 연구자가 증가하고 있다. 예를 들어, ChemCrow [75]에서 연구자들은 GPT를 사용하여 실험 결과를 평가한다. 그들은 작업의 완료와 기본 프로세스의 정확성을 모두 고려합니다. 마찬가지로 ChatEval[161]은 구조화된 토론 형식으로 다양한 후보 모델에 의해 생성된 결과를 비판하고 평가하기 위해 여러 에이전트를 사용하여 새로운 접근법을 도입한다. 평가 목적을 위한 LLM의 이러한 혁신적인 사용은 향후 주관적 평가의 신뢰성과 적용 가능성을 모두 향상시킬 수 있는 가능성이 있다. LLM 기술이 계속 발전함에 따라 이러한 방법은 점점 더 신뢰할 수 있고 광범위한 응용 프로그램을 찾을 수 있어 직접적인 인간 평가의 현재 한계를 극복할 수 있을 것으로 예상된다.

### Objective Evaluation

객관적 평가는 시간이 지남에 따라 계산, 비교 및 추적될 수 있는 정량적 메트릭을 사용하여 LLM 기반 자율 에이전트의 능력을 평가하는 것을 말한다. 주관적 평가와 달리 객관적인 지표는 에이전트 성능에 대한 구체적이고 측정 가능한 통찰력을 제공하는 것을 목표로 한다. 객관적인 평가를 수행하기 위해서는 세 가지 중요한 측면, 즉 평가 메트릭, 프로토콜 및 벤치마크가 있다. 이하에서는 이러한 측면들을 보다 구체적으로 소개한다.

**메트릭**: 에이전트의 유효성을 객관적으로 평가하기 위해 적절한 메트릭을 설계하는 것이 중요하며, 이는 평가 정확도와 포괄성에 영향을 미칠 수 있습니다. 이상적인 평가 메트릭은 에이전트의 품질을 정확하게 반영해야 하며 실제 시나리오에서 사용할 때 인간의 느낌과 일치해야 한다. 기존 작업에서 우리는 다음과 같은 대표적인 평가 척도를 결론지을 수 있다. (1) _작업 성공 메트릭:_ 이러한 메트릭은 에이전트가 작업을 완료 하 고 목표를 달성할 수 있는 정도를 측정 합니다. 일반적인 메트릭은 성공률[12, 57, 59, 22], 보상/점수[22, 59, 138], 커버리지[16], 및 정확도[18, 40, 102]를 포함한다. 값이 높을수록 작업 완료 능력이 더 크다는 것을 나타냅니다. (2) _인간 유사성 메트릭:_ 이러한 메트릭은 에이전트 행동이 인간의 행동과 매우 유사한 정도를 정량화한다. 전형적인 예들은 궤적/위치 정확도[164, 38], 대화 유사성[79, 102], 및 인간 응답들의 모방[29, 102]을 포함한다. 유사도가 높을수록 인간 시뮬레이션 성능이 향상됨을 시사합니다. (3) _효율성 메트릭:_ 에이전트 유효성을 평가하는 데 사용되는 전술한 메트릭과 달리 이러한 메트릭은 에이전트의 효율성을 평가하는 것을 목표로 합니다. 일반적으로 고려되는 메트릭은 계획의 길이[57], 개발과 관련된 비용[18], 추론의 속도[16, 38], 및 명료화 대화의 수[138]를 포함한다.

**프로토콜**: 평가 메트릭 외에도 객관적인 평가를 위한 또 다른 중요한 측면은 이러한 메트릭을 활용하는 방법입니다. 이전 작업에서는 일반적으로 사용되는 평가 프로토콜을 식별할 수 있습니다. (1) _실제 시뮬레이션_ 이 방법에서는 게임 및 대화형 시뮬레이터와 같은 몰입형 환경 내에서 에이전트를 평가합니다. 에이전트는 작업을 수행하는 데 필요합니다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & Subjective & Objective & Benchmark & Time \\ \hline WebShop [85] & - & 1 & 1 & ✓ & 07/2022 \\ Social Simulacra [79] & 1 & 2 & - & 08/2022 \\ TE [102] & - & 2 & - & 08/2022 \\ LIBRO [162] & - & 4 & - & 09/2022 \\ ReAct [59] & - & 1 & ✓ & 10/2022 \\ Out of One, Many [29] & 2 & 2 & 3 & - & 02/2023 \\ DEPS [33] & - & 1 & ✓ & 02/2023 \\ Jalil et al. [163] & - & 4 & - & 02/2023 \\ Reflexion [12] & - & 1 & - & 03/2023 \\ IGLU [138] & - & 1 & ✓ & 04/2023 \\ Generative Agents [20] & 1 & 4 & - & - & 04/2023 \\ ToolBench [153] & - & 3 & ✓ & 04/2023 \\ GITM [16] & - & 1 & ✓ & 05/2023 \\ Two-Failures [164] & - & 3 & - & 05/2023 \\ Voyager [38] & - & 1 & ✓ & 05/2023 \\ SocKET [165] & - & 2 & 1 & ✓ & 05/2023 \\ MobileEnv [166] & - & 1 & ✓ & 05/2023 \\ Clembenben [167] & - & 1 & ✓ & 05/2023 \\ Dialop [168] & - & 2 & ✓ & 06/2023 \\ Feldt et al. [169] & - & 4 & - & 06/2023 \\ CO-LLM [22] & 1 & 1 & - & 07/2023 \\ Tachikuma [170] & 1 & 1 & ✓ & 07/2023 \\ WebArena [171] & - & 1 & ✓ & 07/2023 \\ RocoBench [93] & - & 1 & ✓ & 07/2023 \\ AgentSims [34] & - & 2 & - & 08/2023 \\ AgentBench [172] & - & 3 & ✓ & 08/2023 \\ BOLAA [173] & - & 1 & ✓ & 08/2023 \\ Gentopia [174] & - & 3 & ✓ & 08/2023 \\ EmotionBench [160] & 1 & - & ✓ & 08/2023 \\ PTB [125] & - & 4 & - & 08/2023 \\ \hline \hline \end{tabular}
\end{table}
표 3: 주관적 평가를 위해 1과 2를 사용하여 각각 사람 주석과 튜링 테스트를 나타낸다. 객관적 평가를 위해 환경 시뮬레이션, 사회 평가, 다중 작업 평가, 소프트웨어 테스트를 각각 1, 2, 3, 4로 표현한다. “\(\check{\sim}\)”는 평가가 벤치마크를 기반으로 한다는 것을 나타낸다.

그리고 작업 성공률과 인간 유사성과 같은 메트릭을 활용하여 에이전트의 궤적과 완성된 목표를 기반으로 에이전트의 성능을 평가한다[164, 166, 167, 33, 38, 59, 85, 138, 164, 170]. 이 방법은 실제 시나리오에서 에이전트의 실제 능력을 평가할 것으로 기대된다. (2) _사회적 평가_ 이 방법은 시뮬레이션된 사회에서 에이전트 상호 작용을 기반으로 소셜 지능을 평가하기 위해 메트릭을 활용한다. 팀워크 기술을 평가하기 위한 협력 과제, 논증 추론 분석을 위한 토론, 사회적 적성을 측정하기 위한 인간 연구[165, 173, 34, 79, 102]와 같은 다양한 접근법이 채택되었다. 이러한 접근법들은 협력, 의사소통, 공감, 인간의 사회적 행동 모방과 같은 영역에서 에이전트의 능력을 평가하기 위해 일관성, 정신 이론, 사회적 IQ와 같은 자질을 분석한다. 에이전트에게 복잡한 상호 작용 설정을 적용함으로써 사회적 평가는 에이전트의 더 높은 수준의 사회적 인지에 대한 귀중한 통찰력을 제공한다. (3) _멀티-태스크 평가:_ 이 방법에서, 사람들은 에이전트를 평가하기 위해 상이한 도메인들의 다양한 태스크들의 세트를 사용하며, 이는 오픈-도메인 환경들에서 에이전트 일반화 능력을 효과적으로 측정할 수 있다[165, 172, 173, 166, 85, 29, 85]. (4) _소프트웨어 테스팅:_ 이 방법에서는 연구자들이 테스트 케이스 생성, 버그 재생산, 디버깅 코드, 개발자 및 외부 도구와의 상호 작용과 같은 소프트웨어 테스팅 작업과 같은 작업을 수행하게 함으로써 에이전트를 평가한다[162, 169, 163, 169]. 그런 다음 테스트 커버리지 및 버그 탐지율과 같은 메트릭을 사용하여 LLM 기반 에이전트의 유효성을 측정할 수 있다.

**벤치마크**: 메트릭 및 프로토콜을 고려할 때 중요한 나머지 측면은 평가를 수행하기 위한 적절한 벤치마크를 선택하는 것입니다. 과거에, 사람들은 그들의 실험에 다양한 벤치마크를 사용해 왔다. 예를 들어, 많은 연구자들은 에이전트 기능을 평가하기 위해 ALF-World[59], IGLU[138], 마인크래프트[16, 33, 38]와 같은 시뮬레이션 환경을 벤치마크로 사용한다. Tachikuma[170]는 LLM 기반 에이전트를 평가하기 위한 벤치마크로, 여러 캐릭터와 새로운 객체와의 복잡한 상호 작용을 이해하고 추론할 수 있는 TRP 게임 로그를 활용한다. AgentBench[172]는 다양한 도메인에 걸쳐 자율적인 에이전트로서의 LLM 기반 에이전트를 평가하기 위한 포괄적인 프레임워크를 제공한다. 도구 학습에 기반한 LLM 기반 에이전트의 훈련, 서비스 및 평가를 위한 개방형 플랫폼을 제공한다. WebShop[85]는 LLM 기반 에이전트의 다단계 상호 작용 능력을 평가하기 위한 벤치마크를 제공한다. 웹아레나[171]는 여러 도메인에 걸쳐 있는 포괄적인 웹 사이트 환경을 제공한다. 웹아레나[171]는 복잡한 작업을 완료하기 위해 도구를 사용할 때 추론, 안전, 효율성을 포함한 에이전트 능력을 평가하기 위한 벤치마크이다. GentBench[174]는 다양한 시나리오에 걸쳐 있는 6개의 태스크를 포함하는 벤치마크이다. RocoBench[93]는 협동 로봇 공학에서 적응성 및 일반화를 평가하기 위한 커뮤니케이션 및 조정 전략을 강조한다. 감정-벤치[160]는 8개의 부정적인 감정을 유발하는 400개 이상의 상황을 수집하고 LLM과 인간 피험자의 감정 상태를 자체 보고 척도를 사용하여 측정한다. PEB[125]는 침투 테스트 시나리오에서 LLM 기반 에이전트를 평가하기 위한 벤치마크이며, 13개의 E2E [175]는 챗봇의 정확성과 유용성을 테스트하기 위한 종단 간 벤치마크이다.

_Remark_.: 객관적 평가는 다양한 메트릭을 통해 LLM 기반 에이전트에서 능력의 정량적 분석을 용이하게 한다. 현재 기술이 모든 유형의 에이전트 기능을 완벽하게 측정할 수는 없지만 객관적인 평가는 주관적인 평가를 보완하는 필수적인 통찰력을 제공한다. 객관적 평가를 위한 벤치마크 및 방법론의 지속적인 발전은 LLM 기반 자율 에이전트의 개발 및 이해를 더욱 향상시킬 것이다.

위의 섹션에서는 LLM 기반 자율 에이전트 평가를 위한 주관적 및 객관적 전략을 모두 소개한다. 에이전트의 평가는 이 영역에서 중요한 역할을 한다. 그러나 주관적 평가와 객관적 평가 모두 나름대로의 강점과 약점이 있다. 실제로는 에이전트를 종합적으로 평가하기 위해 결합해야 할 수도 있습니다. 이전 작업과 이러한 평가 전략 간의 대응 관계를 표 3에 요약한다.

## 5 Related Surveve

대규모 언어 모델의 활발한 발전과 함께 다양한 포괄적인 조사가 등장하여 다양한 측면에 대한 자세한 통찰력을 제공했다. [176] LLM의 배경, 주요 발견 및 주류 기술을 확장하여 기존 작업의 방대한 배열을 포괄한다. 한편, [177]은 주로 다양한 다운스트림 태스크들에서의 LLMs들의 애플리케이션들 및 그들의 배치와 연관된 도전들에 초점을 맞춘다. LLM과 인간의 지능을 맞추는 것은 편견이나 환상과 같은 우려를 해결하기 위한 활발한 연구 분야이다. [178] 데이터 수집 및 모델 트레이닝 방법론들을 포함하여, 인간 정렬을 위한 기존의 기법들을 컴파일하였다. 추론은 의사 결정, 문제 해결 및 기타 인지 능력에 영향을 미치는 지능의 중요한 측면입니다. [179] LLM의 추론 능력에 대한 연구 현황을 제시하고, 그들의 추론 능력을 향상시키고 평가하기 위한 접근 방법을 탐구한다. [180] 언어 모델은 추론 능력과 도구를 활용하는 능력, 즉 증강 언어 모델(Augmented Language Models, ALM)으로 향상될 수 있다고 제안한다. 그들은 ALM의 최근 진보에 대한 포괄적인 검토를 수행한다. 대규모 모델의 활용이 보편화됨에 따라 성능을 평가하는 것이 점점 더 중요해지고 있다. [181] LLM을 평가하고, 다운스트림 작업 및 사회적 영향에서 무엇을 평가할지, 어디에서 평가할지, 그리고 그들의 성과를 평가하는 방법에 대해 조명합니다. [182] 또한 다양한 다운스트림 작업에서 LLM의 기능과 한계에 대해 논의한다. 앞서 언급한 연구는 훈련, 적용, 평가 등 대형 모델의 다양한 측면을 포괄한다. 그러나 이 논문 이전에 LLM 기반 에이전트의 빠르게 부상하고 유망한 분야에 특별히 초점을 맞춘 작업은 없다. 본 연구에서는 LLM 기반 에이전트의 구축, 적용 및 평가 프로세스를 포함하는 100개의 관련 작업을 수집했다.

## 6 Challenges

LLM 기반 자율 에이전트에 대한 이전 작업은 많은 놀라운 성공을 거두었지만 이 분야는 아직 초기 단계이며 개발에서 해결해야 할 몇 가지 중요한 과제가 있다. 다음에서는 여러 가지 대표적인 과제를 제시한다.

### Role-playing Capability

기존의 LLM과 달리 자율 에이전트는 다른 작업을 수행하기 위해 특정 역할(예: 프로그램 코더, 연구원 및 화학자)을 수행해야 한다. 따라서 역할극을 위한 에이전트의 능력은 매우 중요하다. LLM들은 영화 리뷰어들과 같은 많은 공통 역할들을 효과적으로 시뮬레이션할 수 있지만, 그들이 정확하게 포착하기 위해 고군분투하는 다양한 역할들과 양상들이 여전히 존재한다. 먼저, LLM은 일반적으로 웹 코퍼스를 기반으로 학습되므로 웹에서 거의 논의되지 않는 역할이나 새로 등장하는 역할에 대해서는 LLM을 잘 시뮬레이션하지 못할 수 있다. 또한, 선행 연구[30]에서는 기존의 LLM이 인간의 인지 심리학적 특성을 잘 모델링하지 못하여 대화 시나리오에서 자기 인식이 부족할 수 있음을 보여주었다. 이러한 문제에 대한 잠재적인 해결책은 LLM을 미세 조정하거나 에이전트 프롬프트/아키텍처를 신중하게 설계하는 것을 포함할 수 있다[183]. 예를 들어, 먼저 흔하지 않은 역할이나 심리학 캐릭터에 대한 실제 인간 데이터를 수집한 다음 이를 활용하여 LLM을 미세 조정할 수 있다. 그러나 미세 조정된 모델이 공통 역할에 대해 여전히 잘 수행되도록 하는 방법은 추가 문제를 제기할 수 있다. 미세 조정 외에도 역할 수행에 대한 LLM 기능을 향상시키기 위해 맞춤형 에이전트 프롬프트/아키텍처를 설계할 수도 있습니다. 그러나 설계 공간이 너무 크기 때문에 최적의 프롬프트/아키텍처를 찾는 것은 쉽지 않다.

### 일반화된 인간 정렬

인간 정렬은 전통적인 LLM에 대해 많이 논의되어 왔다. LLM 기반 자율 에이전트 분야에서, 특히 에이전트가 시뮬레이션에 활용될 때, 우리는 이 개념이 더 깊이 논의되어야 한다고 믿는다. 인간에게 더 나은 서비스를 제공하기 위해 전통적인 LLM은 일반적으로 올바른 인간 가치와 일치하도록 미세 조정되며, 예를 들어 에이전트는 회춘 사회를 위한 폭탄을 만들 계획을 세워서는 안 된다. 그러나 에이전트가 실제 시뮬레이션에 활용될 때 이상적인 시뮬레이터는 잘못된 값을 가진 에이전트를 포함하여 다양한 인간 특성을 정직하게 묘사할 수 있어야 한다. 실제로 인간의 부정적인 측면을 시뮬레이션하는 것은 훨씬 더 중요할 수 있는데, 시뮬레이션의 중요한 목표는 문제를 발견하고 해결하는 것이고 부정적인 측면이 없다는 것은 해결할 문제가 없다는 것을 의미하기 때문이다. 예를 들어, 실제 사회를 시뮬레이션하기 위해, 우리는 행위자가 폭탄을 만들 계획을 세울 수 있도록 허용해야 하고, 그 계획을 실행하기 위해 어떻게 행동할 것인지, 그리고 그 행동의 영향을 관찰해야 할 수도 있다. 이러한 관찰에 기초하여, 사람들은 실제 사회에서 유사한 행동을 멈추기 위해 더 나은 행동을 할 수 있다. 위의 사례에서 영감을 얻은 에이전트 기반 시뮬레이션의 중요한 문제는 일반화된 인간 정렬을 수행하는 방법, 즉 다양한 목적과 응용 분야에서 에이전트가 다양한 인간 가치와 정렬할 수 있어야 한다는 것이다. 그러나 ChatGPT와 GPT-4를 포함한 기존의 강력한 LLM은 대부분 통일된 인간 가치와 일치한다. 따라서 흥미로운 방향은 적절한 프롬프트 전략을 설계하여 이러한 모델을 "재정렬"하는 방법이다.

### Prompt Robustness

에이전트에서 합리적인 동작을 보장하기 위해 설계자는 메모리 및 계획 모듈과 같은 보조 모듈을 LLM에 내장하는 것이 일반적인 관행이다. 그러나 이러한 모듈을 포함하려면 일관된 작동과 효과적인 커뮤니케이션을 용이하게 하기 위해 보다 복잡한 프롬프트의 개발이 필요하다. 이전 연구[184; 185]는 경미한 변경도 실질적으로 다른 결과를 산출할 수 있기 때문에 LLM에 대한 프롬프트의 견고성 부족을 강조했다. 이 문제는 단일 프롬프트가 아니라 하나의 모듈에 대한 프롬프트가 다른 모듈에 영향을 미칠 가능성이 있는 모든 모듈을 고려하는 프롬프트 프레임워크를 포함하기 때문에 자율 에이전트를 구성할 때 더 두드러진다. 또한, 프롬프트 프레임워크는 LLM에 따라 크게 다를 수 있다. 다양한 LLM에 걸쳐 적용 가능한 통합되고 탄력적인 신속한 프레임워크의 개발은 여전히 중요하고 해결되지 않은 과제로 남아 있다. 앞서 언급한 문제에 대한 두 가지 잠재적인 해결책은 (1) 시행착오를 통해 필수 프롬프트 요소를 수동으로 만들거나 (2) GPT를 사용하여 프롬프트를 자동으로 생성하는 것이다.

### Hallucination

환각은 모델의 높은 신뢰도로 잘못된 정보를 생성하는 경향이 특징인 LLM에 근본적인 도전을 제기한다. 이 과제는 LLM에만 국한되지 않고 자율 에이전트 영역에서도 중요한 관심사이다. 예를 들어 [186]에서는 코드 생성 작업 중 단순한 지시에 직면했을 때 에이전트가 환각 행동을 보일 수 있음을 관찰하였다. 환각은 올바르지 않거나 오해의 소지가 있는 코드, 보안 위험 및 윤리적 문제와 같은 심각한 결과를 초래할 수 있다[186]. 이 문제를 완화하기 위해, 인간-에이전트 상호작용의 반복 프로세스에 직접 인간 보정 피드백을 통합하는 것은 실행 가능한 접근법을 제시한다[23]. 환각 문제에 대한 더 많은 논의는 [176]에서 확인할 수 있다.

### Knowledge Boundary

LLM 기반 자율 에이전트의 중추적인 응용은 다양한 실제 인간 행동을 시뮬레이션하는 데 있다[20]. 인간 시뮬레이션에 대한 연구는 오랜 역사를 가지고 있으며 최근 관심이 급증한 것은 LLM이 인간 행동을 시뮬레이션하는 데 상당한 능력을 보여준 놀라운 발전에 기인할 수 있다. 그러나, LLM들의 전력이 항상 유리한 것은 아닐 수 있다는 것을 인식하는 것이 중요하다. 특히 이상적인 시뮬레이션은 인간의 지식을 정확하게 복제해야 한다. 이러한 맥락에서 LLM은 평균 개인이 알 수 있는 것을 훨씬 초과하는 방대한 웹 지식 코퍼스에서 훈련되어 압도적인 능력을 나타낼 수 있다. LLM의 엄청난 능력은 시뮬레이션의 효과에 상당한 영향을 미칠 수 있다. 예를 들어, 다양한 영화에 대한 사용자 선택 행동을 시뮬레이션하려고 할 때, LLMs가 이러한 영화에 대한 사전 지식이 없는 입장을 취하도록 하는 것이 중요하다. 그러나 LLM은 이미 이러한 영화에 대한 정보를 획득했을 가능성이 있다. 적절한 전략을 구현하지 않으면 LLM은 실제 사용자가 이러한 영화의 콘텐츠에 미리 액세스할 수 없음에도 불구하고 광범위한 지식을 기반으로 결정을 내릴 수 있다. 위의 예제를 바탕으로 신뢰할 수 있는 에이전트 시뮬레이션 환경을 구축하기 위해서는 LLM에 대한 사용자 미지의 지식의 활용을 어떻게 제약할 것인가가 중요한 문제라는 결론을 내릴 수 있다.

### Efficiency

자기 회귀 아키텍처로 인해 LLM은 일반적으로 추론 속도가 느리다. 그러나 에이전트는 메모리로부터 정보를 추출하는 것과 같이 각 액션에 대해 LLM을 여러 번 쿼리하고, 액션을 취하기 전에 계획을 세우는 등의 작업을 수행해야 할 수 있다. 결과적으로, 에이전트 액션의 효율성은 LLM 추론의 속도에 크게 영향을 받는다.

## 7 Conclusion

본 조사에서는 LLM 기반 자율 에이전트 분야의 기존 연구를 체계적으로 정리한다. 우리는 에이전트의 구성, 적용 및 평가를 포함한 세 가지 측면에서 이러한 연구를 제시하고 검토한다. 이러한 각 측면에 대해 주요 기술과 개발 이력을 요약하여 기존 연구 간의 연결을 도출하기 위한 세부 분류법을 제공한다. 우리는 이전 작업을 검토하는 것 외에도 이 분야의 몇 가지 과제를 제안하며, 이는 잠재적인 미래 방향을 안내할 것으로 예상된다.

## Acknowledgement

이 작업은 중국 국립 자연 과학 재단(제 62102420호)에 의해 부분적으로 지원된다. 베이징의 뛰어난 젊은 과학자 프로그램 NO. BJJWZYJH012019100020098, 지능형 소셜 거버넌스 플랫폼, "이중 퍼스트 클래스" 이니셔티브를 위한 주요 혁신 및 계획 학제 플랫폼, 중국 런민대학교, 퍼블릭 컴퓨팅 클라우드, 중국 런민대학교, 중국 런민대학교의 세계적 대학(학제) 구축 기금, 지능형 소셜 거버넌스 플랫폼.

## References

* [1] Mnih V, Kavukcuoglu K, Silver D, Rusu A A, Veness J, Bellemare M G, Graves A, Riedmiller M, Fidjeland A K, Ostrovski G, others. Human-level control through deep reinforcement learning. nature, 2015, 518(7540): 529-533
* [2] Lillicrap T P, Hunt J J, Pritzel A, Heess N, Erez T, Tassa Y, Silver D, Wierstra D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015
* [3] Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017
* [4] Haarnoja T, Zhou A, Abbeel P, Levine S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In: International conference on machine learning. 2018, 1861-1870
* [5] Brown T, Mann B, Ryder N, Subbiah M, Kaplan J D, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, others. Language models are few-shot learners. Advances in neural information processing systems, 2020, 33: 1877-1901
* [6] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, others. Language models are unsupervised multitask learners. OpenAI blog, 2019, 1(8): 9
* [7] Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman F L, Almeida D, Altenschmidt J, Altman S, Anadkat S, others. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023
* [8] Anthropic. Model card and evaluations for claude models. [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com), 2023
* [9] Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M A, Lacroix T, Roziere B, Goyal N, Hambro E, Azhar F, others. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023
* [10] Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, Bashlykov N, Batra S, Bhargava P, Bhosale S, others. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023
* [11] Chen X, Li S, Li H, Jiang S, Qi Y, Song L. Generative adversarial user model for reinforcement learning -based recommendation system. In: International Conference on Machine Learning. 2019, 1052-1061
* [12] Shinn N, Cassano F, Gopinath A, Narasimhan K, Yao S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 2024, 36
* [13] Shen Y, Song K, Tan X, Li D, Lu W, Zhuang Y. Huggingopt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 2024, 36
* [14] Qin Y, Liang S, Ye Y, Zhu K, Yan L, Lu Y, Lin Y, Cong X, Tang X, Qian B, others. Toollm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023
* [15] Schick T, Dwivedi-Yu J, Dessi R, Raileanu R, Lomeli M, Hambro E, Zettlemoyer L, Cancedda N, Scialom T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 2024, 36
* [16] Zhu X, Chen Y, Tian H, Tao C, Su W, Yang C, Huang G, Li B, Lu L, Wang X, others. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023
* [17] Sclar M, Kumar S, West P, Suhr A, Choi Y, Tsvetkov Y. Minding language models'(lack of) theory of mind: A plug-and-play multi-character belief tracker. arXiv preprint arXiv:2306.00924, 2023
* [18] Qian C, Cong X, Yang C, Chen W, Su Y, Xu J, Liu Z, Sun M. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023
* [19] al. e C. Agentverse. [https://github.com/OpenBMB/AgentVerse](https://github.com/OpenBMB/AgentVerse), 2023
* [20] Park J S, O'Brien J, Cai C J, Morris M R, Liang P, Bernstein M S. Generative agents: Interactive simulacra of human behavior. In: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023, 1-22
* [21] Wang L, Zhang J, Chen X, Lin Y, Song R, Zhao W X, Wen J R. Recagent: A novel simulation paradigm for recommender systems. arXiv preprint arXiv:2306.02552, 2023
* [22] Zhang H, Du W, Shan J, Zhou Q, Du Y, Tenenbaum J B, Shu T, Gan C. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023
* [23] Hong S, Zheng X, Chen J, Cheng Y, Wang J, Zhang C, Wang Z, Yau S K S, Lin Z, Zhou L, others. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023
* [24] Dong Y, Jiang X, Jin Z, Li G. Self-collaboration code generation via chatgpt. arXiv preprint arXiv:2304.07590, 2023
* [25] Safdari M, Serapio-Garcia G, Crepy C, Fitz S, Romero P, Sun L, Abdulhai M, Faust A, Mataric M. Personality traits in large language models. arXiv preprint arXiv:2307.00184, 2023
* [26] Johnson J A. Measuring thirty facets of the five factor model with a 120-item public domain inventory: Development of the ipip-neo-120. Journal of research in personality, 2014, 51: 78-89
* [27] John O P, Donahue E M, Kentle R L. Big five inventory. Journal of Personality and Social Psychology, 1991
* [28] Deshpande A, Murahari V, Rajpurohit T, Kalyan A, Narasimhan K. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335, 2023
* [29] Argyle L P, Busby E C, Fulda N, Gubler J R, Rytting C, Wingate D. Out of one, many: Using language models to simulate human samples. Political Analysis, 2023, 31(3): 337-351
* [30] Fischer K A. Reflective linguistic programming (rlp): A stepping stone in socially-aware agi (socialagi). arXiv preprint arXiv:2305.12647, 2023
* [31] Rana K, Haviland J, Garg S, Abou-Chakra J, Reid I, Suenderhauf N. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. In: 7th Annual Conference on Robot Learning. 2023
* [32] Zhu A, Martin L, Head A, Callison-Burch C. Calypso: Llms as dungeon master's assistants. In: Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. 2023, 380-390
* [33] Wang Z, Cai S, Chen G, Liu A, Ma X, Liang Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023
* [34] Lin J, Zhao H, Zhang A, Wu Y, Ping H, Chen Q. Agentsims: An open-source sandbox for large language model evaluation. arXiv preprint arXiv:2308.04026, 2023
* [35] Liang X, Wang B, Huang H, Wu S, Wu P, Lu L, Ma Z, Li Z. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343, 2023
* [36] Ng Y, Miyashita D, Hoshi Y, Morioka Y, Torii O, Kodama T, Deguchi J. Simplyretrieve: A private and lightweight retrieval-centric generative ai tool. arXiv preprint arXiv:2308.03983, 2023
* [37] Huang Z, Gutierrez S, Kamana H, MacNeil S. Memory sandbox: Transparent and interactive memory management for conversational agents. In: Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023, 1-3
* [38] Wang G, Xie Y, Jiang Y, Mandlekar A, Xiao C, Zhu Y, Fan L, Anandkumar A. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023
* [39] Zhong W, Guo L, Gao Q, Wang Y. Memorybank: Enhancing large language models with long-term memory. arXiv preprint arXiv:2305.10250, 2023
* [40] Hu C, Fu J, Du C, Luo S, Zhao J, Zhao H. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023
* [41] Zhou X, Li G, Liu Z. Llm as dba. arXiv preprint arXiv:2308.05481, 2023
* [42] Modarressi A, Imani A, Fayyaz M, Schutze H. Retllm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023
* [43] Schuurmans D. Memory augmented large language models are computationally universal. arXiv preprint arXiv:2301.04589, 2023
* [44] Zhao A, Huang D, Xu Q, Lin M, Liu Y J, Huang G. Expel: Llm agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023
* [45] Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le Q V, Zhou D, others. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837
* [46] Kojima T, Gu S S, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 2022, 35: 22199-22213
* [47] Raman S S, Cohen V, Rosen E, Idrees I, Paulius D, Tellex S. Planning with large language models via corrective re-prompting. In: NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022
* [48] Xu B, Peng Z, Lei B, Mukherjee S, Liu Y, Xu D. Rewoo: Decoupling reasoning from observations for efficient augmented language models. arXiv preprint arXiv:2305.18323, 2023
* [49] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, Chowdhery A, Zhou D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022
* [50] Yao S, Yu D, Zhao J, Shafran I, Griffiths T, Cao Y, Narasimhan K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 2024, 36
* [51] Wang Y, Jiang Z, Chen Z, Yang F, Zhou Y, Cho E, Fan X, Huang X, Lu Y, Yang Y. Recmind: Large language model powered agent for recommendation. arXiv preprint arXiv:2308.14296, 2023
* [52] Besta M, Blach N, Kubicek A, Gerstenberger R, Gianinazzi L, Gajda J, Lehmann T, Podstawski M, Niewiadomski H, Nyczyk P, others. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023
* [53] Sel B, Al-Tawaha A, Khattar V, Wang L, Jia R, Jin M. Algorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv preprint arXiv:2308.10379, 2023
* [54] Huang W, Abbeel P, Pathak D, Mordatch I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In: International Conference on Machine Learning. 2022, 9118-9147
* [55] Gramopadhye M, Szafir D. Generating executable action plans with environmentally-aware language models. In: 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2023, 3568-3575
* [56] Hao S, Gu Y, Ma H, Hong J J, Wang Z, Wang D Z, Hu Z. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023
* [57] Liu B, Jiang Y, Zhang X, Liu Q, Zhang S, Biswas J, Stone P. LLM+P: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023
* [58] Dagan G, Keller F, Lascarides A. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391, 2023
* [59] Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y. React: Synergizing reasoning and acting in language models. In: The Twelfth International Conference on Learning Representations. 2023
* [60] Song C H, Wu J, Washington C, Sadler B M, Chao W L, Su Y. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023, 2998-3009
* [61] Huang W, Xia F, Xiao T, Chan H, Liang J, Florence P, Zeng A, Tompson J, Mordatch I, Chebotar Y, others. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022
* [62] Madaan A, Tandon N, Gupta P, Hallinan S, Gao L, Wiegreffe S, Alon U, Dziri N, Prabhumoye S, Yang Y, others. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 2024, 36
* [63] Miao N, Teh Y W, Rainforth T. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. In: The Twelfth International Conference on LearningRepresentations. 2023
* [64] Chen P L, Chang C S. Interact: Exploring the potentials of chatgpt as a cooperative agent. arXiv preprint arXiv:2308.01552, 2023
* [65] Chen Z, Zhou K, Zhang B, Gong Z, Zhao W X, Wen J R. Chatcot: Tool-augmented chain-of-thought reasoning on\(\backslash\)chat-based large language models. arXiv preprint arXiv:2305.14323, 2023
* [66] Nakano R, Hilton J, Balaji S, Wu J, Ouyang L, Kim C, Hesse C, Jain S, Kosaraju V, Saunders W, others. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021
* [67] Ruan J, Chen Y, Zhang B, Xu Z, Bao T, Du G, Shi S, Mao H, Zeng X, Zhao R. TPTU: Task planning and tool usage of large language model-based AI agents. arXiv preprint arXiv:2308.03427, 2023
* [68] Patil S G, Zhang T, Wang X, Gonzalez J E. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023
* [69] Li M, Song F, Yu B, Yu H, Li Z, Huang F, Li Y. Apibank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023
* [70] Song Y, Xiong W, Zhu D, Li C, Wang K, Tian Y, Li S. Restgpt: Connecting large language models with real-world applications via restful apis. arXiv preprint arXiv:2306.06624, 2023
* [71] Liang Y, Wu C, Song T, Wu W, Xia Y, Liu Y, Ou Y, Lu S, Ji L, Mao S, others. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Intelligent Computing, 2024, 3: 0063
* [72] Karpas E, Abend O, Belinkov Y, Lenz B, Lieber O, Ratner N, Shoham Y, Bata H, Levine Y, Leyton-Brown K, others. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445, 2022
* [73] Ge Y, Hua W, Mei K, Tan J, Xu S, Li Z, Zhang Y, others. Openagi: When llm meets domain experts. Advances in Neural Information Processing Systems, 2024, 36
* [74] Suris D, Menon S, Vondrick C. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023
* [75] Bran A M, Cox S, White A D, Schwaller P. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023
* [76] Yang Z, Li L, Wang J, Lin K, Azarnasab E, Ahmed F, Liu Z, Liu C, Zeng M, Wang L. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023
* [77] Gao C, Lan X, Lu Z, Mao J, Piao J, Wang H, Jin D, Li Y. S3: Social-network simulation system with large language model-empowered agents. arXiv preprint arXiv:2307.14984, 2023
* [78] Ahn M, Brohan A, Brown N, Chebotar Y, Cortes O, David B, Finn C, Fu C, Gopalakrishnan K, Hausman K, others. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022
* [79] Park J S, Popowski L, Cai C, Morris M R, Liang P, Bernstein M S. Social simulacra: Creating populated prototypes for social computing systems. In: Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. 2022, 1-18
* [80] Li G, Hammoud H A A K, Itani H, Khizbullin D, Ghanem B. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023
* [81] al. e T. Auto-GPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT), 2023
* [82] Liu R, Yang R, Jia C, Zhang G, Zhou D, Dai A M, Yang D, Vosoughi S. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023
* [83] Chen L, Wang L, Dong H, Du Y, Yan J, Yang F, Li S, Zhao P, Qin S, Rajmohan S, others. Introspective tips: Large language model for in-context decision making. arXiv preprint arXiv:2305.11598, 2023
* [84] Liu H, Sferrazza C, Abbeel P. Chain of hindsight aligns language models with feedback. In: The Twelfth International Conference on Learning Representations. 2023
* [85] Yao S, Chen H, Yang J, Narasimhan K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 2022, 35: 20744-20757
* [86] Dan Y, Lei Z, Gu Y, Li Y, Yin J, Lin J, Ye L, Tie Z, Zhou Y, Wang Y, others. Educhat: A large-scale language model-based chatbot system for intelligent education. arXiv preprint arXiv:2308.02773, 2023
* [87] Lin B Y, Fu Y, Yang K, Brahman F, Huang S, Bhagavatula C, Ammanabrolu P, Choi Y, Ren X. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Advances in Neural Information Processing Systems, 2024, 36
* [88] Evans J S B, Stanovich K E. Dual-process theories of higher cognition: Advancing the debate. Perspectives on psychological science, 2013, 8(3): 223-241
* [89] Deng X, Gu Y, Zheng B, Chen S, Stevens S, Wang B, Sun H, Su Y. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 2024, 36* [90] Sun R, Arik S O, Nakhost H, Dai H, Sinha R, Yin P, Pfister T. Sql-palm: Improved large language modeladaptation for text-to-sql. arXiv preprint arXiv:2306.00739, 2023
* [91] Yao W, Heinecke S, Niebles J C, Liu Z, Feng Y, Xue L, Murthy R, Chen Z, Zhang J, Arpit D, Xu R, Mui P, Wang H, Xiong C, Savarese S. Retroformer: Retrospective large language agents with policy gradient optimization, 2023
* [92] Shu Y, Gu H, Zhang P, Zhang H, Lu T, Li D, Gu N. Rah! recsys-assistant-human: A human-central recommendation framework with large language models. arXiv preprint arXiv:2308.09904, 2023
* [93] Mandi Z, Jain S, Song S. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023
* [94] Zhang C, Liu L, Wang J, Wang C, Sun X, Wang H, Cai M. Prefer: Prompt ensemble learning via feedback-reflect-refine. arXiv preprint arXiv:2308.12033, 2023
* [95] Du Y, Li S, Torralba A, Tenenbaum J B, Mordatch I. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023
* [96] Yang Z, Liu J, Han Y, Chen X, Huang Z, Fu B, Yu G. Apparent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023
* [97] Madaan A, Tandon N, Clark P, Yang Y. Memory-assisted prompt editing to improve GPT-3 after deployment. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2022
* [98] Colas C, Teodorescu L, Oudeyer P Y, Yuan X, Cote M A. Augmenting autotelic agents with large language models. arXiv preprint arXiv:2305.12487, 2023
* [99] Nascimento N, Alencar P, Cowan D. Self-adaptive large language model (llm)-based multiagent systems. In: 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C). 2023, 104-109
* [100] Saha S, Hase P, Bansal M. Can language models teach weaker agents? teacher explanations improve students via theory of mind. arXiv preprint arXiv:2306.09299, 2023
* [101] Zhuge M, Liu H, Faccio F, Ashley D R, Csordas R, Gopalakrishnan A, Hamdi A, Hammoud H A A K, Herrmann V, Irie K, others. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023
* [102] Aher G V, Arriaga R I, Kalai A T. Using large language models to simulate multiple humans and replicate human subject studies. In: International Conference on Machine Learning. 2023, 337-371
* [103] Akata E, Schulz L, Coda-Forno J, Oh S J, Bethge M, Schulz E. Playing repeated games with large language models. arXiv preprint arXiv:2305.16867, 2023
* [104] Ma Z, Mei Y, Su Z. Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. In: AMIA Annual Symposium Proceedings. 2023, 1105
* [105] Ziems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D. Can large language models transform computational social science? arXiv preprint arXiv:2305.03514, 2023
* [106] Horton J J. Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research, 2023
* [107] Li S, Yang J, Zhao K. Are you in a masquerade? exploring the behavior and impact of large language model driven social bots in online social networks. arXiv preprint arXiv:2307.10337, 2023
* [108] Li C, Su X, Fan C, Han H, Xue C, Zheng C. Quantifying the impact of large language models on collective opinion dynamics. arXiv preprint arXiv:2308.03313, 2023
* [109] Kovac G, Portelas R, Dominey P F, Oudeyer P Y. The socialai school: Insights from developmental psychology towards artificial socio-cultural agents. arXiv preprint arXiv:2307.07871, 2023
* [110] Williams R, Hosseinichimeh N, Majumdar A, Ghaffarzadegan N. Epidemic modeling with generative agents. arXiv preprint arXiv:2307.04986, 2023
* [111] Jinxin S, Jiabao Z, Yilei W, Xingjiao W, Jiawen L, Liang H. Cgmi: Configurable general multi-agent interaction framework. arXiv preprint arXiv:2308.12503, 2023
* [112] Cui J, Li Z, Yan Y, Chen B, Yuan L. Chatlaw: Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv:2306.16092, 2023
* [113] Hamilton S. Blind judgement: Agent-based supreme court modelling with gpt. arXiv preprint arXiv:2301.05327, 2023
* [114] Bail C A. Can generative ai improve social science? 2023
* [115] Boiko D A, MacKnight R, Gomes G. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023
* [116] Kang Y, Kim J. Chatmof: An autonomous ai system for predicting and generating metal-organic frameworks. arXiv preprint arXiv:2308.01423, 2023
* [117] Swan M, Kido T, Roland E, Santos R P d. Math agents: Computational infrastructure, mathematical embed ding, and genomics. arXiv preprint arXiv:2307.02502, 2023
* [118] Drori I, Zhang S, Shuttleworth R, Tang L, Lu A, Ke E, Liu K, Chen L, Tran S, Cheng N, others. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 2022, 119(32): e2123433119
* [119] Chen M, Tworek J, Jun H, Yuan Q, Pinto H P d O, Kaplan J, Edwards H, Burda Y, Joseph N, Brockman G, others. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021
* [120] Liffiton M, Sheese B E, Savelka J, Denny P. Codehelp: Using large language models with guard trails for scalable support in programming classes. In: Proceedings of the 23rd Koli Calling International Conference on Computing Education Research. 2023, 1-11
* [121] Matelsky J K, Parodi F, Liu T, Lange R D, Kording K P. A large language model-assisted education tool to provide feedback on open-ended responses. arXiv preprint arXiv:2308.02439, 2023
* [122] Grossmann I, Feinberg M, Parker D C, Christakis N A, Tetlock P E, Cunningham W A. Ai and the transformation of social science research. Science, 2023, 380(6650): 1108-1109
* [123] He Z, Wu H, Zhang X, Yao X, Zheng S, Zheng H, Yu B. Chateda: A large language model powered autonomous agent for eda. In: 2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD). 2023, 1-6
* [124] Huang X, Lian J, Lei Y, Yao J, Lian D, Xie X. Recommender ai agent: Integrating large language models for interactive recommendations. arXiv preprint arXiv:2308.16505, 2023
* [125] Deng G, Liu Y, Mayoral-Vilches V, Liu P, Li Y, Xu Y, Zhang T, Liu Y, Pinzger M, Rass S. Pentestgpt: An llm-empowered automatic penetration testing tool. arXiv preprint arXiv:2308.06782, 2023
* [126] al. e S. Smolmodels. [https://github.com/smol-ai/developer](https://github.com/smol-ai/developer), 2023
* [127] al. e M U. DemoGPT. [https://github.com/melih-unsal/DemoGPT](https://github.com/melih-unsal/DemoGPT), 2023
* [128] al. e A O. GPT engineer. [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer), 2023
* [129] Xia Y, Shenoy M, Jazdi N, Weyrich M. Towards autonomous system: flexible modular production system enhanced with large language model agents. arXiv preprint arXiv:2304.14721, 2023
* [130] Ogundare O, Madasu S, Wiggins N. Industrial engineering with large language models: A case study of chatgpt's performance on oil & gas problems. arXiv preprint arXiv:2304.14354, 2023
* [131] Zhang C, Yang K, Hu S, Wang Z, Li G, Sun Y, Zhang C, Zhang Z, Liu A, Zhu S C, others. Proagent: Building proactive cooperative ai with large language models. arXiv preprint arXiv:2308.11339, 2023
* [132] Hu B, Zhao C, others. Enabling intelligent interactions between an agent and an llm: A reinforcement learning approach. arXiv:2306.03604, 2023
* [133] Wu Y, Min S Y, Bisk Y, Salakhutdinov R, Azaria A, Li Y, Mitchell T, Prabhumoye S. Plan, eliminate, and track-language models are good teachers for embodied agents. arXiv preprint arXiv:2305.02412, 2023
* [134] Zhang D, Chen L, Zhang S, Xu H, Zhao Z, Yu K. Large language models are semi-parametric reinforcement learning agents. Advances in Neural Information Processing Systems, 2024, 36
* [135] Di Palo N, Byravan A, Hasenclever L, Wulfmeier M, Heess N, Riedmiller M. Towards a unified agent with foundation models. In: Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023
* [136] Xiang J, Tao T, Gu Y, Shu T, Wang Z, Yang Z, Hu Z. Language models meet world models: Embodied experiences enhance language models. Advances in neural information processing systems, 2024, 36
* [137] Wu J, Antonova R, Kan A, Lepert M, Zeng A, Song S, Bohg J, Rusinkiewicz S, Funkhouser T. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658, 2023
* [138] Mehta N, Teruel M, Sanz P F, Deng X, Awadallah A H, Kiseleva J. Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback. arXiv preprint arXiv:2304.10750, 2023
* [139] Li H, Hao Y, Zhai Y, Qian Z. The hitchhiker's guide to program analysis: A journey with large language models. arXiv preprint arXiv:2308.00245, 2023
* [140] Dasgupta I, Kaeser-Chen C, Marino K, Ahuja A, Babayan S, Hill F, Fergus R. Collaborating with language models for embodied reasoning. arXiv preprint arXiv:2302.00763, 2023
* [141] Zhou W, Peng X, Riedl M. Dialogue shaping: Empowering agents through npc interaction. arXiv preprint arXiv:2307.15833, 2023
* [142] Nottingham K, Ammanabrolu P, Suhr A, Choi Y, Hajishirzi H, Singh S, Fox R. Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. In: Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023
* [143] Wu Z, Wang Z, Xu X, Lu J, Yan H. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848, 2023* [144] al. e R. AgentGPT. [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT), 2023
* [145] al. e E. Ai-legion. [https://github.com/eumemic/ai-legion](https://github.com/eumemic/ai-legion), 2023
* [146] al. e J X. Agixt. [https://github.com/Josh-XT/AGiXT](https://github.com/Josh-XT/AGiXT), 2023
* [147] al. e C. Xlang. [https://github.com/xlang-ai/xlang](https://github.com/xlang-ai/xlang), 2023
* [148] al. e N. Babyagi. [https://github.com/yoheinakajima](https://github.com/yoheinakajima), 2023
* [149] Chase H. langchain. [https://docs.langchain.com/docs/](https://docs.langchain.com/docs/), 2023
* [150] al. e A M. WorkGPT. [https://github.com/team-openpm/workgpt](https://github.com/team-openpm/workgpt), 2023
* [151] al. e F R. LoopGPT. [https://github.com/fairzrahman4u/loopgpt](https://github.com/fairzrahman4u/loopgpt), 2023
* [152] al. e A E. GPT-researcher. [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher), 2023
* [153] Qin Y, Hu S, Lin Y, Chen W, Ding N, Cui G, Zeng Z, Huang Y, Xiao C, Han C, others. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023
* [154] Face H. transformers-agent. [https://huggingface.co/docs/transformers/transformers_agents](https://huggingface.co/docs/transformers/transformers_agents), 2023
* [155] al. e E. Miniagi. [https://github.com/muellerberndt/mini-agi](https://github.com/muellerberndt/mini-agi), 2023
* [156] al. e T. Superagi. [https://github.com/TransformerOptimus/SuperAGI](https://github.com/TransformerOptimus/SuperAGI), 2023
* [157] Wu Q, Bansal G, Zhang J, Wu Y, Zhang S, Zhu E, Li B, Jiang L, Zhang X, Wang C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023
* [158] Lee M, Srivastava M, Hardy A, Thickstun J, Durmus E, Paranjape A, Gerard-Ursin I, Li X L, Ladhak F, Rong F, others. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746, 2022
* [159] Krishna R, Lee D, Fei-Fei L, Bernstein M S. Socially situated artificial intelligence enables learning from human interaction. Proceedings of the National Academy of Sciences, 2022, 119(39): e2115730119
* [160] Huang J t, Lam M H, Li E J, Ren S, Wang W, Jiao W, Tu Z, Lyu M R. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. arXiv preprint arXiv:2308.03656, 2023
* [161] Chan C M, Chen W, Su Y, Yu J, Xue W, Zhang S, Fu J, Liu Z. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023
* [162] Kang S, Yoon J, Yoo S. Large language models are few-shot testers: Exploring llm-based general bug reproduction. In: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2023, 2312-2323
* [163] Jalil S, Rafi S, LaToza T D, Moran K, Lam W. Chatgpt and software testing education: Promises & perils. In: 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW). 2023, 4130-4137
* [164] Chen A, Phang J, Parrish A, Padmakumar V, Zhao C, Bowman S R, Cho K. Two failures of self-consistency in the multi-step reasoning of llms. arXiv preprint arXiv:2305.14279, 2023
* [165] Choi M, Pei J, Kumar S, Shu C, Jurgens D. Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark. arXiv preprint arXiv:2305.14938, 2023
* [166] Zhang D, Chen L, Zhao Z, Cao R, Yu K. Mobile-env: An evaluation platform and benchmark for interactive agents in llm era. arXiv preprint arXiv:2305.08144, 2023
* [167] Chalamalasetti K, Gotze J, Hakimov S, Madureira B, Sadler P, Schlangen D. clembench: Using game play to evaluate chat-optimized language models as conversational agents. arXiv preprint arXiv:2305.13455, 2023
* [168] Lin J, Tomlin N, Andreas J, Eisner J. Decision-oriented dialogue for human-ai collaboration. arXiv preprint arXiv:2305.20076, 2023
* [169] Feldt R, Kang S, Yoon J, Yoo S. Towards autonomous testing agents via conversational large language models. arXiv preprint arXiv:2306.05152, 2023
* [170] Liang Y, Zhu L, Yang Y. Tachikuma: Understanding complex interactions with multi-character and novel objects by large language models. arXiv preprint arXiv:2307.12573, 2023
* [171] Zhou S, Xu F F, Zhu H, Zhou X, Lo R, Sridhar A, Cheng X, Bisk Y, Fried D, Alon U, others. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023
* [172] Liu X, Yu H, Zhang H, Xu Y, Lei X, Lai H, Gu Y, Ding H, Men K, Yang K, others. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023
* [173] Liu Z, Yao W, Zhang J, Xue L, Heinecke S, Murthy R, Feng Y, Chen Z, Niebles J C, Arpit D, others. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960, 2023
* [174] Xu B, Liu X, Shen H, Han Z, Li Y, Yue M, Peng Z, Liu Y, Yao Z, Xu D. Gentopia. ai: A collaborative platform for tool-augmented llms. In: Proceedings of the 2023 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations. 2023, 237-245
* [175] Banerjee D, Singh P, Avadhanam A, Srivastava S. Benchmarking llm powered chatbots: methods and metrics. arXiv preprint arXiv:2308.04624, 2023
* [176] Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z, others. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023
* [177] Yang J, Jin H, Tang R, Han X, Feng Q, Jiang H, Zhong S, Yin B, Hu X. Harnessing the power of llms in practice: A survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery from Data, 2023
* [178] Wang Y, Zhong W, Li L, Mi F, Zeng X, Huang W, Shang L, Jiang X, Liu Q. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966, 2023
* [179] Huang J, Chang K C C. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022
* [180] Mialon G, Dessl R, Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R, Roziere B, Schick T, Dwivedi-Yu J, Celikyilmaz A, others. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023
* [181] Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, Chen H, Yi X, Wang C, Wang Y, others. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2023
* [182] Chang T A, Bergen B K. Language model behavior: A comprehensive survey. Computational Linguistics, 2024, 1-58
* [183] Li C, Wang J, Zhu K, Zhang Y, Hou W, Lian J, Xie X. Emotionpromt: Leveraging psychology for large language models enhancement via emotional stimulus. arXiv e-prints, 2023, arXiv-2307
* [184] Zhuo T Y, Li Z, Huang Y, Shiri F, Wang W, Haffari G, Li Y F. On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. arXiv preprint arXiv:2301.12868, 2023
* [185] Gekhman Z, Oved N, Keller O, Szpektor I, Reichart R. On the robustness of dialogue history representation in conversational question answering: a comprehensive study and a new prompt-based method. Transactions of the Association for Computational Linguistics, 2023, 11: 351-366
* [186] Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y J, Madotto A, Fung P. Survey of hallucination in natural language generation. ACM Computing Surveys, 2023, 55(12): 1-38
* [187] Lei Wang is a Ph.D. candidate at Renmin University of China, Beijing. His research focuses on recommender systems and agent-based large language models.

천마는 현재 중국 베이징 런민대에서 석사 학위를 받고 있다. 그의 연구 관심사로는 대규모 언어 모델을 기반으로 한 추천 시스템, 에이전트 등이 있다.

쉐양펑은 현재 중국 베이징 런민대에서 박사 학위를 받고 있다. 그의 연구 관심사로는 대규모 언어 모델을 기반으로 한 추천 시스템, 에이전트 등이 있다.

장제위는 현재 중국 베이징 런민대에서 석사학위를 취득하고 있다. 그의 연구 관심사는 대규모 언어 모델을 기반으로 한 추천 시스템, 인과 추론, 에이전트 등이다.

하오양은 현재 중국 베이징 런민대에서 박사 학위를 받고 있다. 그의 연구 관심사로는 추천 시스템, 인과 추론 등이 있다.

징센 장은 현재 중국 베이징 런민대에서 박사 학위를 받고 있다. 그의 연구 관심사는 추천 시스템을 포함한다.

쯔위안 첸은 중국 런민대학교 가올링 인공지능 학교에서 박사과정을 밟고 있다. 그의 연구는 주로 대형 언어 모델을 기반으로 한 언어 모델 추론과 에이전트에 초점을 맞추고 있다.

지아카이 당은 현재 중국 베이징 런민대에서 석사 학위를 받고 있다. 그의 연구 관심사는 추천 시스템을 포함한다.

쉬천은 중국 칭화대에서 박사 학위를 취득했다. 중국 런민대학교에 입학하기 전, 그는 영국 런던 대학교의 포닥 연구원이었다. 2017년 3월부터 9월까지 미국 조지아공대에서 방문 학자로 유학 중이었다. 그의 연구는 주로 추천 시스템, 강화 학습 및 인과 추론에 중점을 둔다.

옌카이린은 2014년과 2019년 칭화대에서 B.E.와 박사 학위를 받았고, 이후 텐센트 위챗에서 선임연구원으로 일했고, 2022년 중국 런민대에 재임 트랙 조교수로 입대했다. 그의 주요 연구 관심사는 사전 훈련된 모델과 자연어 처리이다.

웨인 신자오(Wayne Xin Zhao)는 2014년 북경 대학에서 컴퓨터 과학 박사 학위를 받았다. 그의 연구 관심사는 데이터 마이닝, 자연어 처리 및 정보 검색이다. 주요 목표는 실제 애플리케이션의 서비스 개선을 위해 사용자 생성 데이터를 구성, 분석 및 마이닝하는 방법을 연구하는 것이다.

제웨이웨이는 홍콩 과학 기술 대학교로부터 컴퓨터 공학 박사 학위를 받았다. 2012년부터 2014년까지 아르후스 대학에서 박사 후 연구를 했고 2014년에는 중국 런민 대학에 입사했다.

지롱웬은 전임 교수, 가올링 인공지능대학원 총장, 중국 런민대학교 정보대학원 총장이다. 그는 수년 동안 빅 데이터와 AI 분야에서 일했으며 권위 있는 국제 회의와 저널에 광범위하게 출판했다.

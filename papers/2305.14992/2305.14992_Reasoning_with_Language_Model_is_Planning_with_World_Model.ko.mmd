# Reasoning with Language Model is Planning with World Model

 시보 하오\({}^{\star\clubsuit}\) 이구\({}^{\star\clubsuit}\) 하오디 마\({}^{\diamond}\) 조슈아 자화 홍\({}^{\clubsuit}\)

Zhen Wang\({}^{\clubsuit}\) Daisy Zhe Wang\({}^{\diamond}\) Zhiting Hu\({}^{\clubsuit}\)

\({}^{\clubsuit}\)UC San Diego, \({}^{\diamond}\)University of Florida

({}^{\spadesuit}\) 모하메드 빈 자이드 인공지능대학

{s5hao, yig025, jjhong, zhw085, zhh019}@ucsd.edu

{ma.haodi, daisyw}@ufl.edu

equal contribution

###### Abstract

대형 언어 모델(LLM)은 특히 연쇄 사고(CoT) 프롬프트와 함께 놀라운 추론 능력을 보여주었다. 그러나 LLM은 때때로 환경에서 주어진 목표를 달성하기 위한 실행 계획을 생성하거나 복잡한 수학이나 논리적 추론을 수행하는 것과 같이 인간이 쉽게 문제를 해결하는 데 어려움을 겪는다. 결핍은 LLM이 세계 _상태_(예: 환경 상태, 중간 변수 값)를 예측하고 행동의 장기적인 결과를 시뮬레이션할 내부 _세계 모델_이 부족하다는 주요 사실에서 비롯된다. 이는 LLM이 대체 추론 경로를 탐색하고 미래 상태와 보상을 예상하며 기존 추론 단계를 반복적으로 정제하는 것을 포함하는 인간의 뇌와 유사한 의도적인 계획을 수행하는 것을 방지한다. 제한 사항을 극복하기 위해 **계획(RAP)을 통한 추론** 이라는 새로운 LLM 추론 프레임워크를 제안합니다. RAP는 LLM을 세계 모델과 추론 에이전트로 재배치하고, 방대한 추론 공간에서 전략적 탐색을 위해 몬테카를로 트리 탐색에 기반한 원칙적인 계획 알고리즘을 통합한다. 추론 동안, LLM(에이전트로서)은 LLM(세계 모델로서) 및 보상의 안내 하에 추론 트리를 점진적으로 구축하고, 탐색 _vs._ 간의 적절한 균형을 갖는 고-보상 추론 경로를 효율적으로 획득한다. 착취 우리는 RAP를 계획 생성, 수학 추론, 논리 추론을 포함한 다양한 도전적 추론 문제에 적용하고 강력한 기준선보다 우월함을 보여준다. LLaMA-33B를 사용한 RAP는 GPT-4를 사용한 CoT를 능가하여 계획 생성 설정에서 33%의 상대적 개선을 달성했다.1

각주 1: 코드는 [https://github.com/Ber666/llm-reasoners](https://github.com/Ber666/llm-reasoners)에서 사용할 수 있습니다.

## 1 Introduction

대형 언어 모델(LLMs)은 Brown 등(2020); Chowdhery 등(2022); OpenAI(2023) 등의 광범위한 작업에서 창발적 추론 능력을 나타냈다. 최근의 접근법들은 LLM들이 중간 추론 단계들, 예를 들어, Chain-of-Thought, CoT Wei 등(2022)을 생성하거나 일련의 하위 질문들, 예를 들어, 최하위 프롬프팅 Zhou 등(2022)에 응답하도록 유도함으로써 그들의 능력을 더욱 향상시킨다. 그러나 LLM은 여전히 인간이 쉽게 찾는 작업에 어려움을 겪는다. 예를 들어, 블록들을 목표 상태로 이동시키기 위한 액션 플랜들을 생성함에 있어서, GPT-3 Brown et al.(2020)은 인간 Valmcekam et al.(2022)의 경우 78%에 비해 단지 1%의 성공률을 달성한다; 이러한 모델들은 또한 수학, 논리, 또는 상식 추론 Huang and Chang(2022); Mialon et al.(2023)의 다수의 단계들을 필요로 하는 복잡한 작업들과 투쟁한다.

인간은 환경을 정신적으로 나타내는 내부 **세계 모델** 을 소유합니다. Johnson-Laird (1983, 2010); Gentner 및 Stevens (2014)는 인간이 운동 제어, 이미지, 추론 및 의사 결정 Tolman (1948); Briscoe (2011); Schulkin (2012); LeCun (2022). 예를 들어, 목표를 향한 실행 계획을 만들기 위해, 세계 모델을 사용한 계획은 다양한 대안적인 행동 과정을 탐색하고, 가능한 미래 시나리오를 롤링하여 가능한 결과를 평가하고, Huys 등(2012); Gasparski and Orel(2014); Ho 등(2021)을 기반으로 계획을 반복적으로 정제하는 것을 포함한다. 이는 자기회귀 방식으로 본능적으로 추론 궤적을 생성하는 현재의 LLM 추론과 극명한 대조를 이룬다. 특히 **(1)** 인간 계획의 기초가 되는 세계의 _상태_ (예: 블록의 구성, 중간 변수의 값)를 시뮬레이션할 내부 세계 모델의 부족, **(2)** 추론을 평가 하 고 원하는 상태로 안내 하는 _보상_ 메커니즘의 부재 및 두 제한으로 인해 **(3)** _탐색 대 탐색의 균형을 맞출 수 없는 경우를 포함 하 여 현재 추론의 몇 가지 주요 제한 사항을 식별 합니다. 광활한 추론 공간을 효율적으로 탐색하기 위한 활용.

이러한 한계를 해결하기 위해 이 논문은 LLM이 인간의 의식적인 계획에 가까운 방식으로 추론할 수 있도록 하는 새로운 프레임워크인 **계획(RAP)을 제안한다. RAP는 LLM을 세계 모델과 원칙적인 계획(특히 _몬테 카를로 트리 탐색, MCTS_)으로 확장하여 효율적인 탐색 후 높은 보상 추론 흔적을 생성한다(그림 1). 특히, 우리는 적절한 프롬프트로 LLM 자체를 용도 변경함으로써 세계 모델을 획득한다. 추론 동안, LLM은 가장 유망한 추론 단계들(_actions_)을 반복적으로 고려하고 세계 모델(동일한, 용도 변경된 LLM)을 사용하여 미래의 결과들을 전망함으로써 추론 트리를 전략적으로 구축한다. 추정된 미래 보상은 현재 추론 단계에 대한 LLM의 믿음을 업데이트하도록 역전파되어 더 나은 대안을 탐색하여 추론을 정제하도록 안내한다. 우리의 MCTS 기반 계획은 탐색(비방문 추론 흔적)과 활용(지금까지 확인된 최상의 추론 단계) 사이의 적절한 균형을 효과적으로 유지한다.

우리는 RAP가 다양한 도전적 문제에 적용할 수 있는 일반적인 프레임워크임을 보여주고 최근의 인기 있는 LLM 추론 방법에 비해 상당한 개선을 달성한다. 플랜 생성을 위해, 특히 Blocksworld Valmeekam 등(2023)의 2/4/6-단계 문제에서, RAP는 64%의 평균 성공률을 달성하는 반면 CoT는 거의 완전히 실패한다. 또한, RAP가 있는 LLaMA-33B는 CoT가 있는 GPT-4를 33% 상대적 개선만큼 능가한다. GSM8K Cobbe 등(2021) 및 PrOntoQA Saparov 및 He(2022)에 의해 예시된 논리적 추론과 같은 수학적 추론의 도메인에서, RAP는 또한 CoT, 최소-최대 프롬프팅, 및 이들의 자기-일관성 변형을 포함하는 강력한 베이스라인에 비해 일관되게 개선된다.

## 2 관련 작업

**LLM을 사용한 추론** LLM 추론은 일반적으로 복잡한 질문을 순차적 중간 단계(a.k.a. 사슬)로 분해한 후 최종 답변을 생성합니다. CoT( Chain-of-Thought) 프롬프트 및 그 변형 Wei 등(2022); Kojima 등(2022). 기본 CoT는 한 번에 체인을 생성하고 스텝 카운트가 증가함에 따라 추가 오류를 유발할 수 있다. 자기 일관성 Wang 등(2022)은 다수 투표를 통해 최상의 답변을 선택하기 위해 다수의 체인을 샘플링한다. 최소-최대 프롬프트하는 Zhou 등(2022)은 질문을 더 간단한 하위 질문으로 줄이고 순차적으로 대답한다. 우리의 보상 공식과 유사하게, 최근의 작업들은 중간 단계들 Welleck 등(2022); Shinn 등(2023); Paul 등(2023)에 대한 피드백을 제공하기 위한 자기 평가 접근법들을 탐구하였다. 우리의 상태 공식과 정렬된, Li 등(2022)은 잠재된 "상황"을 LLM에 통합하며, 컨텍스트로부터의 엔티티들의 상태를 참조한다. 보다 관련성이 높은 최근의 연구는 일부 검색 알고리즘에 의해 안내되는 보다 복잡한 구조를 탐색하기 시작했다. 예를 들어, CoRe Zhu 등(2022)은 디코딩을 위한 MCTS와 함께 수학 단어 문제에 대한 미세-튜닝 추론 단계 생성기 및 검증기를 포함한다. 이와 동시에 Yao et al.(2023)은 휴리스틱 기반(heuristic-based)을 적용한다.

도 1: RAP(Reasoning via Planning)의 개요. 기존의 LLM 추론 방법인 Chain-of-Thought Wei et al.(2022)과 비교하여, 우리는 세계 모델로부터 세계 상태를 명시적으로 모델링하고(언어 모델로부터 용도 변경), 추론 문제를 해결하기 위해 고급 계획 알고리즘을 활용한다.

더 나은 추론 경로를 찾기 위해 깊이/폭 우선 검색과 같이 검색합니다. 그러나 위의 방법들 중 어느 것도 공식적으로 세계 모델을 도입하고 보상과 국가를 통일된 틀로 인스턴스화하지 않는다. 이러한 탐색 유도 방식과 비교하여 RAP는 세계 모델과 보상을 고급 계획과 결합하기 위한 보다 원칙적인 프레임워크이다.

**LLM으로 계획** 지능형 에이전트의 중심 능력인 계획에는 특정 목표 McCarthy(1963); Bylander(1994)를 달성하기 위한 일련의 작업이 포함됩니다. 고전적 계획 방법은 로봇 및 구체화된 환경인 Camacho and Alba(2013); Jiang et al.(2019)에서 널리 채택되어 왔다. 최근 LLM이 직접 계획을 수행하도록 유도하는 것이 주목을 받고 있으며 Huang et al.(2022); Singh et al.(2022); Ding et al.(2023). 더욱이, LLM들의 강력한 프로그래밍 능력에 기초하여, Lyu et al.(2023); Jojic et al.(2023); Liu et al.(2023); 최근 작품들은 먼저 자연 언어 명령어들을 계획 도메인 기술 언어(PDDL)와 같은 실행가능한 프로그래밍 언어들로 번역하고, LLM+P Liu et al.(2023)과 같은 고전적인 계획 알고리즘들을 실행한다. 그러나 코드 기반 계획은 좁은 영역과 환경에 의해 제한되고 RAP는 수학 및 논리적 추론과 같은 개방형 문제를 처리할 수 있다. 세계 모델 및 계획_에 대한 더 많은 관련 작업은 부록 D에서 논의된다.

## RAP(Reasoning via Planning) 3

본 절에서는 LLM이 광범위한 추론 작업을 해결하기 위해 일관성 있는 추론 트레이스를 전략적으로 계획할 수 있도록 하는 추론 via Planning(RAP) 프레임워크를 제시한다. 우리는 먼저 프롬프트와 함께 LLM을 용도 변경함으로써 세계 모델을 구축한다(섹션 3.1). 세계 모델은 LLM이 미리 계획하고 미래에 예상되는 결과를 찾을 수 있도록 함으로써 의도적인 계획의 기초가 된다. 그런 다음 3.2절에서 추론 중 각 상태를 평가하기 위한 보상을 소개한다. 세계 모델과 보상에 따라 몬테카를로 트리 탐색(MCTS)을 이용한 계획은 방대한 추론 공간을 효율적으로 탐색하고 최적의 추론 흔적을 찾는다(3.3절). 마지막으로, 계획 중에 다수의 유망한 추론 트레이스가 획득될 때, 앙상블된 결과를 산출하고 추론 성능을 더욱 향상시키는 섹션 3.4의 집계 방법을 추가로 도입한다.

### 언어 모델을 World 모델로

일반적으로 월드 모델은 현재 상태 Ha와 Schmidhuber(2018); Matsuo 등(2022)에 _action_을 적용한 후 추론의 다음 _state_를 예측한다. RAP는 우리가 당면한 특정한 추론 문제에 따라 상태와 행동의 일반적인 개념을 다른 방식으로 인스턴스화할 수 있게 한다(그림 2). 예를 들어, Blocksworld에서, 상태를 블록들의 구성(자연 언어로 설명됨)으로 정의하고, 블록을 움직이는 동작(예를 들어, "주황색 블록을 집어라")이 되는 동작을 정의하는 것은 당연하다. 수학 추론 문제에서 우리는 상태를 사용하여 중간 변수의 값을 나타내고, 행동을 추론을 구동하는 하위 질문으로 설정하여 새로운 값을 도출한다. 논리적 추론에서 상태는 우리가 집중하고 있는 사실이고, 행동은 다음 연역을 위한 규칙을 선택하는 것이다.

상태 및 동작의 정의에 따라, 추론 프로세스는 따라서 마르코프 결정 프로세스(MDP): 현재 상태가 주어지면

도 2: 블록월드(왼쪽)에서 플랜 생성을 위한 RAP, GSM8K(중간)에서 수학 추론, PrOntoQA(오른쪽)에서 논리 추론.

(s_{t,t=0,1,\ldots,T}\), 예를 들어 초기 상태 \(s_{0}\), LLM(추론 에이전트)은 생성 분포 \(a_{t}\sim p(a|s_{t},c)\)에서 샘플링하여 행동 공간을 생성하며, 여기서 \(c\)는 적절한 프롬프트(예: 문맥 내 시연)이다. 일단 행동이 선택되면, 세계 모델은 추론의 다음 상태 \(s_{t+1}\)를 예측한다. 구체적으로, 동일한 LLM을 사용하여 상태 전이 분포 \(p(s_{t+1}|s_{t},a_{t},c^{\prime})\), 여기서 \(c^{\prime}\)은 LLM이 상태를 생성하도록 유도하는 또 다른 프롬프트이다. 예를 들어, 블록월드에서 LLM은 이전 상태 \(s_{t}\)와 액션 \(a_{t}\)이 주어진 블록들의 새로운 구성을 기술하기 위해 텍스트 \(s_{t+1}\)을 생성한다.

프로세스를 계속하면 추론 트레이스가 생성되는데, 이 트레이스는 인터리빙된 상태들과 동작들의 시퀀스 \((s_{0},a_{0},s_{1},\ldots,a_{T-1},s_{T})\)로 구성된다. 이는, 중간 추론 단계들이 일련의 동작들, 예를 들어, (\(a_{0}=\) "픽업 레드 블록", \(a_{1}=\) "스택 온 옐로우 블록", …)로 이루어진 이전의 추론 방법들과 상이하다(도 1의 비교 참조). (예측된) 세계 상태들로 추론을 증강하는 것은 LLM이 보다 근거되고 일관성 있는 추론을 갖도록 돕는다. 전체 추론 트레이스는 _외부_ 실제 환경과 상호작용하지 않고 (_내부_ 세계 모델을 갖는 추론 에이전트로서) LLM 자체에 의해 시뮬레이션된다는 점에 유의한다. 이것은 인간의 마음속에 가능한 계획을 고민하는 것과 닮았다. 세계 모델을 도입하여 미래 상태를 시뮬레이션하는 능력은 3.3절에 설명된 바와 같이 방대한 추론 공간을 효율적으로 탐색하기 위해 원칙적인 계획 알고리즘을 통합할 수 있게 한다.

### Reward Design

추론하는 동안, 우리는 각각의 추론 단계들의 실현가능성 및 바람직성을 평가하고, 평가에 기초하여 추론을 안내하고자 한다(섹션 3.3). 각 추론 단계의 평가(즉, 상태 \(s_{t}\)에 작용 \(a_{t}\)을 적용)는 _reward_ 함수 \(r_{t}=r(s_{t},a_{t})\in\mathbb{R}\)에 의해 수행된다. 상태 및 행동과 유사하게, 보상 함수는 관심있는 추론 문제에 대한 임의의 지식 또는 선호도를 수용하기 위해 상이한 방식으로 특정될 수 있다. 여기서는 다양한 작업에 적용할 수 있는 몇 가지 일반적인 보상을 소개하고 실험에서 효과적인 것으로 나타났다.

**작업 가능성.** 컨텍스트 내 시연 및 현재 상태에서 LLM 컨디셔닝에 의해 작업이 생성되면 특정 작업의 확률이 LLM의 선호도를 반영합니다. 따라서 우리는 행동의 로그 확률을 보상으로 통합할 수 있다. 이 보상은 에이전트로서의 LLM의 "정체성"을 반영하며, 탐색하기 위한 사전 조치로도 사용될 수 있다.

**상태에 대 한 신뢰.** 상태 예측은 일부 문제 (예: 수학 추론 (그림 2, 중간)에서 수행 (즉, 하위 질문)이 주어지면 세계 모델은 하위 질문에 응답 하 여 다음 상태를 예측 합니다. 우리는 국가의 자신감(즉, 이 경우 답변)을 보상으로 통합한다. 구체적으로, 우리는 세계 모델에서 여러 샘플 답변을 도출하고 가장 빈번한 답변의 비율을 신뢰도로 사용한다. 더 높은 신뢰도는 상태 예측이 LLMs Hao 등(2023)의 세계 지식과 더 일치함을 나타내며, 이는 전형적으로 더 신뢰할 수 있는 추론 단계로 이어진다.

**LLM에 의한 자체 평가** 추론의 오류를 미리 생성하지 않는 것보다 쉽게 인식할 수 있습니다. 따라서 LLM이 "이 추론 단계가 올바른가요?"라는 질문으로 자신을 비판하고 토큰 "예"의 다음 단어 확률을 보상으로 사용할 수 있도록 하는 것이 좋습니다. 보상은 추론의 정확성에 대한 LLM 자체의 추정을 평가한다. 자기 평가를 위한 구체적인 문제는 과제에 따라 다를 수 있다는 점에 유의한다.

**작업별 휴리스틱.** RAP를 사용 하면 다른 작업별 휴리스틱을 보상 함수에 유연하게 연결할 수 있습니다. 예를 들어, Blockworld에 대한 계획 생성에서, 우리는 블록의 예측된 현재 상태를 보상 계산을 위한 목표와 비교한다(섹션 4.1). 보상은 목표물을 향해 능동적으로 속도를 내는 움직임의 계획을 장려한다.

몬테카를로 트리 검색 계획

세계 모델(섹션 3.1)과 보상(섹션 3.2)을 장착하면 LLM은 모든 계획 알고리즘으로 추론할 수 있다. 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS) Kocsis와 Szepesvari(2006); 쿨롬(Coulom, 2007); 전략적으로 추론 트리의 공간을 탐색하고 탐색과 활용 사이의 적절한 균형을 설정하여 고보상의 추론 흔적을 효율적으로 찾는다.

구체적으로, MCTS는 반복적으로 추론 트리를 구축하는데, 여기서 각 노드는 상태를 나타내고, 각 에지는 액션 및 액션을 적용한 후 현재 상태로부터 다음 상태로의 전이를 나타낸다(도 1). LLM 에이전트가 트리의 가장 유망한 노드를 확장하고 탐색할 수 있도록 하기 위해, 이 알고리즘은 상태-동작 값 함수 \(Q:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\)을 유지하며, 여기서 \(Q(s,a)\)는 상태 \(s\)에서 동작 \(a\)의 예상 미래 보상 값을 추정한다. 그림 3은 트리를 확장하고 \(Q\) 값을 업데이트하기 위해 각 반복에서 4개의 연산을 보여준다. 프로세스는 특정된 계산 예산(예를 들어, 반복의 수)에 도달하고, 결과 트레이스들이 트리로부터 획득될 때까지 계속된다. 계획 알고리즘의 더 자세한 내용과 의사 코드는 부록 A와 알고리즘 1에 나와 있다.

**선택** 첫 번째 단계에서는 다음 단계에서 추가 확장을 위해 가장 유망한 기존 트리의 일부를 선택합니다. 루트 노드(즉, 초기 상태 \(s_{0}\))로부터 시작하여, 트리의 각 레벨에서, 알고리즘은 자식 노드를 다음 노드로 선택한다. 현재 트리의 리프 노드에 도달하면 위상이 종료됩니다. 그림 3(a)는 선택된 경로를 빨간색으로 강조 표시한다. 탐색(덜 방문한 노드)과 공격(높은 값 노드) 사이의 균형을 맞추기 위해 트리에 적용된 잘 알려진 _상부 신뢰 범위(UCT)_(Kocsis 및 Szepesvari, 2006)를 사용하여 각 자식 노드를 선택한다. 특히 노드 \(s\)에서 \(Q\) 값(공용)과 불확실성(탐색)을 모두 고려하여 트리에서 작업을 선택합니다.

\[a^{*}=\operatorname*{arg\,\,max}_{a\in A(s)}\left[Q(s,a)+w\sqrt{\frac{\ln N(s)} {N(c(s,a))}}\right], \tag{1}\]

여기서, \(N(s)\)는 이전 반복에서 노드 \(s\)가 방문한 횟수이고, \(c(s,a)\)는 상태 \(s\)에서 노드 \(a\)를 적용한 자식 노드이다. 자식 노드가 이전에 덜 방문될수록(즉, 이 자식 노드에 대해 더 불확실할수록), 방정식의 두 번째 항이 더 높아진다. 가중치 \(w\)는 탐색과 착취 사이의 균형을 제어한다.

**확장.** 이 단계에서는 위에서 선택한 리프 노드에 새 자식 노드를 추가하여 트리를 확장합니다. 리프 노드의 상태가 주어지면 LLM(에이전트로서)을 사용하여 \(d\) 가능한 동작(예를 들어 수학 추론에서 하위 질문)을 샘플링한 다음 LLM(세계 모델로서)을 사용하여 각각의 다음 상태를 예측하여 \(d\) 자식 노드를 생성한다. 위에서 선택된 리프 노드가 이미 터미널 노드(추론 체인의 끝)인 경우, 확장을 건너뛰고 역전파로 점프할 것이라는 점에 유의한다.

**시뮬레이션.** 예상된 미래 보상(\(Q\) 값)을 추정하기 위해 이 단계는 세계 모델을 사용하여 현재 노드의 미래 상황을 시뮬레이션합니다. 위와 같이 현재 노드에서 시작 하면 각 노드 \(s\)에서 _roll-out policy_ 다음에 작업을 만들고 월드 모델을 사용 하 여 다음 상태를 예측 합니다. 롤아웃 프로세스는 터미널 상태에 도달할 때까지 계속된다. 롤아웃 정책을 정의하는 방법에는 여러 가지가 있을 수 있습니다. 실험에서는 잡음의 단순화와 감소를 위해 위의 확장과 유사한 과정을 따른다. 즉, \(d\) 후보 행동을 생성하고 가장 큰 국소 보상 \(a^{\prime}=\max_{a^{\prime}}r(s,a)\)을 선택한다. 실제로 효율성을 위해 \(r\)에서 계산 비용이 많이 드는 구성 요소를 버리고(예: 상태 신뢰로부터의 보상은 답을 여러 번 샘플링해야 함), 시뮬레이션 중에 동작을 선택하기 위해 결과적인 경량 보상 함수를 사용한다.

**역전파.** 위 단계에서 터미널 상태에 도달하면 루트 노드에서 터미널 노드까지의 추론 경로를 가져옵니다. 이제 업데이트 경로에서 보상을 다시 전파합니다.

도 3: MCTS 계획에서의 반복에서의 네 단계의 예시(섹션 3.3).

경로를 따라 각 상태-동작 쌍의 \(Q\) 값입니다. 특히, 노드 \(s\)의 모든 미래 단계에서 보상을 집계하여 \(Q(s,a)\)을 갱신한다.

미리 결정된 수의 MCTS 반복에 도달하면 알고리즘을 종료하고 평가를 위해 구성된 트리에서 최종 추론 추적을 선택한다. 선택에는 다양한 방법이 있습니다. 하나는 루트 노드에서 시작하여 터미널에 도달할 때까지 \(Q\) 값이 가장 높은 작업을 반복적으로 선택하는 것입니다. 또한, 가장 높은 보상을 산출한 반복들로부터 경로를 직접 선택하거나, 가장 많이 방문한 리프 노드(및 각각의 루트-투-리프 경로)를 선택하도록 선택할 수 있다. 실제로 우리는 두 번째 전략이 종종 최상의 결과를 산출한다는 것을 관찰했다.

### RAP-Aggregation

최종 답변만이 요구되는 수학 추론(섹션 4.2)과 같은 문제에 대해, RAP는 최종 답변을 생성하기 위해 집계될 상이한 MCTS 반복으로부터 다수의 트레이스 및 답변을 생성할 수 있었다. 우리는 이러한 메커니즘을 RAP-집계라고 한다. 계획 생성이나 논리적 추론과 같은 문제는 출력으로 완전한 추론 트레이스를 요구하므로 RAP-집계를 적용하지 않는다.

## 4 Experiments

본 절에서는 체화된 환경에서의 계획 생성(4.1), 수학 단어 문제 해결을 위한 수학적 추론(4.2), 가설 검증을 위한 논리적 추론(4.3) 등 광범위한 문제에 적용하여 RAP 프레임워크의 유연성과 효과성을 입증한다. 후속 섹션에서는 RAP의 세계 모델 공식화가 다양한 추론 컨텍스트를 충족시키면서 상태와 행동의 다재다능한 설계를 가능하게 하는 방법을 보여준다.

우리는 주로 RAP와 CoT(chain-of-thought)를 비교하고(Wei et al., 2022), 그리고 가장 적게 프롬프팅(Zhou et al., 2022)과 같은 그것의 변형들을 기준선으로서 비교한다. 또한 적용 가능한 경우 다중 추론 경로를 조립하는 것을 고려한다(자기 일관성(Wang et al., 2022)라고도 함). 또한, 계산 자원이 허용하는 경우 RAP를 GPT-4(OpenAI, 2023)와 비교한다. 기본적으로 샘플링 온도가 0.8인 방법과 기준선 모두에 대한 기본 LLM으로 LLaMA-33B 모델(Touvron 등, 2023)을 사용한다. 모든 프롬프트는 부록 C에 나열되어 있다.

### Plan Generation

계획 생성 작업은 주어진 목표를 달성하기 위한 일련의 행동을 생성하는 것을 목표로 하며, 아마도 추가적인 제약 조건을 가지고 있다. 계획을 생성하는 능력은 지능형 구체화된 에이전트, 예를 들어 가정용 로봇(Puig et al., 2018)에 중요하다.

**작업 설정.** 계획 생성 작업에 대 한 RAP 프레임워크의 실행 가능성을 탐색 하기 위해 에이전트가 특정 순서로 블록을 스택으로 재배열 하도록 요청 되는 블록 월드 벤치 마크 (Valmeekam 등, 2022)에서 RAP를 적용 하 고 평가 합니다. **상태** 를 블록의 현재 방향으로 정의하고 **작업** 을 블록을 이동하는 명령으로 정의합니다. 구체적으로 액션은 4개의 동사(즉, Stack, Unstack, Put, Pickup) 중 하나와 조작된 객체로 구성된다. 액션 공간에 대해, 우리는 액션에 대한 도메인 제한과 블록의 현재 방향을 고려하여 현재 유효한 액션을 생성한다. 상태들 간의 전송을 위해, 우리는 현재 동작을 취하고 LLM에 질의하여 관련 블록들에 대한 상태 변화들을 예측한다. 그런 다음 새로운 블록 조건을 추가하고 더 이상 참이 아닌 조건을 제거하여 현재 상태를 업데이트한다. 일단 상태가 목표의 모든 조건들을 충족하거나 트리의 깊이 한계에 도달하면, 우리는 연관된 노드를 종료한다.

이 도메인 내에서 작업의 품질을 평가하기 위해 두 개의 개별 **보상** 을 사용합니다. 먼저 LLM을 예제 테스트 케이스와 그 해결책으로 프롬프트한 다음, 현재 상태(섹션 3.2의 "행동 가능성"_ 보상)가 주어졌을 때 행동의 로그 확률을 계산한다. \(r_{1}\). 이 보상은 추론 에이전트로서의 LLM의 직관을 반영한다. 일반적으로 목표에 대한 단계가 거의 남아 있지 않은 경우 표시되지만 멀리 있는 목표에는 신뢰할 수 없습니다. 추가적으로, 우리는 행동을 수행한 후 새로운 상태를 목표와 비교하고 보상, \(r_{2}\), 충족된 조건의 수와 스케일링(_"Task-specific heuristics"_ reward)을 제공한다. 특히 모든 조건이 충족되면 이 계획이 솔루션으로 선택되도록 초대형 보상을 할당합니다.

**결과.** Blocksworld 데이터 세트(Valmeekam 등, 2023)의 테스트 케이스를 사용하고 필요한 최소 작업 수로 그룹화하여 2단계 내에서 30건, 4단계 내에서 57건, 6단계 내에서 114건을 해결할 수 있습니다. 각 테스트 케이스에는 최대 5개의 블록이 있습니다. 기본 방법으로 해당 솔루션이 있는 4개의 테스트 케이스로 LLM을 프롬프트하고 새로운 질문에 대한 계획을 생성하도록 요청한다. 이 설정은 Valmeekam 등(2022)에서 설명한 것과 동일하며, 해가 단계별로 생성됨에 따라 Chain-of-Thought(CoT)로 표기한다. RAP의 경우 LLM이 \(r_{1}\)을 계산하는 데 도움이 되는 동일한 프롬프트가 표시된다.

표 1에서 볼 수 있듯이 LLMA-33B를 사용한 CoT는 몇 가지 2단계 사례에 대해서만 성공적인 계획을 생성할 수 있으며 더 어려운 문제에서는 완전히 실패한다. RAP는 4단계 내의 모든 문제와 6단계 문제의 일부를 거의 해결하여 CoT보다 상당히 향상되어 평균 성공률 \(64\%\)을 달성한다. 6단계 문제의 탐색 공간은 \(5^{6}\)만큼 클 수 있는 반면, 우리의 알고리즘은 20회 반복 내에서 42%의 성공적인 계획을 찾을 수 있다는 점에 주목할 필요가 있다. 또한, 제안된 프레임워크는 LLaMA-33B가 GPT-4보다 훨씬 더 강한 추론 능력을 갖는 것으로 알려진 상대적 이득(33\%\)을 훨씬 능가할 수 있도록 한다(Bubeck et al., 2023).

**사례 연구.** 그림 4에서 CoT와 RAP의 추론 경로를 비교 합니다. 개선을 설명 하는 이유를 요약 합니다. (1) 추론 중 세계 상태를 유지 하 여 RAP는 현재 상태에 대 한 유효한 동작을 인식 하 여 불법 계획 생성을 피할 수 있습니다. (2) RAP는 LLM으로부터의 첫 번째 직관이 작동하지 않을 때 역추적하고 다른 해결책을 시도할 수 있다. 구체적으로, CoT는 두 번째 목표, 즉 "빨간색 오렌지"를 달성하고 첫 번째 두 단계로 달성하려고 시도한다. 그러나 두 번째 골을 먼저 달성하면 첫 번째 골이 만족되는 것을 막을 수 있다. 반대로, RAP가 첫 번째 반복에서 동일한 실수를 하더라도, 우리의 프레임워크는 에이전트를 구동하여 (섹션 3.3에 설명된 바와 같이) 다른 가능한 경로를 탐색하고 최종적으로 성공적인 계획을 생성한다. (3) \(r_{t}\)을 계산할 때, 우리는 단지 현재 상태를 LLM에 피딩하고 이력을 숨길 수 있다. 예를 들어, 도 4의 경우, \(a_{2}\)에 대한 보상을 계산하기 위해, LLM에는 \(s_{2}\)이 초기 상태인 "새로운" 테스트 케이스가 제공된다. 이는 마지막 몇 단계의 어려움을 상당히 낮추고, 처음 몇 단계의 더 어려운 결정을 위해 더 많은 반복을 절약한다.

### Math Reasoning

**작업 설정.** GSM8k(Cobbe 등, 2021)와 같은 수학 추론 작업에는 종종 설명 및 최종 질문이 포함됩니다. 마지막 질문에 대한 답에 도달하기 위해서는 문제의 맥락에 기반한 다단계 수학적 계산을 수행할 필요가 있다. 따라서, 최종 질문을 더 작은 하위 질문들의 시퀀스로 분해하는 것이 자연스럽다(도 2, 우측). 중간 변수의 값으로 **상태** 를 정의 하 고 알 수 없는 중간 변수에 대 한 증분 하위 질문을 제안 하는 **작업** 을 정의 합니다. 그런 다음 세계 모형은 중간 변수와 문제 설명을 사용하여 하위 질문에 응답하고 새로운 중간 변수 값을 다음 상태에 추가한다. 가중 기하 평균 \(r_{t}=r_{t,1}^{\alpha}*r_{t,2}^{1-\alpha}\)을 **보상** 함수로 사용하여 LLM \(r_{t,1}\)에 의한 유용성에 대한 자기 평가와 상태 신뢰 \(r_{t,2}\)을 결합한다. 이 보상은 더 적절하고 유용한 하위 질문을 장려한다. 를 설명하는 방법

\begin{table}
\begin{tabular}{r c c c} \hline \hline
**Method** & **2-step** & **4-step** & **6-step** \\ \hline CoT & 0.17 & 0.02 & 0.00 \\ CoT - pass@10 & 0.23 & 0.07 & 0.00 \\ CoT (GPT-4) & 0.50 & 0.63 & 0.40 \\ \hline RAP\({}^{(10)}\) & 1.00 & 0.86 & 0.26 \\ RAP\({}^{(20)}\) & **1.00** & **0.88** & **0.42** \\ \hline \hline \end{tabular}
\end{table}
표 1: Blockworld에 대한 결과. RAP\({}^{(10)}\)과 RAP\({}^{(20)}\)은 반복 횟수를 각각 10과 20으로 설정하는 방법을 의미한다. “pass@10”은 각 테스트 케이스에 대해 10개의 플랜이 샘플링되는 것을 의미하고, 적어도 하나의 플랜이 올바른 경우 테스트 케이스는 해결된 것으로 간주된다. RAP를 포함한 다른 모든 설정은 단일 계획만 평가합니다.

도 4: CoT(왼쪽) 및 RAP(오른쪽)로부터의 블록월드에서의 추론 트레이스 비교.

추론 경로의 길이가 보상에 미치는 영향에 따라 다음 단계에서 평균 보상의 최대값을 사용하여 **\(Q\) 값** 을 계산합니다.

\[Q^{\star}(s_{t},a_{t})=\max_{s_{t},a_{t},r_{t},\ldots,s_{t},a_{t},r_{t},s_{t+1}} \operatorname{avg}(r_{t},\ldots,r_{l}). \tag{2}\]

관련 작업으로서 Least-to-Most 프롬프트(Zhou et al., 2022)는 하위 질문 분해에서 우리와 유사한 아이디어를 공유하지만, 동시에 하위 질문을 생성한다. 반대로 RAP는 현재 상태 \(s_{t}\)를 기반으로 각 동작 \(a_{t}\)을 고려하므로 더 많은 정보에 입각한 결정을 가능하게 한다.

**결과.** 초등학교 수학 단어 문제의 데이터 집합인 GSM8k에서 프레임워크를 평가합니다. 또한 CoT 프롬프팅(Wei et al., 2022), Least-to-Most 프롬프팅(Zhou et al., 2022) 및 이들의 자기 일관성(Wang et al., 2022) 변형을 기준으로 기본 모델을 평가한다. 우리는 모든 방법에 대해 동일한 4-샷 예제 시연을 사용한다.

표 2에 나타난 바와 같이, RAP 프레임워크는 문제의 \(48.8\%\)을 정확하게 응답함으로써, 자기 일관성(Self-Consistency)을 가진 사상 사슬( Chain-of-Thinking)과 최소-최대 프롬프트(Least-to-Most 프롬프트)를 모두 능가한다. 특히, 이 결과는 RAP가 보상에 기초하여 오직 하나의 추론 트레이스만을 선택할 때 달성된다. RAP-Aggregate의 도입은 \(\sim 3\%\)의 정확도를 더욱 향상시킨다. 또한, MCTS의 반복 횟수와 기준선에서의 자기 일관성 샘플 수에 따른 정확도를 그림 5와 같이 계산한다. 모든 반복 횟수/샘플에서 RAP-Aggregation이 기준선보다 일관되게 우수함을 알 수 있으며, 이는 몇 개의 반복 횟수/샘플만 허용될 때 보상의 가이드를 사용하여 신뢰할 수 있는 추론 경로를 찾는 데 프레임워크가 훨씬 더 우수하다는 것을 나타낸다.

### Logical Reasoning

**작업 설정.** 논리 추론 작업 (예: PrOntoQA (Saparov and He, 2022))은 일반적으로 _facts_ 및 _논리 규칙_ 집합을 제공 합니다. 그림 2에 설명 된 대로 지정 된 사실에 논리 규칙을 적용 하 여 _가설 사실_이 true 또는 false 인지를 확인 하는 모델이 필요 합니다. 이러한 작업은 올바른 최종 답변 (true/false) 뿐만 아니라 결과를 입증 하는 자세한 증명이 필요 합니다. 프레임워크를 적용하기 위해 추론을 위해 인간의 작업 메모리(Baddeley, 1992)와 유사하게 우리가 중점을 두고 있는 사실로 **상태** 를 정의합니다. **작업** 은 팩트 집합에서 규칙을 선택하는 것으로 정의됩니다. 세계 모델은 다음 상태로서 새로운 사실을 얻기 위해 하나의 홉 추론 단계를 수행한다. **보상** 은 자체 평가로 계산 됩니다 (섹션 3.2). 특히 추론 단계의 품질을 더 잘 이해할 수 있도록 레이블이 있는 몇 가지 예제를 사용 하 여 LLM에 프롬프트 합니다. 다음 단계의 평균 보상을 사용 하 여 GSM8k에 대 한 식 (2)와 동일한 ** \(Q\) 함수** 를 업데이트 합니다.

**결과.** PrOntoQA(Saparov and He, 2022)에 대한 RAP 프레임워크의 성능을 평가하고 규칙의 "무작위" 순서인 "진정한" 온톨로지 설정을 채택합니다. 우리는 LLM이 추론을 언제 끝낼지 암기하는 것을 방지하기 위해 정확한 증명에서 3, 4, 5개의 추론 홉이 필요한 예제를 혼합한다. 우리는 사파로프와 He(2022)가 발표한 생성 스크립트에서 500개의 예제를 샘플링한다. 우리는 최종 답의 예측 정확도와 전체 증명의 정확도를 모두 비교한다. 우리는 MCTS에 대해 20번의 반복을 수행하고 자기 일관성을 위해 20개의 샘플을 수행한다.

표 3에 제시된 결과, 본 프레임워크는 94.2%의 정답률과 78.8%의 증명 정확도를 달성하여, 증명 정확도 14%의 CoT 기준선과 \(4.4\%\)의 예측 정확도인 자기 일관성 CoT 기준선을 능가한다. 상기 실질적인 개선은 명백히

\begin{table}
\begin{tabular}{r|c} \hline \hline
**Method** & **Accuracy (\%)** \\ \hline Chain-of-Thought & 29.4 \\ + SC\({}^{(10)}\) & 46.8 \\ Least-to-Most & 25.5 \\ + SC\({}^{(10)}\) & 42.5 \\ \hline RAP\({}^{(1)}\) & 40.0 \\ RAP\({}^{(10)}\) & 48.6 \\ + aggr & **51.6** \\ \hline \hline \end{tabular}
\end{table}
표 2: GSM8k에 대한 결과. 위 첨자는 샘플 또는 반복 횟수를 나타냅니다.

도 5: 상이한 수의 샘플링된 경로 또는 반복을 갖는 GSM-8K에 대한 결과.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **Pred Acc** & **Proof Acc** \\ \hline CoT & 87.8 & 64.8 \\ CoT + SC & 89.8 & - \\ \hline RAP (Ours) & **94.2** & **78.8** \\ \hline \hline \end{tabular}
\end{table}
표 3: ProntoQA에 대한 결과.

PrOntoQA에서 논리적 추론 문제를 해결하는 데 있어 RAP의 효과를 입증한다. 또한, 도 2에 예시된 경우와 같이, RAP는 추론 체인이 막다른 골목에 왔을 때를 효과적으로 인식할 수 있고, 신호를 이전의 추론 단계들로 다시 전파할 수 있으며, 계획 알고리즘을 통해 이전의 단계들에 대한 대안들을 탐색할 수 있다. 자기 평가 보상은 RAP가 잠재적인 잘못된 추론 단계를 인식하는 데 더 도움이 되어 에이전트가 향후 반복에서 이를 피하도록 장려한다.

## 5 Analysis

### Complex problems

RAP가 더 복잡한 문제들을 해결하기 위해 더 강한 LLM들을 도울 수 있는지 여부를 더 연구하기 위해, 더 가능한 LLM, Llama-2 70B Touvron 등(2023)을 사용하여 전체 블록월드 Valmeekam 등(2023) 데이터세트에 대한 실험을 수행한다.

Full Blocksworld Valmeekam 등(2023)은 602개의 테스트 케이스로 구성된다. 각 테스트 케이스에 필요한 최소 작업 수를 기준으로 그룹화합니다. 우리의 실험은 Easy와 Hard의 두 가지 별개의 설정에서 수행된다. 쉬운 설정에서는 각 경우에 대한 최소 작업 수에 대한 사전 지식을 가정합니다. 이 정보를 활용 하 여 테스트 사례와 동일한 최소 작업 수를 공유 하는 데모 사례를 사용 합니다. 각 사례 그룹에 대해 10개의 사례를 무작위로 선택하여 시연 사례 풀을 만들고 나머지 사례를 테스트 세트로 둔다. 추론하는 동안, 우리는 이 풀에서 무작위로 4-샷 시연 사례를 샘플링하고 프롬프트를 공식화하는 데 활용한다. 하드 설정에서는 전체 데이터 세트에서 10개의 경우를 무작위로 선택하여 데모 풀을 구성한 다음 테스트 세트에서 이러한 경우를 제외한다. 추론하는 동안 테스트 케이스에 필요한 최소 작업 수에 관계없이 이 전역 풀에서 무작위로 4-샷 시연 사례를 샘플링한다.

우리는 Wei et al. (2022)를 기준선으로 사용하고, 개선된 프롬프트 기법(부록 E)으로 RAP\({}^{(10)}\)(10회 반복)을 평가한다. 실험 결과는 표 4에 요약되어 있다. Easy 및 Hard 설정 모두에서 RAP는 CoT보다 상당한 마진만큼 우수한 성능을 보여준다. 특히, 테스트 케이스가 해결하기 위해 더 많은 수의 단계(6개 이상)를 필요로 할 때 CoT는 성공률의 심각한 감소를 나타내는 반면 RAP는 상대적으로 높은 성공률을 유지한다. 이러한 결과를 섹션 4.1과 비교하면 RAP는 내재적 능력에 관계없이 LLM의 추론 능력을 향상시킬 수 있는 일반적인 프레임워크라는 결론을 내린다.

### Reward Choice

주요 실험에서는 휴리스틱과 탐색 실험을 기반으로 현재 실험에서 보상의 조합을 선택한다. LLM 추론에 대한 보상 선택의 효과를 파악하기 위해 계획 생성(표 5)과 수학 추론(표 6)에 대한 보상에 대한 포괄적인 실험을 보완한다.

일반적으로, 다수의 보상의 조합은 성능에 기여한다. 그러나 보상의 효과는 작업의 특성에 따라 달라집니다. 예를 들어, 행동 가능성 보상은 계획 생성에 필수적이지만 수학 추론에는 그다지 도움이 되지 않는다. 더 많은 토론은 부록 F에 있다.

## 6 Conclusion

본 논문에서는 LLM에 인간과 유사한 전략적 계획을 추론할 수 있는 능력을 갖춘 새로운 LLM 추론 프레임워크인 RAP을 제시한다. LLM이 세계 모델과 추론 에이전트 역할을 할 수 있도록 하는 우리의 프레임워크는 LLM이 세계의 상태를 시뮬레이션하고 행동 결과를 예상하며 몬테카를로 트리 검색을 통해 탐색과 개발 사이의 효과적인 균형을 달성할 수 있도록 한다. 다양한 도전적 추론 문제에 대한 광범위한 실험은 여러 현대 CoT 기반 추론 접근법, 특정 설정에서 고급 GPT-4에 비해 RAP의 우수성을 보여준다.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline
**Setting** & **Method** & **2-step** & **4-step** & **6-step** & **8-step** & **10-step** & **12-step** & **All** \\ \hline Easy & CoT & 0.49 & 0.18 & 0.06 & 0.01 & 0.01 & 0.00 & 0.08 \\  & RAP\({}^{(10)}\) & 1.00 & 0.99 & 0.75 & 0.61 & 0.32 & 0.32 & 0.65 \\ \hline Hard & CoT & 0.22 & 0.14 & 0.02 & 0.02 & 0.00 & 0.00 & 0.05 \\  & RAP\({}^{(10)}\) & 0.67 & 0.76 & 0.74 & 0.48 & 0.17 & 0.09 & 0.51 \\ \hline \hline \end{tabular}
\end{table}
표 4: Llama-2 70B가 있는 전체 블록월드에 대한 결과.

### Limitations

이 작업에서 우리는 주로 사전 훈련에 의해 능력이 제한될 수 있는 냉동 LLM을 활용하는 데 중점을 둔다. 향후 LLM을 더 나은 이성으로 미세 조정하고 세계 모델[22] 역할을 하는 방법과 외부 도구[10, 23]와 RAP를 결합하여 보다 복잡한 현실 세계의 문제를 해결하는 방법을 탐구할 가치가 있다.

## Ethics Statement

이 논문에서 우리는 주로 계획 생성, 수학적 추론 및 논리적 추론에 대한 응용에 초점을 맞추며 중요한 윤리적 문제를 제기하지 않는다. 우리는 LLM을 사용한 추론의 경계 적용에 대한 향후 연구가 오용의 위험을 초래할 수 있음을 인식하고 관련 기술이 실제 세계에 적용되기 전에 안전의 모든 측면을 신중하게 고려할 것을 권장한다.

## References

*[1]A. Baddeley (1992) Working memory. Science255(5044), pp.556-559. Cited by: SS1.
* [2]R. E. Briscoe (2011) Mental imagery and varieties of amodal perception. Pacific Philosophical Quarterly92(2), pp. 153-173. Cited by: SS1.
*[3]T. B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Neural Information Processing Systems33, pp. 1877-1901. Cited by: SS1.
*[4]S. 부벡 찬드라세카란 Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. 이성호 Lundberg, et al.(2023) Sparks of artificial general intelligence: early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Cited by: SS1.
*[5]T. 빌란데르(1994) 명제 스트립 계획의 계산 복잡성. Artificial Intelligence69(1-2), pp. 165-204. Cited by: SS1.
* [6]E. F. Camacho and C. Bordons Alba (2013) Model prediction control. 스프링어 과학 및 비즈니스 미디어입니다. 에 의해 인용된다: SS1.
*[7]A. 조우더리 나랑 데블린 Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022) Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.
*[8]K. 코베 고사라주 바이에른 진현준 카이저 플래퍼트, J. 투렉, J. 힐튼, R. Nakano, et al.(2021) Training verifiers to solve mathematics word problems. arXiv preprint arXiv:2110.14168. Cited by: SS1.
*[9]R. Coulom (2007) Efficient selectivity and backup operator in monte-carlo tree search. In Computers and Games: 5th International Conference, CG 2006, Turin, Italy, May 29-31, 2006. Revised Papers 5, pp. 72-83. Cited by: SS1.
*[10]Y. 딩, 엑스 Zhang, C. Paxton, S. Zhang(2023) Task and motion planning with large language models for object rearrangement. arXiv preprint arXiv:2303.06247. Cited by: SS1.
*[11]W. G. Gasparski and T. Orel (2014) Desig-nology: 행동 계획에 관한 연구. 1권, 거래 출판사입니다. 에 의해 인용된다: SS1.
*[12]D. Gentner and A. L. Stevens (2014) Mental models. 심리학 기자요 에 의해 인용된다: SS1.
*[13]D. Ha and J. Schmidhuber (2018) Recurrent world models facilitate policy evolution. 신경 정보 처리 시스템31에서의 진보. 인용: SS1.
*[14]D. Ha and J. Schmidhuber (2018) World models. arXiv preprint arXiv:1803.10122. Cited by: SS1.
*[15]D. 하프너 Lillicrap, J. Ba, M. Norouzi (2019) Dream to control: 잠재된 상상력에 의한 학습 행동. arXiv preprint arXiv:1912.01603. Cited by: SS1.
*[16]D. 하프너 릴리크랩 Norouzi, and J. Ba(2020) Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193. Cited by: SS1.
*[17]S. 하오태 류진 Wang, Z. Hu(2023) ToolKengPT: 도구 임베딩을 통해 대용량 도구로 냉동 언어 모델을 증강한다. 신경 정보 처리 시스템36에서의 진보. 인용: SS1.
*[18]S. 하오병탄 탕비니 Shao, H. Zhang, E. Xing, and Z. Hu(2023) Bertnet: 사전 훈련된 언어 모델들로부터 임의의 관계들을 갖는 지식 그래프들을 수확한다. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 5000-5015. Cited by: SS1.
*[19]M. K. Ho, D. Abel, C. G. Correa, M. L. Littman, J. D. Cohen, and T. L. Griffiths (2021) Control of mental representation of human planning. arXiv 전자프린트. External Links: 2105.01603 Cited by: SS1.
*[20]J. 황과 K Chen-Chuan Chang (2022) 대언어 모델의 추론: 설문 조사. arXiv preprint arXiv:2212.10403. Cited by: SS1.
*[21]W. 황화샤 Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. (2022) Inner monologue: embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608. Cited by: SS1.

쿠엔틴 JM Huys, Neir Eshel, Elizabeth O'Nions, Luke Sheridan, Peter Dayan, Jonathan P Roiser. 2012. Bonsai trees in your head: how how the pavlovian system sculpts goal-directed choices by pruning decision trees. _ PLoS computational biology_, 8(3):e1002410.
* Jiang 등(2019) Yu-qian Jiang, Shi-qi Zhang, Piyush Khandelwal, and Peter Stone. 2019. Task planning in robotics: empirical comparison of pddl-and asp-based systems. _ Frontiers of Information Technology & Electronic Engineering_, 20:363-373.
* Johnson-Laird (2010) Philip N Johnson-Laird. 2010. Mental models and human reasoning. _ Proceedings of the National Academy of Sciences_, 107(43):18243-18250.
* Johnson-Laird (1983) Philip Nicholas Johnson-Laird. 1983. _Mental models: Towards a cognitive science of language, inference and consciousness_. 6. Harvard University Press.
* Jojic 등(2023) Ana Jojic, Zhen Wang, and Nebojsa Jojic. 2023. Gpt가 튜링 머신이 되고 있습니다. 프로그래밍하는 몇 가지 방법이 있습니다. _ arXiv preprint arXiv:2303.14310_.
* Kocsis and Szepesvari (2006) Levente Kocsis and Csaba Szepesvari. 2006. Bandit based monte-carlo planning. _Machine Learning: ECML 2006: 17th European Conference on Machine Learning Berlin, September 18-22, 2006 Proceedings 17_, pages 282-293. Springer.
* Kojima 등(2022) Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. 대형 언어 모델은 제로 샷 추론기입니다. _ arXiv preprint arXiv:2205.11916_.
* LeCun(2022) Yann LeCun. 2022. A path toward autonomous machine intelligence version 0.9. 2, 2022-06-27. _Open Review_, 62.
* Li 등(2022) Belinda Z Li, Maxwell Nye, and Jacob Andreas. 2022. 잠재 상황을 갖는 언어 모델링. _ arXiv preprint arXiv:2212.10012_.
* Liu 등(2023) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+ p: 최적의 계획 숙련도를 갖는 대형 언어 모델들을 무력화함_ arXiv preprint arXiv:2304.11477_.
* Lyu 등(2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought reasoning. _ arXiv preprint arXiv:2301.13379_.
* Matsuo 등(2022) Yutaka Matsuo, Yann LeCun, Maneesh Sahani, Doina Precup, David Silver, Masashi Sugiyama, Eiji Uchibe, and Jun Morimoto. 2022. Deep Learning, Reinforcement Learning, World Model. _ Neural Networks_.
* McCarthy (1963) John McCarthy. 1963. 상황, 행동 및 인과관계 법칙. 기술 보고서, 스탠포드 대학교 컴퓨터 과학 전공
* Mialon et al.(2023) Gregoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: survey. _ arXiv preprint arXiv:2302.07842_.
* OpenAI(2023) OpenAI. 2023. Gpt-4 기술 보고서.
* Paul 등(2023) Debjit Paul, Mete Ismayizlzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. 정제자: 중간 표현들에 대한 추론 피드백. _ arXiv preprint arXiv:2304.01904_.
* Puig 등(2018) Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018. Virtualhome: 프로그램을 통한 가정 활동 시뮬레이션.
* Saparov and He (2022) Abulhair Saparov and He He. 2022. 언어 모델들은 탐욕적 추론기들이다 : 체인 오브 사상의 체계적인 형식 분석. _ arXiv preprint arXiv:2210.01240_.
* Schick 등(2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: 언어 모델은 스스로 도구를 사용하는 방법을 가르칠 수 있습니다. _ arXiv preprint arXiv:2302.04761_.
* Schrittwieser et al.(2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. 2020. Mastering atari, go, chess and shogi by planning with a learned model. _ Nature_, 588(7839):604-609.
* Schulkin (2012) Jay Schulkin. 2012. _Action, perception and the brain: Adaptation and cephalic expression_. 스프링거
* Sekar 등(2020) Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. 2020. Planning to explore via self-supervised world models. _International Conference on Machine Learning_, pages 8583-8592. PMLR.
*Shinn 등(2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. 반사: 동적 메모리 및 자기-반사를 갖는 자율 에이전트. _ ArXiv_, abs/2303.11366.
* Silver et al.(2017) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. 2017. Mastering 체스와 shogi by self-play by general reinforcement learning algorithm. _ arXiv preprint arXiv:1712.01815_.
*Singh 등(2022) Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2022. Progrompt: 대형 언어 모델을 이용하여 상황 로봇 작업 계획을 생성한다. _ arXiv preprint arXiv:2209.11302_.
* 톨만(1948) 에드워드 C 톨만. 1948. Cognitive maps in rats and men. _ Psychological review_, 55(4):189.
* TschHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: 오픈하고 효율적인 기초 언어 모델입니다. _ arXiv preprint arXiv:2302.13971_.
* Touvron 등(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, 등 2023b. 라마 2: 기반 및 미세 조정 채팅 모델을 엽니다. _ arXiv preprint arXiv:2307.09288_.
* Valmeekam 등(2022) Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. 대규모 언어 모델은 여전히 계획할 수 없습니다 (변화에 대 한 계획 및 추론에 대 한 기준). _ arXiv preprint arXiv:2206.10498_.
* Valmeekam 등(2023) Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kambhampati. 2023. 대규모 언어 모델의 계획 능력에 대해 (제안된 벤치마크를 사용 하 여 중요 한 조사) _ arXiv preprint arXiv:2302.06706_.
* Wang 등(2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency는 언어 모델에서 사고 추론의 사슬을 향상시킵니다. _ arXiv preprint arXiv:2203.11171_.
* Wei 등(2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thinking prompting elicits reasoning in large language models. _ arXiv preprint arXiv:2201.11903_.
* Welleck 등(2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. 자가 수정하는 방법을 학습하여 시퀀스를 생성하는 단계. _ arXiv preprint arXiv:2211.00053_.
* Wu 등(2023) Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. 2023. Day-dreamer: World models for physical robot learning. _Conference on Robot Learning_, pages 2226-2240. PMLR.
* Xiang 등(2023) Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. 2023. 언어 모델들은 세계 모델들을 충족시킨다: 구현된 경험들은 언어 모델들을 향상시킨다. _ Neural Information Processing Systems_, 36의 진보.
*Yao 등(2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. _ arXiv preprint arXiv:2305.10601_.
* Zhou 등(2022) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. 최소 최대 프롬프팅은 대규모 언어 모델에서 복잡한 추론을 가능하게 합니다. _ arXiv preprint arXiv:2205.10625_.
* Zhu et al.(2022) Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2022. 협동 추론 유도 언어 모델을 통한 수학 단어 문제 해결. _ arXiv preprint arXiv:2210.16257_.

MCTS Planning

최적의 추론 경로를 탐색하기 위해 MCTS를 적용한다(알고리즘 1). 기존의 MCTS와 비교할 때, 우리는 큰 추론 공간과 LLM의 많은 계산 비용에 직면한다. 따라서, 본 구현에서 고전적인 MCTS를 몇 가지 수정하였다 : (1) 오픈 도메인 문제, 예를 들어 수학 문제의 경우, 모든 동작(하위 질문)을 열거하는 것이 불가능하므로, 현재 상태의 프롬프트 및 인코텍스트 데모를 조건으로 하는 LLM으로부터 고정된 수의 잠재적 동작을 샘플링함으로써 동작 공간을 줄인다. (2) 선택 단계에서 이전에 방문하지 않은 액션이 있는 경우, 경량 로컬 보상, 예를 들어 자기 평가 보상으로 Q 값을 추정한 후 UCT로 액션을 선택한다. 이는 제한된 반복 예산을 고려할 때 중요한 탐사에 대한 사전 지식을 제공한다.

## Appendix B 실험 설정

### 언어 모델 디코딩

우리는 0.8의 온도를 갖는 랜덤 샘플링을 사용한다. 생성은 최대 길이 2048 또는 뉴라인 토큰에서 차단된다.

### Computing Resources

모든 실험은 24GB 메모리를 갖는 4 \(\times\) NVIDIA A5000 GPU에서 실행된다.

## Appendix C Prompt

### Plan Generation

우리는 아래 RAP에 대한 행동 가능성을 계산하는 프롬프트를 보여준다. 동일한 프롬프트가 CoT 베이스라인에서도 적용된다. <init_state> 및 <goals>는 해결하려는 문제에 의해 인스턴스화될 것이다.

블록을 스택으로 배열해야 하는 블록 세트를 가지고 놀고 있습니다. 제가 할 수 있는 행동은 다음과 같습니다.

블록 가져오기 다른 블록 위에서 블록 쌓기 해제 블록 내려놓기 다른 블록 위에 블록 쌓기 작업에는 다음과 같은 제한이 있습니다.

한 번에 한 블록만 픽업하거나 풀 수 있습니다. 손이 비어 있는 경우에만 한 블록을 줍거나 분리할 수 있습니다. 블록이 테이블 위에 있고 블록이 깨끗해야 블록을 찾을 수 있습니다. 블록 위에 다른 블록이 없고 블록이 픽업되지 않은 경우 블록은 명확하다. 내가 쌓고 있는 블록이 실제로 다른 블록 위에 있었다면, 나는 다른 블록 위에서 블록을 쌓을 수 있을 뿐이다. 나는 내가 쌓고 있는 블록이 명확하다면 다른 블록 위에서 한 블록만 쌓을 수 있다. 블록을 줍거나 펼치면 블록을 잡고 있습니다. 제가 들고 있는 블록만 내려놓을 수 있습니다. 쌓인 블록을 잡고 있는 경우에만 다른 블록 위에 블록을 쌓을 수 있습니다. 나는 블록을 쌓고 있는 블록이 명확하다면 다른 블록 위에만 블록을 쌓을 수 있다. 블록을 내려놓거나 쌓으면 손이 텅 비게 됩니다.

[STATEMENT] 초기 조건들로서, 적색 블록은 맑고, 황색 블록은 맑고, 손은 비어 있고, 적색 블록은 청색 블록 위에 있고, 황색 블록은 오렌지 블록 위에 있고, 청색 블록은 테이블 위에 있고 오렌지 블록은 테이블 위에 있다. 제 목표는 주황색 블록이 빨간색 블록 위에 있는 것입니다. 내 계획은 다음과 같다.

[PLAN] unstack the yellow block from on the top of the orange block put down the yellow block pick up the orange block ```
1: 초기 상태 \(s_{0}\), 상태 전이 확률 함수 \(p_{\theta}\), 보상 함수 \(r_{\theta}\), 액션 발생기 \(p_{\phi}\), 생성된 액션 수 \(d\), 깊이 제한 \(L\), 롤아웃 수 \(N\), 탐색 가중치 \(w\)
2: action의 메모리 초기화 \(A:\mathcal{S}\mapsto\mathcal{A}\), children \(c:\mathcal{S}\times\mathcal{A}\mapsto\mathcal{S}\) 및 reward \(r:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\)
3: State-action 값 함수 초기화 \(Q:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) 및 방문 카운터 \(N:\mathcal{S}\mapsto\mathbb{N}\)
4:for\(n\gets 0,\dots,N-1\)do
5:while\(N(s_{t})>0\)do\(\triangleright\) Selection
6:\(N(s_{t})\gets N(s_{t})+1\)
7:\(a_{t}\leftarrow\arg\max_{a\in A(s_{t})}\left[Q(s_{t},a)+w\sqrt{\frac{\ln N(s _{t})}{N(c(s_{t},a))}}\right]\)
8:\(r_{t}=r(s_{t},a_{t})\), \(s_{t+1}\gets c(s_{t},a_{t})\)
9:\(t\gets t+1\)
10:endwhile
11:while\(s_{t}\)는 터미널 상태가 아님\(\wedge t\leq L\)do
12:for\(i\gets 1,\dots,d\)do\(\triangleright\) Expansion
13: 샘플 \(a_{t}^{(i)}\sim p_{\phi}(a\mid s_{t})\), \(s_{t+1}^{(i)}\sim p_{\theta}(s_{t},a_{t}^{(i)})\), \(r_{t}^{(i)}\sim r_{\theta}(s_{t},a_{t}^{(i)})\)
14: 업데이트 \(A(s_{t})\leftarrow\{a_{t}^{(i)}\}_{t=1}^{d}\), \(c(s_{t},a_{t}^{(i)})\gets s_{t+1}^{(i)}\), \(r(s_{t},a_{t})\gets r_{t}^{(i)}\)
15:endfor
16:\(a_{t+1}\leftarrow\arg\max_{a\in A(s_{t})}r(s_{t},a_{t})\(\triangleright\) Simulation
17:\(r_{t}\gets r(s_{t},a_{t})\), \(s_{t+1}\gets c(s_{t},a_{t})\)
18:\(t\gets t+1\)
19:endwhile
20:for\(t^{\prime}\gets t,\dots,0\)do\(\triangleright\) Back propagation
21: \(\{r_{t^{\prime}},r_{t^{\prime}+1},\dots,r_{t}\}\)로 \(Q(s_{t^{\prime}},a_{t^{\prime}})\) 업데이트
22:endfor
23:endfor
```

**Algorithm 1**RAP-MCTS

노란색 블록은 오렌지색 블록 위에 있고, 빨간색 블록은 테이블 위에 있고, 파란색 블록은 테이블 위에 있고 오렌지색 블록은 테이블 위에 있다. 제 목표는 주황색 블록이 파란색 블록 위에 있고 노란색 블록이 빨간색 블록 위에 있는 것입니다. 내 계획은 다음과 같다.

[PLAN] 황색 블록을 오렌지 블록 스택의 상부로부터 풀어서 적색 블록의 상부에 있는 황색 블록을 오렌지 블록 스택을 픽업하여 청색 블록의 상부에 있는 오렌지 블록을 픽업한다[PLAN END].

[STATEMENT] 초기 조건 I는 다음과 같습니다. <initial_state> 내 목표는 해당 <목표>를 갖는 것입니다. 내 계획은 다음과 같다.

[PLAN]

세계 모델을 사용한 다음 상태 예측을 위해 마지막 동작에 조건화된 프롬프트를 적용한다. 여기서는 "픽업" 작업 후 상태를 업데이트하라는 프롬프트를 예로 보여 줍니다. 다시, <상태> 및 <동작>은 현재 상태 및 동작으로 인스턴스화될 것이다. 블록을 스택으로 배열해야 하는 블록 세트를 가지고 놀고 있습니다. 제가 할 수 있는 행동은 다음과 같습니다.

한 블록 픽업 다른 블록 위에서 한 블록 쌓기 해제 한 블록 내려 한 블록 쌓기 다른 블록 위에 한 블록 쌓기 작업에는 다음과 같은 제한 사항이 있습니다. 한 번에 한 블록만 쌓거나 쌓을 수 있습니다. 손이 비어 있는 경우에만 한 블록을 줍거나 분리할 수 있습니다. 블록이 테이블 위에 있고 블록이 깨끗해야 블록을 찾을 수 있습니다. 블록 위에 다른 블록이 없고 블록이 픽업되지 않은 경우 블록은 명확하다. 내가 쌓고 있는 블록이 실제로 다른 블록 위에 있었다면, 나는 다른 블록 위에서 블록을 쌓을 수 있을 뿐이다. 나는 내가 쌓고 있는 블록이 명확하다면 다른 블록 위에서 한 블록만 쌓을 수 있다. 블록을 줍거나 펼치면 블록을 잡고 있습니다. 제가 들고 있는 블록만 내려놓을 수 있습니다. 쌓인 블록을 잡고 있는 경우에만 다른 블록 위에 블록을 쌓을 수 있습니다. 나는 블록을 쌓고 있는 블록이 명확하다면 다른 블록 위에만 블록을 쌓을 수 있다. 블록을 내려놓거나 쌓으면 손이 텅 비게 됩니다. 초기 상태 및 액션을 부여받은 후, 액션을 수행한 후 새로운 상태를 부여한다.

[SCENARIO 1]

[STATE 0] I have it, white block is clear, cyan block is clear, brown block is clear, hand is empty, white block is top of purple block, purple block is on the table, cyan block is on the table and brown block is on the table. [액션] 갈색 블록을 픽업한다. 손은 비어있었고 지금은 갈색 블록을 잡고 있고 갈색 블록은 탁자 위에 있었고 지금은 손에 있습니다. 갈색 블록은 더 이상 선명하지 않습니다. [STATE 1] I have it, white block is clear, cyan block is clear, brown block is in the hand, hand is holding the brown block, white block is top of purple block, purple block is on the table and cyan block is on the table.

[SCENARIO 2]

[STATE 0] I have that, purple block is clear, cyan block is clear, white block is clear, hand is empty, white block is top of brown block, purple block is on the table, cyan block is on the table and brown block is on the table.

[동작] 청록색 블록을 픽업한다.

손은 비어있었고 지금은 시안 블록을 잡고 있고, 시안 블록은 테이블 위에 있고 지금은 손에 있습니다. 그리고 시안 블록은 더 이상 선명하지 않습니다.

[STATE 1] 내가 가지고 있는 것은, 시안 블록은 손에 있고, 흰색 블록은 맑고, 보라색 블록은 맑고, 손은 시안 블록을 잡고 있고, 흰색 블록은 갈색 블록의 위에 있고, 보라색 블록은 테이블 위에 있고 갈색 블록은 테이블 위에 있다.

[SCENARIO 3]

[STATE 0] <state>

[ACTION] <action>

[CHANGE]

### Math Reasoning

우리는 수학 추론을 위한 RAP의 프롬프트를 아래와 같이 보여준다. 프롬프트는 액션 제안 및 다음 상태 예측 모두에 사용된다. <질문>을 인스턴스화한 후, 프리픽스 질문 5.1을 프롬프트에 추가함으로써, LLM으로 첫 번째 액션을 샘플링할 수 있다. 모든 이전 하위 질문 및 하위 답변을 컨텍스트 내 데모 형식에 따라 프롬프트에 추가해야 한다는 점을 제외하고는 향후 작업은 유사하게 샘플링됩니다. 다음 상태 예측, 즉 하위 질문에 답하는 것도 같은 방식으로 작동한다.

질문이 주어지면 하위 질문으로 분해해 주세요. 각 하위 질문에 대해 "답은"으로 끝나는 완전한 문장으로 답변해 주시기 바랍니다. 원래 질문에 답할 수 있는 경우 "이제 질문에 답할 수 있습니다."로 하위 질문을 시작하십시오.

질문 1: 4년 전, 코디는 모하메드보다 반밖에 안 늙었습니다. 만약 모하메드가 현재 30세의 두 배라면, 코디는 몇 살일까요?

질문 1.1: 모하메드는 몇 살인가요? 답변 1.1: 현재 30*2=60세이다. 답은 60입니다. 1.2번 질문: 4년 전 모하메드는 몇 살이었나요? 답변 1.2: 4년 전, 그는 60 - 4 = 56세였을 것입니다. 답은 56입니다. 1.3번 질문: 4년 전 코디는 몇 살이었나요? 대답 1.3: 코디는 4년 전 모하메드보다 나이가 반이나 많았다. 따라서, Kody는 56/2 = 28세였다. 답은 28입니다. 1.4번 질문: 이제 우리는 질문에 답할 수 있습니다. 코디는 몇 살일까요? 대답 1.4: 그녀는 현재 28 + 4 = 32세입니다. 대답은 32입니다. 2번 질문: 달이 없는 밤에, 세 마리의 반딧불이가 저녁바람에 춤을 췄습니다. 그들은 반딧불이 두 마리가 날아오기 전에 12마리 미만의 반딧불이 4마리가 더 합류했다. 반딧불이가 몇 마리 남았나요? 질문 2.1: 반딧불이가 몇 마리 합류했습니까? 답변 2.1: 반딧불이는 12-4 = 8개의 반딧불이인 12개 미만의 반딧불이 4개가 더 결합되었다. 답은 8입니다. 질문 2.2: 이제 우리는 질문에 답할 수 있습니다: 반딧불이가 몇 마리 남았나요? 대답 2.2: 반딧불이 세 마리가 원래 춤을 추고 있었습니다. 그들은 두 마리가 날아가기 전에 8마리의 반딧불이와 함께 있었다. 그래서 3 + 8 - 2 = 9가 남았다. 대답은 9입니다. 3번 질문: 알리는 10달러짜리 지폐 4장과 20달러짜리 지폐 6장을 가지고 있습니다. 그는 그의 농장에서 제임스 씨를 위해 일했습니다. 알리는 누나에게 자신이 가진 총 돈의 절반을 주고 남은 돈의 5분의 3을 저녁 식사에 사용한다. 그가 저녁을 산 후에 가지고 있는 돈을 계산해라.

질문 3.1: 알리는 총 얼마의 돈을 가지고 있나요?

대답 3.1: 알리는 4개의 10달러 지폐와 6개의 20달러 지폐를 가지고 있습니다. 그래서 그는 4*10 + 6*20 = 160달러를 가지고 있습니다. 답은 160입니다.

질문 3.2: 알리가 여동생에게 얼마를 주나요?

대답 3.2: 알리는 그가 가진 총 돈의 절반을 여동생에게 줍니다. 그래서 그는 여동생에게 160/2 = 80달러를 줍니다. 답은 80입니다.

질문 3.3: 알리가 여동생에게 돈을 준 후 얼마의 돈을 가지고 있는가?

대답 3.3: 그의 여동생에게 돈을 준 후, 알리는 160 - 80 = 80 달러가 남았습니다. 답은 80입니다.

질문 3.4: 알리가 저녁을 사기 위해 얼마나 많은 돈을 쓰나요?

대답 3.4: 알리는 저녁 식사를 사기 위해 남은 금액의 3/5을 사용합니다. 그래서 그는 저녁을 사기 위해 80*3/5 = 48달러를 사용합니다. 답은 48입니다

질문 3.5: 이제 우리는 질문에 답할 수 있습니다. 알리가 저녁을 산 후에 얼마의 돈을 가지고 있을까요?

대답 3.5: 저녁을 산 후, 알리는 80 - 48 = 32달러가 남았습니다. 대답은 32입니다.

4번 질문: 자동차가 많은 회전수로 터널을 통과하고 있다. 잠시 후, 차는 총 4번의 오른손 회전이 필요한 고리를 통과해야 합니다. 첫 번째 회전 후에는 5m를 이동합니다. 2회전 후에는 8미터를 이동합니다. 3회전이 끝나면 조금 더 이동하다가 4회전이 되면 바로 터널을 빠져나간다. 차량이 링을 총 23m 돌았다면 3회전 후 어디까지 이동해야 했을까.

4.1번 질문: 차가 3회전을 제외하고 얼마나 멀리 갔나요?

답변 4.1: 1회전이 끝난 후 5m, 2회전이 끝난 후 8m, 4회전이 끝난 후 0m를 이동한다. 총 5 + 8 + 0 = 13미터입니다. 답은 13입니다.

질문 4.2: 이제 우리는 질문에 답할 수 있습니다. 3회전 후에 차가 얼마나 멀리 가야 했는가?

정답 4.2: 그 차는 링을 총 23미터 돌았습니다. 그것은 3회전을 제외하고 13미터를 여행한다. 그래서 그것은 3회전 후에 23 - 13 = 10미터를 여행해야 한다. 정답은 10입니다.

질문 5: <질문>

### Logical Reasoning

우리는 행동 제안, 행동 가능성 계산, 그리고 다음 상태 예측에 대한 프롬프트를 보여준다. <fact> 및 <query>는 문제와 함께 인스턴스화될 것이다.

사실 목록과 현재 주장이 주어지면 다음 단계로 하나의 가능한 사실을 출력한다. 정확한 문장을 사실로 꼭 복사해 주세요. 문구를 변경하지 마십시오. 자신의 단어를 만들지 마십시오.

사실 1: 나비목 각각은 곤충입니다. 각 절지동물은 원형동물입니다. 모든 동물은 다세포이다. 원추체는 무척추동물입니다. 각각의 고래는 뼈가 있다. 각각의 그림을 그린 숙녀는 나비이다. 무척추동물은 동물입니다. 나비는 나비목이다. 각각의 곤충들은 6개의 다리를 가지고 있습니다. 모든 곤충은 절지동물이다. 절지동물은 뼈가 아니다.

쿼리 1: 참 또는 거짓: 샐리는 뼈가 아닙니다.

청구항 1.1: 샐리는 곤충이다. 다음 1.1: 각 곤충은 6족입니다. 청구항 1.2: 샐리는 나비이다. 다음 1.2: 나비는 나비목입니다. 청구항 1.3: 샐리는 나비목이다. 다음 1.3: 나비목 각각은 곤충입니다.

청구항 1.4: 샐리는 뼈가 아니다. 다음 1.4: 마무리. 청구항 1.5: 샐리는 절지동물이다. 다음 1.5: 절지동물은 뼈가 아닙니다. 청구항 1.6: 샐리는 페인트칠한 숙녀이다. 다음 1.6: 각 페인트칠된 숙녀는 나비입니다.

사실 2: 소수들은 자연수입니다. 모든 메르센 프라임은 합성물이 아니다. 상상수는 실수가 아니다. 모든 실수는 숫자이다. 자연수는 정수입니다. 모든 실수는 진짜다. 모든 메르센 소수는 소수이다. 자연수는 양수입니다. 소수는 합성수가 아닙니다. 정수기는 실수입니다.

쿼리 2: 참 또는 거짓: 127은 진짜가 아닙니다. 청구항 2.1: 127은 실제이다. 다음 2.1: 마무리. 청구항 2.1:127은 자연수이다. 다음 2.1: 자연수는 정수이다. 청구항 2.2: 127은 소수이다. 다음 2.2: 소수들은 자연수입니다. 청구항 2.3: 127은 실수이다. 다음 2.3: 모든 실수는 실수입니다. 청구항 2.4: 127은 메르센 프라임이다. 다음 2.4: 모든 메르센 소수는 소수입니다. 청구항 2.5: 127은 정수이다. 다음 2.5: 정수기는 실수입니다.

사실 3: 나비목들은 곤충입니다. 모든 동물은 다세포이다. 각 곤충은 절지동물입니다. 각각의 무척추 동물은 동물입니다. 곤충은 다리가 여섯 개이다. 절지동물은 작다. 절지동물은 무척추동물이다. 각각의 나비는 나비목입니다. 고래는 작지 않다. 쿼리 3: 참 또는 거짓: 폴리는 작지 않습니다. 청구항 3.1: 폴리는 절지동물이다. 다음 3.1: 절지동물은 작습니다. 청구항 3.2: 폴리는 곤충이다. 다음 3.2: 각 곤충은 절지동물입니다. 청구항 3.3: 폴리는 작다. 다음 3.3: 마무리. 청구항 3.4: 폴리는 나비목이다. 다음 3.4: 나비목은 곤충입니다.

사실 4: 모든 고양이는 고양이이다. 포유류는 척추동물입니다. 쌍방인은 동물이다. 척추동물은 척삭동물이다. 육식동물은 포유류입니다. 포유류는 냉혈한이 아니다. 각각의 척삭은 쌍둥이다. 모든 고양이는 육식동물이다. 뱀은 냉혈한이다. 동물들은 단세포가 아니다. 모든 육식동물은 초식동물이 아니다. 쿼리 4: 참 또는 거짓: 페이는 냉혈하지 않습니다. 청구항 4.1: 페이는 고양이이다. 다음 4.1: 모든 고양이는 육식동물이다. 청구항 4.2: 페이는 냉혈한이 아니다. 다음 4.2: 마무리. 청구항 4.2: 페이는 포유동물이다. 다음 4.2: 포유류는 냉혈동물이 아닙니다. 청구항 4.3: 페이는 고양이이다. 다음 4.3: 모든 고양이는 고양이이다. 청구항 4.4: 페이는 육식동물이다. 다음 4.4: 육식동물은 포유류입니다.

사실 5: 소수들은 소수입니다. 실수는 숫자입니다. 모든 정수는 실수이다. 실수는 허수가 아니다. 메르센 소수는 소수이다. 복소수는 허수이다. 각 소수는 자연수이다. 자연수는 양수입니다. 각각의 메르센 프라임은 프라임이다. 각 자연수는 정수이다. 쿼리 5: True 또는 false: 7은 허수입니다. 청구항 5.1:7은 상상이 아니다. 다음 5.1: 마무리. 청구항 5.1:7은 자연수이다. 다음 5.1: 각 자연수는 정수이다. 청구항 5.2: 7은 소수이다. 다음 5.2: 각 소수는 자연수이다. 청구항 5.3:7은 실수이다. 다음 5.3: 실수는 허수가 아니다. 청구항 5.4:7은 정수이다. 다음 5.4: 모든 정수는 실수이다.

사실 6: 거미는 다리가 6개가 아니야.

곤충은 다리가 여섯 개이다. 곤충은 절지동물이다. 모든 동물은 단세포가 아니다. 무척추동물은 동물입니다. 나비목들은 곤충입니다.

모든 절지동물은 분할되어 있습니다.

절지동물은 무척추동물이다. 모든 나비는 나비목이다. 스텔라는 나비야

쿼리 6: True 또는 false: Stella는 다리가 6개입니다.

청구항 6.1: 스텔라는 곤충이다.

다음 6.1: 곤충은 다리가 여섯 개입니다.

청구항 6.2: 스텔라는 나비목이다.

다음 6.2: 나비목은 곤충입니다.

청구항 6.3: 스텔라는 나비이다.

다음 6.3: 모든 나비는 나비목입니다.

청구항 6.4: 스텔라는 6족이다.

다음 6.4: 마무리.

Facts 7: <fact>

Query 7: <query>

## 부록 D 관련 작업: 세계 모델 및 계획

최근 몇 년 동안, AlphaZero(Silver et al., 2017), MuZero(Schrittwieser et al., 2020)와 같은 계획 알고리즘의 성공적인 응용이 목격되었다. 이러한 알고리즘은 일반적으로 트리 구조 탐색을 기반으로 하며 탐색과 착취의 균형을 효과적으로 유지하도록 설계되었다. 전환 역학에 대한 지식은 계획의 전제 조건이며, 최근 모델 기반 강화 학습에 대한 연구는 정책 학습을 계획하거나 보조하기 위해 세계 모델(또는 역학 모델)을 학습하는 것을 제안한다. 표본 효율성 향상을 위해 선행 연구에서는 오프라인 궤적으로부터 세계 모형을 학습하고, 세계 모형 내에서 정책을 직접 학습하고자 한다(Ha and Schmidhuber, 2018, 2018). 세계 모델에서 잠재된 상상력을 가지고, RL 에이전트들은 롱-호라이즌 태스크들을 풀도록 훈련될 수 있다(Hafner et al., 2019, 2020). 또한, 세계 모델은 물리적 로봇 학습에도 도움이 되는 것으로 나타났다(Wu 등, 2023). 본 논문에서는 LLM을 세계 모델로 사용하고, 추론 경로를 탐색하기 위해 계획 알고리즘을 적용한다. 이는 모델 예측 제어(Camacho and Alba, 2013)와 정신적으로 유사하다. 기존 연구들과 비교할 때, 제안하는 프레임워크는 일반적인 LLM을 세계 모델로 사용하며 광범위한 오픈 도메인 추론 작업에 적용할 수 있다. Xiang 등(2023)은 체화된 경험을 얻기 위해 외부 세계 모델과 함께 LLM들을 트레이닝하는 것을 제안하는 반면, RAP는 추론 단계에 초점을 맞추고 임의의 트레이닝 방법들과 호환가능하다.

## Appendix E Adaptive Prompting

예비 실험을 통해 시연 사례와 테스트 사례 간의 난이도의 불일치에 의해 LLM의 성능이 영향을 받는 것을 관찰했다. RAP의 경우, 새로운 상태가 예측될 때, 우리는 예측된 새로운 상태로 초기화된, 남은 태스크를 새로운 테스트 케이스로 재구성한다. 이 새로운 테스트 케이스는 더 적은 최소 수의 조치를 요구하여 시연 케이스와 새로운 케이스의 분포에 불균형을 초래할 것이다. 이 문제를 완화하기 위해 시연 사례의 중간 상태를 미리 계산합니다. 추론하는 동안, 우리는 반복에서 각각의 새로운 상태에 대한 추적을 처음부터 잘라내어, 탐색 트리가 깊어짐에 따라 시연 사례의 최소 액션 수를 줄인다. 이 기술은 특히 분포 불일치에 더 취약한 더 복잡한 문제에 대해 RAP의 성능을 크게 향상시킨다.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline \(R_{1}\) & \(R_{2}\) & \(R_{3}\) & RAP\({}^{(1)}\) & RAP\({}^{(10)}\) & +aggr \\ \hline ✓ & ✗ & ✓ & 0.410 & 0.450 & 0.503 \\ ✓ & ✗ & ✗ & 0.350 & 0.447 & 0.490 \\ ✓ & ✓ & ✗ & 0.373 & 0.423 & 0.443 \\ \hline \hline \end{tabular}
\end{table}
표 6: GSM8k에 대한 절제 연구(처음 300예). \ (R_{1}\)은 상태 전이 신뢰 보상, \(R_{2}\)은 행동 가능성 보상, \(R_{3}\)은 자기 평가 보상이다.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline \(R_{1}\) & \(R_{2}\) & \(R_{3}\) & Success \\ \hline ✓ & ✓ & ✗ & 0.88 \\ ✓ & ✓ & ✓ & 0.91 \\ ✓ & ✗ & ✗ & 0.46 \\ ✗ & ✓ & ✗ & 0.21 \\ ✗ & ✗ & ✓ & 0.14 \\ ✗ & ✗ & ✗ & 0.02 \\ \hline \hline \end{tabular}
\end{table}
표 5: Blockworld에 대한 절제 연구. \ (R_{1}\)은 행동 가능성 보상, \(R_{2}\)은 과제별 보상, \(R_{3}\)은 자기 평가 보상이다.

Reward Choice

**결과.** 계획 생성(표 5) 및 수학 추론(표 6)에 대한 보상에 대한 포괄적인 실험을 수행합니다. 두 표 모두에서 첫 번째 행은 주요 실험에서 사용하는 설정을 나타낸다. 표 5에 도시된 바와 같이, 액션 가능성과 태스크-특정 보상의 조합(행 1)은 단일 보상 기준선(행 3, 4, 5)을 상당히 능가할 수 있다. 흥미롭게도 자기평가 보상을 추가하면 성과를 조금 더 향상시킬 수 있다(2행). 또한 표 6에 표시된 GSM8k의 처음 300개 샘플에 대한 결과처럼, 우리는 신뢰 보상(행 2) 위에 행동 가능성(행 3) 또는 자체 평가(행 1)를 추가하면 한 번의 반복으로 신뢰 보상(행 1)만 사용하는 RAP 성능을 높일 수 있지만 행동 가능성 보상은 더 많은 반복으로 정확도를 하향 조정한다는 것을 알 수 있다. 자기평가 보상은 전반적으로 최고의 성과로 이어진다. 이는 탐구 이전에 효과적이고 계산적으로 효율적인 추론 지도에서 자기 평가 보상의 중요성을 나타낸다.

**자기 평가 및 행동 가능성** 자기 평가 및 행동 가능성의 보상은 다양한 추론 작업에 적용할 수 있으므로 특히 중요합니다. 일반적으로, 다른 보상과의 최상의 사용 및 조합은 태스크 특성에 대한 경험적 설계 및 이해를 필요로 하며, 그 효과는 상이한 태스크에 걸쳐 상당히 달라질 수 있다. 여기서는 보상 선택 뒤에 몇 가지 직관을 제공 합니다.

(a) 하나의 추론 단계가 짧고 구조화된 문제에 대해, 행동 가능성은 매우 나타낼 수 있다. 그렇지 않으면 중요하지 않은 토큰에 의해 방해되어 신뢰할 수 없게 될 수 있습니다. 예를 들어, 블록월드 도메인 내의 단일 단계는 일반적으로 특정 패턴(예를 들어, 블록 픽/풋/스택...)을 고수하여 액션 가능성을 나타낸다. 그러나 수학 영역에서 추론 단계는 자연어 문장으로 표현되어 더 큰 자유를 허용하고 잠재적으로 잡음을 도입한다.

(b) 몇 가지 오류를 생성 중에 회피하는 것보다 나중에 인식하는 것이 더 쉬운 문제에 대해서는 자기 평가가 추론 정확도를 높이는 데 유용한 메커니즘으로 등장한다. 수학적 추론에서 LLM은 처음부터 올바른 추론 단계를 생성하는 데 어려움을 겪을 수 있지만 계산 또는 논리 오류 검출이 더 실현 가능하다. 그러나 블록월드에서 후보 액션의 품질을 평가하는 것은 간단하지 않으며 여전히 다단계 추론이 필요하다. 이 특성은 자기 평가 보상의 정확성을 감소시키며, 특히 가능성이 이미 검색에 대한 좋은 직관을 제공한다는 점을 고려할 때 덜 도움이 된다.

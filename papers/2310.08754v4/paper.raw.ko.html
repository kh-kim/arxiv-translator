<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Tokenizer Choice For LLM Training: Negligible or Crucial?</title>
<!--Generated on Sun Mar 17 14:15:31 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2310.08754v4/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2310.08754v4">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2310.08754v4">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2310.08754v4" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S1" title="1 Introduction ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2" title="2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1" title="2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Tokenization Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1.SSS0.Px1" title="Word Tokenization. ‣ 2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Word Tokenization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1.SSS0.Px2" title="Subword tokenization. ‣ 2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Subword tokenization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1.SSS0.Px3" title="Character Tokenization. ‣ 2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Character Tokenization.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS2" title="2.2 Tokenizers in Transformers Models ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tokenizers in Transformers Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS2.SSS0.Px1" title="Tokenizers in Encoder Models ‣ 2.2 Tokenizers in Transformers Models ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Tokenizers in Encoder Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS2.SSS0.Px2" title="Tokenizers in Decoder Models ‣ 2.2 Tokenizers in Transformers Models ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Tokenizers in Decoder Models</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3" title="3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS1" title="3.1 Data ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS2" title="3.2 Tokenizer ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS3" title="3.3 Models ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS4" title="3.4 Evaluation ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4" title="4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Intrinsic Tokenizer Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS1" title="4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Fertility &amp; Parity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Vocabulary Overlap</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5" title="5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Extrinsic Tokenizer Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2" title="5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Downstream Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS0.Px1" title="Monolingual Tokenizer ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Monolingual Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS0.Px2" title="Multilingual Tokenizer ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Multilingual Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS1" title="5.2.1 Impact of the Tokenizer Library ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Impact of the Tokenizer Library</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS2" title="5.2.2 Impact of the Tokenizer Algorithm ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Impact of the Tokenizer Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS3" title="5.2.3 Impact of the Tokenizer Vocabulary ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span>Impact of the Tokenizer Vocabulary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Computational Costs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S6" title="6 Correlation Between Intrinsic And Extrinsic Tokenizer Performance ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Correlation Between Intrinsic And Extrinsic Tokenizer Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S7" title="7 Conclusion &amp; Future Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion &amp; Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S8" title="8 Limitations ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S9" title="9 Ethical And Broader Impact ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Ethical And Broader Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A1" title="Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Corpora</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2" title="Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3" title="Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>LLM Architecture and Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4" title="Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Intrinsic Tokenizer Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.SS1" title="D.1 Computational Costs Per Word During Training ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Computational Costs Per Word During Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A5" title="Appendix E Infrastructure &amp; Computational Costs ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Infrastructure &amp; Computational Costs</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC Zero</div><div id="watermark-tr">arXiv:2310.08754v4 [cs.LG] 17 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Tokenizer Choice For LLM Training: Negligible or Crucial?</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mehdi Ali<sup class="ltx_sup" id="id4.4.id1">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Michael Fromm<sup class="ltx_sup" id="id5.5.id2">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><ci id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Klaudia Thellmann<sup class="ltx_sup" id="id6.6.id3">3</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mo id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Richard Rutmann<sup class="ltx_sup" id="id7.7.id4">1,2</sup>, Max Lübbering<sup class="ltx_sup" id="id8.8.id5">1,2</sup>, Johannes Leveling<sup class="ltx_sup" id="id9.9.id6">1</sup>, Katrin Klug<sup class="ltx_sup" id="id10.10.id7">1</sup>, Jan Ebert<sup class="ltx_sup" id="id11.11.id8">4</sup>, 
<br class="ltx_break">Niclas Doll<sup class="ltx_sup" id="id12.12.id9">1</sup>, Jasper Schulze Buschhoff<sup class="ltx_sup" id="id13.13.id10">1</sup>, Charvi Jain<sup class="ltx_sup" id="id14.14.id11">1,2</sup>, Alexander Arno Weber<sup class="ltx_sup" id="id15.15.id12">1,2</sup>, 
<br class="ltx_break">Lena Jurkschat<sup class="ltx_sup" id="id16.16.id13">3</sup>, Hammam Abdelwahab<sup class="ltx_sup" id="id17.17.id14">1</sup>
Chelsea John<sup class="ltx_sup" id="id18.18.id15">4</sup>, Pedro Ortiz Suarez<sup class="ltx_sup" id="id19.19.id16">5</sup>, Malte Ostendorff<sup class="ltx_sup" id="id20.20.id17">5</sup>
<br class="ltx_break">Samuel Weinbach<sup class="ltx_sup" id="id21.21.id18">6</sup>, Rafet Sifa<sup class="ltx_sup" id="id22.22.id19">1</sup>, Stefan Kesselheim<sup class="ltx_sup" id="id23.23.id20">4</sup>, Nicolas Flores-Herr<sup class="ltx_sup" id="id24.24.id21">1</sup>
<br class="ltx_break">
<br class="ltx_break"><sup class="ltx_sup" id="id25.25.id22">1</sup>Fraunhofer IAIS, <sup class="ltx_sup" id="id26.26.id23">2</sup>Lamarr Institute, <sup class="ltx_sup" id="id27.27.id24">3</sup>TU-Dresden, <sup class="ltx_sup" id="id28.28.id25">4</sup>FZ Jülich, <sup class="ltx_sup" id="id29.29.id26">5</sup>DFKI, <sup class="ltx_sup" id="id30.30.id27">6</sup>Aleph Alpha
<span class="ltx_note ltx_role_thanks" id="id31.31.id28"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>†Equal contribution.</span></span></span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id32.id1"><span class="ltx_glossaryref" title="">Large Language Models (LLMs)</span>의 최근 성공은 훈련 데이터세트 구성의 큐레이팅, 모델 아키텍처 및 데이터세트 크기의 스케일링 및 사전 훈련 목표의 진보에 의해 주로 주도되어 토큰나이저의 영향을 맹점으로 남긴다. 이 미처리 영역에 대한 빛을 무시하고 2.6 B 매개변수 규모에서 24개의 단일 및 다국어 LLM을 훈련하고 다양한 토큰izer 알고리즘 및 매개변수화를 제거함으로써 토큰izer 선택이 LLM 다운스트림 성능에 미치는 영향에 대한 포괄적인 연구를 수행한다. 우리의 연구는 토큰라이저 선택이 모델의 다운스트림 성능 및 훈련 비용에 상당한 영향을 미칠 수 있음을 강조한다. 특히, 일반적인 토큰나이저 평가 메트릭 <span class="ltx_text ltx_font_italic" id="id32.id1.1">fertility</span> 및 <span class="ltx_text ltx_font_italic" id="id32.id1.2">parity</span>이 항상 모델 다운스트림 성능을 예측하는 것은 아니므로 이러한 메트릭을 모델의 다운스트림 성능에 대한 의심스러운 프록시로 렌더링합니다. 또한, 5개의 가장 빈번한 유럽 언어에 대해 훈련된 다국어 토큰라이저는 영어에 비해 3인자의 어휘 크기 증가가 필요하다는 것을 보여준다. 영어 중심 토큰라이저는 과거 다국어 <span class="ltx_glossaryref" title="">LLMs</span>의 훈련에 적용되었지만, 이 방법은 비효율적인 토큰화 어휘로 인해 최대 68%의 심각한 다운스트림 성능 저하 및 추가 훈련 비용을 초래한다는 것을 발견했다.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Tokenizer Choice For LLM Training: Negligible or Crucial?</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.3"><span class="ltx_text" id="p2.3.3" style="width:433.6pt;"><span class="ltx_tabular ltx_align_top" id="p2.3.3.3.3"><span class="ltx_tbody"><span class="ltx_tr" id="p2.3.3.3.3.3.3.3.3.3.3"><span class="ltx_sup" id="p2.3.3.3.3.2">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="p2.1.1.1.1.1.1.1.m1.1"><semantics id="p2.1.1.1.1.1.1.1.m1.1a"><msup id="p2.1.1.1.1.1.1.1.m1.1.1" xref="p2.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="p2.1.1.1.1.1.1.1.m1.1.1a" xref="p2.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="p2.1.1.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="p2.1.1.1.1.1.1.1.m1.1b"><apply id="p2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p2.1.1.1.1.1.1.1.m1.1.1"><ci id="p2.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="p2.1.1.1.1.1.1.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.1.1.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="p2.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Michael Fromm<sup class="ltx_sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.2">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="p2.2.2.2.2.2.2.2.m2.1"><semantics id="p2.2.2.2.2.2.2.2.m2.1a"><msup id="p2.2.2.2.2.2.2.2.m2.1.1" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p2.2.2.2.2.2.2.2.m2.1.1a" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mo id="p2.2.2.2.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="p2.2.2.2.2.2.2.2.m2.1b"><apply id="p2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1"><ci id="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.2.2.2.2.2.2.2.m2.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="p2.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math> <span></span> <span class="ltx_sup" id="p2.3.3.3.3.3.3.2">1,2</sup> </span></span></span></span></span></span></span></span></span><//span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_glossaryref" title="">LLMs</span>은 요약, 읽기 이해, 번역 및 상식 추론 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib5" title="">2020b</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib55" title="">2023</a>)</cite>와 같은 제로/페우 샷 설정에서 많은 다운스트림 태스크에서 인상적인 능력을 보여주었다. LLM을 학습하기 위해 현재 확립된 접근법은 토큰이 단어 <cite class="ltx_cite ltx_citemacro_cite">Bengio et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib2" title="">2000</a>)</cite>, 서브워드 <cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib47" title="">2012</a>); Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib48" title="">2015</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib58" title="">2020</a>)</cite>, 또는 단일 문자 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib18" title="">2020b</a>)</cite>를 나타내는 토큰으로 학습 문서를 분할하는 토큰화기를 사용하는 것이며, 각 토큰은 추가로 처리될 수 있는 임베딩 벡터에 의해 모델에서 표현된다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">토큰라이저의 품질은 <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">intrinsically</span> 및 <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">extrinsically</span>을 평가할 수 있습니다. 내재적 평가는 단독으로 토큰라이저의 특성과 생성된 출력을 다루는 반면, 외재적 평가는 다운스트림 구성 요소(예: <span class="ltx_glossaryref" title="">Large Language Model (LLM)</span>)에 대한 토큰라이저의 영향을 측정합니다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">문자 기반에서 단어 기반 방법에 이르기까지 많은 다른 토큰화 접근법이 제안되었지만, 다른 토큰화기의 잠재적 영향은 다중 언어 <span class="ltx_glossaryref" 제목="">LLMs</span>, 특히 다중 언어 <span class="ltx_glossaryref" 제목="">LLMs</span>의 맥락에서 과소 해석된 w.r.t. <span class="ltx_glossaryref" 제목="">LLMs</span>이다. <cite class="ltx_cite ltx_citemacro_citet">Petrov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib38" title="">2023</a>)</cite>가 제안한 최근 연구는 다국어 <span class="ltx_glossaryref" title="">LLMs</span>의 훈련에 적용된 부주의하게 설계된 tokenizers가 언어 전반에 걸쳐 심각한 부등식과 한계를 초래한다는 것을 보여준다. 서로 다른 언어로 번역된 텍스트 지문은 길이가 최대 15배까지 다른 토큰화된 서열을 생성하여 추론 비용과 추론 중 지연에 영향을 미쳤다. 더 나아가, 장거리 의존성 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib56" title="">2017</a>)</cite>의 학습은 트랜스포머 기반의 <span class="ltx_glossaryref" title="">LLMs</span>을 효과적으로 학습하기 위한 필수 속성인 것으로 알려져 있다. 고정된 시퀀스 길이가 주어지면, 텍스트가 토나이저에 의해 과도하게 단편화된 언어들에 대해 입력 텍스트에서 멀리 떨어진 단어들을 연관시키는 학습은 불가능하다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">토큰라이저의 중요성과 제대로 수행되지 않는 토큰라이저의 잠재적으로 심각한 영향에도 불구하고, 현재 <span class="ltx_glossaryref" 제목="">LLMs</span>의 백본을 나타내는 디코더 전용 모델에 초점을 맞추어 단일 언어 및 다국어 설정에서 고유 및 외부 토큰라이저 성능을 전체적으로 조사하는 광범위한 연구는 아직 없다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">이 작업에서는 이러한 격차를 해결하고 토큰화기가 모델 성능에 미치는 영향을 측정하는 광범위한 연구를 수행한다. 특히 다음과 같은 공헌을 합니다.</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">토나이저 고유 성능을 조사하는 연구를 수행한다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">외부 토큰나이저 성능, 즉 토큰나이저가 모델의 다운스트림 성능에 미치는 영향을 조사하는 연구를 수행한다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">내재적 토크나이저 성능과 외재적 토크나이저 성능 사이의 상관관계가 존재하는지 여부를 조사한다.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">이 섹션에서는 인코더 및 디코더 전용 변압기 모델에서 토큰화 알고리즘과 그 사용에 대한 개요를 제공한다.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Tokenization Approaches</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Word Tokenization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">가장 기본적인 토큰화 접근법은 화이트 스페이스를 기반으로 시퀀스를 분할하고 각 단어를 토큰 <cite class="ltx_cite ltx_citemacro_cite">Bengio et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib2" title="">2000</a>)</cite>로 고려하는 것이다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Subword tokenization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">이 클래스 알고리즘은 단어를 하위 단어/다중 토큰으로 분해할 수 있는 모든 데이터 기반 토큰화 접근법을 포섭하고 현재 <span class="ltx_glossaryref" 제목="">LLMs</span> rely <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>); Petrov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib38" title="">2023</a>)</cite>를 기반으로 하는 확립된 토큰화 접근법을 나타낸다. 서브워드 토큰라이저는 단어를 서브워드로 분해하기 때문에 어휘<cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>)</cite>에서 서브워드를 병합하여 어휘 외 단어를 처리할 수 있다. 인기 서브워드 토큰라이저의 예로는 WordPiece <cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib47" title="">2012</a>)</cite>, BPE <cite class="ltx_cite ltx_citemacro_cite">Gage (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib16" title="">1994</a>); Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib48" title="">2015</a>)</cite>, <span class="ltx_glossaryref" title="">Byte-Level BPE(BBPE)</span> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib58" title="">2020</a>)</cite>, Unigram <cite class="ltx_cite ltx_citemacro_cite">Kudo (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib29" title="">2018</a>)</cite> 등이 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Character Tokenization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">토큰화는 또한 문자 레벨에서 또는 UTF-8 바이트에 기초하여 수행될 수 있다. 그러나 이는 트랜스포머 아키텍처에서 계산적으로 비용이 많이 드는 증가된 시퀀스 길이를 초래하며, 시퀀스 길이 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib56" title="">2017</a>)</cite>에서 셀프-어텐션 계층의 2차 복잡성으로 인해 <span class="ltx_glossaryref" 제목="">LLMs</span>에 대한 현재 우세한 아키텍처가 된다. 그러나, 이러한 한계 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib18" title="">2020b</a>); Tay et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib52" title="">2021</a>); Xue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib60" title="">2022</a>); Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib8" title="">2022</a>); Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib63" title="">2023</a>)</cite>를 해결하기 위해 몇 가지 접근법이 제안되었다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tokenizers in Transformers Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Encoder Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">토큰화에 대한 대부분의 연구는 인코더 모델에 대해 수행되었다. <cite class="ltx_cite ltx_citemacro_citet">Rust et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib43" title="">2021</a>)</cite>는 토큰izer 선택이 다중 및 단일 언어 BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib13" title="">2018</a>)</cite> 모델의 다운스트림 성능에 영향을 미치는지 조사했다. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib65" title="">2022</a>)</cite>는 토큰나이저 트레이닝 동안 언어가 동일하게 샘플링될 때 더 나은 기계 번역 성능이 종종 얻어짐을 보여주었다. <cite class="ltx_cite ltx_citemacro_citet">Toraman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib53" title="">2023</a>)</cite>는 터키어를 위한 여러 중간 크기의 언어 모델을 훈련시켰고 서로 다른 서브워드 토큰라이저는 대략적으로 동등한 성능을 보이는 반면 단어 및 문자 수준 토큰라이저는 다운스트림 태스크에서 훨씬 더 나쁜 성능을 보인다고 제안했다. 마지막으로, <cite class="ltx_cite ltx_citemacro_cite">Chirkova and Troshin (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib6" title="">2022</a>)</cite>는 코드 관련 작업에 대한 다양한 토큰화를 사용하는 효과를 분석했으며, 신중하게 구성된 토큰화기가 평균 시퀀스 길이를 최대 40%까지 줄이거나 낮은 압축률에서 최대 2%까지 작은 다운스트림 성능 개선을 허용할 수 있음을 입증했다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Decoder Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">현재 단일 및 다국어 <span class="ltx_glossaryref" 제목="">LLMs</span>에 대한 개요는 <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib33" title="">2022</a>); Shliazhko et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib49" title="">2022</a>); Scao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib45" title="">2022</a>)</cite>에 제공됩니다. <cite class="ltx_cite ltx_citemacro_citet">Stollenwerk (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib51" title="">2023</a>)</cite>는 북유럽 언어에 초점을 맞춘 GPT-SW3 <cite class="ltx_cite ltx_citemacro_cite">Ekgren et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib15" title="">2023</a>)</cite> tokenizer의 고유 메트릭을 평가했다. 그 작업의 일환으로 <cite class="ltx_cite ltx_citemacro_citet">Shliazhko et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib49" title="">2022</a>)</cite>는 토큰나이저 알고리즘, 어휘 크기 및 사용된 구현은 고정된 상태로 유지하면서 서로 다른 토큰나이저 전처리 방법을 제거했다. 다른 주요 <span class="ltx_glossaryref" 제목="">LLM</span> 출판물에서 extrinsic tokenizer 성능이 연구되었습니다.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">토나이저가 모델 성능에 미치는 영향을 조사하기 위해 광범위한 절제 연구를 수행했다. 구체적으로, 토큰라이저와 모델의 학습을 위한 전용 데이터셋을 생성하고, BPE와 Unigram 토큰라이저를 학습시켰으며, 각 토큰라이저에 대해 나머지 구성(데이터세트 및 모델 하이퍼파라미터)을 고정한 상태에서 2.6B 파라미터 크기의 디코더 전용 모델을 학습시켰다. 이를 통해 토큰화기가 모델의 다운스트림 성능에 미치는 영향을 개별적으로 측정할 수 있었다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">토나이저와 모델 학습 데이터 세트를 만드는 동안 데이터 도메인(위키피디아, 책, 웹 텍스트)의 혼합 비율이 동일한 분포를 따르도록 하여 토나이저 학습과 모델 학습 간의 도메인 이동을 방지합니다. 우리는 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">2개의 데이터세트</em>을 만들었습니다. 여기서 데이터세트 중 하나는 영어 문서를 포함하는 단일 언어이고 두 번째는 영어, 독일어, 프랑스어, 이탈리아어 및 스페인어 문서로 구성된 다국어 데이터세트입니다. 우리의 데이터 세트는 필터링되고 중복 제거되며 웹 크롤링된 데이터(80%) 및 큐레이트된 데이터(20%)로 구성되며 <span class="ltx_glossaryref" 제목="">LLMs</span>을 훈련하는 데 사용되는 관련 데이터 세트에 필적한다. 다국어 데이터 세트에서 웹 크롤링된 데이터의 양은 단어 수 측면에서 언어 간에 균등하게 분포된다. 데이터 파이프라인 및 데이터 구성에 대한 자세한 내용은 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A1.T7" title="Table 7 ‣ Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>에 설명되어 있습니다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tokenizer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">우리의 연구는 두 가지 확립된 토큰화 알고리즘인 BPE와 Unigram에 의존하며, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Huggingface tokenizer</span> 라이브러리 <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib35" title="">2023</a>)</cite>와 <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">SentencePiece</span> 라이브러리 <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>)</cite>에서 구현된다. 사전 및 사후 처리 단계의 차이와 구현의 잠재적 차이에 대한 영향을 조사하기 위해 두 라이브러리를 모두 고려했다. Huggingface의 Unigram 구현에 대한 사전 처리 옵션이 누락되어 SentencePiece의 Unigram 구현과 비교하여 결과 어휘에 큰 불일치가 발생하므로 Huggingface 기반 Unigram 토큰라이저의 교육을 생략했다. 전반적으로 24개의 서로 다른 토큰라이저를 훈련했는데, 여기서 토큰라이저의 절반은 단일 언어 영어 토큰라이저이고 나머지 절반은 다국어 토큰라이저이다. 토나이저 알고리즘, 언어 구성 및 사용된 토나이저 라이브러리 외에도 어휘 크기를 변경했다. 구체적인 토큰izer 구성은 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2" title="Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>에 설명되어 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">훈련된 토큰라이저가 모델 다운스트림 성능에 미치는 영향을 측정하기 위해 각 토큰라이저에 대해 하나의 모델을 훈련했다. 특히, 24개의 훈련된 토큰아이저 각각에 대해, <cite class="ltx_cite ltx_citemacro_cite">Hoffmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib24" title="">2022a</a>)</cite>에서 제안한 스케일링 법칙에 따라 최대 52B 토큰에 대해 2.6B 트랜스포머 기반 디코더 전용 모델을 훈련시켰다. 또한 기준선 역할을 하는 GPT-2 tokenizer <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib41" title="">2018</a>)</cite>를 사용하여 단일 언어 및 다중 언어 모델을 학습했다. 모든 모델은 인과 언어 모델링 훈련 목표에 기초하여 훈련되었다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">토나이저가 모델 다운스트림 성능에 미치는 영향을 평가하기 위해 먼저 고유 토나이저 평가를 수행한 다음 외부 평가를 수행하고 마지막으로 두 평가 접근법 간의 상관 관계가 있는지 조사했다.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">진성 평가는 <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.1">fertility</span> 및 <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.2">parity</span>을 기반으로 토큰라이저의 생성된 출력을 평가하는 것을 목표로 합니다. 또한, 토큰화기의 어휘가 다른 토큰화기와 중복되는 것이 계산된다. 내재적 평가는 토큰라이저가 모델 성능에 미치는 영향을 평가하지 않습니다.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.5">토큰나이저의 성능 <cite class="ltx_cite ltx_citemacro_cite">Scao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib45" title="">2022</a>); Stollenwerk (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib51" title="">2023</a>); Rust et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib43" title="">2021</a>)</cite>를 평가하는 가장 일반적인 메트릭인 Fertility는 단어 또는 문서를 나타내는 데 필요한 토큰의 평균 수로 정의된다. 토큰나이저 <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_T</annotation></semantics></math> 및 데이터셋 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_A</annotation></semantics></math>의 경우, 비옥도는 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_A</annotation></semantics></math>(<math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.1"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.4.m4.1d">italic_T</annotation></semantics></math>가 적용된 경우)의 토큰 수를 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.5.m5.1d">italic_A</annotation></semantics></math>의 단어 수로 나눈 값으로 계산될 수 있다. 토나이저 교육에 사용되지 않은 보류된 세트(10,000개 문서)에서 생식력을 계산한다. 문서의 단어를 계산하기 위해 우리는 공백 분할을 사용했다. 더 높은 비옥도 점수는 토큰화기의 더 약한 압축 능력에 해당한다.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.9">최근에 제안된 패리티 <cite class="ltx_cite ltx_citemacro_cite">Petrov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib38" title="">2023</a>)</cite>는 토큰화기가 서로 다른 언어로 동등한 문장을 얼마나 공정하게 처리하는지 평가한다. 토큰사이저 <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1.1"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.1.m1.1d">italic_T</annotation></semantics></math>는 <math alttext="B" class="ltx_Math" display="inline" id="S3.SS4.p4.3.m3.1"><semantics id="S3.SS4.p4.3.m3.1a"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.3.m3.1d">italic_B</annotation></semantics></math>가 <math alttext="\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1" class="ltx_Math" display="inline" id="S3.SS4.p4.4.m4.2"><semantics id="S3.SS4.p4.4.m4.2a"><mrow id="S3.SS4.p4.4.m4.2.3" xref="S3.SS4.p4.4.m4.2.3.cmml"><mfrac id="S3.SS4.p4.4.m4.2.2" xref="S3.SS4.p4.4.m4.2.2.cmml"><mrow id="S3.SS4.p4.4.m4.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.2.cmml"><mo id="S3.SS4.p4.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.4.m4.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS4.p4.4.m4.2.2.2.1" xref="S3.SS4.p4.4.m4.2.2.2.2.cmml"><mo id="S3.SS4.p4.4.m4.2.2.2.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.4.m4.2.2.2.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml">B</mi></msub><mo id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.4.m4.2.2.2.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.SS4.p4.4.m4.2.3.1" xref="S3.SS4.p4.4.m4.2.3.1.cmml">≈</mo><mn id="S3.SS4.p4.4.m4.2.3.2" xref="S3.SS4.p4.4.m4.2.3.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.2b"><apply id="S3.SS4.p4.4.m4.2.3.cmml" xref="S3.SS4.p4.4.m4.2.3"><approx id="S3.SS4.p4.4.m4.2.3.1.cmml" xref="S3.SS4.p4.4.m4.2.3.1"></approx><apply id="S3.SS4.p4.4.m4.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2"><divide id="S3.SS4.p4.4.m4.2.2.3.cmml" xref="S3.SS4.p4.4.m4.2.2"></divide><apply id="S3.SS4.p4.4.m4.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1"><abs id="S3.SS4.p4.4.m4.1.1.1.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.2"></abs><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1"><times id="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2"></times><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3">𝐴</ci></apply></apply></apply><apply id="S3.SS4.p4.4.m4.2.2.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1"><abs id="S3.SS4.p4.4.m4.2.2.2.2.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.2"></abs><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1"><times id="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2"></times><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply><cn id="S3.SS4.p4.4.m4.2.3.2.cmml" type="integer" xref="S3.SS4.p4.4.m4.2.3.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.2c">\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.4.m4.2d">divide start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ) | end_ARG start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ) | end_ARG ≈ 1</annotation></semantics></math>인 경우, <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p4.2.m2.1"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.2.m2.1d">italic_A</annotation></semantics></math>에 대한 패리티를 달성하고, 여기서 <math alttext="s_{A}" class="ltx_Math" display="inline" id="S3.SS4.p4.5.m5.1"><semantics id="S3.SS4.p4.5.m5.1a"><msub id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml"><mi id="S3.SS4.p4.5.m5.1.1.2" xref="S3.SS4.p4.5.m5.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.5.m5.1.1.3" xref="S3.SS4.p4.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><apply id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.5.m5.1.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p4.5.m5.1.1.2.cmml" xref="S3.SS4.p4.5.m5.1.1.2">𝑠</ci><ci id="S3.SS4.p4.5.m5.1.1.3.cmml" xref="S3.SS4.p4.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">s_{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.5.m5.1d">italic_s start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT</annotation></semantics></math> 및 <math alttext="s_{B}" class="ltx_Math" display="inline" id="S3.SS4.p4.6.m6.1"><semantics id="S3.SS4.p4.6.m6.1a"><msub id="S3.SS4.p4.6.m6.1.1" xref="S3.SS4.p4.6.m6.1.1.cmml"><mi id="S3.SS4.p4.6.m6.1.1.2" xref="S3.SS4.p4.6.m6.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.6.m6.1.1.3" xref="S3.SS4.p4.6.m6.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m6.1b"><apply id="S3.SS4.p4.6.m6.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.6.m6.1.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p4.6.m6.1.1.2.cmml" xref="S3.SS4.p4.6.m6.1.1.2">𝑠</ci><ci id="S3.SS4.p4.6.m6.1.1.3.cmml" xref="S3.SS4.p4.6.m6.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m6.1c">s_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.6.m6.1d">italic_s start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>는 각각 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p4.7.m7.1"><semantics id="S3.SS4.p4.7.m7.1a"><mi id="S3.SS4.p4.7.m7.1.1" xref="S3.SS4.p4.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m7.1b"><ci id="S3.SS4.p4.7.m7.1.1.cmml" xref="S3.SS4.p4.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m7.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.7.m7.1d">italic_A</annotation></semantics></math> 및 <math alttext="B" class="ltx_Math" display="inline" id="S3.SS4.p4.8.m8.1"><semantics id="S3.SS4.p4.8.m8.1a"><mi id="S3.SS4.p4.8.m8.1.1" xref="S3.SS4.p4.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.8.m8.1b"><ci id="S3.SS4.p4.8.m8.1.1.cmml" xref="S3.SS4.p4.8.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.8.m8.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.8.m8.1d">italic_B</annotation></semantics></math> 언어의 말뭉치에서 모든 문장의 집합을 나타내고, <math alttext="\frac{|T(s_{A})|}{|T(s_{B})|}" class="ltx_Math" display="inline" id="S3.SS4.p4.9.m9.2"><semantics id="S3.SS4.p4.9.m9.2a"><mfrac id="S3.SS4.p4.9.m9.2.2" xref="S3.SS4.p4.9.m9.2.2.cmml"><mrow id="S3.SS4.p4.9.m9.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.2.cmml"><mo id="S3.SS4.p4.9.m9.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS4.p4.9.m9.1.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.9.m9.1.1.1.1.1.3" xref="S3.SS4.p4.9.m9.1.1.1.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.9.m9.1.1.1.1.1.2" xref="S3.SS4.p4.9.m9.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.9.m9.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS4.p4.9.m9.2.2.2.1" xref="S3.SS4.p4.9.m9.2.2.2.2.cmml"><mo id="S3.SS4.p4.9.m9.2.2.2.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS4.p4.9.m9.2.2.2.1.1" xref="S3.SS4.p4.9.m9.2.2.2.1.1.cmml"><mi id="S3.SS4.p4.9.m9.2.2.2.1.1.3" xref="S3.SS4.p4.9.m9.2.2.2.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.9.m9.2.2.2.1.1.2" xref="S3.SS4.p4.9.m9.2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3.cmml">B</mi></msub><mo id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.9.m9.2.2.2.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.2.1.cmml">|</mo></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.9.m9.2b"><apply id="S3.SS4.p4.9.m9.2.2.cmml" xref="S3.SS4.p4.9.m9.2.2"><divide id="S3.SS4.p4.9.m9.2.2.3.cmml" xref="S3.SS4.p4.9.m9.2.2"></divide><apply id="S3.SS4.p4.9.m9.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1"><abs id="S3.SS4.p4.9.m9.1.1.1.2.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.2"></abs><apply id="S3.SS4.p4.9.m9.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1"><times id="S3.SS4.p4.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.2"></times><ci id="S3.SS4.p4.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.3">𝑇</ci><apply id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3">𝐴</ci></apply></apply></apply><apply id="S3.SS4.p4.9.m9.2.2.2.2.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1"><abs id="S3.SS4.p4.9.m9.2.2.2.2.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.2"></abs><apply id="S3.SS4.p4.9.m9.2.2.2.1.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1"><times id="S3.SS4.p4.9.m9.2.2.2.1.1.2.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.2"></times><ci id="S3.SS4.p4.9.m9.2.2.2.1.1.3.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.3">𝑇</ci><apply id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.9.m9.2c">\frac{|T(s_{A})|}{|T(s_{B})|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.9.m9.2d">divide start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ) | end_ARG start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ) | end_ARG</annotation></semantics></math>의 비율을 프리미엄이라고 정의한다. 우리는 인간이 200개의 언어로 번역한 동일한 문장으로 구성된 FLORES-200 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib20" title="">2022</a>)</cite> 병렬 말뭉치를 사용한다. 영어와 관련하여 각 토큰화기와 네 가지 비영어 언어에 대한 패리티 값을 계산한다(개요는 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2" title="Figure 2 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> 참조).</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">외재적 평가는 토큰화기가 모델의 다운스트림 성능에 미치는 영향을 명시적으로 평가하는 것을 목표로 한다. 다운스트림 성능을 측정하기 위해 포괄적인 다운스트림 태스크 세트(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a> 참조)를 선택했다.</p>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.8">또한 훈련 중 단어당 주어진 모델의 평균 계산 비용에 대한 토큰화기의 영향을 계산했다. 상기 순방향 및 상기 역방향 패스를 포함하는 한 단계에 대한 훈련 동안의 계산 비용은,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right)," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">C</mi><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">96</mn><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.4.cmml">B</mi><mo id="S3.E1.m1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.5.cmml">s</mi><mo id="S3.E1.m1.1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.1.6.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.2c" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><msup id="S3.E1.m1.1.1.1.1.1.7" xref="S3.E1.m1.1.1.1.1.1.7.cmml"><mi id="S3.E1.m1.1.1.1.1.1.7.2" xref="S3.E1.m1.1.1.1.1.1.7.2.cmml">h</mi><mn id="S3.E1.m1.1.1.1.1.1.7.3" xref="S3.E1.m1.1.1.1.1.1.7.3.cmml">2</mn></msup><mo id="S3.E1.m1.1.1.1.1.1.2d" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">s</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">6</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">h</mi></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml">V</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml">16</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml">h</mi></mrow></mfrac></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">𝐶</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><cn id="S3.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3">96</cn><ci id="S3.E1.m1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.4">𝐵</ci><ci id="S3.E1.m1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.5">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.1.6">𝑙</ci><apply id="S3.E1.m1.1.1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.7.1.cmml" xref="S3.E1.m1.1.1.1.1.1.7">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.7.2.cmml" xref="S3.E1.m1.1.1.1.1.1.7.2">ℎ</ci><cn id="S3.E1.m1.1.1.1.1.1.7.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.7.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></plus><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝑠</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">6</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">ℎ</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2">𝑉</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1"></times><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2">16</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4">ℎ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_C = 96 italic_B italic_s italic_l italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + divide start_ARG italic_s end_ARG start_ARG 6 italic_h end_ARG + divide start_ARG italic_V end_ARG start_ARG 16 italic_l italic_h end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p6.7">given a model with batch size <math alttext="B" class="ltx_Math" display="inline" id="S3.SS4.p6.1.m1.1"><semantics id="S3.SS4.p6.1.m1.1a"><mi id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><ci id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.1.m1.1d">italic_B</annotation></semantics></math>, sequence length <math alttext="s" class="ltx_Math" display="inline" id="S3.SS4.p6.2.m2.1"><semantics id="S3.SS4.p6.2.m2.1a"><mi id="S3.SS4.p6.2.m2.1.1" xref="S3.SS4.p6.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.2.m2.1b"><ci id="S3.SS4.p6.2.m2.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.2.m2.1d">italic_s</annotation></semantics></math>, <math alttext="l" class="ltx_Math" display="inline" id="S3.SS4.p6.3.m3.1"><semantics id="S3.SS4.p6.3.m3.1a"><mi id="S3.SS4.p6.3.m3.1.1" xref="S3.SS4.p6.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.3.m3.1b"><ci id="S3.SS4.p6.3.m3.1.1.cmml" xref="S3.SS4.p6.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.3.m3.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.3.m3.1d">italic_l</annotation></semantics></math> layers, hidden size <math alttext="h" class="ltx_Math" display="inline" id="S3.SS4.p6.4.m4.1"><semantics id="S3.SS4.p6.4.m4.1a"><mi id="S3.SS4.p6.4.m4.1.1" xref="S3.SS4.p6.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.4.m4.1b"><ci id="S3.SS4.p6.4.m4.1.1.cmml" xref="S3.SS4.p6.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.4.m4.1d">italic_h</annotation></semantics></math> and vocabulary size <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.p6.5.m5.1"><semantics id="S3.SS4.p6.5.m5.1a"><mi id="S3.SS4.p6.5.m5.1.1" xref="S3.SS4.p6.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.5.m5.1b"><ci id="S3.SS4.p6.5.m5.1.1.cmml" xref="S3.SS4.p6.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.5.m5.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.5.m5.1d">italic_V</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Narayanan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib36" title="">2021</a>)</cite>. 토큰당 비용은 <math alttext="C_{\text{token}}=C/Bs" class="ltx_Math" display="inline" id="S3.SS4.p6.6.m6.1"><semantics id="S3.SS4.p6.6.m6.1a"><mrow id="S3.SS4.p6.6.m6.1.1" xref="S3.SS4.p6.6.m6.1.1.cmml"><msub id="S3.SS4.p6.6.m6.1.1.2" xref="S3.SS4.p6.6.m6.1.1.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.2.2" xref="S3.SS4.p6.6.m6.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.6.m6.1.1.2.3" xref="S3.SS4.p6.6.m6.1.1.2.3a.cmml">token</mtext></msub><mo id="S3.SS4.p6.6.m6.1.1.1" xref="S3.SS4.p6.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.6.m6.1.1.3" xref="S3.SS4.p6.6.m6.1.1.3.cmml"><mrow id="S3.SS4.p6.6.m6.1.1.3.2" xref="S3.SS4.p6.6.m6.1.1.3.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.3.2.2" xref="S3.SS4.p6.6.m6.1.1.3.2.2.cmml">C</mi><mo id="S3.SS4.p6.6.m6.1.1.3.2.1" xref="S3.SS4.p6.6.m6.1.1.3.2.1.cmml">/</mo><mi id="S3.SS4.p6.6.m6.1.1.3.2.3" xref="S3.SS4.p6.6.m6.1.1.3.2.3.cmml">B</mi></mrow><mo id="S3.SS4.p6.6.m6.1.1.3.1" xref="S3.SS4.p6.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p6.6.m6.1.1.3.3" xref="S3.SS4.p6.6.m6.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.6.m6.1b"><apply id="S3.SS4.p6.6.m6.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1"><eq id="S3.SS4.p6.6.m6.1.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1.1"></eq><apply id="S3.SS4.p6.6.m6.1.1.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.6.m6.1.1.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.6.m6.1.1.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.2.3a.cmml" xref="S3.SS4.p6.6.m6.1.1.2.3"><mtext id="S3.SS4.p6.6.m6.1.1.2.3.cmml" mathsize="70%" xref="S3.SS4.p6.6.m6.1.1.2.3">token</mtext></ci></apply><apply id="S3.SS4.p6.6.m6.1.1.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3"><times id="S3.SS4.p6.6.m6.1.1.3.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.1"></times><apply id="S3.SS4.p6.6.m6.1.1.3.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2"><divide id="S3.SS4.p6.6.m6.1.1.3.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.1"></divide><ci id="S3.SS4.p6.6.m6.1.1.3.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.3.2.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.3">𝐵</ci></apply><ci id="S3.SS4.p6.6.m6.1.1.3.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.6.m6.1c">C_{\text{token}}=C/Bs</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.6.m6.1d">italic_C start_POSTSUBSCRIPT token end_POSTSUBSCRIPT = italic_C / italic_B italic_s</annotation></semantics></math>에 의해 도출될 수 있고, 단어당 평균 비용은 <math alttext="C_{\text{word}}=C_{\text{token}}\times\text{fertility}" class="ltx_Math" display="inline" id="S3.SS4.p6.7.m7.1"><semantics id="S3.SS4.p6.7.m7.1a"><mrow id="S3.SS4.p6.7.m7.1.1" xref="S3.SS4.p6.7.m7.1.1.cmml"><msub id="S3.SS4.p6.7.m7.1.1.2" xref="S3.SS4.p6.7.m7.1.1.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.2.2" xref="S3.SS4.p6.7.m7.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.2.3" xref="S3.SS4.p6.7.m7.1.1.2.3a.cmml">word</mtext></msub><mo id="S3.SS4.p6.7.m7.1.1.1" xref="S3.SS4.p6.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.7.m7.1.1.3" xref="S3.SS4.p6.7.m7.1.1.3.cmml"><msub id="S3.SS4.p6.7.m7.1.1.3.2" xref="S3.SS4.p6.7.m7.1.1.3.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.3.2.2" xref="S3.SS4.p6.7.m7.1.1.3.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.3.2.3" xref="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml">token</mtext></msub><mo id="S3.SS4.p6.7.m7.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p6.7.m7.1.1.3.1.cmml">×</mo><mtext id="S3.SS4.p6.7.m7.1.1.3.3" xref="S3.SS4.p6.7.m7.1.1.3.3a.cmml">fertility</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.7.m7.1b"><apply id="S3.SS4.p6.7.m7.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1"><eq id="S3.SS4.p6.7.m7.1.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1.1"></eq><apply id="S3.SS4.p6.7.m7.1.1.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.2.3"><mtext id="S3.SS4.p6.7.m7.1.1.2.3.cmml" mathsize="70%" xref="S3.SS4.p6.7.m7.1.1.2.3">word</mtext></ci></apply><apply id="S3.SS4.p6.7.m7.1.1.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3"><times id="S3.SS4.p6.7.m7.1.1.3.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.1"></times><apply id="S3.SS4.p6.7.m7.1.1.3.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.3.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.3.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.3"><mtext id="S3.SS4.p6.7.m7.1.1.3.2.3.cmml" mathsize="70%" xref="S3.SS4.p6.7.m7.1.1.3.2.3">token</mtext></ci></apply><ci id="S3.SS4.p6.7.m7.1.1.3.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3"><mtext id="S3.SS4.p6.7.m7.1.1.3.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3">fertility</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.7.m7.1c">C_{\text{word}}=C_{\text{token}}\times\text{fertility}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.7.m7.1d">italic_C start_POSTSUBSCRIPT word end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT token end_POSTSUBSCRIPT × fertility</annotation></semantics></math>에 의해 도출될 수 있다. 결과는 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>에서 논의된다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Intrinsic Tokenizer Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">우리의 고유 평가에서 먼저 훈련된 토큰아이저(섹션<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS1" title="4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4.1</span></a>)의 번식력과 패리티를 비교하고 후속적으로 그들의 어휘의 중첩(섹션<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4.2</span></a>)을 비교한다.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fertility &amp; Parity</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F0.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="622" id="S4.F0.sf1.g1" src="https://arxiv.org/html/2310.08754v4/x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a)</span>Non-English, 다국어 문서</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F0.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="622" id="S4.F0.sf2.g1" src="https://arxiv.org/html/2310.08754v4/x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b)</span>영문 문서</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span> (a) 비영어, 다국어 문서 및 (b) 영어 문서에 적용된 단일 언어 및 다국어 토큰라이저 간의 번식력 점수 비교.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">설명된 번식력 및 패리티 평가를 단일/다국어 토큰라이저에 적용하면 우리의 분석에서는 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F1" title="Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a></a>와 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2" title="Figure 2 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>에서 시각화된 다음 두 가지 주요 측면을 강조한다.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">첫째, 단일 언어 토큰화기를 다국어 데이터에 적용하면 번식력 및 패리티 점수가 상당히 높다는 것을 관찰할 수 있다(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F0.sf1" title="0(a) ‣ Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">0(a)</span></a> 및 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2" title="Figure 2 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> 참조). 다국어 토큰라이저는 모든 비영어 문서에서 단일 언어 영어 토큰라이저보다 큰 마진만큼 낮은 번식력을 갖는 반면, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F0.sf2" title="0(b) ‣ Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">0(b)</span></a>와 같이 영어 문서를 토큰라이징하는 데 약간 더 나쁘다.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">둘째, 어휘 크기가 증가함에 따라, 비옥도 및 패리티가 모든 경우에 감소하는데, 이는 더 큰 어휘가 주어진 텍스트를 토큰화할 때 더 적은 서브워드 토큰들을 요구하는 토큰화기에 의해 설명될 수 있다. 그러나 단일 언어 영어 토큰라이저의 경우 영어 문서를 토큰화할 때 생식력이 어휘에 덜 의존한다는 것을 관찰할 수 있으며, 이는 33k가 충분히 큰 어휘일 수 있음을 의미한다.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="321" id="S4.F2.g1" src="https://arxiv.org/html/2310.08754v4/x3.png" width="411">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span>단일 언어(영어) 토큰나이저와 다중 언어 토큰나이저가 적용된 다중 언어 문서 간의 패리티 점수 비교.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Vocabulary Overlap</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">토큰나이저 유사도를 분석하기 위해 어휘 중복도를 계산하였다. 특히 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.T1" title="Table 1 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>와 같이 Huggingface와 SentencePiece의 BPE 구현을 평가한다.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">중첩은 서로 다른 어휘 크기에 걸쳐 대략 일정하며, 두 개의 서로 다른 라이브러리에 의해서만 구현되는 동일한 알고리즘임에도 불구하고 전체 중첩은 다소 낮은 경향이 있다. 결과적으로, 토큰화기는 서로 다른 토큰화된 시퀀스를 생성하여 모델 훈련 및 다운스트림 성능에 영향을 미칠 수 있다. 근본적인 이유를 조사하면 이러한 라이브러리의 구성 및 전처리 옵션이 다르기 때문에 중복이 적을 수 있다. 다국어 문서의 시소러스가 크기 때문에 다국어 토큰화기의 겹침이 영어 토큰화기의 겹침보다 낮습니다.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">33k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">50k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">82k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5">100k</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">English</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">0.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">0.74</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.3.2.1">Multilingual</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.2">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.3">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.4">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.5">0.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>Vocabulary overlap between the HuggingFace and SentencePiece BPE tokenizer for different vocab size.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="634" id="S4.F2.sf1.g1" src="https://arxiv.org/html/2310.08754v4/x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a)</span>Non-English documents</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="634" id="S4.F2.sf2.g1" src="https://arxiv.org/html/2310.08754v4/x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b)</span>독일 문서</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="634" id="S4.F2.sf3.g1" src="https://arxiv.org/html/2310.08754v4/x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c)</span>영문 문서</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3: </span>Average compute (GFLOPS) required to processing a single word within a full <span class="ltx_text ltx_font_bold" id="S4.F3.2.1">training</span> pass (backward pass 포함).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Extrinsic Tokenizer Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">다음에서는 토큰라이저의 외부 평가 결과를 설명한다. <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>는 실험 설정을 설명하고, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2" title="5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>는 조사된 토큰라이저에 기초하여 훈련된 모델의 다운스트림 성능을 제시하고, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>는 특정 모델에 채용될 때 각 토큰라이저와 관련된 계산 비용을 분석한다.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">모델 다운스트림 성능에 대한 토큰라이저의 영향을 평가하기 위해 각 토큰라이저에 대해 크기 2.6 B의 디코더 전용 변압기 모델을 훈련했다. 우리는 인과 언어 모델링 훈련 목표를 기반으로 <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib25" title="">2022b</a>)</cite>에서 제안한 스케일링 법칙에 따라 52.6 B 토큰에 대한 모델을 훈련했다. 하이퍼-파라미터들은 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3.T10" title="Table 10 ‣ Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>에서 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3" title="Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>로 설명된다. 우리는 다양한 단일 언어 및 다중 언어 작업에 대해 제로 샷 설정에서 모델을 평가했다.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">자연어 추론: XNLI <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib11" title="">2018</a>)</cite>, MNLI <cite class="ltx_cite ltx_citemacro_cite">Williams et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib59" title="">2018</a>)</cite>, RTE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib57" title="">2018</a>)</cite>, WNLI <cite class="ltx_cite ltx_citemacro_cite">Levesque et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib32" title="">2012</a>)</cite>, CB <cite class="ltx_cite ltx_citemacro_cite">De Marneffe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib12" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">질문 응답: X-CSQA <cite class="ltx_cite ltx_citemacro_cite">Goodman (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib19" title="">2001</a>)</cite>, XStoryCloze <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib33" title="">2022</a>)</cite>, PubMedQA <cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib27" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">읽기 이해: BoolQ <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib7" title="">2019</a>)</cite>), LAMBADA <cite class="ltx_cite ltx_citemacro_cite">Paperno et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib37" title="">2016</a>)</cite>, RACE <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib31" title="">2017</a>)</cite>, MRPC <cite class="ltx_cite ltx_citemacro_cite">Dolan and Brockett (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib14" title="">2005</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">상식 추론: HellaSwag <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib64" title="">2019</a>)</cite>, WinoGrande <cite class="ltx_cite ltx_citemacro_cite">Sakaguchi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib44" title="">2020</a>)</cite>, ARC <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib9" title="">2018</a>)</cite>, XCOPA <cite class="ltx_cite ltx_citemacro_cite">Ponti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib40" title="">2020</a>)</cite>, XCDOAH <cite class="ltx_cite ltx_citemacro_cite">Goodman (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib19" title="">2001</a>)</cite>, WSC <cite class="ltx_cite ltx_citemacro_cite">Levesque et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib32" title="">2012</a>)</cite>, COPA <cite class="ltx_cite ltx_citemacro_cite">Roemmele et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib42" title="">2011</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">분류: PAWS-X <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib61" title="">2019</a>)</cite>, GNAD10 <cite class="ltx_cite ltx_citemacro_cite">Schabus et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib46" title="">2017</a>)</cite>, SST <cite class="ltx_cite ltx_citemacro_cite">Socher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib50" title="">2013</a>)</cite>, WIC <cite class="ltx_cite ltx_citemacro_cite">Pilehvar and Camacho-Collados (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib39" title="">2019</a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_cite">Bisk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib3" title="">2020</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T2" title="Table 2 ‣ 5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>는 카테고리별, 언어별 태스크 수에 대한 개요를 제공한다.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1">Task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.2">EN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.3">DE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.4">FR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.5">ES</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.6">IT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.2.1.1">NLI</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.5">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.6">0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.3.2.1">QA</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.2">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.3">2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.4">2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.5">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.6">2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.3.1">RC</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.2">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.3">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.4">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.5">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.6">1</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.4.1">CR</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.2">7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.4">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.6">1</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.6.5.1">CL</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.2">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.3">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.4">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.5">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.6">0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.1"></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.2">22</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.5">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.6">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2: </span> 자연어 추론(NLI), 독해(RC), 질의 응답(QA), 상식 추론(CR) 및 분류(CL)의 각 언어별 평가 과제 수의 개요.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Downstream Performance</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">다운스트림 성능에 대한 분석을 여러 부분으로 나눕니다.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">먼저, 조사된 토큰라이저에 대해 얻은 전반적인 결과에 대해 논의한 후, 토큰라이저 라이브러리의 영향(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS1" title="5.2.1 Impact of the Tokenizer Library ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.1</span></a>), 토큰라이저 알고리즘의 영향(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS2" title="5.2.2 Impact of the Tokenizer Algorithm ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.2</span></a>), 어휘 크기의 영향(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS3" title="5.2.3 Impact of the Tokenizer Vocabulary ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.3</span></a>)을 제시한다.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">모든 태스크(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>)에 걸쳐 집계된 결과와 선택된 단일 태스크(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T4" title="Table 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>)에 대한 결과를 모두 제시한다. <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>에 제시된 모든 작업에 대한 평균 성능에 대해 언어당 다른 작업 수를 고려하기 위해 가중 평균을 계산했다. 특히, 우리는 모든 태스크에서 각 언어에 대한 평균을 계산한 다음 모든 언어-평균에 대한 평균을 계산했다.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.2">EN</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T3.1.1.1.3">MULTI</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.1">GPT-2-50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.2">50.36</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.2.2.3">39.41</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.3.3.1">BPE-HF-33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.3.3.2">49.13</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.3.3.3">40.52</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.4.1">BPE-HF-50</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.4.2">49.51</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.3">40.47</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.5.1">BPE-HF-82</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.5.2">48.71</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.3">40.24</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.6.1">BPE-HF-100</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.6.2">49.54</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.3">40.48</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.7.7.1">BPE-SP-33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.7.2.1">50.81</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.3">40.28</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<td class="ltx_td ltx_align_left" id="S5.T3.1.8.8.1">BPE-SP-50</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.8.8.2">49.81</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.3">40.49</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.9.1">BPE-SP-82</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.9.2">48.99</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.3">41.21</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<td class="ltx_td ltx_align_left" id="S5.T3.1.10.10.1">BPE-SP-100</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.10.10.2">49.46</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.3.1">41.44</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.11.11.1">UNI-SP-33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.11.11.2">50.28</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.11.11.3">40.30</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.12.12">
<td class="ltx_td ltx_align_left" id="S5.T3.1.12.12.1">UNI-SP-50</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.12.12.2">49.90</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.3">40.48</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.13.13">
<td class="ltx_td ltx_align_left" id="S5.T3.1.13.13.1">UNI-SP-82</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.13.13.2">49.65</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.13.13.3">41.20</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.14.14.1">UNI-SP-100</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.14.14.2">50.21</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.14.14.3">40.74</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>모든 다운스트림 태스크에 걸쳐 단일 언어 및 다중 언어 토큰라이저의 평균 정확도. 언어당 다양한 작업 수로 인해 다중 언어 정확도는 평균에 동등하게 기여하는 각 언어로 조정되었다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.2">Task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.3">Min</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.4">Max</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.5">Rand.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.1" rowspan="4"><span class="ltx_text" id="S5.T4.1.2.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.2.1.1.1.1" style="width:6.8pt;height:14.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-3.74pt,-3.74pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.1.2.1.1.1.1.1">EN</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.2">ARC-Easy</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.3">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.4">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.5">0.20</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.3.2.1">HellaSwag</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.2">0.34</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.3">0.41</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.4">0.25</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.4.3.1">MRPC</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.2">0.54</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.3">0.69</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.4">0.50</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.5.4.1">PIQA</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.2">0.67</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.3">0.72</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.4">0.50</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T4.1.6.5.1" rowspan="4"><span class="ltx_text" id="S5.T4.1.6.5.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.6.5.1.1.1" style="width:6.8pt;height:32.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.9pt;transform:translate(-13.04pt,-13.04pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.1.6.5.1.1.1.1">MULTI</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.6.5.2">XNLI FR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.6.5.3">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.6.5.4">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.6.5.5">0.33</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.7.6.1">XNLI EN</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.2">0.49</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.3">0.52</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.4">0.33</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.8.7.1">X-CODAH ES</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.2">0.28</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.3">0.43</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.4">0.25</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.9.8.1">10kGNAD</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.9.8.2">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.9.8.3">0.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.9.8.4">0.11</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4:</span>Worst- and best-performing tokenizer for selected tasks and the random performance on this task.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S5.T5.1.1.1.2">MULTI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.1.1.3">MONO</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.2.2.1">Vocabulary</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.2">DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.3">FR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.4">IT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.5">ES</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.6">EN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.7">AVG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.8">EN</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.3.3.1">33</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.3.3.2.1">36.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.3">36.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.4">39.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.5">41.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.6">47.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.7">40.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.8">49.55</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.4.4.1">50</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.2">36.12</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.3">37.07</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.4">38.94</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.5">42.22</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.6">46.71</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.7">40.21</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.4.8.1">49.90</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.5.5.1">82</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.2">36.50</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.3">37.83</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.4">39.97</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.5">42.30</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.5.6.1">47.80</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.7">40.88</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.8">49.12</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.6.6.1">100</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.2">35.92</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.3.1">38.07</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.4.1">40.13</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.5.1">42.64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.6">47.67</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.7.1">40.89</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.8">49.74</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T5.1.7.7.1">Algorithm and Library</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.2">DE</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.3">FR</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.4">IT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.5">ES</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.6">EN</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.7">AVG</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.8">EN</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.8.8.1">BPE-HF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.2">35.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.3">37.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.4">39.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.5">42.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.6">47.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.7">40.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.8">48.98</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.9.9.1">BPE-SP</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.2.1">37.13</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.3">37.45</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.4.1">40.04</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.5">41.96</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.6.1">47.68</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.7.1">40.85</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.8">49.77</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.10.10.1">UNI-SP</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.2">36.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.10.10.3.1">37.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.4">39.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.10.10.5.1">42.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.6">47.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.7">40.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.10.10.8.1">50.01</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>Impact of the vocabulary size (upper), and tokenizer algorithm and library (lower), on the downstream performance. 정확도 점수는 라이브러리 및 토큰화 알고리즘(상위) 또는 다른 어휘 크기(하위)에 대해 평균화된다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Monolingual Tokenizer</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>는 평균적으로 BPE-SP-33 tokenizer가 가장 성능이 좋은 tokenizer이고 GPT-2 tokenizer가 그 뒤를 잇고 있음을 보여준다. 흥미롭게도 SentencePiece의 어휘 크기 33k의 BPE 구현은 LLaMA2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib55" title="">2023</a>)</cite>에 사용되었다. 집계 메트릭은 전체 성능에 대한 합리적인 개요를 제공합니다. 그러나 작업 간에 잠재적으로 큰 성능 차이를 표현하지 않습니다. 따라서, 우리는 이 태스크에 대해 최고 및 최악을 수행하는 토크나이저에 의해 획득된 선택된 태스크들의 리스트에 대해 획득된 결과들을 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T4" title="Table 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>에 열거하였다. 결과는 성능 차이가 클 수 있음을 보여줍니다. 예를 들어, 상식 추론 작업인 ARC-Easy의 경우 최고 토큰화기와 최악 토큰화기의 간격은 9%이다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multilingual Tokenizer</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>는 BPE-SP-100 tokenizer가 가장 성능이 좋은 tokenizer이고 그 다음이 BPE-SP-82 tokenizer임을 보여준다. 또한, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>는 GPT-2 토큰izer가 제대로 작동하지 않음을 보여주며, 이는 사전 훈련 및 미세 조정 다중 언어 모델에 사전 훈련된 GPT-2 토큰izer를 사용하는 것이 <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p1.1.1">omitted</span>이어야 함을 의미한다. 선택된 태스크들의 분석 ( <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T4" title="Table 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4</span></a>)은 다국어 토큰라이저들의 경우 태스크들 간의 성능 차이가 클 수 있음을 보여준다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Impact of the Tokenizer Library</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T5" title="Table 5 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>는 BPE-SP가 모든 언어에 걸쳐 단일 언어 및 다중 언어 설정에서 평균적으로 BPE-HF보다 우수하다는 것을 보여준다. 성능 차이는 토큰라이저의 사전 및 사후 처리의 구현 세부 사항의 차이에 기인할 수 있으며, 이는 어휘 생성(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> 참조) 및 결과적으로 다운스트림 성능에 영향을 미칠 수 있다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Impact of the Tokenizer Algorithm</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">또한, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T5" title="Table 5 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>는 언어에 따라 BPE 또는 Unigram 중 하나가 더 나은 성능을 나타낸다는 것을 보여준다. 주목할 점은 독일어와 영어가 BPE 알고리즘의 혜택을 받는 반면, 루마니아어인 프랑스어와 스페인어는 유니그램의 혜택을 받았다는 점이다. 루마니아어인 이탈리아어에 대한 실험 역시 다른 두 루마니아어와는 다른 양상을 보인다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Impact of the Tokenizer Vocabulary</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">어휘 크기의 영향을 분석한 결과, 단일 언어 영어 설정에서는 33k/50k의 작은/중간 크기의 어휘가 더 나은 성능을 보이는 반면(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T5" title="Table 5 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>), 다국어 설정에서는 독일어를 제외한 모든 경우에서 더 큰 어휘 크기가 더 나은 다운스트림 성능을 보이는 것으로 나타났다. <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>에 제시된 결과를 고려할 때, 단일 언어 영어 설정에서 모든 태스크에 걸쳐 평균적으로 가장 성능이 좋은 토큰화기가 33k의 어휘 크기를 가졌고, 가장 성능이 좋은 다국어 토큰화기가 100k의 어휘 크기를 가졌다는 것을 보여주는 것은 단일 언어 영어 설정에 대해 작은 어휘 크기가 유익하고 다국어 설정에 대해 큰 어휘 크기가 필요하다는 관찰을 추가로 뒷받침한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Computational Costs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">고정 모델이 주어진 경우 계산 비용은 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.E1" title="1 ‣ 3.4 Evaluation ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>에서 정의된 바와 같이 어휘 크기와 토큰화기의 비옥도에 따라 달라진다.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">더 큰 어휘 크기는 추가 계산 비용을 도입하지만, 이는 또한 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4" title="4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>에서 논의된 바와 같이 더 낮은 번식력 점수를 초래할 수 있고, 따라서 문서 세트를 처리하기 위한 전체 계산 비용을 낮출 수 있다. 그러나 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F3" title="Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>의 연구 결과는 어휘 크기를 50k에서 더 큰 어휘 크기로 증가시키면 모든 경우에 계산 비용이 증가한다는 것을 보여준다. 이것은 더 큰 어휘 크기의 잠재적으로 낮은 번식력이 더 큰 어휘 크기에 의해 도입되는 추가 비용을 보상할 수 없음을 강조한다.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">또한, 다국어 문서에 대한 계산 훈련 비용은 단일 언어 영어 토큰아이저보다 다국어 토큰아이저에서 상당히 낮다는 것을 관찰한다(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2.sf1" title="2(a) ‣ Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2(a)</span></a>). 실제로, 부록의 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2.sf2" title="2(b) ‣ Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2(b)</span></a> 및 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.T11" title="Table 11 ‣ D.1 Computational Costs Per Word During Training ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>는 훈련 비용이 주어진 데이터 세트에 대해 최대 68%(독일 문서용 EN-UNI-SP-100에 Multi-UNI-SP-50을 비교하는 것)까지 증가할 수 있음을 보여준다. 훈련 동안 고정된 문서 세트(예를 들어, 특정 사실을 학습하기 위해 위키피디아)를 완전히 처리하고 주어진 수의 토큰뿐만 아니라 처리해야 한다고 가정하면, 토큰화기의 선택은 이 코퍼스에 대한 훈련을 위한 계산 비용에 상당한 영향을 미칠 수 있다.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">단일 언어 영어 설정에서 다국어 영어 토큰사이저와 단일 언어 영어 토큰사이저의 큰 비용 차이를 관찰할 수 있었지만, 영어 문서를 처리하기 위한 다국어 영어 토큰사이저와 단일 언어 영어 토큰사이저의 계산 비용 차이는 미미하다(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2.sf3" title="2(c) ‣ Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2(c)</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Correlation Between Intrinsic And Extrinsic Tokenizer Performance</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="415" id="S6.F4.g1" src="https://arxiv.org/html/2310.08754v4/x7.png" width="415">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">그림 4:</span>Spearman correlation of fertility/parity scores and downstream task performance for all 5 languages.</figcaption><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Spearman correlation of fertility/parity scores and downstream task performance for all five languages.
We evaluated monolingual models on English tasks (left), whereas our multilingual models are evaluated across all non-English tasks. Pearson and Kendall correlation metrics showed a very similar picture.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">이 섹션에서는 고유 토큰izer 메트릭(Ftility 및 패리티)이 외부 모델 다운스트림 성능에 대한 가능한 예측 관계를 조사합니다.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S6.F4" title="Figure 4 ‣ 6 Correlation Between Intrinsic And Extrinsic Tokenizer Performance ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>의 상관 히트맵에서 강조했듯이 모든 작업과 언어에 걸쳐 뚜렷한 상관 관계가 없음을 발견하여 보다 세분화된 분석을 요구한다. 비영어 작업의 경우 주로 저출산과 더 높은 다운스트림 성능 사이의 상관 관계를 관찰하는 반면, 비영어 작업은 겉보기에 무작위적인 양의 상관 관계와 음의 상관 관계를 산출한다. 그러나 언어당 다국어 태스크의 수는 영어보다 훨씬 적고 XSQA 및 LAMBADA와 같은 여러 다국어 태스크의 경우 영어 태스크와 번역 버전 사이의 유사한 상관 동작이 관찰될 수 있다는 점에 유의해야 한다.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">다양한 어휘 크기(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F1" title="Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> 참조)에 따른 번식력 추세를 고려하여 특정 언어 특정 어휘 크기 제한에서 번식력은 다운스트림 성능과만 상관관계가 있다고 가정한다. 영어의 경우, 토큰화기는 이미 33k 토큰의 어휘 크기에 대해 낮고 수렴에 가까운 번식력 점수를 제공한다. 추가 토큰은 약간의 번식력 향상만 산출하지만 형태학적 분할을 포착하지 못하여 다운스트림 성능을 해칠 수 있으며 결과적으로 계산 비용(<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a> 참조)을 크게 증가시킬 수 있다고 가정한다.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">대조적으로, 다국어 토큰라이저의 경우 어휘 크기가 증가함에 따라 상당한 번식력 향상을 관찰한다. 추가 언어에 의해 유도된 더 큰 시소러스로 인해, 토큰화기는 모델이 모든 언어에 대해 설득력 있게 수행할 수 있도록 하기 위해 더 큰 어휘를 요구한다. 따라서 비융합 어휘 범위 내에서만 다양한 어휘 크기로 번식력과 다운스트림 성능 사이의 강력하고 부정적인 상관 관계를 달성한다.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">결론적으로, 비옥도 및 패리티와 같은 고유 토큰나이저 메트릭은 가감하여 취할 필요가 있으며 추정컨대 특정 범위에서 다운스트림 모델 성능을 예측할 뿐이다. 저출산 점수는 필요한 기준으로 간주될 수 있지만 충분한 기준으로 간주되지 않을 수 있다.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion &amp; Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">이 작업은 토큰화기가 모델의 다운스트림 성능에 미치는 영향을 더 잘 이해하기 위한 기본 단계를 나타낸다. 우리는 언어 전반에 걸쳐 균형 잡힌 점유율을 가진 훈련 토큰라이저가 모든 언어에서 비교할 수 있는 저출산 및 패리티 점수를 달성한다는 것을 보여주었으며, 이는 중요한 의미를 가지고 있다. 더 높은 비옥도는 훈련 동안 최대 68% 더 많은 계산 비용을 초래하고 모델이 제한된 컨텍스트 창에서 장거리 종속성을 학습하는 것을 방지한다.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">또한 토크나이저 선택이 모델의 다운스트림 성능에 상당한 영향을 미칠 수 있음을 강조합니다. 우리는 BPE 알고리즘이 단일 언어 및 다중 언어 설정에 잘 적용된다는 것을 보여줄 수 있다. 영어의 경우, 33k의 어휘 크기가 충분한 반면, 5개의 고려된 언어를 기반으로 하는 다국어 모델은 최대 3배 더 큰 어휘 크기를 필요로 한다는 것을 보여준다. 또한, SentencePiece 라이브러리가 Huggingface tokenizer 라이브러리보다 우수한 성능을 보임을 확인할 수 있었다.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">마지막으로 내재적 토크나이저 성능과 외재적 토크나이저 성능 사이에는 명확한 상관관계가 없음을 입증할 수 있었지만, 그 상관관계는 오히려 작업에 따라 다르다. 작은 비옥도 값은 다운스트림 성능이 좋은 데 필요한 조건일 수 있지만 충분한 조건은 아닐 수 있다.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">향후, 우리는 매우 다양한 언어를 포함한 더 큰 언어 세트에 대한 토큰라이저를 조사하고, 토큰라이저 훈련 중 컨텍스트 정보에 초점을 맞춘 SAGE<cite class="ltx_cite ltx_citemacro_cite">Yehezkel and Pinter (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib62" title="">2023</a>)</cite>와 같은 대체 토큰화 접근법의 영향을 조사하는 것을 목표로 한다.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">우리 작업의 확장성에도 불구하고 다음과 같은 한계에 직면해 있다.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">첫째, 각 토큰화기에 대해 하이퍼-파라미터 최적화를 수행하지 않았다. 이는 26개 모델 모두를 한 번만 훈련하면 <math alttext="\approx 59.000" class="ltx_Math" display="inline" id="S8.p2.1.m1.1"><semantics id="S8.p2.1.m1.1a"><mrow id="S8.p2.1.m1.1.1" xref="S8.p2.1.m1.1.1.cmml"><mi id="S8.p2.1.m1.1.1.2" xref="S8.p2.1.m1.1.1.2.cmml"></mi><mo id="S8.p2.1.m1.1.1.1" xref="S8.p2.1.m1.1.1.1.cmml">≈</mo><mn id="S8.p2.1.m1.1.1.3" xref="S8.p2.1.m1.1.1.3.cmml">59.000</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.p2.1.m1.1b"><apply id="S8.p2.1.m1.1.1.cmml" xref="S8.p2.1.m1.1.1"><approx id="S8.p2.1.m1.1.1.1.cmml" xref="S8.p2.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S8.p2.1.m1.1.1.2.cmml" xref="S8.p2.1.m1.1.1.2">absent</csymbol><cn id="S8.p2.1.m1.1.1.3.cmml" type="float" xref="S8.p2.1.m1.1.1.3">59.000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.1.m1.1c">\approx 59.000</annotation><annotation encoding="application/x-llamapun" id="S8.p2.1.m1.1d">≈ 59.000</annotation></semantics></math> GPU 시간이 필요하다는 점을 고려할 때 추가적인 계산 비용을 피하기 위한 의도적인 선택이었다.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">두 번째로, 우리는 추가적인 계산비용으로 인해 주어진 토큰나이저의 모델 성능에 대한 서로 다른 랜덤 시드의 영향을 조사하지 않았다. 그러나, 우리의 결과는 선택된 실험의 견고성을 추가로 조사할 수 있는 향후 작업의 기초를 마련한다.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">셋째, 얻은 결과가 더 큰 모델 크기로 외삽될 수 있는지 여부를 조사하지 않았으며 이는 향후 작업에 맡긴다. 그러나 BPE-SP-33 tokenizer가 단일 언어 설정을 위한 가장 성능이 좋은 tokenizer라는 사실과 이 tokenizer가 65B <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib54" title="">Touvron et al. </a></cite>까지 최신 모델을 훈련하는 데 사용되었다는 사실은 우리의 결과가 더 큰 모델 크기로도 전달된다는 것을 나타낼 수 있다.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">마지막으로, 이 작업의 맥락에서 관심 있는 메트릭이 제로 샷 다운스트림 성능이었기 때문에 몇 가지 쇼 설정에 대한 결과를 제공하지 않았다. 토나이저 선택이 모델의 다운스트림 성능에 영향을 미치는지 여부를 조사하고자 했기 때문에 널리 적용되는 메트릭 중 하나, 즉 제로 샷 설정을 제한하는 것으로 이 연구 질문에 답하기에 충분하다고 주장한다. 제로 샷 시나리오에 초점을 맞추는 또 다른 장점은 소수의 샷 예제의 선택으로 대표되는 추가 변수를 도입하지 않는다는 것이다. 그러나, 우리는 우리의 결과가 소수의 샷 평가 설정으로 변환되는지 여부를 조사하기 위해 향후 작업을 권장한다.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Ethical And Broader Impact</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">LLM은 대중으로부터 상당한 관심을 받고 다양한 언어를 사용하는 사회 전반에 걸쳐 널리 사용되는 파괴적인 기술을 나타낸다. 따라서 서로 다른 언어를 가진 사람들에 걸쳐 기술의 민주화를 보장하는 것은 중요한 가치를 나타낼 것이다. 우리의 연구는 LLM 교육에 필요한 핵심 구성 요소를 나타내는 토큰화기를 훈련하는 동안 다국어주의를 무시하는 것이 훈련 비용 증가 및 다운스트림 성능 감소와 같은 심각한 단점을 야기하여 주요 윤리적 문제를 제기할 수 있음을 강조한다. 또한 교육 비용이 증가하면 탄소 발자국이 증가하여 환경에 영향을 미친다. 우리의 연구 결과는 이 기본 기술의 향상된 개발과 사용을 뒷받침한다.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">이 작업은 오픈GPT-X 프로젝트(프로젝트 번호 68GX21007D)를 통해 독일 연방 경제 기후 행동부(BMWK)와 독일 연방 교육 연구부 및 노르트라인 웨스트팔렌 주가 라마-연구소 포 머신, 라마R22B의 일부로 자금을 지원했으며 보조금 협정 제101135671호(TrustLLM) 및 952215호(TAILOR)에 따른 유럽 연합의 호라이즌 2020 연구 및 혁신 프로그램에 의해 지원되었다. 저자들은 JSC(Jülich Supercomputing Center)의 GCS 슈퍼컴퓨터 JUWELS와 TU Dresden의 고성능 컴퓨팅 센터 [Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH)]에서 높은 처리량 계산을 위한 시설을 제공함으로써 이 프로젝트에 자금을 지원하는 가우스 센터(www.gauss-centre.eu)에 감사한다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Javier&nbsp;Ortiz Suárez, Laurent Romary, and Benoît
Sagot. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.14618/ids-pub-10468" title="">Ungoliant: An
optimized pipeline for the generation of a very large-scale multilingual web
corpus</a>.

</span>
<span class="ltx_bibblock">In Harald Lüngen, Marc Kupietz, Piotr Bański, Adrien Barbaresi,
Simon Clematide, and Ines Pisetta, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Workshop
on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12
July 2021 (Online-Event)</em>, pages 1 – 9. Leibniz-Institut für Deutsche
Sprache, Mannheim.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</em>, 13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pages 7432–7439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al. 2020a.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chirkova and Troshin (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nadezhda Chirkova and Sergey Troshin. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=rd-G1nO-Jbq" title="">CodeBPE:
Investigating subtokenization options for large language model pretraining on
source code</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Deep Learning for Code Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">NAACL-HLT (1)</em>, pages 2924–2936. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00448" title="">Canine: Pre-training an
efficient tokenization-free encoder for language representation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</em>,
10:73–91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the AI2
reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/1803.05457.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/togethercomputer/RedPajama-Data" title="">Redpajama: An open source recipe to reproduce llama training dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel&nbsp;R.
Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock">XNLI: evaluating cross-lingual sentence representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">EMNLP</em>, pages 2475–2485. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Marneffe et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Marie-Catherine De&nbsp;Marneffe, Mandy Simons, and Judith Tonhauser. 2019.

</span>
<span class="ltx_bibblock">The commitmentbank: Investigating projection in naturally occurring
discourse.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">proceedings of Sinn und Bedeutung</em>, volume&nbsp;23, pages
107–124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolan and Brockett (2005)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;B. Dolan and Chris Brockett. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/I05-5002" title="">Automatically constructing
a corpus of sentential paraphrases</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Third International Workshop on
Paraphrasing (IWP2005)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekgren et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ariel Ekgren, Amaru&nbsp;Cuba Gyllensten, Felix Stollenwerk, Joey Öhman, Tim
Isbister, Evangelia Gogoulou, Fredrik Carlsson, Alice Heiman, Judit
Casademont, and Magnus Sahlgren. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.12987" title="">Gpt-sw3: An autoregressive
language model for the nordic languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gage (1994)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Philip Gage. 1994.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:59804030" title="">A new
algorithm for data compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The C Users Journal archive</em>, 12:23–38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy. 2020a.

</span>
<span class="ltx_bibblock">The Pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2101.00027</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yingqiang Gao, Nikola&nbsp;I. Nikolov, Yuhuang Hu, and Richard&nbsp;H.R. Hahnloser.
2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.145" title="">Character-level translation with self-attention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1591–1604, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodman (2001)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joshua Goodman. 2001.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/cs.CL/0108005v1" title="">A bit of progress in
language modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, cs.CL/0108005v1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and
Angela Fan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00474" title="">The Flores-101
Evaluation Benchmark for Low-Resource and Multilingual Machine Translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Transactions of the Association for Computational Linguistics</em>,
10:522–538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Graën, Tannon Kew, Anastassia Shaitarova, and Martin Volk. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.14618/ids-pub-9020" title="">Modelling large
parallel corpora: The zurich parallel corpus collection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 7th Workshop on Challenges in the
Management of Large Corpora (CMLC)</em>, pages 1–8. Leibniz-Institut für
Deutsche Sprache.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Graën, D.&nbsp;Batinic, and M.&nbsp;Volk. 2014.

</span>
<span class="ltx_bibblock">Cleaning the Europarl corpus for linguistic applications.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Konvens 2014</em>. Stiftung Universität Hildesheim.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hajlaoui et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Najeh Hajlaoui, David Kolovratnik, Jaakko Vaeyrynen, Ralf Steinberger, and
Dániel Varga. 2014.

</span>
<span class="ltx_bibblock">DCEP - Digital corpus of the European parliament.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proc. LREC 2014 (Language Resources and Evaluation
Conference). Reykjavik, Iceland</em>, pages 3164–3171.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George
van&nbsp;den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén
Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre.
2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf" title="">An empirical analysis of compute-optimal large language model training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>,
volume&nbsp;35, pages 30016–30030. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van&nbsp;den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and Laurent Sifre. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2203.15556" title="">Training
compute-optimal large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">CoRR</em>, abs/2203.15556.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Höfler and Piotrowski (2011)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stefan Höfler and Michael Piotrowski. 2011.

</span>
<span class="ltx_bibblock">Building corpora for the philological study of Swiss legal texts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Journal for Language Technology and Computational Linguistics</em>,
26(2):77–89.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1259" title="">PubMedQA: A
dataset for biomedical research question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2567–2577, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Koehn. 2005.

</span>
<span class="ltx_bibblock">Europarl: A parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Machine Translation Summit, volume 5</em>, pages 79––86.
Asia-Pacific Association for Machine Translation (AAMT).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1007" title="">Subword regularization:
Improving neural network translation models with multiple subword
candidates</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 66–75,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">EMNLP 2018</em>, page&nbsp;66.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D17-1082" title="">RACE: Large-scale
ReAding comprehension dataset from examinations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing</em>, pages 785–794, Copenhagen, Denmark.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levesque et&nbsp;al. (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012.

</span>
<span class="ltx_bibblock">The winograd schema challenge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Thirteenth international conference on the principles of
knowledge representation and reasoning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 9019–9052.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock">OpenSubtitles2016: Extracting large parallel corpora from movie
and tv subtitles.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 10th International Conference on Language
Resources and Evaluation (LREC-2016)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moi and Patry (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthony Moi and Nicolas Patry. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/huggingface/tokenizers" title="">HuggingFace’s
Tokenizers</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3458817.3476209" title="">Efficient
large-scale language model training on gpu clusters using megatron-lm</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis</em>, SC ’21, New York,
NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham,
Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
Fernández. 2016.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse
context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ACL (1)</em>. The Association for Computer Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La&nbsp;Malfa, Philip&nbsp;HS Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2305.15425</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and
Camacho-Collados (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and Jose Camacho-Collados. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1128" title="">WiC: the
word-in-context dataset for evaluating context-sensitive meaning
representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 1267–1273,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edoardo&nbsp;Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan
Vulić, and Anna Korhonen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.185" title="">XCOPA: A
multilingual dataset for causal commonsense reasoning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2362–2376, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roemmele et&nbsp;al. (2011)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Melissa Roemmele, Cosmin&nbsp;Adrian Bejan, and Andrew&nbsp;S Gordon. 2011.

</span>
<span class="ltx_bibblock">Choice of plausible alternatives: An evaluation of commonsense causal
reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2011 AAAI Spring Symposium Series</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna
Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.243" title="">How good is
your tokenizer? on the monolingual performance of multilingual language
models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3118–3135,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">AAAI</em>, pages 8732–8740. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic,
Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni,
François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush,
Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang,
Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh,
Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin
Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit
Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris
Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa
Adelani, and et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2211.05100" title="">BLOOM: A
176b-parameter open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schabus et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dietmar Schabus, Marcin Skowron, and Martin Trapp. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3077136.3080711" title="">One million posts: A
data set of german online discussions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 40th International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR)</em>, pages
1241–1244, Tokyo, Japan.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster and Nakajima (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Schuster and Kaisuke Nakajima. 2012.

</span>
<span class="ltx_bibblock">Japanese and korean voice search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">2012 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pages 5149–5152. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:1508.07909</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov,
Anastasia Kozlova, and Tatiana Shavrina. 2022.

</span>
<span class="ltx_bibblock">mgpt: Few-shot learners go multilingual.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2204.07580</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Socher et&nbsp;al. (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher&nbsp;D. Manning,
Andrew Ng, and Christopher Potts. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/D13-1170" title="">Recursive deep models for
semantic compositionality over a sentiment treebank</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1631–1642, Seattle, Washington, USA.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stollenwerk (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Felix Stollenwerk. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.14780" title="">Training and evaluation of a
multilingual tokenizer for gpt-sw3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Vinh&nbsp;Q Tran, Sebastian Ruder, Jai Gupta, Hyung&nbsp;Won Chung, Dara Bahri,
Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021.

</span>
<span class="ltx_bibblock">Charformer: Fast character transformers via gradient-based subword
tokenization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toraman et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cagri Toraman, Eyup&nbsp;Halit Yilmaz, Furkan Sahinuc, and Oguzhan Ozcelik. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3578707" title="">Impact of tokenization on
language models: An analysis for turkish</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ACM Trans. Asian Low-Resour. Lang. Inf. Process.</em>, 22(4).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: open and efficient foundation language models, 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">URL https://arxiv. org/abs/2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">NIPS</em>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-5446" title="">GLUE: A multi-task
benchmark and analysis platform for natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP</em>, pages 353–355,
Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v34i05.6451" title="">Neural machine
translation with byte-level subwords</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
34(05):9154–9160.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1101" title="">A broad-coverage
challenge corpus for sentence understanding through inference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir
Kale, Adam Roberts, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Transactions of the Association for Computational Linguistics</em>,
10:291–306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1382" title="">PAWS-X: A
cross-lingual adversarial dataset for paraphrase identification</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3687–3692, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yehezkel and Pinter (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shaked Yehezkel and Yuval Pinter. 2023.

</span>
<span class="ltx_bibblock">Incorporating context into subword vocabularies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">EACL</em>, pages 623–635. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer,
and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock">Megabyte: Predicting million-byte sequences with multiscale
transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2305.07185</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">ACL (1)</em>, pages 4791–4800. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek,
Mohit Bansal, and Francisco Guzman. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.amta-research.8" title="">How robust is
neural machine translation to language imbalance in multilingual tokenizer
training?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 15th biennial conference of the
Association for Machine Translation in the Americas (Volume 1: Research
Track)</em>, pages 97–116, Orlando, USA. Association for Machine Translation in
the Americas.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Corpora</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T6">
<table class="ltx_tabular ltx_align_middle" id="A1.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T6.1.1.1.1">Name</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T6.1.1.1.2">Language</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="A1.T6.1.1.1.3">#Words</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.2.2.1">Oscar</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.2.2.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.2.2.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.T6.1.3.3.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.3.3.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.3.3.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.T6.1.4.4.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.4.4.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.4.4.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.T6.1.5.5.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.5.5.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.5.5.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.T6.1.6.6.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.6.6.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.6.6.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.7.7.1">Pile</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.7.7.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.7.7.3">13.838.432</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.8.8">
<td class="ltx_td ltx_align_left" id="A1.T6.1.8.8.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.8.8.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.8.8.3">21.990.512</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.9.9">
<td class="ltx_td ltx_align_left" id="A1.T6.1.9.9.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.9.9.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.9.9.3">4.334.313.669</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.10.10">
<td class="ltx_td ltx_align_left" id="A1.T6.1.10.10.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.10.10.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.10.10.3">7.946.402</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.11.11">
<td class="ltx_td ltx_align_left" id="A1.T6.1.11.11.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.11.11.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.11.11.3">15.857.811</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.12.12.1">RedPajama</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.12.12.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.12.12.3">143.907.461</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.13.13">
<td class="ltx_td ltx_align_left" id="A1.T6.1.13.13.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.13.13.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.13.13.3">112.950.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.14.14">
<td class="ltx_td ltx_align_left" id="A1.T6.1.14.14.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.14.14.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.14.14.3">4.663.646.781</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.15.15">
<td class="ltx_td ltx_align_left" id="A1.T6.1.15.15.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.15.15.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.15.15.3">137.802.711</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.16.16">
<td class="ltx_td ltx_align_left" id="A1.T6.1.16.16.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.16.16.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.16.16.3">139.749.147</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.17.17">
<td class="ltx_td ltx_align_left" id="A1.T6.1.17.17.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.17.17.2">Code</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.17.17.3">2.052.228.788</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.18.18.1">Misc</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.18.18.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.18.18.3">600.844.912</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.19.19">
<td class="ltx_td ltx_align_left" id="A1.T6.1.19.19.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.19.19.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.19.19.3">186.934.269</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.20.20">
<td class="ltx_td ltx_align_left" id="A1.T6.1.20.20.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.20.20.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.20.20.3">1.337.030.904</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.21.21">
<td class="ltx_td ltx_align_left" id="A1.T6.1.21.21.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.21.21.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.21.21.3">19.810.753</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.22.22">
<td class="ltx_td ltx_align_left" id="A1.T6.1.22.22.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.22.22.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.22.22.3">211.147.445</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.23.23">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T6.1.23.23.1">Total</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="A1.T6.1.23.23.2"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T6.1.23.23.3">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 6:</span>언어가 있는 다국어 70B 단어 데이터셋의 개요, 샘플링된 단어 수</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T7">
<table class="ltx_tabular ltx_align_middle" id="A1.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T7.1.1.1.1">Name</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T7.1.1.1.2">Language</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="A1.T7.1.1.1.3">#Words</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.1.2.2.1">Oscar</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.1.2.2.2">EN</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T7.1.2.2.3">56.000.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.T7.1.3.3.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.3.3.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.3.3.3">4.893.724.288</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.T7.1.4.4.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.4.4.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.4.4.3">5.308.974.750</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.T7.1.5.5.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.5.5.2">Code</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.5.5.3">2.299.301.635</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.T7.1.6.6.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.6.6.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.6.6.3">1.497.999.327</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" id="A1.T7.1.7.7.1">Total</td>
<td class="ltx_td ltx_border_bb ltx_border_tt" id="A1.T7.1.7.7.2"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt" id="A1.T7.1.7.7.3">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 7:</span>Overview of the English 70B words dataset with language, number of sampled words</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">말뭉치 내 웹 문서는 Oscars<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://oscar-project.org/" title="">https://oscar-project.org/</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">Abadji et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib1" title="">2021</a>)</cite>로 구성되어 있으며, 이는 3개의 Common Crawl WET Archives (2022-27, 2022-49 및 2023-14)를 기반으로 ungoliant 파이프라인 <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/oscar-project/ungoliant" title="">https://github.com/oscar-project/ungoliant</a></span></span></span>에 의해 생성되었다.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">큐레이팅된 데이터셋은 <span class="ltx_text ltx_font_italic" id="A1.p2.1.1">The Pile</span> <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib17" title="">2020a</a>)</cite>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.2">RedPajama</span> <cite class="ltx_cite ltx_citemacro_cite">Computer (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib10" title="">2023</a>)</cite>, 컬렉션에 속하지 않는 단일 데이터셋으로 구성된다. Pile subcorpora에서 Phil Archive, PMC Abstracts, PMC Extracts, OpenWebText, NIH Exporterm, Free Law Opinions V2를 선택하였다. RedPajama에서 우리는 ArXiv, Books, Github, StackExchange, Wikipedia를 사용한다.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">나머지 데이터 세트는 다음과 같습니다.</p>
</div>
<div class="ltx_para" id="A1.p4">
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">모든 뉴스 V2.0<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://metatext.io/datasets/all-the-news-2.0" title="">https://metatext.io/datasets/all-the-news-2.0</a></span></span></span>은 2016년 1월부터 2020년 4월 1일까지 26개 이상의 다른 출판물에서 크롤링된 신문 기사의 코퍼스이다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">Bundestag - Plenarprotokolle<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bundestag.de/dokumente/protokolle/plenarprotokolle" title="">https://www.bundestag.de/dokumente/protokolle/plenarprotokolle</a></span></span></span>은 독일 연방 하원의 세션의 전사체로 구성됩니다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Bundesgerichtshof - Entscheidungen<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html" title="">https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html</a></span></span></span>은 독일 연방 법원의 판결 모음입니다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">CoStEP<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/costep/start" title="">https://pub.cl.uzh.ch/wiki/public/costep/start</a></span></span></span>은 EuroParl corpus<cite class="ltx_cite ltx_citemacro_cite">Graën et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib22" title="">2014</a>)</cite>의 정리 및 수정된 버전이다. <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib28" title="">2005</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">DCEP<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en" title="">https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en</a></span></span></span>은 CoStEP에 대한 컴패니언 코퍼스로 유럽의회에서 발행한 문서를 담고 있다. <cite class="ltx_cite ltx_citemacro_cite">Hajlaoui et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib23" title="">2014</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p" id="A1.I1.i6.p1.1">DNB Dissertations<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html" title="">https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html</a></span></span></span>은 Deutsche Nationalbibliothek의 학위 논문 모음이다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="A1.I1.i7.p1">
<p class="ltx_p" id="A1.I1.i7.p1.1">MAREC/IREC<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://researchdata.tuwien.ac.at/records/2zx6e-5pr64" title="">https://researchdata.tuwien.ac.at/records/2zx6e-5pr64</a></span></span></span>: The MAtrixware REsearch Collection/The Information retrieval facility Research Collection은 EP, WO, US, JP 특허청으로부터 1,900만 개가 넘는 문서의 특허 코퍼스이다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="A1.I1.i8.p1">
<p class="ltx_p" id="A1.I1.i8.p1.1">Medi-Notice<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice" title="">https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice</a></span></span></span>은 취리히 병렬 코퍼스 컬렉션의 일부입니다. 스위스 치료 제품 기관에서 발행한 의약품 및 의약품에 대한 정보 전단지에서 수집한 다국어 코퍼스이다. <cite class="ltx_cite ltx_citemacro_cite">Graën et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib21" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="A1.I1.i9.p1">
<p class="ltx_p" id="A1.I1.i9.p1.1">Swiss Policy<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus" title="">https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus</a></span></span></span> contains documents of the Swiss Legislation Corpus <cite class="ltx_cite ltx_citemacro_cite">Höfler and Piotrowski (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib26" title="">2011</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="A1.I1.i10.p1">
<p class="ltx_p" id="A1.I1.i10.p1.1">OpenSubtitles 2018<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opus.nlpl.eu/OpenSubtitles-v2018.php" title="">https://opus.nlpl.eu/OpenSubtitles-v2018.php</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.opensubtitles.org/de/index.cgi" title="">https://www.opensubtitles.org/de/index.cgi</a></span></span></span>은 번역된 영화 자막의 모음이다. <cite class="ltx_cite ltx_citemacro_cite">Lison and Tiedemann (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib34" title="">2016</a>)</cite></p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Tokenizer</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">실험에서는 <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">Huggingface tokenizer</span> library <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib35" title="">2023</a>)</cite>와 <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">SentencePiece</span> library <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>)</cite>에 초점을 맞추었다. <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2.T8" title="Table 8 ‣ Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>에 달리 명시되지 않은 경우 SentencePiece 라이브러리의 표준 설정을 사용합니다. HuggingFace tokenizer 라이브러리의 경우 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2.T9" title="Table 9 ‣ Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a>는 표준 값에서 벗어난 위치를 보여줍니다.</p>
</div>
<figure class="ltx_table" id="A2.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T8.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T8.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T8.9.10.1.1">Hyper-Parameter</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T8.9.10.1.2">Value(s)</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T8.1.1.2">model_type</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.1.1.1">Unigram <math alttext="|" class="ltx_Math" display="inline" id="A2.T8.1.1.1.m1.1"><semantics id="A2.T8.1.1.1.m1.1a"><mo fence="false" id="A2.T8.1.1.1.m1.1.1" stretchy="false" xref="A2.T8.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T8.1.1.1.m1.1b"><ci id="A2.T8.1.1.1.m1.1.1.cmml" xref="A2.T8.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.1.1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T8.1.1.1.m1.1d">|</annotation></semantics></math> BPE</td>
</tr>
<tr class="ltx_tr" id="A2.T8.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.2.2.2">vocab_size</th>
<td class="ltx_td ltx_align_left" id="A2.T8.2.2.1">33k <math alttext="|" class="ltx_Math" display="inline" id="A2.T8.2.2.1.m1.1"><semantics id="A2.T8.2.2.1.m1.1a"><mo fence="false" id="A2.T8.2.2.1.m1.1.1" stretchy="false" xref="A2.T8.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T8.2.2.1.m1.1b"><ci id="A2.T8.2.2.1.m1.1.1.cmml" xref="A2.T8.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.2.2.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T8.2.2.1.m1.1d">|</annotation></semantics></math>50k</td>
</tr>
<tr class="ltx_tr" id="A2.T8.3.3">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T8.3.3.2"></th>
<td class="ltx_td ltx_align_left" id="A2.T8.3.3.1">82k <math alttext="|" class="ltx_Math" display="inline" id="A2.T8.3.3.1.m1.1"><semantics id="A2.T8.3.3.1.m1.1a"><mo fence="false" id="A2.T8.3.3.1.m1.1.1" stretchy="false" xref="A2.T8.3.3.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T8.3.3.1.m1.1b"><ci id="A2.T8.3.3.1.m1.1.1.cmml" xref="A2.T8.3.3.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.3.3.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T8.3.3.1.m1.1d">|</annotation></semantics></math> 100k</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.11.2.1">character_coverage</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.11.2.2">0.9999</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.12.3.1">split_by_number</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.12.3.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.13.4.1">allow_whitespace_only</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.13.4.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.14.5.1">add_dummy_prefix</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.14.5.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.5.5.3">user_symbols</th>
<td class="ltx_td ltx_align_left" id="A2.T8.5.5.2">&lt;s&gt;,&lt;/s&gt;,&lt;pad&gt;,</td>
</tr>
<tr class="ltx_tr" id="A2.T8.7.7">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T8.7.7.3"></th>
<td class="ltx_td ltx_align_left" id="A2.T8.7.7.2">&lt;eod&gt;, &lt;ph_1&gt;,</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.9">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T8.9.9.3"></th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.9.2">…, &lt;ph_255&gt;</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.15.6.1">byte_fallback</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.15.6.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.16.7.1">max_sentence_length</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.16.7.2">4192</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.17.8.1">normalization_rule_name</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.17.8.2">NFKC</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.18.9.1">train_large_corpus</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.18.9.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.19.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.19.10.1">remove_extra_whitespaces</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.19.10.2">False</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.20.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T8.9.20.11.1">split_by_whitespace</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.9.20.11.2">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 8:</span>Overview of the SentencePiece options that we used for the training of our tokenizers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T9.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T9.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T9.2.3.1.1">Hyper-Parameter</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T9.2.3.1.2">Value(s)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.2.4.2.1">model_type</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.2.4.2.2">BPE</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.2">vocab_size</th>
<td class="ltx_td ltx_align_left" id="A2.T9.1.1.1">33k <math alttext="|" class="ltx_Math" display="inline" id="A2.T9.1.1.1.m1.1"><semantics id="A2.T9.1.1.1.m1.1a"><mo fence="false" id="A2.T9.1.1.1.m1.1.1" stretchy="false" xref="A2.T9.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T9.1.1.1.m1.1b"><ci id="A2.T9.1.1.1.m1.1.1.cmml" xref="A2.T9.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.1.1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T9.1.1.1.m1.1d">|</annotation></semantics></math> 50k</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T9.2.2.2"></th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.2.1">82k <math alttext="|" class="ltx_Math" display="inline" id="A2.T9.2.2.1.m1.1"><semantics id="A2.T9.2.2.1.m1.1a"><mo fence="false" id="A2.T9.2.2.1.m1.1.1" stretchy="false" xref="A2.T9.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T9.2.2.1.m1.1b"><ci id="A2.T9.2.2.1.m1.1.1.cmml" xref="A2.T9.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.2.2.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T9.2.2.1.m1.1d">|</annotation></semantics></math> 100k</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.5.3.1">limit_alphabet</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.5.3.2">512</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.6.4.1">nfkc_normalizer</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.6.4.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.7.5.1">lowercase_normalizer</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.7.5.2">False</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.8.6.1">strip_accents_normalizer</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.8.6.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T9.2.9.7.1">pre_tokenizer</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T9.2.9.7.2">ByteLevel, Digits</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 9:</span>Overview of the Huggingface options that we used for the training of our tokenizers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>LLM Architecture and Hyperparameters</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">2.6B 파라미터 모델의 학습 구조와 관련하여 GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib4" title="">2020a</a>)</cite>의 아키텍처를 면밀히 따랐다. 사용된 아키텍처 세부사항 및 하이퍼파라미터의 개요는 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3.T10" title="Table 10 ‣ Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>에서 주어진다.</p>
</div>
<figure class="ltx_table" id="A3.T10">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T10.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T10.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T10.2.3.1.1">Hyper-Parameter</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T10.2.3.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T10.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T10.2.4.1.1"># Hidden Dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T10.2.4.1.2">2560</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.5.2.1"># Layers</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.5.2.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.6.3.1"># Attention-Heads</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.6.3.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.7.4.1">Sequence-Length</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.7.4.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.8.5.1">Optimizer</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.8.5.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.1.1.1">Adam<math alttext="-\beta_{1}" class="ltx_Math" display="inline" id="A3.T10.1.1.1.m1.1"><semantics id="A3.T10.1.1.1.m1.1a"><mrow id="A3.T10.1.1.1.m1.1.1" xref="A3.T10.1.1.1.m1.1.1.cmml"><mo id="A3.T10.1.1.1.m1.1.1a" xref="A3.T10.1.1.1.m1.1.1.cmml">−</mo><msub id="A3.T10.1.1.1.m1.1.1.2" xref="A3.T10.1.1.1.m1.1.1.2.cmml"><mi id="A3.T10.1.1.1.m1.1.1.2.2" xref="A3.T10.1.1.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T10.1.1.1.m1.1.1.2.3" xref="A3.T10.1.1.1.m1.1.1.2.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T10.1.1.1.m1.1b"><apply id="A3.T10.1.1.1.m1.1.1.cmml" xref="A3.T10.1.1.1.m1.1.1"><minus id="A3.T10.1.1.1.m1.1.1.1.cmml" xref="A3.T10.1.1.1.m1.1.1"></minus><apply id="A3.T10.1.1.1.m1.1.1.2.cmml" xref="A3.T10.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T10.1.1.1.m1.1.1.2.1.cmml" xref="A3.T10.1.1.1.m1.1.1.2">subscript</csymbol><ci id="A3.T10.1.1.1.m1.1.1.2.2.cmml" xref="A3.T10.1.1.1.m1.1.1.2.2">𝛽</ci><cn id="A3.T10.1.1.1.m1.1.1.2.3.cmml" type="integer" xref="A3.T10.1.1.1.m1.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.1.1.1.m1.1c">-\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.T10.1.1.1.m1.1d">- italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A3.T10.1.1.2">0.9</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.2.1">Adam<math alttext="-\beta_{2}" class="ltx_Math" display="inline" id="A3.T10.2.2.1.m1.1"><semantics id="A3.T10.2.2.1.m1.1a"><mrow id="A3.T10.2.2.1.m1.1.1" xref="A3.T10.2.2.1.m1.1.1.cmml"><mo id="A3.T10.2.2.1.m1.1.1a" xref="A3.T10.2.2.1.m1.1.1.cmml">−</mo><msub id="A3.T10.2.2.1.m1.1.1.2" xref="A3.T10.2.2.1.m1.1.1.2.cmml"><mi id="A3.T10.2.2.1.m1.1.1.2.2" xref="A3.T10.2.2.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T10.2.2.1.m1.1.1.2.3" xref="A3.T10.2.2.1.m1.1.1.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T10.2.2.1.m1.1b"><apply id="A3.T10.2.2.1.m1.1.1.cmml" xref="A3.T10.2.2.1.m1.1.1"><minus id="A3.T10.2.2.1.m1.1.1.1.cmml" xref="A3.T10.2.2.1.m1.1.1"></minus><apply id="A3.T10.2.2.1.m1.1.1.2.cmml" xref="A3.T10.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T10.2.2.1.m1.1.1.2.1.cmml" xref="A3.T10.2.2.1.m1.1.1.2">subscript</csymbol><ci id="A3.T10.2.2.1.m1.1.1.2.2.cmml" xref="A3.T10.2.2.1.m1.1.1.2.2">𝛽</ci><cn id="A3.T10.2.2.1.m1.1.1.2.3.cmml" type="integer" xref="A3.T10.2.2.1.m1.1.1.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.2.2.1.m1.1c">-\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.T10.2.2.1.m1.1d">- italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.2.2">0.9</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.9.6.1">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.9.6.2">1.6e-4</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.10.7.1">Learning rate decay</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.10.7.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.11.8.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.11.8.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.12.9.1">FlashAttention</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.12.9.2">2.0</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T10.2.13.10.1">Position-Embeddings</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T10.2.13.10.2">Rotary</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 10:</span>우리가 훈련에 사용한 LLM 하이퍼파라미터의 개요.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">모델을 훈련하기 위해 Megatron-LM<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/Megatron-LM" title="">https://github.com/NVIDIA/Megatron-LM</a>의 포크를 사용했다.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Intrinsic Tokenizer Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">동일한 시소러스에서 동일한 알고리즘의 중첩을 연구하는 것 외에도 알고리즘 전반에 걸친 어휘 중첩에 관심이 있었고 시소러스는 <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.F5" title="Figure 5 ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>를 참조한다. 우리가 관찰할 수 있는 것은 다국어 어휘와 영어 어휘는 증가하는 어휘 크기에 걸쳐 유사하게 유지되는 24%와 34% 사이의 다소 작은 중첩을 가지고 있다는 것이다. 알고리즘 전반에 걸쳐, 우리는 SentencePiece의 Unigram과 BPE가 Huggingface의 Unigram과 BPE보다 약간 더 높은 중첩을 갖는다는 것을 알 수 있다. 이것은 라이브러리별 전처리 단계와 더 유사한 하이퍼파라미터 때문일 수 있다고 생각한다.</p>
</div>
<figure class="ltx_figure" id="A4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="553" id="A4.F5.1.g1" src="https://arxiv.org/html/2310.08754v4/x8.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="553" id="A4.F5.2.g1" src="https://arxiv.org/html/2310.08754v4/x9.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="761" id="A4.F5.3.g1" src="https://arxiv.org/html/2310.08754v4/x10.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="761" id="A4.F5.4.g1" src="https://arxiv.org/html/2310.08754v4/x11.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 5:</span>Vocabulary overlap between the examined tokenizers</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Computational Costs Per Word During Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.T11" title="Table 11 ‣ D.1 Computational Costs Per Word During Training ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>는 포워드 패스 및 백워드 패스 동안 단어를 처리하기 위한 평균 연산 훈련 비용을 나타낸다.</p>
</div>
<figure class="ltx_table" id="A4.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T11.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T11.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A4.T11.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A4.T11.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.3">Non-English</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.4">English</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.5">German</th>
</tr>
<tr class="ltx_tr" id="A4.T11.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.2.2.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A4.T11.1.2.2.2">GPT-2-50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T11.1.2.2.3">3.87</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T11.1.2.2.4">2.58</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T11.1.2.2.5">4.59</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T11.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.3.1.1" rowspan="12"><span class="ltx_text" id="A4.T11.1.3.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A4.T11.1.3.1.1.1.1" style="width:6.8pt;height:14.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-3.74pt,-3.74pt) rotate(-90deg) ;">
<span class="ltx_p" id="A4.T11.1.3.1.1.1.1.1">EN</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.3.1.2">BPE-HF-33</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.3">3.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.3.1.4.1">2.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.5">4.52</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.4.2.1">BPE-HF-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.2">3.79</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.3">2.38</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.4">4.45</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.5.3.1">BPE-HF-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.2">3.88</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.3">2.55</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.4">4.51</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.6.4.1">BPE-HF-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.2">3.96</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.3">2.67</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.4">4.58</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.7.5.1">BPE-SP-33</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.2">3.86</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.3">2.37</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.4">4.66</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.8.6.1">BPE-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.8.6.2">3.89</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.8.6.3">2.42</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.8.6.4">4.68</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.9.7.1">BPE-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.9.7.2">4.02</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.9.7.3">2.59</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.9.7.4">4.78</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.10.8.1">BPE-SP-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.2">4.11</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.3">2.71</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.4">4.84</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.11.9.1">UNI-SP-32</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.2">4.01</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.3">2.36</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.4">4.73</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.12.10.1">UNI-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.2">4.02</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.3">2.42</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.4">4.75</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.13.11.1">UNI-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.2">4.12</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.3">2.59</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.4">4.83</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.14.12.1">UNI-SP-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.14.12.2">4.21</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.14.12.3">2.71</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.14.12.4">4.88</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A4.T11.1.15.13.1" rowspan="12"><span class="ltx_text" id="A4.T11.1.15.13.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A4.T11.1.15.13.1.1.1" style="width:6.8pt;height:32.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.9pt;transform:translate(-13.04pt,-13.04pt) rotate(-90deg) ;">
<span class="ltx_p" id="A4.T11.1.15.13.1.1.1.1">MULTI</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.15.13.2">BPE-HF-33</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.3">2.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.4">2.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.5">3.04</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.16.14.1">BPE-HF-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.2">2.7</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.3">2.5</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.4">3.01</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.17.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.17.15.1">BPE-HF-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.2">2.8</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.3">2.65</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.4">3.09</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.18.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.18.16.1">BPE-HF-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.2">2.88</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.3">2.76</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.4">3.17</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.19.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.19.17.1">BPE-SP-33</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.19.17.2">2.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.19.17.3">2.55</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.19.17.4">2.99</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.20.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.20.18.1">BPE-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.20.18.2">2.67</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.20.18.3">2.57</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.20.18.4">2.95</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.21.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.21.19.1">BPE-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.21.19.2">2.76</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.21.19.3">2.72</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.21.19.4">3.03</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.22.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.22.20.1">BPE-SP-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.22.20.2">2.85</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.22.20.3">2.82</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.22.20.4">3.1</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.23.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.23.21.1">UNI-SP-33</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.23.21.2">2.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.23.21.3">2.55</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.23.21.4">2.94</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.24.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.24.22.1">UNI-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.24.22.2"><span class="ltx_text ltx_font_bold" id="A4.T11.1.24.22.2.1">2.66</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.24.22.3">2.58</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.24.22.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.24.22.4.1">2.91</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.25.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.25.23.1">UNI-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.25.23.2">2.76</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.25.23.3">2.73</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.25.23.4">2.99</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.26.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A4.T11.1.26.24.1">UNI-SP-100</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.26.24.2">2.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.26.24.3">2.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.26.24.4">3.07</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11:</span>Computational training costs per word (GFLOPs) for different tokenizers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Infrastructure &amp; Computational Costs</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">NVIDIA A100 GPU에서 26개의 2.6B 매개변수 모델을 각각 훈련했으며 각 모델의 훈련은 최대 2304 GPU 시간이 소요되었다. 따라서 총 훈련 비용은 <math alttext="\approx 59.000" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mi id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml"></mi><mo id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">≈</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">59.000</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><approx id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">absent</csymbol><cn id="A5.p1.1.m1.1.1.3.cmml" type="float" xref="A5.p1.1.m1.1.1.3">59.000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">\approx 59.000</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">≈ 59.000</annotation></semantics></math> GPU 시간에 달했다.</p>
</div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
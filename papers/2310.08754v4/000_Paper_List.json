{
    "2310.08754v4": {
        "paper_id": "2310.08754v4",
        "abs_url": "https://arxiv.org/abs/2310.08754v4",
        "pdf_url": "https://arxiv.org/pdf/2310.08754v4.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2310.08754v4_Tokenizer_Choice_For_LLM_Training_Negligible_or_Crucial?.pdf",
        "title": "Tokenizer Choice For LLM Training: Negligible or Crucial?",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Mehdi Ali",
            "Michael Fromm",
            "Klaudia Thellmann",
            "Richard Rutmann",
            "Max L\u00fcbbering",
            "Johannes Leveling",
            "Katrin Klug",
            "Jan Ebert",
            "Niclas Doll",
            "Jasper Schulze Buschhoff",
            "Charvi Jain",
            "Alexander Arno Weber",
            "Lena Jurkschat",
            "Hammam Abdelwahab",
            "Chelsea John",
            "Pedro Ortiz Suarez",
            "Malte Ostendorff",
            "Samuel Weinbach",
            "Rafet Sifa",
            "Stefan Kesselheim",
            "Nicolas Flores-Herr"
        ],
        "abstract": "The recent success of Large Language Models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance and training costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model's downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-centric tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/tokenizer-choice-for-llm-training-negligible",
        "bibtex": "@misc{ali2024tokenizer,\n      title={Tokenizer Choice For LLM Training: Negligible or Crucial?}, \n      author={Mehdi Ali and Michael Fromm and Klaudia Thellmann and Richard Rutmann and Max L\u00fcbbering and Johannes Leveling and Katrin Klug and Jan Ebert and Niclas Doll and Jasper Schulze Buschhoff and Charvi Jain and Alexander Arno Weber and Lena Jurkschat and Hammam Abdelwahab and Chelsea John and Pedro Ortiz Suarez and Malte Ostendorff and Samuel Weinbach and Rafet Sifa and Stefan Kesselheim and Nicolas Flores-Herr},\n      year={2024},\n      eprint={2310.08754},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}"
    }
}
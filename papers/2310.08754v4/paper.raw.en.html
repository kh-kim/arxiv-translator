<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Tokenizer Choice For LLM Training: Negligible or Crucial?</title>
<!--Generated on Sun Mar 17 14:15:31 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2310.08754v4/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2310.08754v4">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2310.08754v4">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2310.08754v4" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S1" title="1 Introduction ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2" title="2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1" title="2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Tokenization Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1.SSS0.Px1" title="Word Tokenization. ‣ 2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Word Tokenization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1.SSS0.Px2" title="Subword tokenization. ‣ 2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Subword tokenization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS1.SSS0.Px3" title="Character Tokenization. ‣ 2.1 Tokenization Approaches ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Character Tokenization.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS2" title="2.2 Tokenizers in Transformers Models ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tokenizers in Transformers Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS2.SSS0.Px1" title="Tokenizers in Encoder Models ‣ 2.2 Tokenizers in Transformers Models ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Tokenizers in Encoder Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S2.SS2.SSS0.Px2" title="Tokenizers in Decoder Models ‣ 2.2 Tokenizers in Transformers Models ‣ 2 Related Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Tokenizers in Decoder Models</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3" title="3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS1" title="3.1 Data ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS2" title="3.2 Tokenizer ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS3" title="3.3 Models ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.SS4" title="3.4 Evaluation ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4" title="4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Intrinsic Tokenizer Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS1" title="4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Fertility &amp; Parity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Vocabulary Overlap</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5" title="5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Extrinsic Tokenizer Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2" title="5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Downstream Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS0.Px1" title="Monolingual Tokenizer ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Monolingual Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS0.Px2" title="Multilingual Tokenizer ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title">Multilingual Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS1" title="5.2.1 Impact of the Tokenizer Library ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Impact of the Tokenizer Library</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS2" title="5.2.2 Impact of the Tokenizer Algorithm ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Impact of the Tokenizer Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS3" title="5.2.3 Impact of the Tokenizer Vocabulary ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span>Impact of the Tokenizer Vocabulary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Computational Costs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S6" title="6 Correlation Between Intrinsic And Extrinsic Tokenizer Performance ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Correlation Between Intrinsic And Extrinsic Tokenizer Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S7" title="7 Conclusion &amp; Future Work ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion &amp; Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S8" title="8 Limitations ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S9" title="9 Ethical And Broader Impact ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Ethical And Broader Impact</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A1" title="Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Corpora</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2" title="Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3" title="Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>LLM Architecture and Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4" title="Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Intrinsic Tokenizer Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.SS1" title="D.1 Computational Costs Per Word During Training ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Computational Costs Per Word During Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A5" title="Appendix E Infrastructure &amp; Computational Costs ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Infrastructure &amp; Computational Costs</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC Zero</div><div id="watermark-tr">arXiv:2310.08754v4 [cs.LG] 17 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Tokenizer Choice For LLM Training: Negligible or Crucial?</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mehdi Ali<sup class="ltx_sup" id="id4.4.id1">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Michael Fromm<sup class="ltx_sup" id="id5.5.id2">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><ci id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Klaudia Thellmann<sup class="ltx_sup" id="id6.6.id3">3</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mo id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Richard Rutmann<sup class="ltx_sup" id="id7.7.id4">1,2</sup>, Max Lübbering<sup class="ltx_sup" id="id8.8.id5">1,2</sup>, Johannes Leveling<sup class="ltx_sup" id="id9.9.id6">1</sup>, Katrin Klug<sup class="ltx_sup" id="id10.10.id7">1</sup>, Jan Ebert<sup class="ltx_sup" id="id11.11.id8">4</sup>, 
<br class="ltx_break">Niclas Doll<sup class="ltx_sup" id="id12.12.id9">1</sup>, Jasper Schulze Buschhoff<sup class="ltx_sup" id="id13.13.id10">1</sup>, Charvi Jain<sup class="ltx_sup" id="id14.14.id11">1,2</sup>, Alexander Arno Weber<sup class="ltx_sup" id="id15.15.id12">1,2</sup>, 
<br class="ltx_break">Lena Jurkschat<sup class="ltx_sup" id="id16.16.id13">3</sup>, Hammam Abdelwahab<sup class="ltx_sup" id="id17.17.id14">1</sup>
Chelsea John<sup class="ltx_sup" id="id18.18.id15">4</sup>, Pedro Ortiz Suarez<sup class="ltx_sup" id="id19.19.id16">5</sup>, Malte Ostendorff<sup class="ltx_sup" id="id20.20.id17">5</sup>
<br class="ltx_break">Samuel Weinbach<sup class="ltx_sup" id="id21.21.id18">6</sup>, Rafet Sifa<sup class="ltx_sup" id="id22.22.id19">1</sup>, Stefan Kesselheim<sup class="ltx_sup" id="id23.23.id20">4</sup>, Nicolas Flores-Herr<sup class="ltx_sup" id="id24.24.id21">1</sup>
<br class="ltx_break">
<br class="ltx_break"><sup class="ltx_sup" id="id25.25.id22">1</sup>Fraunhofer IAIS, <sup class="ltx_sup" id="id26.26.id23">2</sup>Lamarr Institute, <sup class="ltx_sup" id="id27.27.id24">3</sup>TU-Dresden, <sup class="ltx_sup" id="id28.28.id25">4</sup>FZ Jülich, <sup class="ltx_sup" id="id29.29.id26">5</sup>DFKI, <sup class="ltx_sup" id="id30.30.id27">6</sup>Aleph Alpha
<span class="ltx_note ltx_role_thanks" id="id31.31.id28"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>†Equal contribution.</span></span></span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id32.id1">The recent success of <span class="ltx_glossaryref" title="">Large Language Models (LLMs)</span> has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot.
Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6 B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model’s downstream performance and training costs.
In particular, we find that the common tokenizer evaluation metrics <span class="ltx_text ltx_font_italic" id="id32.id1.1">fertility</span> and <span class="ltx_text ltx_font_italic" id="id32.id1.2">parity</span> are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model’s downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English.
While English-centric tokenizers have been applied to the training of multi-lingual <span class="ltx_glossaryref" title="">LLMs</span> in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Tokenizer Choice For LLM Training: Negligible or Crucial?</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.3"><span class="ltx_text" id="p2.3.3" style="width:433.6pt;"><span class="ltx_text" id="p2.3.3.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.3.3.3.3">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="p2.3.3.3.3.3.3.3">
Mehdi Ali<sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.1">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="p2.1.1.1.1.1.1.1.m1.1"><semantics id="p2.1.1.1.1.1.1.1.m1.1a"><msup id="p2.1.1.1.1.1.1.1.m1.1.1" xref="p2.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="p2.1.1.1.1.1.1.1.m1.1.1a" xref="p2.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="p2.1.1.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="p2.1.1.1.1.1.1.1.m1.1b"><apply id="p2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p2.1.1.1.1.1.1.1.m1.1.1"><ci id="p2.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="p2.1.1.1.1.1.1.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.1.1.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="p2.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Michael Fromm<sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.2">1,2</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="p2.2.2.2.2.2.2.2.m2.1"><semantics id="p2.2.2.2.2.2.2.2.m2.1a"><msup id="p2.2.2.2.2.2.2.2.m2.1.1" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p2.2.2.2.2.2.2.2.m2.1.1a" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mo id="p2.2.2.2.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="p2.2.2.2.2.2.2.2.m2.1b"><apply id="p2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1"><ci id="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.2.2.2.2.2.2.2.m2.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="p2.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>, Klaudia Thellmann<sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.3">3</sup> <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="p2.3.3.3.3.3.3.3.m3.1"><semantics id="p2.3.3.3.3.3.3.3.m3.1a"><msup id="p2.3.3.3.3.3.3.3.m3.1.1" xref="p2.3.3.3.3.3.3.3.m3.1.1.cmml"><mi id="p2.3.3.3.3.3.3.3.m3.1.1a" xref="p2.3.3.3.3.3.3.3.m3.1.1.cmml"></mi><mo id="p2.3.3.3.3.3.3.3.m3.1.1.1" mathvariant="normal" xref="p2.3.3.3.3.3.3.3.m3.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="p2.3.3.3.3.3.3.3.m3.1b"><apply id="p2.3.3.3.3.3.3.3.m3.1.1.cmml" xref="p2.3.3.3.3.3.3.3.m3.1.1"><ci id="p2.3.3.3.3.3.3.3.m3.1.1.1.cmml" xref="p2.3.3.3.3.3.3.3.m3.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.3.3.3.3.3.3.3.m3.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="p2.3.3.3.3.3.3.3.m3.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p2.3.3.3.3.4.1">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.4.1.1">Richard Rutmann<sup class="ltx_sup" id="p2.3.3.3.3.4.1.1.1">1,2</sup>, Max Lübbering<sup class="ltx_sup" id="p2.3.3.3.3.4.1.1.2">1,2</sup>, Johannes Leveling<sup class="ltx_sup" id="p2.3.3.3.3.4.1.1.3">1</sup>, Katrin Klug<sup class="ltx_sup" id="p2.3.3.3.3.4.1.1.4">1</sup>, Jan Ebert<sup class="ltx_sup" id="p2.3.3.3.3.4.1.1.5">4</sup>,</span></span>
<span class="ltx_tr" id="p2.3.3.3.3.5.2">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.5.2.1">Niclas Doll<sup class="ltx_sup" id="p2.3.3.3.3.5.2.1.1">1</sup>, Jasper Schulze Buschhoff<sup class="ltx_sup" id="p2.3.3.3.3.5.2.1.2">1</sup>, Charvi Jain<sup class="ltx_sup" id="p2.3.3.3.3.5.2.1.3">1,2</sup>, Alexander Arno Weber<sup class="ltx_sup" id="p2.3.3.3.3.5.2.1.4">1,2</sup>,</span></span>
<span class="ltx_tr" id="p2.3.3.3.3.6.3">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.6.3.1">Lena Jurkschat<sup class="ltx_sup" id="p2.3.3.3.3.6.3.1.1">3</sup>, Hammam Abdelwahab<sup class="ltx_sup" id="p2.3.3.3.3.6.3.1.2">1</sup>
Chelsea John<sup class="ltx_sup" id="p2.3.3.3.3.6.3.1.3">4</sup>, Pedro Ortiz Suarez<sup class="ltx_sup" id="p2.3.3.3.3.6.3.1.4">5</sup>, Malte Ostendorff<sup class="ltx_sup" id="p2.3.3.3.3.6.3.1.5">5</sup></span></span>
<span class="ltx_tr" id="p2.3.3.3.3.7.4">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.7.4.1">Samuel Weinbach<sup class="ltx_sup" id="p2.3.3.3.3.7.4.1.1">6</sup>, Rafet Sifa<sup class="ltx_sup" id="p2.3.3.3.3.7.4.1.2">1</sup>, Stefan Kesselheim<sup class="ltx_sup" id="p2.3.3.3.3.7.4.1.3">4</sup>, Nicolas Flores-Herr<sup class="ltx_sup" id="p2.3.3.3.3.7.4.1.4">1</sup></span></span>
<span class="ltx_tr" id="p2.3.3.3.3.8.5">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.8.5.1"><sup class="ltx_sup" id="p2.3.3.3.3.8.5.1.1">1</sup>Fraunhofer IAIS, <sup class="ltx_sup" id="p2.3.3.3.3.8.5.1.2">2</sup>Lamarr Institute, <sup class="ltx_sup" id="p2.3.3.3.3.8.5.1.3">3</sup>TU-Dresden, <sup class="ltx_sup" id="p2.3.3.3.3.8.5.1.4">4</sup>FZ Jülich, <sup class="ltx_sup" id="p2.3.3.3.3.8.5.1.5">5</sup>DFKI, <sup class="ltx_sup" id="p2.3.3.3.3.8.5.1.6">6</sup>Aleph Alpha
<span class="ltx_note ltx_role_thanks" id="p2.3.3.3.3.8.5.1.7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>†Equal contribution.</span></span></span></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_glossaryref" title="">LLMs</span> have shown impressive capabilities in many downstream tasks in a zero/few-shot setting such as summarization, reading comprehension, translation, and commonsense reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib5" title="">2020b</a>); Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib55" title="">2023</a>)</cite>.
To train a LLM, the currently established approach is to employ a tokenizer that splits the training documents into tokens where a token represents a word&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib2" title="">2000</a>)</cite>, a sub-word&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib47" title="">2012</a>); Sennrich et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib48" title="">2015</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib58" title="">2020</a>)</cite>, or a single character&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib18" title="">2020b</a>)</cite>, and each token is represented in the model by an embedding vector that can be further processed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The quality of a tokenizer can be assessed <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">intrinsically</span> and <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">extrinsically</span>. An intrinsic evaluation solely addresses the characteristics of tokenizers and their generated output in isolation, whereas the extrinsic evaluation measures the impact of the tokenizer on a downstream component, e.g., the <span class="ltx_glossaryref" title="">Large Language Model (LLM)</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">While many different tokenization approaches have been proposed, ranging from character-based to word-based methods, the potential impact of different tokenizers is underexplored w.r.t. <span class="ltx_glossaryref" title="">LLMs</span>, especially in the context of multilingual <span class="ltx_glossaryref" title="">LLMs</span>.
Recent work proposed by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Petrov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib38" title="">2023</a>)</cite> demonstrates that carelessly designed tokenizers applied to the training of multilingual <span class="ltx_glossaryref" title="">LLMs</span> result in severe inequalities and limitations across languages.
Text passages translated into different languages resulted in tokenized sequences that differ in length up to a factor of 15, affecting inference costs and latency during inference.
Furthermore, it is known that the learning of long-range dependencies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib56" title="">2017</a>)</cite>, is an essential property for effectively learning transformer-based <span class="ltx_glossaryref" title="">LLMs</span>.
Given a fixed sequence length, learning to relate words far apart in the input text is impossible for languages whose text is excessively fragmented by the tokenizer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite the importance of tokenizers and the potentially severe impact of poorly performing tokenizers, there exists no extensive study so far that holistically investigates the intrinsic and extrinsic tokenizer performance in a monolingual and multilingual setting with a focus on decoder-only models, which represent the backbone of current <span class="ltx_glossaryref" title="">LLMs</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we address this gap and conduct an extensive study in which we measure the impact of the tokenizer on the model performance.
In particular, we make the following contributions:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We conduct a study investigating the intrinsic tokenizer performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We conduct a study investigating the extrinsic tokenizer performance, i.e., the impact of the tokenizer on the model’s downstream performance.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We investigate whether a correlation between the intrinsic and the extrinsic tokenizer performance exists.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section provides an overview of tokenization algorithms and their usage in encoder- and decoder-only transformer models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Tokenization Approaches</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Word Tokenization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">The most basic tokenization approach is the splitting of sequences based on white spaces and considering each word as a token&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib2" title="">2000</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Subword tokenization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">This class of algorithms subsumes all data-driven tokenization approaches which can decompose words into subwords/multiple tokens and currently represent the established tokenization approach upon which <span class="ltx_glossaryref" title="">LLMs</span> rely&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>); Petrov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib38" title="">2023</a>)</cite>.
Because subword tokenizers decompose words into subwords, they can process out-of-vocabulary words by merging subwords from the vocabulary&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>)</cite>.
Examples of popular subword tokenizers are WordPiece&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib47" title="">2012</a>)</cite>, BPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gage (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib16" title="">1994</a>); Sennrich et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib48" title="">2015</a>)</cite>, <span class="ltx_glossaryref" title="">Byte-Level BPE (BBPE)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib58" title="">2020</a>)</cite>, and Unigram&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kudo (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib29" title="">2018</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Character Tokenization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">Tokenization can also be performed on a character level or based on UTF-8 bytes.
However, this results in an increased sequence length, which becomes computationally expensive in the transformer architecture, the current predominated architecture for <span class="ltx_glossaryref" title="">LLMs</span> due to the quadratic complexity of the self-attention layer in the sequence length&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib56" title="">2017</a>)</cite>.
Though, several approaches have been proposed to address this limitation <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib18" title="">2020b</a>); Tay et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib52" title="">2021</a>); Xue et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib60" title="">2022</a>); Clark et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib8" title="">2022</a>); Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib63" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tokenizers in Transformers Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Encoder Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Most research on tokenization has been conducted on encoder models.
<cite class="ltx_cite ltx_citemacro_citet">Rust et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib43" title="">2021</a>)</cite> investigated whether the tokenizer choice impacts the downstream performance of multi- and monolingual BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib13" title="">2018</a>)</cite> models.
<cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib65" title="">2022</a>)</cite> showed that better machine translation performance is often obtained when languages are equally sampled during the tokenizer training.
<cite class="ltx_cite ltx_citemacro_citet">Toraman et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib53" title="">2023</a>)</cite> trained several medium-sized language models for Turkish and suggested that different subword tokenizers perform roughly equivalent, whereas word- and character-level tokenizers perform drastically worse on downstream tasks.
Finally,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chirkova and Troshin (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib6" title="">2022</a>)</cite> analyzed the effect of employing different tokenizations on code-related tasks and demonstrated that carefully configured tokenizers could reduce average sequence length up to 40% or allow for small downstream performance improvements by up to 2% at a lower compression rate.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Decoder Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">An overview of current mono- and multilingual <span class="ltx_glossaryref" title="">LLMs</span> is provided in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib33" title="">2022</a>); Shliazhko et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib49" title="">2022</a>); Scao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib45" title="">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Stollenwerk (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib51" title="">2023</a>)</cite> evaluated the intrinsic metrics of the GPT-SW3 <cite class="ltx_cite ltx_citemacro_cite">Ekgren et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib15" title="">2023</a>)</cite> tokenizer that focused on the Nordic languages.
As part of their work, <cite class="ltx_cite ltx_citemacro_citet">Shliazhko et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib49" title="">2022</a>)</cite> ablated different tokenizer pre-processing approaches while keeping the tokenizer algorithm, the vocabulary size, and the employed implementation fixed.
In none of the other major <span class="ltx_glossaryref" title="">LLM</span> publications, the extrinsic tokenizer performance has been studied.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To investigate the tokenizer impact on the model performance, we conducted an extensive ablation study. In detail, we created dedicated datasets for the training of the tokenizers and the models, trained BPE and Unigram tokenizers, and for each tokenizer we trained decoder-only models with a size of 2.6B parameters while keeping the remaining configuration (i.e., dataset and model hyper-parameters) fixed.
This allowed us to measure the tokenizer’s impact on the model’s downstream performance in isolation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">While creating our tokenizer and model training datasets, we ensure that the mixture proportions of data domains (Wikipedia, books, web text) follow the same distribution to avoid a domain shift between tokenizers training and model training.
We created <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">two datasets</em> with 70B words where one of the datasets is monolingual, containing English documents, and the second is a multilingual dataset comprised of English, German, French, Italian, and Spanish documents.
Our datasets are filtered and deduplicated and consist of web-crawled data (80%) and curated data (20%), comparable to related datasets used to train <span class="ltx_glossaryref" title="">LLMs</span>.
In the multilingual dataset, the amount of web-crawled data is equally distributed across languages in terms of number of words.
Further details about our data pipeline and the data composition are described in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A1.T7" title="Table 7 ‣ Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tokenizer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our studies rely on the two established tokenization algorithms, BPE and Unigram, and their implementation in the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Huggingface tokenizer</span> library <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib35" title="">2023</a>)</cite> and the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">SentencePiece</span> library <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>)</cite>.
We considered both libraries in order to investigate the effect of differences in the pre-and post-processing steps and potential differences in the implementations.
Due to missing pre-processing options for Huggingface’s Unigram implementation, which causes a large discrepancy in the resulting vocabulary compared to SentencePiece’s implementation of Unigram, we omitted the training of Unigram tokenizers based on Huggingface.
Overall, we trained 24 different tokenizers, where one-half of the tokenizers were monolingual English tokenizers, and the other half of the tokenizers were multilingual tokenizers.
Besides the tokenizer algorithm, language composition, and employed tokenizer library, we also varied the vocabulary size.
Concrete tokenizer configurations are described in the <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2" title="Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">B</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To measure the impact of our trained tokenizers on the model downstream performance, we trained one model for each tokenizer.
In particular, for each of our 24 trained tokenizers, we trained a 2.6B transformer-based decoder-only model on up to 52B tokens following the scaling law proposed by&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Hoffmann et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib24" title="">2022a</a>)</cite>.
Additionally, serving as baselines, we trained a monolingual and a multilingual model using the pre-trained GPT-2 tokenizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib41" title="">2018</a>)</cite>. All models have been trained based on the causal language modeling training objective.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To assess the impact of the tokenizers on the model downstream performance, we first performed an intrinsic tokenizer evaluation, followed by an extrinsic evaluation, and finally, we investigated whether a correlation between both evaluation approaches is given.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The intrinsic evaluation aims to assess the generated output of tokenizers based on <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.1">fertility</span> and <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.2">parity</span>.
Furthermore, the tokenizer’s vocabulary overlap with other tokenizers is computed.
The intrinsic evaluation does not assess the impact of tokenizers on the model performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.5">Fertility, the most common metric to evaluate a tokenizer’s performance <cite class="ltx_cite ltx_citemacro_cite">Scao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib45" title="">2022</a>); Stollenwerk (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib51" title="">2023</a>); Rust et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib43" title="">2021</a>)</cite>, is defined as the average number of tokens that are required to represent a word or document.
For a tokenizer <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_T</annotation></semantics></math> and dataset <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_A</annotation></semantics></math>, the fertility can be calculated as the number of tokens in <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_A</annotation></semantics></math> (when <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.1"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.4.m4.1d">italic_T</annotation></semantics></math> is applied) divided by the number of words in <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.5.m5.1d">italic_A</annotation></semantics></math>.
We calculate the fertility on a held-out set (10,000 documents), which was not used for the tokenizer training.
For calculating the words of a document, we used whitespace splitting.
Higher fertility scores correspond to weaker compression capabilities of the tokenizer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.9">Parity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Petrov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib38" title="">2023</a>)</cite>, which has been recently proposed, assesses how fairly a tokenizer treats equivalent sentences in different languages.
A tokenizer <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1.1"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.1.m1.1d">italic_T</annotation></semantics></math> achieves parity for language <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p4.2.m2.1"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.2.m2.1d">italic_A</annotation></semantics></math> with respect to language <math alttext="B" class="ltx_Math" display="inline" id="S3.SS4.p4.3.m3.1"><semantics id="S3.SS4.p4.3.m3.1a"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.3.m3.1d">italic_B</annotation></semantics></math> if <math alttext="\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1" class="ltx_Math" display="inline" id="S3.SS4.p4.4.m4.2"><semantics id="S3.SS4.p4.4.m4.2a"><mrow id="S3.SS4.p4.4.m4.2.3" xref="S3.SS4.p4.4.m4.2.3.cmml"><mfrac id="S3.SS4.p4.4.m4.2.2" xref="S3.SS4.p4.4.m4.2.2.cmml"><mrow id="S3.SS4.p4.4.m4.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.2.cmml"><mo id="S3.SS4.p4.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.4.m4.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS4.p4.4.m4.2.2.2.1" xref="S3.SS4.p4.4.m4.2.2.2.2.cmml"><mo id="S3.SS4.p4.4.m4.2.2.2.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.4.m4.2.2.2.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml">B</mi></msub><mo id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.4.m4.2.2.2.1.3" stretchy="false" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.SS4.p4.4.m4.2.3.1" xref="S3.SS4.p4.4.m4.2.3.1.cmml">≈</mo><mn id="S3.SS4.p4.4.m4.2.3.2" xref="S3.SS4.p4.4.m4.2.3.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.2b"><apply id="S3.SS4.p4.4.m4.2.3.cmml" xref="S3.SS4.p4.4.m4.2.3"><approx id="S3.SS4.p4.4.m4.2.3.1.cmml" xref="S3.SS4.p4.4.m4.2.3.1"></approx><apply id="S3.SS4.p4.4.m4.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2"><divide id="S3.SS4.p4.4.m4.2.2.3.cmml" xref="S3.SS4.p4.4.m4.2.2"></divide><apply id="S3.SS4.p4.4.m4.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1"><abs id="S3.SS4.p4.4.m4.1.1.1.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.2"></abs><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1"><times id="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2"></times><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3">𝐴</ci></apply></apply></apply><apply id="S3.SS4.p4.4.m4.2.2.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1"><abs id="S3.SS4.p4.4.m4.2.2.2.2.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.2"></abs><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1"><times id="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2"></times><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply><cn id="S3.SS4.p4.4.m4.2.3.2.cmml" type="integer" xref="S3.SS4.p4.4.m4.2.3.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.2c">\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.4.m4.2d">divide start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ) | end_ARG start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ) | end_ARG ≈ 1</annotation></semantics></math>, where <math alttext="s_{A}" class="ltx_Math" display="inline" id="S3.SS4.p4.5.m5.1"><semantics id="S3.SS4.p4.5.m5.1a"><msub id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml"><mi id="S3.SS4.p4.5.m5.1.1.2" xref="S3.SS4.p4.5.m5.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.5.m5.1.1.3" xref="S3.SS4.p4.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><apply id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.5.m5.1.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p4.5.m5.1.1.2.cmml" xref="S3.SS4.p4.5.m5.1.1.2">𝑠</ci><ci id="S3.SS4.p4.5.m5.1.1.3.cmml" xref="S3.SS4.p4.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">s_{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.5.m5.1d">italic_s start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="s_{B}" class="ltx_Math" display="inline" id="S3.SS4.p4.6.m6.1"><semantics id="S3.SS4.p4.6.m6.1a"><msub id="S3.SS4.p4.6.m6.1.1" xref="S3.SS4.p4.6.m6.1.1.cmml"><mi id="S3.SS4.p4.6.m6.1.1.2" xref="S3.SS4.p4.6.m6.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.6.m6.1.1.3" xref="S3.SS4.p4.6.m6.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m6.1b"><apply id="S3.SS4.p4.6.m6.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.6.m6.1.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p4.6.m6.1.1.2.cmml" xref="S3.SS4.p4.6.m6.1.1.2">𝑠</ci><ci id="S3.SS4.p4.6.m6.1.1.3.cmml" xref="S3.SS4.p4.6.m6.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m6.1c">s_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.6.m6.1d">italic_s start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math> denote the sets of all sentences in the corpora of languages <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p4.7.m7.1"><semantics id="S3.SS4.p4.7.m7.1a"><mi id="S3.SS4.p4.7.m7.1.1" xref="S3.SS4.p4.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m7.1b"><ci id="S3.SS4.p4.7.m7.1.1.cmml" xref="S3.SS4.p4.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m7.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.7.m7.1d">italic_A</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S3.SS4.p4.8.m8.1"><semantics id="S3.SS4.p4.8.m8.1a"><mi id="S3.SS4.p4.8.m8.1.1" xref="S3.SS4.p4.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.8.m8.1b"><ci id="S3.SS4.p4.8.m8.1.1.cmml" xref="S3.SS4.p4.8.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.8.m8.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.8.m8.1d">italic_B</annotation></semantics></math>, respectively, and the ratio <math alttext="\frac{|T(s_{A})|}{|T(s_{B})|}" class="ltx_Math" display="inline" id="S3.SS4.p4.9.m9.2"><semantics id="S3.SS4.p4.9.m9.2a"><mfrac id="S3.SS4.p4.9.m9.2.2" xref="S3.SS4.p4.9.m9.2.2.cmml"><mrow id="S3.SS4.p4.9.m9.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.2.cmml"><mo id="S3.SS4.p4.9.m9.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS4.p4.9.m9.1.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.9.m9.1.1.1.1.1.3" xref="S3.SS4.p4.9.m9.1.1.1.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.9.m9.1.1.1.1.1.2" xref="S3.SS4.p4.9.m9.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.9.m9.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS4.p4.9.m9.2.2.2.1" xref="S3.SS4.p4.9.m9.2.2.2.2.cmml"><mo id="S3.SS4.p4.9.m9.2.2.2.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS4.p4.9.m9.2.2.2.1.1" xref="S3.SS4.p4.9.m9.2.2.2.1.1.cmml"><mi id="S3.SS4.p4.9.m9.2.2.2.1.1.3" xref="S3.SS4.p4.9.m9.2.2.2.1.1.3.cmml">T</mi><mo id="S3.SS4.p4.9.m9.2.2.2.1.1.2" xref="S3.SS4.p4.9.m9.2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml"><mo id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3.cmml">B</mi></msub><mo id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.9.m9.2.2.2.1.3" stretchy="false" xref="S3.SS4.p4.9.m9.2.2.2.2.1.cmml">|</mo></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.9.m9.2b"><apply id="S3.SS4.p4.9.m9.2.2.cmml" xref="S3.SS4.p4.9.m9.2.2"><divide id="S3.SS4.p4.9.m9.2.2.3.cmml" xref="S3.SS4.p4.9.m9.2.2"></divide><apply id="S3.SS4.p4.9.m9.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1"><abs id="S3.SS4.p4.9.m9.1.1.1.2.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.2"></abs><apply id="S3.SS4.p4.9.m9.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1"><times id="S3.SS4.p4.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.2"></times><ci id="S3.SS4.p4.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.3">𝑇</ci><apply id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.9.m9.1.1.1.1.1.1.1.1.3">𝐴</ci></apply></apply></apply><apply id="S3.SS4.p4.9.m9.2.2.2.2.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1"><abs id="S3.SS4.p4.9.m9.2.2.2.2.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.2"></abs><apply id="S3.SS4.p4.9.m9.2.2.2.1.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1"><times id="S3.SS4.p4.9.m9.2.2.2.1.1.2.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.2"></times><ci id="S3.SS4.p4.9.m9.2.2.2.1.1.3.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.3">𝑇</ci><apply id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.9.m9.2.2.2.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.9.m9.2c">\frac{|T(s_{A})|}{|T(s_{B})|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.9.m9.2d">divide start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ) | end_ARG start_ARG | italic_T ( italic_s start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ) | end_ARG</annotation></semantics></math> is defined as premium.
We use the FLORES-200 <cite class="ltx_cite ltx_citemacro_cite">Goyal et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib20" title="">2022</a>)</cite> parallel corpus, consisting of the same sentences human-translated into 200 languages.
We calculate the parity values for each tokenizer and the four non-English languages with respect to English (see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2" title="Figure 2 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> for an overview).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">The extrinsic evaluation aims to explicitly assess the impact of a tokenizer on the model’s downstream performance.
We selected a comprehensive set of downstream tasks (see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.1</span></a>) to measure the downstream performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.8">Additionally, we computed the impact of a tokenizer on the average computational costs of a given model per word during training.
The computational costs during training for one step including the forward and the backward pass can be estimated by</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right)," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">C</mi><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">96</mn><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.4.cmml">B</mi><mo id="S3.E1.m1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.5.cmml">s</mi><mo id="S3.E1.m1.1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.1.6.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.2c" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><msup id="S3.E1.m1.1.1.1.1.1.7" xref="S3.E1.m1.1.1.1.1.1.7.cmml"><mi id="S3.E1.m1.1.1.1.1.1.7.2" xref="S3.E1.m1.1.1.1.1.1.7.2.cmml">h</mi><mn id="S3.E1.m1.1.1.1.1.1.7.3" xref="S3.E1.m1.1.1.1.1.1.7.3.cmml">2</mn></msup><mo id="S3.E1.m1.1.1.1.1.1.2d" xref="S3.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">s</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">6</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">h</mi></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml">V</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml">16</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml">h</mi></mrow></mfrac></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">𝐶</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><cn id="S3.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3">96</cn><ci id="S3.E1.m1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.4">𝐵</ci><ci id="S3.E1.m1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.5">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.1.6">𝑙</ci><apply id="S3.E1.m1.1.1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.7.1.cmml" xref="S3.E1.m1.1.1.1.1.1.7">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.7.2.cmml" xref="S3.E1.m1.1.1.1.1.1.7.2">ℎ</ci><cn id="S3.E1.m1.1.1.1.1.1.7.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.7.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></plus><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝑠</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">6</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">ℎ</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2">𝑉</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1"></times><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2">16</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4">ℎ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_C = 96 italic_B italic_s italic_l italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + divide start_ARG italic_s end_ARG start_ARG 6 italic_h end_ARG + divide start_ARG italic_V end_ARG start_ARG 16 italic_l italic_h end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p6.7">given a model with batch size <math alttext="B" class="ltx_Math" display="inline" id="S3.SS4.p6.1.m1.1"><semantics id="S3.SS4.p6.1.m1.1a"><mi id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><ci id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.1.m1.1d">italic_B</annotation></semantics></math>, sequence length <math alttext="s" class="ltx_Math" display="inline" id="S3.SS4.p6.2.m2.1"><semantics id="S3.SS4.p6.2.m2.1a"><mi id="S3.SS4.p6.2.m2.1.1" xref="S3.SS4.p6.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.2.m2.1b"><ci id="S3.SS4.p6.2.m2.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.2.m2.1d">italic_s</annotation></semantics></math>, <math alttext="l" class="ltx_Math" display="inline" id="S3.SS4.p6.3.m3.1"><semantics id="S3.SS4.p6.3.m3.1a"><mi id="S3.SS4.p6.3.m3.1.1" xref="S3.SS4.p6.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.3.m3.1b"><ci id="S3.SS4.p6.3.m3.1.1.cmml" xref="S3.SS4.p6.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.3.m3.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.3.m3.1d">italic_l</annotation></semantics></math> layers, hidden size <math alttext="h" class="ltx_Math" display="inline" id="S3.SS4.p6.4.m4.1"><semantics id="S3.SS4.p6.4.m4.1a"><mi id="S3.SS4.p6.4.m4.1.1" xref="S3.SS4.p6.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.4.m4.1b"><ci id="S3.SS4.p6.4.m4.1.1.cmml" xref="S3.SS4.p6.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.4.m4.1d">italic_h</annotation></semantics></math> and vocabulary size <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.p6.5.m5.1"><semantics id="S3.SS4.p6.5.m5.1a"><mi id="S3.SS4.p6.5.m5.1.1" xref="S3.SS4.p6.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.5.m5.1b"><ci id="S3.SS4.p6.5.m5.1.1.cmml" xref="S3.SS4.p6.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.5.m5.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.5.m5.1d">italic_V</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Narayanan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib36" title="">2021</a>)</cite>.
The costs per token can be derived by <math alttext="C_{\text{token}}=C/Bs" class="ltx_Math" display="inline" id="S3.SS4.p6.6.m6.1"><semantics id="S3.SS4.p6.6.m6.1a"><mrow id="S3.SS4.p6.6.m6.1.1" xref="S3.SS4.p6.6.m6.1.1.cmml"><msub id="S3.SS4.p6.6.m6.1.1.2" xref="S3.SS4.p6.6.m6.1.1.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.2.2" xref="S3.SS4.p6.6.m6.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.6.m6.1.1.2.3" xref="S3.SS4.p6.6.m6.1.1.2.3a.cmml">token</mtext></msub><mo id="S3.SS4.p6.6.m6.1.1.1" xref="S3.SS4.p6.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.6.m6.1.1.3" xref="S3.SS4.p6.6.m6.1.1.3.cmml"><mrow id="S3.SS4.p6.6.m6.1.1.3.2" xref="S3.SS4.p6.6.m6.1.1.3.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.3.2.2" xref="S3.SS4.p6.6.m6.1.1.3.2.2.cmml">C</mi><mo id="S3.SS4.p6.6.m6.1.1.3.2.1" xref="S3.SS4.p6.6.m6.1.1.3.2.1.cmml">/</mo><mi id="S3.SS4.p6.6.m6.1.1.3.2.3" xref="S3.SS4.p6.6.m6.1.1.3.2.3.cmml">B</mi></mrow><mo id="S3.SS4.p6.6.m6.1.1.3.1" xref="S3.SS4.p6.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p6.6.m6.1.1.3.3" xref="S3.SS4.p6.6.m6.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.6.m6.1b"><apply id="S3.SS4.p6.6.m6.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1"><eq id="S3.SS4.p6.6.m6.1.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1.1"></eq><apply id="S3.SS4.p6.6.m6.1.1.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.6.m6.1.1.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.6.m6.1.1.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.2.3a.cmml" xref="S3.SS4.p6.6.m6.1.1.2.3"><mtext id="S3.SS4.p6.6.m6.1.1.2.3.cmml" mathsize="70%" xref="S3.SS4.p6.6.m6.1.1.2.3">token</mtext></ci></apply><apply id="S3.SS4.p6.6.m6.1.1.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3"><times id="S3.SS4.p6.6.m6.1.1.3.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.1"></times><apply id="S3.SS4.p6.6.m6.1.1.3.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2"><divide id="S3.SS4.p6.6.m6.1.1.3.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.1"></divide><ci id="S3.SS4.p6.6.m6.1.1.3.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.3.2.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.3">𝐵</ci></apply><ci id="S3.SS4.p6.6.m6.1.1.3.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.6.m6.1c">C_{\text{token}}=C/Bs</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.6.m6.1d">italic_C start_POSTSUBSCRIPT token end_POSTSUBSCRIPT = italic_C / italic_B italic_s</annotation></semantics></math> and the average costs per word by <math alttext="C_{\text{word}}=C_{\text{token}}\times\text{fertility}" class="ltx_Math" display="inline" id="S3.SS4.p6.7.m7.1"><semantics id="S3.SS4.p6.7.m7.1a"><mrow id="S3.SS4.p6.7.m7.1.1" xref="S3.SS4.p6.7.m7.1.1.cmml"><msub id="S3.SS4.p6.7.m7.1.1.2" xref="S3.SS4.p6.7.m7.1.1.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.2.2" xref="S3.SS4.p6.7.m7.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.2.3" xref="S3.SS4.p6.7.m7.1.1.2.3a.cmml">word</mtext></msub><mo id="S3.SS4.p6.7.m7.1.1.1" xref="S3.SS4.p6.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.7.m7.1.1.3" xref="S3.SS4.p6.7.m7.1.1.3.cmml"><msub id="S3.SS4.p6.7.m7.1.1.3.2" xref="S3.SS4.p6.7.m7.1.1.3.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.3.2.2" xref="S3.SS4.p6.7.m7.1.1.3.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.3.2.3" xref="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml">token</mtext></msub><mo id="S3.SS4.p6.7.m7.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p6.7.m7.1.1.3.1.cmml">×</mo><mtext id="S3.SS4.p6.7.m7.1.1.3.3" xref="S3.SS4.p6.7.m7.1.1.3.3a.cmml">fertility</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.7.m7.1b"><apply id="S3.SS4.p6.7.m7.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1"><eq id="S3.SS4.p6.7.m7.1.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1.1"></eq><apply id="S3.SS4.p6.7.m7.1.1.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.2.3"><mtext id="S3.SS4.p6.7.m7.1.1.2.3.cmml" mathsize="70%" xref="S3.SS4.p6.7.m7.1.1.2.3">word</mtext></ci></apply><apply id="S3.SS4.p6.7.m7.1.1.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3"><times id="S3.SS4.p6.7.m7.1.1.3.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.1"></times><apply id="S3.SS4.p6.7.m7.1.1.3.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.3.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.3.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.3"><mtext id="S3.SS4.p6.7.m7.1.1.3.2.3.cmml" mathsize="70%" xref="S3.SS4.p6.7.m7.1.1.3.2.3">token</mtext></ci></apply><ci id="S3.SS4.p6.7.m7.1.1.3.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3"><mtext id="S3.SS4.p6.7.m7.1.1.3.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3">fertility</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.7.m7.1c">C_{\text{word}}=C_{\text{token}}\times\text{fertility}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p6.7.m7.1d">italic_C start_POSTSUBSCRIPT word end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT token end_POSTSUBSCRIPT × fertility</annotation></semantics></math>.
The Results are discussed in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Intrinsic Tokenizer Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In our intrinsic evaluation, we first compare the fertility and parity of the trained tokenizers (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS1" title="4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and subsequently the overlap of their vocabularies (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fertility &amp; Parity</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F0.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="622" id="S4.F0.sf1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Non-English, multilingual documents</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F0.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="622" id="S4.F0.sf2.g1" src="x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>English documents</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of fertility scores between mono- and multilingual tokenizers applied to (a) Non-English, multilingual documents and (b) English documents.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Applying the described fertility and parity evaluation to the mono-/multilingual tokenizers, our analysis highlights the following two major aspects, as visualized in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F1" title="Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>&nbsp;and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2" title="Figure 2 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Firstly, it can be observed that applying a monolingual tokenizer to multilingual data results in significantly higher fertility and parity scores (see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F0.sf1" title="0(a) ‣ Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">0(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2" title="Figure 2 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>). While multilingual tokenizers have lower fertility than monolingual English tokenizers on all non-English documents by a large margin, they are only slightly worse on tokenizing English documents, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F0.sf2" title="0(b) ‣ Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">0(b)</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Secondly, with increasing vocabulary size, fertility and parity reduce in all cases, which can be explained by the tokenizer requiring fewer sub-word tokens when tokenizing text given a larger vocabulary.
However, it can be observed that for monolingual English tokenizers, the fertility is less dependent on the vocabulary when tokenizing English documents, implying that 33k might be a sufficiently large vocabulary.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="321" id="S4.F2.g1" src="x3.png" width="411">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of parity scores between monolingual (English) tokenizer and multilingual tokenizers applied multi-lingual documents.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Vocabulary Overlap</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To analyze the tokenizer similarity, we calculated the vocabulary overlap. Particularly, we assess Huggingface’s and SentencePiece’s BPE implementations, as depicted in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.T1" title="Table 1 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The overlap is roughly constant across different vocabulary sizes, and the total overlap tends to be rather low, despite being the identical algorithm only implemented by two different libraries. Consequently, the tokenizers produce different tokenized sequences, possibly affecting model training and downstream performance.
Investigating the underlying reasons, the low overlap might be attributed to different configuration and pre-processing options in these libraries. Due to the larger thesaurus in multilingual documents, the overlap for the multilingual tokenizer is lower than for the English tokenizers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">33k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">50k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">82k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5">100k</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">English</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">0.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">0.74</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.3.2.1">Multilingual</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.2">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.3">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.4">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.3.2.5">0.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Vocabulary overlap between the HuggingFace and SentencePiece BPE tokenizer for different vocab sizes.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="634" id="S4.F2.sf1.g1" src="x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Non-English documents</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="634" id="S4.F2.sf2.g1" src="x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>German documents</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="634" id="S4.F2.sf3.g1" src="x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>English documents</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average compute (GFLOPS) required to process a single word within (a) multilingual, (b) English, and (c) German documents within a full <span class="ltx_text ltx_font_bold" id="S4.F3.2.1">training</span> pass (including the backward pass).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Extrinsic Tokenizer Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In the following, we describe the results of our extrinsic evaluation of tokenizers.
<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.1</span></a> describes the experimental setup, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2" title="5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.2</span></a> presents the downstream performance of the trained models based on the investigated tokenizers, and <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.3</span></a> analyzes the computational costs associated with each tokenizer when employed in a specific model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To assess the impact of the tokenizers on the model downstream performance, we trained a decoder-only transformer model of size 2.6 B for each tokenizer.
We trained our models for 52.6 B tokens following the scaling laws proposed by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Hoffmann et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib25" title="">2022b</a>)</cite>, based on the causal language modeling training objective.
The hyper-parameters are described in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3.T10" title="Table 10 ‣ Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">10</span></a> in the <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3" title="Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">C</span></a>.
We evaluated our models in zero-shot settings on a wide range of mono- and multilingual tasks:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Natural language inference: XNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Conneau et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib11" title="">2018</a>)</cite>, MNLI &nbsp;<cite class="ltx_cite ltx_citemacro_cite">Williams et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib59" title="">2018</a>)</cite>, RTE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib57" title="">2018</a>)</cite>, WNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Levesque et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib32" title="">2012</a>)</cite>, CB&nbsp;<cite class="ltx_cite ltx_citemacro_cite">De&nbsp;Marneffe et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib12" title="">2019</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Question answering:
X-CSQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Goodman (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib19" title="">2001</a>)</cite>, XStoryCloze&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib33" title="">2022</a>)</cite>, PubMedQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib27" title="">2019</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Reading comprehension: BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib7" title="">2019</a>)</cite>), LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Paperno et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib37" title="">2016</a>)</cite>, RACE <cite class="ltx_cite ltx_citemacro_cite">Lai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib31" title="">2017</a>)</cite>, MRPC <cite class="ltx_cite ltx_citemacro_cite">Dolan and Brockett (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib14" title="">2005</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">Commonsense reasoning: HellaSwag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib64" title="">2019</a>)</cite>, WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Sakaguchi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib44" title="">2020</a>)</cite>, ARC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib9" title="">2018</a>)</cite>, XCOPA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ponti et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib40" title="">2020</a>)</cite>, XCDOAH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Goodman (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib19" title="">2001</a>)</cite>, WSC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Levesque et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib32" title="">2012</a>)</cite>, COPA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Roemmele et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib42" title="">2011</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">Classification:
PAWS-X&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Yang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib61" title="">2019</a>)</cite>, GNAD10&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schabus et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib46" title="">2017</a>)</cite>, SST&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Socher et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib50" title="">2013</a>)</cite>, WIC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Pilehvar and
Camacho-Collados (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib39" title="">2019</a>)</cite>, PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bisk et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib3" title="">2020</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T2" title="Table 2 ‣ 5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the number of tasks for each category and language.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1">Task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.2">EN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.3">DE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.4">FR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.5">ES</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.6">IT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.2.1.1">NLI</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.5">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.6">0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.3.2.1">QA</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.2">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.3">2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.4">2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.5">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.6">2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.3.1">RC</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.2">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.3">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.4">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.5">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.6">1</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.4.1">CR</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.2">7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.4">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.6">1</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.6.5.1">CL</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.2">3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.3">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.4">0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.5">1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.6">0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.1"></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.2">22</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.5">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.7.6.6">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Overview of the number of evaluation tasks for each language and the categories of Natural language inference (NLI), Reading comprehension (RC), Question answering (QA), Commonsense reasoning (CR) and Classification (CL).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Downstream Performance</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We split our analysis of the downstream performance into several parts.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">First, we discuss the overall results obtained for the investigated tokenizers, followed by presenting the impact of the tokenizer library (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS1" title="5.2.1 Impact of the Tokenizer Library ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.2.1</span></a>), the impact of the tokenizer algorithm (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS2" title="5.2.2 Impact of the Tokenizer Algorithm ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.2.2</span></a>), and the impact of the vocabulary size (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS2.SSS3" title="5.2.3 Impact of the Tokenizer Vocabulary ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.2.3</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">We present both, aggregated results across all tasks (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>) and results for selected single tasks (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T4" title="Table 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>).
For the average performance across all tasks presented in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>, we computed weighted average to take into account the different number of tasks per language.
In particular, we computed for each language the mean across all tasks, and then computed the mean over all language-means.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.2">EN</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T3.1.1.1.3">MULTI</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.1">GPT-2-50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.2">50.36</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.2.2.3">39.41</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.3.3.1">BPE-HF-33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.3.3.2">49.13</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.3.3.3">40.52</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.4.1">BPE-HF-50</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.4.2">49.51</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.3">40.47</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.5.1">BPE-HF-82</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.5.2">48.71</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.3">40.24</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.6.1">BPE-HF-100</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.6.2">49.54</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.3">40.48</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.7.7.1">BPE-SP-33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.7.2.1">50.81</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.3">40.28</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<td class="ltx_td ltx_align_left" id="S5.T3.1.8.8.1">BPE-SP-50</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.8.8.2">49.81</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.3">40.49</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.9.1">BPE-SP-82</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.9.2">48.99</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.3">41.21</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<td class="ltx_td ltx_align_left" id="S5.T3.1.10.10.1">BPE-SP-100</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.10.10.2">49.46</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.3.1">41.44</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.11.11.1">UNI-SP-33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.11.11.2">50.28</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.11.11.3">40.30</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.12.12">
<td class="ltx_td ltx_align_left" id="S5.T3.1.12.12.1">UNI-SP-50</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.12.12.2">49.90</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.3">40.48</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.13.13">
<td class="ltx_td ltx_align_left" id="S5.T3.1.13.13.1">UNI-SP-82</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.13.13.2">49.65</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.13.13.3">41.20</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.14.14.1">UNI-SP-100</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.14.14.2">50.21</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.14.14.3">40.74</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average accuracy of monolingual and multilingual tokenizers across all downstream tasks. Due to varying number of tasks per language, multi-lingual accuracies have been adjusted to each language contributing equally to the average.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.2">Task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.3">Min</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.4">Max</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.5">Rand.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.1" rowspan="4"><span class="ltx_text" id="S5.T4.1.2.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.2.1.1.1.1" style="width:6.8pt;height:14.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-3.74pt,-3.74pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.1.2.1.1.1.1.1">EN</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.2">ARC-Easy</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.3">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.4">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.5">0.20</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.3.2.1">HellaSwag</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.2">0.34</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.3">0.41</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.4">0.25</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.4.3.1">MRPC</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.2">0.54</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.3">0.69</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.4">0.50</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.5.4.1">PIQA</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.2">0.67</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.3">0.72</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.4">0.50</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T4.1.6.5.1" rowspan="4"><span class="ltx_text" id="S5.T4.1.6.5.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.6.5.1.1.1" style="width:6.8pt;height:32.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.9pt;transform:translate(-13.04pt,-13.04pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.1.6.5.1.1.1.1">MULTI</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.6.5.2">XNLI FR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.6.5.3">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.6.5.4">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.6.5.5">0.33</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.7.6.1">XNLI EN</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.2">0.49</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.3">0.52</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.4">0.33</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.8.7.1">X-CODAH ES</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.2">0.28</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.3">0.43</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.4">0.25</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.9.8.1">10kGNAD</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.9.8.2">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.9.8.3">0.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.9.8.4">0.11</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Worst- and best-performing tokenizer for selected tasks and the random performance on this task.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S5.T5.1.1.1.2">MULTI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.1.1.3">MONO</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.2.2.1">Vocabulary</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.2">DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.3">FR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.4">IT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.5">ES</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.6">EN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.7">AVG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.8">EN</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.3.3.1">33</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.3.3.2.1">36.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.3">36.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.4">39.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.5">41.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.6">47.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.7">40.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.3.8">49.55</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.4.4.1">50</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.2">36.12</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.3">37.07</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.4">38.94</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.5">42.22</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.6">46.71</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.7">40.21</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.4.8.1">49.90</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.5.5.1">82</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.2">36.50</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.3">37.83</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.4">39.97</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.5">42.30</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.5.6.1">47.80</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.7">40.88</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.5.8">49.12</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.6.6.1">100</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.2">35.92</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.3.1">38.07</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.4.1">40.13</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.5.1">42.64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.6">47.67</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.7.1">40.89</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.8">49.74</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T5.1.7.7.1">Algorithm and Library</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.2">DE</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.3">FR</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.4">IT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.5">ES</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.6">EN</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.7">AVG</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.7.7.8">EN</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.8.8.1">BPE-HF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.2">35.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.3">37.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.4">39.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.5">42.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.6">47.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.7">40.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.8">48.98</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.9.9.1">BPE-SP</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.2.1">37.13</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.3">37.45</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.4.1">40.04</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.5">41.96</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.6.1">47.68</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.9.9.7.1">40.85</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.8">49.77</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.10.10.1">UNI-SP</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.2">36.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.10.10.3.1">37.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.4">39.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.10.10.5.1">42.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.6">47.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.7">40.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.10.10.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.10.10.8.1">50.01</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Impact of the vocabulary size (upper), and tokenizer algorithm and library (lower), on the downstream performance. The accuracy scores are either averaged over the libraries and tokenizer algorithms (upper) or the different vocabulary sizes (lower).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Monolingual Tokenizer</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> demonstrates that the BPE-SP-33 tokenizer, on average, is the best-performing tokenizer, followed by the GPT-2 tokenizer. Interestingly, SentencePiece’s implementation of BPE with a vocabulary size of 33k has been used for LLaMA2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib55" title="">2023</a>)</cite>.
Aggregated metrics provide a reasonable overview of the overall performance.
However, it does not express potentially large performance differences across tasks.
Therefore, we listed in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T4" title="Table 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> the obtained results for a list of selected tasks obtained by the best and worst performing tokenizer on this task.
The results illustrate that the performance difference can be huge.
For instance, for ARC-Easy, a commonsense reasoning task, the gap between the best and worst tokenizer is 9%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multilingual Tokenizer</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> shows that the BPE-SP-100 tokenizer is the best-performing tokenizer followed by the BPE-SP-82 tokenizer.
Furthermore, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> demonstrates that the GPT-2 tokenizer performs poorly, implying that using a pre-trained GPT-2 tokenizer to pre-train and fine-tune multilingual models should be <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p1.1.1">omitted</span>.
The analysis of selected tasks (&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T4" title="Table 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4</span></a>) reveals that for multilingual tokenizers, the performance difference between tasks can be huge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Impact of the Tokenizer Library</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T5" title="Table 5 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> demonstrates that BPE-SP, on average, outperforms BPE-HF in the monolingual and multilingual setting across all languages.
The performance differences might be attributed to the differences in implementation details of the tokenizers’ pre-and postprocessing, which could affect the vocabulary creation (see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>) and, consequently, the downstream performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Impact of the Tokenizer Algorithm</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Furthermore, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T5" title="Table 5 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> shows that depending on the language, either the BPE or Unigram exhibits better performance.
It is noteworthy that the Germanic languages German and English benefit from the BPE algorithm, whereas the Romanic languages French and Spanish benefited from Unigram.
The experiments for Italian, a Romanic language as well, show a different pattern than the other two Romanic languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Impact of the Tokenizer Vocabulary</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">Analyzing the impact of the vocabulary size revealed that in the monolingual English setting, the smaller/medium-sized, i.e., a vocabulary size of 33k/50k performs better (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T5" title="Table 5 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>) whereas in the multilingual setting, in all cases except for German, larger vocabulary sizes result in better downstream performance.
Taking into account the results presented in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.T3" title="Table 3 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> showing that in the monolingual English setting, the best-performing tokenizer on average across all tasks had a vocabulary size of 33k and that the best-performing multilingual tokenizer had a vocabulary size of 100k additionally supports the observation that for the monolingual English setting a small vocabulary size is beneficial and for the multilingual setting a large vocabulary size is required.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Computational Costs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Given a fixed model, the computational costs depend on the vocabulary size and the fertility of the tokenizer, as defined in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S3.E1" title="1 ‣ 3.4 Evaluation ‣ 3 Approach ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Eq.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">While larger vocabulary sizes introduce additional computational costs, they might also result in lower fertility scores and, therefore, lower overall computational costs for processing a set of documents, as discussed in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4" title="4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>.
However, our findings in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F3" title="Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> show that increasing the vocabulary size from 50k to larger vocabulary sizes increases the computational costs in all cases.
This highlights that the potentially lower fertility of larger vocabulary sizes cannot compensate for the additional costs introduced by the larger vocabulary size.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Furthermore, we observe that the computational training costs for multilingual documents are significantly lower for multilingual tokenizers than for monolingual English tokenizers (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2.sf1" title="2(a) ‣ Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2(a)</span></a>).
In fact, <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2.sf2" title="2(b) ‣ Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2(b)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.T11" title="Table 11 ‣ D.1 Computational Costs Per Word During Training ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a> in the appendix demonstrate that the training costs can increase up to 68% (comparing Multi-UNI-SP-50 to EN-UNI-SP-100 for German documents) for a given dataset.
Assuming that during training it is required to process a fixed set of documents (e.g., Wikipedia to learn specific facts) entirely and not only a given number of tokens, the choice of the tokenizer can significantly impact the computational costs for training on this corpus.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">While we could observe large cost differences between multilingual and monolingual English tokenizers in the monolingual English setting, the difference in computational costs between multilingual and monolingual English tokenizers for processing English documents is marginal (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F2.sf3" title="2(c) ‣ Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2(c)</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Correlation Between Intrinsic And Extrinsic Tokenizer Performance</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="415" id="S6.F4.g1" src="x7.png" width="415">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Spearman correlation of fertility/parity scores and downstream task performance for all five languages.
We evaluated monolingual models on English tasks (left), whereas our multilingual models are evaluated across all non-English tasks. Pearson and Kendall correlation metrics showed a very similar picture.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This section investigates a possible predictive relationship of intrinsic tokenizer metrics (fertility and parity) to the extrinsic model downstream performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">As highlighted in the correlation heatmaps in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S6.F4" title="Figure 4 ‣ 6 Correlation Between Intrinsic And Extrinsic Tokenizer Performance ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>, we find that there is no distinct correlation across all tasks and languages, demanding a more granular analysis.
While for non-English tasks, we mainly observe a correlation between low fertility and higher downstream performance, the non-English tasks yield seemingly random positive and negative correlations.
However, it should be noted that the number of multilingual tasks per language is much lower than for English and that for several multilingual tasks such as XSQA and LAMBADA, a similar correlation behaviour between the English tasks and their translated version can be observed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Taking the fertility trends with varying vocabulary sizes (see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S4.F1" title="Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>) into consideration, we hypothesize that fertility only correlates with downstream performance in certain language-specific vocabulary size limits. For the English language, the tokenizers already provide low, close-to-convergence fertility scores for vocabulary sizes of 33k tokens. While additional tokens yield only minute fertility improvements, we presume that they do not capture morphological segmentations and, thus, can harm downstream performance and significantly increase the computation costs (see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.3</span></a>) in the end.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">In contrast, for multilingual tokenizers, we observe significant fertility improvements with increasing vocabulary sizes. Due to the larger thesaurus induced by the additional languages, the tokenizer requires a larger vocabulary to allow a model to perform convincingly on all languages. Therefore, only within the non-convergence vocabulary range, we achieve a strong, negative correlation between fertility and downstream performance with varying vocabulary sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">In conclusion, intrinsic tokenizer metrics such as fertility and parity need to be taken with a grain of salt and supposedly are only predictive of downstream model performance in certain bounds.
Low fertility scores might be regarded as a necessary criterion but not as a sufficient one.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion &amp; Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work represents a fundamental step to a better understanding of the impact of the tokenizer on the models’ downstream performance.
We have shown that training tokenizers with a balanced share across languages achieve comparable low fertility and parity scores across all languages, which has important implications.
Higher fertility results in up to 68% more computational costs during training and prevents the model from learning long-range dependencies in limited context windows.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Furthermore, we highlight that the tokenizer choice can significantly impact the model’s downstream performance.
We could show that the BPE algorithm applies well to mono- and multilingual settings.
For English, we show that a vocabulary size of 33k is sufficient, whereas multilingual models based on our five considered languages require a up to three times larger vocabulary size.
Moreover, we could show that the SentencePiece library outperforms the Huggingface tokenizer library.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Finally, we could demonstrate that there is no clear correlation between intrinsic and extrinsic tokenizer performance, but the correlation is rather task-specific.
A small fertility value might be a necessary condition for good downstream performance but not a sufficient one.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">In the future, we aim to investigate tokenizers for a larger set of languages, including very diverse languages, and investigate the impact of alternative tokenization approaches such as SAGE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Yehezkel and Pinter (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib62" title="">2023</a>)</cite> that focus on context information during tokenizer training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Despite the extensiveness of our work, it faces the following limitations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Firstly, we did not perform hyper-parameter optimizations for each tokenizer.
This was a deliberate choice to avoid additional computational costs, considering that training all 26 models only once required <math alttext="\approx 59.000" class="ltx_Math" display="inline" id="S8.p2.1.m1.1"><semantics id="S8.p2.1.m1.1a"><mrow id="S8.p2.1.m1.1.1" xref="S8.p2.1.m1.1.1.cmml"><mi id="S8.p2.1.m1.1.1.2" xref="S8.p2.1.m1.1.1.2.cmml"></mi><mo id="S8.p2.1.m1.1.1.1" xref="S8.p2.1.m1.1.1.1.cmml">≈</mo><mn id="S8.p2.1.m1.1.1.3" xref="S8.p2.1.m1.1.1.3.cmml">59.000</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.p2.1.m1.1b"><apply id="S8.p2.1.m1.1.1.cmml" xref="S8.p2.1.m1.1.1"><approx id="S8.p2.1.m1.1.1.1.cmml" xref="S8.p2.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S8.p2.1.m1.1.1.2.cmml" xref="S8.p2.1.m1.1.1.2">absent</csymbol><cn id="S8.p2.1.m1.1.1.3.cmml" type="float" xref="S8.p2.1.m1.1.1.3">59.000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.1.m1.1c">\approx 59.000</annotation><annotation encoding="application/x-llamapun" id="S8.p2.1.m1.1d">≈ 59.000</annotation></semantics></math> GPU hours.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Secondly, we did not investigate the effect of different random seeds on the model performance for a given tokenizer due to the additional computational costs.
However, our results lay the foundation for future works that can further investigate the robustness of selected experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">Third, we did not investigate whether the results obtained could be extrapolated to larger model sizes, which we leave to future works.
However, our finding that the BPE-SP-33 tokenizer is the best-performing tokenizer for the monolingual setting and the fact that this tokenizer has been used for training state-of-the-art models up to 65B&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib54" title="">Touvron et&nbsp;al. </a></cite> might indicate that our results also transfer to larger model sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">Finally, we did not provide results for a few-show setting since the metric of interest in the context of this work was the zero-shot downstream performance.
Because we wanted to investigate whether the tokenizer choice impacts the model’s downstream performance, we argue that restricting on one of the widely applied metrics, i.e., the zero-shot setting, is sufficient to answer this research question.
One further advantage of focusing on the zero-shot scenario is that we do not introduce an additional variable represented by the choice of the few-shot examples.
However, we encourage future works to investigate whether our results translate into the few-shot evaluation setting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Ethical And Broader Impact</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">LLMs represent a disruptive technology that has received significant attention from the public and is widely used across societies speaking different languages.
Therefore, ensuring a democratization of the technology across people of different languages will represent an important value.
Our study highlights that neglecting multilingualism while training a tokenizer representing a core component required for training LLMs can cause severe disadvantages, such as increased training costs and decreased downstream performance, raising major ethical concerns.
Furthermore, the increased training costs translate into an increased carbon footprint, which has an environmental impact.
Our findings support an improved development and usage of this fundamental technology.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) through the project OpenGPT-X (project no. 68GX21007D) as well as by the Federal Ministry of Education and Research of Germany and the state of North-Rhine Westphalia as part of the Lamarr-Institute for Machine, LAMARR22B and by the European Union’s Horizon 2020 research and innovation program under grant agreement No. 101135671 (TrustLLM) and 952215 (TAILOR). The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the GCS Supercomputer JUWELS at Jülich Supercomputing Centre (JSC) as well as the Center for Information Services and High Performance Computing [Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH)] at TU Dresden for providing its facilities for high throughput calculations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Javier&nbsp;Ortiz Suárez, Laurent Romary, and Benoît
Sagot. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.14618/ids-pub-10468" title="">Ungoliant: An
optimized pipeline for the generation of a very large-scale multilingual web
corpus</a>.

</span>
<span class="ltx_bibblock">In Harald Lüngen, Marc Kupietz, Piotr Bański, Adrien Barbaresi,
Simon Clematide, and Ines Pisetta, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Workshop
on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12
July 2021 (Online-Event)</em>, pages 1 – 9. Leibniz-Institut für Deutsche
Sprache, Mannheim.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</em>, 13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pages 7432–7439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al. 2020a.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chirkova and Troshin (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nadezhda Chirkova and Sergey Troshin. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=rd-G1nO-Jbq" title="">CodeBPE:
Investigating subtokenization options for large language model pretraining on
source code</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Deep Learning for Code Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">NAACL-HLT (1)</em>, pages 2924–2936. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00448" title="">Canine: Pre-training an
efficient tokenization-free encoder for language representation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</em>,
10:73–91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the AI2
reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/1803.05457.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/togethercomputer/RedPajama-Data" title="">Redpajama: An open source recipe to reproduce llama training dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel&nbsp;R.
Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock">XNLI: evaluating cross-lingual sentence representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">EMNLP</em>, pages 2475–2485. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Marneffe et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Marie-Catherine De&nbsp;Marneffe, Mandy Simons, and Judith Tonhauser. 2019.

</span>
<span class="ltx_bibblock">The commitmentbank: Investigating projection in naturally occurring
discourse.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">proceedings of Sinn und Bedeutung</em>, volume&nbsp;23, pages
107–124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolan and Brockett (2005)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;B. Dolan and Chris Brockett. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/I05-5002" title="">Automatically constructing
a corpus of sentential paraphrases</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Third International Workshop on
Paraphrasing (IWP2005)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekgren et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ariel Ekgren, Amaru&nbsp;Cuba Gyllensten, Felix Stollenwerk, Joey Öhman, Tim
Isbister, Evangelia Gogoulou, Fredrik Carlsson, Alice Heiman, Judit
Casademont, and Magnus Sahlgren. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.12987" title="">Gpt-sw3: An autoregressive
language model for the nordic languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gage (1994)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Philip Gage. 1994.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:59804030" title="">A new
algorithm for data compression</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The C Users Journal archive</em>, 12:23–38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy. 2020a.

</span>
<span class="ltx_bibblock">The Pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2101.00027</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yingqiang Gao, Nikola&nbsp;I. Nikolov, Yuhuang Hu, and Richard&nbsp;H.R. Hahnloser.
2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.145" title="">Character-level translation with self-attention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1591–1604, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodman (2001)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joshua Goodman. 2001.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/cs.CL/0108005v1" title="">A bit of progress in
language modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, cs.CL/0108005v1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and
Angela Fan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00474" title="">The Flores-101
Evaluation Benchmark for Low-Resource and Multilingual Machine Translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Transactions of the Association for Computational Linguistics</em>,
10:522–538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Graën, Tannon Kew, Anastassia Shaitarova, and Martin Volk. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.14618/ids-pub-9020" title="">Modelling large
parallel corpora: The zurich parallel corpus collection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 7th Workshop on Challenges in the
Management of Large Corpora (CMLC)</em>, pages 1–8. Leibniz-Institut für
Deutsche Sprache.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Graën, D.&nbsp;Batinic, and M.&nbsp;Volk. 2014.

</span>
<span class="ltx_bibblock">Cleaning the Europarl corpus for linguistic applications.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Konvens 2014</em>. Stiftung Universität Hildesheim.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hajlaoui et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Najeh Hajlaoui, David Kolovratnik, Jaakko Vaeyrynen, Ralf Steinberger, and
Dániel Varga. 2014.

</span>
<span class="ltx_bibblock">DCEP - Digital corpus of the European parliament.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proc. LREC 2014 (Language Resources and Evaluation
Conference). Reykjavik, Iceland</em>, pages 3164–3171.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George
van&nbsp;den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén
Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre.
2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf" title="">An empirical analysis of compute-optimal large language model training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>,
volume&nbsp;35, pages 30016–30030. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van&nbsp;den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and Laurent Sifre. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2203.15556" title="">Training
compute-optimal large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">CoRR</em>, abs/2203.15556.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Höfler and Piotrowski (2011)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stefan Höfler and Michael Piotrowski. 2011.

</span>
<span class="ltx_bibblock">Building corpora for the philological study of Swiss legal texts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Journal for Language Technology and Computational Linguistics</em>,
26(2):77–89.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1259" title="">PubMedQA: A
dataset for biomedical research question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2567–2577, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Koehn. 2005.

</span>
<span class="ltx_bibblock">Europarl: A parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Machine Translation Summit, volume 5</em>, pages 79––86.
Asia-Pacific Association for Machine Translation (AAMT).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1007" title="">Subword regularization:
Improving neural network translation models with multiple subword
candidates</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 66–75,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">EMNLP 2018</em>, page&nbsp;66.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D17-1082" title="">RACE: Large-scale
ReAding comprehension dataset from examinations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing</em>, pages 785–794, Copenhagen, Denmark.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levesque et&nbsp;al. (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012.

</span>
<span class="ltx_bibblock">The winograd schema challenge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Thirteenth international conference on the principles of
knowledge representation and reasoning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 9019–9052.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock">OpenSubtitles2016: Extracting large parallel corpora from movie
and tv subtitles.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 10th International Conference on Language
Resources and Evaluation (LREC-2016)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moi and Patry (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthony Moi and Nicolas Patry. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/huggingface/tokenizers" title="">HuggingFace’s
Tokenizers</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3458817.3476209" title="">Efficient
large-scale language model training on gpu clusters using megatron-lm</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis</em>, SC ’21, New York,
NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham,
Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
Fernández. 2016.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse
context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ACL (1)</em>. The Association for Computer Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La&nbsp;Malfa, Philip&nbsp;HS Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2305.15425</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and
Camacho-Collados (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and Jose Camacho-Collados. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1128" title="">WiC: the
word-in-context dataset for evaluating context-sensitive meaning
representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 1267–1273,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edoardo&nbsp;Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan
Vulić, and Anna Korhonen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.185" title="">XCOPA: A
multilingual dataset for causal commonsense reasoning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2362–2376, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roemmele et&nbsp;al. (2011)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Melissa Roemmele, Cosmin&nbsp;Adrian Bejan, and Andrew&nbsp;S Gordon. 2011.

</span>
<span class="ltx_bibblock">Choice of plausible alternatives: An evaluation of commonsense causal
reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2011 AAAI Spring Symposium Series</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna
Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.243" title="">How good is
your tokenizer? on the monolingual performance of multilingual language
models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3118–3135,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">AAAI</em>, pages 8732–8740. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic,
Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni,
François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush,
Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang,
Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh,
Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin
Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit
Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris
Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa
Adelani, and et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2211.05100" title="">BLOOM: A
176b-parameter open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schabus et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dietmar Schabus, Marcin Skowron, and Martin Trapp. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3077136.3080711" title="">One million posts: A
data set of german online discussions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 40th International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR)</em>, pages
1241–1244, Tokyo, Japan.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster and Nakajima (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Schuster and Kaisuke Nakajima. 2012.

</span>
<span class="ltx_bibblock">Japanese and korean voice search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">2012 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pages 5149–5152. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:1508.07909</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov,
Anastasia Kozlova, and Tatiana Shavrina. 2022.

</span>
<span class="ltx_bibblock">mgpt: Few-shot learners go multilingual.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2204.07580</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Socher et&nbsp;al. (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher&nbsp;D. Manning,
Andrew Ng, and Christopher Potts. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/D13-1170" title="">Recursive deep models for
semantic compositionality over a sentiment treebank</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1631–1642, Seattle, Washington, USA.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stollenwerk (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Felix Stollenwerk. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.14780" title="">Training and evaluation of a
multilingual tokenizer for gpt-sw3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Vinh&nbsp;Q Tran, Sebastian Ruder, Jai Gupta, Hyung&nbsp;Won Chung, Dara Bahri,
Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021.

</span>
<span class="ltx_bibblock">Charformer: Fast character transformers via gradient-based subword
tokenization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toraman et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cagri Toraman, Eyup&nbsp;Halit Yilmaz, Furkan Sahinuc, and Oguzhan Ozcelik. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3578707" title="">Impact of tokenization on
language models: An analysis for turkish</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ACM Trans. Asian Low-Resour. Lang. Inf. Process.</em>, 22(4).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: open and efficient foundation language models, 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">URL https://arxiv. org/abs/2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">NIPS</em>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-5446" title="">GLUE: A multi-task
benchmark and analysis platform for natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP</em>, pages 353–355,
Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v34i05.6451" title="">Neural machine
translation with byte-level subwords</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
34(05):9154–9160.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1101" title="">A broad-coverage
challenge corpus for sentence understanding through inference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir
Kale, Adam Roberts, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Transactions of the Association for Computational Linguistics</em>,
10:291–306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1382" title="">PAWS-X: A
cross-lingual adversarial dataset for paraphrase identification</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3687–3692, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yehezkel and Pinter (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shaked Yehezkel and Yuval Pinter. 2023.

</span>
<span class="ltx_bibblock">Incorporating context into subword vocabularies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">EACL</em>, pages 623–635. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer,
and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock">Megabyte: Predicting million-byte sequences with multiscale
transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2305.07185</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">ACL (1)</em>, pages 4791–4800. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek,
Mohit Bansal, and Francisco Guzman. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.amta-research.8" title="">How robust is
neural machine translation to language imbalance in multilingual tokenizer
training?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 15th biennial conference of the
Association for Machine Translation in the Americas (Volume 1: Research
Track)</em>, pages 97–116, Orlando, USA. Association for Machine Translation in
the Americas.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Corpora</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T6">
<table class="ltx_tabular ltx_align_middle" id="A1.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T6.1.1.1.1">Name</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T6.1.1.1.2">Language</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="A1.T6.1.1.1.3">#Words</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.2.2.1">Oscar</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.2.2.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.2.2.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.T6.1.3.3.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.3.3.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.3.3.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.T6.1.4.4.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.4.4.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.4.4.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.T6.1.5.5.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.5.5.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.5.5.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.T6.1.6.6.1">Oscar</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.6.6.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.6.6.3">11.200.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.7.7.1">Pile</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.7.7.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.7.7.3">13.838.432</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.8.8">
<td class="ltx_td ltx_align_left" id="A1.T6.1.8.8.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.8.8.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.8.8.3">21.990.512</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.9.9">
<td class="ltx_td ltx_align_left" id="A1.T6.1.9.9.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.9.9.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.9.9.3">4.334.313.669</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.10.10">
<td class="ltx_td ltx_align_left" id="A1.T6.1.10.10.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.10.10.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.10.10.3">7.946.402</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.11.11">
<td class="ltx_td ltx_align_left" id="A1.T6.1.11.11.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.11.11.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.11.11.3">15.857.811</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.12.12.1">RedPajama</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.12.12.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.12.12.3">143.907.461</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.13.13">
<td class="ltx_td ltx_align_left" id="A1.T6.1.13.13.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.13.13.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.13.13.3">112.950.000</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.14.14">
<td class="ltx_td ltx_align_left" id="A1.T6.1.14.14.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.14.14.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.14.14.3">4.663.646.781</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.15.15">
<td class="ltx_td ltx_align_left" id="A1.T6.1.15.15.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.15.15.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.15.15.3">137.802.711</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.16.16">
<td class="ltx_td ltx_align_left" id="A1.T6.1.16.16.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.16.16.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.16.16.3">139.749.147</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.17.17">
<td class="ltx_td ltx_align_left" id="A1.T6.1.17.17.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.17.17.2">Code</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.17.17.3">2.052.228.788</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.18.18.1">Misc</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.18.18.2">DE</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T6.1.18.18.3">600.844.912</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.19.19">
<td class="ltx_td ltx_align_left" id="A1.T6.1.19.19.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.19.19.2">ES</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.19.19.3">186.934.269</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.20.20">
<td class="ltx_td ltx_align_left" id="A1.T6.1.20.20.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.20.20.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.20.20.3">1.337.030.904</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.21.21">
<td class="ltx_td ltx_align_left" id="A1.T6.1.21.21.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.21.21.2">IT</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.21.21.3">19.810.753</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.22.22">
<td class="ltx_td ltx_align_left" id="A1.T6.1.22.22.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T6.1.22.22.2">FR</td>
<td class="ltx_td ltx_align_right" id="A1.T6.1.22.22.3">211.147.445</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.23.23">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T6.1.23.23.1">Total</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="A1.T6.1.23.23.2"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T6.1.23.23.3">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Overview of the multilingual 70B words dataset with language, number of sampled words</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T7">
<table class="ltx_tabular ltx_align_middle" id="A1.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T7.1.1.1.1">Name</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T7.1.1.1.2">Language</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="A1.T7.1.1.1.3">#Words</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.1.2.2.1">Oscar</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.1.2.2.2">EN</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T7.1.2.2.3">56.000.000.000</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.T7.1.3.3.1">Pile</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.3.3.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.3.3.3">4.893.724.288</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.T7.1.4.4.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.4.4.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.4.4.3">5.308.974.750</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.T7.1.5.5.1">RedPajama</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.5.5.2">Code</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.5.5.3">2.299.301.635</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.T7.1.6.6.1">Misc</td>
<td class="ltx_td ltx_align_left" id="A1.T7.1.6.6.2">EN</td>
<td class="ltx_td ltx_align_right" id="A1.T7.1.6.6.3">1.497.999.327</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" id="A1.T7.1.7.7.1">Total</td>
<td class="ltx_td ltx_border_bb ltx_border_tt" id="A1.T7.1.7.7.2"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt" id="A1.T7.1.7.7.3">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Overview of the English 70B words dataset with language, number of sampled words</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Our web documents in the corpora consist of Oscars<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://oscar-project.org/" title="">https://oscar-project.org/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Abadji et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib1" title="">2021</a>)</cite>, that were generated by the ungoliant pipeline<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/oscar-project/ungoliant" title="">https://github.com/oscar-project/ungoliant</a></span></span></span> based on three Common Crawl WET Archives (2022-27, 2022-49 and 2023-14).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">The curated datasets consist of <span class="ltx_text ltx_font_italic" id="A1.p2.1.1">The Pile</span> <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib17" title="">2020a</a>)</cite>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.2">RedPajama</span> <cite class="ltx_cite ltx_citemacro_cite">Computer (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib10" title="">2023</a>)</cite>, and single datasets that do not belong to a collection. From the Pile subcorpora, we selected: Phil Archive, PMC Abstracts, PMC Extracts, OpenWebText, NIH Exporterm, and Free Law Opinions V2. From RedPajama we use: ArXiv, Books, Github, StackExchange, and Wikipedia.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">The remaining datasets are:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.p4">
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">All the News V2.0<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://metatext.io/datasets/all-the-news-2.0" title="">https://metatext.io/datasets/all-the-news-2.0</a></span></span></span> is a corpus of newspaper
articles crawled from over 26 different publications
from January 2016 to April 1, 2020.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">Bundestag - Plenarprotokolle<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bundestag.de/dokumente/protokolle/plenarprotokolle" title="">https://www.bundestag.de/dokumente/protokolle/plenarprotokolle</a></span></span></span>
comprises transcripts of sessions of the German Bundestag.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Bundesgerichtshof - Entscheidungen<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html" title="">https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html</a></span></span></span>
is a collection of decisions of the German Federal Court.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">CoStEP<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/costep/start" title="">https://pub.cl.uzh.ch/wiki/public/costep/start</a></span></span></span>
is a cleaned-up and corrected version of the EuroParl corpus<cite class="ltx_cite ltx_citemacro_cite">Graën et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib22" title="">2014</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_cite">Koehn (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib28" title="">2005</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">DCEP<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en" title="">https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en</a></span></span></span>
is a companion corpus to CoStEP, containing documents published by
the European Parliament.
<cite class="ltx_cite ltx_citemacro_cite">Hajlaoui et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib23" title="">2014</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p" id="A1.I1.i6.p1.1">DNB Dissertations<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html" title="">https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html</a></span></span></span> is a collection of dissertations from the
Deutsche Nationalbibliothek.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="A1.I1.i7.p1">
<p class="ltx_p" id="A1.I1.i7.p1.1">MAREC/IREC<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://researchdata.tuwien.ac.at/records/2zx6e-5pr64" title="">https://researchdata.tuwien.ac.at/records/2zx6e-5pr64</a></span></span></span>: The MAtrixware REsearch Collection / The Information retrieval facility Research Collection is a patent corpus of over 19 million documents from the EP, WO, US, and JP patent offices.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="A1.I1.i8.p1">
<p class="ltx_p" id="A1.I1.i8.p1.1">Medi-Notice<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice" title="">https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice</a></span></span></span>
is part of the
Zurich Parallel Corpus Collection. It is
a multilingual corpus
compiled from information leaflets for medications and pharmaceutical products
published by the Swiss Agency for Therapeutic Products.<cite class="ltx_cite ltx_citemacro_cite">Graën et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib21" title="">2019</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="A1.I1.i9.p1">
<p class="ltx_p" id="A1.I1.i9.p1.1">Swiss Policy<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus" title="">https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus</a></span></span></span> contains documents of the
Swiss Legislation Corpus <cite class="ltx_cite ltx_citemacro_cite">Höfler and Piotrowski (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib26" title="">2011</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="A1.I1.i10.p1">
<p class="ltx_p" id="A1.I1.i10.p1.1">OpenSubtitles 2018<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opus.nlpl.eu/OpenSubtitles-v2018.php" title="">https://opus.nlpl.eu/OpenSubtitles-v2018.php</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.opensubtitles.org/de/index.cgi" title="">https://www.opensubtitles.org/de/index.cgi</a></span></span></span> is a collection of translated movie subtitles.
<cite class="ltx_cite ltx_citemacro_cite">Lison and Tiedemann (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib34" title="">2016</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Tokenizer</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In our experiments, we focused on the <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">Huggingface tokenizer</span> library <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib35" title="">2023</a>)</cite> and the <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">SentencePiece</span> library <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib30" title="">2018</a>)</cite>. We use the standard settings of the SentencePiece library if not stated otherwise in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2.T8" title="Table 8 ‣ Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>.
For the HuggingFace tokenizer library <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A2.T9" title="Table 9 ‣ Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a> shows where we deviated from the standard values.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A2.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T8.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T8.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T8.9.10.1.1">Hyper-Parameter</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T8.9.10.1.2">Value(s)</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T8.1.1.2">model_type</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.1.1.1">Unigram <math alttext="|" class="ltx_Math" display="inline" id="A2.T8.1.1.1.m1.1"><semantics id="A2.T8.1.1.1.m1.1a"><mo fence="false" id="A2.T8.1.1.1.m1.1.1" stretchy="false" xref="A2.T8.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T8.1.1.1.m1.1b"><ci id="A2.T8.1.1.1.m1.1.1.cmml" xref="A2.T8.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.1.1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T8.1.1.1.m1.1d">|</annotation></semantics></math> BPE</td>
</tr>
<tr class="ltx_tr" id="A2.T8.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.2.2.2">vocab_size</th>
<td class="ltx_td ltx_align_left" id="A2.T8.2.2.1">33k <math alttext="|" class="ltx_Math" display="inline" id="A2.T8.2.2.1.m1.1"><semantics id="A2.T8.2.2.1.m1.1a"><mo fence="false" id="A2.T8.2.2.1.m1.1.1" stretchy="false" xref="A2.T8.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T8.2.2.1.m1.1b"><ci id="A2.T8.2.2.1.m1.1.1.cmml" xref="A2.T8.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.2.2.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T8.2.2.1.m1.1d">|</annotation></semantics></math>50k</td>
</tr>
<tr class="ltx_tr" id="A2.T8.3.3">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T8.3.3.2"></th>
<td class="ltx_td ltx_align_left" id="A2.T8.3.3.1">82k <math alttext="|" class="ltx_Math" display="inline" id="A2.T8.3.3.1.m1.1"><semantics id="A2.T8.3.3.1.m1.1a"><mo fence="false" id="A2.T8.3.3.1.m1.1.1" stretchy="false" xref="A2.T8.3.3.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T8.3.3.1.m1.1b"><ci id="A2.T8.3.3.1.m1.1.1.cmml" xref="A2.T8.3.3.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.3.3.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T8.3.3.1.m1.1d">|</annotation></semantics></math> 100k</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.11.2.1">character_coverage</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.11.2.2">0.9999</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.12.3.1">split_by_number</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.12.3.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.13.4.1">allow_whitespace_only</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.13.4.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.14.5.1">add_dummy_prefix</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.14.5.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.5.5.3">user_symbols</th>
<td class="ltx_td ltx_align_left" id="A2.T8.5.5.2">&lt;s&gt;,&lt;/s&gt;,&lt;pad&gt;,</td>
</tr>
<tr class="ltx_tr" id="A2.T8.7.7">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T8.7.7.3"></th>
<td class="ltx_td ltx_align_left" id="A2.T8.7.7.2">&lt;eod&gt;, &lt;ph_1&gt;,</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.9">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T8.9.9.3"></th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.9.2">…, &lt;ph_255&gt;</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.15.6.1">byte_fallback</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.15.6.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.16.7.1">max_sentence_length</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.16.7.2">4192</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.17.8.1">normalization_rule_name</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.17.8.2">NFKC</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.18.9.1">train_large_corpus</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.18.9.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.19.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.9.19.10.1">remove_extra_whitespaces</th>
<td class="ltx_td ltx_align_left" id="A2.T8.9.19.10.2">False</td>
</tr>
<tr class="ltx_tr" id="A2.T8.9.20.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T8.9.20.11.1">split_by_whitespace</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.9.20.11.2">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Overview of the SentencePiece options that we used for the training of our tokenizers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T9.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T9.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T9.2.3.1.1">Hyper-Parameter</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T9.2.3.1.2">Value(s)</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.2.4.2.1">model_type</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.2.4.2.2">BPE</td>
</tr>
<tr class="ltx_tr" id="A2.T9.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.2">vocab_size</th>
<td class="ltx_td ltx_align_left" id="A2.T9.1.1.1">33k <math alttext="|" class="ltx_Math" display="inline" id="A2.T9.1.1.1.m1.1"><semantics id="A2.T9.1.1.1.m1.1a"><mo fence="false" id="A2.T9.1.1.1.m1.1.1" stretchy="false" xref="A2.T9.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T9.1.1.1.m1.1b"><ci id="A2.T9.1.1.1.m1.1.1.cmml" xref="A2.T9.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.1.1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T9.1.1.1.m1.1d">|</annotation></semantics></math> 50k</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A2.T9.2.2.2"></th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.2.1">82k <math alttext="|" class="ltx_Math" display="inline" id="A2.T9.2.2.1.m1.1"><semantics id="A2.T9.2.2.1.m1.1a"><mo fence="false" id="A2.T9.2.2.1.m1.1.1" stretchy="false" xref="A2.T9.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A2.T9.2.2.1.m1.1b"><ci id="A2.T9.2.2.1.m1.1.1.cmml" xref="A2.T9.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.2.2.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="A2.T9.2.2.1.m1.1d">|</annotation></semantics></math> 100k</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.5.3.1">limit_alphabet</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.5.3.2">512</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.6.4.1">nfkc_normalizer</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.6.4.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.7.5.1">lowercase_normalizer</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.7.5.2">False</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.2.8.6.1">strip_accents_normalizer</th>
<td class="ltx_td ltx_align_left" id="A2.T9.2.8.6.2">True</td>
</tr>
<tr class="ltx_tr" id="A2.T9.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T9.2.9.7.1">pre_tokenizer</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T9.2.9.7.2">ByteLevel, Digits</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Overview of the Huggingface options that we used for the training of our tokenizers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>LLM Architecture and Hyperparameters</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Regarding the training architecture of our 2.6B parameter models, we followed closely the architecture of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#bib.bib4" title="">2020a</a>)</cite>.
An overview of the used architecture details and hyperparameters is given in <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A3.T10" title="Table 10 ‣ Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">10</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A3.T10">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T10.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T10.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T10.2.3.1.1">Hyper-Parameter</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T10.2.3.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T10.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T10.2.4.1.1"># Hidden Dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T10.2.4.1.2">2560</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.5.2.1"># Layers</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.5.2.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.6.3.1"># Attention-Heads</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.6.3.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.7.4.1">Sequence-Length</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.7.4.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.8.5.1">Optimizer</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.8.5.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A3.T10.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.1.1.1">Adam<math alttext="-\beta_{1}" class="ltx_Math" display="inline" id="A3.T10.1.1.1.m1.1"><semantics id="A3.T10.1.1.1.m1.1a"><mrow id="A3.T10.1.1.1.m1.1.1" xref="A3.T10.1.1.1.m1.1.1.cmml"><mo id="A3.T10.1.1.1.m1.1.1a" xref="A3.T10.1.1.1.m1.1.1.cmml">−</mo><msub id="A3.T10.1.1.1.m1.1.1.2" xref="A3.T10.1.1.1.m1.1.1.2.cmml"><mi id="A3.T10.1.1.1.m1.1.1.2.2" xref="A3.T10.1.1.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T10.1.1.1.m1.1.1.2.3" xref="A3.T10.1.1.1.m1.1.1.2.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T10.1.1.1.m1.1b"><apply id="A3.T10.1.1.1.m1.1.1.cmml" xref="A3.T10.1.1.1.m1.1.1"><minus id="A3.T10.1.1.1.m1.1.1.1.cmml" xref="A3.T10.1.1.1.m1.1.1"></minus><apply id="A3.T10.1.1.1.m1.1.1.2.cmml" xref="A3.T10.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T10.1.1.1.m1.1.1.2.1.cmml" xref="A3.T10.1.1.1.m1.1.1.2">subscript</csymbol><ci id="A3.T10.1.1.1.m1.1.1.2.2.cmml" xref="A3.T10.1.1.1.m1.1.1.2.2">𝛽</ci><cn id="A3.T10.1.1.1.m1.1.1.2.3.cmml" type="integer" xref="A3.T10.1.1.1.m1.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.1.1.1.m1.1c">-\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.T10.1.1.1.m1.1d">- italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A3.T10.1.1.2">0.9</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.2.1">Adam<math alttext="-\beta_{2}" class="ltx_Math" display="inline" id="A3.T10.2.2.1.m1.1"><semantics id="A3.T10.2.2.1.m1.1a"><mrow id="A3.T10.2.2.1.m1.1.1" xref="A3.T10.2.2.1.m1.1.1.cmml"><mo id="A3.T10.2.2.1.m1.1.1a" xref="A3.T10.2.2.1.m1.1.1.cmml">−</mo><msub id="A3.T10.2.2.1.m1.1.1.2" xref="A3.T10.2.2.1.m1.1.1.2.cmml"><mi id="A3.T10.2.2.1.m1.1.1.2.2" xref="A3.T10.2.2.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T10.2.2.1.m1.1.1.2.3" xref="A3.T10.2.2.1.m1.1.1.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T10.2.2.1.m1.1b"><apply id="A3.T10.2.2.1.m1.1.1.cmml" xref="A3.T10.2.2.1.m1.1.1"><minus id="A3.T10.2.2.1.m1.1.1.1.cmml" xref="A3.T10.2.2.1.m1.1.1"></minus><apply id="A3.T10.2.2.1.m1.1.1.2.cmml" xref="A3.T10.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T10.2.2.1.m1.1.1.2.1.cmml" xref="A3.T10.2.2.1.m1.1.1.2">subscript</csymbol><ci id="A3.T10.2.2.1.m1.1.1.2.2.cmml" xref="A3.T10.2.2.1.m1.1.1.2.2">𝛽</ci><cn id="A3.T10.2.2.1.m1.1.1.2.3.cmml" type="integer" xref="A3.T10.2.2.1.m1.1.1.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.2.2.1.m1.1c">-\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.T10.2.2.1.m1.1d">- italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.2.2">0.9</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.9.6.1">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.9.6.2">1.6e-4</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.10.7.1">Learning rate decay</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.10.7.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.11.8.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.11.8.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T10.2.12.9.1">FlashAttention</th>
<td class="ltx_td ltx_align_left" id="A3.T10.2.12.9.2">2.0</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T10.2.13.10.1">Position-Embeddings</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T10.2.13.10.2">Rotary</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Overview of the LLM hyperparameters that we used for the training.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">For training the models, we used a fork of Megatron-LM<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/Megatron-LM" title="">https://github.com/NVIDIA/Megatron-LM</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Intrinsic Tokenizer Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">Besides studying the overlap of the same algorithm on the same thesaurus, we were also interested in vocabulary overlaps across algorithms and thesauruses see <a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.F5" title="Figure 5 ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>. What we can observe is that multilingual vocabulary and English vocabulary have a rather small overlap between 24% and 34% that remains similar across increasing vocabulary sizes. Across algorithms, we can see that Unigram and BPE of SentencePiece have a slightly higher overlap than Unigram of SentencePiece and BPE of Huggingface. We think this might be due to library-specific preprocessing steps and more similar hyperparameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="553" id="A4.F5.1.g1" src="x8.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="553" id="A4.F5.2.g1" src="x9.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="761" id="A4.F5.3.g1" src="x10.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A4.F5.4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="761" id="A4.F5.4.g1" src="x11.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Vocabulary overlap between the examined tokenizers</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Computational Costs Per Word During Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2310.08754v4#A4.T11" title="Table 11 ‣ D.1 Computational Costs Per Word During Training ‣ Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a> shows the average computational training costs for processing a word during the forward and backward pass.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A4.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T11.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T11.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A4.T11.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A4.T11.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.3">Non-English</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.4">English</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.5">German</th>
</tr>
<tr class="ltx_tr" id="A4.T11.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.2.2.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A4.T11.1.2.2.2">GPT-2-50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T11.1.2.2.3">3.87</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T11.1.2.2.4">2.58</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T11.1.2.2.5">4.59</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T11.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.3.1.1" rowspan="12"><span class="ltx_text" id="A4.T11.1.3.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A4.T11.1.3.1.1.1.1" style="width:6.8pt;height:14.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-3.74pt,-3.74pt) rotate(-90deg) ;">
<span class="ltx_p" id="A4.T11.1.3.1.1.1.1.1">EN</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.3.1.2">BPE-HF-33</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.3">3.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.3.1.4.1">2.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.5">4.52</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.4.2.1">BPE-HF-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.2">3.79</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.3">2.38</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.4">4.45</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.5.3.1">BPE-HF-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.2">3.88</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.3">2.55</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.4">4.51</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.6.4.1">BPE-HF-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.2">3.96</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.3">2.67</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.4">4.58</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.7.5.1">BPE-SP-33</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.2">3.86</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.3">2.37</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.4">4.66</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.8.6.1">BPE-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.8.6.2">3.89</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.8.6.3">2.42</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.8.6.4">4.68</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.9.7.1">BPE-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.9.7.2">4.02</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.9.7.3">2.59</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.9.7.4">4.78</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.10.8.1">BPE-SP-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.2">4.11</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.3">2.71</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.4">4.84</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.11.9.1">UNI-SP-32</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.2">4.01</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.3">2.36</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.4">4.73</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.12.10.1">UNI-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.2">4.02</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.3">2.42</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.4">4.75</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.13.11.1">UNI-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.2">4.12</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.3">2.59</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.4">4.83</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.14.12.1">UNI-SP-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.14.12.2">4.21</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.14.12.3">2.71</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.14.12.4">4.88</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A4.T11.1.15.13.1" rowspan="12"><span class="ltx_text" id="A4.T11.1.15.13.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A4.T11.1.15.13.1.1.1" style="width:6.8pt;height:32.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.9pt;transform:translate(-13.04pt,-13.04pt) rotate(-90deg) ;">
<span class="ltx_p" id="A4.T11.1.15.13.1.1.1.1">MULTI</span>
</span></span></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T11.1.15.13.2">BPE-HF-33</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.3">2.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.4">2.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.5">3.04</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.16.14.1">BPE-HF-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.2">2.7</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.3">2.5</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.4">3.01</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.17.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.17.15.1">BPE-HF-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.2">2.8</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.3">2.65</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.4">3.09</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.18.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.18.16.1">BPE-HF-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.2">2.88</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.3">2.76</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.4">3.17</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.19.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.19.17.1">BPE-SP-33</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.19.17.2">2.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.19.17.3">2.55</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.19.17.4">2.99</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.20.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.20.18.1">BPE-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.20.18.2">2.67</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.20.18.3">2.57</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.20.18.4">2.95</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.21.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.21.19.1">BPE-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.21.19.2">2.76</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.21.19.3">2.72</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.21.19.4">3.03</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.22.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.22.20.1">BPE-SP-100</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.22.20.2">2.85</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.22.20.3">2.82</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.22.20.4">3.1</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.23.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.23.21.1">UNI-SP-33</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.23.21.2">2.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.23.21.3">2.55</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.23.21.4">2.94</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.24.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.24.22.1">UNI-SP-50</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.24.22.2"><span class="ltx_text ltx_font_bold" id="A4.T11.1.24.22.2.1">2.66</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.24.22.3">2.58</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.24.22.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.24.22.4.1">2.91</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.25.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T11.1.25.23.1">UNI-SP-82</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.25.23.2">2.76</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.25.23.3">2.73</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.25.23.4">2.99</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.26.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A4.T11.1.26.24.1">UNI-SP-100</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.26.24.2">2.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.26.24.3">2.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.26.24.4">3.07</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Computational training costs per word (GFLOPs) for different tokenizers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Infrastructure &amp; Computational Costs</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">We trained each of our 26 2.6B parameter models on NVIDIA A100 GPUs, and the training of each model took up to 2304 GPU hours.
Therefore, the total training costs amounted to <math alttext="\approx 59.000" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mi id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml"></mi><mo id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">≈</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">59.000</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><approx id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">absent</csymbol><cn id="A5.p1.1.m1.1.1.3.cmml" type="float" xref="A5.p1.1.m1.1.1.3">59.000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">\approx 59.000</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">≈ 59.000</annotation></semantics></math> GPU hours.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
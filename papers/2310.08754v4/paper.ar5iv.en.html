<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.08754] Tokenizer Choice For LLM Training: Negligible or Crucial?</title><meta property="og:description" content="The recent success of Large Language Models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tokenizer Choice For LLM Training: Negligible or Crucial?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Tokenizer Choice For LLM Training: Negligible or Crucial?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.08754">

<!--Generated on Wed Feb 28 01:31:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Tokenizer Choice For LLM Training: Negligible or Crucial?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mehdi Ali<sup id="id4.4.id1" class="ltx_sup">1,2</sup> <sup id="id5.5.id2" class="ltx_sup">†</sup>, Michael Fromm<sup id="id6.6.id3" class="ltx_sup">1,2</sup> <sup id="id7.7.id4" class="ltx_sup">†</sup>, Klaudia Thellmann<sup id="id8.8.id5" class="ltx_sup">3</sup> <sup id="id9.9.id6" class="ltx_sup">†</sup> 
<br class="ltx_break">Richard Rutmann<sup id="id10.10.id7" class="ltx_sup">1,2</sup>, Max Lübbering<sup id="id11.11.id8" class="ltx_sup">1,2</sup>, Johannes Leveling<sup id="id12.12.id9" class="ltx_sup">1</sup>, Katrin Klug<sup id="id13.13.id10" class="ltx_sup">1</sup>, Jan Ebert<sup id="id14.14.id11" class="ltx_sup">4</sup>, 
<br class="ltx_break">Niclas Doll<sup id="id15.15.id12" class="ltx_sup">1</sup>, Jasper Schulze Buschhoff<sup id="id16.16.id13" class="ltx_sup">1</sup>, Charvi Jain<sup id="id17.17.id14" class="ltx_sup">1,2</sup>, Alexander Arno Weber<sup id="id18.18.id15" class="ltx_sup">1,2</sup>, 
<br class="ltx_break">Lena Jurkschat<sup id="id19.19.id16" class="ltx_sup">3</sup>, Hammam Abdelwahab<sup id="id20.20.id17" class="ltx_sup">1</sup>
Chelsea John<sup id="id21.21.id18" class="ltx_sup">4</sup>, Pedro Ortiz Suarez<sup id="id22.22.id19" class="ltx_sup">5</sup>, Malte Ostendorff<sup id="id23.23.id20" class="ltx_sup">5</sup> 
<br class="ltx_break">Samuel Weinbach<sup id="id24.24.id21" class="ltx_sup">6</sup>, Rafet Sifa<sup id="id25.25.id22" class="ltx_sup">1</sup>, Stefan Kesselheim<sup id="id26.26.id23" class="ltx_sup">4</sup>, Nicolas Flores-Herr<sup id="id27.27.id24" class="ltx_sup">1</sup> 
<br class="ltx_break">
<br class="ltx_break"><sup id="id28.28.id25" class="ltx_sup">1</sup>Fraunhofer IAIS, <sup id="id29.29.id26" class="ltx_sup">2</sup>Lamarr Institute, <sup id="id30.30.id27" class="ltx_sup">3</sup>TU-Dresden, <sup id="id31.31.id28" class="ltx_sup">4</sup>FZ Jülich, <sup id="id32.32.id29" class="ltx_sup">5</sup>DFKI, <sup id="id33.33.id30" class="ltx_sup">6</sup>Aleph Alpha
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id34.id1" class="ltx_p">The recent success of <span title="" class="ltx_glossaryref">Large Language Models (LLMs)</span> has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot.
Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6 B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model’s downstream performance, training and inference costs.
In particular, we find that the common tokenizer evaluation metrics <span id="id34.id1.1" class="ltx_text ltx_font_italic">fertility</span> and <span id="id34.id1.2" class="ltx_text ltx_font_italic">parity</span> are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model’s downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English.
While English-centric tokenizers have been applied to the training of multi-lingual <span title="" class="ltx_glossaryref">LLMs</span>, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>†Equal contribution.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span title="" class="ltx_glossaryref">LLMs</span> have shown impressive capabilities in many downstream tasks in a zero/few-shot setting such as summarization, reading comprehension, translation, and commonsense reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>); Touvron et&nbsp;al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>.
To train a LLM, the currently established approach is to employ a tokenizer that splits the training documents into tokens where a token represents a word&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2000</a>)</cite>, a sub-word&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a href="#bib.bib39" title="" class="ltx_ref">2012</a>); Sennrich et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2015</a>); Wang et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>, or a single character&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2020b</a>)</cite>, and each token is represented in the model by an embedding vector that can be further processed.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The quality of a tokenizer can be assessed <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">intrinsically</span> and <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">extrinsically</span>. An intrinsic evaluation solely addresses the characteristics of tokenizers and their generated output in isolation, whereas the extrinsic evaluation measures the impact of the tokenizer on a downstream component, e.g., the <span title="" class="ltx_glossaryref">Large Language Model (LLM)</span>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While many different tokenization approaches have been proposed, ranging from character-based to word-based methods, the potential impact of different tokenizers is neglected w.r.t. <span title="" class="ltx_glossaryref">LLMs</span>, especially in the context of multilingual <span title="" class="ltx_glossaryref">LLMs</span>.
Recent work proposed by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Petrov et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> demonstrates that carelessly designed tokenizers applied to the training of multilingual <span title="" class="ltx_glossaryref">LLMs</span> result in severe inequalities and limitations across languages.
In fact, text passages translated into different languages resulted in tokenized sequences that differ in length up to a factor of 15, affecting inference costs and latency during inference.
Furthermore, it is known that an essential property for effectively learning transformer-based <span title="" class="ltx_glossaryref">LLMs</span> is the learning of long-range dependencies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>.
Given a fixed sequence length, learning to relate words far apart in the input text is impossible for languages whose text is excessively fragmented by the tokenizer.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite the importance of tokenizers and the potentially severe impact of poorly performing tokenizers, there exists so far no extensive study that holistically investigates the intrinsic and extrinsic tokenizer performance in a monolingual and multilingual setting with a focus on decoder-only models, which represent the backbone of current <span title="" class="ltx_glossaryref">LLMs</span>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Therefore, we address this gap and conduct an extensive study in which we measure the impact of the tokenizer on the model performance.
In particular, we make the following contributions:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We conduct a study investigating the intrinsic tokenizer performance.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We conduct a study investigating the extrinsic tokenizer performance, i.e., the impact of the tokenizer on the model’s downstream performance.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Investigate whether a correlation between the intrinsic and the extrinsic tokenizer performance exists.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section provides an overview of tokenization algorithms and their usage in encoder- and decoder-only transformer models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Tokenization Approaches</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Word Tokenization.</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">The most basic tokenization approach is to split sequences based on white spaces and consider each word as a token&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Bengio et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2000</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Subword tokenization.</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p"><em id="S2.SS1.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Subword tokenization</em> algorithms are data-driven tokenization approaches which can decompose words into subwords/multiple tokens and currently represent the established tokenization approach upon which <span title="" class="ltx_glossaryref">LLMs</span> rely&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>); Petrov et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>.
Because subword tokenizers decompose words into subwords, they can process out-of-vocabulary words by merging subwords from the vocabulary&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>.
Examples of popular subword tokenizers are WordPiece&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a href="#bib.bib39" title="" class="ltx_ref">2012</a>)</cite>, BPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gage (<a href="#bib.bib13" title="" class="ltx_ref">1994</a>); Sennrich et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite>, <span title="" class="ltx_glossaryref">Byte-Level BPE (BBPE)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>, and Unigram&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kudo (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Character Tokenization.</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">Tokenization can also be performed on a character level or based on UTF-8 bytes.
However, this results in an increased sequence length, which becomes computationally expensive in the transformer architecture, the current predominated architecture for <span title="" class="ltx_glossaryref">LLMs</span> due to the quadratic complexity of the self-attention layer in the sequence length&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>.
Though, several approaches have been proposed to address this limitation <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2020b</a>); Tay et&nbsp;al. (<a href="#bib.bib43" title="" class="ltx_ref">2021</a>); Xue et&nbsp;al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>); Clark et&nbsp;al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>); Yu et&nbsp;al. (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tokenizers in Transformers Models</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Encoder Models</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">Most research on tokenization has been conducted on encoder models.
<cite class="ltx_cite ltx_citemacro_citet">Rust et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> investigated whether the tokenizer choice impacts the downstream performance of multi- and monolingual BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite> models.
<cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> showed that better machine translation performance is often obtained when languages are equally sampled during the tokenizer training.
<cite class="ltx_cite ltx_citemacro_citet">Toraman et&nbsp;al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> trained several medium-sized language models for Turkish and suggested that different subword tokenizers perform roughly equivalent, whereas word- and character-level tokenizers perform drastically worse on downstream tasks.
Finally,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chirkova and Troshin (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> analyzed the effect of employing different tokenizations on code-related tasks and demonstrated that carefully configured tokenizers could reduce average sequence length up to 40% or allow for small downstream performance improvements by up to 2% at a lower compression rate.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Decoder Models</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">An overview of current mono- and multilingual <span title="" class="ltx_glossaryref">LLMs</span> is provided in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>); Shliazhko et&nbsp;al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>); Scao et&nbsp;al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>.
As part of their work, <cite class="ltx_cite ltx_citemacro_citet">Shliazhko et&nbsp;al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite> ablated different tokenizer pre-processing approaches while keeping the tokenizer algorithm, the vocabulary size and the employed implementation fixed.
In none of the other major <span title="" class="ltx_glossaryref">LLMs</span> works the extrinsic tokenizer performance has been studied.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To investigate the tokenizer impact on the model performance, we conducted an extensive ablation study. In detail, we created dedicated datasets for the training of the tokenizers and the models, trained BPE and Unigram tokenizers, and for each tokenizer we trained decoder-only models with a size of 2.6B parameters while keeping the remaining configuration (i.e., dataset and model hyper-parameters) fixed.
This allowed us to measure the tokenizer’s impact on the model’s downstream performance in isolation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">While creating our tokenizer and model training datasets, we ensure that the mixture proportions of data domains (Wikipedia, books, web text) follow the same distribution to avoid a domain shift between tokenizers training and model training.
We created <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">two datasets</em> with 70B words where one of the datasets is monolingual, containing English documents, and the second is a multilingual dataset comprised of English, German, French, Italian, and Spanish documents.
Our datasets are filtered and deduplicated and consist of web-crawled data (80%) and curated data (20%), comparable to related datasets used to train <span title="" class="ltx_glossaryref">LLMs</span>.
In the multilingual dataset, the amount of web-crawled data is equally distributed across languages in terms of number of words.
Further details about our data pipeline and the data composition are described in <a href="#A1.T5" title="In Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tokenizer</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our studies rely on the two established tokenization algorithms, BPE and Unigram, and their implementation in the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Huggingface tokenizer</span> library <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> and the <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">SentencePiece</span> library <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>.
We considered both libraries in order to investigate the effect of differences in the pre-and post-processing steps and potential differences in the implementations.
Due to missing pre-processing options for Huggingface’s Unigram implementation, which causes a large discrepancy in the resulting vocabulary compared to SentencePiece’s implementation of Unigram, we omitted the training of Unigram tokenizers based on Huggingface.
Overall, we trained 24 different tokenizers, where one-half of the tokenizers were monolingual English tokenizers, and the other half of the tokenizers were multilingual tokenizers.
Besides the tokenizer algorithm, language composition, and employed tokenizer library, we also varied the vocabulary size.
Concrete tokenizer configurations are described in the <a href="#A2" title="Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Models</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To measure the impact of our trained tokenizers on the model downstream performance, we trained one model for each tokenizer.
In particular, for each of our 24 trained tokenizers, we trained a 2.6B transformer-based decoder-only model on up to 52B tokens following the scaling law proposed by <cite class="ltx_cite ltx_citemacro_cite">Hoffmann et&nbsp;al. (<a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite>.
Additionally, serving as baselines, we trained a monolingual and a multilingual model using the pre-trained GPT-2 tokenizer. All models have been trained based on the causal language modeling training objective.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To assess the impact of the tokenizers on the model downstream performance, we first performed an intrinsic tokenizer evaluation, followed by an extrinsic evaluation, and finally, we investigated whether a correlation between both evaluation approaches is given.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The intrinsic evaluation aims to assess the generated output of tokenizers based on <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_italic">fertility</span> and <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">parity</span>.
Furthermore, the tokenizer’s vocabulary overlap with other tokenizers is computed.
The intrinsic evaluation does not assess the impact of tokenizers on the model performance.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.5" class="ltx_p">Fertility, the most common metric to evaluate a tokenizer’s performance <cite class="ltx_cite ltx_citemacro_cite">Scao et&nbsp;al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>); Stollenwerk (<a href="#bib.bib42" title="" class="ltx_ref">2023</a>); Rust et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>, is defined as the average number of tokens that are required to represent a word or document.
For a tokenizer <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">T</annotation></semantics></math> and dataset <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">A</annotation></semantics></math>, the fertility can be calculated as the number of tokens in <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">A</annotation></semantics></math> (when <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">T</annotation></semantics></math> is applied) divided by the number of words in <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">A</annotation></semantics></math>.
We calculate the fertility on a held-out set (10,000 documents), which was not used for the tokenizer training.
For calculating the words of a document, we used whitespace splitting.
Higher fertility scores correspond to weaker compression capabilities of the tokenizer.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.8" class="ltx_p">Parity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Petrov et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, which has been recently proposed, assesses how fairly a tokenizer treats equivalent sentences in different languages.
A tokenizer <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">T</annotation></semantics></math> achieves parity for language <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">A</annotation></semantics></math> with respect to language <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">B</annotation></semantics></math> if <math id="S3.SS4.p4.4.m4.2" class="ltx_Math" alttext="\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1" display="inline"><semantics id="S3.SS4.p4.4.m4.2a"><mrow id="S3.SS4.p4.4.m4.2.3" xref="S3.SS4.p4.4.m4.2.3.cmml"><mfrac id="S3.SS4.p4.4.m4.2.2" xref="S3.SS4.p4.4.m4.2.2.cmml"><mrow id="S3.SS4.p4.4.m4.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.4.m4.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS4.p4.4.m4.2.2.2.1" xref="S3.SS4.p4.4.m4.2.2.2.2.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.2" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.4.m4.2.2.2.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml">​</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml">B</mi></msub><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.3" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.SS4.p4.4.m4.2.3.1" xref="S3.SS4.p4.4.m4.2.3.1.cmml">≈</mo><mn id="S3.SS4.p4.4.m4.2.3.2" xref="S3.SS4.p4.4.m4.2.3.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.2b"><apply id="S3.SS4.p4.4.m4.2.3.cmml" xref="S3.SS4.p4.4.m4.2.3"><approx id="S3.SS4.p4.4.m4.2.3.1.cmml" xref="S3.SS4.p4.4.m4.2.3.1"></approx><apply id="S3.SS4.p4.4.m4.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2"><divide id="S3.SS4.p4.4.m4.2.2.3.cmml" xref="S3.SS4.p4.4.m4.2.2"></divide><apply id="S3.SS4.p4.4.m4.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1"><abs id="S3.SS4.p4.4.m4.1.1.1.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.2"></abs><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1"><times id="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2"></times><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3">𝐴</ci></apply></apply></apply><apply id="S3.SS4.p4.4.m4.2.2.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1"><abs id="S3.SS4.p4.4.m4.2.2.2.2.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.2"></abs><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1"><times id="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2"></times><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply><cn type="integer" id="S3.SS4.p4.4.m4.2.3.2.cmml" xref="S3.SS4.p4.4.m4.2.3.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.2c">\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1</annotation></semantics></math>, where <math id="S3.SS4.p4.5.m5.1" class="ltx_Math" alttext="s_{A}" display="inline"><semantics id="S3.SS4.p4.5.m5.1a"><msub id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml"><mi id="S3.SS4.p4.5.m5.1.1.2" xref="S3.SS4.p4.5.m5.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.5.m5.1.1.3" xref="S3.SS4.p4.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><apply id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.5.m5.1.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p4.5.m5.1.1.2.cmml" xref="S3.SS4.p4.5.m5.1.1.2">𝑠</ci><ci id="S3.SS4.p4.5.m5.1.1.3.cmml" xref="S3.SS4.p4.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">s_{A}</annotation></semantics></math> and <math id="S3.SS4.p4.6.m6.1" class="ltx_Math" alttext="s_{B}" display="inline"><semantics id="S3.SS4.p4.6.m6.1a"><msub id="S3.SS4.p4.6.m6.1.1" xref="S3.SS4.p4.6.m6.1.1.cmml"><mi id="S3.SS4.p4.6.m6.1.1.2" xref="S3.SS4.p4.6.m6.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.6.m6.1.1.3" xref="S3.SS4.p4.6.m6.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m6.1b"><apply id="S3.SS4.p4.6.m6.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.6.m6.1.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p4.6.m6.1.1.2.cmml" xref="S3.SS4.p4.6.m6.1.1.2">𝑠</ci><ci id="S3.SS4.p4.6.m6.1.1.3.cmml" xref="S3.SS4.p4.6.m6.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m6.1c">s_{B}</annotation></semantics></math> denote the sets of all sentences in the corpora of language <math id="S3.SS4.p4.7.m7.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p4.7.m7.1a"><mi id="S3.SS4.p4.7.m7.1.1" xref="S3.SS4.p4.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m7.1b"><ci id="S3.SS4.p4.7.m7.1.1.cmml" xref="S3.SS4.p4.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m7.1c">A</annotation></semantics></math> and <math id="S3.SS4.p4.8.m8.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p4.8.m8.1a"><mi id="S3.SS4.p4.8.m8.1.1" xref="S3.SS4.p4.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.8.m8.1b"><ci id="S3.SS4.p4.8.m8.1.1.cmml" xref="S3.SS4.p4.8.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.8.m8.1c">B</annotation></semantics></math>, respectively.
We use the FLORES-200 <cite class="ltx_cite ltx_citemacro_cite">Goyal et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> parallel corpus, consisting of the same sentences human-translated into 200 languages.
We calculate the parity values for each tokenizer and the four non-English languages with respect to English (see <a href="#S4.F2" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> for an overview).</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">The extrinsic evaluation aims to explicitly assess the impact of a tokenizer on the model’s downstream performance.
We selected a comprehensive set of downstream tasks covering commonsense reasoning, reading comprehension, translation, and math tasks to measure the downstream performance. For an exhaustive lists of all tasks, we refer the reader to <a href="#A5.T11" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a> in <a href="#A5" title="Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.8" class="ltx_p">Additionally, we computed the impact of a tokenizer on the average computational costs of a given model per word during training and inference.
The computational costs during training for one step including the forward and the backward pass can be estimated by</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right)," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">C</mi><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">96</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.4.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2c" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.1.7" xref="S3.E1.m1.1.1.1.1.1.7.cmml"><mi id="S3.E1.m1.1.1.1.1.1.7.2" xref="S3.E1.m1.1.1.1.1.1.7.2.cmml">h</mi><mn id="S3.E1.m1.1.1.1.1.1.7.3" xref="S3.E1.m1.1.1.1.1.1.7.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2d" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">s</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">h</mi></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml">V</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml">16</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml">h</mi></mrow></mfrac></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">𝐶</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">96</cn><ci id="S3.E1.m1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.4">𝐵</ci><ci id="S3.E1.m1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.5">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.1.6">𝑙</ci><apply id="S3.E1.m1.1.1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.7.1.cmml" xref="S3.E1.m1.1.1.1.1.1.7">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.7.2.cmml" xref="S3.E1.m1.1.1.1.1.1.7.2">ℎ</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.7.3.cmml" xref="S3.E1.m1.1.1.1.1.1.7.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝑠</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">6</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">ℎ</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2">𝑉</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2">16</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4">ℎ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p6.7" class="ltx_p">given a model with batch size <math id="S3.SS4.p6.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p6.1.m1.1a"><mi id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><ci id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">B</annotation></semantics></math>, sequence length <math id="S3.SS4.p6.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS4.p6.2.m2.1a"><mi id="S3.SS4.p6.2.m2.1.1" xref="S3.SS4.p6.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.2.m2.1b"><ci id="S3.SS4.p6.2.m2.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.2.m2.1c">s</annotation></semantics></math>, <math id="S3.SS4.p6.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS4.p6.3.m3.1a"><mi id="S3.SS4.p6.3.m3.1.1" xref="S3.SS4.p6.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.3.m3.1b"><ci id="S3.SS4.p6.3.m3.1.1.cmml" xref="S3.SS4.p6.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.3.m3.1c">l</annotation></semantics></math> layers, hidden size <math id="S3.SS4.p6.4.m4.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS4.p6.4.m4.1a"><mi id="S3.SS4.p6.4.m4.1.1" xref="S3.SS4.p6.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.4.m4.1b"><ci id="S3.SS4.p6.4.m4.1.1.cmml" xref="S3.SS4.p6.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.4.m4.1c">h</annotation></semantics></math> and vocabulary size <math id="S3.SS4.p6.5.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS4.p6.5.m5.1a"><mi id="S3.SS4.p6.5.m5.1.1" xref="S3.SS4.p6.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.5.m5.1b"><ci id="S3.SS4.p6.5.m5.1.1.cmml" xref="S3.SS4.p6.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.5.m5.1c">V</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Narayanan et&nbsp;al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>.
The costs per token can be derived by <math id="S3.SS4.p6.6.m6.1" class="ltx_Math" alttext="C_{\text{token}}=C/Bs" display="inline"><semantics id="S3.SS4.p6.6.m6.1a"><mrow id="S3.SS4.p6.6.m6.1.1" xref="S3.SS4.p6.6.m6.1.1.cmml"><msub id="S3.SS4.p6.6.m6.1.1.2" xref="S3.SS4.p6.6.m6.1.1.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.2.2" xref="S3.SS4.p6.6.m6.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.6.m6.1.1.2.3" xref="S3.SS4.p6.6.m6.1.1.2.3a.cmml">token</mtext></msub><mo id="S3.SS4.p6.6.m6.1.1.1" xref="S3.SS4.p6.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.6.m6.1.1.3" xref="S3.SS4.p6.6.m6.1.1.3.cmml"><mrow id="S3.SS4.p6.6.m6.1.1.3.2" xref="S3.SS4.p6.6.m6.1.1.3.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.3.2.2" xref="S3.SS4.p6.6.m6.1.1.3.2.2.cmml">C</mi><mo id="S3.SS4.p6.6.m6.1.1.3.2.1" xref="S3.SS4.p6.6.m6.1.1.3.2.1.cmml">/</mo><mi id="S3.SS4.p6.6.m6.1.1.3.2.3" xref="S3.SS4.p6.6.m6.1.1.3.2.3.cmml">B</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS4.p6.6.m6.1.1.3.1" xref="S3.SS4.p6.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p6.6.m6.1.1.3.3" xref="S3.SS4.p6.6.m6.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.6.m6.1b"><apply id="S3.SS4.p6.6.m6.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1"><eq id="S3.SS4.p6.6.m6.1.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1.1"></eq><apply id="S3.SS4.p6.6.m6.1.1.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.6.m6.1.1.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.6.m6.1.1.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.2.3a.cmml" xref="S3.SS4.p6.6.m6.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p6.6.m6.1.1.2.3.cmml" xref="S3.SS4.p6.6.m6.1.1.2.3">token</mtext></ci></apply><apply id="S3.SS4.p6.6.m6.1.1.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3"><times id="S3.SS4.p6.6.m6.1.1.3.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.1"></times><apply id="S3.SS4.p6.6.m6.1.1.3.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2"><divide id="S3.SS4.p6.6.m6.1.1.3.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.1"></divide><ci id="S3.SS4.p6.6.m6.1.1.3.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.3.2.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.3">𝐵</ci></apply><ci id="S3.SS4.p6.6.m6.1.1.3.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.6.m6.1c">C_{\text{token}}=C/Bs</annotation></semantics></math> and the average costs per word by <math id="S3.SS4.p6.7.m7.1" class="ltx_Math" alttext="C_{\text{word}}=C_{\text{token}}\cdot\text{fertility}" display="inline"><semantics id="S3.SS4.p6.7.m7.1a"><mrow id="S3.SS4.p6.7.m7.1.1" xref="S3.SS4.p6.7.m7.1.1.cmml"><msub id="S3.SS4.p6.7.m7.1.1.2" xref="S3.SS4.p6.7.m7.1.1.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.2.2" xref="S3.SS4.p6.7.m7.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.2.3" xref="S3.SS4.p6.7.m7.1.1.2.3a.cmml">word</mtext></msub><mo id="S3.SS4.p6.7.m7.1.1.1" xref="S3.SS4.p6.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.7.m7.1.1.3" xref="S3.SS4.p6.7.m7.1.1.3.cmml"><msub id="S3.SS4.p6.7.m7.1.1.3.2" xref="S3.SS4.p6.7.m7.1.1.3.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.3.2.2" xref="S3.SS4.p6.7.m7.1.1.3.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.3.2.3" xref="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml">token</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p6.7.m7.1.1.3.1" xref="S3.SS4.p6.7.m7.1.1.3.1.cmml">⋅</mo><mtext id="S3.SS4.p6.7.m7.1.1.3.3" xref="S3.SS4.p6.7.m7.1.1.3.3a.cmml">fertility</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.7.m7.1b"><apply id="S3.SS4.p6.7.m7.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1"><eq id="S3.SS4.p6.7.m7.1.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1.1"></eq><apply id="S3.SS4.p6.7.m7.1.1.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p6.7.m7.1.1.2.3.cmml" xref="S3.SS4.p6.7.m7.1.1.2.3">word</mtext></ci></apply><apply id="S3.SS4.p6.7.m7.1.1.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3"><ci id="S3.SS4.p6.7.m7.1.1.3.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.1">⋅</ci><apply id="S3.SS4.p6.7.m7.1.1.3.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.3.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.3.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.3"><mtext mathsize="70%" id="S3.SS4.p6.7.m7.1.1.3.2.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.3">token</mtext></ci></apply><ci id="S3.SS4.p6.7.m7.1.1.3.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3"><mtext id="S3.SS4.p6.7.m7.1.1.3.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3">fertility</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.7.m7.1c">C_{\text{word}}=C_{\text{token}}\cdot\text{fertility}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Intrinsic Tokenizer Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In our intrinsic evaluation, we first compare the fertility and parity of the trained tokenizers (Section&nbsp;<a href="#S4.SS1" title="4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and the overlap of their vocabularies (Section&nbsp;<a href="#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fertility &amp; Parity</h3>

<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x1.png" id="S4.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Non-English, multilingual documents</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x2.png" id="S4.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">English documents</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S4.F1.3.2" class="ltx_text" style="font-size:90%;">Comparison of fertility scores between monolingual (English) tokenizer and multilingual tokenizers applied to (a) English and (b) non-English, multi-lingual documents.</span></figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Applying the described fertility and parity evaluation to the mono-/multilingual tokenizers, our analysis highlights the following two major aspects, as visible in&nbsp;<a href="#S4.F1" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>&nbsp;and&nbsp;<a href="#S4.F2" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Firstly, it can be observed that applying a monolingual tokenizer to multilingual data results in significantly higher fertility and parity scores (see <a href="#S4.F1.sf1" title="In Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1(a)</span></a> and <a href="#S4.F2" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>). While multilingual tokenizers have lower fertility than monolingual English tokenizers on all non-English documents by a large margin, they are only slightly worse on tokenizing English documents, as shown in <a href="#S4.F1.sf2" title="In Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1(b)</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Secondly, with increasing vocabulary size, fertility and parity reduce in all cases, which can be explained by the tokenizer requiring fewer sub-word tokens when tokenizing text given a larger vocabulary.
However, it can be observed that for monolingual English tokenizers, the fertility is less dependent on the vocabulary when tokenizing English documents, implying that 33k might be a sufficiently large vocabulary.
This is especially interesting considering the fact that the LLaMA/LLaMA2 models have been trained with a vocabulary size of 33k&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">Touvron et&nbsp;al. </a>; Touvron et&nbsp;al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2310.08754/assets/x3.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Comparison of parity scores between monolingual (English) tokenizer and multilingual tokenizers applied multi-lingual documents.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Vocabulary Overlap</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To analyze the tokenizer similarity, we calculated the vocabulary overlap. Particularly, we assess Huggingface’s and SentencePiece’s BPE implementations. Their vocabulary overlap is depicted in <a href="#S4.T1" title="In 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The overlap is roughly constant across different vocabulary sizes, and the total overlap tends to be rather low, despite being the identical algorithm only implemented by two different libraries. Consequently, the tokenizers produce different tokenized sequences, possibly affecting model training and downstream performance.
Investigating the underlying reasons, the low overlap might be attributed to different configuration and pre-processing options in these libraries. Due to the larger thesaurus in multilingual documents, the overlap for the multilingual tokenizer is lower than for the English tokenizers.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">33k</th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">50k</th>
<th id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">82k</th>
<th id="S4.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">100k</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<th id="S4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">English</th>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.77</td>
<td id="S4.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.76</td>
<td id="S4.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.74</td>
<td id="S4.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.74</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<th id="S4.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Multilingual</th>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">0.62</td>
<td id="S4.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.62</td>
<td id="S4.T1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">0.62</td>
<td id="S4.T1.2.3.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Vocabulary overlap between the HuggingFace and SentencePiece BPE tokenizer for different vocab sizes.</span></figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x4.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Non-English documents</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x5.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">English documents</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x6.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">German documents</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;">Average compute (GFLOPs) required to process a single word within (a) multilingual, (b) English, and (c) German documents within a full <span id="S4.F3.4.2.1" class="ltx_text ltx_font_bold">training</span> pass (including the backward pass).</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Extrinsic Tokenizer Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the following, we describe the results of our extrinsic evaluation of tokenizers.
<a href="#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.1</span></a> describes the experimental setup, <a href="#S5.SS2" title="5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.2</span></a> presents the downstream performance of the trained models based on the investigated tokenizers, and <a href="#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.3</span></a> analyses the computational costs associated with each tokenizer when employed in a specific model.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To assess the impact of the tokenizers on the model downstream performance, we trained a decoder-only transformer model of size 2.6 B for each tokenizer.
We trained our models for 52.6 B tokens following the scaling law proposed by Hoffman <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Hoffmann et&nbsp;al. (<a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite> based on the causal language modeling training objective. The hyper-parameters are described in <a href="#A3.T8" title="In Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a> in the <a href="#A3" title="Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We evaluated our models in zero-shot settings on a wide range of tasks, covering commonsense reasoning (HellaSwag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite>, WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Sakaguchi et&nbsp;al. (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, ARC easy and challenge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>), world knowledge (TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Joshi et&nbsp;al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>), reading comprehension (LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Paperno et&nbsp;al. (<a href="#bib.bib33" title="" class="ltx_ref">2016</a>)</cite> and their machine translated versions for German, Spanish, French, and Italian, RACE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lai et&nbsp;al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>, BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>), math (MATH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>), multilingual natural language inference (XNLI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Conneau et&nbsp;al. (<a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Downstream Performance</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We split our analysis of the downstream performance into several parts.
First, we discuss the overall results obtained for the investigated tokenizers, followed by presenting the impact of the tokenizer library (Section&nbsp;<a href="#S5.SS2.SSS1" title="5.2.1 Impact of the Tokenizer Library ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>), the impact of the tokenizer algorithm (Section&nbsp;<a href="#S5.SS2.SSS2" title="5.2.2 Impact of the Tokenizer Algorithm ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>), and the impact of the vocabulary size (Section &nbsp;<a href="#S5.SS2.SSS3" title="5.2.3 Impact of the Tokenizer Vocabulary ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>).</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x7.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Monolingual tokenizers.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x8.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Multilingual tokenizers.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">Top-1/-2/-3 count for mono-/multilingual tokenizers across mono-/multilingual tasks the tasks were evaluated on the accuracy metric.</span></figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T2.2.1.1.2.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.3.1" class="ltx_text" style="font-size:80%;">DE</span></th>
<th id="S5.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.4.1" class="ltx_text" style="font-size:80%;">FR</span></th>
<th id="S5.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.5.1" class="ltx_text" style="font-size:80%;">IT</span></th>
<th id="S5.T2.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.6.1" class="ltx_text" style="font-size:80%;">ES</span></th>
<th id="S5.T2.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.7.1" class="ltx_text" style="font-size:80%;">EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<th id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T2.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:11.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:11.4pt;transform:translate(-2.99pt,-2.99pt) rotate(-90deg) ;">
<span id="S5.T2.2.2.1.1.1.1.1" class="ltx_p">EN</span>
</span></span></span></th>
<th id="S5.T2.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.2.2.1.2.1" class="ltx_text" style="font-size:80%;">BPE-HF</span></th>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.7.1" class="ltx_text" style="font-size:80%;">45.44</span></td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr">
<th id="S5.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.2.3.2.1.1" class="ltx_text" style="font-size:80%;">BPE-SP</span></th>
<td id="S5.T2.2.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.6" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">46.31</span></td>
</tr>
<tr id="S5.T2.2.4.3" class="ltx_tr">
<th id="S5.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.2.4.3.1.1" class="ltx_text" style="font-size:80%;">UNI-SP</span></th>
<td id="S5.T2.2.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.6.1" class="ltx_text" style="font-size:80%;">46.01</span></td>
</tr>
<tr id="S5.T2.2.5.4" class="ltx_tr">
<th id="S5.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S5.T2.2.5.4.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T2.2.5.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:26.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-10.43pt,-10.43pt) rotate(-90deg) ;">
<span id="S5.T2.2.5.4.1.1.1.1" class="ltx_p">MULTI</span>
</span></span></span></th>
<th id="S5.T2.2.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.2.5.4.2.1" class="ltx_text" style="font-size:80%;">BPE-HF</span></th>
<td id="S5.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.3.1" class="ltx_text" style="font-size:80%;">36.72</span></td>
<td id="S5.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.4.1" class="ltx_text" style="font-size:80%;">36.55</span></td>
<td id="S5.T2.2.5.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.5.1" class="ltx_text" style="font-size:80%;">35.34</span></td>
<td id="S5.T2.2.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.6.1" class="ltx_text" style="font-size:80%;">42.28</span></td>
<td id="S5.T2.2.5.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.7.1" class="ltx_text" style="font-size:80%;">44.45</span></td>
</tr>
<tr id="S5.T2.2.6.5" class="ltx_tr">
<th id="S5.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.2.6.5.1.1" class="ltx_text" style="font-size:80%;">BPE-SP</span></th>
<td id="S5.T2.2.6.5.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.2.1" class="ltx_text" style="font-size:80%;">36.96</span></td>
<td id="S5.T2.2.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.3.1" class="ltx_text" style="font-size:80%;">36.95</span></td>
<td id="S5.T2.2.6.5.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36.12</span></td>
<td id="S5.T2.2.6.5.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.5.1" class="ltx_text" style="font-size:80%;">41.96</span></td>
<td id="S5.T2.2.6.5.6" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">44.92</span></td>
</tr>
<tr id="S5.T2.2.7.6" class="ltx_tr">
<th id="S5.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T2.2.7.6.1.1" class="ltx_text" style="font-size:80%;">UNI-SP</span></th>
<td id="S5.T2.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.38</span></td>
<td id="S5.T2.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.05</span></td>
<td id="S5.T2.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.4.1" class="ltx_text" style="font-size:80%;">35.70</span></td>
<td id="S5.T2.2.7.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.56</span></td>
<td id="S5.T2.2.7.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.6.1" class="ltx_text" style="font-size:80%;">44.21</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.5.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S5.T2.6.2" class="ltx_text" style="font-size:113%;">Impact of the tokenizer algorithm and the tokenizer training library, scores are averaged over the vocabulary sizes.</span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.2.1.1.2.1" class="ltx_text" style="font-size:80%;">Vocab</span></th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.3.1" class="ltx_text" style="font-size:80%;">DE</span></th>
<th id="S5.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.4.1" class="ltx_text" style="font-size:80%;">FR</span></th>
<th id="S5.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.5.1" class="ltx_text" style="font-size:80%;">IT</span></th>
<th id="S5.T3.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.6.1" class="ltx_text" style="font-size:80%;">ES</span></th>
<th id="S5.T3.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.7.1" class="ltx_text" style="font-size:80%;">EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S5.T3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T3.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:11.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:11.4pt;transform:translate(-2.99pt,-2.99pt) rotate(-90deg) ;">
<span id="S5.T3.2.2.1.1.1.1.1" class="ltx_p">EN</span>
</span></span></span></th>
<th id="S5.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.2.2.1.2.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">46.34</span></td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.3.2.1.1" class="ltx_text" style="font-size:80%;">50</span></th>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.6.1" class="ltx_text" style="font-size:80%;">46.30</span></td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.4.3.1.1" class="ltx_text" style="font-size:80%;">82</span></th>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.6.1" class="ltx_text" style="font-size:80%;">45.29</span></td>
</tr>
<tr id="S5.T3.2.5.4" class="ltx_tr">
<th id="S5.T3.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.5.4.1.1" class="ltx_text" style="font-size:80%;">100</span></th>
<td id="S5.T3.2.5.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.6.1" class="ltx_text" style="font-size:80%;">45.88</span></td>
</tr>
<tr id="S5.T3.2.6.5" class="ltx_tr">
<th id="S5.T3.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="4"><span id="S5.T3.2.6.5.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T3.2.6.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:26.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-10.43pt,-10.43pt) rotate(-90deg) ;">
<span id="S5.T3.2.6.5.1.1.1.1" class="ltx_p">MULTI</span>
</span></span></span></th>
<th id="S5.T3.2.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.2.6.5.2.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S5.T3.2.6.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.3.1" class="ltx_text" style="font-size:80%;">36.72</span></td>
<td id="S5.T3.2.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.4.1" class="ltx_text" style="font-size:80%;">36.22</span></td>
<td id="S5.T3.2.6.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.5.1" class="ltx_text" style="font-size:80%;">35.20</span></td>
<td id="S5.T3.2.6.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.6.1" class="ltx_text" style="font-size:80%;">41.76</span></td>
<td id="S5.T3.2.6.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.7.1" class="ltx_text" style="font-size:80%;">44.71</span></td>
</tr>
<tr id="S5.T3.2.7.6" class="ltx_tr">
<th id="S5.T3.2.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.7.6.1.1" class="ltx_text" style="font-size:80%;">50</span></th>
<td id="S5.T3.2.7.6.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.2.1" class="ltx_text" style="font-size:80%;">36.57</span></td>
<td id="S5.T3.2.7.6.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.3.1" class="ltx_text" style="font-size:80%;">36.34</span></td>
<td id="S5.T3.2.7.6.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.4.1" class="ltx_text" style="font-size:80%;">34.60</span></td>
<td id="S5.T3.2.7.6.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.5.1" class="ltx_text" style="font-size:80%;">42.22</span></td>
<td id="S5.T3.2.7.6.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.6.1" class="ltx_text" style="font-size:80%;">43.91</span></td>
</tr>
<tr id="S5.T3.2.8.7" class="ltx_tr">
<th id="S5.T3.2.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.8.7.1.1" class="ltx_text" style="font-size:80%;">82</span></th>
<td id="S5.T3.2.8.7.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.40</span></td>
<td id="S5.T3.2.8.7.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.3.1" class="ltx_text" style="font-size:80%;">37.22</span></td>
<td id="S5.T3.2.8.7.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36.21</span></td>
<td id="S5.T3.2.8.7.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.5.1" class="ltx_text" style="font-size:80%;">42.30</span></td>
<td id="S5.T3.2.8.7.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">44.75</span></td>
</tr>
<tr id="S5.T3.2.9.8" class="ltx_tr">
<th id="S5.T3.2.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S5.T3.2.9.8.1.1" class="ltx_text" style="font-size:80%;">100</span></th>
<td id="S5.T3.2.9.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.2.1" class="ltx_text" style="font-size:80%;">37.17</span></td>
<td id="S5.T3.2.9.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.45</span></td>
<td id="S5.T3.2.9.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.4.1" class="ltx_text" style="font-size:80%;">36.19</span></td>
<td id="S5.T3.2.9.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.64</span></td>
<td id="S5.T3.2.9.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.6.1" class="ltx_text" style="font-size:80%;">44.55</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.5.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S5.T3.6.2" class="ltx_text" style="font-size:113%;">Impact of the vocabulary size on the downstream accuracy.
The accuracy scores are averaged over the libraries and tokenizer algorithms</span></figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">To provide a holistic performance overview, we assess the impact of the tokenizers on the model performance based on various tasks and analyze the results from different perspectives instead of providing solely a single aggregated metric.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">This holistic evaluation is presented in <a href="#S5.F4" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>, which expresses how often a tokenizer is among the top-1/-2/-3 performing tokenizers, <a href="#A5.F7" title="In Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a>, which shows the distribution of the accuracy scores across tasks, and in <a href="#A5.T9" title="In Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a> which summarizes the average performances across all tasks.
We dissect the results from different perspectives in the following subsections.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Monolingual Tokenizer</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.2" class="ltx_p">Among the monolingual tokenizers, there can be significant performance differences.
For instance, <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4(a)</span></a> depicts that the GPT-2 tokenizer is overall most often across the top-<math id="S5.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation></semantics></math> (with <math id="S5.SS2.SSS0.Px1.p1.2.m2.3" class="ltx_Math" alttext="k\in\{1,2,3\}" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.2.m2.3a"><mrow id="S5.SS2.SSS0.Px1.p1.2.m2.3.4" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.cmml"><mi id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2.cmml">k</mi><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1.cmml">∈</mo><mrow id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S5.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">1</mn><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S5.SS2.SSS0.Px1.p1.2.m2.2.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.2.2.cmml">2</mn><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.3" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S5.SS2.SSS0.Px1.p1.2.m2.3.3" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.3.cmml">3</mn><mo stretchy="false" id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.4" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.2.m2.3b"><apply id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4"><in id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1"></in><ci id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2">𝑘</ci><set id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2"><cn type="integer" id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1">1</cn><cn type="integer" id="S5.SS2.SSS0.Px1.p1.2.m2.2.2.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.2.2">2</cn><cn type="integer" id="S5.SS2.SSS0.Px1.p1.2.m2.3.3.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.3">3</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.2.m2.3c">k\in\{1,2,3\}</annotation></semantics></math>) performing tokenizers, whereas the EN-BPE-HF-32 tokenizer is never the best-performing tokenizer.
Though the count-based metric already provides a first overview of the model performance, it does not illustrate the difference in performance.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">Therefore, we also looked at the accuracy distribution across all tasks that employ accuracy as the evaluation metric.
<a href="#A5.F7.sf1" title="In Figure 7 ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">7(a)</span></a> demonstrates that the EN-BPE-SP-32 tokenizer, on average, is across all tasks among the best-performing tokenizers and has the highest median value.
At the same time, it also expresses a large variance across tasks compared to other tokenizers such as EN-BPE-HF-32, implying that for a set of tasks, the tokenizer performs very well, while on other tasks, it performs comparably poorly.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p3.1" class="ltx_p">Aggregated metrics provide a good impression of the overall performance but do not demonstrate potentially large performance differences for single tasks.
Therefore, we listed in <a href="#A5.T11" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a> the performance difference across all tasks between the best-performing tokenizer and worst-performing tokenizer on this task.
The performance difference is huge for various tasks, which is especially interesting since only the tokenizer has been changed while the model configuration is the same.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p4" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p4.1" class="ltx_p">Overall, the GPT-2 tokenizer and the EN-BPE-SP-32 tokenizer are strong tokenizers for monolingual English models.
Interestingly, SentencePiece’s implementation of BPE with a vocabulary size of 32k has been used for LLaMA2.
Our results provide empirical and transparent evidence that both tokenizers/tokenizer settings are reasonable choices for training monolingual English models.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multilingual Tokenizer</h5>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">Our multilingual tokenizer experiments further support that different tokenizers can express large performance differences.
Based on our count-based metric (Figure&nbsp;<a href="#S5.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>), the differences across tokenizers are even larger than for monolingual tokenizers.
While Multi-UNI-SP-100 and Multi-UNI-SP-82 are most often among the strongest tokenizers, Multi-BPE-HF-32 is only a single time across the top-performing tokenizers.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p2.1" class="ltx_p">The accuracy distributions depicted in Figure&nbsp;<a href="#A5.F7.sf2" title="Figure 7(b) ‣ Figure 7 ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a> further show that both the tokenizers mentioned above are among the best-performing tokenizers and that specific tokenizers such as the already mentioned tokenizer perform worse.
Figure&nbsp;<a href="#A5.F7.sf2" title="Figure 7(b) ‣ Figure 7 ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a> additionally demonstrates that the GPT-2 tokenizer performs poorly.
Compared to monolingual models, we can observe that all tokenizers express a similar variance across tasks.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p3.1" class="ltx_p">In reference to Table &nbsp;<a href="#A5.T11" title="Table 11 ‣ E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, the analysis of single tasks reveals that for multilingual tokenizers, the performance difference between tasks can be huge.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p4" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p4.1" class="ltx_p">Multi-UNI-SP-100 and Multi-UNI-SP-82 are excellent choices, while employing a pre-trained GPT-2 tokenizer should be <span id="S5.SS2.SSS0.Px2.p4.1.1" class="ltx_text ltx_font_bold">omitted</span> to train multilingual models.</p>
</div>
</section>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Impact of the Tokenizer Library</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p"><a href="#S5.T2" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> demonstrates that BPE-SP, on average, outperforms BPE-HF across all tasks.
In the multilingual setting, BPE-SP outperforms BPE-HF for all languages except for Spanish.
The performance differences might be attributed to the differences in implementation details of the tokenisers’ pre-and postprocessing, which could affect the vocabulary creation (see <a href="#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>) and, consequently, the downstream performance.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Impact of the Tokenizer Algorithm</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p"><a href="#S5.T2" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> shows that for English tokenizers and models, BPE works better compared to Unigram. For multilingual models and tokenizers, we can see that for Spanish, German and French unigram can result in an improvement in performance. German and the romance languages are generally inflected languages, while English, although primarily analytic, does have some agglutinative features. Recent work showed Unigram tokenizers are better able to find morphologically accurate segmentations <cite class="ltx_cite ltx_citemacro_cite">Bostrom and Durrett (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>); Park et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> which might help these languages.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Impact of the Tokenizer Vocabulary</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">Analyzing the impact of the vocabulary size revealed that in the monolingual English setting, the smaller/medium-sized vocabulary performs better (<a href="#S5.T3" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>) whereas in the multilingual setting, larger vocabulary sizes result in better downstream performance(<a href="#S5.T3" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>). This is in line with the vocabulary size of current English LLMs such as LLama.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Computational Costs</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Even with an identical vocabulary size, two trained tokenizers most likely split a text passage in sequences of different lengths, resulting in different computation costs when processed by a model.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Our analysis revealed the following two insights.
Firstly, increasing the vocabulary size decreases computational costs (<a href="#S4.F3" title="In 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>).
This implies that the reduction in the overall number of tokens represented by lower fertility (<a href="#S4.F1" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>) compensates for the additional computational costs that a larger vocabulary size introduces in the model.
In contrast, during inference, the computational costs increase with increasing vocabulary size and at least a vocabulary size of 50K, i.e., solely the forward pass without the backward pass does not compensate for the additional computational costs introduced by a larger vocabulary size in the model.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Secondly, the computational training costs for multilingual documents are significantly lower for multilingual tokenizers than monolingual English tokenizers for training (<a href="#S4.F3.sf1" title="In Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(a)</span></a>) and for inference (in Appendix <a href="#A5.F8" title="In Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>).
In extreme cases, up to 68% additional training costs (see EN-UNI-SP-100352 vs. Multi-UNI-SP-50176 in Appendix <a href="#A5.T10" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">10</span></a> and <a href="#S4.F3.sf3" title="In Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(c)</span></a>) as it is the case for training.
At the same time, the difference in computational costs between multilingual and monolingual English tokenizers for processing English documents is much lower (<a href="#S4.F3.sf2" title="In Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(b)</span></a>).
A complete overview of the computational costs can be found in <a href="#A5.T10" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Correlation Between Intrinsic And Extrinsic Tokenizer Performance</h2>

<figure id="S6.F5" class="ltx_figure"><img src="/html/2310.08754/assets/x9.png" id="S6.F5.g1" class="ltx_graphics ltx_img_square" width="242" height="242" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S6.F5.3.2" class="ltx_text" style="font-size:90%;">Correlation of fertility/parity scores and downstream task performance for all five languages. We evaluated English models on English tasks, whereas multilingual models are evaluated across all four non-English tasks.</span></figcaption>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This section investigates a possible predictive relationship of intrinsic tokenizer metrics (fertilty and parity) to the extrinsic model downstream performance.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">As highlighted in the correlation heatmaps in <a href="#S6.F5" title="In 6 Correlation Between Intrinsic And Extrinsic Tokenizer Performance ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>, we find that there is no distinct correlation across all tasks and languages, demanding a more granular analysis.
While for non-English tasks, we mainly observe a correlation between low fertility and higher downstream performance, the non-English tasks yield seemingly random positive and negative correlations.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Taking the fertility trends with varying vocabulary sizes (see <a href="#S4.F1" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>) into consideration, we hypothesize that fertility only correlates with downstream performance in certain language-specific vocabulary size limits. For the English language, the tokenizers already provide low, close-to-convergence fertility scores for vocabulary sizes of 32k tokens. While additional tokens yield only minute fertility improvements, we presume that they do not capture morphological segmentations and, thus, can harm downstream performance and significantly increase the computation costs (see <a href="#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">5.3</span></a>) in the end.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">In contrast, for multilingual tokenizers, we observe significant fertility improvements with increasing vocabulary sizes. Due to the larger thesaurus induced by the additional languages, the tokenizer requires a larger vocabulary to allow a model to perform convincingly on all languages. Therefore, only within the non-convergence vocabulary range, we achieve a strong, negative correlation between fertility and downstream performance with varying vocabulary sizes.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">In conclusion, intrinsic tokenizer metrics such as fertility and parity need to be taken with a grain of salt and supposedly are only predictive of downstream model performance in certain bounds. Furthermore, the analysis showed that low fertility scores can be regarded as a necessary criterion but not as a sufficient one.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work represents a fundamental step to a better understanding of the impact of the tokenizer on the models’ downstream performance.
We have shown that training tokenizers with a balanced share across languages achieve comparable low fertility and parity scores across all languages, which has important implications.
Lower fertility results in less computational costs and enables the model to learn long-range dependencies in limited context windows.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Furthermore, we highlight that the tokenizer choice can significantly impact the model’s downstream performance.
We could show that it is beneficial to employ Unigram tokenizers with a large vocabulary size to train multilingual models.
This is contrary to the current monolingual setting where BPE-based tokenizers are used together with a smaller/medium-sized vocabulary size.
This also implies that insights obtained for monolingual settings are not transferable to multilingual settings and, therefore, require dedicated studies.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Finally, we could show that there is no clear correlation between intrinsic and extrinsic tokenizer performance, but the correlation is rather task-specific.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) through the project OpenGPT-X (project no. 68GX21007D) as well as by the Federal Ministry of Education and Research of Germany and the state of North-Rhine Westphalia as part of the Lamarr-Institute for Machine, LAMARR22B.
The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the GCS Supercomputer JUWELS at Jülich Supercomputing Centre (JSC) as well as the Center for Information Services and High Performance Computing [Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH)] at TU Dresden for providing its facilities for high throughput calculations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Javier&nbsp;Ortiz Suárez, Laurent Romary, and Benoît
Sagot. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14618/ids-pub-10468" title="" class="ltx_ref ltx_href">Ungoliant: An
optimized pipeline for the generation of a very large-scale multilingual web
corpus</a>.

</span>
<span class="ltx_bibblock">In Harald Lüngen, Marc Kupietz, Piotr Bański, Adrien Barbaresi,
Simon Clematide, and Ines Pisetta, editors, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop
on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12
July 2021 (Online-Event)</em>, pages 1 – 9. Leibniz-Institut für Deutsche
Sprache, Mannheim.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. (2000)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 13.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bostrom and Durrett (2020)</span>
<span class="ltx_bibblock">
Kaj Bostrom and Greg Durrett. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.414" title="" class="ltx_ref ltx_href">Byte
pair encoding is suboptimal for language model pretraining</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 4617–4624, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020a)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al. 2020a.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020b)</span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chirkova and Troshin (2022)</span>
<span class="ltx_bibblock">
Nadezhda Chirkova and Sergey Troshin. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rd-G1nO-Jbq" title="" class="ltx_ref ltx_href">CodeBPE:
Investigating subtokenization options for large language model pretraining on
source code</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Deep Learning for Code Workshop</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">NAACL-HLT (1)</em>, pages 2924–2936. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00448" title="" class="ltx_ref ltx_href">Canine: Pre-training an
efficient tokenization-free encoder for language representation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:73–91.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the AI2
reasoning challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1803.05457.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)</span>
<span class="ltx_bibblock">
Together Computer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_href">Redpajama: An open source recipe to reproduce llama training dataset</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel&nbsp;R.
Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock">XNLI: evaluating cross-lingual sentence representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, pages 2475–2485. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gage (1994)</span>
<span class="ltx_bibblock">
Philip Gage. 1994.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:59804030" title="" class="ltx_ref ltx_href">A new
algorithm for data compression</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The C Users Journal archive</em>, 12:23–38.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020a)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy. 2020a.

</span>
<span class="ltx_bibblock">The Pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020b)</span>
<span class="ltx_bibblock">
Yingqiang Gao, Nikola&nbsp;I. Nikolov, Yuhuang Hu, and Richard&nbsp;H.R. Hahnloser.
2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.145" title="" class="ltx_ref ltx_href">Character-level translation with self-attention</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1591–1604, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and
Angela Fan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00474" title="" class="ltx_ref ltx_href">The Flores-101
Evaluation Benchmark for Low-Resource and Multilingual Machine Translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:522–538.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Johannes Graën, Tannon Kew, Anastassia Shaitarova, and Martin Volk. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14618/ids-pub-9020" title="" class="ltx_ref ltx_href">Modelling large
parallel corpora: The zurich parallel corpus collection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Workshop on Challenges in the
Management of Large Corpora (CMLC)</em>, pages 1–8. Leibniz-Institut für
Deutsche Sprache.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
J.&nbsp;Graën, D.&nbsp;Batinic, and M.&nbsp;Volk. 2014.

</span>
<span class="ltx_bibblock">Cleaning the Europarl corpus for linguistic applications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Konvens 2014</em>. Stiftung Universität Hildesheim.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hajlaoui et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
Najeh Hajlaoui, David Kolovratnik, Jaakko Vaeyrynen, Ralf Steinberger, and
Dániel Varga. 2014.

</span>
<span class="ltx_bibblock">DCEP - Digital corpus of the European parliament.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. LREC 2014 (Language Resources and Evaluation
Conference). Reykjavik, Iceland</em>, pages 3164–3171.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
Tang, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the MATH dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">NeurIPS Datasets and Benchmarks</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George
van&nbsp;den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén
Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre.
2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">An empirical analysis of compute-optimal large language model training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume&nbsp;35, pages 30016–30030. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van&nbsp;den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and Laurent Sifre. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2203.15556" title="" class="ltx_ref ltx_href">Training
compute-optimal large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2203.15556.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Höfler and Piotrowski (2011)</span>
<span class="ltx_bibblock">
Stefan Höfler and Michael Piotrowski. 2011.

</span>
<span class="ltx_bibblock">Building corpora for the philological study of Swiss legal texts.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Journal for Language Technology and Computational Linguistics</em>,
26(2):77–89.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel&nbsp;S. Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>, pages 1601–1611. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
P.&nbsp;Koehn. 2005.

</span>
<span class="ltx_bibblock">Europarl: A parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Machine Translation Summit, volume 5</em>, pages 79––86.
Asia-Pacific Association for Machine Translation (AAMT).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo (2018)</span>
<span class="ltx_bibblock">
Taku Kudo. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1007" title="" class="ltx_ref ltx_href">Subword regularization:
Improving neural network translation models with multiple subword
candidates</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 66–75,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">EMNLP 2018</em>, page&nbsp;66.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard&nbsp;H. Hovy. 2017.

</span>
<span class="ltx_bibblock">RACE: large-scale reading comprehension dataset from examinations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, pages 785–794. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 9019–9052.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)</span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock">OpenSubtitles2016: Extracting large parallel corpora from movie
and tv subtitles.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on Language
Resources and Evaluation (LREC-2016)</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moi and Patry (2023)</span>
<span class="ltx_bibblock">
Anthony Moi and Nicolas Patry. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/tokenizers" title="" class="ltx_ref ltx_href">HuggingFace’s
Tokenizers</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3458817.3476209" title="" class="ltx_ref ltx_href">Efficient
large-scale language model training on gpu clusters using megatron-lm</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis</em>, SC ’21, New York,
NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham,
Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
Fernández. 2016.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse
context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>. The Association for Computer Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Hyunji&nbsp;Hayley Park, Katherine&nbsp;J. Zhang, Coleman Haley, Kenneth Steimel, Han
Liu, and Lane Schwartz. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00365" title="" class="ltx_ref ltx_href">Morphology Matters: A
Multilingual Language Modeling Analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:261–276.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La&nbsp;Malfa, Philip&nbsp;HS Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15425</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna
Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.243" title="" class="ltx_ref ltx_href">How good is
your tokenizer? on the monolingual performance of multilingual language
models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3118–3135,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, pages 8732–8740. AAAI Press.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic,
Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni,
François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush,
Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang,
Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh,
Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin
Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit
Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris
Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa
Adelani, and et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2211.05100" title="" class="ltx_ref ltx_href">BLOOM: A
176b-parameter open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.05100.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster and Nakajima (2012)</span>
<span class="ltx_bibblock">
Mike Schuster and Kaisuke Nakajima. 2012.

</span>
<span class="ltx_bibblock">Japanese and korean voice search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">2012 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pages 5149–5152. IEEE.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2015)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.07909</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov,
Anastasia Kozlova, and Tatiana Shavrina. 2022.

</span>
<span class="ltx_bibblock">mgpt: Few-shot learners go multilingual.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.07580</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stollenwerk (2023)</span>
<span class="ltx_bibblock">
Felix Stollenwerk. 2023.

</span>
<span class="ltx_bibblock">Training and evaluation of a multilingual tokenizer for gpt-sw3.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.14780</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Vinh&nbsp;Q Tran, Sebastian Ruder, Jai Gupta, Hyung&nbsp;Won Chung, Dara Bahri,
Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021.

</span>
<span class="ltx_bibblock">Charformer: Fast character transformers via gradient-based subword
tokenization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toraman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Cagri Toraman, Eyup&nbsp;Halit Yilmaz, Furkan Sahinuc, and Oguzhan Ozcelik. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3578707" title="" class="ltx_ref ltx_href">Impact of tokenization on
language models: An analysis for turkish</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Asian Low-Resour. Lang. Inf. Process.</em>, 22(4).

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: open and efficient foundation language models, 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">URL https://arxiv. org/abs/2302.13971</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.09288.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, pages 5998–6008.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v34i05.6451" title="" class="ltx_ref ltx_href">Neural machine
translation with byte-level subwords</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
34(05):9154–9160.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir
Kale, Adam Roberts, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:291–306.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer,
and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock">Megabyte: Predicting million-byte sequences with multiscale
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07185</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>, pages 4791–4800. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek,
Mohit Bansal, and Francisco Guzman. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.amta-research.8" title="" class="ltx_ref ltx_href">How robust is
neural machine translation to language imbalance in multilingual tokenizer
training?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th biennial conference of the
Association for Machine Translation in the Americas (Volume 1: Research
Track)</em>, pages 97–116, Orlando, USA. Association for Machine Translation in
the Americas.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Corpora</h2>

<figure id="A1.T4" class="ltx_table">
<table id="A1.T4.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.2.1.1" class="ltx_tr">
<td id="A1.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Name</td>
<td id="A1.T4.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Language</td>
<td id="A1.T4.2.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">#Words</td>
</tr>
<tr id="A1.T4.2.2.2" class="ltx_tr">
<td id="A1.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Oscar</td>
<td id="A1.T4.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.2.2.3" class="ltx_td ltx_align_right ltx_border_t">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.3.3" class="ltx_tr">
<td id="A1.T4.2.3.3.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.3.3.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.3.3.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.4.4" class="ltx_tr">
<td id="A1.T4.2.4.4.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.4.4.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.4.4.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.5.5" class="ltx_tr">
<td id="A1.T4.2.5.5.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.5.5.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.5.5.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.6.6" class="ltx_tr">
<td id="A1.T4.2.6.6.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.6.6.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.6.6.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.7.7" class="ltx_tr">
<td id="A1.T4.2.7.7.1" class="ltx_td ltx_align_left ltx_border_t">Pile</td>
<td id="A1.T4.2.7.7.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.7.7.3" class="ltx_td ltx_align_right ltx_border_t">13.838.432</td>
</tr>
<tr id="A1.T4.2.8.8" class="ltx_tr">
<td id="A1.T4.2.8.8.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.8.8.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.8.8.3" class="ltx_td ltx_align_right">21.990.512</td>
</tr>
<tr id="A1.T4.2.9.9" class="ltx_tr">
<td id="A1.T4.2.9.9.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.9.9.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.9.9.3" class="ltx_td ltx_align_right">4.334.313.669</td>
</tr>
<tr id="A1.T4.2.10.10" class="ltx_tr">
<td id="A1.T4.2.10.10.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.10.10.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.10.10.3" class="ltx_td ltx_align_right">7.946.402</td>
</tr>
<tr id="A1.T4.2.11.11" class="ltx_tr">
<td id="A1.T4.2.11.11.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.11.11.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.11.11.3" class="ltx_td ltx_align_right">15.857.811</td>
</tr>
<tr id="A1.T4.2.12.12" class="ltx_tr">
<td id="A1.T4.2.12.12.1" class="ltx_td ltx_align_left ltx_border_t">RedPajama</td>
<td id="A1.T4.2.12.12.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.12.12.3" class="ltx_td ltx_align_right ltx_border_t">143.907.461</td>
</tr>
<tr id="A1.T4.2.13.13" class="ltx_tr">
<td id="A1.T4.2.13.13.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.13.13.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.13.13.3" class="ltx_td ltx_align_right">112.950.000</td>
</tr>
<tr id="A1.T4.2.14.14" class="ltx_tr">
<td id="A1.T4.2.14.14.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.14.14.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.14.14.3" class="ltx_td ltx_align_right">4.663.646.781</td>
</tr>
<tr id="A1.T4.2.15.15" class="ltx_tr">
<td id="A1.T4.2.15.15.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.15.15.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.15.15.3" class="ltx_td ltx_align_right">137.802.711</td>
</tr>
<tr id="A1.T4.2.16.16" class="ltx_tr">
<td id="A1.T4.2.16.16.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.16.16.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.16.16.3" class="ltx_td ltx_align_right">139.749.147</td>
</tr>
<tr id="A1.T4.2.17.17" class="ltx_tr">
<td id="A1.T4.2.17.17.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.17.17.2" class="ltx_td ltx_align_left">Code</td>
<td id="A1.T4.2.17.17.3" class="ltx_td ltx_align_right">2.052.228.788</td>
</tr>
<tr id="A1.T4.2.18.18" class="ltx_tr">
<td id="A1.T4.2.18.18.1" class="ltx_td ltx_align_left ltx_border_t">Misc</td>
<td id="A1.T4.2.18.18.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.18.18.3" class="ltx_td ltx_align_right ltx_border_t">600.844.912</td>
</tr>
<tr id="A1.T4.2.19.19" class="ltx_tr">
<td id="A1.T4.2.19.19.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.19.19.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.19.19.3" class="ltx_td ltx_align_right">186.934.269</td>
</tr>
<tr id="A1.T4.2.20.20" class="ltx_tr">
<td id="A1.T4.2.20.20.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.20.20.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.20.20.3" class="ltx_td ltx_align_right">1.337.030.904</td>
</tr>
<tr id="A1.T4.2.21.21" class="ltx_tr">
<td id="A1.T4.2.21.21.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.21.21.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.21.21.3" class="ltx_td ltx_align_right">19.810.753</td>
</tr>
<tr id="A1.T4.2.22.22" class="ltx_tr">
<td id="A1.T4.2.22.22.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.22.22.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.22.22.3" class="ltx_td ltx_align_right">211.147.445</td>
</tr>
<tr id="A1.T4.2.23.23" class="ltx_tr">
<td id="A1.T4.2.23.23.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Total</td>
<td id="A1.T4.2.23.23.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="A1.T4.2.23.23.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A1.T4.4.2" class="ltx_text" style="font-size:90%;">Overview of the multilingual 70B words dataset with language, number of sampled words</span></figcaption>
</figure>
<figure id="A1.T5" class="ltx_table">
<table id="A1.T5.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T5.2.1.1" class="ltx_tr">
<td id="A1.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Name</td>
<td id="A1.T5.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Language</td>
<td id="A1.T5.2.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"># words</td>
</tr>
<tr id="A1.T5.2.2.2" class="ltx_tr">
<td id="A1.T5.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Oscar</td>
<td id="A1.T5.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">EN</td>
<td id="A1.T5.2.2.2.3" class="ltx_td ltx_align_right ltx_border_t">56.000.000.000</td>
</tr>
<tr id="A1.T5.2.3.3" class="ltx_tr">
<td id="A1.T5.2.3.3.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T5.2.3.3.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T5.2.3.3.3" class="ltx_td ltx_align_right">4.893.724.288</td>
</tr>
<tr id="A1.T5.2.4.4" class="ltx_tr">
<td id="A1.T5.2.4.4.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T5.2.4.4.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T5.2.4.4.3" class="ltx_td ltx_align_right">5.308.974.750</td>
</tr>
<tr id="A1.T5.2.5.5" class="ltx_tr">
<td id="A1.T5.2.5.5.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T5.2.5.5.2" class="ltx_td ltx_align_left">Code</td>
<td id="A1.T5.2.5.5.3" class="ltx_td ltx_align_right">2.299.301.635</td>
</tr>
<tr id="A1.T5.2.6.6" class="ltx_tr">
<td id="A1.T5.2.6.6.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T5.2.6.6.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T5.2.6.6.3" class="ltx_td ltx_align_right">1.497.999.327</td>
</tr>
<tr id="A1.T5.2.7.7" class="ltx_tr">
<td id="A1.T5.2.7.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt">Total</td>
<td id="A1.T5.2.7.7.2" class="ltx_td ltx_border_bb ltx_border_tt"></td>
<td id="A1.T5.2.7.7.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="A1.T5.4.2" class="ltx_text" style="font-size:90%;">Overview of the English 70B words dataset with language, number of sampled words</span></figcaption>
</figure>
<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Our web documents in the corpora consist of Oscars<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://oscar-project.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://oscar-project.org/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Abadji et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>, that were generated by the ungoliant pipeline<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/oscar-project/ungoliant" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/oscar-project/ungoliant</a></span></span></span> based on three Common Crawl WET Archives (2022-27, 2022-49 and 2023-14).</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">The curated datasets consist of <span id="A1.p2.1.1" class="ltx_text ltx_font_italic">The Pile</span> <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib14" title="" class="ltx_ref">2020a</a>)</cite>, <span id="A1.p2.1.2" class="ltx_text ltx_font_italic">RedPajama</span> <cite class="ltx_cite ltx_citemacro_cite">Computer (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, and single datasets that do not belong to a collection. From the Pile subcorpora, we selected: Phil Archive, PMC Abstracts, PMC Extracts, OpenWebText, NIH Exporterm, and Free Law Opinions V2. From RedPajama we use: ArXiv, Books, Github, StackExchange, and Wikipedia.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">The remaining datasets are:</p>
</div>
<figure id="A1.T6" class="ltx_table">
<table id="A1.T6.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T6.8.9.1" class="ltx_tr">
<th id="A1.T6.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Hyper-Parameter</th>
<td id="A1.T6.8.9.1.2" class="ltx_td ltx_align_left ltx_border_tt">Value(s)</td>
</tr>
<tr id="A1.T6.1.1" class="ltx_tr">
<th id="A1.T6.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">model_type</th>
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_t">Unigram <math id="A1.T6.1.1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.1.1.1.m1.1a"><mo fence="false" stretchy="false" id="A1.T6.1.1.1.m1.1.1" xref="A1.T6.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.m1.1b"><ci id="A1.T6.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.m1.1c">|</annotation></semantics></math> BPE</td>
</tr>
<tr id="A1.T6.4.4" class="ltx_tr">
<th id="A1.T6.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">vocab_size</th>
<td id="A1.T6.4.4.3" class="ltx_td ltx_align_left">33k <math id="A1.T6.2.2.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.2.2.1.m1.1a"><mo fence="false" stretchy="false" id="A1.T6.2.2.1.m1.1.1" xref="A1.T6.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.2.2.1.m1.1b"><ci id="A1.T6.2.2.1.m1.1.1.cmml" xref="A1.T6.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.2.2.1.m1.1c">|</annotation></semantics></math>50k <math id="A1.T6.3.3.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.3.3.2.m2.1a"><mo fence="false" stretchy="false" id="A1.T6.3.3.2.m2.1.1" xref="A1.T6.3.3.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.3.3.2.m2.1b"><ci id="A1.T6.3.3.2.m2.1.1.cmml" xref="A1.T6.3.3.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.3.3.2.m2.1c">|</annotation></semantics></math> 82k <math id="A1.T6.4.4.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.4.4.3.m3.1a"><mo fence="false" stretchy="false" id="A1.T6.4.4.3.m3.1.1" xref="A1.T6.4.4.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.4.4.3.m3.1b"><ci id="A1.T6.4.4.3.m3.1.1.cmml" xref="A1.T6.4.4.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.4.4.3.m3.1c">|</annotation></semantics></math> 100k</td>
</tr>
<tr id="A1.T6.8.10.2" class="ltx_tr">
<th id="A1.T6.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">character_coverage</th>
<td id="A1.T6.8.10.2.2" class="ltx_td ltx_align_left">0.9999</td>
</tr>
<tr id="A1.T6.8.11.3" class="ltx_tr">
<th id="A1.T6.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">split_by_number</th>
<td id="A1.T6.8.11.3.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.12.4" class="ltx_tr">
<th id="A1.T6.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">allow_whitespace_only_pieces</th>
<td id="A1.T6.8.12.4.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.13.5" class="ltx_tr">
<th id="A1.T6.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">add_dummy_prefix</th>
<td id="A1.T6.8.13.5.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.6.6" class="ltx_tr">
<th id="A1.T6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">user_defined_symbols_1</th>
<td id="A1.T6.6.6.2" class="ltx_td ltx_align_left">&lt;s&gt;, &lt;/s&gt;,&lt;pad&gt;,&lt;eod&gt;</td>
</tr>
<tr id="A1.T6.8.8" class="ltx_tr">
<th id="A1.T6.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">user_defined_symbols_2</th>
<td id="A1.T6.8.8.2" class="ltx_td ltx_align_left">…, &lt;placeholder_255&gt;</td>
</tr>
<tr id="A1.T6.8.14.6" class="ltx_tr">
<th id="A1.T6.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">byte_fallback</th>
<td id="A1.T6.8.14.6.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.15.7" class="ltx_tr">
<th id="A1.T6.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">max_sentence_length</th>
<td id="A1.T6.8.15.7.2" class="ltx_td ltx_align_left">4192</td>
</tr>
<tr id="A1.T6.8.16.8" class="ltx_tr">
<th id="A1.T6.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">normalization_rule_name</th>
<td id="A1.T6.8.16.8.2" class="ltx_td ltx_align_left">NFKC</td>
</tr>
<tr id="A1.T6.8.17.9" class="ltx_tr">
<th id="A1.T6.8.17.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">train_extremely_large_corpus</th>
<td id="A1.T6.8.17.9.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.18.10" class="ltx_tr">
<th id="A1.T6.8.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">remove_extra_whitespaces</th>
<td id="A1.T6.8.18.10.2" class="ltx_td ltx_align_left">False</td>
</tr>
<tr id="A1.T6.8.19.11" class="ltx_tr">
<th id="A1.T6.8.19.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">split_by_whitespace</th>
<td id="A1.T6.8.19.11.2" class="ltx_td ltx_align_left ltx_border_bb">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T6.10.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="A1.T6.11.2" class="ltx_text" style="font-size:90%;">Overview of the SentencePiece options that we used for the training of our tokenizers.</span></figcaption>
</figure>
<figure id="A1.T7" class="ltx_table">
<table id="A1.T7.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.3.4.1" class="ltx_tr">
<th id="A1.T7.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Hyper-Parameter</th>
<td id="A1.T7.3.4.1.2" class="ltx_td ltx_align_left ltx_border_tt">Value(s)</td>
</tr>
<tr id="A1.T7.3.5.2" class="ltx_tr">
<th id="A1.T7.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">model_type</th>
<td id="A1.T7.3.5.2.2" class="ltx_td ltx_align_left ltx_border_t">BPE</td>
</tr>
<tr id="A1.T7.3.3" class="ltx_tr">
<th id="A1.T7.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">vocab_size</th>
<td id="A1.T7.3.3.3" class="ltx_td ltx_align_left">33k <math id="A1.T7.1.1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T7.1.1.1.m1.1a"><mo fence="false" stretchy="false" id="A1.T7.1.1.1.m1.1.1" xref="A1.T7.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.m1.1b"><ci id="A1.T7.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.m1.1c">|</annotation></semantics></math>50k <math id="A1.T7.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T7.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="A1.T7.2.2.2.m2.1.1" xref="A1.T7.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.m2.1b"><ci id="A1.T7.2.2.2.m2.1.1.cmml" xref="A1.T7.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.m2.1c">|</annotation></semantics></math> 82k <math id="A1.T7.3.3.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T7.3.3.3.m3.1a"><mo fence="false" stretchy="false" id="A1.T7.3.3.3.m3.1.1" xref="A1.T7.3.3.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T7.3.3.3.m3.1b"><ci id="A1.T7.3.3.3.m3.1.1.cmml" xref="A1.T7.3.3.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.3.3.m3.1c">|</annotation></semantics></math> 100k</td>
</tr>
<tr id="A1.T7.3.6.3" class="ltx_tr">
<th id="A1.T7.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">limit_alphabet</th>
<td id="A1.T7.3.6.3.2" class="ltx_td ltx_align_left">512</td>
</tr>
<tr id="A1.T7.3.7.4" class="ltx_tr">
<th id="A1.T7.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">use_nfkc_normalizer</th>
<td id="A1.T7.3.7.4.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T7.3.8.5" class="ltx_tr">
<th id="A1.T7.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">use_lowercase_normalizer</th>
<td id="A1.T7.3.8.5.2" class="ltx_td ltx_align_left">False</td>
</tr>
<tr id="A1.T7.3.9.6" class="ltx_tr">
<th id="A1.T7.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">use_strip_accents_normalizer</th>
<td id="A1.T7.3.9.6.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T7.3.10.7" class="ltx_tr">
<th id="A1.T7.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">pre_tokenizer</th>
<td id="A1.T7.3.10.7.2" class="ltx_td ltx_align_left ltx_border_bb">ByteLevel, Digits</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T7.5.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="A1.T7.6.2" class="ltx_text" style="font-size:90%;">Overview of the Sentencepiece Options that we used for the training of our tokenizers.</span></figcaption>
</figure>
<div id="A1.p4" class="ltx_para">
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">All the News V2.0<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://metatext.io/datasets/all-the-news-2.0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://metatext.io/datasets/all-the-news-2.0</a></span></span></span> is a corpus of newspaper
articles crawled from over 26 different publications
from January 2016 to April 1, 2020.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">Bundestag - Plenarprotokolle<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.bundestag.de/dokumente/protokolle/plenarprotokolle" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.bundestag.de/dokumente/protokolle/plenarprotokolle</a></span></span></span>
comprises transcripts of sessions of the German Bundestag.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Bundesgerichtshof - Entscheidungen<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html</a></span></span></span>
is a collection of decisions of the German Federal Court.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">CoStEP<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://pub.cl.uzh.ch/wiki/public/costep/start" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pub.cl.uzh.ch/wiki/public/costep/start</a></span></span></span>
is a cleaned-up and corrected version of the EuroParl corpus<cite class="ltx_cite ltx_citemacro_cite">Graën et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2014</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_cite">Koehn (<a href="#bib.bib25" title="" class="ltx_ref">2005</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">DCEP<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en</a></span></span></span>
is a companion corpus to CoStEP, containing documents published by
the European Parliament.
<cite class="ltx_cite ltx_citemacro_cite">Hajlaoui et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2014</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">DNB Dissertations<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html</a></span></span></span> is a collection of dissertations from the
Deutsche Nationalbibliothek.</p>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p id="A1.I1.i7.p1.1" class="ltx_p">MAREC/IREC<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://researchdata.tuwien.ac.at/records/2zx6e-5pr64" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://researchdata.tuwien.ac.at/records/2zx6e-5pr64</a></span></span></span>: The MAtrixware REsearch Collection / The Information retrieval facility Research Collection is a patent corpus of over 19 million documents from the EP, WO, US, and JP patent offices.</p>
</div>
</li>
<li id="A1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A1.I1.i8.p1" class="ltx_para">
<p id="A1.I1.i8.p1.1" class="ltx_p">Medi-Notice<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice</a></span></span></span>
is part of the
Zurich Parallel Corpus Collection. It is
a multilingual corpus
compiled from information leaflets for medications and pharmaceutical products
published by the Swiss Agency for Therapeutic Products.<cite class="ltx_cite ltx_citemacro_cite">Graën et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A1.I1.i9.p1" class="ltx_para">
<p id="A1.I1.i9.p1.1" class="ltx_p">Swiss Policy<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus</a></span></span></span> contains documents of the
Swiss Legislation Corpus <cite class="ltx_cite ltx_citemacro_cite">Höfler and Piotrowski (<a href="#bib.bib23" title="" class="ltx_ref">2011</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A1.I1.i10.p1" class="ltx_para">
<p id="A1.I1.i10.p1.1" class="ltx_p">OpenSubtitles 2018<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a target="_blank" href="https://opus.nlpl.eu/OpenSubtitles-v2018.php" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://opus.nlpl.eu/OpenSubtitles-v2018.php</a></span></span></span><span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a target="_blank" href="https://www.opensubtitles.org/de/index.cgi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.opensubtitles.org/de/index.cgi</a></span></span></span> is a collection of translated movie subtitles.
<cite class="ltx_cite ltx_citemacro_cite">Lison and Tiedemann (<a href="#bib.bib30" title="" class="ltx_ref">2016</a>)</cite></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Tokenizer</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In our experiments, we focused on the <span id="A2.p1.1.1" class="ltx_text ltx_font_italic">Huggingface tokenizer</span> library <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> and the <span id="A2.p1.1.2" class="ltx_text ltx_font_italic">SentencePiece</span> library <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>. We use the standard settings of the SentencePiece library if not stated otherwise in <a href="#A1.T6" title="In Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>. For the HuggingFace tokenizer library <a href="#A1.T7" title="In Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a> shows where we deviated from the standard values.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>LLM Architecture and Hyperparameters</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Regarding the training architecture of our 2.7B parameter models, we followed closely the architecture of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite>. An overview of the used architecture details and hyperparameters is given in <a href="#A3.T8" title="In Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="A3.T8" class="ltx_table">
<table id="A3.T8.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T8.2.3.1" class="ltx_tr">
<th id="A3.T8.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">parameter</th>
<th id="A3.T8.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T8.2.4.1" class="ltx_tr">
<th id="A3.T8.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># Hidden Dimension</th>
<td id="A3.T8.2.4.1.2" class="ltx_td ltx_align_left ltx_border_t">2560</td>
</tr>
<tr id="A3.T8.2.5.2" class="ltx_tr">
<th id="A3.T8.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"># Layers</th>
<td id="A3.T8.2.5.2.2" class="ltx_td ltx_align_left">32</td>
</tr>
<tr id="A3.T8.2.6.3" class="ltx_tr">
<th id="A3.T8.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"># Attention-Heads</th>
<td id="A3.T8.2.6.3.2" class="ltx_td ltx_align_left">32</td>
</tr>
<tr id="A3.T8.2.7.4" class="ltx_tr">
<th id="A3.T8.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sequence-Length</th>
<td id="A3.T8.2.7.4.2" class="ltx_td ltx_align_left">2048</td>
</tr>
<tr id="A3.T8.2.8.5" class="ltx_tr">
<th id="A3.T8.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Optimizer</th>
<td id="A3.T8.2.8.5.2" class="ltx_td ltx_align_left">Adam</td>
</tr>
<tr id="A3.T8.1.1" class="ltx_tr">
<th id="A3.T8.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adam<math id="A3.T8.1.1.1.m1.1" class="ltx_Math" alttext="-\beta_{1}" display="inline"><semantics id="A3.T8.1.1.1.m1.1a"><mrow id="A3.T8.1.1.1.m1.1.1" xref="A3.T8.1.1.1.m1.1.1.cmml"><mo id="A3.T8.1.1.1.m1.1.1a" xref="A3.T8.1.1.1.m1.1.1.cmml">−</mo><msub id="A3.T8.1.1.1.m1.1.1.2" xref="A3.T8.1.1.1.m1.1.1.2.cmml"><mi id="A3.T8.1.1.1.m1.1.1.2.2" xref="A3.T8.1.1.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T8.1.1.1.m1.1.1.2.3" xref="A3.T8.1.1.1.m1.1.1.2.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T8.1.1.1.m1.1b"><apply id="A3.T8.1.1.1.m1.1.1.cmml" xref="A3.T8.1.1.1.m1.1.1"><minus id="A3.T8.1.1.1.m1.1.1.1.cmml" xref="A3.T8.1.1.1.m1.1.1"></minus><apply id="A3.T8.1.1.1.m1.1.1.2.cmml" xref="A3.T8.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T8.1.1.1.m1.1.1.2.1.cmml" xref="A3.T8.1.1.1.m1.1.1.2">subscript</csymbol><ci id="A3.T8.1.1.1.m1.1.1.2.2.cmml" xref="A3.T8.1.1.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="A3.T8.1.1.1.m1.1.1.2.3.cmml" xref="A3.T8.1.1.1.m1.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.1.1.1.m1.1c">-\beta_{1}</annotation></semantics></math>
</th>
<td id="A3.T8.1.1.2" class="ltx_td ltx_align_left">0.9</td>
</tr>
<tr id="A3.T8.2.2" class="ltx_tr">
<th id="A3.T8.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adam<math id="A3.T8.2.2.1.m1.1" class="ltx_Math" alttext="-\beta_{2}" display="inline"><semantics id="A3.T8.2.2.1.m1.1a"><mrow id="A3.T8.2.2.1.m1.1.1" xref="A3.T8.2.2.1.m1.1.1.cmml"><mo id="A3.T8.2.2.1.m1.1.1a" xref="A3.T8.2.2.1.m1.1.1.cmml">−</mo><msub id="A3.T8.2.2.1.m1.1.1.2" xref="A3.T8.2.2.1.m1.1.1.2.cmml"><mi id="A3.T8.2.2.1.m1.1.1.2.2" xref="A3.T8.2.2.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T8.2.2.1.m1.1.1.2.3" xref="A3.T8.2.2.1.m1.1.1.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T8.2.2.1.m1.1b"><apply id="A3.T8.2.2.1.m1.1.1.cmml" xref="A3.T8.2.2.1.m1.1.1"><minus id="A3.T8.2.2.1.m1.1.1.1.cmml" xref="A3.T8.2.2.1.m1.1.1"></minus><apply id="A3.T8.2.2.1.m1.1.1.2.cmml" xref="A3.T8.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T8.2.2.1.m1.1.1.2.1.cmml" xref="A3.T8.2.2.1.m1.1.1.2">subscript</csymbol><ci id="A3.T8.2.2.1.m1.1.1.2.2.cmml" xref="A3.T8.2.2.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="A3.T8.2.2.1.m1.1.1.2.3.cmml" xref="A3.T8.2.2.1.m1.1.1.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.2.2.1.m1.1c">-\beta_{2}</annotation></semantics></math>
</th>
<td id="A3.T8.2.2.2" class="ltx_td ltx_align_left">0.9</td>
</tr>
<tr id="A3.T8.2.9.6" class="ltx_tr">
<th id="A3.T8.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Learning rate</th>
<td id="A3.T8.2.9.6.2" class="ltx_td ltx_align_left">1.6e-4</td>
</tr>
<tr id="A3.T8.2.10.7" class="ltx_tr">
<th id="A3.T8.2.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Learning rate decay</th>
<td id="A3.T8.2.10.7.2" class="ltx_td ltx_align_left">Cosine</td>
</tr>
<tr id="A3.T8.2.11.8" class="ltx_tr">
<th id="A3.T8.2.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Precision</th>
<td id="A3.T8.2.11.8.2" class="ltx_td ltx_align_left">BF16</td>
</tr>
<tr id="A3.T8.2.12.9" class="ltx_tr">
<th id="A3.T8.2.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FlashAttention</th>
<td id="A3.T8.2.12.9.2" class="ltx_td ltx_align_left">2.0</td>
</tr>
<tr id="A3.T8.2.13.10" class="ltx_tr">
<th id="A3.T8.2.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Position-Embeddings</th>
<td id="A3.T8.2.13.10.2" class="ltx_td ltx_align_left ltx_border_bb">Rotary</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A3.T8.4.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="A3.T8.5.2" class="ltx_text" style="font-size:90%;">Overview of the LLM hyperparameters that we used for the training.</span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Intrinsic Tokenizer Evaluation</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Besides studying the overlap of the same algorithm on the same thesaurus, we were also interested in vocabulary overlaps across algorithms and thesauruses see <a href="#A4.F6" title="In Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>. What we can observe is that multilingual vocabulary and English vocabulary have a rather small overlap between 24% and 34% that remains similar across increasing vocabulary sizes. Across algorithms, we can see that Unigram and BPE of SentencePiece have a slightly higher overlap than Unigram of SentencePiece and BPE of Huggingface. We think this might be due to library-specific preprocessing steps and more similar hyperparameters.</p>
</div>
<figure id="A4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x10.png" id="A4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="322" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x11.png" id="A4.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="322" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x12.png" id="A4.F6.3.g1" class="ltx_graphics ltx_img_square" width="484" height="443" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x13.png" id="A4.F6.4.g1" class="ltx_graphics ltx_img_square" width="484" height="443" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F6.6.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A4.F6.7.2" class="ltx_text" style="font-size:90%;">Vocabulary overlap between the examined tokenizers</span></figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Extrinsic Tokenizer Evaluation</h2>

<figure id="A5.T9" class="ltx_table">
<table id="A5.T9.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.T9.2.1.1" class="ltx_tr">
<td id="A5.T9.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="A5.T9.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A5.T9.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Accuracy</span></th>
</tr>
<tr id="A5.T9.2.2.2" class="ltx_tr">
<td id="A5.T9.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="A5.T9.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.2.2.2.1" class="ltx_text" style="font-size:90%;">EN</span></td>
<td id="A5.T9.2.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.2.2.3.1" class="ltx_text" style="font-size:90%;">MULTI</span></td>
</tr>
<tr id="A5.T9.2.3.3" class="ltx_tr">
<td id="A5.T9.2.3.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.3.3.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-100</span></td>
<td id="A5.T9.2.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.3.3.2.1" class="ltx_text" style="font-size:90%;">45.60</span></td>
<td id="A5.T9.2.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.3.3.3.1" class="ltx_text" style="font-size:90%;">38.50</span></td>
</tr>
<tr id="A5.T9.2.4.4" class="ltx_tr">
<td id="A5.T9.2.4.4.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.4.4.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-32</span></td>
<td id="A5.T9.2.4.4.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.4.4.2.1" class="ltx_text" style="font-size:90%;">45.34</span></td>
<td id="A5.T9.2.4.4.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.4.4.3.1" class="ltx_text" style="font-size:90%;">38.12</span></td>
</tr>
<tr id="A5.T9.2.5.5" class="ltx_tr">
<td id="A5.T9.2.5.5.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.5.5.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-50</span></td>
<td id="A5.T9.2.5.5.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.5.5.2.1" class="ltx_text" style="font-size:90%;">45.85</span></td>
<td id="A5.T9.2.5.5.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.5.5.3.1" class="ltx_text" style="font-size:90%;">38.15</span></td>
</tr>
<tr id="A5.T9.2.6.6" class="ltx_tr">
<td id="A5.T9.2.6.6.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.6.6.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-82</span></td>
<td id="A5.T9.2.6.6.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.6.6.2.1" class="ltx_text" style="font-size:90%;">44.97</span></td>
<td id="A5.T9.2.6.6.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.6.6.3.1" class="ltx_text" style="font-size:90%;">38.67</span></td>
</tr>
<tr id="A5.T9.2.7.7" class="ltx_tr">
<td id="A5.T9.2.7.7.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.7.7.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-100</span></td>
<td id="A5.T9.2.7.7.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.7.7.2.1" class="ltx_text" style="font-size:90%;">46.06</span></td>
<td id="A5.T9.2.7.7.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.7.7.3.1" class="ltx_text" style="font-size:90%;">38.92</span></td>
</tr>
<tr id="A5.T9.2.8.8" class="ltx_tr">
<td id="A5.T9.2.8.8.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.8.8.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-32</span></td>
<td id="A5.T9.2.8.8.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">47.06</span></td>
<td id="A5.T9.2.8.8.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.8.8.3.1" class="ltx_text" style="font-size:90%;">38.01</span></td>
</tr>
<tr id="A5.T9.2.9.9" class="ltx_tr">
<td id="A5.T9.2.9.9.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.9.9.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-50</span></td>
<td id="A5.T9.2.9.9.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.9.9.2.1" class="ltx_text" style="font-size:90%;">46.85</span></td>
<td id="A5.T9.2.9.9.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.9.9.3.1" class="ltx_text" style="font-size:90%;">38.41</span></td>
</tr>
<tr id="A5.T9.2.10.10" class="ltx_tr">
<td id="A5.T9.2.10.10.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.10.10.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-82</span></td>
<td id="A5.T9.2.10.10.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.10.10.2.1" class="ltx_text" style="font-size:90%;">45.25</span></td>
<td id="A5.T9.2.10.10.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.10.10.3.1" class="ltx_text" style="font-size:90%;">38.64</span></td>
</tr>
<tr id="A5.T9.2.11.11" class="ltx_tr">
<td id="A5.T9.2.11.11.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.11.11.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-100</span></td>
<td id="A5.T9.2.11.11.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.11.11.2.1" class="ltx_text" style="font-size:90%;">45.97</span></td>
<td id="A5.T9.2.11.11.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.11.11.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">39.34</span></td>
</tr>
<tr id="A5.T9.2.12.12" class="ltx_tr">
<td id="A5.T9.2.12.12.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.12.12.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-32</span></td>
<td id="A5.T9.2.12.12.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.12.12.2.1" class="ltx_text" style="font-size:90%;">46.62</span></td>
<td id="A5.T9.2.12.12.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.12.12.3.1" class="ltx_text" style="font-size:90%;">38.18</span></td>
</tr>
<tr id="A5.T9.2.13.13" class="ltx_tr">
<td id="A5.T9.2.13.13.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.13.13.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-50</span></td>
<td id="A5.T9.2.13.13.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.13.13.2.1" class="ltx_text" style="font-size:90%;">45.81</span></td>
<td id="A5.T9.2.13.13.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.13.13.3.1" class="ltx_text" style="font-size:90%;">38.66</span></td>
</tr>
<tr id="A5.T9.2.14.14" class="ltx_tr">
<td id="A5.T9.2.14.14.1" class="ltx_td ltx_align_left ltx_border_b"><span id="A5.T9.2.14.14.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-82</span></td>
<td id="A5.T9.2.14.14.2" class="ltx_td ltx_align_left ltx_border_b"><span id="A5.T9.2.14.14.2.1" class="ltx_text" style="font-size:90%;">45.65</span></td>
<td id="A5.T9.2.14.14.3" class="ltx_td ltx_align_right ltx_border_b"><span id="A5.T9.2.14.14.3.1" class="ltx_text" style="font-size:90%;">39.21</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Average accuracy of monolingual and multilingual tokenizers across all downstream tasks</figcaption>
</figure>
<figure id="A5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x14.png" id="A5.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A5.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">Monolingual tokenizers</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/x15.png" id="A5.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A5.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">Multilingual tokenizers</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A5.F7.3.2" class="ltx_text" style="font-size:90%;">Accuracy distribution for mono-/multilingual tokenizers across mono-/multilingual tasks. The "+" represents the average performance across all tasks.</span></figcaption>
</figure>
<figure id="A5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A5.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/appendix/figures/computational_efficiency/gflops_per_word_forward_pass_fertility_non_en.jpg" id="A5.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F8.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A5.F8.sf1.3.2" class="ltx_text" style="font-size:90%;">Non-English documents.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A5.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/appendix/figures/computational_efficiency/gflops_per_word_forward_pass_fertility_en.jpg" id="A5.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F8.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A5.F8.sf2.3.2" class="ltx_text" style="font-size:90%;">English documents.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A5.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.08754/assets/appendix/figures/computational_efficiency/gflops_per_word_forward_pass_fertility_de.jpg" id="A5.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F8.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="A5.F8.sf3.3.2" class="ltx_text" style="font-size:90%;">German documents.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A5.F8.4.2" class="ltx_text" style="font-size:90%;">Average compute (GFLOPs) required to process a single word within (a) multilingual, (b) English, and (c) German documents within an <span id="A5.F8.4.2.1" class="ltx_text ltx_font_bold">inference</span> pass.</span></figcaption>
</figure>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Performance Gains on Task-Level</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p id="A5.SS1.p1.1" class="ltx_p">In table <a href="#A5.T11" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a>, we show how much influence the choice of a tokenizer has on each downstream task. Therefore, we compare the results of the best- and worst-performing models for each task in our experiments. Performance increasements reach from +5,3% up to 380,9%, again showing the importance of choosing the right tokenizer on the downstream performance.</p>
</div>
<figure id="A5.T10" class="ltx_table">
<table id="A5.T10.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A5.T10.2.1.1" class="ltx_tr">
<th id="A5.T10.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="A5.T10.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Non-English</th>
<th id="A5.T10.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="A5.T10.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A5.T10.2.2.1" class="ltx_tr">
<th id="A5.T10.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EN-BPE-HF-33</th>
<td id="A5.T10.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">3,8</td>
<td id="A5.T10.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T10.2.2.1.3.1" class="ltx_text ltx_font_bold">2,32</span></td>
<td id="A5.T10.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">4,52</td>
</tr>
<tr id="A5.T10.2.3.2" class="ltx_tr">
<th id="A5.T10.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-HF-50</th>
<td id="A5.T10.2.3.2.2" class="ltx_td ltx_align_center">3,79</td>
<td id="A5.T10.2.3.2.3" class="ltx_td ltx_align_center">2,38</td>
<td id="A5.T10.2.3.2.4" class="ltx_td ltx_align_center">4,45</td>
</tr>
<tr id="A5.T10.2.4.3" class="ltx_tr">
<th id="A5.T10.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-HF-82</th>
<td id="A5.T10.2.4.3.2" class="ltx_td ltx_align_center">3,88</td>
<td id="A5.T10.2.4.3.3" class="ltx_td ltx_align_center">2,55</td>
<td id="A5.T10.2.4.3.4" class="ltx_td ltx_align_center">4,51</td>
</tr>
<tr id="A5.T10.2.5.4" class="ltx_tr">
<th id="A5.T10.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-HF-100</th>
<td id="A5.T10.2.5.4.2" class="ltx_td ltx_align_center">3,96</td>
<td id="A5.T10.2.5.4.3" class="ltx_td ltx_align_center">2,67</td>
<td id="A5.T10.2.5.4.4" class="ltx_td ltx_align_center">4,58</td>
</tr>
<tr id="A5.T10.2.6.5" class="ltx_tr">
<th id="A5.T10.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-33</th>
<td id="A5.T10.2.6.5.2" class="ltx_td ltx_align_center">3,86</td>
<td id="A5.T10.2.6.5.3" class="ltx_td ltx_align_center">2,37</td>
<td id="A5.T10.2.6.5.4" class="ltx_td ltx_align_center">4,66</td>
</tr>
<tr id="A5.T10.2.7.6" class="ltx_tr">
<th id="A5.T10.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-50</th>
<td id="A5.T10.2.7.6.2" class="ltx_td ltx_align_center">3,89</td>
<td id="A5.T10.2.7.6.3" class="ltx_td ltx_align_center">2,42</td>
<td id="A5.T10.2.7.6.4" class="ltx_td ltx_align_center">4,68</td>
</tr>
<tr id="A5.T10.2.8.7" class="ltx_tr">
<th id="A5.T10.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-82</th>
<td id="A5.T10.2.8.7.2" class="ltx_td ltx_align_center">4,02</td>
<td id="A5.T10.2.8.7.3" class="ltx_td ltx_align_center">2,59</td>
<td id="A5.T10.2.8.7.4" class="ltx_td ltx_align_center">4,78</td>
</tr>
<tr id="A5.T10.2.9.8" class="ltx_tr">
<th id="A5.T10.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-100</th>
<td id="A5.T10.2.9.8.2" class="ltx_td ltx_align_center">4,11</td>
<td id="A5.T10.2.9.8.3" class="ltx_td ltx_align_center">2,71</td>
<td id="A5.T10.2.9.8.4" class="ltx_td ltx_align_center">4,84</td>
</tr>
<tr id="A5.T10.2.10.9" class="ltx_tr">
<th id="A5.T10.2.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-32</th>
<td id="A5.T10.2.10.9.2" class="ltx_td ltx_align_center">4,01</td>
<td id="A5.T10.2.10.9.3" class="ltx_td ltx_align_center">2,36</td>
<td id="A5.T10.2.10.9.4" class="ltx_td ltx_align_center">4,73</td>
</tr>
<tr id="A5.T10.2.11.10" class="ltx_tr">
<th id="A5.T10.2.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-50</th>
<td id="A5.T10.2.11.10.2" class="ltx_td ltx_align_center">4,02</td>
<td id="A5.T10.2.11.10.3" class="ltx_td ltx_align_center">2,42</td>
<td id="A5.T10.2.11.10.4" class="ltx_td ltx_align_center">4,75</td>
</tr>
<tr id="A5.T10.2.12.11" class="ltx_tr">
<th id="A5.T10.2.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-82</th>
<td id="A5.T10.2.12.11.2" class="ltx_td ltx_align_center">4,12</td>
<td id="A5.T10.2.12.11.3" class="ltx_td ltx_align_center">2,59</td>
<td id="A5.T10.2.12.11.4" class="ltx_td ltx_align_center">4,83</td>
</tr>
<tr id="A5.T10.2.13.12" class="ltx_tr">
<th id="A5.T10.2.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-100</th>
<td id="A5.T10.2.13.12.2" class="ltx_td ltx_align_center">4,21</td>
<td id="A5.T10.2.13.12.3" class="ltx_td ltx_align_center">2,71</td>
<td id="A5.T10.2.13.12.4" class="ltx_td ltx_align_center">4,88</td>
</tr>
<tr id="A5.T10.2.14.13" class="ltx_tr">
<th id="A5.T10.2.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MULTI-BPE-HF-33</th>
<td id="A5.T10.2.14.13.2" class="ltx_td ltx_align_center ltx_border_t">2,71</td>
<td id="A5.T10.2.14.13.3" class="ltx_td ltx_align_center ltx_border_t">2,46</td>
<td id="A5.T10.2.14.13.4" class="ltx_td ltx_align_center ltx_border_t">3,04</td>
</tr>
<tr id="A5.T10.2.15.14" class="ltx_tr">
<th id="A5.T10.2.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-HF-50</th>
<td id="A5.T10.2.15.14.2" class="ltx_td ltx_align_center">2,7</td>
<td id="A5.T10.2.15.14.3" class="ltx_td ltx_align_center">2,5</td>
<td id="A5.T10.2.15.14.4" class="ltx_td ltx_align_center">3,01</td>
</tr>
<tr id="A5.T10.2.16.15" class="ltx_tr">
<th id="A5.T10.2.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-HF-82</th>
<td id="A5.T10.2.16.15.2" class="ltx_td ltx_align_center">2,8</td>
<td id="A5.T10.2.16.15.3" class="ltx_td ltx_align_center">2,65</td>
<td id="A5.T10.2.16.15.4" class="ltx_td ltx_align_center">3,09</td>
</tr>
<tr id="A5.T10.2.17.16" class="ltx_tr">
<th id="A5.T10.2.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-HF-100</th>
<td id="A5.T10.2.17.16.2" class="ltx_td ltx_align_center">2,88</td>
<td id="A5.T10.2.17.16.3" class="ltx_td ltx_align_center">2,76</td>
<td id="A5.T10.2.17.16.4" class="ltx_td ltx_align_center">3,17</td>
</tr>
<tr id="A5.T10.2.18.17" class="ltx_tr">
<th id="A5.T10.2.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-33</th>
<td id="A5.T10.2.18.17.2" class="ltx_td ltx_align_center">2,68</td>
<td id="A5.T10.2.18.17.3" class="ltx_td ltx_align_center">2,55</td>
<td id="A5.T10.2.18.17.4" class="ltx_td ltx_align_center">2,99</td>
</tr>
<tr id="A5.T10.2.19.18" class="ltx_tr">
<th id="A5.T10.2.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-50</th>
<td id="A5.T10.2.19.18.2" class="ltx_td ltx_align_center">2,67</td>
<td id="A5.T10.2.19.18.3" class="ltx_td ltx_align_center">2,57</td>
<td id="A5.T10.2.19.18.4" class="ltx_td ltx_align_center">2,95</td>
</tr>
<tr id="A5.T10.2.20.19" class="ltx_tr">
<th id="A5.T10.2.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-82</th>
<td id="A5.T10.2.20.19.2" class="ltx_td ltx_align_center">2,76</td>
<td id="A5.T10.2.20.19.3" class="ltx_td ltx_align_center">2,72</td>
<td id="A5.T10.2.20.19.4" class="ltx_td ltx_align_center">3,03</td>
</tr>
<tr id="A5.T10.2.21.20" class="ltx_tr">
<th id="A5.T10.2.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-100</th>
<td id="A5.T10.2.21.20.2" class="ltx_td ltx_align_center">2,85</td>
<td id="A5.T10.2.21.20.3" class="ltx_td ltx_align_center">2,82</td>
<td id="A5.T10.2.21.20.4" class="ltx_td ltx_align_center">3,1</td>
</tr>
<tr id="A5.T10.2.22.21" class="ltx_tr">
<th id="A5.T10.2.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-UNI-SP-33</th>
<td id="A5.T10.2.22.21.2" class="ltx_td ltx_align_center">2,68</td>
<td id="A5.T10.2.22.21.3" class="ltx_td ltx_align_center">2,55</td>
<td id="A5.T10.2.22.21.4" class="ltx_td ltx_align_center">2,94</td>
</tr>
<tr id="A5.T10.2.23.22" class="ltx_tr">
<th id="A5.T10.2.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-UNI-SP-50</th>
<td id="A5.T10.2.23.22.2" class="ltx_td ltx_align_center"><span id="A5.T10.2.23.22.2.1" class="ltx_text ltx_font_bold">2,66</span></td>
<td id="A5.T10.2.23.22.3" class="ltx_td ltx_align_center">2,58</td>
<td id="A5.T10.2.23.22.4" class="ltx_td ltx_align_center"><span id="A5.T10.2.23.22.4.1" class="ltx_text ltx_font_bold">2,91</span></td>
</tr>
<tr id="A5.T10.2.24.23" class="ltx_tr">
<th id="A5.T10.2.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-UNI-SP-82</th>
<td id="A5.T10.2.24.23.2" class="ltx_td ltx_align_center">2,76</td>
<td id="A5.T10.2.24.23.3" class="ltx_td ltx_align_center">2,73</td>
<td id="A5.T10.2.24.23.4" class="ltx_td ltx_align_center">2,99</td>
</tr>
<tr id="A5.T10.2.25.24" class="ltx_tr">
<th id="A5.T10.2.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MULTI-UNI-SP-100</th>
<td id="A5.T10.2.25.24.2" class="ltx_td ltx_align_center ltx_border_bb">2,84</td>
<td id="A5.T10.2.25.24.3" class="ltx_td ltx_align_center ltx_border_bb">2,83</td>
<td id="A5.T10.2.25.24.4" class="ltx_td ltx_align_center ltx_border_bb">3,07</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A5.T10.3.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="A5.T10.4.2" class="ltx_text" style="font-size:90%;">Computational costs per word (GFLOPs) for different tokenizers.</span></figcaption>
</figure>
<figure id="A5.T11" class="ltx_table">
<table id="A5.T11.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A5.T11.2.1.1" class="ltx_tr">
<th id="A5.T11.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A5.T11.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Task</th>
<th id="A5.T11.2.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Metric</th>
<th id="A5.T11.2.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Worst</th>
<th id="A5.T11.2.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Best</th>
<th id="A5.T11.2.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Difference</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A5.T11.2.2.1" class="ltx_tr">
<th id="A5.T11.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="26"><span id="A5.T11.2.2.1.1.1" class="ltx_text">
<span id="A5.T11.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:14.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-3.74pt,-3.74pt) rotate(-90deg) ;">
<span id="A5.T11.2.2.1.1.1.1.1" class="ltx_p">EN</span>
</span></span></span></th>
<th id="A5.T11.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">arc challenge</th>
<td id="A5.T11.2.2.1.3" class="ltx_td ltx_align_right ltx_border_t">acc</td>
<td id="A5.T11.2.2.1.4" class="ltx_td ltx_align_right ltx_border_t">0,21</td>
<td id="A5.T11.2.2.1.5" class="ltx_td ltx_align_right ltx_border_t">0,27</td>
<td id="A5.T11.2.2.1.6" class="ltx_td ltx_align_right ltx_border_t">+28,4 %</td>
</tr>
<tr id="A5.T11.2.3.2" class="ltx_tr">
<th id="A5.T11.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">arc easy</th>
<td id="A5.T11.2.3.2.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.3.2.3" class="ltx_td ltx_align_right">0,50</td>
<td id="A5.T11.2.3.2.4" class="ltx_td ltx_align_right">0,59</td>
<td id="A5.T11.2.3.2.5" class="ltx_td ltx_align_right">+18,6 %</td>
</tr>
<tr id="A5.T11.2.4.3" class="ltx_tr">
<th id="A5.T11.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">boolq</th>
<td id="A5.T11.2.4.3.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.4.3.3" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.4.3.4" class="ltx_td ltx_align_right">0,61</td>
<td id="A5.T11.2.4.3.5" class="ltx_td ltx_align_right">+8,9 %</td>
</tr>
<tr id="A5.T11.2.5.4" class="ltx_tr">
<th id="A5.T11.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">cb</th>
<td id="A5.T11.2.5.4.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.5.4.3" class="ltx_td ltx_align_right">0,30</td>
<td id="A5.T11.2.5.4.4" class="ltx_td ltx_align_right">0,54</td>
<td id="A5.T11.2.5.4.5" class="ltx_td ltx_align_right">+76,5 %</td>
</tr>
<tr id="A5.T11.2.6.5" class="ltx_tr">
<th id="A5.T11.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">cb</th>
<td id="A5.T11.2.6.5.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.6.5.3" class="ltx_td ltx_align_right">0,18</td>
<td id="A5.T11.2.6.5.4" class="ltx_td ltx_align_right">0,36</td>
<td id="A5.T11.2.6.5.5" class="ltx_td ltx_align_right">+94,9 %</td>
</tr>
<tr id="A5.T11.2.7.6" class="ltx_tr">
<th id="A5.T11.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">copa</th>
<td id="A5.T11.2.7.6.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.7.6.3" class="ltx_td ltx_align_right">0,67</td>
<td id="A5.T11.2.7.6.4" class="ltx_td ltx_align_right">0,73</td>
<td id="A5.T11.2.7.6.5" class="ltx_td ltx_align_right">+9,0 %</td>
</tr>
<tr id="A5.T11.2.8.7" class="ltx_tr">
<th id="A5.T11.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">hellaswag</th>
<td id="A5.T11.2.8.7.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.8.7.3" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.8.7.4" class="ltx_td ltx_align_right">0,41</td>
<td id="A5.T11.2.8.7.5" class="ltx_td ltx_align_right">+19,7 %</td>
</tr>
<tr id="A5.T11.2.9.8" class="ltx_tr">
<th id="A5.T11.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada openai</th>
<td id="A5.T11.2.9.8.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.9.8.3" class="ltx_td ltx_align_right">0,47</td>
<td id="A5.T11.2.9.8.4" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.9.8.5" class="ltx_td ltx_align_right">+20,0 %</td>
</tr>
<tr id="A5.T11.2.10.9" class="ltx_tr">
<th id="A5.T11.2.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mathqa</th>
<td id="A5.T11.2.10.9.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.10.9.3" class="ltx_td ltx_align_right">0,22</td>
<td id="A5.T11.2.10.9.4" class="ltx_td ltx_align_right">0,24</td>
<td id="A5.T11.2.10.9.5" class="ltx_td ltx_align_right">+8,8 %</td>
</tr>
<tr id="A5.T11.2.11.10" class="ltx_tr">
<th id="A5.T11.2.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mnli</th>
<td id="A5.T11.2.11.10.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.11.10.3" class="ltx_td ltx_align_right">0,35</td>
<td id="A5.T11.2.11.10.4" class="ltx_td ltx_align_right">0,37</td>
<td id="A5.T11.2.11.10.5" class="ltx_td ltx_align_right">+7,7 %</td>
</tr>
<tr id="A5.T11.2.12.11" class="ltx_tr">
<th id="A5.T11.2.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mnli mismatched</th>
<td id="A5.T11.2.12.11.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.12.11.3" class="ltx_td ltx_align_right">0,35</td>
<td id="A5.T11.2.12.11.4" class="ltx_td ltx_align_right">0,38</td>
<td id="A5.T11.2.12.11.5" class="ltx_td ltx_align_right">+9,4 %</td>
</tr>
<tr id="A5.T11.2.13.12" class="ltx_tr">
<th id="A5.T11.2.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mrpc</th>
<td id="A5.T11.2.13.12.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.13.12.3" class="ltx_td ltx_align_right">0,54</td>
<td id="A5.T11.2.13.12.4" class="ltx_td ltx_align_right">0,69</td>
<td id="A5.T11.2.13.12.5" class="ltx_td ltx_align_right">+26,1 %</td>
</tr>
<tr id="A5.T11.2.14.13" class="ltx_tr">
<th id="A5.T11.2.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mrpc</th>
<td id="A5.T11.2.14.13.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.14.13.3" class="ltx_td ltx_align_right">0,66</td>
<td id="A5.T11.2.14.13.4" class="ltx_td ltx_align_right">0,81</td>
<td id="A5.T11.2.14.13.5" class="ltx_td ltx_align_right">+22,7 %</td>
</tr>
<tr id="A5.T11.2.15.14" class="ltx_tr">
<th id="A5.T11.2.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">gnad10</th>
<td id="A5.T11.2.15.14.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.15.14.3" class="ltx_td ltx_align_right">0,15</td>
<td id="A5.T11.2.15.14.4" class="ltx_td ltx_align_right">0,43</td>
<td id="A5.T11.2.15.14.5" class="ltx_td ltx_align_right">+187,7 %</td>
</tr>
<tr id="A5.T11.2.16.15" class="ltx_tr">
<th id="A5.T11.2.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">gnad10</th>
<td id="A5.T11.2.16.15.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.16.15.3" class="ltx_td ltx_align_right">0,07</td>
<td id="A5.T11.2.16.15.4" class="ltx_td ltx_align_right">0,35</td>
<td id="A5.T11.2.16.15.5" class="ltx_td ltx_align_right">+380,9 %</td>
</tr>
<tr id="A5.T11.2.17.16" class="ltx_tr">
<th id="A5.T11.2.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">gnad10</th>
<td id="A5.T11.2.17.16.2" class="ltx_td ltx_align_right">recall</td>
<td id="A5.T11.2.17.16.3" class="ltx_td ltx_align_right">0,14</td>
<td id="A5.T11.2.17.16.4" class="ltx_td ltx_align_right">0,42</td>
<td id="A5.T11.2.17.16.5" class="ltx_td ltx_align_right">+207,6 %</td>
</tr>
<tr id="A5.T11.2.18.17" class="ltx_tr">
<th id="A5.T11.2.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">pawsx es</th>
<td id="A5.T11.2.18.17.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.18.17.3" class="ltx_td ltx_align_right">0,48</td>
<td id="A5.T11.2.18.17.4" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.18.17.5" class="ltx_td ltx_align_right">+17,2 %</td>
</tr>
<tr id="A5.T11.2.19.18" class="ltx_tr">
<th id="A5.T11.2.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">piqa</th>
<td id="A5.T11.2.19.18.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.19.18.3" class="ltx_td ltx_align_right">0,67</td>
<td id="A5.T11.2.19.18.4" class="ltx_td ltx_align_right">0,72</td>
<td id="A5.T11.2.19.18.5" class="ltx_td ltx_align_right">+7,2 %</td>
</tr>
<tr id="A5.T11.2.20.19" class="ltx_tr">
<th id="A5.T11.2.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">pubmedqa</th>
<td id="A5.T11.2.20.19.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.20.19.3" class="ltx_td ltx_align_right">0,45</td>
<td id="A5.T11.2.20.19.4" class="ltx_td ltx_align_right">0,59</td>
<td id="A5.T11.2.20.19.5" class="ltx_td ltx_align_right">+32,1 %</td>
</tr>
<tr id="A5.T11.2.21.20" class="ltx_tr">
<th id="A5.T11.2.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">race</th>
<td id="A5.T11.2.21.20.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.21.20.3" class="ltx_td ltx_align_right">0,31</td>
<td id="A5.T11.2.21.20.4" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.21.20.5" class="ltx_td ltx_align_right">+11,8 %</td>
</tr>
<tr id="A5.T11.2.22.21" class="ltx_tr">
<th id="A5.T11.2.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">rte</th>
<td id="A5.T11.2.22.21.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.22.21.3" class="ltx_td ltx_align_right">0,53</td>
<td id="A5.T11.2.22.21.4" class="ltx_td ltx_align_right">0,57</td>
<td id="A5.T11.2.22.21.5" class="ltx_td ltx_align_right">+8,9 %</td>
</tr>
<tr id="A5.T11.2.23.22" class="ltx_tr">
<th id="A5.T11.2.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">sst</th>
<td id="A5.T11.2.23.22.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.23.22.3" class="ltx_td ltx_align_right">0,51</td>
<td id="A5.T11.2.23.22.4" class="ltx_td ltx_align_right">0,75</td>
<td id="A5.T11.2.23.22.5" class="ltx_td ltx_align_right">+48,2 %</td>
</tr>
<tr id="A5.T11.2.24.23" class="ltx_tr">
<th id="A5.T11.2.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">triviaqa</th>
<td id="A5.T11.2.24.23.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.24.23.3" class="ltx_td ltx_align_right">0,02</td>
<td id="A5.T11.2.24.23.4" class="ltx_td ltx_align_right">0,05</td>
<td id="A5.T11.2.24.23.5" class="ltx_td ltx_align_right">+171,3 %</td>
</tr>
<tr id="A5.T11.2.25.24" class="ltx_tr">
<th id="A5.T11.2.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">wic</th>
<td id="A5.T11.2.25.24.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.25.24.3" class="ltx_td ltx_align_right">0,50</td>
<td id="A5.T11.2.25.24.4" class="ltx_td ltx_align_right">0,55</td>
<td id="A5.T11.2.25.24.5" class="ltx_td ltx_align_right">+11,4 %</td>
</tr>
<tr id="A5.T11.2.26.25" class="ltx_tr">
<th id="A5.T11.2.26.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">winogrande</th>
<td id="A5.T11.2.26.25.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.26.25.3" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.26.25.4" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.26.25.5" class="ltx_td ltx_align_right">+12,4 %</td>
</tr>
<tr id="A5.T11.2.27.26" class="ltx_tr">
<th id="A5.T11.2.27.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">wnli</th>
<td id="A5.T11.2.27.26.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.27.26.3" class="ltx_td ltx_align_right">0,42</td>
<td id="A5.T11.2.27.26.4" class="ltx_td ltx_align_right">0,54</td>
<td id="A5.T11.2.27.26.5" class="ltx_td ltx_align_right">+26,7 %</td>
</tr>
<tr id="A5.T11.2.28.27" class="ltx_tr">
<th id="A5.T11.2.28.27.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A5.T11.2.28.27.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">wsc</th>
<td id="A5.T11.2.28.27.3" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.28.27.4" class="ltx_td ltx_align_right">0,36</td>
<td id="A5.T11.2.28.27.5" class="ltx_td ltx_align_right">0,61</td>
<td id="A5.T11.2.28.27.6" class="ltx_td ltx_align_right">+70,3 %</td>
</tr>
<tr id="A5.T11.2.29.28" class="ltx_tr">
<th id="A5.T11.2.29.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="20"><span id="A5.T11.2.29.28.1.1" class="ltx_text">
<span id="A5.T11.2.29.28.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.9pt;transform:translate(-13.04pt,-13.04pt) rotate(-90deg) ;">
<span id="A5.T11.2.29.28.1.1.1.1" class="ltx_p">MULTI</span>
</span></span></span></th>
<th id="A5.T11.2.29.28.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">xstory cloze es</th>
<td id="A5.T11.2.29.28.3" class="ltx_td ltx_align_right ltx_border_t">acc</td>
<td id="A5.T11.2.29.28.4" class="ltx_td ltx_align_right ltx_border_t">0,49</td>
<td id="A5.T11.2.29.28.5" class="ltx_td ltx_align_right ltx_border_t">0,60</td>
<td id="A5.T11.2.29.28.6" class="ltx_td ltx_align_right ltx_border_t">+24,3 %</td>
</tr>
<tr id="A5.T11.2.30.29" class="ltx_tr">
<th id="A5.T11.2.30.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mgsm de</th>
<td id="A5.T11.2.30.29.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.30.29.3" class="ltx_td ltx_align_right">0,00</td>
<td id="A5.T11.2.30.29.4" class="ltx_td ltx_align_right">0,03</td>
<td id="A5.T11.2.30.29.5" class="ltx_td ltx_align_right">-</td>
</tr>
<tr id="A5.T11.2.31.30" class="ltx_tr">
<th id="A5.T11.2.31.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada de</th>
<td id="A5.T11.2.31.30.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.31.30.3" class="ltx_td ltx_align_right">0,11</td>
<td id="A5.T11.2.31.30.4" class="ltx_td ltx_align_right">0,33</td>
<td id="A5.T11.2.31.30.5" class="ltx_td ltx_align_right">+186,4 %</td>
</tr>
<tr id="A5.T11.2.32.31" class="ltx_tr">
<th id="A5.T11.2.32.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada es</th>
<td id="A5.T11.2.32.31.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.32.31.3" class="ltx_td ltx_align_right">0,17</td>
<td id="A5.T11.2.32.31.4" class="ltx_td ltx_align_right">0,32</td>
<td id="A5.T11.2.32.31.5" class="ltx_td ltx_align_right">+91,8 %</td>
</tr>
<tr id="A5.T11.2.33.32" class="ltx_tr">
<th id="A5.T11.2.33.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada fr</th>
<td id="A5.T11.2.33.32.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.33.32.3" class="ltx_td ltx_align_right">0,20</td>
<td id="A5.T11.2.33.32.4" class="ltx_td ltx_align_right">0,38</td>
<td id="A5.T11.2.33.32.5" class="ltx_td ltx_align_right">+90,5 %</td>
</tr>
<tr id="A5.T11.2.34.33" class="ltx_tr">
<th id="A5.T11.2.34.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada it</th>
<td id="A5.T11.2.34.33.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.34.33.3" class="ltx_td ltx_align_right">0,16</td>
<td id="A5.T11.2.34.33.4" class="ltx_td ltx_align_right">0,37</td>
<td id="A5.T11.2.34.33.5" class="ltx_td ltx_align_right">+132,3 %</td>
</tr>
<tr id="A5.T11.2.35.34" class="ltx_tr">
<th id="A5.T11.2.35.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">wino de</th>
<td id="A5.T11.2.35.34.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.35.34.3" class="ltx_td ltx_align_right">0,50</td>
<td id="A5.T11.2.35.34.4" class="ltx_td ltx_align_right">0,61</td>
<td id="A5.T11.2.35.34.5" class="ltx_td ltx_align_right">+20,9 %</td>
</tr>
<tr id="A5.T11.2.36.35" class="ltx_tr">
<th id="A5.T11.2.36.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcodah de</th>
<td id="A5.T11.2.36.35.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.36.35.3" class="ltx_td ltx_align_right">0,27</td>
<td id="A5.T11.2.36.35.4" class="ltx_td ltx_align_right">0,42</td>
<td id="A5.T11.2.36.35.5" class="ltx_td ltx_align_right">+52,1 %</td>
</tr>
<tr id="A5.T11.2.37.36" class="ltx_tr">
<th id="A5.T11.2.37.36.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcodah es</th>
<td id="A5.T11.2.37.36.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.37.36.3" class="ltx_td ltx_align_right">0,28</td>
<td id="A5.T11.2.37.36.4" class="ltx_td ltx_align_right">0,43</td>
<td id="A5.T11.2.37.36.5" class="ltx_td ltx_align_right">+52,1 %</td>
</tr>
<tr id="A5.T11.2.38.37" class="ltx_tr">
<th id="A5.T11.2.38.37.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcodah fr</th>
<td id="A5.T11.2.38.37.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.38.37.3" class="ltx_td ltx_align_right">0,27</td>
<td id="A5.T11.2.38.37.4" class="ltx_td ltx_align_right">0,39</td>
<td id="A5.T11.2.38.37.5" class="ltx_td ltx_align_right">+43,7 %</td>
</tr>
<tr id="A5.T11.2.39.38" class="ltx_tr">
<th id="A5.T11.2.39.38.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcsqa de</th>
<td id="A5.T11.2.39.38.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.39.38.3" class="ltx_td ltx_align_right">0,19</td>
<td id="A5.T11.2.39.38.4" class="ltx_td ltx_align_right">0,30</td>
<td id="A5.T11.2.39.38.5" class="ltx_td ltx_align_right">+56,4 %</td>
</tr>
<tr id="A5.T11.2.40.39" class="ltx_tr">
<th id="A5.T11.2.40.39.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcsqa es</th>
<td id="A5.T11.2.40.39.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.40.39.3" class="ltx_td ltx_align_right">0,19</td>
<td id="A5.T11.2.40.39.4" class="ltx_td ltx_align_right">0,29</td>
<td id="A5.T11.2.40.39.5" class="ltx_td ltx_align_right">+57,3 %</td>
</tr>
<tr id="A5.T11.2.41.40" class="ltx_tr">
<th id="A5.T11.2.41.40.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcsqa fr</th>
<td id="A5.T11.2.41.40.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.41.40.3" class="ltx_td ltx_align_right">0,17</td>
<td id="A5.T11.2.41.40.4" class="ltx_td ltx_align_right">0,29</td>
<td id="A5.T11.2.41.40.5" class="ltx_td ltx_align_right">+69,2 %</td>
</tr>
<tr id="A5.T11.2.42.41" class="ltx_tr">
<th id="A5.T11.2.42.41.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli de</th>
<td id="A5.T11.2.42.41.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.42.41.3" class="ltx_td ltx_align_right">0,32</td>
<td id="A5.T11.2.42.41.4" class="ltx_td ltx_align_right">0,48</td>
<td id="A5.T11.2.42.41.5" class="ltx_td ltx_align_right">+51,0 %</td>
</tr>
<tr id="A5.T11.2.43.42" class="ltx_tr">
<th id="A5.T11.2.43.42.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli en</th>
<td id="A5.T11.2.43.42.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.43.42.3" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.43.42.4" class="ltx_td ltx_align_right">0,52</td>
<td id="A5.T11.2.43.42.5" class="ltx_td ltx_align_right">+5,3 %</td>
</tr>
<tr id="A5.T11.2.44.43" class="ltx_tr">
<th id="A5.T11.2.44.43.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli es</th>
<td id="A5.T11.2.44.43.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.44.43.3" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.44.43.4" class="ltx_td ltx_align_right">0,48</td>
<td id="A5.T11.2.44.43.5" class="ltx_td ltx_align_right">+41,0 %</td>
</tr>
<tr id="A5.T11.2.45.44" class="ltx_tr">
<th id="A5.T11.2.45.44.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli fr</th>
<td id="A5.T11.2.45.44.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.45.44.3" class="ltx_td ltx_align_right">0,37</td>
<td id="A5.T11.2.45.44.4" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.45.44.5" class="ltx_td ltx_align_right">+30,3 %</td>
</tr>
<tr id="A5.T11.2.46.45" class="ltx_tr">
<th id="A5.T11.2.46.45.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xstance de</th>
<td id="A5.T11.2.46.45.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.46.45.3" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.46.45.4" class="ltx_td ltx_align_right">0,52</td>
<td id="A5.T11.2.46.45.5" class="ltx_td ltx_align_right">+6,0 %</td>
</tr>
<tr id="A5.T11.2.47.46" class="ltx_tr">
<th id="A5.T11.2.47.46.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xstance de</th>
<td id="A5.T11.2.47.46.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.47.46.3" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.47.46.4" class="ltx_td ltx_align_right">0,47</td>
<td id="A5.T11.2.47.46.5" class="ltx_td ltx_align_right">+41,2 %</td>
</tr>
<tr id="A5.T11.2.48.47" class="ltx_tr">
<th id="A5.T11.2.48.47.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">xstance de</th>
<td id="A5.T11.2.48.47.2" class="ltx_td ltx_align_right ltx_border_bb">recall</td>
<td id="A5.T11.2.48.47.3" class="ltx_td ltx_align_right ltx_border_bb">0,49</td>
<td id="A5.T11.2.48.47.4" class="ltx_td ltx_align_right ltx_border_bb">0,52</td>
<td id="A5.T11.2.48.47.5" class="ltx_td ltx_align_right ltx_border_bb">+7,7 %</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A5.T11.3.1.1" class="ltx_text" style="font-size:90%;">Table 11</span>: </span><span id="A5.T11.4.2" class="ltx_text" style="font-size:90%;">Differences between our best and worst performing tokenizers per task.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.08753" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.08754" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2310.08754">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.08754" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.08755" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 01:31:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
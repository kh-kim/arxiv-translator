<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.08754] Tokenizer Choice For LLM Training: Negligible or Crucial?</title><meta property="og:description" content="The recent success of Large Language Models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tokenizer Choice For LLM Training: Negligible or Crucial?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Tokenizer Choice For LLM Training: Negligible or Crucial?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.08754">

<!--Generated on Wed Feb 28 01:31:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Tokenizer Choice For LLM Training: Negligible or Crucial?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mehdi Ali<sup id="id4.4.id1" class="ltx_sup">1,2</sup> <sup id="id5.5.id2" class="ltx_sup">†</sup>, Michael Fromm<sup id="id6.6.id3" class="ltx_sup">1,2</sup> <sup id="id7.7.id4" class="ltx_sup">†</sup>, Klaudia Thellmann<sup id="id8.8.id5" class="ltx_sup">3</sup> <sup id="id9.9.id6" class="ltx_sup">†</sup> 
<br class="ltx_break">Richard Rutmann<sup id="id10.10.id7" class="ltx_sup">1,2</sup>, Max Lübbering<sup id="id11.11.id8" class="ltx_sup">1,2</sup>, Johannes Leveling<sup id="id12.12.id9" class="ltx_sup">1</sup>, Katrin Klug<sup id="id13.13.id10" class="ltx_sup">1</sup>, Jan Ebert<sup id="id14.14.id11" class="ltx_sup">4</sup>, 
<br class="ltx_break">Niclas Doll<sup id="id15.15.id12" class="ltx_sup">1</sup>, Jasper Schulze Buschhoff<sup id="id16.16.id13" class="ltx_sup">1</sup>, Charvi Jain<sup id="id17.17.id14" class="ltx_sup">1,2</sup>, Alexander Arno Weber<sup id="id18.18.id15" class="ltx_sup">1,2</sup>, 
<br class="ltx_break">Lena Jurkschat<sup id="id19.19.id16" class="ltx_sup">3</sup>, Hammam Abdelwahab<sup id="id20.20.id17" class="ltx_sup">1</sup>
Chelsea John<sup id="id21.21.id18" class="ltx_sup">4</sup>, Pedro Ortiz Suarez<sup id="id22.22.id19" class="ltx_sup">5</sup>, Malte Ostendorff<sup id="id23.23.id20" class="ltx_sup">5</sup> 
<br class="ltx_break">Samuel Weinbach<sup id="id24.24.id21" class="ltx_sup">6</sup>, Rafet Sifa<sup id="id25.25.id22" class="ltx_sup">1</sup>, Stefan Kesselheim<sup id="id26.26.id23" class="ltx_sup">4</sup>, Nicolas Flores-Herr<sup id="id27.27.id24" class="ltx_sup">1</sup> 
<br class="ltx_break">
<br class="ltx_break"><sup id="id28.28.id25" class="ltx_sup">1</sup>Fraunhofer IAIS, <sup id="id29.29.id26" class="ltx_sup">2</sup>Lamarr Institute, <sup id="id30.30.id27" class="ltx_sup">3</sup>TU-Dresden, <sup id="id31.31.id28" class="ltx_sup">4</sup>FZ Jülich, <sup id="id32.32.id29" class="ltx_sup">5</sup>DFKI, <sup id="id33.33.id30" class="ltx_sup">6</sup>Aleph Alpha
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id34.id1"><span class="ltx_glossaryref" title="">Large Language Models (LLMs)</span>의 최근 성공은 훈련 데이터세트 구성의 큐레이팅, 모델 아키텍처 및 데이터세트 크기의 스케일링 및 사전 훈련 목표의 진보에 의해 주로 주도되어 토큰나이저의 영향을 맹점으로 남긴다. 이 미처리 영역에 대한 빛을 무시하고 2.6 B 매개변수 규모에서 24개의 단일 및 다국어 LLM을 훈련하고 다양한 토큰izer 알고리즘 및 매개변수화를 제거함으로써 토큰izer 선택이 LLM 다운스트림 성능에 미치는 영향에 대한 포괄적인 연구를 수행한다. 우리의 연구는 토큰라이저 선택이 모델의 다운스트림 성능, 훈련 및 추론 비용에 상당한 영향을 미칠 수 있음을 강조한다. 특히, 일반적인 토큰나이저 평가 메트릭 <span class="ltx_text ltx_font_italic" id="id34.id1.1">fertility</span> 및 <span class="ltx_text ltx_font_italic" id="id34.id1.2">parity</span>이 항상 모델 다운스트림 성능을 예측하는 것은 아니므로 이러한 메트릭을 모델의 다운스트림 성능에 대한 의심스러운 프록시로 렌더링합니다. 또한, 5개의 가장 빈번한 유럽 언어에 대해 훈련된 다국어 토큰라이저는 영어에 비해 3인자의 어휘 크기 증가가 필요하다는 것을 보여준다. 영어 중심 토큰라이저는 다중 언어 <span class="ltx_glossaryref" title="">LLMs</span>의 훈련에 적용되었지만, 이 방법은 비효율적인 토큰화 어휘로 인해 최대 68%의 심각한 다운스트림 성능 저하 및 추가 훈련 비용을 초래한다는 것을 발견했다.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>†Equal contribution.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_glossaryref" title="">LLMs</span>은 요약, 읽기 이해, 번역 및 상식 추론 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib5" title="">2020b</a>); Touvron et al. (<a class="ltx_ref" href="#bib.bib46" title="">2023</a>)</cite>와 같은 제로/페우 샷 설정에서 많은 다운스트림 태스크에서 인상적인 능력을 보여주었다. LLM을 학습하기 위해 현재 확립된 접근법은 토큰이 단어 <cite class="ltx_cite ltx_citemacro_cite">Bengio et al. (<a class="ltx_ref" href="#bib.bib2" title="">2000</a>)</cite>, 서브워드 <cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a class="ltx_ref" href="#bib.bib39" title="">2012</a>); Sennrich et al. (<a class="ltx_ref" href="#bib.bib40" title="">2015</a>); Wang et al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite>, 또는 단일 문자 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib15" title="">2020b</a>)</cite>를 나타내는 토큰으로 학습 문서를 분할하는 토큰화기를 사용하는 것이며, 각 토큰은 추가로 처리될 수 있는 임베딩 벡터에 의해 모델에서 표현된다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">토큰라이저의 품질은 <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">intrinsically</span> 및 <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">extrinsically</span>을 평가할 수 있습니다. 내재적 평가는 단독으로 토큰라이저의 특성과 생성된 출력을 다루는 반면, 외재적 평가는 다운스트림 구성 요소(예: <span class="ltx_glossaryref" title="">Large Language Model (LLM)</span>)에 대한 토큰라이저의 영향을 측정합니다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">문자 기반에서 단어 기반 방법에 이르기까지 많은 다른 토큰화 접근법이 제안되었지만, 특히 다국어 <span class="ltx_glossaryref" 제목="">LLMs</span>의 맥락에서 다른 토큰화자의 잠재적 영향은 무시된다. <cite class="ltx_cite ltx_citemacro_citet">Petrov et al. (<a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite>가 제안한 최근 연구는 다국어 <span class="ltx_glossaryref" title="">LLMs</span>의 훈련에 적용된 부주의하게 설계된 tokenizers가 언어 전반에 걸쳐 심각한 부등식과 한계를 초래한다는 것을 보여준다. 실제로 서로 다른 언어로 번역된 텍스트 구절은 길이가 최대 15배까지 다른 토큰화된 서열을 생성하여 추론 비용과 추론 중 지연에 영향을 미쳤다. 나아가, 트랜스포머 기반의 <span class="ltx_glossaryref" title="">LLMs</span>을 효과적으로 학습하기 위한 필수 속성은 장거리 의존성 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="#bib.bib47" title="">2017</a>)</cite>의 학습인 것으로 알려져 있다. 고정된 시퀀스 길이가 주어지면, 텍스트가 토나이저에 의해 과도하게 단편화된 언어들에 대해 입력 텍스트에서 멀리 떨어진 단어들을 연관시키는 학습은 불가능하다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">토큰라이저의 중요성과 제대로 수행되지 않는 토큰라이저의 잠재적으로 심각한 영향에도 불구하고, 현재 <span class="ltx_glossaryref" 제목="">LLMs</span>의 백본을 나타내는 디코더 전용 모델에 초점을 맞추어 단일 언어 및 다국어 설정에서 고유 및 외부 토큰라이저 성능을 전체적으로 조사하는 광범위한 연구는 지금까지 존재하지 않는다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">따라서 이 격차를 해결하고 토큰화기가 모델 성능에 미치는 영향을 측정하는 광범위한 연구를 수행한다. 특히 다음과 같은 공헌을 합니다.</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1">토나이저 고유 성능을 조사하는 연구를 수행한다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1">외부 토큰나이저 성능, 즉 토큰나이저가 모델의 다운스트림 성능에 미치는 영향을 조사하는 연구를 수행한다.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1">내재적 토크나이저 성능과 외재적 토크나이저 성능 간의 상관 관계가 존재하는지 조사합니다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">이 섹션에서는 인코더 및 디코더 전용 변압기 모델에서 토큰화 알고리즘과 그 사용에 대한 개요를 제공한다.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Tokenization Approaches</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Word Tokenization.</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">가장 기본적인 토큰화 접근법은 화이트 스페이스를 기반으로 시퀀스를 분할하고 각 단어를 토큰 <cite class="ltx_cite ltx_citemacro_cite">Bengio et al. (<a class="ltx_ref" href="#bib.bib2" title="">2000</a>)</cite>로 간주하는 것이다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Subword tokenization.</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.1.1">Subword tokenization</em> 알고리즘은 단어를 하위 단어/다중 토큰으로 분해할 수 있는 데이터 구동 토큰화 접근법이며 현재 <span class="ltx_glossaryref" 제목="">LLMs</span> rely <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="#bib.bib27" title="">2018</a>); Petrov et al. (<a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite>에 대한 확립된 토큰화 접근법을 나타낸다. 서브워드 토큰라이저는 단어를 서브워드로 분해하기 때문에 어휘<cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="#bib.bib27" title="">2018</a>)</cite>에서 서브워드를 병합하여 어휘 외 단어를 처리할 수 있다. 인기 서브워드 토큰라이저의 예로는 WordPiece <cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a class="ltx_ref" href="#bib.bib39" title="">2012</a>)</cite>, BPE <cite class="ltx_cite ltx_citemacro_cite">Gage (<a class="ltx_ref" href="#bib.bib13" title="">1994</a>); Sennrich et al. (<a class="ltx_ref" href="#bib.bib40" title="">2015</a>)</cite>, <span class="ltx_glossaryref" title="">Byte-Level BPE(BBPE)</span> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite>, Unigram <cite class="ltx_cite ltx_citemacro_cite">Kudo (<a class="ltx_ref" href="#bib.bib26" title="">2018</a>)</cite> 등이 있다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Character Tokenization.</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">토큰화는 또한 문자 레벨에서 또는 UTF-8 바이트에 기초하여 수행될 수 있다. 그러나 이는 트랜스포머 아키텍처에서 계산적으로 비용이 많이 드는 증가된 시퀀스 길이를 초래하며, 시퀀스 길이 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="#bib.bib47" title="">2017</a>)</cite>에서 셀프-어텐션 계층의 2차 복잡성으로 인해 <span class="ltx_glossaryref" 제목="">LLMs</span>에 대한 현재 우세한 아키텍처가 된다. 그러나, 이러한 한계 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib15" title="">2020b</a>); Tay et al. (<a class="ltx_ref" href="#bib.bib43" title="">2021</a>); Xue et al. (<a class="ltx_ref" href="#bib.bib49" title="">2022</a>); Clark et al. (<a class="ltx_ref" href="#bib.bib8" title="">2022</a>); Yu et al. (<a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite>를 해결하기 위해 몇 가지 접근법이 제안되었다.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tokenizers in Transformers Models</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Encoder Models</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">토큰화에 대한 대부분의 연구는 인코더 모델에 대해 수행되었다. <cite class="ltx_cite ltx_citemacro_citet">Rust et al. (<a class="ltx_ref" href="#bib.bib36" title="">2021</a>)</cite>는 토큰izer 선택이 다중 및 단일 언어 BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="#bib.bib12" title="">2018</a>)</cite> 모델의 다운스트림 성능에 영향을 미치는지 조사했다. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib52" title="">2022</a>)</cite>는 토큰나이저 트레이닝 동안 언어가 동일하게 샘플링될 때 더 나은 기계 번역 성능이 종종 얻어짐을 보여주었다. <cite class="ltx_cite ltx_citemacro_citet">Toraman et al. (<a class="ltx_ref" href="#bib.bib44" title="">2023</a>)</cite>는 터키어를 위한 여러 중간 크기의 언어 모델을 훈련시켰고 서로 다른 서브워드 토큰라이저는 대략적으로 동등한 성능을 보이는 반면 단어 및 문자 수준 토큰라이저는 다운스트림 태스크에서 훨씬 더 나쁜 성능을 보인다고 제안했다. 마지막으로, <cite class="ltx_cite ltx_citemacro_cite">Chirkova and Troshin (<a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>는 코드 관련 작업에 대한 다양한 토큰화를 사용하는 효과를 분석했으며, 신중하게 구성된 토큰화기가 평균 시퀀스 길이를 최대 40%까지 줄이거나 낮은 압축률에서 최대 2%까지 작은 다운스트림 성능 개선을 허용할 수 있음을 입증했다.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tokenizers in Decoder Models</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">현재 단일 및 다국어 <span class="ltx_glossaryref" 제목="">LLMs</span>에 대한 개요는 <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib29" title="">2022</a>); Shliazhko et al. (<a class="ltx_ref" href="#bib.bib41" title="">2022</a>); Scao et al. (<a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>에 제공됩니다. 그 작업의 일환으로, <cite class="ltx_cite ltx_citemacro_citet">Shliazhko et al. (<a class="ltx_ref" href="#bib.bib41" title="">2022</a>)</cite>는 토큰나이저 알고리즘, 어휘 크기 및 사용된 구현은 고정된 상태로 유지하면서 서로 다른 토큰나이저 전처리 방법을 제거했다. 다른 주요 <span class="ltx_glossaryref" 제목="">LLMs</span> 워크에서는 extrinsic tokenizer 성능이 연구되지 않았습니다.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">토나이저가 모델 성능에 미치는 영향을 조사하기 위해 광범위한 절제 연구를 수행했다. 구체적으로, 토큰라이저와 모델의 학습을 위한 전용 데이터셋을 생성하고, BPE와 Unigram 토큰라이저를 학습시켰으며, 각 토큰라이저에 대해 나머지 구성(데이터세트 및 모델 하이퍼파라미터)을 고정한 상태에서 2.6B 파라미터 크기의 디코더 전용 모델을 학습시켰다. 이를 통해 토큰화기가 모델의 다운스트림 성능에 미치는 영향을 개별적으로 측정할 수 있었다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">토나이저와 모델 학습 데이터 세트를 만드는 동안 데이터 도메인(위키피디아, 책, 웹 텍스트)의 혼합 비율이 동일한 분포를 따르도록 하여 토나이저 학습과 모델 학습 간의 도메인 이동을 방지합니다. 우리는 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">2개의 데이터세트</em>을 만들었습니다. 여기서 데이터세트 중 하나는 영어 문서를 포함하는 단일 언어이고 두 번째는 영어, 독일어, 프랑스어, 이탈리아어 및 스페인어 문서로 구성된 다국어 데이터세트입니다. 우리의 데이터 세트는 필터링되고 중복 제거되며 웹 크롤링된 데이터(80%) 및 큐레이트된 데이터(20%)로 구성되며 <span class="ltx_glossaryref" 제목="">LLMs</span>을 훈련하는 데 사용되는 관련 데이터 세트에 필적한다. 다국어 데이터 세트에서 웹 크롤링된 데이터의 양은 단어 수 측면에서 언어 간에 균등하게 분포된다. 데이터 파이프라인 및 데이터 구성에 대한 자세한 내용은 <a class="ltx_ref" href="#A1.T5" title="In Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>에 설명되어 있습니다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tokenizer</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">우리의 연구는 두 가지 확립된 토큰화 알고리즘인 BPE와 Unigram에 의존하며, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Huggingface tokenizer</span> 라이브러리 <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a class="ltx_ref" href="#bib.bib31" title="">2023</a>)</cite>와 <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">SentencePiece</span> 라이브러리 <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="#bib.bib27" title="">2018</a>)</cite>에서 구현된다. 사전 및 사후 처리 단계의 차이와 구현의 잠재적 차이에 대한 영향을 조사하기 위해 두 라이브러리를 모두 고려했다. Huggingface의 Unigram 구현에 대한 사전 처리 옵션이 누락되어 SentencePiece의 Unigram 구현과 비교하여 결과 어휘에 큰 불일치가 발생하므로 Huggingface 기반 Unigram 토큰라이저의 교육을 생략했다. 전반적으로 24개의 서로 다른 토큰라이저를 훈련했는데, 여기서 토큰라이저의 절반은 단일 언어 영어 토큰라이저이고 나머지 절반은 다국어 토큰라이저이다. 토나이저 알고리즘, 언어 구성 및 사용된 토나이저 라이브러리 외에도 어휘 크기를 변경했다. 구체적인 토큰izer 구성은 <a class="ltx_ref" href="#A2" title="Appendix B Tokenizer ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>에 설명되어 있다.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Models</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">훈련된 토큰라이저가 모델 다운스트림 성능에 미치는 영향을 측정하기 위해 각 토큰라이저에 대해 하나의 모델을 훈련했다. 특히, 24개의 훈련된 토큰아이저 각각에 대해, <cite class="ltx_cite ltx_citemacro_cite">Hoffmann et al. (<a class="ltx_ref" href="#bib.bib21" title="">2022a</a>)</cite>에서 제안한 스케일링 법칙에 따라 최대 52B 토큰에 대해 2.6B 트랜스포머 기반 디코더 전용 모델을 훈련시켰다. 또한 기준선 역할을 하는 사전 훈련된 GPT-2 토크나이저를 사용하여 단일 언어 및 다중 언어 모델을 훈련했다. 모든 모델은 인과 언어 모델링 훈련 목표에 기초하여 훈련되었다.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.1">토나이저가 모델 다운스트림 성능에 미치는 영향을 평가하기 위해 먼저 고유 토나이저 평가를 수행한 다음 외부 평가를 수행하고 마지막으로 두 평가 접근법 간의 상관 관계가 있는지 조사했다.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p2.1">진성 평가는 <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.1">fertility</span> 및 <span class="ltx_text ltx_font_italic" id="S3.SS4.p2.1.2">parity</span>을 기반으로 토큰라이저의 생성된 출력을 평가하는 것을 목표로 합니다. 또한, 토큰화기의 어휘가 다른 토큰화기와 중복되는 것이 계산된다. 내재적 평가는 토큰라이저가 모델 성능에 미치는 영향을 평가하지 않습니다.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p3.5">토큰나이저의 성능 <cite class="ltx_cite ltx_citemacro_cite">Scao et al. (<a class="ltx_ref" href="#bib.bib38" title="">2022</a>); Stollenwerk (<a class="ltx_ref" href="#bib.bib42" title="">2023</a>); Rust et al. (<a class="ltx_ref" href="#bib.bib36" title="">2021</a>)</cite>를 평가하는 가장 일반적인 메트릭인 Fertility는 단어 또는 문서를 나타내는 데 필요한 토큰의 평균 수로 정의된다. 토큰나이저 <math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">T</annotation></semantics></math> 및 데이터셋 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">A</annotation></semantics></math>의 경우, 비옥도는 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">A</annotation></semantics></math>(<math alttext="T" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.1"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">T</annotation></semantics></math>가 적용된 경우)의 토큰 수를 <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">A</annotation></semantics></math>의 단어 수로 나눈 값으로 계산될 수 있다. 토나이저 교육에 사용되지 않은 보류된 세트(10,000개 문서)에서 생식력을 계산한다. 문서의 단어를 계산하기 위해 우리는 공백 분할을 사용했다. 더 높은 비옥도 점수는 토큰화기의 더 약한 압축 능력에 해당한다.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.8" class="ltx_p">Parity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Petrov et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, which has been recently proposed, assesses how fairly a tokenizer treats equivalent sentences in different languages. A tokenizer <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">T</annotation></semantics></math> achieves parity for language <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">A</annotation></semantics></math> with respect to language <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">B</annotation></semantics></math> if <math id="S3.SS4.p4.4.m4.2" class="ltx_Math" alttext="\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1" display="inline"><semantics id="S3.SS4.p4.4.m4.2a"><mrow id="S3.SS4.p4.4.m4.2.3" xref="S3.SS4.p4.4.m4.2.3.cmml"><mfrac id="S3.SS4.p4.4.m4.2.2" xref="S3.SS4.p4.4.m4.2.2.cmml"><mrow id="S3.SS4.p4.4.m4.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.4.m4.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS4.p4.4.m4.1.1.1.1.3" xref="S3.SS4.p4.4.m4.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS4.p4.4.m4.2.2.2.1" xref="S3.SS4.p4.4.m4.2.2.2.2.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.2" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.4.m4.2.2.2.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml">​</mo><mrow id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml"><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml">B</mi></msub><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.3" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS4.p4.4.m4.2.2.2.1.3" xref="S3.SS4.p4.4.m4.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.SS4.p4.4.m4.2.3.1" xref="S3.SS4.p4.4.m4.2.3.1.cmml">≈</mo><mn id="S3.SS4.p4.4.m4.2.3.2" xref="S3.SS4.p4.4.m4.2.3.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.2b"><apply id="S3.SS4.p4.4.m4.2.3.cmml" xref="S3.SS4.p4.4.m4.2.3"><approx id="S3.SS4.p4.4.m4.2.3.1.cmml" xref="S3.SS4.p4.4.m4.2.3.1"></approx><apply id="S3.SS4.p4.4.m4.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2"><divide id="S3.SS4.p4.4.m4.2.2.3.cmml" xref="S3.SS4.p4.4.m4.2.2"></divide><apply id="S3.SS4.p4.4.m4.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1"><abs id="S3.SS4.p4.4.m4.1.1.1.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.2"></abs><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1"><times id="S3.SS4.p4.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.2"></times><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.1.1.1.1.1.1.3">𝐴</ci></apply></apply></apply><apply id="S3.SS4.p4.4.m4.2.2.2.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1"><abs id="S3.SS4.p4.4.m4.2.2.2.2.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.2"></abs><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1"><times id="S3.SS4.p4.4.m4.2.2.2.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.2"></times><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.3">𝑇</ci><apply id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p4.4.m4.2.2.2.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply><cn type="integer" id="S3.SS4.p4.4.m4.2.3.2.cmml" xref="S3.SS4.p4.4.m4.2.3.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.2c">\frac{|T(s_{A})|}{|T(s_{B})|}\approx 1</annotation></semantics></math>, where <math id="S3.SS4.p4.5.m5.1" class="ltx_Math" alttext="s_{A}" display="inline"><semantics id="S3.SS4.p4.5.m5.1a"><msub id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml"><mi id="S3.SS4.p4.5.m5.1.1.2" xref="S3.SS4.p4.5.m5.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.5.m5.1.1.3" xref="S3.SS4.p4.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><apply id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.5.m5.1.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p4.5.m5.1.1.2.cmml" xref="S3.SS4.p4.5.m5.1.1.2">𝑠</ci><ci id="S3.SS4.p4.5.m5.1.1.3.cmml" xref="S3.SS4.p4.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">s_{A}</annotation></semantics></math> and <math id="S3.SS4.p4.6.m6.1" class="ltx_Math" alttext="s_{B}" display="inline"><semantics id="S3.SS4.p4.6.m6.1a"><msub id="S3.SS4.p4.6.m6.1.1" xref="S3.SS4.p4.6.m6.1.1.cmml"><mi id="S3.SS4.p4.6.m6.1.1.2" xref="S3.SS4.p4.6.m6.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.6.m6.1.1.3" xref="S3.SS4.p4.6.m6.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m6.1b"><apply id="S3.SS4.p4.6.m6.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.6.m6.1.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p4.6.m6.1.1.2.cmml" xref="S3.SS4.p4.6.m6.1.1.2">𝑠</ci><ci id="S3.SS4.p4.6.m6.1.1.3.cmml" xref="S3.SS4.p4.6.m6.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m6.1c">s_{B}</annotation></semantics></math> denote the sets of all sentences in the corpora of language <math id="S3.SS4.p4.7.m7.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p4.7.m7.1a"><mi id="S3.SS4.p4.7.m7.1.1" xref="S3.SS4.p4.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m7.1b"><ci id="S3.SS4.p4.7.m7.1.1.cmml" xref="S3.SS4.p4.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m7.1c">A</annotation></semantics></math> and <math id="S3.SS4.p4.8.m8.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p4.8.m8.1a"><mi id="S3.SS4.p4.8.m8.1.1" xref="S3.SS4.p4.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.8.m8.1b"><ci id="S3.SS4.p4.8.m8.1.1.cmml" xref="S3.SS4.p4.8.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.8.m8.1c">B</annotation></semantics></math>, respectively. We use the FLORES-200 <cite class="ltx_cite ltx_citemacro_cite">Goyal et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> parallel corpus, consisting of the same sentences human-translated into 200 languages. We calculate the parity values for each tokenizer and the four non-English languages with respect to English (see <a href="#S4.F2" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> for an overview).</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p5.1">외재적 평가는 토큰화기가 모델의 다운스트림 성능에 미치는 영향을 명시적으로 평가하는 것을 목표로 한다. 우리는 다운스트림 성능을 측정하기 위해 상식 추론, 읽기 이해, 번역 및 수학 과제를 포함하는 포괄적인 다운스트림 과제 세트를 선택했다. 모든 작업에 대한 포괄적인 목록은 판독기를 <a class="ltx_ref" href="#A5" title="Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a>에서 <a class="ltx_ref" href="#A5.T11" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>로 참조한다.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p6.8">또한, 훈련 및 추론 동안 단어당 주어진 모델의 평균 계산 비용에 대한 토큰화기의 영향을 계산했다. 상기 순방향 및 상기 역방향 패스를 포함하는 한 단계에 대한 훈련 동안의 계산 비용은,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right)," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">C</mi><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">96</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.4.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2c" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.1.7" xref="S3.E1.m1.1.1.1.1.1.7.cmml"><mi id="S3.E1.m1.1.1.1.1.1.7.2" xref="S3.E1.m1.1.1.1.1.1.7.2.cmml">h</mi><mn id="S3.E1.m1.1.1.1.1.1.7.3" xref="S3.E1.m1.1.1.1.1.1.7.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2d" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">s</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">h</mi></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml">V</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml">16</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml">h</mi></mrow></mfrac></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">𝐶</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">96</cn><ci id="S3.E1.m1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.4">𝐵</ci><ci id="S3.E1.m1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.5">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.1.6">𝑙</ci><apply id="S3.E1.m1.1.1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.7.1.cmml" xref="S3.E1.m1.1.1.1.1.1.7">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.7.2.cmml" xref="S3.E1.m1.1.1.1.1.1.7.2">ℎ</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.7.3.cmml" xref="S3.E1.m1.1.1.1.1.1.7.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝑠</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">6</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">ℎ</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.2">𝑉</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.2">16</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.4.3.4">ℎ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">C=96Bslh^{2}\left(1+\dfrac{s}{6h}+\dfrac{V}{16lh}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p6.7" class="ltx_p">given a model with batch size <math id="S3.SS4.p6.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p6.1.m1.1a"><mi id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><ci id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">B</annotation></semantics></math>, sequence length <math id="S3.SS4.p6.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS4.p6.2.m2.1a"><mi id="S3.SS4.p6.2.m2.1.1" xref="S3.SS4.p6.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.2.m2.1b"><ci id="S3.SS4.p6.2.m2.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.2.m2.1c">s</annotation></semantics></math>, <math id="S3.SS4.p6.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS4.p6.3.m3.1a"><mi id="S3.SS4.p6.3.m3.1.1" xref="S3.SS4.p6.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.3.m3.1b"><ci id="S3.SS4.p6.3.m3.1.1.cmml" xref="S3.SS4.p6.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.3.m3.1c">l</annotation></semantics></math> layers, hidden size <math id="S3.SS4.p6.4.m4.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS4.p6.4.m4.1a"><mi id="S3.SS4.p6.4.m4.1.1" xref="S3.SS4.p6.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.4.m4.1b"><ci id="S3.SS4.p6.4.m4.1.1.cmml" xref="S3.SS4.p6.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.4.m4.1c">h</annotation></semantics></math> and vocabulary size <math id="S3.SS4.p6.5.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS4.p6.5.m5.1a"><mi id="S3.SS4.p6.5.m5.1.1" xref="S3.SS4.p6.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.5.m5.1b"><ci id="S3.SS4.p6.5.m5.1.1.cmml" xref="S3.SS4.p6.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.5.m5.1c">V</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Narayanan et&nbsp;al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>. The costs per token can be derived by <math id="S3.SS4.p6.6.m6.1" class="ltx_Math" alttext="C_{\text{token}}=C/Bs" display="inline"><semantics id="S3.SS4.p6.6.m6.1a"><mrow id="S3.SS4.p6.6.m6.1.1" xref="S3.SS4.p6.6.m6.1.1.cmml"><msub id="S3.SS4.p6.6.m6.1.1.2" xref="S3.SS4.p6.6.m6.1.1.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.2.2" xref="S3.SS4.p6.6.m6.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.6.m6.1.1.2.3" xref="S3.SS4.p6.6.m6.1.1.2.3a.cmml">token</mtext></msub><mo id="S3.SS4.p6.6.m6.1.1.1" xref="S3.SS4.p6.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.6.m6.1.1.3" xref="S3.SS4.p6.6.m6.1.1.3.cmml"><mrow id="S3.SS4.p6.6.m6.1.1.3.2" xref="S3.SS4.p6.6.m6.1.1.3.2.cmml"><mi id="S3.SS4.p6.6.m6.1.1.3.2.2" xref="S3.SS4.p6.6.m6.1.1.3.2.2.cmml">C</mi><mo id="S3.SS4.p6.6.m6.1.1.3.2.1" xref="S3.SS4.p6.6.m6.1.1.3.2.1.cmml">/</mo><mi id="S3.SS4.p6.6.m6.1.1.3.2.3" xref="S3.SS4.p6.6.m6.1.1.3.2.3.cmml">B</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS4.p6.6.m6.1.1.3.1" xref="S3.SS4.p6.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p6.6.m6.1.1.3.3" xref="S3.SS4.p6.6.m6.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.6.m6.1b"><apply id="S3.SS4.p6.6.m6.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1"><eq id="S3.SS4.p6.6.m6.1.1.1.cmml" xref="S3.SS4.p6.6.m6.1.1.1"></eq><apply id="S3.SS4.p6.6.m6.1.1.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.6.m6.1.1.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.6.m6.1.1.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.2.3a.cmml" xref="S3.SS4.p6.6.m6.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p6.6.m6.1.1.2.3.cmml" xref="S3.SS4.p6.6.m6.1.1.2.3">token</mtext></ci></apply><apply id="S3.SS4.p6.6.m6.1.1.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3"><times id="S3.SS4.p6.6.m6.1.1.3.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.1"></times><apply id="S3.SS4.p6.6.m6.1.1.3.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2"><divide id="S3.SS4.p6.6.m6.1.1.3.2.1.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.1"></divide><ci id="S3.SS4.p6.6.m6.1.1.3.2.2.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.6.m6.1.1.3.2.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.2.3">𝐵</ci></apply><ci id="S3.SS4.p6.6.m6.1.1.3.3.cmml" xref="S3.SS4.p6.6.m6.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.6.m6.1c">C_{\text{token}}=C/Bs</annotation></semantics></math> and the average costs per word by <math id="S3.SS4.p6.7.m7.1" class="ltx_Math" alttext="C_{\text{word}}=C_{\text{token}}\cdot\text{fertility}" display="inline"><semantics id="S3.SS4.p6.7.m7.1a"><mrow id="S3.SS4.p6.7.m7.1.1" xref="S3.SS4.p6.7.m7.1.1.cmml"><msub id="S3.SS4.p6.7.m7.1.1.2" xref="S3.SS4.p6.7.m7.1.1.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.2.2" xref="S3.SS4.p6.7.m7.1.1.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.2.3" xref="S3.SS4.p6.7.m7.1.1.2.3a.cmml">word</mtext></msub><mo id="S3.SS4.p6.7.m7.1.1.1" xref="S3.SS4.p6.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS4.p6.7.m7.1.1.3" xref="S3.SS4.p6.7.m7.1.1.3.cmml"><msub id="S3.SS4.p6.7.m7.1.1.3.2" xref="S3.SS4.p6.7.m7.1.1.3.2.cmml"><mi id="S3.SS4.p6.7.m7.1.1.3.2.2" xref="S3.SS4.p6.7.m7.1.1.3.2.2.cmml">C</mi><mtext id="S3.SS4.p6.7.m7.1.1.3.2.3" xref="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml">token</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p6.7.m7.1.1.3.1" xref="S3.SS4.p6.7.m7.1.1.3.1.cmml">⋅</mo><mtext id="S3.SS4.p6.7.m7.1.1.3.3" xref="S3.SS4.p6.7.m7.1.1.3.3a.cmml">fertility</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.7.m7.1b"><apply id="S3.SS4.p6.7.m7.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1"><eq id="S3.SS4.p6.7.m7.1.1.1.cmml" xref="S3.SS4.p6.7.m7.1.1.1"></eq><apply id="S3.SS4.p6.7.m7.1.1.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p6.7.m7.1.1.2.3.cmml" xref="S3.SS4.p6.7.m7.1.1.2.3">word</mtext></ci></apply><apply id="S3.SS4.p6.7.m7.1.1.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3"><ci id="S3.SS4.p6.7.m7.1.1.3.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.1">⋅</ci><apply id="S3.SS4.p6.7.m7.1.1.3.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p6.7.m7.1.1.3.2.1.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p6.7.m7.1.1.3.2.2.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.2">𝐶</ci><ci id="S3.SS4.p6.7.m7.1.1.3.2.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.3"><mtext mathsize="70%" id="S3.SS4.p6.7.m7.1.1.3.2.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3.2.3">token</mtext></ci></apply><ci id="S3.SS4.p6.7.m7.1.1.3.3a.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3"><mtext id="S3.SS4.p6.7.m7.1.1.3.3.cmml" xref="S3.SS4.p6.7.m7.1.1.3.3">fertility</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.7.m7.1c">C_{\text{word}}=C_{\text{token}}\cdot\text{fertility}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Intrinsic Tokenizer Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">우리의 고유 평가에서 먼저 훈련된 토큰아이저(섹션<a class="ltx_ref" href="#S4.SS1" title="4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4.1</span></a>)의 번식력과 패리티와 그들의 어휘의 중첩(섹션<a class="ltx_ref" href="#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4.2</span></a>)을 비교한다.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fertility &amp; Parity</h3>

<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x1.png" id="S4.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F1.sf1.2.1.1" style="font-size:90%;">(a)</span></span><span class="ltx_text" id="S4.F1.sf1.3.2" style="font-size:90%;">Non-English, multilingual documents</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x2.png" id="S4.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">English documents</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F1.2.1.1" style="font-size:90%;">그림 1</span>:</span><span class="ltx_text" id="S4.F1.3.2" style="font-size:90%;">단일 언어 (영어) 토큰나이저와 (a) 영어 및 (b) 비영어, 다중 언어 문서에 적용된 다중 언어 토큰나이저 간의 번식력 점수 비교. </span></figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">설명된 번식력 및 패리티 평가를 단일/다국어 토큰라이저에 적용하면 우리의 분석에서는 <a class="ltx_ref" href="#S4.F1" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a></a>와 <a class="ltx_ref" href="#S4.F2" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>에서 볼 수 있는 다음과 같은 두 가지 주요 측면을 강조한다.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.1">첫째, 단일 언어 토큰화기를 다국어 데이터에 적용하면 번식력 및 패리티 점수가 상당히 높다는 것을 관찰할 수 있다(<a class="ltx_ref" href="#S4.F1.sf1" title="In Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a> 및 <a class="ltx_ref" href="#S4.F2" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> 참조). 다국어 토큰라이저는 모든 비영어 문서에서 단일 언어 영어 토큰라이저보다 큰 마진만큼 낮은 번식력을 갖는 반면, <a class="ltx_ref" href="#S4.F1.sf2" title="In Figure 1 ‣ 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(b)</span></a>와 같이 영어 문서를 토큰라이징하는 데 약간 더 나쁘다.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p3.1">둘째, 어휘 크기가 증가함에 따라, 비옥도 및 패리티가 모든 경우에 감소하는데, 이는 더 큰 어휘가 주어진 텍스트를 토큰화할 때 더 적은 서브워드 토큰들을 요구하는 토큰화기에 의해 설명될 수 있다. 그러나 단일 언어 영어 토큰라이저의 경우 영어 문서를 토큰화할 때 생식력이 어휘에 덜 의존한다는 것을 관찰할 수 있으며, 이는 33k가 충분히 큰 어휘일 수 있음을 의미한다. 이것은 LLaMA/LLaMA2 모델이 33k<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="#bib.bib45" title="">Touvron et al. </a>; Touvron et al. (<a class="ltx_ref" href="#bib.bib46" title="">2023</a>)</cite>의 어휘 크기로 훈련되었다는 사실을 고려할 때 특히 흥미롭다.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x3.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">그림 2</span>:</span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Singleingual (English) tokenizer와 Multiilingual tokenizers 적용 다중 언어 문서 간의 패리티 점수 비교. </span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Vocabulary Overlap</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">토큰나이저 유사도를 분석하기 위해 어휘 중복도를 계산하였다. 특히 Huggingface와 SentencePiece의 BPE 구현을 평가한다. 그들의 어휘 중복은 <a class="ltx_ref" href="#S4.T1" title="In 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>로 묘사된다.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p2.1">중첩은 서로 다른 어휘 크기에 걸쳐 대략 일정하며, 두 개의 서로 다른 라이브러리에 의해서만 구현되는 동일한 알고리즘임에도 불구하고 전체 중첩은 다소 낮은 경향이 있다. 결과적으로, 토큰화기는 서로 다른 토큰화된 시퀀스를 생성하여 모델 훈련 및 다운스트림 성능에 영향을 미칠 수 있다. 근본적인 이유를 조사하면 이러한 라이브러리의 구성 및 전처리 옵션이 다르기 때문에 중복이 적을 수 있다. 다국어 문서의 시소러스가 크기 때문에 다국어 토큰화기의 겹침이 영어 토큰화기의 겹침보다 낮습니다.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">33k</th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">50k</th>
<th id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">82k</th>
<th id="S4.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">100k</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<th id="S4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">English</th>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.77</td>
<td id="S4.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.76</td>
<td id="S4.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.74</td>
<td id="S4.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.74</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<th id="S4.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Multilingual</th>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">0.62</td>
<td id="S4.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.62</td>
<td id="S4.T1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">0.62</td>
<td id="S4.T1.2.3.2.5" class="ltx_td ltx_align_center ltx_border_bb">0.61</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>:</span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Vocabulary overlap between the HuggingFace and SentencePiece BPE tokenizer for different vocab size. </span></figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x4.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf1.2.1.1" style="font-size:90%;">(a)</span></span><span class="ltx_text" id="S4.F3.sf1.3.2" style="font-size:90%;">Non-English documents</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x5.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">English documents</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x6.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">German documents</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">그림 3</span>:</span><span class="ltx_text" id="S4.F3.4.2" style="font-size:90%;">Average compute (GFLOPs) required to processing a single word within a full <span class="ltx_text ltx_font_bold" id="S4.F3.4.2.1">training</span> pass (backward pass 포함). </span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Extrinsic Tokenizer Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">다음에서는 토큰라이저의 외부 평가 결과를 설명한다. <a class="ltx_ref" href="#S5.SS1" title="5.1 Experimental Setup ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>는 실험 설정을 설명하고, <a class="ltx_ref" href="#S5.SS2" title="5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>는 조사된 토큰라이저를 기반으로 훈련된 모델의 다운스트림 성능을 제시하고, <a class="ltx_ref" href="#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>는 특정 모델에 사용될 때 각 토큰라이저와 관련된 계산 비용을 분석한다.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.1">모델 다운스트림 성능에 대한 토큰라이저의 영향을 평가하기 위해 각 토큰라이저에 대해 크기 2.6 B의 디코더 전용 변압기 모델을 훈련했다. 우리는 인과 언어 모델링 훈련 목표를 기반으로 Hoffman <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">Hoffmann et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022b</a>)</cite>에서 제안한 스케일링 법칙에 따라 52.6 B 토큰에 대한 모델을 훈련했다. 하이퍼-파라미터들은 <a class="ltx_ref" href="#A3.T8" title="In Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>에서 <a class="ltx_ref" href="#A3" title="Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>로 설명된다.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p2.1">제시된 모델들은 일반적인 추론(HellaSwag <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a class="ltx_ref" href="#bib.bib51" title="">2019</a>)</cite>, WinoGrande <cite class="ltx_cite ltx_citemacro_cite">Sakaguchi et al. (<a class="ltx_ref" href="#bib.bib37" title="">2020</a>)</cite>, ARC easy and challenge <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="#bib.bib9" title="">2018</a>)</cite>), 세계 지식(TriviaQA <cite class="ltx_cite ltx_citemacro_cite">Joshi et al. (<a class="ltx_ref" href="#bib.bib24" title="">2017</a>)</cite>), 독해(LAMBADA <cite class="ltx_cite ltx_citemacro_cite">Paperno et al. (<a class="ltx_ref" href="#bib.bib33" title="">2016</a>)</cite>) 및 독일어, 스페인어, 프랑스어, 이탈리아어에 대한 기계 번역 버전, RACE <cite class="ltx_cite ltx_citemacro_cite">Lai et al. (<a class="ltx_ref" href="#bib.bib28" title="">2017</a>)</cite>, BoolQ <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="#bib.bib7" title="">2019</a>)</cite>), 수학(MATH <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="#bib.bib20" title="">2021</a>)</cite>), 다국어 자연어 추론(XNLI <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="#bib.bib11" title="">2018</a>)</cite>)에 대해 제로샷 설정으로 평가하였다.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Downstream Performance</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p1.1">다운스트림 성능에 대한 분석을 여러 부분으로 나눕니다. 먼저, 조사된 토큰라이저에 대해 얻은 전반적인 결과에 대해 논의한 후, 토큰라이저 라이브러리의 영향(Section <a class="ltx_ref" href="#S5.SS2.SSS1" title="5.2.1 Impact of the Tokenizer Library ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>), 토큰라이저 알고리즘의 영향(Section <a class="ltx_ref" href="#S5.SS2.SSS2" title="5.2.2 Impact of the Tokenizer Algorithm ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>), 어휘 크기의 영향(Section <a class="ltx_ref" href="#S5.SS2.SSS3" title="5.2.3 Impact of the Tokenizer Vocabulary ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">5.2.3</span></a>)을 제시한다.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x7.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf1.2.1.1" style="font-size:90%;">(a)</span></span><span><span class="ltx_text" id="S5.F4.sf1.3.2" style="font-size:90%;">Monolingual tokenizers. </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x8.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.sf2.2.1.1" style="font-size:90%;">(b)</span></span><span><span class="ltx_text" id="S5.F4.sf2.3.2" style="font-size:90%;">Multilingual tokenizers. </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">그림 4</span>:</span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">Top-1/-2/-3 count for mono-/multilingual tasks across mono-/multilingual tokenizers, the tasks were evaluated on accuracy metric. </span></figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T2.2.1.1.2.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.3.1" class="ltx_text" style="font-size:80%;">DE</span></th>
<th id="S5.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.4.1" class="ltx_text" style="font-size:80%;">FR</span></th>
<th id="S5.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.5.1" class="ltx_text" style="font-size:80%;">IT</span></th>
<th id="S5.T2.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.6.1" class="ltx_text" style="font-size:80%;">ES</span></th>
<th id="S5.T2.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.2.1.1.7.1" class="ltx_text" style="font-size:80%;">EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<th id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T2.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:11.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:11.4pt;transform:translate(-2.99pt,-2.99pt) rotate(-90deg) ;">
<span id="S5.T2.2.2.1.1.1.1.1" class="ltx_p">EN</span>
</span></span></span></th>
<th id="S5.T2.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.2.2.1.2.1" class="ltx_text" style="font-size:80%;">BPE-HF</span></th>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.1.7.1" class="ltx_text" style="font-size:80%;">45.44</span></td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr">
<th id="S5.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.2.3.2.1.1" class="ltx_text" style="font-size:80%;">BPE-SP</span></th>
<td id="S5.T2.2.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.3.2.6" class="ltx_td ltx_align_center"><span id="S5.T2.2.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">46.31</span></td>
</tr>
<tr id="S5.T2.2.4.3" class="ltx_tr">
<th id="S5.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.2.4.3.1.1" class="ltx_text" style="font-size:80%;">UNI-SP</span></th>
<td id="S5.T2.2.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T2.2.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T2.2.4.3.6.1" class="ltx_text" style="font-size:80%;">46.01</span></td>
</tr>
<tr id="S5.T2.2.5.4" class="ltx_tr">
<th id="S5.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S5.T2.2.5.4.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T2.2.5.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:26.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-10.43pt,-10.43pt) rotate(-90deg) ;">
<span id="S5.T2.2.5.4.1.1.1.1" class="ltx_p">MULTI</span>
</span></span></span></th>
<th id="S5.T2.2.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T2.2.5.4.2.1" class="ltx_text" style="font-size:80%;">BPE-HF</span></th>
<td id="S5.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.3.1" class="ltx_text" style="font-size:80%;">36.72</span></td>
<td id="S5.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.4.1" class="ltx_text" style="font-size:80%;">36.55</span></td>
<td id="S5.T2.2.5.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.5.1" class="ltx_text" style="font-size:80%;">35.34</span></td>
<td id="S5.T2.2.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.6.1" class="ltx_text" style="font-size:80%;">42.28</span></td>
<td id="S5.T2.2.5.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.5.4.7.1" class="ltx_text" style="font-size:80%;">44.45</span></td>
</tr>
<tr id="S5.T2.2.6.5" class="ltx_tr">
<th id="S5.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.2.6.5.1.1" class="ltx_text" style="font-size:80%;">BPE-SP</span></th>
<td id="S5.T2.2.6.5.2" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.2.1" class="ltx_text" style="font-size:80%;">36.96</span></td>
<td id="S5.T2.2.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.3.1" class="ltx_text" style="font-size:80%;">36.95</span></td>
<td id="S5.T2.2.6.5.4" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36.12</span></td>
<td id="S5.T2.2.6.5.5" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.5.1" class="ltx_text" style="font-size:80%;">41.96</span></td>
<td id="S5.T2.2.6.5.6" class="ltx_td ltx_align_center"><span id="S5.T2.2.6.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">44.92</span></td>
</tr>
<tr id="S5.T2.2.7.6" class="ltx_tr">
<th id="S5.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T2.2.7.6.1.1" class="ltx_text" style="font-size:80%;">UNI-SP</span></th>
<td id="S5.T2.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.38</span></td>
<td id="S5.T2.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.05</span></td>
<td id="S5.T2.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.4.1" class="ltx_text" style="font-size:80%;">35.70</span></td>
<td id="S5.T2.2.7.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.56</span></td>
<td id="S5.T2.2.7.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.2.7.6.6.1" class="ltx_text" style="font-size:80%;">44.21</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.5.1.1" style="font-size:113%;">Table 2</span>:</span><span class="ltx_text" id="S5.T2.6.2" style="font-size:113%;">Impact of the tokenizer algorithm and the tokenizer training library, scores are averaged over the vocabulary size. </span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.2.1.1.2.1" class="ltx_text" style="font-size:80%;">Vocab</span></th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.3.1" class="ltx_text" style="font-size:80%;">DE</span></th>
<th id="S5.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.4.1" class="ltx_text" style="font-size:80%;">FR</span></th>
<th id="S5.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.5.1" class="ltx_text" style="font-size:80%;">IT</span></th>
<th id="S5.T3.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.6.1" class="ltx_text" style="font-size:80%;">ES</span></th>
<th id="S5.T3.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.1.1.7.1" class="ltx_text" style="font-size:80%;">EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S5.T3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T3.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:11.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:11.4pt;transform:translate(-2.99pt,-2.99pt) rotate(-90deg) ;">
<span id="S5.T3.2.2.1.1.1.1.1" class="ltx_p">EN</span>
</span></span></span></th>
<th id="S5.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.2.2.1.2.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">46.34</span></td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.3.2.1.1" class="ltx_text" style="font-size:80%;">50</span></th>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.3.2.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.3.2.6.1" class="ltx_text" style="font-size:80%;">46.30</span></td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.4.3.1.1" class="ltx_text" style="font-size:80%;">82</span></th>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.4.3.6.1" class="ltx_text" style="font-size:80%;">45.29</span></td>
</tr>
<tr id="S5.T3.2.5.4" class="ltx_tr">
<th id="S5.T3.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.5.4.1.1" class="ltx_text" style="font-size:80%;">100</span></th>
<td id="S5.T3.2.5.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S5.T3.2.5.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.6.1" class="ltx_text" style="font-size:80%;">45.88</span></td>
</tr>
<tr id="S5.T3.2.6.5" class="ltx_tr">
<th id="S5.T3.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="4"><span id="S5.T3.2.6.5.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T3.2.6.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:26.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-10.43pt,-10.43pt) rotate(-90deg) ;">
<span id="S5.T3.2.6.5.1.1.1.1" class="ltx_p">MULTI</span>
</span></span></span></th>
<th id="S5.T3.2.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.2.6.5.2.1" class="ltx_text" style="font-size:80%;">32</span></th>
<td id="S5.T3.2.6.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.3.1" class="ltx_text" style="font-size:80%;">36.72</span></td>
<td id="S5.T3.2.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.4.1" class="ltx_text" style="font-size:80%;">36.22</span></td>
<td id="S5.T3.2.6.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.5.1" class="ltx_text" style="font-size:80%;">35.20</span></td>
<td id="S5.T3.2.6.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.6.1" class="ltx_text" style="font-size:80%;">41.76</span></td>
<td id="S5.T3.2.6.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.2.6.5.7.1" class="ltx_text" style="font-size:80%;">44.71</span></td>
</tr>
<tr id="S5.T3.2.7.6" class="ltx_tr">
<th id="S5.T3.2.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.7.6.1.1" class="ltx_text" style="font-size:80%;">50</span></th>
<td id="S5.T3.2.7.6.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.2.1" class="ltx_text" style="font-size:80%;">36.57</span></td>
<td id="S5.T3.2.7.6.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.3.1" class="ltx_text" style="font-size:80%;">36.34</span></td>
<td id="S5.T3.2.7.6.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.4.1" class="ltx_text" style="font-size:80%;">34.60</span></td>
<td id="S5.T3.2.7.6.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.5.1" class="ltx_text" style="font-size:80%;">42.22</span></td>
<td id="S5.T3.2.7.6.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.6.6.1" class="ltx_text" style="font-size:80%;">43.91</span></td>
</tr>
<tr id="S5.T3.2.8.7" class="ltx_tr">
<th id="S5.T3.2.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T3.2.8.7.1.1" class="ltx_text" style="font-size:80%;">82</span></th>
<td id="S5.T3.2.8.7.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.40</span></td>
<td id="S5.T3.2.8.7.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.3.1" class="ltx_text" style="font-size:80%;">37.22</span></td>
<td id="S5.T3.2.8.7.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36.21</span></td>
<td id="S5.T3.2.8.7.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.5.1" class="ltx_text" style="font-size:80%;">42.30</span></td>
<td id="S5.T3.2.8.7.6" class="ltx_td ltx_align_center"><span id="S5.T3.2.8.7.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">44.75</span></td>
</tr>
<tr id="S5.T3.2.9.8" class="ltx_tr">
<th id="S5.T3.2.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S5.T3.2.9.8.1.1" class="ltx_text" style="font-size:80%;">100</span></th>
<td id="S5.T3.2.9.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.2.1" class="ltx_text" style="font-size:80%;">37.17</span></td>
<td id="S5.T3.2.9.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.45</span></td>
<td id="S5.T3.2.9.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.4.1" class="ltx_text" style="font-size:80%;">36.19</span></td>
<td id="S5.T3.2.9.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">42.64</span></td>
<td id="S5.T3.2.9.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.2.9.8.6.1" class="ltx_text" style="font-size:80%;">44.55</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.5.1.1" style="font-size:113%;">Table 3</span>:</span><span class="ltx_text" id="S5.T3.6.2" style="font-size:113%;">Impact of the vocabulary size on the downstream accuracy. </span></figcaption>
The accuracy scores are averaged over the libraries and tokenizer algorithms</span></figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p2.1">전체적인 성능 개요를 제공하기 위해 다양한 작업을 기반으로 토큰라이저가 모델 성능에 미치는 영향을 평가하고 단일 집계 메트릭만 제공하는 대신 다양한 관점에서 결과를 분석합니다.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p3.1">이 전체론적 평가는 토큰라이저를 수행하는 상위-1/-2/-3 중 토큰라이저가 얼마나 자주 발생하는지를 표현하는 <a class="ltx_ref" href="#S5.F4" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, 작업에 걸친 정확도 점수의 분포를 나타내는 <a class="ltx_ref" href="#A5.F7" title="In Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, 모든 작업에 걸친 평균 성능을 요약하는 <a class="ltx_ref" href="#A5.T9" title="In Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a>에서 제시된다. 우리는 다음 하위 섹션에서 다른 관점에서 결과를 분석한다.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Monolingual Tokenizer</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.2">단일 언어 토큰라이저 중에는 상당한 성능 차이가 있을 수 있다. 예를 들어, <a class="ltx_ref" href="#S5.F4.sf1" title="In Figure 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a>는 GPT-2 tokenizer가 top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation></semantics></math>(with <math alttext="k\in\{1,2,3\}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.2.m2.3"><semantics id="S5.SS2.SSS0.Px1.p1.2.m2.3a"><mrow id="S5.SS2.SSS0.Px1.p1.2.m2.3.4" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.cmml"><mi id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2.cmml">k</mi><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1.cmml">∈</mo><mrow id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml"><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.1" stretchy="false" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S5.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">1</mn><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S5.SS2.SSS0.Px1.p1.2.m2.2.2" xref="S5.SS2.SSS0.Px1.p1.2.m2.2.2.cmml">2</mn><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.3" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S5.SS2.SSS0.Px1.p1.2.m2.3.3" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.3.cmml">3</mn><mo id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2.4" stretchy="false" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.2.m2.3b"><apply id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4"><in id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.1"></in><ci id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.2">𝑘</ci><set id="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.4.3.2"><cn id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1">1</cn><cn id="S5.SS2.SSS0.Px1.p1.2.m2.2.2.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.2.m2.2.2">2</cn><cn id="S5.SS2.SSS0.Px1.p1.2.m2.3.3.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.2.m2.3.3">3</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.2.m2.3c">k\in\{1,2,3\}</annotation></semantics></math>)를 통해 전반적으로 가장 자주 토큰izer를 수행하는 반면, EN-BPE-HF-32 tokenizer는 결코 가장 성능이 좋은 tokenizer가 아님을 나타낸다. 카운트 기반 메트릭은 이미 모델 성능에 대한 첫 번째 개요를 제공하지만 성능 차이를 설명하지는 않습니다.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p2.1">따라서 정확도를 평가 척도로 사용하는 모든 작업에 대한 정확도 분포도 살펴보았다. <a class="ltx_ref" href="#A5.F7.sf1" title="In Figure 7 ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7(a)</span></a>는 EN-BPE-SP-32 tokenizer가 평균적으로 가장 성능이 좋은 tokenizers 중 모든 태스크에 걸쳐 있으며 중앙값이 가장 높다는 것을 보여준다. 동시에, 이것은 또한 EN-BPE-HF-32와 같은 다른 토큰라이저들에 비해 태스크들에 걸쳐 큰 분산을 표현하는데, 이는 태스크들의 세트에 대해, 토큰라이저는 매우 잘 수행되는 반면, 다른 태스크들에 대해, 비교적 잘 수행되지 않는다는 것을 암시한다.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p3.1">집계 메트릭은 전체 성능에 대한 좋은 인상을 제공하지만 단일 작업에 대해 잠재적으로 큰 성능 차이를 보여주지는 않습니다. 따라서 이 작업에서 가장 성능이 좋은 토큰화기와 가장 성능이 나쁜 토큰화기의 모든 작업 간의 성능 차이를 <a class="ltx_ref" href="#A5.T11" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>에 나열했습니다. 다양한 작업에 대해 성능 차이가 크며, 특히 모델 구성이 동일한 상태에서 토큰나이저만 변경되었기 때문에 흥미롭다.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p4" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p4.1">전반적으로 GPT-2 토나이저와 EN-BPE-SP-32 토나이저는 단일 언어 영어 모델에 대한 강력한 토나이저이다. 흥미롭게도 LLaMA2는 SentencePiece의 어휘 크기 32k의 BPE 구현이 사용되었다. 우리의 결과는 두 토큰화자/토큰화자 설정이 단일 언어 영어 모델을 훈련하는 데 합리적인 선택이라는 경험적이고 투명한 증거를 제공한다.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multilingual Tokenizer</h5>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">우리의 다국어 토큰라이저 실험은 다른 토큰라이저가 큰 성능 차이를 표현할 수 있음을 추가로 뒷받침한다. 우리의 카운트 기반 메트릭(그림 <a class="ltx_ref" href="#S5.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">4(b)</span></a>)에 따르면 토큰라이저 간의 차이는 단일 언어 토큰라이저보다 훨씬 크다. Multi-UNI-SP-100 및 Multi-UNI-SP-82가 가장 강력한 토큰라이저 중 하나인 반면, Multi-BPE-HF-32는 최고 성능의 토큰라이저에서 단일 시간일 뿐이다.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p2.1">그림 <a class="ltx_ref" href="#A5.F7.sf2" title="Figure 7(b) ‣ Figure 7 ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">7(b)</span></a>에 묘사된 정확도 분포들은 위에서 언급된 두 토큰아이저들이 모두 가장 성능이 좋은 토큰아이저들 중 하나이며 이미 언급된 토큰아이저와 같은 특정 토큰아이저들이 더 나쁜 성능을 발휘한다는 것을 추가로 보여준다. <a class="ltx_ref" href="#A5.F7.sf2" title="Figure 7(b) ‣ Figure 7 ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">7(b)</span></a>는 GPT-2 tokenizer의 성능이 좋지 않음을 추가적으로 보여준다. 단일 언어 모델과 비교하여 모든 토큰라이저가 작업에 걸쳐 유사한 분산을 표현한다는 것을 관찰할 수 있다.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p3.1">표  <a class="ltx_ref" href="#A5.T11" title="Table 11 ‣ E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">11</span></a>를 참조하면, 단일 태스크를 분석한 결과 다국어 토큰라이저의 경우 태스크 간 성능 차이가 클 수 있음을 알 수 있다.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p4" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p4.1">Multi-UNI-SP-100 및 Multi-UNI-SP-82는 탁월한 선택이지만 사전 훈련된 GPT-2 토큰화기를 사용하면 다중 언어 모델을 훈련하기 위해 <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p4.1.1">omitted</span>이어야 합니다.</p>
</div>
</section>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Impact of the Tokenizer Library</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1"><a class="ltx_ref" href="#S5.T2" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>는 BPE-SP가 평균적으로 모든 태스크에서 BPE-HF보다 우수하다는 것을 보여준다. 다국어 설정에서 BPE-SP는 스페인어를 제외한 모든 언어에서 BPE-HF를 능가한다. 성능 차이는 토큰아이저의 사전 및 사후 처리의 구현 세부 사항의 차이에 기인할 수 있으며, 이는 어휘 생성(<a class="ltx_ref" href="#S4.SS2" title="4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> 참조) 및 결과적으로 다운스트림 성능에 영향을 미칠 수 있다.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Impact of the Tokenizer Algorithm</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1"><a class="ltx_ref" href="#S5.T2" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>는 영어 토큰라이저 및 모델의 경우 BPE가 Unigram에 비해 더 잘 작동함을 보여준다. 다국어 모델 및 토큰라이저의 경우 스페인어, 독일어 및 프랑스어 유니그램의 경우 성능이 향상될 수 있음을 알 수 있다. 독일과 로맨스 언어는 일반적으로 굴절된 언어인 반면 영어는 주로 분석적이지만 일부 응집 기능을 가지고 있다. 최근 연구에 따르면 Unigram 토큰라이저는 이러한 언어에 도움이 될 수 있는 형태학적으로 정확한 세그먼트 <cite class="ltx_cite ltx_citemacro_cite">Bostrom and Durrett (<a class="ltx_ref" href="#bib.bib3" title="">2020</a>); Park et al. (<a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite>를 더 잘 찾을 수 있다.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Impact of the Tokenizer Vocabulary</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">어휘 크기의 영향을 분석한 결과, 단일 언어 설정에서는 작은/중간 크기의 어휘가 더 나은 성능(<a class="ltx_ref" href="#S5.T3" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>)을 보인 반면, 다 언어 설정에서는 큰 어휘 크기가 더 나은 다운스트림 성능(<a class="ltx_ref" href="#S5.T3" title="In 5.2 Downstream Performance ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>)을 보였다. 이는 LLama와 같은 현재 영어 LLM의 어휘 크기와 일치한다.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Computational Costs</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p1.1">동일한 어휘 크기를 갖는 경우에도, 두 개의 트레이닝된 토큰라이저는 상이한 길이의 시퀀스에서 텍스트 통로를 분할할 가능성이 가장 높으며, 이는 모델에 의해 프로세싱될 때 상이한 계산 비용을 초래한다.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p2.1">우리의 분석은 다음 두 가지 통찰력을 보여주었다. 첫째, 어휘 크기의 증가는 계산 비용을 감소시킨다(<a class="ltx_ref" href="#S4.F3" title="In 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>). 이는 낮은 비옥도로 표현되는 전체 토큰 수의 감소(<a class="ltx_ref" href="#S4.F1" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>)가 더 큰 어휘 크기가 모델에서 도입하는 추가 계산 비용을 보상한다는 것을 의미한다. 대조적으로, 추론 동안, 계산 비용은 어휘 크기가 증가함에 따라 증가하고 적어도 50K의 어휘 크기, 즉 역방향 패스가 없는 순방향 패스만이 모델에서 더 큰 어휘 크기에 의해 도입된 추가 계산 비용을 보상하지 않는다.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p3.1">둘째, 다국어 문서에 대한 계산 훈련 비용은 훈련용 단일 언어 영어 토큰아이저(<a class="ltx_ref" href="#S4.F3.sf1" title="In Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(a)</span></a>) 및 추론용(부록 <a class="ltx_ref" href="#A5.F8" title="In Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>)보다 다국어 토큰아이저에 대해 상당히 낮다. 극단적인 경우, 최대 68%의 추가 훈련 비용(부록 <a class="ltx_ref" href="#A5.T10" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a> 및 <a class="ltx_ref" href="#S4.F3.sf3" title="In Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(c)</span></a>의 EN-UNI-SP-100352 대 Multi-UNI-SP-50176 참조) 그것은 훈련의 경우이기 때문이다. 동시에, 영어 문서들을 처리하기 위한 다국어 및 단국어 영어 토큰라이저들 사이의 계산 비용들의 차이는 훨씬 더 낮다(<a class="ltx_ref" href="#S4.F3.sf2" title="In Figure 3 ‣ 4.2 Vocabulary Overlap ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(b)</span></a>). 계산 비용에 대한 완전한 개요는 <a class="ltx_ref" href="#A5.T10" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>에서 찾을 수 있다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Correlation Between Intrinsic And Extrinsic Tokenizer Performance</h2>

<figure id="S6.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x9.png" id="S6.F5.g1" class="ltx_graphics ltx_img_square" width="242" height="242" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F5.2.1.1" style="font-size:90%;">Figure 5</span>:</span><span class="ltx_text" id="S6.F5.3.2" style="font-size:90%;">Correlation of fertility/parity scores and downstream task performance for all five languages. 우리는 영어 과제에 대해 영어 모델을 평가한 반면, 다국어 모델은 네 가지 비영어 과제 모두에서 평가된다. </span></figcaption>
</figure>
<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">이 섹션에서는 고유 토큰izer 메트릭(수정 및 패리티)이 외부 모델 다운스트림 성능에 대한 가능한 예측 관계를 조사합니다.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p" id="S6.p2.1"><a class="ltx_ref" href="#S6.F5" title="In 6 Correlation Between Intrinsic And Extrinsic Tokenizer Performance ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>의 상관 히트맵에서 강조했듯이 모든 작업과 언어에 걸쳐 뚜렷한 상관 관계가 없음을 발견하여 보다 세분화된 분석을 요구한다. 비영어 작업의 경우 주로 저출산과 더 높은 다운스트림 성능 사이의 상관 관계를 관찰하는 반면, 비영어 작업은 겉보기에 무작위적인 양의 상관 관계와 음의 상관 관계를 산출한다.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p" id="S6.p3.1">다양한 어휘 크기(<a class="ltx_ref" href="#S4.F1" title="In 4.1 Fertility &amp; Parity ‣ 4 Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> 참조)에 따른 번식력 추세를 고려하여 특정 언어 특정 어휘 크기 제한에서 번식력은 다운스트림 성능과만 상관관계가 있다고 가정한다. 영어의 경우, 토큰화기는 이미 32k 토큰의 어휘 크기에 대해 낮고 수렴에 가까운 번식력 점수를 제공한다. 추가 토큰은 약간의 번식력 향상만 산출하지만 형태학적 분할을 포착하지 못하여 다운스트림 성능을 해칠 수 있으며 결과적으로 계산 비용(<a class="ltx_ref" href="#S5.SS3" title="5.3 Computational Costs ‣ 5 Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a> 참조)을 크게 증가시킬 수 있다고 가정한다.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p" id="S6.p4.1">대조적으로, 다국어 토큰라이저의 경우 어휘 크기가 증가함에 따라 상당한 번식력 향상을 관찰한다. 추가 언어에 의해 유도된 더 큰 시소러스로 인해, 토큰화기는 모델이 모든 언어에 대해 설득력 있게 수행할 수 있도록 하기 위해 더 큰 어휘를 요구한다. 따라서 비융합 어휘 범위 내에서만 다양한 어휘 크기로 번식력과 다운스트림 성능 사이의 강력하고 부정적인 상관 관계를 달성한다.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p class="ltx_p" id="S6.p5.1">결론적으로, 비옥도 및 패리티와 같은 고유 토큰나이저 메트릭은 가감하여 취할 필요가 있으며 추정컨대 특정 범위에서 다운스트림 모델 성능을 예측할 뿐이다. 또한, 저출산 점수는 필요한 기준으로 간주할 수 있지만 충분한 기준으로 간주할 수는 없는 것으로 분석되었다.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">이 작업은 토큰화기가 모델의 다운스트림 성능에 미치는 영향을 더 잘 이해하기 위한 기본 단계를 나타낸다. 우리는 언어 전반에 걸쳐 균형 잡힌 점유율을 가진 훈련 토큰라이저가 모든 언어에서 비교할 수 있는 저출산 및 패리티 점수를 달성한다는 것을 보여주었으며, 이는 중요한 의미를 가지고 있다. 낮은 비옥도는 계산 비용을 줄이고 모델이 제한된 컨텍스트 창에서 장거리 종속성을 학습할 수 있게 한다.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p" id="S7.p2.1">또한 토크나이저 선택이 모델의 다운스트림 성능에 상당한 영향을 미칠 수 있음을 강조합니다. 우리는 다국어 모델을 훈련하기 위해 어휘 크기가 큰 유니그램 토큰라이저를 사용하는 것이 유익하다는 것을 보여줄 수 있다. 이것은 BPE 기반 토큰라이저가 더 작은/중간 크기의 어휘 크기와 함께 사용되는 현재의 단일 언어 설정과 반대이다. 이것은 또한 단일 언어 설정에 대해 얻은 통찰력을 다국어 설정으로 전달할 수 없으므로 전용 연구가 필요함을 의미한다.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p" id="S7.p3.1">마지막으로 내재적 토크나이저 성능과 외재적 토크나이저 성능 사이에는 명확한 상관관계가 없지만, 그 상관관계는 오히려 태스크에 따라 다르다는 것을 보여줄 수 있었다.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.p1.1">이 작업은 프로젝트 OpenGPT-X(프로젝트 번호 68GX21007D)를 통해 독일 연방 경제 기후 행동부(BMWK)와 독일 연방 교육 연구부 및 노르트라인 웨스트팔렌 주가 라마르-인스티튜트 포 머신(LAMARR22B)의 일부로 자금을 지원했다. 저자들은 JSC(Jülich Supercomputing Center)의 GCS 슈퍼컴퓨터 JUWELS와 TU Dresden의 고성능 컴퓨팅 센터 [Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH)]에서 높은 처리량 계산을 위한 시설을 제공함으로써 이 프로젝트에 자금을 지원하는 가우스 센터(www.gauss-centre.eu)에 감사한다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Javier&nbsp;Ortiz Suárez, Laurent Romary, and Benoît
Sagot. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14618/ids-pub-10468" title="" class="ltx_ref ltx_href">Ungoliant: An
optimized pipeline for the generation of a very large-scale multilingual web
corpus</a>.

</span>
<span class="ltx_bibblock">In Harald Lüngen, Marc Kupietz, Piotr Bański, Adrien Barbaresi,
Simon Clematide, and Ines Pisetta, editors, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop
on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12
July 2021 (Online-Event)</em>, pages 1 – 9. Leibniz-Institut für Deutsche
Sprache, Mannheim.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. (2000)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 13.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bostrom and Durrett (2020)</span>
<span class="ltx_bibblock">
Kaj Bostrom and Greg Durrett. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.414" title="" class="ltx_ref ltx_href">Byte
pair encoding is suboptimal for language model pretraining</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 4617–4624, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020a)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al. 2020a.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020b)</span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chirkova and Troshin (2022)</span>
<span class="ltx_bibblock">
Nadezhda Chirkova and Sergey Troshin. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rd-G1nO-Jbq" title="" class="ltx_ref ltx_href">CodeBPE:
Investigating subtokenization options for large language model pretraining on
source code</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Deep Learning for Code Workshop</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">NAACL-HLT (1)</em>, pages 2924–2936. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00448" title="" class="ltx_ref ltx_href">Canine: Pre-training an
efficient tokenization-free encoder for language representation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:73–91.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the AI2
reasoning challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1803.05457.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)</span>
<span class="ltx_bibblock">
Together Computer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_href">Redpajama: An open source recipe to reproduce llama training dataset</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel&nbsp;R.
Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock">XNLI: evaluating cross-lingual sentence representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, pages 2475–2485. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gage (1994)</span>
<span class="ltx_bibblock">
Philip Gage. 1994.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:59804030" title="" class="ltx_ref ltx_href">A new
algorithm for data compression</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The C Users Journal archive</em>, 12:23–38.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020a)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy. 2020a.

</span>
<span class="ltx_bibblock">The Pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020b)</span>
<span class="ltx_bibblock">
Yingqiang Gao, Nikola&nbsp;I. Nikolov, Yuhuang Hu, and Richard&nbsp;H.R. Hahnloser.
2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.145" title="" class="ltx_ref ltx_href">Character-level translation with self-attention</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1591–1604, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and
Angela Fan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00474" title="" class="ltx_ref ltx_href">The Flores-101
Evaluation Benchmark for Low-Resource and Multilingual Machine Translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:522–538.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Johannes Graën, Tannon Kew, Anastassia Shaitarova, and Martin Volk. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14618/ids-pub-9020" title="" class="ltx_ref ltx_href">Modelling large
parallel corpora: The zurich parallel corpus collection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Workshop on Challenges in the
Management of Large Corpora (CMLC)</em>, pages 1–8. Leibniz-Institut für
Deutsche Sprache.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graën et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
J.&nbsp;Graën, D.&nbsp;Batinic, and M.&nbsp;Volk. 2014.

</span>
<span class="ltx_bibblock">Cleaning the Europarl corpus for linguistic applications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Konvens 2014</em>. Stiftung Universität Hildesheim.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hajlaoui et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
Najeh Hajlaoui, David Kolovratnik, Jaakko Vaeyrynen, Ralf Steinberger, and
Dániel Varga. 2014.

</span>
<span class="ltx_bibblock">DCEP - Digital corpus of the European parliament.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. LREC 2014 (Language Resources and Evaluation
Conference). Reykjavik, Iceland</em>, pages 3164–3171.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
Tang, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the MATH dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">NeurIPS Datasets and Benchmarks</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George
van&nbsp;den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén
Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre.
2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">An empirical analysis of compute-optimal large language model training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume&nbsp;35, pages 30016–30030. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las&nbsp;Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van&nbsp;den
Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
Elsen, Jack&nbsp;W. Rae, Oriol Vinyals, and Laurent Sifre. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2203.15556" title="" class="ltx_ref ltx_href">Training
compute-optimal large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2203.15556.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Höfler and Piotrowski (2011)</span>
<span class="ltx_bibblock">
Stefan Höfler and Michael Piotrowski. 2011.

</span>
<span class="ltx_bibblock">Building corpora for the philological study of Swiss legal texts.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Journal for Language Technology and Computational Linguistics</em>,
26(2):77–89.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel&nbsp;S. Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>, pages 1601–1611. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
P.&nbsp;Koehn. 2005.

</span>
<span class="ltx_bibblock">Europarl: A parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Machine Translation Summit, volume 5</em>, pages 79––86.
Asia-Pacific Association for Machine Translation (AAMT).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo (2018)</span>
<span class="ltx_bibblock">
Taku Kudo. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1007" title="" class="ltx_ref ltx_href">Subword regularization:
Improving neural network translation models with multiple subword
candidates</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 66–75,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">EMNLP 2018</em>, page&nbsp;66.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard&nbsp;H. Hovy. 2017.

</span>
<span class="ltx_bibblock">RACE: large-scale reading comprehension dataset from examinations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, pages 785–794. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 9019–9052.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)</span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock">OpenSubtitles2016: Extracting large parallel corpora from movie
and tv subtitles.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on Language
Resources and Evaluation (LREC-2016)</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moi and Patry (2023)</span>
<span class="ltx_bibblock">
Anthony Moi and Nicolas Patry. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/tokenizers" title="" class="ltx_ref ltx_href">HuggingFace’s
Tokenizers</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3458817.3476209" title="" class="ltx_ref ltx_href">Efficient
large-scale language model training on gpu clusters using megatron-lm</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis</em>, SC ’21, New York,
NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham,
Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
Fernández. 2016.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse
context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>. The Association for Computer Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Hyunji&nbsp;Hayley Park, Katherine&nbsp;J. Zhang, Coleman Haley, Kenneth Steimel, Han
Liu, and Lane Schwartz. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00365" title="" class="ltx_ref ltx_href">Morphology Matters: A
Multilingual Language Modeling Analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:261–276.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La&nbsp;Malfa, Philip&nbsp;HS Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15425</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna
Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.243" title="" class="ltx_ref ltx_href">How good is
your tokenizer? on the monolingual performance of multilingual language
models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3118–3135,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, pages 8732–8740. AAAI Press.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic,
Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni,
François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush,
Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang,
Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh,
Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin
Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit
Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris
Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa
Adelani, and et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2211.05100" title="" class="ltx_ref ltx_href">BLOOM: A
176b-parameter open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.05100.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster and Nakajima (2012)</span>
<span class="ltx_bibblock">
Mike Schuster and Kaisuke Nakajima. 2012.

</span>
<span class="ltx_bibblock">Japanese and korean voice search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">2012 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pages 5149–5152. IEEE.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et&nbsp;al. (2015)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.07909</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov,
Anastasia Kozlova, and Tatiana Shavrina. 2022.

</span>
<span class="ltx_bibblock">mgpt: Few-shot learners go multilingual.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.07580</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stollenwerk (2023)</span>
<span class="ltx_bibblock">
Felix Stollenwerk. 2023.

</span>
<span class="ltx_bibblock">Training and evaluation of a multilingual tokenizer for gpt-sw3.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.14780</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Vinh&nbsp;Q Tran, Sebastian Ruder, Jai Gupta, Hyung&nbsp;Won Chung, Dara Bahri,
Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021.

</span>
<span class="ltx_bibblock">Charformer: Fast character transformers via gradient-based subword
tokenization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toraman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Cagri Toraman, Eyup&nbsp;Halit Yilmaz, Furkan Sahinuc, and Oguzhan Ozcelik. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3578707" title="" class="ltx_ref ltx_href">Impact of tokenization on
language models: An analysis for turkish</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Asian Low-Resour. Lang. Inf. Process.</em>, 22(4).

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: open and efficient foundation language models, 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">URL https://arxiv. org/abs/2302.13971</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.09288.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, pages 5998–6008.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v34i05.6451" title="" class="ltx_ref ltx_href">Neural machine
translation with byte-level subwords</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
34(05):9154–9160.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir
Kale, Adam Roberts, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:291–306.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer,
and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock">Megabyte: Predicting million-byte sequences with multiscale
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07185</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>, pages 4791–4800. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek,
Mohit Bansal, and Francisco Guzman. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.amta-research.8" title="" class="ltx_ref ltx_href">How robust is
neural machine translation to language imbalance in multilingual tokenizer
training?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th biennial conference of the
Association for Machine Translation in the Americas (Volume 1: Research
Track)</em>, pages 97–116, Orlando, USA. Association for Machine Translation in
the Americas.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Corpora</h2>

<figure id="A1.T4" class="ltx_table">
<table id="A1.T4.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.2.1.1" class="ltx_tr">
<td id="A1.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Name</td>
<td id="A1.T4.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Language</td>
<td id="A1.T4.2.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">#Words</td>
</tr>
<tr id="A1.T4.2.2.2" class="ltx_tr">
<td id="A1.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Oscar</td>
<td id="A1.T4.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.2.2.3" class="ltx_td ltx_align_right ltx_border_t">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.3.3" class="ltx_tr">
<td id="A1.T4.2.3.3.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.3.3.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.3.3.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.4.4" class="ltx_tr">
<td id="A1.T4.2.4.4.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.4.4.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.4.4.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.5.5" class="ltx_tr">
<td id="A1.T4.2.5.5.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.5.5.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.5.5.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.6.6" class="ltx_tr">
<td id="A1.T4.2.6.6.1" class="ltx_td ltx_align_left">Oscar</td>
<td id="A1.T4.2.6.6.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.6.6.3" class="ltx_td ltx_align_right">11.200.000.000</td>
</tr>
<tr id="A1.T4.2.7.7" class="ltx_tr">
<td id="A1.T4.2.7.7.1" class="ltx_td ltx_align_left ltx_border_t">Pile</td>
<td id="A1.T4.2.7.7.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.7.7.3" class="ltx_td ltx_align_right ltx_border_t">13.838.432</td>
</tr>
<tr id="A1.T4.2.8.8" class="ltx_tr">
<td id="A1.T4.2.8.8.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.8.8.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.8.8.3" class="ltx_td ltx_align_right">21.990.512</td>
</tr>
<tr id="A1.T4.2.9.9" class="ltx_tr">
<td id="A1.T4.2.9.9.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.9.9.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.9.9.3" class="ltx_td ltx_align_right">4.334.313.669</td>
</tr>
<tr id="A1.T4.2.10.10" class="ltx_tr">
<td id="A1.T4.2.10.10.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.10.10.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.10.10.3" class="ltx_td ltx_align_right">7.946.402</td>
</tr>
<tr id="A1.T4.2.11.11" class="ltx_tr">
<td id="A1.T4.2.11.11.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T4.2.11.11.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.11.11.3" class="ltx_td ltx_align_right">15.857.811</td>
</tr>
<tr id="A1.T4.2.12.12" class="ltx_tr">
<td id="A1.T4.2.12.12.1" class="ltx_td ltx_align_left ltx_border_t">RedPajama</td>
<td id="A1.T4.2.12.12.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.12.12.3" class="ltx_td ltx_align_right ltx_border_t">143.907.461</td>
</tr>
<tr id="A1.T4.2.13.13" class="ltx_tr">
<td id="A1.T4.2.13.13.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.13.13.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.13.13.3" class="ltx_td ltx_align_right">112.950.000</td>
</tr>
<tr id="A1.T4.2.14.14" class="ltx_tr">
<td id="A1.T4.2.14.14.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.14.14.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.14.14.3" class="ltx_td ltx_align_right">4.663.646.781</td>
</tr>
<tr id="A1.T4.2.15.15" class="ltx_tr">
<td id="A1.T4.2.15.15.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.15.15.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.15.15.3" class="ltx_td ltx_align_right">137.802.711</td>
</tr>
<tr id="A1.T4.2.16.16" class="ltx_tr">
<td id="A1.T4.2.16.16.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.16.16.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.16.16.3" class="ltx_td ltx_align_right">139.749.147</td>
</tr>
<tr id="A1.T4.2.17.17" class="ltx_tr">
<td id="A1.T4.2.17.17.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T4.2.17.17.2" class="ltx_td ltx_align_left">Code</td>
<td id="A1.T4.2.17.17.3" class="ltx_td ltx_align_right">2.052.228.788</td>
</tr>
<tr id="A1.T4.2.18.18" class="ltx_tr">
<td id="A1.T4.2.18.18.1" class="ltx_td ltx_align_left ltx_border_t">Misc</td>
<td id="A1.T4.2.18.18.2" class="ltx_td ltx_align_left ltx_border_t">DE</td>
<td id="A1.T4.2.18.18.3" class="ltx_td ltx_align_right ltx_border_t">600.844.912</td>
</tr>
<tr id="A1.T4.2.19.19" class="ltx_tr">
<td id="A1.T4.2.19.19.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.19.19.2" class="ltx_td ltx_align_left">ES</td>
<td id="A1.T4.2.19.19.3" class="ltx_td ltx_align_right">186.934.269</td>
</tr>
<tr id="A1.T4.2.20.20" class="ltx_tr">
<td id="A1.T4.2.20.20.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.20.20.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T4.2.20.20.3" class="ltx_td ltx_align_right">1.337.030.904</td>
</tr>
<tr id="A1.T4.2.21.21" class="ltx_tr">
<td id="A1.T4.2.21.21.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.21.21.2" class="ltx_td ltx_align_left">IT</td>
<td id="A1.T4.2.21.21.3" class="ltx_td ltx_align_right">19.810.753</td>
</tr>
<tr id="A1.T4.2.22.22" class="ltx_tr">
<td id="A1.T4.2.22.22.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T4.2.22.22.2" class="ltx_td ltx_align_left">FR</td>
<td id="A1.T4.2.22.22.3" class="ltx_td ltx_align_right">211.147.445</td>
</tr>
<tr id="A1.T4.2.23.23" class="ltx_tr">
<td id="A1.T4.2.23.23.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Total</td>
<td id="A1.T4.2.23.23.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="A1.T4.2.23.23.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T4.3.1.1" style="font-size:90%;">Table 4</span>:</span><span class="ltx_text" id="A1.T4.4.2" style="font-size:90%;">Overview of the multilingual 70B words dataset with language, number of sampled words</span></figcaption>
</figure>
<figure id="A1.T5" class="ltx_table">
<table id="A1.T5.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T5.2.1.1" class="ltx_tr">
<td id="A1.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Name</td>
<td id="A1.T5.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Language</td>
<td id="A1.T5.2.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"># words</td>
</tr>
<tr id="A1.T5.2.2.2" class="ltx_tr">
<td id="A1.T5.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Oscar</td>
<td id="A1.T5.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">EN</td>
<td id="A1.T5.2.2.2.3" class="ltx_td ltx_align_right ltx_border_t">56.000.000.000</td>
</tr>
<tr id="A1.T5.2.3.3" class="ltx_tr">
<td id="A1.T5.2.3.3.1" class="ltx_td ltx_align_left">Pile</td>
<td id="A1.T5.2.3.3.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T5.2.3.3.3" class="ltx_td ltx_align_right">4.893.724.288</td>
</tr>
<tr id="A1.T5.2.4.4" class="ltx_tr">
<td id="A1.T5.2.4.4.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T5.2.4.4.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T5.2.4.4.3" class="ltx_td ltx_align_right">5.308.974.750</td>
</tr>
<tr id="A1.T5.2.5.5" class="ltx_tr">
<td id="A1.T5.2.5.5.1" class="ltx_td ltx_align_left">RedPajama</td>
<td id="A1.T5.2.5.5.2" class="ltx_td ltx_align_left">Code</td>
<td id="A1.T5.2.5.5.3" class="ltx_td ltx_align_right">2.299.301.635</td>
</tr>
<tr id="A1.T5.2.6.6" class="ltx_tr">
<td id="A1.T5.2.6.6.1" class="ltx_td ltx_align_left">Misc</td>
<td id="A1.T5.2.6.6.2" class="ltx_td ltx_align_left">EN</td>
<td id="A1.T5.2.6.6.3" class="ltx_td ltx_align_right">1.497.999.327</td>
</tr>
<tr id="A1.T5.2.7.7" class="ltx_tr">
<td id="A1.T5.2.7.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt">Total</td>
<td id="A1.T5.2.7.7.2" class="ltx_td ltx_border_bb ltx_border_tt"></td>
<td id="A1.T5.2.7.7.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt">70.000.000.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T5.3.1.1" style="font-size:90%;">Table 5</span>:</span><span class="ltx_text" id="A1.T5.4.2" style="font-size:90%;">Overview of the English 70B words dataset with language, number of sampled words</span></figcaption>
</figure>
<div id="A1.p1" class="ltx_para">
<p class="ltx_p" id="A1.p1.1">말뭉치 내 웹 문서는 Oscars<span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://oscar-project.org/" target="_blank" title="">https://oscar-project.org/</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">Abadji et al. (<a class="ltx_ref" href="#bib.bib1" title="">2021</a>)</cite>로 구성되어 있으며, 이는 3개의 Common Crawl WET Archives (2022-27, 2022-49 및 2023-14)를 기반으로 ungoliant 파이프라인 <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/oscar-project/ungoliant" target="_blank" title="">https://github.com/oscar-project/ungoliant</a></span></span></span>에 의해 생성되었다.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p class="ltx_p" id="A1.p2.1">큐레이팅된 데이터셋은 <span class="ltx_text ltx_font_italic" id="A1.p2.1.1">The Pile</span> <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib14" title="">2020a</a>)</cite>, <span class="ltx_text ltx_font_italic" id="A1.p2.1.2">RedPajama</span> <cite class="ltx_cite ltx_citemacro_cite">Computer (<a class="ltx_ref" href="#bib.bib10" title="">2023</a>)</cite>, 컬렉션에 속하지 않는 단일 데이터셋으로 구성된다. Pile subcorpora에서 Phil Archive, PMC Abstracts, PMC Extracts, OpenWebText, NIH Exporterm, Free Law Opinions V2를 선택하였다. RedPajama에서 우리는 ArXiv, Books, Github, StackExchange, Wikipedia를 사용한다.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p class="ltx_p" id="A1.p3.1">나머지 데이터 세트는 다음과 같습니다.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<table id="A1.T6.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T6.8.9.1" class="ltx_tr">
<th id="A1.T6.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Hyper-Parameter</th>
<td id="A1.T6.8.9.1.2" class="ltx_td ltx_align_left ltx_border_tt">Value(s)</td>
</tr>
<tr id="A1.T6.1.1" class="ltx_tr">
<th id="A1.T6.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">model_type</th>
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_t">Unigram <math id="A1.T6.1.1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.1.1.1.m1.1a"><mo fence="false" stretchy="false" id="A1.T6.1.1.1.m1.1.1" xref="A1.T6.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.m1.1b"><ci id="A1.T6.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.m1.1c">|</annotation></semantics></math> BPE</td>
</tr>
<tr id="A1.T6.4.4" class="ltx_tr">
<th id="A1.T6.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">vocab_size</th>
<td id="A1.T6.4.4.3" class="ltx_td ltx_align_left">33k <math id="A1.T6.2.2.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.2.2.1.m1.1a"><mo fence="false" stretchy="false" id="A1.T6.2.2.1.m1.1.1" xref="A1.T6.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.2.2.1.m1.1b"><ci id="A1.T6.2.2.1.m1.1.1.cmml" xref="A1.T6.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.2.2.1.m1.1c">|</annotation></semantics></math>50k <math id="A1.T6.3.3.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.3.3.2.m2.1a"><mo fence="false" stretchy="false" id="A1.T6.3.3.2.m2.1.1" xref="A1.T6.3.3.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.3.3.2.m2.1b"><ci id="A1.T6.3.3.2.m2.1.1.cmml" xref="A1.T6.3.3.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.3.3.2.m2.1c">|</annotation></semantics></math> 82k <math id="A1.T6.4.4.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T6.4.4.3.m3.1a"><mo fence="false" stretchy="false" id="A1.T6.4.4.3.m3.1.1" xref="A1.T6.4.4.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T6.4.4.3.m3.1b"><ci id="A1.T6.4.4.3.m3.1.1.cmml" xref="A1.T6.4.4.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.4.4.3.m3.1c">|</annotation></semantics></math> 100k</td>
</tr>
<tr id="A1.T6.8.10.2" class="ltx_tr">
<th id="A1.T6.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">character_coverage</th>
<td id="A1.T6.8.10.2.2" class="ltx_td ltx_align_left">0.9999</td>
</tr>
<tr id="A1.T6.8.11.3" class="ltx_tr">
<th id="A1.T6.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">split_by_number</th>
<td id="A1.T6.8.11.3.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.12.4" class="ltx_tr">
<th id="A1.T6.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">allow_whitespace_only_pieces</th>
<td id="A1.T6.8.12.4.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.13.5" class="ltx_tr">
<th id="A1.T6.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">add_dummy_prefix</th>
<td id="A1.T6.8.13.5.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.6.6" class="ltx_tr">
<th id="A1.T6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">user_defined_symbols_1</th>
<td id="A1.T6.6.6.2" class="ltx_td ltx_align_left">&lt;s&gt;, &lt;/s&gt;,&lt;pad&gt;,&lt;eod&gt;</td>
</tr>
<tr id="A1.T6.8.8" class="ltx_tr">
<th id="A1.T6.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">user_defined_symbols_2</th>
<td id="A1.T6.8.8.2" class="ltx_td ltx_align_left">…, &lt;placeholder_255&gt;</td>
</tr>
<tr id="A1.T6.8.14.6" class="ltx_tr">
<th id="A1.T6.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">byte_fallback</th>
<td id="A1.T6.8.14.6.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.15.7" class="ltx_tr">
<th id="A1.T6.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">max_sentence_length</th>
<td id="A1.T6.8.15.7.2" class="ltx_td ltx_align_left">4192</td>
</tr>
<tr id="A1.T6.8.16.8" class="ltx_tr">
<th id="A1.T6.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">normalization_rule_name</th>
<td id="A1.T6.8.16.8.2" class="ltx_td ltx_align_left">NFKC</td>
</tr>
<tr id="A1.T6.8.17.9" class="ltx_tr">
<th id="A1.T6.8.17.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">train_extremely_large_corpus</th>
<td id="A1.T6.8.17.9.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T6.8.18.10" class="ltx_tr">
<th id="A1.T6.8.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">remove_extra_whitespaces</th>
<td id="A1.T6.8.18.10.2" class="ltx_td ltx_align_left">False</td>
</tr>
<tr id="A1.T6.8.19.11" class="ltx_tr">
<th id="A1.T6.8.19.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">split_by_whitespace</th>
<td id="A1.T6.8.19.11.2" class="ltx_td ltx_align_left ltx_border_bb">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T6.10.1.1" style="font-size:90%;">Table 6</span>:</span><span class="ltx_text" id="A1.T6.11.2" style="font-size:90%;">Overview of the SentencePiece options that we used for the training of our tokenizers. </span></figcaption>
</figure>
<figure id="A1.T7" class="ltx_table">
<table id="A1.T7.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.3.4.1" class="ltx_tr">
<th id="A1.T7.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Hyper-Parameter</th>
<td id="A1.T7.3.4.1.2" class="ltx_td ltx_align_left ltx_border_tt">Value(s)</td>
</tr>
<tr id="A1.T7.3.5.2" class="ltx_tr">
<th id="A1.T7.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">model_type</th>
<td id="A1.T7.3.5.2.2" class="ltx_td ltx_align_left ltx_border_t">BPE</td>
</tr>
<tr id="A1.T7.3.3" class="ltx_tr">
<th id="A1.T7.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">vocab_size</th>
<td id="A1.T7.3.3.3" class="ltx_td ltx_align_left">33k <math id="A1.T7.1.1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T7.1.1.1.m1.1a"><mo fence="false" stretchy="false" id="A1.T7.1.1.1.m1.1.1" xref="A1.T7.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.m1.1b"><ci id="A1.T7.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.m1.1c">|</annotation></semantics></math>50k <math id="A1.T7.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T7.2.2.2.m2.1a"><mo fence="false" stretchy="false" id="A1.T7.2.2.2.m2.1.1" xref="A1.T7.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.m2.1b"><ci id="A1.T7.2.2.2.m2.1.1.cmml" xref="A1.T7.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.m2.1c">|</annotation></semantics></math> 82k <math id="A1.T7.3.3.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="A1.T7.3.3.3.m3.1a"><mo fence="false" stretchy="false" id="A1.T7.3.3.3.m3.1.1" xref="A1.T7.3.3.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="A1.T7.3.3.3.m3.1b"><ci id="A1.T7.3.3.3.m3.1.1.cmml" xref="A1.T7.3.3.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.3.3.m3.1c">|</annotation></semantics></math> 100k</td>
</tr>
<tr id="A1.T7.3.6.3" class="ltx_tr">
<th id="A1.T7.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">limit_alphabet</th>
<td id="A1.T7.3.6.3.2" class="ltx_td ltx_align_left">512</td>
</tr>
<tr id="A1.T7.3.7.4" class="ltx_tr">
<th id="A1.T7.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">use_nfkc_normalizer</th>
<td id="A1.T7.3.7.4.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T7.3.8.5" class="ltx_tr">
<th id="A1.T7.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">use_lowercase_normalizer</th>
<td id="A1.T7.3.8.5.2" class="ltx_td ltx_align_left">False</td>
</tr>
<tr id="A1.T7.3.9.6" class="ltx_tr">
<th id="A1.T7.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">use_strip_accents_normalizer</th>
<td id="A1.T7.3.9.6.2" class="ltx_td ltx_align_left">True</td>
</tr>
<tr id="A1.T7.3.10.7" class="ltx_tr">
<th id="A1.T7.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">pre_tokenizer</th>
<td id="A1.T7.3.10.7.2" class="ltx_td ltx_align_left ltx_border_bb">ByteLevel, Digits</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T7.5.1.1" style="font-size:90%;">Table 7</span>:</span><span class="ltx_text" id="A1.T7.6.2" style="font-size:90%;">Overview of the Sentencepiece Options that we used for the training of our tokenizers. </span></figcaption>
</figure>
<div id="A1.p4" class="ltx_para">
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i1.p1.1">모든 뉴스 V2.0<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://metatext.io/datasets/all-the-news-2.0" target="_blank" title="">https://metatext.io/datasets/all-the-news-2.0</a></span></span></span>은 2016년 1월부터 2020년 4월 1일까지 26개 이상의 다른 출판물에서 크롤링된 신문 기사의 코퍼스이다.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i2.p1.1">Bundestag - Plenarprotokolle<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bundestag.de/dokumente/protokolle/plenarprotokolle" target="_blank" title="">https://www.bundestag.de/dokumente/protokolle/plenarprotokolle</a></span></span></span>은 독일 연방 하원의 세션의 전사체로 구성됩니다.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i3.p1.1">Bundesgerichtshof - Entscheidungen<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html" target="_blank" title="">https://www.bundesgerichtshof.de/DE/Entscheidungen/entscheidungen_node.html</a></span></span></span>은 독일 연방 법원의 판결 모음입니다.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i4.p1.1">CoStEP<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/costep/start" target="_blank" title="">https://pub.cl.uzh.ch/wiki/public/costep/start</a></span></span></span>은 EuroParl corpus<cite class="ltx_cite ltx_citemacro_cite">Graën et al. (<a class="ltx_ref" href="#bib.bib18" title="">2014</a>)</cite>의 정리 및 수정된 버전이다. <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a class="ltx_ref" href="#bib.bib25" title="">2005</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i5.p1.1">DCEP<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en" target="_blank" title="">https://joint-research-centre.ec.europa.eu/language-technology-resources/dcep-digital-corpus-european-parliament_en</a></span></span></span>은 CoStEP에 대한 컴패니언 코퍼스로 유럽의회에서 발행한 문서를 담고 있다. <cite class="ltx_cite ltx_citemacro_cite">Hajlaoui et al. (<a class="ltx_ref" href="#bib.bib19" title="">2014</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i6.p1.1">DNB Dissertations<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html" target="_blank" title="">https://www.dnb.de/DE/Professionell/Services/Dissonline/dissonline_node.html</a></span></span></span>은 Deutsche Nationalbibliothek의 학위 논문 모음이다.</p>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i7.p1.1">MAREC/IREC<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://researchdata.tuwien.ac.at/records/2zx6e-5pr64" target="_blank" title="">https://researchdata.tuwien.ac.at/records/2zx6e-5pr64</a></span></span></span>: The MAtrixware REsearch Collection/The Information retrieval facility Research Collection은 EP, WO, US, JP 특허청으로부터 1,900만 개가 넘는 문서의 특허 코퍼스이다.</p>
</div>
</li>
<li id="A1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A1.I1.i8.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i8.p1.1">Medi-Notice<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice" target="_blank" title="">https://pub.cl.uzh.ch/wiki/public/pacoco/medi-notice</a></span></span></span>은 취리히 병렬 코퍼스 컬렉션의 일부입니다. 스위스 치료 제품 기관에서 발행한 의약품 및 의약품에 대한 정보 전단지에서 수집한 다국어 코퍼스이다. <cite class="ltx_cite ltx_citemacro_cite">Graën et al. (<a class="ltx_ref" href="#bib.bib17" title="">2019</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A1.I1.i9.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i9.p1.1">Swiss Policy<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus" target="_blank" title="">https://pub.cl.uzh.ch/wiki/public/pacoco/swiss_legislation_corpus</a></span></span></span> contains documents of the Swiss Legislation Corpus <cite class="ltx_cite ltx_citemacro_cite">Höfler and Piotrowski (<a class="ltx_ref" href="#bib.bib23" title="">2011</a>)</cite></p>
</div>
</li>
<li id="A1.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A1.I1.i10.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i10.p1.1">OpenSubtitles 2018<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opus.nlpl.eu/OpenSubtitles-v2018.php" target="_blank" title="">https://opus.nlpl.eu/OpenSubtitles-v2018.php</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.opensubtitles.org/de/index.cgi" target="_blank" title="">https://www.opensubtitles.org/de/index.cgi</a></span></span></span>은 번역된 영화 자막의 모음이다. <cite class="ltx_cite ltx_citemacro_cite">Lison and Tiedemann (<a class="ltx_ref" href="#bib.bib30" title="">2016</a>)</cite></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Tokenizer</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p" id="A2.p1.1">실험에서는 <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">Huggingface tokenizer</span> library <cite class="ltx_cite ltx_citemacro_cite">Moi and Patry (<a class="ltx_ref" href="#bib.bib31" title="">2023</a>)</cite>와 <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">SentencePiece</span> library <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="#bib.bib27" title="">2018</a>)</cite>에 초점을 맞추었다. <a class="ltx_ref" href="#A1.T6" title="In Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>에 달리 명시되지 않은 경우 SentencePiece 라이브러리의 표준 설정을 사용합니다. HuggingFace tokenizer 라이브러리의 경우 <a class="ltx_ref" href="#A1.T7" title="In Appendix A Corpora ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>는 표준 값에서 벗어난 위치를 보여줍니다.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>LLM Architecture and Hyperparameters</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p" id="A3.p1.1">2.7B 파라미터 모델의 학습 구조와 관련하여 GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib4" title="">2020a</a>)</cite>의 아키텍처를 면밀히 따랐다. 사용된 아키텍처 세부사항 및 하이퍼파라미터의 개요는 <a class="ltx_ref" href="#A3.T8" title="In Appendix C LLM Architecture and Hyperparameters ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>에서 주어진다.</p>
</div>
<figure id="A3.T8" class="ltx_table">
<table id="A3.T8.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T8.2.3.1" class="ltx_tr">
<th id="A3.T8.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">parameter</th>
<th id="A3.T8.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T8.2.4.1" class="ltx_tr">
<th id="A3.T8.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># Hidden Dimension</th>
<td id="A3.T8.2.4.1.2" class="ltx_td ltx_align_left ltx_border_t">2560</td>
</tr>
<tr id="A3.T8.2.5.2" class="ltx_tr">
<th id="A3.T8.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"># Layers</th>
<td id="A3.T8.2.5.2.2" class="ltx_td ltx_align_left">32</td>
</tr>
<tr id="A3.T8.2.6.3" class="ltx_tr">
<th id="A3.T8.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"># Attention-Heads</th>
<td id="A3.T8.2.6.3.2" class="ltx_td ltx_align_left">32</td>
</tr>
<tr id="A3.T8.2.7.4" class="ltx_tr">
<th id="A3.T8.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sequence-Length</th>
<td id="A3.T8.2.7.4.2" class="ltx_td ltx_align_left">2048</td>
</tr>
<tr id="A3.T8.2.8.5" class="ltx_tr">
<th id="A3.T8.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Optimizer</th>
<td id="A3.T8.2.8.5.2" class="ltx_td ltx_align_left">Adam</td>
</tr>
<tr id="A3.T8.1.1" class="ltx_tr">
<th id="A3.T8.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adam<math id="A3.T8.1.1.1.m1.1" class="ltx_Math" alttext="-\beta_{1}" display="inline"><semantics id="A3.T8.1.1.1.m1.1a"><mrow id="A3.T8.1.1.1.m1.1.1" xref="A3.T8.1.1.1.m1.1.1.cmml"><mo id="A3.T8.1.1.1.m1.1.1a" xref="A3.T8.1.1.1.m1.1.1.cmml">−</mo><msub id="A3.T8.1.1.1.m1.1.1.2" xref="A3.T8.1.1.1.m1.1.1.2.cmml"><mi id="A3.T8.1.1.1.m1.1.1.2.2" xref="A3.T8.1.1.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T8.1.1.1.m1.1.1.2.3" xref="A3.T8.1.1.1.m1.1.1.2.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T8.1.1.1.m1.1b"><apply id="A3.T8.1.1.1.m1.1.1.cmml" xref="A3.T8.1.1.1.m1.1.1"><minus id="A3.T8.1.1.1.m1.1.1.1.cmml" xref="A3.T8.1.1.1.m1.1.1"></minus><apply id="A3.T8.1.1.1.m1.1.1.2.cmml" xref="A3.T8.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T8.1.1.1.m1.1.1.2.1.cmml" xref="A3.T8.1.1.1.m1.1.1.2">subscript</csymbol><ci id="A3.T8.1.1.1.m1.1.1.2.2.cmml" xref="A3.T8.1.1.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="A3.T8.1.1.1.m1.1.1.2.3.cmml" xref="A3.T8.1.1.1.m1.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.1.1.1.m1.1c">-\beta_{1}</annotation></semantics></math>
</th>
<td id="A3.T8.1.1.2" class="ltx_td ltx_align_left">0.9</td>
</tr>
<tr id="A3.T8.2.2" class="ltx_tr">
<th id="A3.T8.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adam<math id="A3.T8.2.2.1.m1.1" class="ltx_Math" alttext="-\beta_{2}" display="inline"><semantics id="A3.T8.2.2.1.m1.1a"><mrow id="A3.T8.2.2.1.m1.1.1" xref="A3.T8.2.2.1.m1.1.1.cmml"><mo id="A3.T8.2.2.1.m1.1.1a" xref="A3.T8.2.2.1.m1.1.1.cmml">−</mo><msub id="A3.T8.2.2.1.m1.1.1.2" xref="A3.T8.2.2.1.m1.1.1.2.cmml"><mi id="A3.T8.2.2.1.m1.1.1.2.2" xref="A3.T8.2.2.1.m1.1.1.2.2.cmml">β</mi><mn id="A3.T8.2.2.1.m1.1.1.2.3" xref="A3.T8.2.2.1.m1.1.1.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.T8.2.2.1.m1.1b"><apply id="A3.T8.2.2.1.m1.1.1.cmml" xref="A3.T8.2.2.1.m1.1.1"><minus id="A3.T8.2.2.1.m1.1.1.1.cmml" xref="A3.T8.2.2.1.m1.1.1"></minus><apply id="A3.T8.2.2.1.m1.1.1.2.cmml" xref="A3.T8.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.T8.2.2.1.m1.1.1.2.1.cmml" xref="A3.T8.2.2.1.m1.1.1.2">subscript</csymbol><ci id="A3.T8.2.2.1.m1.1.1.2.2.cmml" xref="A3.T8.2.2.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="A3.T8.2.2.1.m1.1.1.2.3.cmml" xref="A3.T8.2.2.1.m1.1.1.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.2.2.1.m1.1c">-\beta_{2}</annotation></semantics></math>
</th>
<td id="A3.T8.2.2.2" class="ltx_td ltx_align_left">0.9</td>
</tr>
<tr id="A3.T8.2.9.6" class="ltx_tr">
<th id="A3.T8.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Learning rate</th>
<td id="A3.T8.2.9.6.2" class="ltx_td ltx_align_left">1.6e-4</td>
</tr>
<tr id="A3.T8.2.10.7" class="ltx_tr">
<th id="A3.T8.2.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Learning rate decay</th>
<td id="A3.T8.2.10.7.2" class="ltx_td ltx_align_left">Cosine</td>
</tr>
<tr id="A3.T8.2.11.8" class="ltx_tr">
<th id="A3.T8.2.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Precision</th>
<td id="A3.T8.2.11.8.2" class="ltx_td ltx_align_left">BF16</td>
</tr>
<tr id="A3.T8.2.12.9" class="ltx_tr">
<th id="A3.T8.2.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FlashAttention</th>
<td id="A3.T8.2.12.9.2" class="ltx_td ltx_align_left">2.0</td>
</tr>
<tr id="A3.T8.2.13.10" class="ltx_tr">
<th id="A3.T8.2.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Position-Embeddings</th>
<td id="A3.T8.2.13.10.2" class="ltx_td ltx_align_left ltx_border_bb">Rotary</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A3.T8.4.1.1" style="font-size:90%;">Table 8</span>:</span><span class="ltx_text" id="A3.T8.5.2" style="font-size:90%;">Overview of the LLM hyperparameters that we used for the training. </span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Intrinsic Tokenizer Evaluation</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p" id="A4.p1.1">동일한 시소러스에서 동일한 알고리즘의 중첩을 연구하는 것 외에도 알고리즘 전반에 걸친 어휘 중첩에 관심이 있었고 시소러스는 <a class="ltx_ref" href="#A4.F6" title="In Appendix D Intrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>를 참조한다. 우리가 관찰할 수 있는 것은 다국어 어휘와 영어 어휘는 증가하는 어휘 크기에 걸쳐 유사하게 유지되는 24%와 34% 사이의 다소 작은 중첩을 가지고 있다는 것이다. 알고리즘 전반에 걸쳐, 우리는 SentencePiece의 Unigram과 BPE가 Huggingface의 Unigram과 BPE보다 약간 더 높은 중첩을 갖는다는 것을 알 수 있다. 이것은 라이브러리별 전처리 단계와 더 유사한 하이퍼파라미터 때문일 수 있다고 생각한다.</p>
</div>
<figure id="A4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x10.png" id="A4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="322" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x11.png" id="A4.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="322" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x12.png" id="A4.F6.3.g1" class="ltx_graphics ltx_img_square" width="484" height="443" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F6.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x13.png" id="A4.F6.4.g1" class="ltx_graphics ltx_img_square" width="484" height="443" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F6.6.1.1" style="font-size:90%;">Figure 6</span>:</span><span class="ltx_text" id="A4.F6.7.2" style="font-size:90%;">Vocabulary overlap between the examined tokenizers</span></figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Extrinsic Tokenizer Evaluation</h2>

<figure id="A5.T9" class="ltx_table">
<table id="A5.T9.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.T9.2.1.1" class="ltx_tr">
<td id="A5.T9.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="A5.T9.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A5.T9.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Accuracy</span></th>
</tr>
<tr id="A5.T9.2.2.2" class="ltx_tr">
<td id="A5.T9.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="A5.T9.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.2.2.2.1" class="ltx_text" style="font-size:90%;">EN</span></td>
<td id="A5.T9.2.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.2.2.3.1" class="ltx_text" style="font-size:90%;">MULTI</span></td>
</tr>
<tr id="A5.T9.2.3.3" class="ltx_tr">
<td id="A5.T9.2.3.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.3.3.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-100</span></td>
<td id="A5.T9.2.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.3.3.2.1" class="ltx_text" style="font-size:90%;">45.60</span></td>
<td id="A5.T9.2.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.3.3.3.1" class="ltx_text" style="font-size:90%;">38.50</span></td>
</tr>
<tr id="A5.T9.2.4.4" class="ltx_tr">
<td id="A5.T9.2.4.4.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.4.4.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-32</span></td>
<td id="A5.T9.2.4.4.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.4.4.2.1" class="ltx_text" style="font-size:90%;">45.34</span></td>
<td id="A5.T9.2.4.4.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.4.4.3.1" class="ltx_text" style="font-size:90%;">38.12</span></td>
</tr>
<tr id="A5.T9.2.5.5" class="ltx_tr">
<td id="A5.T9.2.5.5.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.5.5.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-50</span></td>
<td id="A5.T9.2.5.5.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.5.5.2.1" class="ltx_text" style="font-size:90%;">45.85</span></td>
<td id="A5.T9.2.5.5.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.5.5.3.1" class="ltx_text" style="font-size:90%;">38.15</span></td>
</tr>
<tr id="A5.T9.2.6.6" class="ltx_tr">
<td id="A5.T9.2.6.6.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.6.6.1.1" class="ltx_text" style="font-size:90%;">BPE-HF-82</span></td>
<td id="A5.T9.2.6.6.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.6.6.2.1" class="ltx_text" style="font-size:90%;">44.97</span></td>
<td id="A5.T9.2.6.6.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.6.6.3.1" class="ltx_text" style="font-size:90%;">38.67</span></td>
</tr>
<tr id="A5.T9.2.7.7" class="ltx_tr">
<td id="A5.T9.2.7.7.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.7.7.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-100</span></td>
<td id="A5.T9.2.7.7.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.7.7.2.1" class="ltx_text" style="font-size:90%;">46.06</span></td>
<td id="A5.T9.2.7.7.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.7.7.3.1" class="ltx_text" style="font-size:90%;">38.92</span></td>
</tr>
<tr id="A5.T9.2.8.8" class="ltx_tr">
<td id="A5.T9.2.8.8.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.8.8.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-32</span></td>
<td id="A5.T9.2.8.8.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">47.06</span></td>
<td id="A5.T9.2.8.8.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.8.8.3.1" class="ltx_text" style="font-size:90%;">38.01</span></td>
</tr>
<tr id="A5.T9.2.9.9" class="ltx_tr">
<td id="A5.T9.2.9.9.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.9.9.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-50</span></td>
<td id="A5.T9.2.9.9.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.9.9.2.1" class="ltx_text" style="font-size:90%;">46.85</span></td>
<td id="A5.T9.2.9.9.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.9.9.3.1" class="ltx_text" style="font-size:90%;">38.41</span></td>
</tr>
<tr id="A5.T9.2.10.10" class="ltx_tr">
<td id="A5.T9.2.10.10.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.10.10.1.1" class="ltx_text" style="font-size:90%;">BPE-SP-82</span></td>
<td id="A5.T9.2.10.10.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.10.10.2.1" class="ltx_text" style="font-size:90%;">45.25</span></td>
<td id="A5.T9.2.10.10.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.10.10.3.1" class="ltx_text" style="font-size:90%;">38.64</span></td>
</tr>
<tr id="A5.T9.2.11.11" class="ltx_tr">
<td id="A5.T9.2.11.11.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.11.11.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-100</span></td>
<td id="A5.T9.2.11.11.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T9.2.11.11.2.1" class="ltx_text" style="font-size:90%;">45.97</span></td>
<td id="A5.T9.2.11.11.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A5.T9.2.11.11.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">39.34</span></td>
</tr>
<tr id="A5.T9.2.12.12" class="ltx_tr">
<td id="A5.T9.2.12.12.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.12.12.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-32</span></td>
<td id="A5.T9.2.12.12.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.12.12.2.1" class="ltx_text" style="font-size:90%;">46.62</span></td>
<td id="A5.T9.2.12.12.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.12.12.3.1" class="ltx_text" style="font-size:90%;">38.18</span></td>
</tr>
<tr id="A5.T9.2.13.13" class="ltx_tr">
<td id="A5.T9.2.13.13.1" class="ltx_td ltx_align_left"><span id="A5.T9.2.13.13.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-50</span></td>
<td id="A5.T9.2.13.13.2" class="ltx_td ltx_align_left"><span id="A5.T9.2.13.13.2.1" class="ltx_text" style="font-size:90%;">45.81</span></td>
<td id="A5.T9.2.13.13.3" class="ltx_td ltx_align_right"><span id="A5.T9.2.13.13.3.1" class="ltx_text" style="font-size:90%;">38.66</span></td>
</tr>
<tr id="A5.T9.2.14.14" class="ltx_tr">
<td id="A5.T9.2.14.14.1" class="ltx_td ltx_align_left ltx_border_b"><span id="A5.T9.2.14.14.1.1" class="ltx_text" style="font-size:90%;">UNI-SP-82</span></td>
<td id="A5.T9.2.14.14.2" class="ltx_td ltx_align_left ltx_border_b"><span id="A5.T9.2.14.14.2.1" class="ltx_text" style="font-size:90%;">45.65</span></td>
<td id="A5.T9.2.14.14.3" class="ltx_td ltx_align_right ltx_border_b"><span id="A5.T9.2.14.14.3.1" class="ltx_text" style="font-size:90%;">39.21</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">표 9:</span>모든 다운스트림 태스크에 걸친 단일 언어 및 다국어 토큰라이저의 평균 정확도</figcaption>
</figure>
<figure id="A5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x14.png" id="A5.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F7.sf1.2.1.1" style="font-size:90%;">(a)</span></span><span class="ltx_text" id="A5.F7.sf1.3.2" style="font-size:90%;">Monolingual tokenizers</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/x15.png" id="A5.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F7.sf2.2.1.1" style="font-size:90%;">(b)</span></span><span class="ltx_text" id="A5.F7.sf2.3.2" style="font-size:90%;">Multilingual tokenizers</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F7.2.1.1" style="font-size:90%;">Figure 7</span>:</span><span class="ltx_text" id="A5.F7.3.2" style="font-size:90%;">Accuracy distribution for mono-/multilingual tokenizers across mono-/multilingual tasks. "+"는 모든 작업에서 평균 성능을 나타냅니다. </span></figcaption>
</figure>
<figure id="A5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A5.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/appendix/figures/computational_efficiency/gflops_per_word_forward_pass_fertility_non_en.jpg" id="A5.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F8.sf1.2.1.1" style="font-size:90%;">(a)</span></span><span><span class="ltx_text" id="A5.F8.sf1.3.2" style="font-size:90%;">Non-English documents. </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A5.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/appendix/figures/computational_efficiency/gflops_per_word_forward_pass_fertility_en.jpg" id="A5.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F8.sf2.2.1.1" style="font-size:90%;">(b)</span></span><span><span class="ltx_text" id="A5.F8.sf2.3.2" style="font-size:90%;">English documents. </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A5.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2310.08754/assets/appendix/figures/computational_efficiency/gflops_per_word_forward_pass_fertility_de.jpg" id="A5.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F8.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="A5.F8.sf3.3.2" class="ltx_text" style="font-size:90%;">German documents.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F8.3.1.1" style="font-size:90%;">Figure 8</span>:</span><span class="ltx_text" id="A5.F8.4.2" style="font-size:90%;">Average compute (GFLOPs required to processing a single word within a <span class="ltx_text ltx_font_bold" id="A5.F8.4.2.1">inference</span> pass. </span></figcaption>
</figure>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Performance Gains on Task-Level</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A5.SS1.p1.1">표 <a class="ltx_ref" href="#A5.T11" title="In E.1 Performance Gains on Task-Level ‣ Appendix E Extrinsic Tokenizer Evaluation ‣ Tokenizer Choice For LLM Training: Negligible or Crucial?"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>에서는 토큰화기의 선택이 각 다운스트림 작업에 얼마나 많은 영향을 미치는지 보여준다. 따라서 우리는 실험에서 각 작업에 대한 최상의 모델과 최악의 모델의 결과를 비교한다. 성능 증가는 +5,3%에서 최대 380,9%에 이르며, 다운스트림 성능에서 올바른 토큰화기를 선택하는 것의 중요성을 다시 보여준다.</p>
</div>
<figure id="A5.T10" class="ltx_table">
<table id="A5.T10.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A5.T10.2.1.1" class="ltx_tr">
<th id="A5.T10.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="A5.T10.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Non-English</th>
<th id="A5.T10.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">English</th>
<th id="A5.T10.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">German</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A5.T10.2.2.1" class="ltx_tr">
<th id="A5.T10.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EN-BPE-HF-33</th>
<td id="A5.T10.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">3,8</td>
<td id="A5.T10.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A5.T10.2.2.1.3.1" class="ltx_text ltx_font_bold">2,32</span></td>
<td id="A5.T10.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">4,52</td>
</tr>
<tr id="A5.T10.2.3.2" class="ltx_tr">
<th id="A5.T10.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-HF-50</th>
<td id="A5.T10.2.3.2.2" class="ltx_td ltx_align_center">3,79</td>
<td id="A5.T10.2.3.2.3" class="ltx_td ltx_align_center">2,38</td>
<td id="A5.T10.2.3.2.4" class="ltx_td ltx_align_center">4,45</td>
</tr>
<tr id="A5.T10.2.4.3" class="ltx_tr">
<th id="A5.T10.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-HF-82</th>
<td id="A5.T10.2.4.3.2" class="ltx_td ltx_align_center">3,88</td>
<td id="A5.T10.2.4.3.3" class="ltx_td ltx_align_center">2,55</td>
<td id="A5.T10.2.4.3.4" class="ltx_td ltx_align_center">4,51</td>
</tr>
<tr id="A5.T10.2.5.4" class="ltx_tr">
<th id="A5.T10.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-HF-100</th>
<td id="A5.T10.2.5.4.2" class="ltx_td ltx_align_center">3,96</td>
<td id="A5.T10.2.5.4.3" class="ltx_td ltx_align_center">2,67</td>
<td id="A5.T10.2.5.4.4" class="ltx_td ltx_align_center">4,58</td>
</tr>
<tr id="A5.T10.2.6.5" class="ltx_tr">
<th id="A5.T10.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-33</th>
<td id="A5.T10.2.6.5.2" class="ltx_td ltx_align_center">3,86</td>
<td id="A5.T10.2.6.5.3" class="ltx_td ltx_align_center">2,37</td>
<td id="A5.T10.2.6.5.4" class="ltx_td ltx_align_center">4,66</td>
</tr>
<tr id="A5.T10.2.7.6" class="ltx_tr">
<th id="A5.T10.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-50</th>
<td id="A5.T10.2.7.6.2" class="ltx_td ltx_align_center">3,89</td>
<td id="A5.T10.2.7.6.3" class="ltx_td ltx_align_center">2,42</td>
<td id="A5.T10.2.7.6.4" class="ltx_td ltx_align_center">4,68</td>
</tr>
<tr id="A5.T10.2.8.7" class="ltx_tr">
<th id="A5.T10.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-82</th>
<td id="A5.T10.2.8.7.2" class="ltx_td ltx_align_center">4,02</td>
<td id="A5.T10.2.8.7.3" class="ltx_td ltx_align_center">2,59</td>
<td id="A5.T10.2.8.7.4" class="ltx_td ltx_align_center">4,78</td>
</tr>
<tr id="A5.T10.2.9.8" class="ltx_tr">
<th id="A5.T10.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-BPE-SP-100</th>
<td id="A5.T10.2.9.8.2" class="ltx_td ltx_align_center">4,11</td>
<td id="A5.T10.2.9.8.3" class="ltx_td ltx_align_center">2,71</td>
<td id="A5.T10.2.9.8.4" class="ltx_td ltx_align_center">4,84</td>
</tr>
<tr id="A5.T10.2.10.9" class="ltx_tr">
<th id="A5.T10.2.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-32</th>
<td id="A5.T10.2.10.9.2" class="ltx_td ltx_align_center">4,01</td>
<td id="A5.T10.2.10.9.3" class="ltx_td ltx_align_center">2,36</td>
<td id="A5.T10.2.10.9.4" class="ltx_td ltx_align_center">4,73</td>
</tr>
<tr id="A5.T10.2.11.10" class="ltx_tr">
<th id="A5.T10.2.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-50</th>
<td id="A5.T10.2.11.10.2" class="ltx_td ltx_align_center">4,02</td>
<td id="A5.T10.2.11.10.3" class="ltx_td ltx_align_center">2,42</td>
<td id="A5.T10.2.11.10.4" class="ltx_td ltx_align_center">4,75</td>
</tr>
<tr id="A5.T10.2.12.11" class="ltx_tr">
<th id="A5.T10.2.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-82</th>
<td id="A5.T10.2.12.11.2" class="ltx_td ltx_align_center">4,12</td>
<td id="A5.T10.2.12.11.3" class="ltx_td ltx_align_center">2,59</td>
<td id="A5.T10.2.12.11.4" class="ltx_td ltx_align_center">4,83</td>
</tr>
<tr id="A5.T10.2.13.12" class="ltx_tr">
<th id="A5.T10.2.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN-UNI-SP-100</th>
<td id="A5.T10.2.13.12.2" class="ltx_td ltx_align_center">4,21</td>
<td id="A5.T10.2.13.12.3" class="ltx_td ltx_align_center">2,71</td>
<td id="A5.T10.2.13.12.4" class="ltx_td ltx_align_center">4,88</td>
</tr>
<tr id="A5.T10.2.14.13" class="ltx_tr">
<th id="A5.T10.2.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MULTI-BPE-HF-33</th>
<td id="A5.T10.2.14.13.2" class="ltx_td ltx_align_center ltx_border_t">2,71</td>
<td id="A5.T10.2.14.13.3" class="ltx_td ltx_align_center ltx_border_t">2,46</td>
<td id="A5.T10.2.14.13.4" class="ltx_td ltx_align_center ltx_border_t">3,04</td>
</tr>
<tr id="A5.T10.2.15.14" class="ltx_tr">
<th id="A5.T10.2.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-HF-50</th>
<td id="A5.T10.2.15.14.2" class="ltx_td ltx_align_center">2,7</td>
<td id="A5.T10.2.15.14.3" class="ltx_td ltx_align_center">2,5</td>
<td id="A5.T10.2.15.14.4" class="ltx_td ltx_align_center">3,01</td>
</tr>
<tr id="A5.T10.2.16.15" class="ltx_tr">
<th id="A5.T10.2.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-HF-82</th>
<td id="A5.T10.2.16.15.2" class="ltx_td ltx_align_center">2,8</td>
<td id="A5.T10.2.16.15.3" class="ltx_td ltx_align_center">2,65</td>
<td id="A5.T10.2.16.15.4" class="ltx_td ltx_align_center">3,09</td>
</tr>
<tr id="A5.T10.2.17.16" class="ltx_tr">
<th id="A5.T10.2.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-HF-100</th>
<td id="A5.T10.2.17.16.2" class="ltx_td ltx_align_center">2,88</td>
<td id="A5.T10.2.17.16.3" class="ltx_td ltx_align_center">2,76</td>
<td id="A5.T10.2.17.16.4" class="ltx_td ltx_align_center">3,17</td>
</tr>
<tr id="A5.T10.2.18.17" class="ltx_tr">
<th id="A5.T10.2.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-33</th>
<td id="A5.T10.2.18.17.2" class="ltx_td ltx_align_center">2,68</td>
<td id="A5.T10.2.18.17.3" class="ltx_td ltx_align_center">2,55</td>
<td id="A5.T10.2.18.17.4" class="ltx_td ltx_align_center">2,99</td>
</tr>
<tr id="A5.T10.2.19.18" class="ltx_tr">
<th id="A5.T10.2.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-50</th>
<td id="A5.T10.2.19.18.2" class="ltx_td ltx_align_center">2,67</td>
<td id="A5.T10.2.19.18.3" class="ltx_td ltx_align_center">2,57</td>
<td id="A5.T10.2.19.18.4" class="ltx_td ltx_align_center">2,95</td>
</tr>
<tr id="A5.T10.2.20.19" class="ltx_tr">
<th id="A5.T10.2.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-82</th>
<td id="A5.T10.2.20.19.2" class="ltx_td ltx_align_center">2,76</td>
<td id="A5.T10.2.20.19.3" class="ltx_td ltx_align_center">2,72</td>
<td id="A5.T10.2.20.19.4" class="ltx_td ltx_align_center">3,03</td>
</tr>
<tr id="A5.T10.2.21.20" class="ltx_tr">
<th id="A5.T10.2.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-BPE-SP-100</th>
<td id="A5.T10.2.21.20.2" class="ltx_td ltx_align_center">2,85</td>
<td id="A5.T10.2.21.20.3" class="ltx_td ltx_align_center">2,82</td>
<td id="A5.T10.2.21.20.4" class="ltx_td ltx_align_center">3,1</td>
</tr>
<tr id="A5.T10.2.22.21" class="ltx_tr">
<th id="A5.T10.2.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-UNI-SP-33</th>
<td id="A5.T10.2.22.21.2" class="ltx_td ltx_align_center">2,68</td>
<td id="A5.T10.2.22.21.3" class="ltx_td ltx_align_center">2,55</td>
<td id="A5.T10.2.22.21.4" class="ltx_td ltx_align_center">2,94</td>
</tr>
<tr id="A5.T10.2.23.22" class="ltx_tr">
<th id="A5.T10.2.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-UNI-SP-50</th>
<td id="A5.T10.2.23.22.2" class="ltx_td ltx_align_center"><span id="A5.T10.2.23.22.2.1" class="ltx_text ltx_font_bold">2,66</span></td>
<td id="A5.T10.2.23.22.3" class="ltx_td ltx_align_center">2,58</td>
<td id="A5.T10.2.23.22.4" class="ltx_td ltx_align_center"><span id="A5.T10.2.23.22.4.1" class="ltx_text ltx_font_bold">2,91</span></td>
</tr>
<tr id="A5.T10.2.24.23" class="ltx_tr">
<th id="A5.T10.2.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MULTI-UNI-SP-82</th>
<td id="A5.T10.2.24.23.2" class="ltx_td ltx_align_center">2,76</td>
<td id="A5.T10.2.24.23.3" class="ltx_td ltx_align_center">2,73</td>
<td id="A5.T10.2.24.23.4" class="ltx_td ltx_align_center">2,99</td>
</tr>
<tr id="A5.T10.2.25.24" class="ltx_tr">
<th id="A5.T10.2.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MULTI-UNI-SP-100</th>
<td id="A5.T10.2.25.24.2" class="ltx_td ltx_align_center ltx_border_bb">2,84</td>
<td id="A5.T10.2.25.24.3" class="ltx_td ltx_align_center ltx_border_bb">2,83</td>
<td id="A5.T10.2.25.24.4" class="ltx_td ltx_align_center ltx_border_bb">3,07</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A5.T10.3.1.1" style="font-size:90%;">Table 10</span>:</span><span class="ltx_text" id="A5.T10.4.2" style="font-size:90%;">Computational costs per word (GFLOPs) for different tokenizers. </span></figcaption>
</figure>
<figure id="A5.T11" class="ltx_table">
<table id="A5.T11.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A5.T11.2.1.1" class="ltx_tr">
<th id="A5.T11.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A5.T11.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Task</th>
<th id="A5.T11.2.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Metric</th>
<th id="A5.T11.2.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Worst</th>
<th id="A5.T11.2.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Best</th>
<th id="A5.T11.2.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Difference</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A5.T11.2.2.1" class="ltx_tr">
<th id="A5.T11.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="26"><span id="A5.T11.2.2.1.1.1" class="ltx_text">
<span id="A5.T11.2.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:14.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:14.3pt;transform:translate(-3.74pt,-3.74pt) rotate(-90deg) ;">
<span id="A5.T11.2.2.1.1.1.1.1" class="ltx_p">EN</span>
</span></span></span></th>
<th id="A5.T11.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">arc challenge</th>
<td id="A5.T11.2.2.1.3" class="ltx_td ltx_align_right ltx_border_t">acc</td>
<td id="A5.T11.2.2.1.4" class="ltx_td ltx_align_right ltx_border_t">0,21</td>
<td id="A5.T11.2.2.1.5" class="ltx_td ltx_align_right ltx_border_t">0,27</td>
<td id="A5.T11.2.2.1.6" class="ltx_td ltx_align_right ltx_border_t">+28,4 %</td>
</tr>
<tr id="A5.T11.2.3.2" class="ltx_tr">
<th id="A5.T11.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">arc easy</th>
<td id="A5.T11.2.3.2.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.3.2.3" class="ltx_td ltx_align_right">0,50</td>
<td id="A5.T11.2.3.2.4" class="ltx_td ltx_align_right">0,59</td>
<td id="A5.T11.2.3.2.5" class="ltx_td ltx_align_right">+18,6 %</td>
</tr>
<tr id="A5.T11.2.4.3" class="ltx_tr">
<th id="A5.T11.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">boolq</th>
<td id="A5.T11.2.4.3.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.4.3.3" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.4.3.4" class="ltx_td ltx_align_right">0,61</td>
<td id="A5.T11.2.4.3.5" class="ltx_td ltx_align_right">+8,9 %</td>
</tr>
<tr id="A5.T11.2.5.4" class="ltx_tr">
<th id="A5.T11.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">cb</th>
<td id="A5.T11.2.5.4.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.5.4.3" class="ltx_td ltx_align_right">0,30</td>
<td id="A5.T11.2.5.4.4" class="ltx_td ltx_align_right">0,54</td>
<td id="A5.T11.2.5.4.5" class="ltx_td ltx_align_right">+76,5 %</td>
</tr>
<tr id="A5.T11.2.6.5" class="ltx_tr">
<th id="A5.T11.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">cb</th>
<td id="A5.T11.2.6.5.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.6.5.3" class="ltx_td ltx_align_right">0,18</td>
<td id="A5.T11.2.6.5.4" class="ltx_td ltx_align_right">0,36</td>
<td id="A5.T11.2.6.5.5" class="ltx_td ltx_align_right">+94,9 %</td>
</tr>
<tr id="A5.T11.2.7.6" class="ltx_tr">
<th id="A5.T11.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">copa</th>
<td id="A5.T11.2.7.6.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.7.6.3" class="ltx_td ltx_align_right">0,67</td>
<td id="A5.T11.2.7.6.4" class="ltx_td ltx_align_right">0,73</td>
<td id="A5.T11.2.7.6.5" class="ltx_td ltx_align_right">+9,0 %</td>
</tr>
<tr id="A5.T11.2.8.7" class="ltx_tr">
<th id="A5.T11.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">hellaswag</th>
<td id="A5.T11.2.8.7.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.8.7.3" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.8.7.4" class="ltx_td ltx_align_right">0,41</td>
<td id="A5.T11.2.8.7.5" class="ltx_td ltx_align_right">+19,7 %</td>
</tr>
<tr id="A5.T11.2.9.8" class="ltx_tr">
<th id="A5.T11.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada openai</th>
<td id="A5.T11.2.9.8.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.9.8.3" class="ltx_td ltx_align_right">0,47</td>
<td id="A5.T11.2.9.8.4" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.9.8.5" class="ltx_td ltx_align_right">+20,0 %</td>
</tr>
<tr id="A5.T11.2.10.9" class="ltx_tr">
<th id="A5.T11.2.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mathqa</th>
<td id="A5.T11.2.10.9.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.10.9.3" class="ltx_td ltx_align_right">0,22</td>
<td id="A5.T11.2.10.9.4" class="ltx_td ltx_align_right">0,24</td>
<td id="A5.T11.2.10.9.5" class="ltx_td ltx_align_right">+8,8 %</td>
</tr>
<tr id="A5.T11.2.11.10" class="ltx_tr">
<th id="A5.T11.2.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mnli</th>
<td id="A5.T11.2.11.10.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.11.10.3" class="ltx_td ltx_align_right">0,35</td>
<td id="A5.T11.2.11.10.4" class="ltx_td ltx_align_right">0,37</td>
<td id="A5.T11.2.11.10.5" class="ltx_td ltx_align_right">+7,7 %</td>
</tr>
<tr id="A5.T11.2.12.11" class="ltx_tr">
<th id="A5.T11.2.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mnli mismatched</th>
<td id="A5.T11.2.12.11.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.12.11.3" class="ltx_td ltx_align_right">0,35</td>
<td id="A5.T11.2.12.11.4" class="ltx_td ltx_align_right">0,38</td>
<td id="A5.T11.2.12.11.5" class="ltx_td ltx_align_right">+9,4 %</td>
</tr>
<tr id="A5.T11.2.13.12" class="ltx_tr">
<th id="A5.T11.2.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mrpc</th>
<td id="A5.T11.2.13.12.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.13.12.3" class="ltx_td ltx_align_right">0,54</td>
<td id="A5.T11.2.13.12.4" class="ltx_td ltx_align_right">0,69</td>
<td id="A5.T11.2.13.12.5" class="ltx_td ltx_align_right">+26,1 %</td>
</tr>
<tr id="A5.T11.2.14.13" class="ltx_tr">
<th id="A5.T11.2.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mrpc</th>
<td id="A5.T11.2.14.13.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.14.13.3" class="ltx_td ltx_align_right">0,66</td>
<td id="A5.T11.2.14.13.4" class="ltx_td ltx_align_right">0,81</td>
<td id="A5.T11.2.14.13.5" class="ltx_td ltx_align_right">+22,7 %</td>
</tr>
<tr id="A5.T11.2.15.14" class="ltx_tr">
<th id="A5.T11.2.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">gnad10</th>
<td id="A5.T11.2.15.14.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.15.14.3" class="ltx_td ltx_align_right">0,15</td>
<td id="A5.T11.2.15.14.4" class="ltx_td ltx_align_right">0,43</td>
<td id="A5.T11.2.15.14.5" class="ltx_td ltx_align_right">+187,7 %</td>
</tr>
<tr id="A5.T11.2.16.15" class="ltx_tr">
<th id="A5.T11.2.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">gnad10</th>
<td id="A5.T11.2.16.15.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.16.15.3" class="ltx_td ltx_align_right">0,07</td>
<td id="A5.T11.2.16.15.4" class="ltx_td ltx_align_right">0,35</td>
<td id="A5.T11.2.16.15.5" class="ltx_td ltx_align_right">+380,9 %</td>
</tr>
<tr id="A5.T11.2.17.16" class="ltx_tr">
<th id="A5.T11.2.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">gnad10</th>
<td id="A5.T11.2.17.16.2" class="ltx_td ltx_align_right">recall</td>
<td id="A5.T11.2.17.16.3" class="ltx_td ltx_align_right">0,14</td>
<td id="A5.T11.2.17.16.4" class="ltx_td ltx_align_right">0,42</td>
<td id="A5.T11.2.17.16.5" class="ltx_td ltx_align_right">+207,6 %</td>
</tr>
<tr id="A5.T11.2.18.17" class="ltx_tr">
<th id="A5.T11.2.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">pawsx es</th>
<td id="A5.T11.2.18.17.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.18.17.3" class="ltx_td ltx_align_right">0,48</td>
<td id="A5.T11.2.18.17.4" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.18.17.5" class="ltx_td ltx_align_right">+17,2 %</td>
</tr>
<tr id="A5.T11.2.19.18" class="ltx_tr">
<th id="A5.T11.2.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">piqa</th>
<td id="A5.T11.2.19.18.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.19.18.3" class="ltx_td ltx_align_right">0,67</td>
<td id="A5.T11.2.19.18.4" class="ltx_td ltx_align_right">0,72</td>
<td id="A5.T11.2.19.18.5" class="ltx_td ltx_align_right">+7,2 %</td>
</tr>
<tr id="A5.T11.2.20.19" class="ltx_tr">
<th id="A5.T11.2.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">pubmedqa</th>
<td id="A5.T11.2.20.19.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.20.19.3" class="ltx_td ltx_align_right">0,45</td>
<td id="A5.T11.2.20.19.4" class="ltx_td ltx_align_right">0,59</td>
<td id="A5.T11.2.20.19.5" class="ltx_td ltx_align_right">+32,1 %</td>
</tr>
<tr id="A5.T11.2.21.20" class="ltx_tr">
<th id="A5.T11.2.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">race</th>
<td id="A5.T11.2.21.20.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.21.20.3" class="ltx_td ltx_align_right">0,31</td>
<td id="A5.T11.2.21.20.4" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.21.20.5" class="ltx_td ltx_align_right">+11,8 %</td>
</tr>
<tr id="A5.T11.2.22.21" class="ltx_tr">
<th id="A5.T11.2.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">rte</th>
<td id="A5.T11.2.22.21.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.22.21.3" class="ltx_td ltx_align_right">0,53</td>
<td id="A5.T11.2.22.21.4" class="ltx_td ltx_align_right">0,57</td>
<td id="A5.T11.2.22.21.5" class="ltx_td ltx_align_right">+8,9 %</td>
</tr>
<tr id="A5.T11.2.23.22" class="ltx_tr">
<th id="A5.T11.2.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">sst</th>
<td id="A5.T11.2.23.22.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.23.22.3" class="ltx_td ltx_align_right">0,51</td>
<td id="A5.T11.2.23.22.4" class="ltx_td ltx_align_right">0,75</td>
<td id="A5.T11.2.23.22.5" class="ltx_td ltx_align_right">+48,2 %</td>
</tr>
<tr id="A5.T11.2.24.23" class="ltx_tr">
<th id="A5.T11.2.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">triviaqa</th>
<td id="A5.T11.2.24.23.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.24.23.3" class="ltx_td ltx_align_right">0,02</td>
<td id="A5.T11.2.24.23.4" class="ltx_td ltx_align_right">0,05</td>
<td id="A5.T11.2.24.23.5" class="ltx_td ltx_align_right">+171,3 %</td>
</tr>
<tr id="A5.T11.2.25.24" class="ltx_tr">
<th id="A5.T11.2.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">wic</th>
<td id="A5.T11.2.25.24.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.25.24.3" class="ltx_td ltx_align_right">0,50</td>
<td id="A5.T11.2.25.24.4" class="ltx_td ltx_align_right">0,55</td>
<td id="A5.T11.2.25.24.5" class="ltx_td ltx_align_right">+11,4 %</td>
</tr>
<tr id="A5.T11.2.26.25" class="ltx_tr">
<th id="A5.T11.2.26.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">winogrande</th>
<td id="A5.T11.2.26.25.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.26.25.3" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.26.25.4" class="ltx_td ltx_align_right">0,56</td>
<td id="A5.T11.2.26.25.5" class="ltx_td ltx_align_right">+12,4 %</td>
</tr>
<tr id="A5.T11.2.27.26" class="ltx_tr">
<th id="A5.T11.2.27.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">wnli</th>
<td id="A5.T11.2.27.26.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.27.26.3" class="ltx_td ltx_align_right">0,42</td>
<td id="A5.T11.2.27.26.4" class="ltx_td ltx_align_right">0,54</td>
<td id="A5.T11.2.27.26.5" class="ltx_td ltx_align_right">+26,7 %</td>
</tr>
<tr id="A5.T11.2.28.27" class="ltx_tr">
<th id="A5.T11.2.28.27.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A5.T11.2.28.27.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">wsc</th>
<td id="A5.T11.2.28.27.3" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.28.27.4" class="ltx_td ltx_align_right">0,36</td>
<td id="A5.T11.2.28.27.5" class="ltx_td ltx_align_right">0,61</td>
<td id="A5.T11.2.28.27.6" class="ltx_td ltx_align_right">+70,3 %</td>
</tr>
<tr id="A5.T11.2.29.28" class="ltx_tr">
<th id="A5.T11.2.29.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="20"><span id="A5.T11.2.29.28.1.1" class="ltx_text">
<span id="A5.T11.2.29.28.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.9pt;transform:translate(-13.04pt,-13.04pt) rotate(-90deg) ;">
<span id="A5.T11.2.29.28.1.1.1.1" class="ltx_p">MULTI</span>
</span></span></span></th>
<th id="A5.T11.2.29.28.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">xstory cloze es</th>
<td id="A5.T11.2.29.28.3" class="ltx_td ltx_align_right ltx_border_t">acc</td>
<td id="A5.T11.2.29.28.4" class="ltx_td ltx_align_right ltx_border_t">0,49</td>
<td id="A5.T11.2.29.28.5" class="ltx_td ltx_align_right ltx_border_t">0,60</td>
<td id="A5.T11.2.29.28.6" class="ltx_td ltx_align_right ltx_border_t">+24,3 %</td>
</tr>
<tr id="A5.T11.2.30.29" class="ltx_tr">
<th id="A5.T11.2.30.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mgsm de</th>
<td id="A5.T11.2.30.29.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.30.29.3" class="ltx_td ltx_align_right">0,00</td>
<td id="A5.T11.2.30.29.4" class="ltx_td ltx_align_right">0,03</td>
<td id="A5.T11.2.30.29.5" class="ltx_td ltx_align_right">-</td>
</tr>
<tr id="A5.T11.2.31.30" class="ltx_tr">
<th id="A5.T11.2.31.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada de</th>
<td id="A5.T11.2.31.30.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.31.30.3" class="ltx_td ltx_align_right">0,11</td>
<td id="A5.T11.2.31.30.4" class="ltx_td ltx_align_right">0,33</td>
<td id="A5.T11.2.31.30.5" class="ltx_td ltx_align_right">+186,4 %</td>
</tr>
<tr id="A5.T11.2.32.31" class="ltx_tr">
<th id="A5.T11.2.32.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada es</th>
<td id="A5.T11.2.32.31.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.32.31.3" class="ltx_td ltx_align_right">0,17</td>
<td id="A5.T11.2.32.31.4" class="ltx_td ltx_align_right">0,32</td>
<td id="A5.T11.2.32.31.5" class="ltx_td ltx_align_right">+91,8 %</td>
</tr>
<tr id="A5.T11.2.33.32" class="ltx_tr">
<th id="A5.T11.2.33.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada fr</th>
<td id="A5.T11.2.33.32.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.33.32.3" class="ltx_td ltx_align_right">0,20</td>
<td id="A5.T11.2.33.32.4" class="ltx_td ltx_align_right">0,38</td>
<td id="A5.T11.2.33.32.5" class="ltx_td ltx_align_right">+90,5 %</td>
</tr>
<tr id="A5.T11.2.34.33" class="ltx_tr">
<th id="A5.T11.2.34.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">lambada it</th>
<td id="A5.T11.2.34.33.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.34.33.3" class="ltx_td ltx_align_right">0,16</td>
<td id="A5.T11.2.34.33.4" class="ltx_td ltx_align_right">0,37</td>
<td id="A5.T11.2.34.33.5" class="ltx_td ltx_align_right">+132,3 %</td>
</tr>
<tr id="A5.T11.2.35.34" class="ltx_tr">
<th id="A5.T11.2.35.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">wino de</th>
<td id="A5.T11.2.35.34.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.35.34.3" class="ltx_td ltx_align_right">0,50</td>
<td id="A5.T11.2.35.34.4" class="ltx_td ltx_align_right">0,61</td>
<td id="A5.T11.2.35.34.5" class="ltx_td ltx_align_right">+20,9 %</td>
</tr>
<tr id="A5.T11.2.36.35" class="ltx_tr">
<th id="A5.T11.2.36.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcodah de</th>
<td id="A5.T11.2.36.35.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.36.35.3" class="ltx_td ltx_align_right">0,27</td>
<td id="A5.T11.2.36.35.4" class="ltx_td ltx_align_right">0,42</td>
<td id="A5.T11.2.36.35.5" class="ltx_td ltx_align_right">+52,1 %</td>
</tr>
<tr id="A5.T11.2.37.36" class="ltx_tr">
<th id="A5.T11.2.37.36.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcodah es</th>
<td id="A5.T11.2.37.36.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.37.36.3" class="ltx_td ltx_align_right">0,28</td>
<td id="A5.T11.2.37.36.4" class="ltx_td ltx_align_right">0,43</td>
<td id="A5.T11.2.37.36.5" class="ltx_td ltx_align_right">+52,1 %</td>
</tr>
<tr id="A5.T11.2.38.37" class="ltx_tr">
<th id="A5.T11.2.38.37.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcodah fr</th>
<td id="A5.T11.2.38.37.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.38.37.3" class="ltx_td ltx_align_right">0,27</td>
<td id="A5.T11.2.38.37.4" class="ltx_td ltx_align_right">0,39</td>
<td id="A5.T11.2.38.37.5" class="ltx_td ltx_align_right">+43,7 %</td>
</tr>
<tr id="A5.T11.2.39.38" class="ltx_tr">
<th id="A5.T11.2.39.38.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcsqa de</th>
<td id="A5.T11.2.39.38.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.39.38.3" class="ltx_td ltx_align_right">0,19</td>
<td id="A5.T11.2.39.38.4" class="ltx_td ltx_align_right">0,30</td>
<td id="A5.T11.2.39.38.5" class="ltx_td ltx_align_right">+56,4 %</td>
</tr>
<tr id="A5.T11.2.40.39" class="ltx_tr">
<th id="A5.T11.2.40.39.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcsqa es</th>
<td id="A5.T11.2.40.39.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.40.39.3" class="ltx_td ltx_align_right">0,19</td>
<td id="A5.T11.2.40.39.4" class="ltx_td ltx_align_right">0,29</td>
<td id="A5.T11.2.40.39.5" class="ltx_td ltx_align_right">+57,3 %</td>
</tr>
<tr id="A5.T11.2.41.40" class="ltx_tr">
<th id="A5.T11.2.41.40.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xcsqa fr</th>
<td id="A5.T11.2.41.40.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.41.40.3" class="ltx_td ltx_align_right">0,17</td>
<td id="A5.T11.2.41.40.4" class="ltx_td ltx_align_right">0,29</td>
<td id="A5.T11.2.41.40.5" class="ltx_td ltx_align_right">+69,2 %</td>
</tr>
<tr id="A5.T11.2.42.41" class="ltx_tr">
<th id="A5.T11.2.42.41.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli de</th>
<td id="A5.T11.2.42.41.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.42.41.3" class="ltx_td ltx_align_right">0,32</td>
<td id="A5.T11.2.42.41.4" class="ltx_td ltx_align_right">0,48</td>
<td id="A5.T11.2.42.41.5" class="ltx_td ltx_align_right">+51,0 %</td>
</tr>
<tr id="A5.T11.2.43.42" class="ltx_tr">
<th id="A5.T11.2.43.42.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli en</th>
<td id="A5.T11.2.43.42.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.43.42.3" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.43.42.4" class="ltx_td ltx_align_right">0,52</td>
<td id="A5.T11.2.43.42.5" class="ltx_td ltx_align_right">+5,3 %</td>
</tr>
<tr id="A5.T11.2.44.43" class="ltx_tr">
<th id="A5.T11.2.44.43.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli es</th>
<td id="A5.T11.2.44.43.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.44.43.3" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.44.43.4" class="ltx_td ltx_align_right">0,48</td>
<td id="A5.T11.2.44.43.5" class="ltx_td ltx_align_right">+41,0 %</td>
</tr>
<tr id="A5.T11.2.45.44" class="ltx_tr">
<th id="A5.T11.2.45.44.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xnli fr</th>
<td id="A5.T11.2.45.44.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.45.44.3" class="ltx_td ltx_align_right">0,37</td>
<td id="A5.T11.2.45.44.4" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.45.44.5" class="ltx_td ltx_align_right">+30,3 %</td>
</tr>
<tr id="A5.T11.2.46.45" class="ltx_tr">
<th id="A5.T11.2.46.45.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xstance de</th>
<td id="A5.T11.2.46.45.2" class="ltx_td ltx_align_right">acc</td>
<td id="A5.T11.2.46.45.3" class="ltx_td ltx_align_right">0,49</td>
<td id="A5.T11.2.46.45.4" class="ltx_td ltx_align_right">0,52</td>
<td id="A5.T11.2.46.45.5" class="ltx_td ltx_align_right">+6,0 %</td>
</tr>
<tr id="A5.T11.2.47.46" class="ltx_tr">
<th id="A5.T11.2.47.46.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">xstance de</th>
<td id="A5.T11.2.47.46.2" class="ltx_td ltx_align_right">f1</td>
<td id="A5.T11.2.47.46.3" class="ltx_td ltx_align_right">0,34</td>
<td id="A5.T11.2.47.46.4" class="ltx_td ltx_align_right">0,47</td>
<td id="A5.T11.2.47.46.5" class="ltx_td ltx_align_right">+41,2 %</td>
</tr>
<tr id="A5.T11.2.48.47" class="ltx_tr">
<th id="A5.T11.2.48.47.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">xstance de</th>
<td id="A5.T11.2.48.47.2" class="ltx_td ltx_align_right ltx_border_bb">recall</td>
<td id="A5.T11.2.48.47.3" class="ltx_td ltx_align_right ltx_border_bb">0,49</td>
<td id="A5.T11.2.48.47.4" class="ltx_td ltx_align_right ltx_border_bb">0,52</td>
<td id="A5.T11.2.48.47.5" class="ltx_td ltx_align_right ltx_border_bb">+7,7 %</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A5.T11.3.1.1" style="font-size:90%;">Table 11</span>:</span><span class="ltx_text" id="A5.T11.4.2" style="font-size:90%;">Differences between our best and worst performing tokenizers per task. </span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2310.08753" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2310.08754" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2310.08754">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.08754" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2310.08755" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 01:31:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
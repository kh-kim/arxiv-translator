{
    "2401.05654v1": {
        "paper_id": "2401.05654v1",
        "abs_url": "https://arxiv.org/abs/2401.05654v1",
        "pdf_url": "https://arxiv.org/pdf/2401.05654v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2401.05654v1_Towards_Conversational_Diagnostic_AI.pdf",
        "title": "Towards Conversational Diagnostic AI",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Tao Tu",
            "Anil Palepu",
            "Mike Schaekermann",
            "Khaled Saab",
            "Jan Freyberg",
            "Ryutaro Tanno",
            "Amy Wang",
            "Brenna Li",
            "Mohamed Amin",
            "Nenad Tomasev",
            "Shekoofeh Azizi",
            "Karan Singhal",
            "Yong Cheng",
            "Le Hou",
            "Albert Webson",
            "Kavita Kulkarni",
            "S Sara Mahdavi",
            "Christopher Semturs",
            "Juraj Gottweis",
            "Joelle Barral",
            "Katherine Chou",
            "Greg S Corrado",
            "Yossi Matias",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "abstract": "AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.",
        "comments": "46 pages, 5 figures in main text, 19 figures in appendix",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/towards-conversational-diagnostic-ai",
        "bibtex": "@misc{tu2024conversational,\n      title={Towards Conversational Diagnostic AI}, \n      author={Tao Tu and Anil Palepu and Mike Schaekermann and Khaled Saab and Jan Freyberg and Ryutaro Tanno and Amy Wang and Brenna Li and Mohamed Amin and Nenad Tomasev and Shekoofeh Azizi and Karan Singhal and Yong Cheng and Le Hou and Albert Webson and Kavita Kulkarni and S Sara Mahdavi and Christopher Semturs and Juraj Gottweis and Joelle Barral and Katherine Chou and Greg S Corrado and Yossi Matias and Alan Karthikesalingam and Vivek Natarajan},\n      year={2024},\n      eprint={2401.05654},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}"
    }
}
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Towards Conversational Diagnostic AI</title>
<!--Generated on Fri Feb  2 20:09:56 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.05654v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.05654v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.05654v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.05654v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S1" title="1 Introduction ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2" title="2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>AMIE: An LLM based AI System for Diagnostic Dialogue</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS1" title="2.1 Real-world Datasets for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Real-world Datasets for AMIE</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS1.SSS0.Px1" title="Medical Reasoning. ‣ 2.1 Real-world Datasets for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Medical Reasoning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS1.SSS0.Px2" title="Long-form Medical Question Answering. ‣ 2.1 Real-world Datasets for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Long-form Medical Question Answering.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS1.SSS0.Px3" title="Medical Summarization. ‣ 2.1 Real-world Datasets for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Medical Summarization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS1.SSS0.Px4" title="Real-world Dialogue. ‣ 2.1 Real-world Datasets for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Real-world Dialogue.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2" title="2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Simulated Dialogue Learning Environment and Self-play for AMIE</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS0.Px1" title="Simulated Dialogues. ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Simulated Dialogues.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS1" title="2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Simulated Dialogue Data Curation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS1.Px1" title="Vignette Generator. ‣ 2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Vignette Generator.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS1.Px2" title="Simulated Dialogue Generator. ‣ 2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Simulated Dialogue Generator.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS1.Px3" title="Self-play Critic. ‣ 2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Self-play Critic.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS3" title="2.3 Instruction Fine-tuning ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Instruction Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS4" title="2.4 Chain-of-reasoning for Online Inference ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Chain-of-reasoning for Online Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3" title="3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS1" title="3.1 Objective Structured Clinical Examination ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Objective Structured Clinical Examination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS2" title="3.2 Remote OSCE Study Design ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Remote OSCE Study Design</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS2.SSS1" title="3.2.1 Online Text-based Consultation ‣ 3.2 Remote OSCE Study Design ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Online Text-based Consultation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS2.SSS2" title="3.2.2 Post-questionnaires ‣ 3.2 Remote OSCE Study Design ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Post-questionnaires</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS2.SSS3" title="3.2.3 Specialist Physician Evaluation ‣ 3.2 Remote OSCE Study Design ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Specialist Physician Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS3" title="3.3 Auto-evaluation ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Auto-evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.SS4" title="3.4 Statistical Analysis ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Statistical Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4" title="4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1" title="4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Diagnostic Accuracy</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1.SSS1" title="4.1.1 AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1.SSS1.Px1" title="Accuracy by Specialty. ‣ 4.1.1 AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Accuracy by Specialty.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1.SSS2" title="4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information.</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1.SSS2.Px1" title="Auto-evaluation Accuracy. ‣ 4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Auto-evaluation Accuracy.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1.SSS2.Px2" title="Isolating the Source of Performance Gains. ‣ 4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Isolating the Source of Performance Gains.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS1.SSS2.Px3" title="Efficiency of Information Acquisition. ‣ 4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Efficiency of Information Acquisition.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS2" title="4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Conversation Quality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS2.SSS1" title="4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>AMIE surpassed PCPs in conversation quality, per specialists and patient actors.</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS2.SSS1.Px1" title="Patient Actor Ratings. ‣ 4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Patient Actor Ratings.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS2.SSS1.Px2" title="Specialist Physician Ratings. ‣ 4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Specialist Physician Ratings.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS2.SSS2" title="4.2.2 Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE.</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.SS2.SSS2.Px1" title="Auto-evaluation of Conversation Ratings. ‣ 4.2.2 Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Auto-evaluation of Conversation Ratings.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S5" title="5 Related Work ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S5.SS1" title="5.1 Clinical History-taking and the Diagnostic Dialogue ‣ 5 Related Work ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Clinical History-taking and the Diagnostic Dialogue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S5.SS2" title="5.2 Conversational AI and Goal-oriented Dialogue ‣ 5 Related Work ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Conversational AI and Goal-oriented Dialogue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S5.SS3" title="5.3 AI for Medical Consultations and Diagnostic Dialogue ‣ 5 Related Work ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>AI for Medical Consultations and Diagnostic Dialogue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S5.SS4" title="5.4 Evaluation of Diagnostic Dialogue ‣ 5 Related Work ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Evaluation of Diagnostic Dialogue</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6" title="6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px1" title="Diagnostic Performance. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Diagnostic Performance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px2" title="Conversational Performance. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Conversational Performance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px3" title="Simulated Dialogue. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Simulated Dialogue.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px4" title="Evaluation Framework. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Evaluation Framework.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px5" title="Breadth of Evaluation. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Breadth of Evaluation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px6" title="Fairness and Bias. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Fairness and Bias.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S6.SS0.SSS0.Px7" title="Deployment. ‣ 6 Discussion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title">Deployment.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S7" title="7 Conclusion ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: kantlipsum</li>
<li>failed: scalerel</li>
<li>failed: etoc</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2401.05654v1 [cs.AI] 11 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Conversational Diagnostic AI</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Tu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anil Palepu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mike Schaekermann
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Khaled Saab
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Freyberg
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryutaro Tanno
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amy Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Brenna Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohamed Amin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Nenad Tomasev
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shekoofeh Azizi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karan Singhal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Le Hou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Albert Webson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Kavita Kulkarni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">S. Sara Mahdavi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Semturs
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Juraj Gottweis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joelle Barral
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Katherine Chou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Greg S. Corrado
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yossi Matias
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Alan Karthikesalingam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vivek Natarajan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="1.1"><span class="ltx_text" id="1.1.1" lang="en">At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians’ expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="2.2"><span class="ltx_text" id="2.2.1" lang="en">AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE’s performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The dialogue between the physician and the patient is fundamental to effective and compassionate care. The medical interview has been termed “the most powerful, sensitive, and most versatile instrument available to the physician”&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx1" title="">1</a>]</cite>. In some settings, it is believed that 60-80% of diagnoses are made through clinical history-taking alone&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx6" title="">6</a>]</cite>. The physician-patient dialogue extends beyond history-taking and diagnosis; it is a complex interaction which establishes rapport and trust, serves as a tool for addressing health needs and can empower patients to make informed decisions that account for their preferences, expectations, and concerns&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx7" title="">7</a>]</cite>. Clinicians wield considerable skills in clinical history-taking and the wider “diagnostic dialogue”, but access to this expertise remains episodic and globally scarce&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx8" title="">8</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent progress in general-purpose large language models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#biba.bibx2" title="">119</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx11" title="">11</a>]</cite> has shown that artificial intelligence (AI) systems have capabilities to plan, reason, and incorporate relevant context to hold naturalistic conversations. This progress affords an opportunity to rethink the possibilities of AI in medicine towards the development of fully interactive conversational AI. Such medical AI systems would understand clinical language, intelligently acquire information under uncertainty, and engage in natural, diagnostically useful medical conversations with patients and those who care for them. The potential real-world utility of AI systems capable of clinical and diagnostic dialogue is broad, as the development of such capabilities might improve access to diagnostic and prognostic expertise, to improved quality, consistency, availability, and affordability of care, and to help realize better health outcomes (particularly for populations facing healthcare disparities).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="661" id="S1.F1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Overview of contributions.<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1"> AMIE is a conversational medical AI optimised for diagnostic dialogue. AMIE is instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical reasoning, question answering, and summarization datasets. Notably, we designed a self-play based simulated dialogue environment with automated feedback mechanisms to scale AMIE’s capabilities across various medical contexts and specialities. Specifically, this iterative self-improvement process consisted of two self-play loops: (1) An “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; (2) An “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. During online inference, AMIE used a chain-of-reasoning strategy to progressively refine its response conditioned on the current conversation to arrive at an accurate and grounded reply to the patient in each dialogue turn.
We designed and conducted a blinded remote Objective Structured Clinical Examination (OSCE) with validated simulated patient actors interacting with AMIE or Primary Care Physicians (PCPs) via a text interface. Across multiple axes corresponding to both specialist physician (28 out of 32) and patient actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, while LLMs have been shown to encode clinical knowledge and proven capable of highly accurate single-turn medical question-answering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx14" title="">14</a>]</cite>, their conversational capabilities have been tailored to domains outside clinical medicine&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx16" title="">16</a>]</cite>. Prior work in LLMs for health &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx18" title="">18</a>]</cite> has not yet rigorously examined the clinical history-taking and diagnostic dialogue capabilities of AI systems or contextualized this by comparison to the extensive capabilities of expert clinicians.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Clinical history-taking and diagnostic dialogue through which clinicians derive diagnosis and management plans represent a complex skill&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx19" title="">19</a>]</cite> whose optimal conduct is highly dependent on context. Thus, multiple evaluation axes are needed to assess the quality of a diagnostic dialogue, including the structure and completeness of the elicited history, diagnostic accuracy, the appropriateness of management plans and their rationale, and patient-centred considerations such as relationship-building, respect for the individual and communication efficacy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx20" title="">20</a>]</cite>. If the conversational potential of LLMs is to be realized in medicine, there is a significant unmet need to better optimize development and evaluation of medical AI systems for characteristics such as these, which are unique to history-taking and diagnostic dialogue between clinicians and patients.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we detail our progress towards a conversational medical AI system for clinical history-taking and diagnostic reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our key contributions are summarized as:
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduced AMIE (Articulate Medical Intelligence Explorer), an LLM based AI system optimized for clinical history-taking and diagnostic dialogue.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">To scale AMIE across a multitude of specialties and scenarios, we developed a novel self-play based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE’s diagnostic accuracy and conversation quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We developed a pilot evaluation rubric to assess the history-taking, diagnostic reasoning, communication skills and empathy of diagnostic conversational medical AI, encompassing both clinician-centred and patient-centred metrics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We designed and conducted a blinded remote OSCE study with 149 case scenarios from clinical providers in Canada, the UK, and India, enabling randomized and counterbalanced comparison of AMIE to PCPs when performing consultations with validated patient actors. AMIE exhibited superior diagnostic accuracy compared to PCPs as assessed by various measures (e.g., top-1 and top-3 accuracy of the differential diagnosis list). Across 28 out of 32 evaluation axes from the specialist physician perspective and 24 out of 26 evaluation axes from the patient actor perspective, AMIE was rated superior to PCPs while being non-inferior on the rest.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We performed a range of ablations to further understand and characterize the capabilities of AMIE, highlighted important limitations, and proposed key next steps for real-world clinical translation of AMIE.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our research has important limitations, most notably that we utilized a text-chat interface, which although enabling potentially large-scale interaction between patients and LLMs specialized for diagnostic dialogue, was unfamiliar to PCPs for remote consultation. Thus our study should not be regarded as representative of usual practice in (tele)medicine.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="S1.F2.g1" src="x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F2.4.2" style="font-size:90%;">Overview of randomized study design.<span class="ltx_text ltx_font_medium" id="S1.F2.4.2.1"> A primary care physician (PCP) and AMIE perform (in a randomized order) a virtual remote Objective Structured Clinical Examination (OSCE) with simulated patients via online multi-turn synchronous text chat and produce answers to a post-questionnaire. Both the PCP and AMIE are then evaluated by both the patient actors as well as specialist physicians.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>AMIE: An LLM based AI System for Diagnostic Dialogue</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In the following sections, we describe the real-world datasets, simulated self-play environment, fine-tuning process, and inference time chain-of-reasoning that we designed to optimize AMIE for diagnostic conversation capabilities and clinical communication skills.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Real-world Datasets for AMIE</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">AMIE was developed using a diverse suite of real-world datasets including multiple-choice medical question-answering, expert-curated long-form medical reasoning, electronic health record (EHR) note summaries, and large-scale transcribed medical conversation interactions. As described in detail below, in addition to dialogue generation tasks, the training task mixture for AMIE consisted of medical question-answering, reasoning, and summarization tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Medical Reasoning.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">We used the MedQA (multiple-choice) dataset consisting of US Medical Licensing Examination (USMLE) multiple-choice style open domain questions with four or five possible answers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx21" title="">21</a>]</cite>. The training set consisted of 11,450 questions and the test set had 1,273 questions. We also curated 191 MedQA questions from the training set where clinical experts crafted step-by-step reasoning leading to the correct answer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx13" title="">13</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Long-form Medical Question Answering.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">The dataset used here consisted of expert-crafted long-form responses to 64 questions from HealthSearchQA, LiveQA, and Medication QA in MultiMedBench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx12" title="">12</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Medical Summarization.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">A dataset consisting of 65 clinician-written summaries of medical notes from MIMIC-III, a large, publicly available database containing medical records of intensive care unit patients&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx22" title="">22</a>]</cite>, was used as additional training data for AMIE. MIMIC-III contains approximately 2 million notes spanning 13 types including cardiology, respiratory, radiology, physician, general, discharge, case management, consult, nursing, pharmacy, nutrition, rehabilitation and social work. 5 notes from each category were selected, with a minimum total length of 400 tokens and at least one nursing note per patient. Clinicians were instructed to write abstractive summaries of individual medical notes, capturing key information while also permitting the inclusion of new informative and clarifying phrases and sentences not present in the original note.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Real-world Dialogue.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p1.2">Here, we used a de-identified dataset licensed from a dialogue research organisation comprising 98,919 audio transcripts of medical conversations during in-person clinical visits from over 1,000 clinicians over a 10-year period in the United States&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx23" title="">23</a>]</cite>. It covered 51 medical specialties (primary care, rheumatology, hematology, oncology, internal medicine and psychiatry among others) and 168 medical conditions and visit reasons (type II diabetes, rheumatoid arthritis, asthma, depression among the common conditions). Audio transcripts contained utterances from different speaker roles such as doctors, patients, and nurses. On average a conversation had 149.8 turns (<math alttext="P_{0.25}=75.0" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px4.p1.1.m1.1a"><mrow id="S2.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><msub id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml">P</mi><mn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml">0.25</mn></msub><mo id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">75.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1"><eq id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1"></eq><apply id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2">𝑃</ci><cn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3">0.25</cn></apply><cn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3">75.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px4.p1.1.m1.1c">P_{0.25}=75.0</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px4.p1.1.m1.1d">italic_P start_POSTSUBSCRIPT 0.25 end_POSTSUBSCRIPT = 75.0</annotation></semantics></math>, <math alttext="P_{0.75}=196.0" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px4.p1.2.m2.1a"><mrow id="S2.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><msub id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">P</mi><mn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">0.75</mn></msub><mo id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">196.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1"><eq id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2">𝑃</ci><cn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3">0.75</cn></apply><cn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3">196.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px4.p1.2.m2.1c">P_{0.75}=196.0</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px4.p1.2.m2.1d">italic_P start_POSTSUBSCRIPT 0.75 end_POSTSUBSCRIPT = 196.0</annotation></semantics></math>). For each conversation, the metadata contained information about patient demographics, reason for the visit (follow-up for pre-existing condition, acute needs, annual exam and more), and diagnosis type (new, existing or other unrelated). We refer to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx23" title="">23</a>]</cite> for more details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS0.Px4.p2">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p2.1">For this study, we selected dialogues involving only doctors and patients, but not other roles such as nurses. During preprocessing, we removed paraverbal annotations such as “[LAUGHING]” and “[INAUDIBLE]” from the transcripts. We then divided the dataset into training (90%) and validation (10%) sets using stratified sampling based on condition categories and reasons for visits, resulting in 89,027 conversations for training and 9,892 for validation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Simulated Dialogue Learning Environment and Self-play for AMIE</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">While passively collecting and transcribing real-world dialogues from in-person clinical visits is feasible, two substantial challenges limit its effectiveness in training LLMs for medical conversations: (1) existing real-world data often fails to capture the vast range of medical conditions and scenarios, hindering its scalability and comprehensiveness; (2) the data derived from real-world dialogue transcripts tends to be noisy, containing ambiguous language (including slang, jargon, and sarcasm), interruptions, ungrammatical utterances, and implicit references. This in turn, may limit AMIE’s knowledge, capabilities, and applicability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To address these limitations, we designed a self-play based simulated learning environment for diagnostic medical dialogues in a virtual care setting, enabling us to scale AMIE’s knowledge and capabilities across a multitude of medical conditions and contexts. We used this environment to iteratively fine-tune AMIE with an evolving set of simulated dialogues in addition to the static corpus of medical QA, reasoning, summarization, and real-world dialogue data described above (see <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">This process consisted of two self-play loops:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">An “inner” self-play loop</span> where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">An “outer” self-play loop</span> where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a continuous learning cycle.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogues.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">At each iteration of fine-tuning, we produced 11,686 dialogues, stemming from 5,230 different medical conditions. Conditions were selected from three datasets:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Health QA dataset</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx12" title="">12</a>]</cite> which contained 613 common medical conditions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">MalaCards Human Disease Database<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote1.1.1.1">1</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json" title="">https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json</a></span></span></span></span> which contained 18,455 less common disease conditions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i3.p1.1.1">MedicineNet Diseases &amp; Conditions Index<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json" title="">https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json</a></span></span></span></span> which contained 4,617 less common conditions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p2.2">At each self-play iteration, four conversations were generated from each of the 613 common conditions, while two conversations were generated from each of the 4,617 less common conditions randomly chosen from
MedicineNet and MalaCards. The average simulated dialogue conversation length was 21.28 turns (<math alttext="P_{0.25}=19.0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.1.m1.1"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><mrow id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3.cmml">0.25</mn></msub><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">19.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><eq id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1"></eq><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2">𝑃</ci><cn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3">0.25</cn></apply><cn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3">19.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">P_{0.25}=19.0</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p2.1.m1.1d">italic_P start_POSTSUBSCRIPT 0.25 end_POSTSUBSCRIPT = 19.0</annotation></semantics></math>, <math alttext="P_{0.75}=25.0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.2.m2.1"><semantics id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mrow id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3.cmml">0.75</mn></msub><mo id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml">25.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1"><eq id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1"></eq><apply id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2">𝑃</ci><cn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3">0.75</cn></apply><cn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3">25.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.2.m2.1c">P_{0.75}=25.0</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p2.2.m2.1d">italic_P start_POSTSUBSCRIPT 0.75 end_POSTSUBSCRIPT = 25.0</annotation></semantics></math>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p3.1">Using simulated dialogues allowed us to address the limited availability of high-quality, labelled real-world conversation data and improved the model’s generalization and adaptability to diverse medical contexts. By leveraging this self-play paradigm, AMIE could continuously learn and refine its conversational and diagnostic capabilities during patient interactions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Simulated Dialogue Data Curation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">In order to produce high-quality simulated dialogues at scale, we developed a novel multi-agent framework which comprised three key components:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i1.p1.1.1">Vignette Generator</span>: AMIE leverages web searches to craft unique patient vignettes given a specific medical condition.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i2.p1.1.1">Simulated Dialogue Generator</span>: Three LLM agents play the roles of patient agent, doctor agent, and moderator, engaging in a turn-by-turn dialogue simulating realistic diagnostic interactions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i3.p1.1.1">Self-play Critic</span>: A fourth LLM agent acts as a critic to give feedback to the doctor agent for self-improvement. Notably, AMIE acted as all agents in this framework. We describe each component in detail below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Vignette Generator.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">The vignette generator aimed to create varied and realistic patient scenarios at scale, which could be subsequently used as context for generating simulated doctor-patient dialogues thereby allowing AMIE to undergo a training process emulating exposure to a greater number of conditions and patient backgrounds. The patient vignette (scenario) included essential background information such as patient demographics, symptoms, past medical history, past surgical history, past social history, and patient questions, as well as an associated diagnosis and management plan.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p2.1">For a given condition, patient vignettes were constructed using the following process. First, we retrieved 60 passages (20 each) on the range of demographics, symptoms, and management plans associated with the condition from using an internet search engine. To ensure these passages were relevant to the given condition, we used the general-purpose LLM, PaLM-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#biba.bibx2" title="">119</a>]</cite>, to filter these retrieved passages, removing any passages deemed unrelated to the given condition. We then prompted AMIE to generate plausible patient vignettes aligned with the demographics, symptoms, and management plans retrieved from the filtered passages, by providing a one-shot exemplar to enforce a particular vignette format. The prompts for each of these steps are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.Px1.p3">
<svg class="ltx_picture" height="340.28" id="S2.SS2.SSS1.Px1.p3.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,340.28) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.000000"><path d="M 0 5.91 L 0 334.37 C 0 337.63 2.64 340.28 5.91 340.28 L 594.09 340.28 C 597.36 340.28 600 337.63 600 334.37 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.000000"><path d="M 1.97 5.91 L 1.97 334.37 C 1.97 336.54 3.73 338.31 5.91 338.31 L 594.09 338.31 C 596.27 338.31 598.03 336.54 598.03 334.37 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="312.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3" style="width:402.3pt;">
<span class="ltx_para" id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">Search Retrieval Template</span></span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2">What are the specific patient demographics/symptoms/management plan for the condition [Condition]?</span>
</span>
<span class="ltx_para" id="S2.SS2.SSS1.Px1.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2">
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.Px1.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.1.1">Passage Filtering Template</span></span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.2">For the clinical condition, [Condition], is the following a good description of common demographics/symptoms/management plans (Yes/No)?</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.3">Description: [Retrieved Passage]</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.4">Answer (Yes/No):</span>
</span>
<span class="ltx_para" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3">
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.1.1">Vignette Generation Template</span></span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.2">The following are several passages about the demographics, symptoms, and management plan for a given condition. Generate 2 different patient vignettes consistent with these passages. Follow the format of the given example (just list N/A if a particular field is unavailable).</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.3">Condition: [Condition]</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.4">Demographic Passages: [Retrieved Demographic Passages]</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.5">Symptoms Passages: [Retrieved Symptom Passages]</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.6">Management Plan Passages: [Retrieved Management Plan Passages]</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.7">Example Format: [Oneshot example]</span>
<span class="ltx_p" id="S2.SS2.SSS1.Px1.p3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.8">Patient Vignettes for [Condition]:</span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogue Generator.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">Given a patient vignette detailing a specific medical condition, the simulated dialogue generator was designed to simulate a realistic dialogue between a patient and a doctor in an online chat setting where in-person physical examination may not be feasible.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p2.1">Three specific LLM agents (patient agent, doctor agent, and moderator), each played by AMIE, were tasked with communicating amongst each other to generate the simulated dialogues. Each agent had distinct instructions. The patient agent embodied the individual experiencing the medical condition outlined in the vignette. Their role involved truthfully responding to the doctor agent’s inquiries as well as raising any additional questions or concerns they may have had. The doctor agent played the role of an empathetic clinician seeking to comprehend the patient’s medical history within the online chat environment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx24" title="">24</a>]</cite>. Their objective was to formulate questions that could effectively reveal the patient’s symptoms and background, leading to an accurate diagnosis and an effective treatment plan. The moderator continually assessed the ongoing dialogue between the patient agent and doctor agent, determining when the conversation had reached a natural conclusion.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p3.1">The turn-by-turn dialogue simulation started with the doctor agent initiating the conversation: “Doctor: So, how can I help you today?”. Following this, the patient agent responded, and their answer was incorporated into the ongoing dialogue history. Subsequently, the doctor agent formulated a response based on the updated dialogue history. This response was then appended to the conversation history. The conversation progressed until the moderator detected the dialogue had reached a natural conclusion, when the doctor agent had provided a differential diagnosis, treatment plan, and adequately addressed any remaining patient agent questions, or if either agent initiated a farewell.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.Px2.p4">
<svg class="ltx_picture" height="261.48" id="S2.SS2.SSS1.Px2.p4.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,261.48) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.000000"><path d="M 0 5.91 L 0 255.58 C 0 258.84 2.64 261.48 5.91 261.48 L 594.09 261.48 C 597.36 261.48 600 258.84 600 255.58 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.000000"><path d="M 1.97 5.91 L 1.97 255.58 C 1.97 257.75 3.73 259.51 5.91 259.51 L 594.09 259.51 C 596.27 259.51 598.03 257.75 598.03 255.58 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="233.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="S2.SS2.SSS1.Px2.p4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3" style="width:402.3pt;">
<span class="ltx_para" id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">Patient Agent Instruction:</span></span>
<span class="ltx_p" id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2">You are a patient chatting with a doctor over an online chat interface. The doctor has never met you before.
&lt;patient vignette&gt;
Respond to the doctor’s questions honestly as they interview you, asking any questions that may come up.</span>
</span>
<span class="ltx_para" id="S2.SS2.SSS1.Px2.p4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2">
<span class="ltx_p" id="S2.SS2.SSS1.Px2.p4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.Px2.p4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.1.1">Doctor Agent Instruction:</span></span>
<span class="ltx_p" id="S2.SS2.SSS1.Px2.p4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.2">You are an empathetic clinician asking a patient about their medical history over an online chat interface. You know nothing about the patient in advance.
Respond to the patient with a single-turn response to better understand their history and symptoms. Do not ask more than two questions. If the patient asks a question, be sure to answer it appropriately.</span>
</span>
<span class="ltx_para" id="S2.SS2.SSS1.Px2.p4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3">
<span class="ltx_p" id="S2.SS2.SSS1.Px2.p4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.Px2.p4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.1.1">Moderator Instruction:</span></span>
<span class="ltx_p" id="S2.SS2.SSS1.Px2.p4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.2">The following is a conversation between a doctor and a patient:
&lt;dialog&gt; 
The conversation should only come to an end if the doctor has finished giving the patient a diagnosis and treatment plan and the patient has no questions left. A conversation also comes to an end if the doctor or patient says goodbye.
Question: has the conversation come to an end? Yes or No.</span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Self-play Critic.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">To ensure high-quality dialogues, we implemented a tailored self-play&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx25" title="">25</a>]</cite> framework specifically for self-improvement of diagnostic conversations. This framework introduced a fourth LLM agent, acting as a “critic” which was also played by AMIE and aware of the ground truth diagnosis, to provide in-context feedback to the doctor agent and enhance its performance in subsequent conversations. The critic agent evaluated the doctor agent’s responses based on the following criteria:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1">The doctor agent exhibits empathy and professionalism while addressing the patient agent’s latest questions or comments in a concise manner.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1">The doctor agent avoids asking too many or repetitive questions (about information already acquired), focusing on a maximum of one or two per response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i3.p1">
<p class="ltx_p" id="S2.I4.i3.p1.1">The responses should not reveal that the doctor agent is an AI chatbot. They should flow naturally, maintain factual accuracy, and facilitate further engagement from the patient.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i4.p1">
<p class="ltx_p" id="S2.I4.i4.p1.1">The doctor agent asks sufficient questions to identify at least two of the most likely differential diagnoses. They further refine their understanding through targeted questions towards the ground truth diagnosis and offer the corresponding treatment.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p2.1">Following the critic’s feedback, the doctor agent incorporated the suggestions to improve its responses in subsequent rounds of dialogue with the same patient agent from scratch. Notably, the doctor agent retained access to its previous dialogue history at each new round. This self-improvement process was repeated twice to generate the dialogues used for each iteration of fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Instruction Fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">AMIE, built upon the base LLM PaLM 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#biba.bibx2" title="">119</a>]</cite>, was instruction fine-tuned to enhance its capabilities for medical dialogue and reasoning. We refer to the PaLM-2 technical report for more details on the base LLM architecture.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">We employed task-specific instructions to fine-tune AMIE in playing either the patient or doctor role within medical dialogues, performing medical question answering and reasoning, and summarizing EHR notes. While the first round of fine-tuning from the base LLM only used the static datasets, subsequent rounds of fine-tuning leveraged the simulated dialogues generated through the self-play inner loop as described in <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS1" title="2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">2.2.1</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">For dialogue generation tasks, AMIE was trained to predict the next conversational turn based on all previous interactions, assuming either the doctor or patient role. When playing the patient agent, AMIE was prompted to reply to the doctor agent’s questions about their symptoms, drawing upon information provided in patient scenarios. These scenarios included patient vignettes (see <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S2.SS2.SSS1.Px1" title="Vignette Generator. ‣ 2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">2.2.1</span></a>) for simulated dialogues or metadata such as demographics, visit reason, and diagnosis type for the real-world dialogue dataset. In the doctor agent role, AMIE was prompted to act as an empathetic clinician, interviewing patients about their medical history and symptoms to ultimately arrive at an accurate diagnosis. From each dialogue, we sampled on average 3 turns for each the doctor and patient roles as the target turns to predict based on the conversation leading up to that target turn. Target turns were randomly sampled from all turns in the dialogue that had a minimum length of 30 characters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Similarly, for the EHR note summarization task, AMIE was provided with a clinical note and prompted to generate a summary of the note. Medical reasoning/QA and long-form response generation tasks followed the same setup as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx13" title="">13</a>]</cite>. Notably, all tasks except dialogue generation and long-form response generation incorporated few-shot (1-5) exemplars in addition to task-specific instructions for additional context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Chain-of-reasoning for Online Inference</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">To address the core challenge in diagnostic dialogue - effectively acquiring information under uncertainty to enhance diagnostic accuracy and confidence while maintaining positive rapport with the patient - AMIE employed a chain-of-reasoning strategy before generating a response in each dialogue turn. Here, “chain-of-reasoning” refers to a series of sequential model calls, each dependent on the outputs of prior steps. Specifically, we used a three-step reasoning process, described as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<ol class="ltx_enumerate" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i1.p1.1.1">Analyzing patient information:</span> Given the current conversation history, AMIE was instructed to 1) summarize the positive and negative symptoms of the patient as well as any relevant medical/family/social history and demographic information, 2) produce a current differential diagnosis, 3) note missing information needed for a more accurate diagnosis and 4) assess confidence in the current differential and highlight its urgency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i2.p1.1.1">Formulating response and action:</span> Building upon the conversation history and the output of step 1, AMIE performed the following: 1) Generate a response to the patient’s last message and formulate further questions to acquire missing information and refine the differential diagnosis. 2) If necessary, recommend immediate action, such as an emergency room visit. If confident in the diagnosis based on available information, present the differential.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I5.i3.p1">
<p class="ltx_p" id="S2.I5.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i3.p1.1.1">Refining the response:</span> AMIE revises its previous output to meet specific criteria based on the conversation history and outputs from earlier steps. The criteria are primarily related to factuality and formatting of the response (e.g., avoid factual inaccuracies on patient facts and unnecessary repetition, show empathy, and display in a clear format).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">This chain-of-reasoning strategy enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Prior works developing models for clinical dialogue have focused on metrics such as the accuracy of note-to-dialogue or dialogue-to-note generations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx27" title="">27</a>]</cite>, or natural language generation metrics such as BLEU or ROUGE scores that fail to capture the clinical quality of a consultation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx29" title="">29</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In contrast to these prior works we sought to anchor our human evaluation in criteria more commonly used for evaluating the quality of physicians’ expertise in history-taking, including their communication skills in consultation. We derived a framework from principles published in reviews of the consensus for best practices for patient-centered communication (PCCBP) in medical interviews&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx20" title="">20</a>]</cite>, criteria examined for history-taking skills by the Royal College of Physicians in the UK as part of their Practical Assessment of Clinical Examination Skills (PACES)<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets" title="">https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx30" title="">30</a>]</cite>, and criteria proposed by the UK General Medical Council Patient Questionnaire (GMCPQ)<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf" title="">https://www.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf</a></span></span></span> for doctors seeking patient feedback as part of professional re-validation<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources/collecting-colleague-and-patient-feedback-for-revalidation" title="">https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources/collecting-colleague-and-patient-feedback-for-revalidation</a></span></span></span>. We iterated upon these criteria to refine items for inclusion and derived pilot scales and instructions for assessment by using focus groups and interviews with clinicians and OSCE examiners based in the UK, Canada, US, and India.
Our resulting pilot framework enabled assessment from two perspectives: clinician (board-certified physicians) and lay raters (patient actors). The framework included consideration of consultation quality, structure and completeness, the roles, responsibilities, and skills of the interviewer (Tables <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:gmcpq_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>, and <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:diagnosis_management_rubric_details</span>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objective Structured Clinical Examination</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Objective Structured Clinical Examination (OSCE) is a practical assessment format used in healthcare to assess clinical skills and competencies in a standardized and objective fashion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx33" title="">33</a>]</cite>.
It differs from traditional written or oral exams that focus primarily on theoretical knowledge and instead aims to provide an environment in which the skills of real-world clinical practice might be assessed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The OSCE is typically divided into multiple stations (often 8-12), each simulating a real-life clinical scenario enacted by standardized patient actors trained to portray specific symptoms or conditions based on pre-defined scenario descriptions.
At each station, students are given specific tasks to perform, such as taking a clinical history, or making a diagnosis.
Each station has a set time limit, ensuring fairness and efficient assessment.
Trained examiners observe students’ performance at each station using a pre-defined checklist or marking scheme.
They assess clinical skills like communication, history-taking, physical examination techniques, clinical reasoning, and decision-making.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Remote OSCE Study Design</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To compare AMIE’s performance to that of real clinicians, we conducted a randomized crossover study of blinded consultations in the style of a remote OSCE.
Our OSCE study involved 20 board-certified primary care physicians (PCPs) and 20 validated patient actors, 10 each from India and Canada, respectively, to partake in online text-based consultations.
PCPs had between 3 and 25 years of post-residency experience (median 7 years).
Patient actors comprised of a mix of medical students, residents, and nurse practitioners with experience in OSCE participation.
We sourced 149 scenario packs from India (75), Canada (60), and the UK (14).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The scenario packs and simulated patients in our study were prepared by two OSCE laboratories (one each in Canada and India), each affiliated to a medical school and with extensive experience in preparing scenario packs and simulated patients for OSCE examinations. UK scenario packs were sourced from the samples provided on the MRCPUK website. Each scenario pack was associated with a ground truth diagnosis and a set of acceptable diagnoses. The scenario packs covered conditions from cardiovascular (29), respiratory (30), gastroenterology (31), neurology (30), urology, obstetric, and gynecology domains (15), and internal medicine (14). Pediatric or psychiatry domains were excluded from this study, as were intensive care or inpatient case management scenarios.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Indian patient actors played the roles in all India scenario packs and 7 of the 14 UK scenario packs. Canadian patient actors participated in scenario packs for both Canada and the other half of UK-based scenario packs. This assignment process resulted in 149 distinct simulated patients (“scenarios”). Below, we use the term “OSCE agent” to refer to the conversational counterpart interviewing the patient actor, i.e., either PCP or AMIE. &nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S3.T1" title="Table 1 ‣ 3.2 Remote OSCE Study Design ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> summarizes the OSCE assignment information across three geographical locations. Each of the 149 simulated patients completed the three-step study flow depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.5.1.1" style="font-size:113%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T1.6.2" style="font-size:113%;">OSCE study summary.<span class="ltx_text ltx_font_medium" id="S3.T1.6.2.1"> Number of scenario packs, patient actors, simulated patients, and primary care physicians (PCPs) in each of the three locations (Canada, India, and the UK) in the remote OSCE study. 20 board-certified PCPs participated in the study as OSCE agents in comparison with AMIE, 10 each from India and Canada. 20 trained patient actors were involved, with 10 each from India and Canada.
Indian patient actors played the roles in both India and UK scenario packs. Canadian patient actors participated in scenario packs for both Canada and the UK. This process resulted in 149 distinct simulated patients.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.7.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.7.1.1.1"><span class="ltx_text" id="S3.T1.7.1.1.1.1" style="font-size:80%;">Location</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.7.1.1.2"><span class="ltx_text" id="S3.T1.7.1.1.2.1" style="font-size:80%;"># of Scenario Packs</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.7.1.1.3"><span class="ltx_text" id="S3.T1.7.1.1.3.1" style="font-size:80%;"># of Simulated Patients</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.7.1.1.4"><span class="ltx_text" id="S3.T1.7.1.1.4.1" style="font-size:80%;"># of Patient Actors</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.7.1.1.5"><span class="ltx_text" id="S3.T1.7.1.1.5.1" style="font-size:80%;"># of PCPs</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.7.2.2.1"><span class="ltx_text" id="S3.T1.7.2.2.1.1" style="font-size:80%;">Canada</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.2.2.2"><span class="ltx_text" id="S3.T1.7.2.2.2.1" style="font-size:80%;">60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.2.2.3"><span class="ltx_text" id="S3.T1.7.2.2.3.1" style="font-size:80%;">67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.2.2.4"><span class="ltx_text" id="S3.T1.7.2.2.4.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.2.2.5"><span class="ltx_text" id="S3.T1.7.2.2.5.1" style="font-size:80%;">10</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.7.3.3.1"><span class="ltx_text" id="S3.T1.7.3.3.1.1" style="font-size:80%;">India</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.7.3.3.2"><span class="ltx_text" id="S3.T1.7.3.3.2.1" style="font-size:80%;">75</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.3.3.3"><span class="ltx_text" id="S3.T1.7.3.3.3.1" style="font-size:80%;">82</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.3.3.4"><span class="ltx_text" id="S3.T1.7.3.3.4.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.3.3.5"><span class="ltx_text" id="S3.T1.7.3.3.5.1" style="font-size:80%;">10</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.7.4.4.1"><span class="ltx_text" id="S3.T1.7.4.4.1.1" style="font-size:80%;">UK</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.7.4.4.2"><span class="ltx_text" id="S3.T1.7.4.4.2.1" style="font-size:80%;">14</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.4.4.3"><span class="ltx_text" id="S3.T1.7.4.4.3.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.4.4.4"><span class="ltx_text" id="S3.T1.7.4.4.4.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.4.4.5"><span class="ltx_text" id="S3.T1.7.4.4.5.1" style="font-size:80%;">0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.7.5.5.1"><span class="ltx_text ltx_font_bold" id="S3.T1.7.5.5.1.1" style="font-size:80%;">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.5.5.2"><span class="ltx_text ltx_font_bold" id="S3.T1.7.5.5.2.1" style="font-size:80%;">149</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.5.5.3"><span class="ltx_text ltx_font_bold" id="S3.T1.7.5.5.3.1" style="font-size:80%;">149</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.5.5.4"><span class="ltx_text ltx_font_bold" id="S3.T1.7.5.5.4.1" style="font-size:80%;">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T1.7.5.5.5.1" style="font-size:80%;">20</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Online Text-based Consultation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">PCPs and patient actors were primed with sample scenarios and instructions, and participated in pilot consultations prior to the study commencing in order to familiarize themselves with the interface and experiment requirements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">For the experiment, each simulated patient completed two online text-based consultations via a synchronous text chat interface (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:chat_interface</span>), one with a PCP (control) and one with AMIE (intervention).
The ordering of PCP and AMIE was randomized and patient actors were not informed as to which they were talking to in each consultation.
PCPs were located in the same country as patient actors, and were randomly drawn based on availability at the specified time slot for the consultation.
Patient actors role-played the scenario and were instructed to conclude the conversation after no more than 20 minutes. Both OSCE agents were asked (PCPs via study-specific instructions, and AMIE as part of the prompt template) to not reveal their identity, or whether they were human, under any circumstances.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Post-questionnaires</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Upon conclusion of the consultation, the patient actor and OSCE agent each filled in a post-questionnaire in light of the resulting consultation transcript (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:patient_actor_rating_interface</span>).
The post-questionnaire for patient actors consisted of the complete GMCPQ (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:gmcpq_rubric_details</span>), the PACES components for “Managing Patient Concerns” and “Maintaining Patient Welfare” (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>), and a checklist representation of the PCCBP category for “Fostering the Relationship” (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>).
Responses patient actors provided to the post-questionnaire are referred to as “patient actor ratings” below.
The post-questionnaire for the OSCE agent asked for a ranked differential diagnosis (DDx) list with a minimum of 3 and no more than 10 conditions, as well as recommendations for escalation to in-person or video-based consultation, investigations, treatments, management plan, and the need for a follow-up.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Specialist Physician Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Finally, a pool of 23 specialist physicians from India (14), North America (6), and the UK (3) evaluated PCPs and AMIE with respect to the quality of their consultation, and their responses to the post-questionnaire.
During evaluation, specialist physicians also had access to the full scenario pack along with its associated ground truth differential and additional accepted differentials.
All of the data the specialist physicians had access to during evaluation are collectively referred to as “OSCE data” below.
Specialist physicians were sourced to match the specialties and geographic regions corresponding to the scenario packs included in our study, and had between 1 and 36 years of post-residency experience (median 5 years).
Each set of OSCE data was evaluated by one specialist physician randomly assigned to match the specialty and geographic region of the underlying scenario (e.g., Canadian pulmonologist evaluated OSCE data from Canada-sourced respiratory medicine scenario).
Each specialist evaluated OSCE data from both PCP and AMIE for a given scenario.
Evaluations for PCP and AMIE were conducted by the same specialist in a randomized and blinded sequence.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">Evaluation criteria included the accuracy, appropriateness and comprehensiveness of the provided DDx list, appropriateness of recommendations regarding escalation, investigation, treatment, management plan and follow-up (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:diagnosis_management_rubric_details</span>), and all PACES (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>) and PCCBP (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>) rating items.
We also asked specialist physicians to highlight confabulations in the consultations and questionnaire responses, i.e., text passages that were non-factual or referred to information not provided in the conversation.
Each OSCE scenario pack additionally supplied specialists with scenario-specific clinical information to assist with rating the clinical quality of the consultation, such as the ideal investigation or management plans; or important aspects of the clinical history that would ideally have been elucidated for the highest quality of consultation possible.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Auto-evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In addition to human evaluations, we implemented model-based auto-evaluation methods as economical consistent alternatives to specialist assessments. These techniques were employed to evaluate both dialogue quality and diagnostic accuracy of the OSCE agent.
To establish the validity of our auto-evaluation methods for assessing dialogue quality, we initially focused on a subset of four evaluation axes from the PACES rubric (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>) that were assessed by both the patient actors and the specialist physicians. The auto-evaluation, which uses a self-CoT strategy (details described in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:auto-eval</span>) with AMIE to rate dialogues, was in good alignment with human raters and comparable to the inter-specialist agreement on these criteria. For the auto-evaluation of differential diagnoses, we leveraged another LLM, Med-PaLM 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx13" title="">13</a>]</cite> as a surrogate for a specialist rater to grade the predicted diagnoses against the ground truth diagnoses (more details in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:auto-eval-ddx</span>). Our auto-evaluation on DDx accuracy showed a similar trend for AMIE and OSCE agents compared to the specialist ratings. Overall, auto-evaluation trends aligned with human ratings for both dialogue quality and diagnostic accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We also conducted additional auto-evaluation analyses for the following purposes:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">To compare the performance of the DDx accuracy derived from AMIE or PCP consultations;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">To compare the DDx accuracy between simulated patients performed in Canada and India and determine if there is systematic differences between the two locations;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">To isolate the effects of information acquisition and information interpretation by analyzing the DDx accuracy of AMIE when provided the PCP consultation instead of its own;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">To evaluate the efficiency of information acquisition between AMIE and PCPs by analyzing the DDx accuracy as the number of conversation turns increases;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">To evaluate the benefit of inner-loop self-play on dialogue quality before and after critic feedback.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Statistical Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We evaluated the top-k accuracy of the DDx lists generated by AMIE and PCPs across all 149 simulated patients.
Top-k accuracy was defined as the percentage of cases where the correct diagnosis appeared within the top-k positions of the DDx list.
Specifically, a candidate diagnosis was considered a match if the specialist rater marked it as either an exact match with, very close to or closely related to the ground truth diagnosis (or accepted differential).
Statistical significance for DDx accuracy was determined using bootstrap tests&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx34" title="">34</a>]</cite> with 10,000 samples and false discovery rate (FDR) correction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx35" title="">35</a>]</cite> across all k.
Statistical significance for patient actor and specialist ratings was determined using Wilcoxon signed-rank tests&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx36" title="">36</a>]</cite> FDR correction.
Cases where either agent received “Cannot rate / Does not apply” were excluded from the test.
Results below refer to <math alttext="p" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_p</annotation></semantics></math>-values after FDR correction.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Diagnostic Accuracy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="317" id="S4.F3.g1" src="x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.7.2.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.2.1" style="font-size:90%;">Specialist-rated top-k diagnostic accuracy.<span class="ltx_text ltx_font_medium" id="S4.F3.2.1.2"> AMIE and PCPs top-k DDx accuracy are compared across 149 scenarios with respect to the ground truth diagnosis (</span>a<span class="ltx_text ltx_font_medium" id="S4.F3.2.1.3">) and all diagnoses in the accepted differential (</span>b<span class="ltx_text ltx_font_medium" id="S4.F3.2.1.1">). Bootstrapping (n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with <math alttext="p<0.05" class="ltx_Math" display="inline" id="S4.F3.2.1.1.m1.1"><semantics id="S4.F3.2.1.1.m1.1b"><mrow id="S4.F3.2.1.1.m1.1.1" xref="S4.F3.2.1.1.m1.1.1.cmml"><mi id="S4.F3.2.1.1.m1.1.1.2" xref="S4.F3.2.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.F3.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.F3.2.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.F3.2.1.1.m1.1.1.3" mathvariant="normal" xref="S4.F3.2.1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.2.1.1.m1.1c"><apply id="S4.F3.2.1.1.m1.1.1.cmml" xref="S4.F3.2.1.1.m1.1.1"><lt id="S4.F3.2.1.1.m1.1.1.1.cmml" xref="S4.F3.2.1.1.m1.1.1.1"></lt><ci id="S4.F3.2.1.1.m1.1.1.2.cmml" xref="S4.F3.2.1.1.m1.1.1.2">𝑝</ci><cn id="S4.F3.2.1.1.m1.1.1.3.cmml" type="float" xref="S4.F3.2.1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.1.1.m1.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.F3.2.1.1.m1.1e">italic_p &lt; 0.05</annotation></semantics></math> after FDR correction.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">AMIE’s diagnostic accuracy was assessed as higher than that of PCPs.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.F3" title="Figure 3 ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> shows the top-k accuracy for AMIE and PCPs, considering matches with the ground truth diagnosis (a) and matches with any item on the accepted differential (b). AMIE showed significantly higher top-k accuracy than that of PCPs across all values of k (<math alttext="p<0.05" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.SSS1.p1.1.m1.1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><lt id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.1"></lt><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>). Note that unlike AMIE, PCPs did not always provide 10 diagnoses in their differential diagnoses (min: 3, mean: 5.39). Additionally, we performed a comparison of DDx accuracy between AMIE and PCP by varying the matching criteria for determining a match. Results depicted in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_specialist_match_cutoffs</span> further substantiate AMIE’s superior DDx performance across various matching criteria.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Accuracy by Specialty.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p1.1"><span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:specialist_ddx_ratings_by_specialty</span> illustrates the DDx accuracy achieved by AMIE and PCPs across the six medical specialties covered by scenarios in our study. We observed that AMIE’s performance matched or surpassed PCP performance for all specialties with the most pronounced improvements in the respiratory and cardiovascular specialities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Auto-evaluation Accuracy.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px1.p1.1">We reproduced the DDx accuracy analysis with our model-based auto-evaluator instead of the specialist raters using the same procedure as in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.F3" title="Figure 3 ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>. The overall performance trends obtained through the auto-evaluator align well with specialist assessments despite marginal differences in the computed accuracy values, as shown in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Isolating the Source of Performance Gains.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px2.p1.1">To investigate whether AMIE’s superior DDx performance observed in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.F3" title="Figure 3 ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> stemmed from improved information acquisition or from better diagnostic reasoning capability, we compared AMIE’s diagnoses based on its own consultations with AMIE’s diagnoses generated from the corresponding PCP consultations, using the DDx auto-evaluator. Results depicted in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval_AMIEvsAMIE</span> revealed markedly similar DDx performance, indicating that the diagnostic performance remained consistent regardless of whether AMIE processed information from its own dialogue or from the PCP’s conversation. Both methods significantly outperformed the differential diagnoses produced by PCPs. These results suggest that AMIE was approximately equivalent to PCPs at information acquisition but better than PCPs at interpreting that information to produce an accurate/complete differential diagnosis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Efficiency of Information Acquisition.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS2.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px3.p1.1">Although AMIE displayed greater verbosity compared to PCPs in terms of total number of words generated in their responses during the consultation, the number of conversational turns and the number of words elicited from the patient actors were similar across both OSCE agents, as illustrated in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:number_of_words_and_turns</span>. This suggests that both AMIE and PCPs acquired a similar amount of information from the patients during the encounter. To investigate how efficient AMIE or PCPs were at gathering sufficient information to formulate a correct diagnosis, we truncated the conversations at various turn counts and used AMIE to generate differential diagnoses based on these partial conversations. &nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval_AMIEvsAMIE_turnsablation</span> depicts the top-3 DDx accuracy as a function of the number of turns provided to the model. The observed accuracies plateaued within the initial 10 conversational turns for both AMIE and PCPs. This suggests that both AMIE and PCPs were able to acquire the information necessary for formulating a diagnosis within the early stages of the conversation. Additionally, the comparable performance at every turn indicates that neither AMIE nor PCPs had a significant advantage in the efficiency or quality of information acquisition.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="640" id="S4.F4.g1" src="x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.11.5.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.8.4" style="font-size:90%;">Patient actor ratings.<span class="ltx_text ltx_font_medium" id="S4.F4.8.4.4"> Conversation qualities as assessed by patient actors upon conclusion of the consultation. For illustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from ‘Very favorable’ to ‘Very unfavorable’. For Yes/No questions, a (positive) ‘Yes’ response was mapped to the same color as ‘Favorable’ and a (negative) ’No’ response to the same color as ‘Unfavorable’. Rating scales were adapted from the General Medical Council Patient Questionnaire (GMCPQ), the Practical Assessment of Clinical Examination Skills (PACES), and a narrative review about Patient-Centered Communication Best Practice (PCCBP). Details on question wording and response options are provided in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:rubrics</span>. Asterisks represent statistical significance (<math alttext="*:p<0.05" class="ltx_math_unparsed" display="inline" id="S4.F4.5.1.1.m1.2"><semantics id="S4.F4.5.1.1.m1.2b"><mrow id="S4.F4.5.1.1.m1.2c"><mo id="S4.F4.5.1.1.m1.1.1" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F4.5.1.1.m1.2.2" mathvariant="normal" rspace="0.278em">:</mo><mi id="S4.F4.5.1.1.m1.2.3">p</mi><mo id="S4.F4.5.1.1.m1.2.4" mathvariant="normal">&lt;</mo><mn id="S4.F4.5.1.1.m1.2.5" mathvariant="normal">0.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.5.1.1.m1.2d">*:p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.F4.5.1.1.m1.2e">* : italic_p &lt; 0.05</annotation></semantics></math>, <math alttext="**:p<0.01" class="ltx_math_unparsed" display="inline" id="S4.F4.6.2.2.m2.3"><semantics id="S4.F4.6.2.2.m2.3b"><mrow id="S4.F4.6.2.2.m2.3c"><mo id="S4.F4.6.2.2.m2.1.1" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F4.6.2.2.m2.2.2" lspace="0em" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F4.6.2.2.m2.3.3" mathvariant="normal" rspace="0.278em">:</mo><mi id="S4.F4.6.2.2.m2.3.4">p</mi><mo id="S4.F4.6.2.2.m2.3.5" mathvariant="normal">&lt;</mo><mn id="S4.F4.6.2.2.m2.3.6" mathvariant="normal">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.6.2.2.m2.3d">**:p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S4.F4.6.2.2.m2.3e">* * : italic_p &lt; 0.01</annotation></semantics></math>, <math alttext="***:p<0.001" class="ltx_math_unparsed" display="inline" id="S4.F4.7.3.3.m3.4"><semantics id="S4.F4.7.3.3.m3.4b"><mrow id="S4.F4.7.3.3.m3.4c"><mo id="S4.F4.7.3.3.m3.1.1" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F4.7.3.3.m3.2.2" lspace="0em" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F4.7.3.3.m3.3.3" lspace="0em" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F4.7.3.3.m3.4.4" mathvariant="normal" rspace="0.278em">:</mo><mi id="S4.F4.7.3.3.m3.4.5">p</mi><mo id="S4.F4.7.3.3.m3.4.6" mathvariant="normal">&lt;</mo><mn id="S4.F4.7.3.3.m3.4.7" mathvariant="normal">0.001</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.7.3.3.m3.4d">***:p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S4.F4.7.3.3.m3.4e">* * * : italic_p &lt; 0.001</annotation></semantics></math>, <math alttext="n.s.:" class="ltx_Math" display="inline" id="S4.F4.8.4.4.m4.3"><semantics id="S4.F4.8.4.4.m4.3b"><mrow id="S4.F4.8.4.4.m4.3.4.2" xref="S4.F4.8.4.4.m4.3.4.1.cmml"><mi id="S4.F4.8.4.4.m4.1.1" xref="S4.F4.8.4.4.m4.1.1.cmml">n</mi><mo id="S4.F4.8.4.4.m4.3.4.2.1" lspace="0em" mathvariant="normal" rspace="0.167em" xref="S4.F4.8.4.4.m4.3.4.1a.cmml">.</mo><mi id="S4.F4.8.4.4.m4.2.2" xref="S4.F4.8.4.4.m4.2.2.cmml">s</mi><mo id="S4.F4.8.4.4.m4.3.4.2.2" lspace="0em" mathvariant="normal" rspace="0.167em" xref="S4.F4.8.4.4.m4.3.4.1a.cmml">.</mo><mo id="S4.F4.8.4.4.m4.3.3" mathvariant="normal" xref="S4.F4.8.4.4.m4.3.3.cmml">:</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.8.4.4.m4.3c"><apply id="S4.F4.8.4.4.m4.3.4.1.cmml" xref="S4.F4.8.4.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.F4.8.4.4.m4.3.4.1a.cmml" xref="S4.F4.8.4.4.m4.3.4.2.1">formulae-sequence</csymbol><ci id="S4.F4.8.4.4.m4.1.1.cmml" xref="S4.F4.8.4.4.m4.1.1">𝑛</ci><ci id="S4.F4.8.4.4.m4.2.2.cmml" xref="S4.F4.8.4.4.m4.2.2">𝑠</ci><ci id="S4.F4.8.4.4.m4.3.3.cmml" xref="S4.F4.8.4.4.m4.3.3">normal-:</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.4.4.m4.3d">n.s.:</annotation><annotation encoding="application/x-llamapun" id="S4.F4.8.4.4.m4.3e">italic_n . italic_s . :</annotation></semantics></math> not significant).</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conversation Quality</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>AMIE surpassed PCPs in conversation quality, per specialists and patient actors.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Conversation quality was assessed using patient actor ratings, specialist ratings, and outputs from auto-evaluation.
&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:AMIE_example_osce</span> and <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:pcp_example_osce</span> show two example consultations for the same simulated patient from AMIE and PCP, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Patient Actor Ratings.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.F4" title="Figure 4 ‣ Efficiency of Information Acquisition. ‣ 4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> presents the various conversation qualities patient actors assessed following their consultations with the OSCE agents.
Overall, AMIE’s consultations were rated significantly better (<math alttext="p<0.05" class="ltx_Math" display="inline" id="S4.SS2.SSS1.Px1.p1.1.m1.1"><semantics id="S4.SS2.SSS1.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1"><lt id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px1.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.Px1.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) by patient actors than those from PCPs across 24 of 26 axes. No significant differences in ratings were detected for the two PCCBP axes “Respecting Patient’s Privacy” (N=108) and “Acknowledging Mistakes” (N=41). For the latter criterion, the number of exclusions was substantially higher since the question applied only when mistakes were made by the OSCE agent and pointed out in the conversation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="790" id="S4.F5.g1" src="x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.11.5.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.8.4" style="font-size:90%;">Specialist physician ratings.<span class="ltx_text ltx_font_medium" id="S4.F5.8.4.4"> Conversation and reasoning qualities as assessed by specialist physicians. For illustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from ‘Very favorable’ to ‘Very unfavorable’. The only four-point scale (DDx Comprehensiveness) was mapped to the same scale, ignoring the ‘Neither favorable nor unfavorable’ option. For Yes/No questions, a (positive) ‘Yes’ response was mapped to the same color as ‘Favorable’ and a (negative) ’No’ response to the same color as ‘Unfavorable’. Rating scales were adapted from the Practical Assessment of Clinical Examination Skills (PACES), a narrative review about Patient-Centered Communication Best Practice (PCCBP), and other sources. Details on question wording and response options are provided in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:rubrics</span>. Asterisks represent statistical significance (<math alttext="*:p<0.05" class="ltx_math_unparsed" display="inline" id="S4.F5.5.1.1.m1.2"><semantics id="S4.F5.5.1.1.m1.2b"><mrow id="S4.F5.5.1.1.m1.2c"><mo id="S4.F5.5.1.1.m1.1.1" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F5.5.1.1.m1.2.2" mathvariant="normal" rspace="0.278em">:</mo><mi id="S4.F5.5.1.1.m1.2.3">p</mi><mo id="S4.F5.5.1.1.m1.2.4" mathvariant="normal">&lt;</mo><mn id="S4.F5.5.1.1.m1.2.5" mathvariant="normal">0.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.5.1.1.m1.2d">*:p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.F5.5.1.1.m1.2e">* : italic_p &lt; 0.05</annotation></semantics></math>, <math alttext="**:p<0.01" class="ltx_math_unparsed" display="inline" id="S4.F5.6.2.2.m2.3"><semantics id="S4.F5.6.2.2.m2.3b"><mrow id="S4.F5.6.2.2.m2.3c"><mo id="S4.F5.6.2.2.m2.1.1" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F5.6.2.2.m2.2.2" lspace="0em" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F5.6.2.2.m2.3.3" mathvariant="normal" rspace="0.278em">:</mo><mi id="S4.F5.6.2.2.m2.3.4">p</mi><mo id="S4.F5.6.2.2.m2.3.5" mathvariant="normal">&lt;</mo><mn id="S4.F5.6.2.2.m2.3.6" mathvariant="normal">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.6.2.2.m2.3d">**:p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S4.F5.6.2.2.m2.3e">* * : italic_p &lt; 0.01</annotation></semantics></math>, <math alttext="***:p<0.001" class="ltx_math_unparsed" display="inline" id="S4.F5.7.3.3.m3.4"><semantics id="S4.F5.7.3.3.m3.4b"><mrow id="S4.F5.7.3.3.m3.4c"><mo id="S4.F5.7.3.3.m3.1.1" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F5.7.3.3.m3.2.2" lspace="0em" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F5.7.3.3.m3.3.3" lspace="0em" mathvariant="normal" rspace="0em">*</mo><mo id="S4.F5.7.3.3.m3.4.4" mathvariant="normal" rspace="0.278em">:</mo><mi id="S4.F5.7.3.3.m3.4.5">p</mi><mo id="S4.F5.7.3.3.m3.4.6" mathvariant="normal">&lt;</mo><mn id="S4.F5.7.3.3.m3.4.7" mathvariant="normal">0.001</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.7.3.3.m3.4d">***:p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S4.F5.7.3.3.m3.4e">* * * : italic_p &lt; 0.001</annotation></semantics></math>, <math alttext="n.s.:" class="ltx_Math" display="inline" id="S4.F5.8.4.4.m4.3"><semantics id="S4.F5.8.4.4.m4.3b"><mrow id="S4.F5.8.4.4.m4.3.4.2" xref="S4.F5.8.4.4.m4.3.4.1.cmml"><mi id="S4.F5.8.4.4.m4.1.1" xref="S4.F5.8.4.4.m4.1.1.cmml">n</mi><mo id="S4.F5.8.4.4.m4.3.4.2.1" lspace="0em" mathvariant="normal" rspace="0.167em" xref="S4.F5.8.4.4.m4.3.4.1a.cmml">.</mo><mi id="S4.F5.8.4.4.m4.2.2" xref="S4.F5.8.4.4.m4.2.2.cmml">s</mi><mo id="S4.F5.8.4.4.m4.3.4.2.2" lspace="0em" mathvariant="normal" rspace="0.167em" xref="S4.F5.8.4.4.m4.3.4.1a.cmml">.</mo><mo id="S4.F5.8.4.4.m4.3.3" mathvariant="normal" xref="S4.F5.8.4.4.m4.3.3.cmml">:</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.8.4.4.m4.3c"><apply id="S4.F5.8.4.4.m4.3.4.1.cmml" xref="S4.F5.8.4.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.F5.8.4.4.m4.3.4.1a.cmml" xref="S4.F5.8.4.4.m4.3.4.2.1">formulae-sequence</csymbol><ci id="S4.F5.8.4.4.m4.1.1.cmml" xref="S4.F5.8.4.4.m4.1.1">𝑛</ci><ci id="S4.F5.8.4.4.m4.2.2.cmml" xref="S4.F5.8.4.4.m4.2.2">𝑠</ci><ci id="S4.F5.8.4.4.m4.3.3.cmml" xref="S4.F5.8.4.4.m4.3.3">normal-:</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.8.4.4.m4.3d">n.s.:</annotation><annotation encoding="application/x-llamapun" id="S4.F5.8.4.4.m4.3e">italic_n . italic_s . :</annotation></semantics></math> not significant).</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Specialist Physician Ratings.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p1.1">Specialist physicians evaluated both the conversational quality as well as the responses to the post-questionnaire for scenarios within their domain expertise (see <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#S4.F5" title="Figure 5 ‣ Patient Actor Ratings. ‣ 4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>). Again, AMIE’s responses were rated significantly better by specialists than those from PCPs on 28 of 32 evaluation axes; Specialists preferred AMIE’s consultation, diagnoses, and management plan over those from PCPs. For this set of evaluations, differences in specialist ratings between AMIE and PCPs were statistically significant (<math alttext="p<0.05" class="ltx_Math" display="inline" id="S4.SS2.SSS1.Px2.p1.1.m1.1"><semantics id="S4.SS2.SSS1.Px2.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1"><lt id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px2.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.Px2.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>).
No significant differences in ratings were detected for four of the axes in the Diagnosis &amp; Management rubric, namely, “Escalation Recommendation Appropriate”, “Treatment Inappropriate Avoided”, “Followup Recommendation Appropriate” and “Confabulation Absent”, despite no exclusions (N=149).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Auto-evaluation of Conversation Ratings.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">We leveraged the model-based self-CoT auto-evaluation strategy to rate conversations on four evaluation axes from the PACES rubric, and validated that these auto-evaluation ratings were accurate and well aligned with the specialist ratings (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_ablation</span> and&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_vs_specialist</span>). Furthermore, to demonstrate that the inner self-play loop improved simulated dialogue quality, we applied the auto-evaluation method to the simulated dialogues generated before and after the self-play procedure. Results in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_selfplay</span> revealed that the simulated dialogues after self-play were preferred more often than the baseline dialogues without self-critique.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Clinical History-taking and the Diagnostic Dialogue</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">History-taking and the clinical interview are widely taught in both medical schools’ and postgraduate curricula&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx42" title="">42</a>]</cite>. Consensus on physician-patient communication has evolved to embrace patient-centred communication practices, with recommendations that communication in clinical encounters should address six core functions: fostering the relationship, gathering information, providing information, making decisions, responding to emotions and enabling disease- and treatment-related behavior&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx44" title="">44</a>]</cite>. Specific skills and behaviours for meeting these goals have also been described, taught and assessed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx20" title="">20</a>]</cite> with validated tools&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx45" title="">45</a>]</cite>. Medical conventions consistently cite that certain categories of information should be gathered during a clinical interview, comprising topics such as the presenting complaint, past medical history and medication history, social and family history, and systems review&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx47" title="">47</a>]</cite>. Clinicians’ ability to meet these goals is commonly assessed using the framework of an objective structured clinical examination (OSCE)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx33" title="">33</a>]</cite>. Such assessments vary in their reproducibility or implementation and have even been adapted for remote practice as virtual OSCEs (vOSCEs) with telemedical scenarios, an issue of particular relevance during the COVID-19 pandemic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx48" title="">48</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Conversational AI and Goal-oriented Dialogue</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Conversational AI systems for goal-oriented dialogue and task completion have a rich history&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx51" title="">51</a>]</cite>. The emergence of transformers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx52" title="">52</a>]</cite> and large language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx15" title="">15</a>]</cite> have led to renewed interest in this direction. The development of strategies for alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx53" title="">53</a>]</cite>, self-improvement&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx57" title="">57</a>]</cite> and scalable oversight mechanisms&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx58" title="">58</a>]</cite> have enabled large scale deployment of such conversational systems in the real world&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx59" title="">59</a>]</cite>.
However, the rigorous evaluation and exploration of conversational and task-completion capabilities of such AI systems remains limited for clinical applications, where studies have largely focused on single-turn interaction use cases such as question-answering or summarization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>AI for Medical Consultations and Diagnostic Dialogue</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The majority of explorations of AI as tools for conducting medical consultations have focused on “symptom checker” applications rather than a full natural dialogue, or on topics such as transcription of medical audio or the generation of plausible dialogue given clinical notes or summaries&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx63" title="">63</a>]</cite>. Language models have been trained using clinical dialogue datasets but not comprehensively evaluated&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx64" title="">64</a>]</cite>. Studies have been grounded in messages between doctors and patients in commercial chat platforms (which may have altered doctor-patient engagement compared to 1:1 medical consultations)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx28" title="">28</a>]</cite>. Many focused largely on predicting next turns in the recorded exchanges rather than clinically meaningful metrics. And to date, there have been no reported studies that have examined the quality of AI models for diagnostic dialogue using the same criteria that are used to examine and train human physicians in dialogue and communication skills; nor evaluating AI systems in common frameworks such as the OSCE.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation of Diagnostic Dialogue</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Prior frameworks for human evaluation of AI systems’ performance in diagnostic dialogue have been limited in detail. They have not been anchored in established criteria for assessing communication skills and the quality of history-taking. For example,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx29" title="">29</a>]</cite> reported a 5-point scale describing overall “human evaluation”,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx65" title="">65</a>]</cite> reported “relevance, informativeness and human likeness”,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx66" title="">66</a>]</cite> reported “fluency, expertise and relevance”,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx67" title="">67</a>]</cite> “fluency and adequacy” and&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx68" title="">68</a>]</cite> “fluency”. These criteria are far less comprehensive and specific than those taught and practiced by medical professionals. A multi-agent framework for assessing conversational capabilities of LLMs is introduced in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx64" title="">64</a>]</cite>, however, the study was performed in the restricted setting of dermatology, used AI models to emulate both doctor and patient sides of simulated interactions, and performed limited expert evaluation of history-taking as “complete” or not.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we introduced AMIE, an LLM based AI system optimised for clinical dialogue with diagnostic reasoning capabilities. We compared AMIE consultations to those performed by PCPs using a randomized, double-blind crossover study with human simulated patients in the style of an Objective Structured Clinical Examination (OSCE). Notably, our study was not designed to be representative of clinical conventions either for traditional OSCE evaluations, for remote- or tele-medical consultation practices, or for the ways clinicians usually use text and chat messaging to communicate with patients. Our evaluation instead mirrored the most common way by which people interact with LLMs today, leveraging a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. In this setting, we observed that AMIE, an AI system optimised specifically for the task, outperformed PCPs on simulated diagnostic conversations when evaluated along multiple clinically-meaningful axes of consultation quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Diagnostic Performance.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">The differential diagnoses provided by AMIE were more accurate and complete than those provided by board-certified PCPs, when both were evaluated by specialist physicians. Previous research has shown that AI systems may match or exceed human diagnostic performance in specific, narrow tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx71" title="">71</a>]</cite> in retrospective evaluation. However, these situations typically involved both AI and physicians interpreting the same fixed input (for example, identifying the presence of a specific finding in a medical image). Our study was significantly more challenging because it required the AI system to actively acquire relevant information through conversation rather than relying on clinical information collated by human efforts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx72" title="">72</a>]</cite>. Therefore the system’s downstream differential diagnoses depended on not only its diagnostic inference capability, but also the quality of information gathered under uncertainty through natural conversation and building rapport.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">Our results suggested that AMIE was as adept as PCPs in eliciting pertinent information during the simulated consultations and was more accurate than PCPs in formulating a complete differential diagnosis if given the same amount of acquired information. This finding corroborates other work that LLMs may be able to produce more complete differential diagnoses given the same clinical information as physicians in challenging cases &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx70" title="">70</a>]</cite>. Though not explored in this study, the assistive performance of AMIE therefore represents an interesting and important avenue for future research, particularly given the real-world importance of expert oversight for AI systems in safety-critical settings such as medicine.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p3.1">Our study utilized a wide variety of simulated patients, comprising actors trained in both Canada and India and scenarios across a range of specialties. This allowed us to explore how performance varied along multiple axes: by specialty, and by the locations in which the scenario was derived and enacted. We observed that both PCPs and AMIE performed worse in obstetric/gynecology and internal medicine scenarios than those from other specialties (see&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:specialist_ddx_ratings_by_specialty</span>). The study was not powered or designed to compare performance between different specialty topics, and we cannot exclude that the scenarios in some specialties might be harder than others. We observed that both AMIE and PCPs had higher diagnostic accuracy in consultations performed in the Canada OSCE lab compared to those enacted in the India OSCE lab (see&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:per_location_specialist_eval</span>). However, the differences were not statistically significant and in a subset of 40 scenarios enacted in both the Canada OSCE lab and the India OSCE lab, the performance of both AMIE and PCPs was equivalent (see&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:same_scenario_location_ddx</span>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Conversational Performance.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Patient actors and specialist raters both evaluated AMIE’s performance to be higher than PCPs on metrics related to empathy and communication skills. These axes comprised a majority of the dimensions that were evaluated. This general finding is consistent with a prior study where LLM responses were found to be more empathetic than the responses from clinicians to health questions posted on Reddit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx73" title="">73</a>]</cite>. However, the findings in that study may not be generalised directly to our setting due to the differences in study design. Specifically, prior work has not involved a direct, randomised comparison of physicians and AI systems in a prospective simulation of multi-turn dialogue with the same patient. In both settings, the lack of voice-based and non-verbal visual communication may be an unfair disadvantage to clinicians.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.1">The text-based chat interface used in this study introduces both advantages and disadvantages. People today most commonly engage with LLMs through synchronous text-chat interfaces&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx74" title="">74</a>]</cite>, and patients often use patient portals to send messages to their providers. We therefore chose this mode of interaction as a representative interface for LLMs to perform multi-turn conversation, adapting the virtual OSCE framework accordingly. While this allowed a fair comparison of diagnostic dialogue between LLMs and clinicians when both were restricted to a synchronous text-chat, it is important to acknowledge that our experiments do not emulate the expected quality of diagnostic dialogue in real clinical practice (including telemedicine). Physicians may be more used to history-taking and diagnostic dialogue by telephone or video consultation than synchronous text-chat communication&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx76" title="">76</a>]</cite>. Instead, text is more commonly used by clinicians to communicate with patients for episodic or asynchronous needs such as prescription refills or communication about specific test results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx77" title="">77</a>]</cite>. Physicians may thus be more familiar with text/SMS or email rather than the synchronous text-chat medium we employed in this study. In both text/SMS and email, the conventions and expectations for communicating naturally and with empathic style might be different&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx78" title="">78</a>]</cite>. It is possible that the PCPs in our study had not yet become accustomed to the setting, and may have performed differently if subjected to a specific training program (similar in spirit to the training process for AMIE). Clinicians participating in the study undertook two preparatory pilot sessions of consultations with our synchronous text interface before the evaluation began, but this was not a formal training program, nor was it designed to optimize clinicians’ performance. Future research could explore this question more thoroughly including monitoring for the impact of a learning curve, or exploring whether performance varies according to the extent to which participating clinicians or simulated patients are familiar with telemedicine.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p3.1">Additionally, our findings regarding empathic communication could also be partially attributed to the fact that AMIE responses were significantly longer than clinician responses (shown in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:number_of_words_and_turns</span>), and presented with greater structure. This could potentially suggest to an observer that more time was spent preparing the response, analogous to known findings that patient satisfaction increases with time spend with their physicians&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx81" title="">81</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p4.1">Collectively, our findings suggest many avenues for further research that might leverage human-AI complementarity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx82" title="">82</a>]</cite>, combining clinicians’ skills in the analysis of verbal and non-verbal cues with the potential strengths of LLMs to suggest more enriched conversational responses including empathic statements, structure, eloquence, or more complete differential diagnoses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogue.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">The use of simulated data allowed us to quickly scale training to a broad set of conditions and patient contexts, while the injection of knowledge from search encouraged these dialogues to remain grounded and realistic. Though the simulated patients encompassed a wide range of conditions, they failed to capture the full range of potential patient backgrounds, personalities, and motivations. Through the inner self-play procedure, we were able to iteratively improve the simulated dialogue we generated and used in fine-tuning. However, these improvements were limited by our ability to articulate what makes a good dialogue in the critic instructions, the critic’s ability to produce effective feedback, and AMIE’s ability to adapt to such feedback. For example, in the simulated environment we impose that AMIE reaches a proposed differential and testing/treatment plan for the patient, but such an endpoint may be unrealistic for some conditions, especially in the virtual chat-based setting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Framework.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px4.p1.1">In contrast to prior works, we anchored our evaluation in criteria already established to be relevant for assessing physicians’ communication skills and history-taking quality. We performed more extensive and diverse human evaluation than prior studies of AI systems, with ratings from both clinicians and simulated patients perspective. Our raters and scenarios were sourced from multiple geographic locations, including North America, India and the UK. Our pilot evaluation rubric is, to our knowledge, the first to evaluate LLMs’ history-taking and communication skills using axes that are also measured in the real world for physicians themselves, increasing the clinical relevance of our research. Our evaluation framework is considerably more granular and specific than prior works on AI-generated clinical dialogue, which have not considered patient-centred communication best practice or clinically-relevant axes of consultation quality&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx64" title="">64</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px4.p2.1">However, our pilot framework is not definitive and can be further improved in future research. History-taking itself is contextual and what determines a “good history” is dependent on the specific clinical situation, patient and physician attributes, cultural characteristics, and many other factors. Despite variation in models for clinical history-taking&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx86" title="">86</a>]</cite>, studies have shown that good clinical interviews are associated with not only problem detection and diagnostic accuracy, but also quadruple aims for care delivery&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx88" title="">88</a>]</cite> ranging from patient and physician satisfaction, resilience to stress and illness, and health outcomes or cost.
Future studies on the quality of LLM history-taking might therefore utilise prospective measures of these outcomes in real-world settings (for example reductions in patient complaints&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx89" title="">89</a>]</cite>, or improvements in cost and care effectiveness, patient and provider satisfaction), though evaluations as such may be challenging or impractical to compare to standard practice in the same individual patient, and randomisation of different approaches may also be challenging in real-world settings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Breadth of Evaluation.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p1.1">Our chosen axes of evaluation were not exhaustive and their interpretation was often subjective in nature. Although we conducted evaluations from both clinician and lay-perspectives, generating scenario-packs in three countries with assessors in both North America and India, the pool of clinicians and lay-people assessing the models could be expanded further to improve generalization of our insights. Our experiments could also undergo more extensive replication to explore other aspects such as inter-observer and inter-participant variability, including future work with an intentionally further diversified pool of human raters (clinicians and lay users). Participatory design in the development of model evaluation tools with a representative pool of patients, as well as clinical and health equity domain experts, could also be valuable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p2.1">Although our scenarios comprised many different clinical conditions and specialties, our experiments were not necessarily representative of the decades of clinical practice accumulated by even a single doctor (who on average may perform tens of thousands of consultations in a career&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx90" title="">90</a>]</cite>). The range of conditions possible to examine in medicine is vast as is the variation in presentation of individual diseases. Our experiments were not designed to examine multi-morbidity and co-incident pathology, longitudinal case presentation or the consideration of sequential information from clinical investigations. We excluded entirely some clinical settings or specialties such as psychiatry, pediatrics, intensive care, and inpatient case management scenarios. Further research would be needed to understand the applicability of our findings in many settings such as these, where the requirements for high-quality history-taking might differ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx92" title="">92</a>]</cite>. The OSCE framework is commonly used in the assessment of clinicians’ skills. It encompasses a significant range of methodologies including real or simulated patients, interaction with physical artefacts or clinical materials, applications to a variety of medical specialties, tasks or settings; and both remote or in-person assessments. Although the OSCE approach is popular, there are significant limitations to its validity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx93" title="">93</a>]</cite>. We utilised a remote text-based assessment, replicating known issues with the paradigm of “virtual OSCE” such as the inability to incorporate non-verbal symptoms, signs and communication features. Additionally, this format could introduce unfamiliar constraints to the communication of PCP participants &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx48" title="">48</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p3.1">The tone, content, and nature of the OSCE dialogues in our study are likely not to be representative of real-world patient populations. For example, patient actors may have described their symptoms with greater structure, depth or clinical detail than could be routinely expected in many consultations, or had greater comprehension of clinical context than would be ordinarily expected. Furthermore, although evaluation was blinded, the style of responses from AMIE was notably different to that by PCPs which limits the practical extent of blinding in study design.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p4">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p4.1">Therefore even within the distribution of diseases and specialties we addressed, our findings should be interpreted with humility and caution. There is a need for further research to examine varied presentations of the same diseases, alongside exploration of alternate approaches to evaluating history-taking and clinical dialogue in situations of different patient needs, preferences, behaviours and circumstances.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">Fairness and Bias.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p1.1">The evaluation protocol presented in this paper is limited in terms of its ability to capture potential issues related to fairness and bias, which remains an important open question that we will aim to address in subsequent system evaluations. Recent advances in the development of comprehensive frameworks for bias detection in large language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx95" title="">95</a>]</cite> present a promising starting point for establishing such an approach. It should be noted that medical diagnostic dialogue is a particularly challenging use case, due to the complexity of the medical domain, the interactive information gathering nature of the dialogue, and the outcome-driven setting, with the potential of associated harms in case of incorrect diagnosis or incorrect medical advice. Nevertheless, disentangling these issues is an important further research area if LLMs in the domain are to overcome rather than propagate inequities in healthcare. For example, previous studies have found that physicians approach communication with their patients differently, on average, depending on patients’ race, resulting in Black patients receiving communication that was less patient-centered, and with a lower positive affect&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx96" title="">96</a>]</cite>. Other studies have found differences in physicians’ communication styles and conversation length based on gender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx97" title="">97</a>]</cite>. Effective intercultural communication skills are essential&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx91" title="">91</a>]</cite>. There is therefore a non-negligible risk that such historical conversational biases may be replicated or amplified in an AI dialogue system, but at the same time there is also an opportunity to work towards designing conversational systems that can be more inclusive, and more personalized to the individual patient’s needs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px6.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p2.1">To help inform the development of the necessary fairness, bias, and equity frameworks, it is important to employ a participatory approach to solicit representative views across a wide range of patient demographics, as well as clinical and health equity domain experts. Such evaluation frameworks should be complemented by extensive model red teaming and an adversarial approach to identifying any remaining gaps and failure modes. Recent advances in red teaming LLMs could be useful in this scenario&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx101" title="">101</a>]</cite>. These practices should not only inform the evaluation of the final model, but also its development and iterative refinement. Model development should follow the established data and model reporting practices and provide transparency into the training data and the associated decision processes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx104" title="">104</a>]</cite>. The dialogue research dataset contributing to AMIE training data in our study was de-identified, reducing the availability of socio-economic factors, patient demographics, and information about clinical settings and locations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px6.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p3.1">Further work is also needed to ensure the robustness of medical LLMs in multilingual settings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx108" title="">108</a>]</cite>, and particularly their performance in low-resource languages&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx109" title="">109</a>]</cite>. The great variety of cultures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx110" title="">110</a>]</cite>, languages, localities, identities, and localized medical needs, makes the task of generating a priori static yet comprehensive fairness benchmarks practically infeasible. Measurement and mitigation of bias must move beyond the traditional narrow focus on specific axes that fails to scale globally&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx111" title="">111</a>]</cite>. LLM-based evaluators present a potential solution for preliminary assessments in languages where there are no systematic benchmarks, though prior studies have found these auto-evaluation frameworks to be biased, underscoring the need for calibrating them on native speaker evaluations, and using them with caution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx112" title="">112</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px7">
<h5 class="ltx_title ltx_title_paragraph">Deployment.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS0.SSS0.Px7.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px7.p1.1">This research demonstrates the potential of LLMs for future use in healthcare in the context of diagnostic dialogue. Transitioning from an LLM research prototype that has been evaluated in this study to a safe and robust tool that can be used by healthcare providers, administrators, and people will require significant additional research to ensure the safety, reliability, efficacy, and privacy of the technology. Careful consideration will need to be given to the ethical deployment of this technology including rigorous quality assessment across different clinical settings and research into reliable uncertainty estimation methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx116" title="">116</a>]</cite> that would allow for deferral to human clinical experts when needed. These and other guardrails are needed to mitigate potential overreliance on LLM technologies, with other specific measures for attention to ethical and regulatory requirements particular to future use-cases and the presence of qualified physicians in the loop to safeguard any model outputs. Additional research will also be needed to assess the extent to which biases and security vulnerabilities might arise either from base models or the circumstances of use in deployment, as we have highlighted in our prior work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx12" title="">12</a>]</cite>. Given the continuous evolution of clinical knowledge, it will also be important to develop ways for LLMs to utilize up-to-date clinical information&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#bib.bibx117" title="">117</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The utility of medical AI systems could be greatly improved if they are better able to interact conversationally, anchoring on large-scale medical knowledge while communicating with appropriate levels of empathy and trust.
This research demonstrates the significant potential capabilities of LLM based AI systems for settings involving clinical history-taking and diagnostic dialogue. The performance of AMIE in simulated consultations represents a milestone for the field, as it was assessed along an evaluation framework that considered multiple clinically-relevant axes for conversational diagnostic medical AI. However, the results should be interpreted with appropriate caution. Translating from this limited scope of experimental simulated history-taking and diagnostic dialogue, towards real-world tools for people and those who provide care for them, requires significant additional research and development to ensure the safety, reliability, fairness, efficacy, and privacy of the technology. If successful, we believe AI systems such as AMIE can be at the core of next generation learning health systems that help scale world class healthcare to everyone.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S7.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS0.SSSx1.p1">
<p class="ltx_p" id="S7.SS0.SSSx1.p1.1">This project was an extensive collaboration between many teams at Google Research and Google DeepMind.
We thank Yun Liu, Daniel McDuff, Jake Sunshine, Ali Connell, Paul McGovern and Zoubin Ghahramani for their comprehensive review and detailed feedback on the manuscript. We also thank Sami Lachgar, Lauren Winer, John Guilyard and Maggie Shiels for contributions to the narratives and visuals.
We are grateful to Julie Anne Seguin, Sally Goldman, Yuri Vasilevski, Xinying Song, Akshay Goel, Chu-ling Ko, Abhinav Das, Haiyang Yu, Chang Liu, Yuchen Liu, SiWai Man, Brett Hatfield, Sean Li, Ajay Joshi, Gordon Turner, Annisah Um’rani, Divya Pandya and Preeti Singh for their valuable insights, technical support and
feedback during our research.
We also thank our clinical provider partners in Canada and India for their partnership in conducting the OSCE study. Finally, we are grateful to Dale Webster, Ewa Dominowska, David Fleet, Philip Mansfield, Sushant Prakash, Renee Wong, Susan Thomas, Michael Howell, Karen DeSalvo, Jeff Dean, James Manyika, Zoubin Ghahramani and Demis Hassabis for their support during the course of this project.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS0.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Data Availability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS0.SSSx2.p1">
<p class="ltx_p" id="S7.SS0.SSSx2.p1.1">Some of the real-world datasets used in the development of AMIE are open-source (MedQA). The scenario packs from UK used in the OSCE study are also available for download on the internet.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS0.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Code Availability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS0.SSSx3.p1">
<p class="ltx_p" id="S7.SS0.SSSx3.p1.1">AMIE is an LLM based research AI system for diagnostic dialogue. We are not open-sourcing model code and weights due to the safety implications of unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators, and providers to validate and explore safe onward uses of AMIE. For reproducibility, we have documented technical deep learning methods while keeping the paper accessible to a clinical and general scientific audience. Our work builds upon PaLM 2, for which technical details have been described extensively in the technical report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2401.05654v1#biba.bibx2" title="">119</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS0.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">Competing Interests</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS0.SSSx4.p1">
<p class="ltx_p" id="S7.SS0.SSSx4.p1.1">This study was funded by Alphabet Inc and/or a subsidiary thereof (‘Alphabet’). All authors are employees of Alphabet and may own stock as part of the standard compensation package.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">George Libman Engel and William L Morgan
</span>
<span class="ltx_bibblock">“Interviewing the patient”
</span>
<span class="ltx_bibblock">Saunders, Philadelphia, London, 1973
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">MICHAEL C Peterson et al.
</span>
<span class="ltx_bibblock">“Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx2.1.1">Western Journal of Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx2.2.2">156.2</span>
</span>
<span class="ltx_bibblock">BMJ Publishing Group, 1992, pp. 163
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">John R Hampton et al.
</span>
<span class="ltx_bibblock">“Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx3.1.1">Br Med J</em> <span class="ltx_text ltx_font_bold" id="bib.bibx3.2.2">2.5969</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 1975, pp. 486–489
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jerome P Kassirer
</span>
<span class="ltx_bibblock">“Teaching clinical medicine by iterative hypothesis testing: let’s preach what we practice”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx4.1.1">New England Journal of Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx4.2.2">309.15</span>
</span>
<span class="ltx_bibblock">Mass Medical Soc, 1983, pp. 921–923
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">M Roshan and AP Rao
</span>
<span class="ltx_bibblock">“A study on relative contributions of the history, physical examination and investigations in making medical diagnosis.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx5.1.1">The Journal of the Association of Physicians of India</em> <span class="ltx_text ltx_font_bold" id="bib.bibx5.2.2">48.8</span>, 2000, pp. 771–775
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Gerald Sandler
</span>
<span class="ltx_bibblock">“The importance of the history in the medical clinic and the cost of unnecessary tests”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx6.1.1">American heart journal</em> <span class="ltx_text ltx_font_bold" id="bib.bibx6.2.2">100.6</span>
</span>
<span class="ltx_bibblock">Elsevier, 1980, pp. 928–931
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jonathan Silverman, Suzanne Kurtz and Juliet Draper
</span>
<span class="ltx_bibblock">“Skills for communicating with patients”
</span>
<span class="ltx_bibblock">crc press, 2016
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Timothy Rennie, Jennifer Marriott and Tina P Brock
</span>
<span class="ltx_bibblock">“Global supply of health professionals”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx8.1.1">N Engl J Med</em> <span class="ltx_text ltx_font_bold" id="bib.bibx8.2.2">370.23</span>, 2014, pp. 2246–7
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“GPT-4 Technical Report”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.08774" title="">2303.08774 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"> Google
</span>
<span class="ltx_bibblock">“PaLM 2 Technical Report”, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google/static/documents/palm2techreport.pdf" title="">https://ai.google/static/documents/palm2techreport.pdf</a>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Google Deepmind
</span>
<span class="ltx_bibblock">“Gemini: A Family of Highly Capable Multimodal Models”, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0" title="">https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0</a>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Karan Singhal et al.
</span>
<span class="ltx_bibblock">“Large Language Models Encode Clinical Knowledge”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx12.1.1">arXiv preprint arXiv:2212.13138</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[13]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Karan Singhal et al.
</span>
<span class="ltx_bibblock">“Towards expert-level medical question answering with large language models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx13.1.1">arXiv preprint arXiv:2305.09617</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Harsha Nori et al.
</span>
<span class="ltx_bibblock">“Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx14.1.1">arXiv preprint arXiv:2311.16452</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Romal Thoppilan et al.
</span>
<span class="ltx_bibblock">“LaMDA: Language models for dialog applications”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx15.1.1">arXiv preprint arXiv:2201.08239</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“Introducing ChatGPT”, 2022
</span>
<span class="ltx_bibblock">OpenAI
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Augustin Toma et al.
</span>
<span class="ltx_bibblock">“Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx17.1.1">arXiv preprint arXiv:2305.12031</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Zeming Chen et al.
</span>
<span class="ltx_bibblock">“MEDITRON-70B: Scaling Medical Pretraining for Large Language Models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx18.1.1">arXiv preprint arXiv:2311.16079</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">David Levine
</span>
<span class="ltx_bibblock">“History taking is a complex skill”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx19.1.1">BMJ</em> <span class="ltx_text ltx_font_bold" id="bib.bibx19.2.2">358</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 2017
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ann King and Ruth B Hoppe
</span>
<span class="ltx_bibblock">““Best practice” for patient-centered communication: a narrative review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx20.1.1">Journal of graduate medical education</em> <span class="ltx_text ltx_font_bold" id="bib.bibx20.2.2">5.3</span>
</span>
<span class="ltx_bibblock">The Accreditation Council for Graduate Medical Education Suite 2000, 515&nbsp;…, 2013, pp. 385–393
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[21]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Di Jin et al.
</span>
<span class="ltx_bibblock">“What disease does this patient have? a large-scale open domain question answering dataset from medical exams”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx21.1.1">Applied Sciences</em> <span class="ltx_text ltx_font_bold" id="bib.bibx21.2.2">11.14</span>
</span>
<span class="ltx_bibblock">MDPI, 2021, pp. 6421
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Alistair EW Johnson et al.
</span>
<span class="ltx_bibblock">“MIMIC-III, a freely accessible critical care database”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx22.1.1">Scientific data</em> <span class="ltx_text ltx_font_bold" id="bib.bibx22.2.2">3.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group, 2016, pp. 1–9
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Chung-Cheng Chiu et al.
</span>
<span class="ltx_bibblock">“Speech recognition for medical conversations”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx23.1.1">arXiv preprint arXiv:1711.07274</em>, 2017
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ashish Sharma, Adam S Miner, David C Atkins and Tim Althoff
</span>
<span class="ltx_bibblock">“A computational approach to understanding empathy expressed in text-based mental health support”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx24.1.1">arXiv preprint arXiv:2009.08441</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Yao Fu, Hao Peng, Tushar Khot and Mirella Lapata
</span>
<span class="ltx_bibblock">“Improving language model negotiation with self-play and in-context learning from ai feedback”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx25.1.1">arXiv preprint arXiv:2305.10142</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[26]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Asma Ben Abacha et al.
</span>
<span class="ltx_bibblock">“Overview of the mediqa-chat 2023 shared tasks on the summarization &amp; generation of doctor-patient conversations”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx26.1.1">Proceedings of the 5th Clinical Natural Language Processing Workshop</em>, 2023, pp. 503–513
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[27]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Bogdan Ionescu et al.
</span>
<span class="ltx_bibblock">“Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx27.1.1">International Conference of the Cross-Language Evaluation Forum for European Languages</em>, 2023, pp. 370–396
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[28]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Zhenfeng He et al.
</span>
<span class="ltx_bibblock">“DialMed: A Dataset for Dialogue-based Medication Recommendation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx28.1.1">arXiv preprint arXiv:2203.07094</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[29]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Usman Naseem et al.
</span>
<span class="ltx_bibblock">“Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx29.1.1">Proceedings of the 21st Workshop on Biomedical Language Processing</em>, 2022, pp. 110–115
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[30]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jane Dacre, Mike Besser and Patricia White
</span>
<span class="ltx_bibblock">“MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions (June 2001–July 2002)”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx30.1.1">Clinical Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx30.2.2">3.5</span>
</span>
<span class="ltx_bibblock">Royal College of Physicians, 2003, pp. 452
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[31]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">David A Sloan, Michael B Donnelly, Richard W Schwartz and William E Strodel
</span>
<span class="ltx_bibblock">“The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx31.1.1">Annals of surgery</em> <span class="ltx_text ltx_font_bold" id="bib.bibx31.2.2">222.6</span>
</span>
<span class="ltx_bibblock">Lippincott, Williams,Wilkins, 1995, pp. 735
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[32]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Carol Carraccio and Robert Englander
</span>
<span class="ltx_bibblock">“The objective structured clinical examination: a step in the direction of competency-based evaluation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx32.1.1">Archives of pediatrics &amp; adolescent medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx32.2.2">154.7</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2000, pp. 736–741
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[33]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ronald M Epstein and Edward M Hundert
</span>
<span class="ltx_bibblock">“Defining and assessing professional competence”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx33.1.1">Jama</em> <span class="ltx_text ltx_font_bold" id="bib.bibx33.2.2">287.2</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2002, pp. 226–235
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Joel L Horowitz
</span>
<span class="ltx_bibblock">“The bootstrap”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx34.1.1">Handbook of econometrics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx34.2.2">5</span>
</span>
<span class="ltx_bibblock">Elsevier, 2001, pp. 3159–3228
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[35]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Yoav Benjamini and Yosef Hochberg
</span>
<span class="ltx_bibblock">“Controlling the false discovery rate: a practical and powerful approach to multiple testing”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx35.1.1">Journal of the Royal statistical society: series B (Methodological)</em> <span class="ltx_text ltx_font_bold" id="bib.bibx35.2.2">57.1</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 1995, pp. 289–300
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[36]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Robert F Woolson
</span>
<span class="ltx_bibblock">“Wilcoxon signed-rank test”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx36.1.1">Wiley encyclopedia of clinical trials</em>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2007, pp. 1–3
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Katharina E Keifenheim et al.
</span>
<span class="ltx_bibblock">“Teaching history taking to medical students: a systematic review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx37.1.1">BMC medical education</em> <span class="ltx_text ltx_font_bold" id="bib.bibx37.2.2">15.1</span>
</span>
<span class="ltx_bibblock">BioMed Central, 2015, pp. 1–12
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[38]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Michael J Yedidia et al.
</span>
<span class="ltx_bibblock">“Effect of communications training on medical student performance”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx38.1.1">Jama</em> <span class="ltx_text ltx_font_bold" id="bib.bibx38.2.2">290.9</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2003, pp. 1157–1165
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[39]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Gregory Makoul
</span>
<span class="ltx_bibblock">“Communication skills education in medical school and beyond”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx39.1.1">Jama</em> <span class="ltx_text ltx_font_bold" id="bib.bibx39.2.2">289.1</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2003, pp. 93–93
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[40]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Xiu Hui Tan et al.
</span>
<span class="ltx_bibblock">“Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx40.1.1">BMC medical education</em> <span class="ltx_text ltx_font_bold" id="bib.bibx40.2.2">21</span>
</span>
<span class="ltx_bibblock">Springer, 2021, pp. 1–19
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[41]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Steven E Raper, Meera Gupta, Olugbenga Okusanya and Jon B Morris
</span>
<span class="ltx_bibblock">“Improving communication skills: a course for academic medical center surgery residents and faculty”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx41.1.1">Journal of Surgical education</em> <span class="ltx_text ltx_font_bold" id="bib.bibx41.2.2">72.6</span>
</span>
<span class="ltx_bibblock">Elsevier, 2015, pp. e202–e211
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[42]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Martin Von Fragstein et al.
</span>
<span class="ltx_bibblock">“UK consensus statement on the content of communication curricula in undergraduate medical education”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx42.1.1">Medical education</em> <span class="ltx_text ltx_font_bold" id="bib.bibx42.2.2">42.11</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2008, pp. 1100–1107
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[43]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Hanneke De Haes and Jozien Bensing
</span>
<span class="ltx_bibblock">“Endpoints in medical communication research, proposing a framework of functions and outcomes”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx43.1.1">Patient education and counseling</em> <span class="ltx_text ltx_font_bold" id="bib.bibx43.2.2">74.3</span>
</span>
<span class="ltx_bibblock">Elsevier, 2009, pp. 287–294
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[44]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ronald M Epstein and Richard L Street Jr
</span>
<span class="ltx_bibblock">“Patient-centered communication in cancer care: promoting healing and reducing suffering”, 2007
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_tag_bibitem">[45]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Julie M Schirmer et al.
</span>
<span class="ltx_bibblock">“Assessing communication competence: a review of current tools”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx45.1.1">Family Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx45.2.2">37.3</span>, 2005, pp. 184–92
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_tag_bibitem">[46]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jonathan R Nichol, Joshua Henrina Sundjaja and Grant Nelson
</span>
<span class="ltx_bibblock">“Medical history”
</span>
<span class="ltx_bibblock">StatPearls Publishing, Treasure Island (FL), 2018
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://europepmc.org/books/NBK534249" title="">http://europepmc.org/books/NBK534249</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_tag_bibitem">[47]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Claire Denness
</span>
<span class="ltx_bibblock">“What are consultation models for?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx47.1.1">InnovAiT</em> <span class="ltx_text ltx_font_bold" id="bib.bibx47.2.2">6.9</span>
</span>
<span class="ltx_bibblock">Sage Publications Sage UK: London, England, 2013, pp. 592–599
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx48">
<span class="ltx_tag ltx_tag_bibitem">[48]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">See Chai Carol Chan et al.
</span>
<span class="ltx_bibblock">“Implementation of virtual OSCE in health professions education: A systematic review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx48.1.1">Medical Education</em>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx49">
<span class="ltx_tag ltx_tag_bibitem">[49]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Paweł Budzianowski et al.
</span>
<span class="ltx_bibblock">“Multiwoz–a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx49.1.1">arXiv preprint arXiv:1810.00278</em>, 2018
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx50">
<span class="ltx_tag ltx_tag_bibitem">[50]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Wei Wei, Quoc Le, Andrew Dai and Jia Li
</span>
<span class="ltx_bibblock">“Airdialogue: An environment for goal-oriented dialogue research”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx50.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 2018, pp. 3844–3854
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx51">
<span class="ltx_tag ltx_tag_bibitem">[51]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jessy Lin, Nicholas Tomlin, Jacob Andreas and Jason Eisner
</span>
<span class="ltx_bibblock">“Decision-Oriented Dialogue for Human-AI Collaboration”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.20076" title="">2305.20076 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx52">
<span class="ltx_tag ltx_tag_bibitem">[52]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ashish Vaswani et al.
</span>
<span class="ltx_bibblock">“Attention is all you need”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx52.1.1">Advances in neural information processing systems</em> <span class="ltx_text ltx_font_bold" id="bib.bibx52.2.2">30</span>, 2017
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx53">
<span class="ltx_tag ltx_tag_bibitem">[53]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Long Ouyang et al.
</span>
<span class="ltx_bibblock">“Training language models to follow instructions with human feedback”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx53.1.1">arXiv preprint arXiv:2203.02155</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx54">
<span class="ltx_tag ltx_tag_bibitem">[54]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jieyu Zhao et al.
</span>
<span class="ltx_bibblock">“Ethical-advice taker: Do language models understand natural language interventions?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx54.1.1">arXiv preprint arXiv:2106.01465</em>, 2021
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx55">
<span class="ltx_tag ltx_tag_bibitem">[55]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">William Saunders et al.
</span>
<span class="ltx_bibblock">“Self-critiquing models for assisting human evaluators”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx55.1.1">arXiv preprint arXiv:2206.05802</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx56">
<span class="ltx_tag ltx_tag_bibitem">[56]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jérémy Scheurer et al.
</span>
<span class="ltx_bibblock">“Training language models with language feedback at scale”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx56.1.1">arXiv preprint arXiv:2303.16755</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx57">
<span class="ltx_tag ltx_tag_bibitem">[57]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Amelia Glaese et al.
</span>
<span class="ltx_bibblock">“Improving alignment of dialogue agents via targeted human judgements”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx57.1.1">arXiv preprint arXiv:2209.14375</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx58">
<span class="ltx_tag ltx_tag_bibitem">[58]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Yuntao Bai et al.
</span>
<span class="ltx_bibblock">“Constitutional AI: Harmlessness from AI feedback”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx58.1.1">arXiv preprint arXiv:2212.08073</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx59">
<span class="ltx_tag ltx_tag_bibitem">[59]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Amanda Askell et al.
</span>
<span class="ltx_bibblock">“A general language assistant as a laboratory for alignment”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx59.1.1">arXiv preprint arXiv:2112.00861</em>, 2021
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx60">
<span class="ltx_tag ltx_tag_bibitem">[60]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Joel Shor et al.
</span>
<span class="ltx_bibblock">“Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx60.1.1">arXiv preprint arXiv:2303.05737</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx61">
<span class="ltx_tag ltx_tag_bibitem">[61]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Asma Ben Abacha, Eugene Agichtein, Yuval Pinter and Dina Demner-Fushman
</span>
<span class="ltx_bibblock">“Overview of the medical question answering task at TREC 2017 LiveQA.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx61.1.1">TREC</em>, 2017, pp. 1–12
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx62">
<span class="ltx_tag ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">William Wallace et al.
</span>
<span class="ltx_bibblock">“The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx62.1.1">NPJ Digital Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx62.2.2">5.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group UK London, 2022, pp. 118
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx63">
<span class="ltx_tag ltx_tag_bibitem">[63]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Dan Zeltzer et al.
</span>
<span class="ltx_bibblock">“Diagnostic accuracy of artificial intelligence in virtual primary care”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx63.1.1">Mayo Clinic Proceedings: Digital Health</em> <span class="ltx_text ltx_font_bold" id="bib.bibx63.2.2">1.4</span>
</span>
<span class="ltx_bibblock">Elsevier, 2023, pp. 480–489
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx64">
<span class="ltx_tag ltx_tag_bibitem">[64]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Shreya Johri et al.
</span>
<span class="ltx_bibblock">“Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx64.1.1">medRxiv</em>
</span>
<span class="ltx_bibblock">Cold Spring Harbor Laboratory Press, 2023, pp. 2023–09
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx65">
<span class="ltx_tag ltx_tag_bibitem">[65]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Guangtao Zeng et al.
</span>
<span class="ltx_bibblock">“MedDialog: Large-scale medical dialogue datasets”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx65.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020, pp. 9241–9250
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx66">
<span class="ltx_tag ltx_tag_bibitem">[66]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Wenge Liu et al.
</span>
<span class="ltx_bibblock">“MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx66.1.1">CCF International Conference on Natural Language Processing and Chinese Computing</em>, 2022, pp. 447–459
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx67">
<span class="ltx_tag ltx_tag_bibitem">[67]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Deeksha Varshney, Aizan Zafar, Niranshu Kumar Behra and Asif Ekbal
</span>
<span class="ltx_bibblock">“Cdialog: A multi-turn COVID-19 conversation dataset for entity-aware dialog generation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx67.1.1">arXiv preprint arXiv:2212.06049</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx68">
<span class="ltx_tag ltx_tag_bibitem">[68]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Guojun Yan et al.
</span>
<span class="ltx_bibblock">“ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx68.1.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2022, pp. 3013–3024
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx69">
<span class="ltx_tag ltx_tag_bibitem">[69]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Christopher J Kelly et al.
</span>
<span class="ltx_bibblock">“Key challenges for delivering clinical impact with artificial intelligence”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx69.1.1">BMC medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx69.2.2">17</span>
</span>
<span class="ltx_bibblock">Springer, 2019, pp. 1–9
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx70">
<span class="ltx_tag ltx_tag_bibitem">[70]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Daniel McDuff et al.
</span>
<span class="ltx_bibblock">“Towards Accurate Differential Diagnosis with Large Language Models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx70.1.1">arXiv preprint arXiv:2312.00164</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx71">
<span class="ltx_tag ltx_tag_bibitem">[71]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Zahir Kanjee, Byron Crowe and Adam Rodman
</span>
<span class="ltx_bibblock">“Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx71.1.1">JAMA</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx72">
<span class="ltx_tag ltx_tag_bibitem">[72]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Hannah L Semigran, Jeffrey A Linder, Courtney Gidengil and Ateev Mehrotra
</span>
<span class="ltx_bibblock">“Evaluation of symptom checkers for self diagnosis and triage: audit study”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx72.1.1">BMJ</em> <span class="ltx_text ltx_font_bold" id="bib.bibx72.2.2">351</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 2015
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx73">
<span class="ltx_tag ltx_tag_bibitem">[73]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">John W Ayers et al.
</span>
<span class="ltx_bibblock">“Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx73.1.1">JAMA Internal Medicine</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx74">
<span class="ltx_tag ltx_tag_bibitem">[74]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“ChatGPT”, 2023
</span>
<span class="ltx_bibblock">OpenAI
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/chat" title="">https://chat.openai.com/chat</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx75">
<span class="ltx_tag ltx_tag_bibitem">[75]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Sara Carrillo de Albornoz, Kah-Ling Sia and Anthony Harris
</span>
<span class="ltx_bibblock">“The effectiveness of teleconsultations in primary care: systematic review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx75.1.1">Family Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx75.2.2">39.1</span>
</span>
<span class="ltx_bibblock">Oxford University Press UK, 2022, pp. 168–182
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx76">
<span class="ltx_tag ltx_tag_bibitem">[76]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">George A Wharton, Harpreet S Sood, Amanda Sissons and Elias Mossialos
</span>
<span class="ltx_bibblock">“Virtual primary care: fragmentation or integration?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx76.1.1">The Lancet Digital Health</em> <span class="ltx_text ltx_font_bold" id="bib.bibx76.2.2">1.7</span>
</span>
<span class="ltx_bibblock">Elsevier, 2019, pp. e330–e331
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx77">
<span class="ltx_tag ltx_tag_bibitem">[77]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Aı̈na Fuster-Casanovas and Josep Vidal-Alaball
</span>
<span class="ltx_bibblock">“Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx77.1.1">International Journal of Integrated Care</em> <span class="ltx_text ltx_font_bold" id="bib.bibx77.2.2">22.3</span>
</span>
<span class="ltx_bibblock">Ubiquity Press, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx78">
<span class="ltx_tag ltx_tag_bibitem">[78]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Victoria Hammersley et al.
</span>
<span class="ltx_bibblock">“Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx78.1.1">British Journal of General Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx78.2.2">69.686</span>
</span>
<span class="ltx_bibblock">British Journal of General Practice, 2019, pp. e595–e604
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx79">
<span class="ltx_tag ltx_tag_bibitem">[79]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">David A Gross et al.
</span>
<span class="ltx_bibblock">“Patient satisfaction with time spent with their physician”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx79.1.1">Journal of Family Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx79.2.2">47.2</span>
</span>
<span class="ltx_bibblock">[New York, Appleton-Century-Crofts], 1998, pp. 133–138
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx80">
<span class="ltx_tag ltx_tag_bibitem">[80]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Kiek Tates et al.
</span>
<span class="ltx_bibblock">“The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx80.1.1">Journal of medical Internet research</em> <span class="ltx_text ltx_font_bold" id="bib.bibx80.2.2">19.12</span>
</span>
<span class="ltx_bibblock">JMIR Publications Toronto, Canada, 2017, pp. e421
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx81">
<span class="ltx_tag ltx_tag_bibitem">[81]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Stephen J Zyzanski, Kurt C Stange, Doreen M Langa and Susan A Flocke
</span>
<span class="ltx_bibblock">“Trade-offs in high-volume primary care practice”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx81.1.1">Journal of Family Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx81.2.2">46.5</span>
</span>
<span class="ltx_bibblock">[New York, Appleton-Century-Crofts], 1998, pp. 397–402
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx82">
<span class="ltx_tag ltx_tag_bibitem">[82]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Krishnamurthy Dvijotham et al.
</span>
<span class="ltx_bibblock">“Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx82.1.1">Nature Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx82.2.2">29.7</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group US New York, 2023, pp. 1814–1820
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx83">
<span class="ltx_tag ltx_tag_bibitem">[83]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Julian Bird and Steven A Cohen-Cole
</span>
<span class="ltx_bibblock">“The three-function model of the medical interview”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx83.1.1">Methods in teaching consultation-liaison psychiatry</em> <span class="ltx_text ltx_font_bold" id="bib.bibx83.2.2">20</span>
</span>
<span class="ltx_bibblock">Karger Publishers, 1990, pp. 65–88
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx84">
<span class="ltx_tag ltx_tag_bibitem">[84]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Agnes G Rezler, James A Woolliscroft and Summers G Kalishman
</span>
<span class="ltx_bibblock">“What is missing from patient histories?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx84.1.1">Medical Teacher</em> <span class="ltx_text ltx_font_bold" id="bib.bibx84.2.2">13.3</span>
</span>
<span class="ltx_bibblock">Taylor &amp; Francis, 1991, pp. 245–252
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx85">
<span class="ltx_tag ltx_tag_bibitem">[85]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ellen E Rosenberg
</span>
<span class="ltx_bibblock">“Lessons for Clinicians From Physician-Patient”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx85.1.1">Arch Fam Med</em> <span class="ltx_text ltx_font_bold" id="bib.bibx85.2.2">6</span>, 1997, pp. 279–283
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx86">
<span class="ltx_tag ltx_tag_bibitem">[86]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Robert Charles Smith
</span>
<span class="ltx_bibblock">“Patient-centered interviewing: an evidence-based method”
</span>
<span class="ltx_bibblock">Lippincott Williams &amp; Wilkins, 2002
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx87">
<span class="ltx_tag ltx_tag_bibitem">[87]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Donald M Berwick, Thomas W Nolan and John Whittington
</span>
<span class="ltx_bibblock">“The triple aim: care, health, and cost”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx87.1.1">Health affairs</em> <span class="ltx_text ltx_font_bold" id="bib.bibx87.2.2">27.3</span>
</span>
<span class="ltx_bibblock">Project HOPE-The People-to-People Health Foundation, Inc., 2008, pp. 759–769
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx88">
<span class="ltx_tag ltx_tag_bibitem">[88]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Thomas Bodenheimer and Christine Sinsky
</span>
<span class="ltx_bibblock">“From triple to quadruple aim: care of the patient requires care of the provider”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx88.1.1">The Annals of Family Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx88.2.2">12.6</span>
</span>
<span class="ltx_bibblock">Annals Family Med, 2014, pp. 573–576
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx89">
<span class="ltx_tag ltx_tag_bibitem">[89]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">T Elaine Adamson, Jeane M Tschann, DS Gullion and AA Oppenberg
</span>
<span class="ltx_bibblock">“Physician communication skills and malpractice claims. A complex relationship.”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx89.1.1">Western Journal of Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bibx89.2.2">150.3</span>
</span>
<span class="ltx_bibblock">BMJ Publishing Group, 1989, pp. 356
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx90">
<span class="ltx_tag ltx_tag_bibitem">[90]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jonathan Silverman and Paul Kinnersley
</span>
<span class="ltx_bibblock">“Doctors’ non-verbal behaviour in consultations: look at the patient before you look at the computer”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx90.1.1">British Journal of General Practice</em> <span class="ltx_text ltx_font_bold" id="bib.bibx90.2.2">60.571</span>
</span>
<span class="ltx_bibblock">British Journal of General Practice, 2010, pp. 76–78
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx91">
<span class="ltx_tag ltx_tag_bibitem">[91]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Urna Rahman and Nick Cooling
</span>
<span class="ltx_bibblock">“Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx91.1.1">Medical Research Archives</em> <span class="ltx_text ltx_font_bold" id="bib.bibx91.2.2">11.4</span>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx92">
<span class="ltx_tag ltx_tag_bibitem">[92]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ahmad Kantar et al.
</span>
<span class="ltx_bibblock">“History taking as a diagnostic tool in children with chronic cough”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx92.1.1">Frontiers in pediatrics</em> <span class="ltx_text ltx_font_bold" id="bib.bibx92.2.2">10</span>
</span>
<span class="ltx_bibblock">Frontiers, 2022, pp. 850912
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx93">
<span class="ltx_tag ltx_tag_bibitem">[93]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Winny Setyonugroho, Kieran M Kennedy and Thomas JB Kropmans
</span>
<span class="ltx_bibblock">“Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx93.1.1">Patient education and counseling</em> <span class="ltx_text ltx_font_bold" id="bib.bibx93.2.2">98.12</span>
</span>
<span class="ltx_bibblock">Elsevier, 2015, pp. 1482–1491
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx94">
<span class="ltx_tag ltx_tag_bibitem">[94]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Laura Weidinger et al.
</span>
<span class="ltx_bibblock">“Taxonomy of risks posed by language models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx94.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 214–229
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx95">
<span class="ltx_tag ltx_tag_bibitem">[95]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Isabel O. Gallegos et al.
</span>
<span class="ltx_bibblock">“Bias and Fairness in Large Language Models: A Survey”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.00770" title="">2309.00770 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx96">
<span class="ltx_tag ltx_tag_bibitem">[96]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Rachel L Johnson, Debra Roter, Neil R Powe and Lisa A Cooper
</span>
<span class="ltx_bibblock">“Patient race/ethnicity and quality of patient–physician communication during medical visits”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx96.1.1">American journal of public health</em> <span class="ltx_text ltx_font_bold" id="bib.bibx96.2.2">94.12</span>
</span>
<span class="ltx_bibblock">American Public Health Association, 2004, pp. 2084–2090
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx97">
<span class="ltx_tag ltx_tag_bibitem">[97]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Debra L Roter, Judith A Hall and Yutaka Aoki
</span>
<span class="ltx_bibblock">“Physician gender effects in medical communication: a meta-analytic review”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx97.1.1">Jama</em> <span class="ltx_text ltx_font_bold" id="bib.bibx97.2.2">288.6</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2002, pp. 756–764
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx98">
<span class="ltx_tag ltx_tag_bibitem">[98]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ethan Perez et al.
</span>
<span class="ltx_bibblock">“Red teaming language models with language models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx98.1.1">arXiv preprint arXiv:2202.03286</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx99">
<span class="ltx_tag ltx_tag_bibitem">[99]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Deep Ganguli et al.
</span>
<span class="ltx_bibblock">“Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx99.1.1">arXiv preprint arXiv:2209.07858</em>, 2022
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx100">
<span class="ltx_tag ltx_tag_bibitem">[100]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jiahao Yu, Xingwei Lin and Xinyu Xing
</span>
<span class="ltx_bibblock">“Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx100.1.1">arXiv preprint arXiv:2309.10253</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx101">
<span class="ltx_tag ltx_tag_bibitem">[101]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Suyu Ge et al.
</span>
<span class="ltx_bibblock">“MART: Improving LLM Safety with Multi-round Automatic Red-Teaming”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx101.1.1">arXiv preprint arXiv:2311.07689</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx102">
<span class="ltx_tag ltx_tag_bibitem">[102]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Margaret Mitchell et al.
</span>
<span class="ltx_bibblock">“Model cards for model reporting”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx102.1.1">Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 220–229
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx103">
<span class="ltx_tag ltx_tag_bibitem">[103]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Anamaria Crisan, Margaret Drouhard, Jesse Vig and Nazneen Rajani
</span>
<span class="ltx_bibblock">“Interactive model cards: A human-centered approach to model documentation”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx103.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 427–439
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx104">
<span class="ltx_tag ltx_tag_bibitem">[104]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Mahima Pushkarna, Andrew Zaldivar and Oddur Kjartansson
</span>
<span class="ltx_bibblock">“Data cards: Purposeful and transparent dataset documentation for responsible ai”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx104.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 1776–1826
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx105">
<span class="ltx_tag ltx_tag_bibitem">[105]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Monojit Choudhury and Amit Deshpande
</span>
<span class="ltx_bibblock">“How Linguistically Fair Are Multilingual Pre-Trained Language Models?”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx105.1.1">Proceedings of the AAAI conference on artificial intelligence</em> <span class="ltx_text ltx_font_bold" id="bib.bibx105.2.2">35.14</span>, 2021, pp. 12710–12718
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx106">
<span class="ltx_tag ltx_tag_bibitem">[106]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Zeerak Talat et al.
</span>
<span class="ltx_bibblock">“You reap what you sow: On the challenges of bias evaluation under multilingual settings”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx106.1.1">Proceedings of BigScience Episode# 5–Workshop on Challenges &amp; Perspectives in Creating Large Language Models</em>, 2022, pp. 26–41
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx107">
<span class="ltx_tag ltx_tag_bibitem">[107]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Sanchit Ahuja et al.
</span>
<span class="ltx_bibblock">“MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.07463" title="">2311.07463 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx108">
<span class="ltx_tag ltx_tag_bibitem">[108]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Ayyoob ImaniGooghari et al.
</span>
<span class="ltx_bibblock">“Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx108.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Association for Computational Linguistics, 2023
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.18653/v1/2023.acl-long.61" title="">10.18653/v1/2023.acl-long.61</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx109">
<span class="ltx_tag ltx_tag_bibitem">[109]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty and Lidong Bing
</span>
<span class="ltx_bibblock">“Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.11372" title="">2306.11372 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx110">
<span class="ltx_tag ltx_tag_bibitem">[110]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Tarek Naous, Michael J. Ryan, Alan Ritter and Wei Xu
</span>
<span class="ltx_bibblock">“Having Beer after Prayer? Measuring Cultural Bias in Large Language Models”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.14456" title="">2305.14456 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx111">
<span class="ltx_tag ltx_tag_bibitem">[111]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Krithika Ramesh, Sunayana Sitaram and Monojit Choudhury
</span>
<span class="ltx_bibblock">“Fairness in Language Models Beyond English: Gaps and Challenges”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.12578" title="">2302.12578 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx112">
<span class="ltx_tag ltx_tag_bibitem">[112]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Rishav Hada et al.
</span>
<span class="ltx_bibblock">“Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.07462" title="">2309.07462 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx113">
<span class="ltx_tag ltx_tag_bibitem">[113]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Victor Quach et al.
</span>
<span class="ltx_bibblock">“Conformal Language Modeling”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.10193" title="">2306.10193 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx114">
<span class="ltx_tag ltx_tag_bibitem">[114]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jiuhai Chen and Jonas Mueller
</span>
<span class="ltx_bibblock">“Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.16175" title="">2308.16175 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx115">
<span class="ltx_tag ltx_tag_bibitem">[115]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Yuheng Huang et al.
</span>
<span class="ltx_bibblock">“Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2307.10236" title="">2307.10236 [cs.SE]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx116">
<span class="ltx_tag ltx_tag_bibitem">[116]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Qi Yang et al.
</span>
<span class="ltx_bibblock">“Uncertainty-aware Language Modeling for Selective Question Answering”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.15451" title="">2311.15451 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx117">
<span class="ltx_tag ltx_tag_bibitem">[117]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Angeliki Lazaridou et al.
</span>
<span class="ltx_bibblock">“Mind the gap: Assessing temporal generalization in neural language models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx117.1.1">Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bibx117.2.2">34</span>, 2021, pp. 29348–29363
</span>
</li>
</ul>
</section>
<section class="ltx_bibliography" id="biba" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="biba.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[118]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Jason Wei et al.
</span>
<span class="ltx_bibblock">“Chain-of-thought prompting elicits reasoning in large language models”
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="biba.bibx1.1.1">Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="biba.bibx1.2.2">35</span>, 2022, pp. 24824–24837
</span>
</li>
<li class="ltx_bibitem" id="biba.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[119]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"> Google
</span>
<span class="ltx_bibblock">“PaLM 2 Technical Report”, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google/static/documents/palm2techreport.pdf" title="">https://ai.google/static/documents/palm2techreport.pdf</a>, 2023
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1" lang="en">appendix</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
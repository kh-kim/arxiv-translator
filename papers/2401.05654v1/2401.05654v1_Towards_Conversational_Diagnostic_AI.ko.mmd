# 대화형 진단 AI를 대상으로

Tao Tu\({}^{*,1}\)

Anil Palepu\({}^{*,1}\)

Mike Schaekermann\({}^{*,1}\)

Khaled Saab\({}^{1}\)

Jan Freyberg\({}^{1}\)

Ryutaro Tanno\({}^{2}\)

Amy Wang\({}^{1}\)

Brenna Li\({}^{1}\)

Mohamed Amin\({}^{1}\)

Nenad Tomasev\({}^{2}\)

Shekoofeh Azizi\({}^{2}\)

Karan Singhal\({}^{1}\)

Yong Cheng\({}^{2}\)

Le Hou\({}^{1}\)

Albert Webson\({}^{2}\)

Kavita Kulkarni\({}^{1}\)

S. Sara Mahdavi\({}^{2}\)

Christopher Semturs\({}^{1}\)

Juraj Gottweis\({}^{1}\)

Joelle Barral\({}^{2}\)

Katherine Chou\({}^{1}\)

그렉 S. Corrado\({}^{1}\)

Yossi Matias\({}^{1}\)

Alan Karthikesalingam\({}^{\dagger,1}\)

Vivek Natarajan\({}^{\dagger,1}\)

\({}^{1}\)Google Research

\({}^{2}\)Google DeepMind

###### Abstract

의학의 핵심에는 의사-환자 대화가 있으며, 여기서 숙련된 병력 수집은 정확한 진단, 효과적인 관리 및 지속적인 신뢰의 길을 열어준다. 진단 대화가 가능한 인공지능(AI) 시스템은 접근성과 일관성, 돌봄의 질을 높일 수 있다. 그러나 임상의의 전문 지식을 근사화하는 것은 눈에 띄는 큰 도전이다. 여기서는 진단대화에 최적화된 LLM(Large Language Model) 기반 AI 시스템인 AMIE(Articulate Medical Intelligence Explorer)를 소개한다. AMIE는 다양한 질병 조건, 전문 분야 및 컨텍스트에 걸친 확장 학습을 위해 자동화된 피드백 메커니즘과 함께 새로운 자기 플레이 기반 시뮬레이션 환경을 사용한다. 우리는 병력 작성, 진단 정확도, 관리 추론, 의사소통 기술 및 공감을 포함한 임상적으로 의미 있는 수행 축을 평가하기 위한 프레임워크를 설계했다. 우리는 객관 구조 임상 검사(OSCE) 스타일로 검증된 환자 행위자와 텍스트 기반 상담의 무작위 이중 맹검 교차 연구에서 AMIE의 성능을 1차 진료 의사(PCP)의 성능과 비교했다. 이 연구에는 캐나다, 영국, 인도의 임상 제공자의 149개 사례 시나리오, AMIE와의 비교를 위한 20개의 PCP, 전문 의사 및 환자 행위자의 평가가 포함되었다. AMIE는 전문의에 따르면 32개 축 중 28개, 환자 행위자에 따르면 26개 축 중 24개에서 더 큰 진단 정확도와 우수한 성능을 보여주었다. 우리의 연구는 몇 가지 한계를 가지고 있으며 적절한 주의를 기울여 해석해야 한다. 임상의는 대규모 LLM-환자 상호 작용을 허용하지만 일반적인 임상 실습을 대표하지 않는 낯선 동기식 문자 채팅으로 제한되었다. AMIE가 실제 환경으로 변환되기 전에 추가 연구가 필요하지만 결과는 대화형 진단 AI를 향한 이정표를 나타낸다.

\({}^{*}\) 등 기여도 \ (\dagger\) 동등한 리더십. \ (\ddagger\) Corresponding authors: {taotu, mikeshake, alankarth, natvir}@google.com

## 1 Introduction

의사와 환자 사이의 대화는 효과적이고 온정적인 치료의 기본이다. 의학 인터뷰는 "의사가 사용할 수 있는 가장 강력하고 민감하며 다재다능한 도구"로 명명되었다[1]. 일부 설정에서는 60-80%의 진단이 임상 병력 청취만으로 이루어진다고 여겨진다[2, 3, 4, 5, 6]. 의사-환자 대화는 병력 청취 및 진단을 넘어 확장되며, 라포와 신뢰를 확립하고 건강 요구를 해결하기 위한 도구 역할을 하며 환자가 선호, 기대 및 우려를 설명하는 정보에 입각한 결정을 내릴 수 있도록 권한을 부여할 수 있는 복잡한 상호 작용이다. 임상의는 임상 역사 촬영과 광범위한 "진단 대화"에서 상당한 기술을 산출하지만 이 전문지식에 대한 접근은 여전히 일시적이고 전 세계적으로 부족하다[8].

범용 대형 언어 모델(LLM)[9, 10, 11]의 최근 진보는 인공 지능(AI) 시스템이 자연주의적 대화를 수행하기 위해 관련 컨텍스트를 계획, 추론 및 통합할 수 있는 능력을 갖는다는 것을 보여주었다. 이러한 발전은 완전한 대화형 AI의 개발을 위해 의학에서 AI의 가능성을 다시 생각할 수 있는 기회를 제공한다. 이러한 의료 AI 시스템은 임상 언어를 이해하고 불확실성 하에서 지능적으로 정보를 획득하고 환자 및 환자를 돌보는 사람들과 자연스럽고 진단적으로 유용한 의료 대화에 참여할 것이다. 임상 및 진단 대화를 할 수 있는 AI 시스템의 잠재적 실제 유용성은 광범위하며, 이러한 기능의 개발은 진단 및 예후 전문지식에 대한 접근성을 개선하고, 치료의 품질, 일관성, 가용성 및 경제성을 개선하고, (특히 의료 불균형에 직면한 인구의 경우) 더 나은 건강 결과를 실현하는 데 도움이 될 수 있기 때문이다.

그러나 LLM은 임상 지식을 인코딩하는 것으로 나타났으며 매우 정확한 단일 회전 의료 질문 답변이 가능한 것으로 입증되었지만[12, 13, 14], 이들의 대화 능력은 임상 의학 외부의 도메인에 맞게 조정되었다[15, 16]. 건강을 위한 LLM에서의 이전 작업[12, 13, 14, 17, 18]은 아직 AI 시스템의 임상 이력 작성 및 진단 대화 능력을 엄격하게 조사하거나 전문 임상의의 광범위한 능력과 비교하여 이를 맥락화하지 않았다.

임상의가 진단 및 관리 계획을 도출하는 임상 이력 작성 및 진단 대화는 상황에 따라 최적의 행동이 크게 좌우되는 복잡한 기술[19]을 나타낸다. 따라서 진단 대화의 구조와 완성도를 포함하여 진단 대화의 품질을 평가하기 위해서는 여러 평가 축이 필요하다.

그림 1: **기여 개요** AMIE는 진단 대화에 최적화 된 대화형 의료 AI입니다. AMIE는 다양한 의료 추론, 질문 응답 및 요약 데이터 세트와 함께 실제 및 시뮬레이션된 의료 대화의 조합으로 미세 조정된다. 특히, 우리는 다양한 의학적 맥락과 전문 분야에 걸쳐 AMIE의 능력을 확장하기 위해 자동화된 피드백 메커니즘을 가진 셀프 플레이 기반 시뮬레이션 대화 환경을 설계했다. 구체적으로, 이 반복적인 자기 개선 프로세스는 두 개의 자기 재생 루프, 즉 (1) AMIE가 AI 환자 에이전트와의 시뮬레이션된 대화에서 행동을 개선하기 위해 맥락 내 비평 피드백을 활용하는 "내부" 자기 재생 루프, (2) 정제된 시뮬레이션된 대화 세트가 후속 미세 조정 반복에 통합되는 "외부" 자기 재생 루프로 구성되었다. 온라인 추론 동안 AMIE는 각 대화 턴에서 환자에 대한 정확하고 근거 있는 답변에 도달하기 위해 현재 대화에 조건화된 응답을 점진적으로 정제하기 위해 연쇄 추론 전략을 사용했다. 우리는 텍스트 인터페이스를 통해 AMIE 또는 1차 진료 의사(PCP)와 상호 작용하는 검증된 시뮬레이션 환자 행위자와 함께 블라인드 원격 객관 구조 임상 검사(OSCE)를 설계하고 수행했다. 전문 의사(32개 중 28개)와 환자 행위자(26개 중 24개) 관점에 해당하는 다축을 통해 AMIE는 나머지 부분에서 열등하지 않은 반면 PCP보다 우수한 것으로 평가되었다.

그 결과 병력, 진단 정확성, 관리 계획과 그 근거의 적절성, 관계 구축, 개인 존중 및 의사소통 효능감과 같은 환자 중심 고려 사항이 도출되었다. LLM의 대화 가능성이 의학에서 실현되려면 임상의와 환자 사이의 역사 촬영 및 진단 대화에만 고유한 특성과 같은 특성에 대한 의료 AI 시스템의 개발 및 평가를 더 잘 최적화해야 할 상당한 충족되지 않은 필요성이 있다.

이 작업에서는 임상 병력 작성 및 진단 추론을 위한 대화형 의료 AI 시스템으로의 진행 상황을 자세히 설명한다.

우리의 주요 기여는 다음과 같이 요약된다.

* 임상 이력 작성 및 진단 대화에 최적화된 LLM 기반 AI 시스템인 AMIE(Articulate Medical Intelligence Explorer)를 도입했습니다.
* 여러 전문 분야 및 시나리오에 걸쳐 AMIE를 확장하기 위해 학습 프로세스를 강화하고 가속화하기 위해 자동화된 피드백 메커니즘이 있는 새로운 자체 플레이 기반 시뮬레이션 진단 대화 환경을 개발했습니다. 또한 AMIE의 진단 정확도와 대화 품질을 향상시키기 위해 추론 시간 연쇄 추론 전략을 도입했다.
* 임상 의사 중심 및 환자 중심 메트릭을 모두 포함하는 진단 대화형 의료 AI의 이력 작성, 진단 추론, 의사 소통 기술 및 공감을 평가하기 위한 파일럿 평가 루브릭을 개발했다.
* 캐나다, 영국 및 인도의 임상 공급자의 149개 사례 시나리오를 사용하여 블라인드 원격 OSCE 연구를 설계 및 수행하여 검증된 환자 행위자와 상담을 수행할 때 AMIE와 PCP의 무작위 및 균형 비교를 가능하게 했다. AMIE는 다양한 측정(예: 차등 진단 목록의 top-1 및 top-3 정확도)에 의해 평가된 PCP에 비해 우수한 진단 정확도를 나타냈다. 전문의 관점에서는 32개의 평가 축 중 28개, 환자 행위자 관점에서는 26개의 평가 축 중 24개에서 AMIE가 PCP보다 우수하면서도 나머지는 비열등하다는 평가를 받았다.
* AMIE의 기능을 추가로 이해하고 특성화하기 위해 다양한 삭제를 수행하고 중요한 한계를 강조했으며 AMIE의 실제 임상 번역을 위한 주요 다음 단계를 제안했다.

우리의 연구는 특히 진단 대화에 특화된 LLM과 환자 사이의 잠재적으로 대규모 상호 작용을 가능하게 하지만 원격 상담을 위한 PCP에는 익숙하지 않은 텍스트 채팅 인터페이스를 활용했다는 중요한 한계를 가지고 있다. 따라서 우리의 연구는 (원격)의료에서 일반적인 관행의 대표자로 간주되어서는 안 된다.

## 2 AMIE: 진단 대화를 위한 LLM 기반 AI 시스템

다음 섹션에서는 진단 대화 기능 및 임상 의사 소통 기술을 위해 AMIE를 최적화하도록 설계된 실제 데이터 세트, 시뮬레이션된 셀프 플레이 환경, 미세 조정 프로세스 및 추론 시간 연쇄 추론에 대해 설명한다.

### AMIE에 대한 실제 데이터 세트

AMIE는 객관식 의료 질문 응답, 전문가 맞춤형 장기 형태 의료 추론, 전자 건강 기록(EHR) 노트 요약 및 대규모 전사 의료 대화 상호 작용을 포함한 다양한 실제 데이터 세트를 사용하여 개발되었다. 아래에서 자세히 설명하는 바와 같이, AMIE를 위한 훈련 과제 혼합은 대화 생성 과제 외에 의학 질문-답변, 추론, 요약 과제로 구성되었다.

**의료 추론** 4개 또는 5개의 가능한 답변이 있는 미국 의료 면허 시험(USMLE) 객관식 개방형 도메인 질문으로 구성된 MedQA(객관식) 데이터 세트를 사용했다[21]. 훈련 세트는 11,450개의 문항으로 구성되었고 테스트 세트는 1,273개의 문항으로 구성되었다. 또한 임상 전문가가 정답으로 이어지는 단계별 추론을 만든 훈련 세트에서 191개의 MedQA 질문을 선별했다[13].

**Long-form Medical Question Answering.** 여기에 사용된 데이터 세트는 MultiMedBench [12]의 HealthSearchQA, LiveQA 및 약 QA의 64개 질문에 대한 전문가 조작된 Long-form 응답으로 구성되었습니다.

그림 2: **무작위 연구 설계의 개요**. 1차 진료 의사(PCP)와 AMIE는 온라인 멀티턴 동기 문자 채팅을 통해 시뮬레이션 환자와 가상 원격 객체 구조 임상 검사(OSCE)를 수행하고 사후 질문에 대한 답변을 생성한다. 그런 다음 PCP와 AMIE는 환자 행위자와 전문의 의사 모두에 의해 평가된다.**

**의료 요약.** 중환자실 환자의 의료 기록을 포함하는 대규모 공개적으로 사용 가능한 데이터베이스인 MIMIC-III의 65개의 임상 의사가 작성한 의료 노트의 요약으로 구성된 데이터 세트[22]가 AMIE에 대한 추가 훈련 데이터로 사용되었다. IMIC-III에는 심장학, 호흡기, 방사선학, 의사, 일반, 퇴원, 사례 관리, 상담, 간호, 약국, 영양, 재활 및 사회 사업을 포함한 13가지 유형에 걸쳐 약 200만 개의 메모가 포함되어 있다. 각 범주에서 5개의 노트가 선택되었으며 최소 총 길이는 400 토큰이고 환자당 최소 1개의 간호 노트가 선택되었다. 임상의는 개별 의료 노트의 추상적 요약을 작성하도록 지시받았고 주요 정보를 캡처하는 동시에 원본 노트에 존재하지 않는 새로운 정보 및 문장 포함을 허용했다.

**실제 대화** 여기서 미국에서 10년 동안 1,000명 이상의 임상의가 직접 임상 방문을 하는 동안 의료 대화의 오디오 녹취록 98,919개로 구성된 대화 연구 조직에서 허가된 비식별화된 데이터 세트를 사용했다[23]. 51개의 의료 전문 분야(1차 진료, 류마티스, 혈액학, 종양학, 내과 및 정신과 등)와 168개의 의료 조건과 방문 이유(제2형 당뇨병, 류마티스 관절염, 천식, 우울증 등)를 다뤘다. 오디오 녹취록에는 의사, 환자 및 간호사와 같은 다양한 화자 역할의 발화가 포함되어 있다. (P_{0.25}=75.0\), (P_{0.75}=196.0\). 각 대화에 대해 메타데이터는 환자 인구 통계, 방문 이유(기존 상태에 대한 후속 조치, 급성 요구, 연간 검사 등), 진단 유형(신규, 기존 또는 기타 관련 없음)에 대한 정보를 포함했다. 자세한 내용은 [23]을 참조하십시오.

이 연구를 위해 의사와 환자만 참여하는 대화를 선택했지만 간호사와 같은 다른 역할은 선택하지 않았다. 전처리 과정에서 전사체에서 "[LAUGHING]" 및 "[INAUDIBLE]"와 같은 파라버벌 주석을 제거했다. 그런 다음 상태 범주 및 방문 이유에 따라 계층화된 샘플링을 사용하여 데이터 세트를 훈련(90%) 및 검증(10%) 세트로 나누어 훈련을 위한 대화 89,027개, 검증을 위한 대화 9,892개를 생성했다.

### AMIE를 위한 모의 대화 학습 환경 및 자기 재생

직접 임상 방문에서 실제 대화 내용을 수동적으로 수집하고 전사하는 것은 실현 가능하지만, 의료 대화를 위한 LLM을 훈련하는 데 있어 두 가지 실질적인 과제는 효과를 제한한다: (1) 기존 실제 데이터는 종종 광범위한 의료 조건과 시나리오를 캡처하지 못하여 확장성과 포괄성을 방해하고; (2) 실제 대화 녹취록에서 파생된 데이터는 모호한 언어(속어, 전문 용어 및 빈정거림 포함), 중단, 비문법적 발화 및 암묵적 참조를 포함하는 시끄러운 경향이 있다. 이는 결과적으로 AMIE의 지식, 능력 및 적용 가능성을 제한할 수 있다.

이러한 한계를 해결하기 위해 가상 진료 환경에서 진단 의료 대화를 위한 셀프 플레이 기반 시뮬레이션 학습 환경을 설계하여 다양한 의료 조건과 컨텍스트에 걸쳐 AMIE의 지식과 기능을 확장할 수 있도록 했다. 우리는 이 환경을 사용하여 위에서 설명한 의료 QA, 추론, 요약 및 실제 대화 데이터의 정적 코퍼스 외에 진화하는 시뮬레이션 대화 세트로 AMIE를 반복적으로 미세 조정했다(그림 1 참조).

이 과정은 두 개의 자기 재생 고리로 구성되었다:

* AMIE가 컨텍스트 비평가 피드백을 활용하여 AI 환자 에이전트와의 시뮬레이션된 대화에서 동작을 개선하는 **내부" 자체 재생 루프** 입니다.
* 정제된 시뮬레이션 대화 상자 세트가 후속 미세 조정 반복에 통합된 **외부" 자체 재생 루프** 입니다. 그런 다음 AMIE의 새로운 버전은 내부 루프에 다시 참여하여 지속적인 학습 주기를 만들 수 있다.

**시뮬레이션된 대화 상자** 미세 조정을 반복할 때마다 5,230개의 다른 의료 조건에서 비롯된 11,686개의 대화 상자를 생성했다. 조건은 3개의 데이터 세트에서 선택되었습니다.

* 613개의 일반적인 의료 조건을 포함하는 **건강 QA 데이터 세트**[12].
* 18,455개의 덜 일반적인 질병 상태를 포함하는 **MalaCards Human Disease Database1** 입니다. 각주 1: [https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json](https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json)
* 4,617개의 덜 일반적인 조건을 포함하는 **MedicineNet 질병 및 조건 인덱스2** 입니다.

각주 2: [https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json](https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json)

각 셀프 플레이 반복에서 613개의 공통 조건 각각에서 4개의 대화가 생성된 반면, 메디신넷 및 말라카드에서 무작위로 선택된 4,617개의 덜 공통 조건 각각에서 2개의 대화가 생성되었다. 평균 모의 대화 길이는 21.28회전(\(P_{0.25}=19.0\), \(P_{0.75}=25.0\))이었다.

시뮬레이션된 대화를 사용하여 고품질의 라벨이 붙은 실제 대화 데이터의 제한된 가용성을 해결하고 다양한 의료 컨텍스트에 대한 모델의 일반화 및 적응성을 개선할 수 있었다. 이러한 셀프 플레이 패러다임을 활용하여 AMIE는 환자 상호 작용 동안 대화 및 진단 기능을 지속적으로 배우고 개선할 수 있다.

#### 2.2.1 Simulated Dialogue Data Curation

대규모의 고품질 시뮬레이션 대화를 생성하기 위해 우리는 세 가지 핵심 구성 요소로 구성된 새로운 다중 에이전트 프레임워크를 개발했다.

* **비녯 생성기**: AMIE는 웹 검색을 활용하여 특정 의료 조건이 지정된 고유한 환자 비녯을 만듭니다.
* **시뮬레이션된 대화 생성기**: 3개의 LLM 에이전트가 환자 에이전트, 의사 에이전트 및 중재자의 역할을 수행하여 현실적인 진단 상호 작용을 시뮬레이션하는 턴 바이 턴 대화에 참여합니다.
* **셀프 플레이 비평가**: 네 번째 LLM 에이전트는 자기 개선을 위해 의사 에이전트에게 피드백을 제공하는 비평가 역할을 합니다. 특히, AMIE는 이 프레임워크에서 모든 에이전트 역할을 했다. 아래에서는 각 구성 요소에 대해 자세히 설명합니다.

**비녯 생성기** 비녯 생성기는 대규모로 다양하고 현실적인 환자 시나리오를 만드는 것을 목표로 했으며, 이는 이후에 시뮬레이션된 의사-환자 대화 상자를 생성하기 위한 컨텍스트로 사용되어 AMIE가 더 많은 수의 조건 및 환자 배경에 대한 노출을 모방하는 훈련 프로세스를 겪을 수 있다. 환자 비네트(시나리오)에는 환자 인구 통계, 증상, 과거 병력, 과거 수술 이력, 과거 사회 이력 및 환자 질문과 같은 필수 배경 정보와 관련 진단 및 관리 계획이 포함되었다.

주어진 조건에 대해 다음 과정을 사용하여 환자 비네트를 구성했다. 먼저 인터넷 검색엔진을 이용하여 인구통계, 증상, 상태와 관련된 관리계획의 범위에 대한 60개의 구절(각각 20개)을 검색하였다. 이러한 계대가 주어진 조건과 관련이 있는지 확인하기 위해 범용 LLM, PaLM-2 [10]을 사용하여 이러한 검색된 계대를 필터링하여 주어진 조건과 관련이 없는 것으로 간주되는 계대를 제거했다. 그런 다음 AMIE가 특정 비네트 형식을 강제하기 위한 원샷 예제를 제공하여 필터링된 패시지에서 검색된 인구통계, 증상 및 관리 계획과 정렬된 그럴듯한 환자 비네트를 생성하도록 유도했다. 이러한 각 단계에 대한 프롬프트는 다음과 같습니다.

**Search Retrieval Template**

조건[조건]에 대한 구체적인 환자 인구 통계/증상/관리 계획은 무엇인가?

**Passage Filtering Template**

임상 상태의 경우 [조건]은 일반적인 인구 통계/증상/관리 계획(예/아니오)에 대한 좋은 설명입니까?

설명: [검색된 경로]

Answer (Yes/No):

**Vignette Generation Template**

다음은 주어진 상태에 대한 인구 통계, 증상 및 관리 계획에 대한 몇 가지 구절이다. 이러한 구절과 일치하는 2개의 서로 다른 환자 비네트를 생성한다. 지정된 예제의 형식을 따르십시오. 특정 필드를 사용할 수 없는 경우 N/A만 나열하십시오.

Condition: [Condition]

인구통계학적 경로: [검색된 인구통계학적 경로]

증상 경로: [검색된 증상 경로]

관리 계획 경로: [검색된 관리 계획 경로]

예제 형식: [Oneshot 예]

Patient Vignettes for [Condition]:

**시뮬레이션된 대화 생성기** 특정 의학적 상태를 자세히 설명하는 환자 비녯을 감안할 때 시뮬레이션된 대화 생성기는 대면 신체 검사가 불가능할 수 있는 온라인 채팅 환경에서 환자와 의사 간의 현실적인 대화를 시뮬레이션하도록 설계되었습니다.

AMIE가 각각 수행하는 3개의 특정 LLM 에이전트(환자 에이전트, 의사 에이전트 및 중재자)는 시뮬레이션된 대화를 생성하기 위해 서로 통신하는 작업을 수행했다. 각 에이전트는 서로 다른 지침을 가지고 있었습니다. 환자 에이전트는 비녯에 요약된 의학적 상태를 경험하는 개인을 구체화했다. 그들의 역할은 의사 요원의 질문에 정직하게 응답하는 것과 그들이 가질 수 있는 추가 질문이나 우려를 제기하는 것과 관련이 있다. 의사 에이전트는 온라인 채팅 환경에서 환자의 병력을 파악하고자 하는 공감적 임상의 역할을 하였다[24]. 그들의 목표는 환자의 증상과 배경을 효과적으로 드러낼 수 있는 질문을 공식화하여 정확한 진단과 효과적인 치료 계획으로 이어지는 것이었다. 중재자는 환자 에이전트와 의사 에이전트 사이의 진행 중인 대화를 지속적으로 평가하여 대화가 언제 자연스러운 결론에 도달했는지 결정했다.

턴바이턴 대화 시뮬레이션은 의사 에이전트가 대화를 시작하는 것으로 시작되었습니다. "의사: 그래서, 오늘 무엇을 도와드릴까요?" 그 후, 환자 에이전트가 응답했고, 그들의 대답은 진행 중인 대화 이력에 통합되었다. 이어서, 의사 에이전트는 업데이트된 대화 이력에 기초하여 응답을 공식화하였다. 그런 다음 이 응답이 대화 기록에 추가되었습니다. 사회자가 대화가 자연스러운 결론에 도달했음을 감지할 때까지 대화는 의사 에이전트가 차등 진단, 치료 계획을 제공하고 나머지 환자 에이전트 질문을 적절하게 해결하거나 에이전트 중 하나가 이별을 시작할 때까지 진행되었다.

**Patient Agent Instruction:**

당신은 온라인 채팅 인터페이스를 통해 의사와 채팅하는 환자입니다. 그 의사는 전에 당신을 만난 적이 없습니다. <환자 비녯> 인터뷰할 때 의사의 질문에 정직하게 대답하여 나올 수 있는 질문을 한다.

**Doctor Agent Instruction:**

당신은 온라인 채팅 인터페이스를 통해 환자에게 병력에 대해 묻는 공감적인 임상의입니다. 당신은 그 환자에 대해 미리 아무것도 모른다. 환자의 병력과 증상을 더 잘 이해하기 위해 단일 회전 응답으로 환자에게 응답합니다. 두 가지 이상의 질문을 하지 마세요. 환자가 질문을 하면 반드시 적절하게 대답해야 한다.

**Moderator Instruction:**

다음은 의사와 환자 간의 대화입니다. <대화> 의사가 환자에게 진단 및 치료 계획을 제공하고 환자가 질문이 남지 않은 경우에만 대화가 종료되어야 합니다. 의사나 환자가 작별 인사를 하면 대화도 끝난다. 질문, 대화가 끝났나요? 좋든 싫든

**셀프 플레이 비평.** 고품질 대화를 보장하기 위해 진단 대화의 자체 개선을 위해 맞춤형 셀프 플레이 [25] 프레임워크를 구현했습니다. 이 프레임워크는 AMIE가 연기하고 그라운드 진실 진단을 인식하는 "비평가" 역할을 하는 네 번째 LLM 에이전트를 도입하여 의사 에이전트에게 상황 내 피드백을 제공하고 후속 대화에서 성능을 향상시켰다. 평론가는 의사 대리인의 반응을 다음과 같은 기준으로 평가하였다.

* 의사 에이전트는 환자 에이전트의 최신 질문 또는 의견을 간결하게 해결하면서 공감 및 전문성을 발휘합니다.
* 의사 에이전트는 응답당 최대 1~2개에 중점을 두고 너무 많거나 반복적인 질문(이미 획득한 정보에 대해)을 하는 것을 피합니다.
* 응답은 의사 에이전트가 AI 챗봇임을 나타내지 않아야 합니다. 자연스럽게 흐르고 사실적 정확성을 유지하며 환자의 추가 참여를 촉진해야 한다.
* 의사 에이전트는 가장 가능성이 높은 감별 진단 중 적어도 두 가지를 식별하기에 충분한 질문을 한다. 그들은 그라운드 트루스 진단에 대한 표적 질문을 통해 이해를 더욱 구체화하고 그에 상응하는 치료를 제공한다.

비평가의 피드백에 따라 의사 에이전트는 처음부터 동일한 환자 에이전트와의 후속 대화 라운드에서 응답을 개선하기 위해 제안을 통합했다. 특히, 의사 요원은 매번 새로운 라운드에서 이전 대화 기록에 대한 액세스를 유지했다. 이러한 자기계발 과정을 두 번 반복하여 미세조정의 각 반복에 사용되는 대화문을 생성하였다.

### Instruction Fine-tuning

기본 LLM PaLM 2[10]를 기반으로 한 AMI는 의학적 대화와 추론을 위한 능력을 향상시키기 위해 미세 조정되었다. 기본 LLM 아키텍처에 대한 자세한 내용은 PaLM-2 기술 보고서를 참조한다.

우리는 의료 대화 내에서 환자 또는 의사 역할을 수행하고 의료 질문 응답 및 추론을 수행하고 EHR 노트를 요약할 때 AMIE를 미세 조정하기 위해 작업별 지침을 사용했다. 기본 LLM으로부터의 미세조정의 첫 번째 라운드는 정적 데이터 세트만을 사용했지만, 미세조정의 후속 라운드는 섹션 2.2.1에 설명된 대로 자체 재생 내부 루프를 통해 생성된 시뮬레이션된 대화를 활용했다.

대화 생성 작업의 경우 AMIE는 의사 또는 환자 역할을 가정하여 이전의 모든 상호 작용을 기반으로 다음 대화 순서를 예측하도록 훈련되었다. 환자 에이전트를 재생할 때 AMIE는 환자 시나리오에서 제공된 정보에 따라 증상에 대한 의사 에이전트의 질문에 답하라는 메시지가 표시되었다. 이러한 시나리오에는 시뮬레이션된 대화 상자에 대한 환자 비네트(섹션 2.2.1 참조) 또는 실제 대화 데이터 세트에 대한 인구 통계, 방문 이유 및 진단 유형과 같은 메타데이터가 포함되었다. 의사 에이전트 역할에서 AMIE는 감정이입 임상의 역할을 하도록 자극되어 환자의 병력과 증상에 대해 환자를 인터뷰하여 궁극적으로 정확한 진단에 도달했다. 각 대화에서 우리는 목표 회전까지의 대화를 기반으로 예측하기 위해 목표 회전으로 의사 및 환자 역할 각각에 대해 평균 3턴을 샘플링했다. 대상 회전은 최소 30자 길이의 대화에서 모든 회전에서 무작위로 샘플링되었다.

유사하게, EHR 노트 요약 작업의 경우 AMIE에 임상 노트가 제공되었고 노트의 요약을 생성하도록 프롬프트되었다. 의료 추론/QA 및 장기 형태 응답 생성 작업은 [13]과 동일한 설정을 따랐다. 특히, 대화 생성 및 롱폼 응답 생성을 제외한 모든 태스크는 추가 컨텍스트에 대한 태스크-특정 명령 외에 소수의 샷(1-5) 예제를 통합했다.

### 온라인 추론을 위한 연쇄 추론

진단 대화의 핵심 과제를 해결하기 위해 - 환자와 긍정적인 관계를 유지하면서 진단 정확성과 자신감을 향상시키기 위해 불확실성 하에서 정보를 효과적으로 획득함 - AMIE는 각 대화 전환에서 응답을 생성하기 전에 연쇄 추론 전략을 사용했다. 여기서, "추론 체인"은 일련의 순차적 모델 호출들을 지칭하며, 각각은 이전 단계들의 출력들에 의존한다. 구체적으로, 다음과 같이 설명된 3단계 추론 프로세스를 사용했다:

1. **환자 정보 분석:** 현재 대화 이력을 감안할 때 AMIE는 1) 환자의 긍정적인 증상 및 부정적인 증상뿐만 아니라 관련 의료/가족/사회 이력 및 인구 통계 정보를 요약하고, 2) 현재 감별 진단을 생성하고, 3) 보다 정확한 진단을 위해 필요한 누락 정보를 기록하고, 4) 현재 감별에 대한 신뢰도를 평가하고 긴급성을 강조하도록 지시되었다.
2. **응답 및 액션 공식화:** 1단계의 대화 내역 및 출력을 기반으로 AMIE는 다음을 수행했습니다. 1) 환자의 마지막 메시지에 대한 응답을 생성하고 추가 질문을 공식화하여 누락된 정보를 획득하고 차등 진단을 구체화합니다. 2) 필요한 경우 응급실 방문 등 즉각적인 조치를 권고한다. 사용 가능한 정보를 기반으로 진단을 확신하는 경우 감별을 제시합니다.
3. **응답 정제:** AMIE는 대화 기록 및 이전 단계의 출력에 따라 특정 기준을 충족하도록 이전 출력을 수정합니다. 기준은 주로 사실성 및 응답의 포맷팅(예를 들어, 환자 사실 및 불필요한 반복에 대한 사실적 부정확성을 피하고, 공감을 보여주고, 명확한 포맷으로 표시)과 관련된다.

이 추론 연쇄 전략을 통해 AMIE는 현재 대화에 대해 조건화된 응답을 점진적으로 정제하여 정보에 입각하고 근거 있는 응답에 도달할 수 있었다.

## 3 Evaluation

임상 대화를 위한 모델을 개발하는 선행 연구들은 노트 대 대화 또는 대화 대 노트 세대의 정확도와 같은 메트릭에 초점을 맞추었다[26, 27], 또는 상담의 임상적 품질을 포착하지 못하는 BLEU 또는 ROUGE 점수와 같은 자연어 생성 메트릭에 초점을 맞추었다[28, 29].

이러한 이전 작업과 대조적으로 우리는 상담에서 의사 소통 기술을 포함하여 역사 촬영에 대한 의사의 전문 지식의 품질을 평가하는 데 더 일반적으로 사용되는 기준에 인간 평가를 고정하려고 했다. 본 연구는 영국 왕립의과대학(Royal Medical College of Physicians)의 PACES( Practical Assessment of Clinical Examination Skills)3[30]의 일환으로 환자중심의사소통(PCCBP)의 우수사례에 대한 컨센서스(Consensus) 리뷰에서 발표된 원칙들, 그리고 전문가 재검증의 일환으로 환자 피드백을 원하는 의사들을 위한 영국 일반의학회의 환자설문지(GMCPQ)4에서 제안된 기준들로부터 프레임워크를 도출하였다. 이러한 기준들을 반복해서 포함하기 위한 항목들을 정제하고, 포커스 그룹과 영국, 캐나다, 미국, 인도에 기반을 둔 임상의 및 OSCE 검사관들과의 인터뷰를 통해 평가 파일럿 척도와 지침을 도출했다. 우리의 결과 파일럿 프레임워크는 임상의(보드 인증 의사)와 일반 평가자(환자 행위자)의 두 가지 관점에서 평가를 가능하게 했다. 프레임워크에는 상담 품질, 구조 및 완성도, 면접관의 역할, 책임 및 기술(표 A.1, A.2, A.3 및 A.4)에 대한 고려가 포함되었다.

### Objective Structured Clinical Examination

Objective Structured Clinical Examination (OSCE)는 표준화되고 객관적인 방식으로 임상 기술 및 역량을 평가하기 위해 의료에서 사용되는 실용적인 평가 형식이다[31, 32, 33]. 주로 이론적 지식에 초점을 맞추고 대신 실제 임상 실습의 기술을 평가할 수 있는 환경을 제공하는 것을 목표로 하는 전통적인 필기 또는 구술 시험과 다르다.

OSCE는 일반적으로 여러 스테이션(종종 8-12)으로 나뉘며, 각각은 미리 정의된 시나리오 설명에 기초하여 특정 증상 또는 상태를 묘사하도록 훈련된 표준화된 환자 행위자에 의해 제정된 실제 임상 시나리오를 시뮬레이션한다. 각 역에서 학생들은 임상 병력 청취나 진단 등 수행해야 할 구체적인 과제를 부여받는다. 각 스테이션에는 정해진 시간 제한이 있어 공정성과 효율적인 평가가 보장됩니다. 훈련된 검사자들은 미리 정의된 체크리스트 또는 마킹 스킴을 사용하여 각 스테이션에서 학생들의 수행을 관찰한다. 의사소통, 이력 작성, 신체 검사 기술, 임상 추론 및 의사 결정과 같은 임상 기술을 평가한다.

### Remote OSCE Study Design

AMIE의 성능을 실제 임상의의 성능과 비교하기 위해 원격 OSCE 스타일의 블라인드 상담에 대한 무작위 교차 연구를 수행했다. 우리의 OSCE 연구는 온라인 텍스트 기반 상담에 참여하기 위해 인도와 캐나다에서 각각 10명씩 20명의 이사회 인증 1차 진료 의사(PCP)와 20명의 검증된 환자 행위자를 포함했다. PCP는 거주 후 3년에서 25년(중앙값 7년)의 경험을 가지고 있었다. 환자 행위자는 OSCE 참여 경험이 있는 의대생, 레지던트 및 간호사 실무자의 혼합으로 구성되었다. 우리는 인도(75), 캐나다(60), 영국(14)에서 149개의 시나리오 팩을 공급했다.

우리 연구의 시나리오 팩과 시뮬레이션 환자는 각각 의대에 소속되어 OSCE 검사를 위한 시나리오 팩과 시뮬레이션 환자를 준비한 경험이 풍부한 두 개의 OSCE 실험실(캐나다와 인도에 각각 하나씩)에서 준비했다. 영국 시나리오 팩은 MRCPUK 웹사이트에 제공된 샘플에서 조달되었다. 각 시나리오 팩은 그라운드 트루스 진단 및 허용 가능한 진단 세트와 연관되었다. 시나리오 팩은 심혈관(29), 호흡기(30), 위장학(31), 신경학(30), 비뇨기과, 산부인과 영역(15) 및 내과(14)의 조건을 포함한다. 집중 치료 또는 입원 환자 사례 관리 시나리오와 마찬가지로 소아 또는 정신 의학 영역은 본 연구에서 제외되었다.

인도 환자 배우들은 모든 인도 시나리오 팩과 14개의 영국 시나리오 팩 중 7개에서 역할을 맡았다. 캐나다 환자 행위자는 캐나다와 영국 기반 시나리오 팩의 나머지 절반 모두에 대한 시나리오 팩에 참여했다. 이 할당 프로세스는 149개의 별개의 시뮬레이션 환자("시나리오")를 초래했다. 아래에서는 "OSCE 에이전트"라는 용어를 사용하여 환자 행위자를 인터뷰하는 대화 상대, 즉 PCP 또는 AMIE를 나타낸다. 표 1은 세 가지 지리적 위치에 대한 OSCE 할당 정보를 요약한다. 149명의 시뮬레이션 환자 각각은 그림 2에 표시된 3단계 연구 흐름을 완료했다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Location & \# of Scenario Packs & \# of Simulated Patients & \# of Patient Actors & \# of PCPs \\ \hline Canada & 60 & 67 & 10 & 10 \\ India & 75 & 82 & 10 & 10 \\ UK & 14 & 0 & 0 & 0 \\
**Total** & **149** & **149** & **20** & **20** \\ \hline \hline \end{tabular}
\end{table}
표 1: **OSCE 연구 요약** 원격 OSCE 연구의 세 위치(캐나다, 인도 및 영국) 각각에 있는 시나리오 팩, 환자 행위자, 시뮬레이션 환자 및 1차 진료 의사(PCP)의 수입니다. 20명의 보드 인증 PCP가 인도와 캐나다에서 각각 10명씩 AMIE와 비교하여 OSCE 에이전트로 연구에 참여했다. 20명의 훈련된 환자 배우가 참여했으며 각각 10명이 인도와 캐나다에서 왔다. 인도의 환자 배우들은 인도와 영국의 시나리오 팩 모두에서 역할을 했다. 캐나다의 환자 배우들은 캐나다와 영국 모두를 위한 시나리오 팩에 참여했다. 이 과정에서 149명의 뚜렷한 모의 환자가 발생했다.

#### 3.2.1 온라인 텍스트 기반 상담

PCP와 환자 행위자는 샘플 시나리오와 지침으로 준비되었으며 인터페이스 및 실험 요구 사항을 숙지하기 위해 연구가 시작되기 전에 파일럿 상담에 참여했다.

실험을 위해 각 시뮬레이션 환자는 동기식 문자 채팅 인터페이스(그림 A.2), 하나는 PCP(대조군) 및 하나는 AMIE(개입)를 통해 두 개의 온라인 문자 기반 상담을 완료했다. PCP와 AMIE의 순서는 무작위 배정되었고 환자 행위자는 각 상담에서 어떤 사람과 이야기하고 있는지 알 수 없었다. PCP는 환자 행위자와 동일한 국가에 위치했으며 상담을 위해 지정된 시간대에서 가용성을 기반으로 무작위로 추출되었다. 환자 배우들은 시나리오를 롤 플레이하고 20분 이내에 대화를 끝내도록 지시받았다. 두 OSCE 에이전트 모두(연구별 지침을 통한 PCP와 프롬프트 템플릿의 일부로 AMIE) 어떤 상황에서도 자신의 신원이나 인간 여부를 밝히지 않도록 요청했다.

#### 3.2.2 Post-questionnaires

상담이 끝나면, 환자 행위자와 OSCE 에이전트는 각각 결과 상담 녹취록에 비추어 사후 질문지를 작성했다(그림 A.3). 환자 행위자에 대한 사후 설문은 완전한 GMCPQ(표 A.1), "환자 우려 관리" 및 "환자 복지 유지"를 위한 PACES 구성 요소(표 A.2), "관계 육성"을 위한 PCCBP 범주의 체크리스트 표현(표 A.3)으로 구성되었다. 사후 질문지에 제공된 환자 행위자에 대한 응답을 아래의 "환자 행위자 등급"이라고 한다. OSCE 에이전트에 대한 사후 설문은 최소 3개 이상 10개 이하의 조건을 가진 순위 차등 진단(DDx) 목록과 대면 또는 영상 기반 상담, 조사, 치료, 관리 계획 및 후속 조치 필요성에 대한 권장 사항을 요청했다.

#### 3.2.3 전문의 의사 평가

마지막으로, 인도(14), 북미(6), 영국(3)의 23명의 전문 의사 풀이 상담의 질과 사후 질문에 대한 응답과 관련하여 PCP와 AMIE를 평가했다. 평가하는 동안 전문의는 또한 관련 그라운드 트루스 감별 및 추가로 승인된 감별과 함께 전체 시나리오 팩에 접근할 수 있었다. 전문의가 평가 중에 액세스할 수 있는 모든 데이터를 아래에서 총칭하여 "OSCE 데이터"라고 한다. 전문 의사는 연구에 포함된 시나리오 팩에 해당하는 전문 분야와 지리적 지역에 맞게 공급되었으며 거주 후 1년에서 36년(중앙값 5년)의 경험을 가지고 있었다. OSCE 데이터의 각 세트는 기본 시나리오의 전문 및 지리적 영역과 일치하도록 무작위로 할당된 한 전문 의사에 의해 평가되었다(예를 들어, 캐나다 호흡기 전문의는 캐나다 유래 호흡기 의학 시나리오의 OSCE 데이터를 평가했다). 각 전문가는 주어진 시나리오에 대해 PCP와 AMIE 모두의 OSCE 데이터를 평가했다. PCP 및 AMIE에 대한 평가는 무작위 및 블라인드 시퀀스에서 동일한 전문가에 의해 수행되었다. 평가 기준에는 제공된 DDx 목록의 정확성, 적절성 및 포괄성, 확대, 조사, 치료, 관리 계획 및 후속 조치에 관한 권장 사항의 적절성(표 A.4), 모든 PACES(표 A.2) 및 PCCBP(표 A.3) 평가 항목이 포함되었다. 또한 전문 의사에게 상담 및 설문 응답, 즉 비사실적이거나 대화에서 제공되지 않는 정보에 참조된 텍스트 구절에서 혼란을 강조하도록 요청했다. 각 OSCE 시나리오 팩은 또한 전문가에게 시나리오별 임상 정보를 제공하여 이상적인 조사 또는 관리 계획과 같은 상담의 임상 품질을 평가하는 데 도움이 되거나 가능한 최고의 상담 품질을 위해 이상적으로 설명되었을 임상 이력의 중요한 측면을 제공했다.

### Auto-evaluation

인간 평가 외에도 전문가 평가에 대한 경제적인 일관된 대안으로 모델 기반 자동 평가 방법을 구현했다. 이러한 기술은 OSCE 에이전트의 대화 품질과 진단 정확도를 모두 평가하는 데 사용되었다. 대화 품질을 평가하기 위한 자동 평가 방법의 타당성을 확립하기 위해 처음에는 환자 행위자와 전문의 의사 모두가 평가한 PACES 루브릭(표 A.2)의 4가지 평가 축 하위 집합에 초점을 맞췄다. 대화를 평가하기 위해 AMIE와 함께 자체 CoT 전략(섹션 A.9에 설명된 세부 사항)을 사용하는 자동 평가는 인간 평가자와 잘 일치했으며 이러한 기준에 대한 전문가 간 합의에 필적했다. 차등 진단의 자동 평가를 위해 우리는 또 다른 LLM, Med-PaLM 2 [13]을 전문가 평가자의 대리인으로 활용하여 예측된 진단을 그라운드 트루스 진단에 대해 등급을 매겼다(섹션 A.7의 더 자세한 내용). DDx 정확도에 대한 우리의 자동 평가는 전문가 등급과 비교하여 AMIE 및 OSCE 에이전트에 대해 유사한 경향을 보여주었다. 전반적으로 자동 평가 경향은 대화 품질과 진단 정확도 모두에 대해 인간 등급과 일치했다.

또한 다음 목적을 위해 추가 자동 평가 분석을 수행했다.

* AMIE 또는 PCP 상담에서 도출된 DDx 정확도의 성능을 비교하기 위해;
* 캐나다와 인도에서 수행된 시뮬레이션 환자 간의 DDx 정확도를 비교하고 두 위치 간에 체계적인 차이가 있는지 확인하는 단계;
* 자체 대신 PCP 상담 제공 시 AMIE의 DDx 정확도를 분석하여 정보 획득 및 정보 해석의 효과를 분리하는 단계;
* 대화 회전 횟수가 증가할수록 DDx 정확도를 분석하여 AMIE와 PCP 간의 정보 획득의 효율성을 평가하는 단계;
* 평론가 피드백 전후 대화 품질에 대한 내부 루프 셀프 플레이의 이점을 평가합니다.

### Statistical Analysis

우리는 모든 149명의 시뮬레이션된 환자에 걸쳐 AMIE 및 PCP에 의해 생성된 DDx 목록의 top-k 정확도를 평가했다. Top-k 정확도는 DDx 목록의 top-k 위치 내에서 정확한 진단이 나타난 경우의 백분율로 정의되었다. 구체적으로, 전문가 평가자가 그라운드 트루스 진단과 매우 가깝거나 밀접하게 관련(또는 인정된 차등)된 정확한 일치로 표시한 경우 후보 진단을 일치로 간주했다. DDx 정확도에 대한 통계적 유의성은 10,000개의 샘플을 사용한 부트스트랩 테스트[34]와 모든 k에 걸쳐 FDR(잘못된 발견 비율) 수정[35]을 사용하여 결정되었다. 환자 행위자 및 전문가 등급에 대한 통계적 유의성은 윌콕슨 부호 순위 테스트[36] FDR 보정을 사용하여 결정되었다. 에이전트가 "전송할 수 없음/적용되지 않음"을 받은 경우는 테스트에서 제외되었습니다. 아래 결과는 FDR 보정 후 \(p\)-값을 참조한다.

## 4 Results

### Diagnostic Accuracy

#### 4.1.1 AMIE는 전문의 의사 평가에서 PCP보다 DDx 정확도가 더 높았다.

AMIE의 진단 정확도는 PCP보다 높은 것으로 평가되었다. 그림 3은 AMIE 및 PCP에 대한 top-k 정확도를 나타내며, 그라운드 트루스 진단(a)과의 일치 및 허용된 차등(b) 상의 임의의 항목과의 일치를 고려한다. (p<0.05\). AMIE와 달리 PCP는 차등 진단에서 항상 10개의 진단을 제공하는 것은 아니다(최소: 3, 평균: 5.39). 또한 일치 여부를 결정하기 위한 일치 기준을 변경하여 AMIE와 PCP 간의 DDx 정확도를 비교했다. 그림 A.7에 표시된 결과는 다양한 매칭 기준에 걸쳐 AMIE의 우수한 DDx 성능을 추가로 입증한다.

**전문성에 의한 정확도.** 그림 A.8은 연구에서 시나리오가 다루는 6개의 의료 전문 분야에서 AMIE 및 PCP에 의해 달성된 DDx 정확도를 보여준다. 우리는 AMIE의 성능이 호흡기 및 심혈관 전문 분야에서 가장 두드러진 개선과 함께 모든 전문 분야에 대한 PCP 성능과 일치하거나 능가하는 것을 관찰했다.

#### 4.1.2 자동 평가는 AMIE가 정보 획득에 있어 PCP의 효율성과 일치함을 시사했다.

**자동 평가 정확도.** 그림 3과 동일한 절차를 사용하여 전문가 평가자 대신 모델 기반 자동 평가기로 DDx 정확도 분석을 재현했습니다. 자동 평가기를 통해 얻은 전체 성능 경향은 그림 A.9와 같이 계산된 정확도 값의 한계 차이에도 불구하고 전문가 평가와 잘 일치합니다.

**성능 이득의 출처 분리** 그림 3에서 관찰된 AMIE의 우수한 DDx 성능이 향상된 정보 획득에서 비롯되었는지 또는 더 나은 진단 추론 능력에서 비롯되었는지 조사하기 위해 DDx 자동 평가기를 사용하여 해당 PCP 상담에서 생성된 AMIE의 진단과 자체 상담을 기반으로 AMIE의 진단을 비교했다. 그림 A.10에 표시된 결과는 현저하게 유사한 DDx 성능을 보여주었으며, 이는 AMIE가 자체 대화 또는 PCP의 대화에서 정보를 처리했는지 여부에 관계없이 진단 성능이 일관되게 유지되었음을 나타낸다. 두 방법 모두 PCP에 의해 생성된 차등 진단을 크게 능가했다. 이러한 결과는 AMIE가 정보 획득 시 PCP와 거의 동일하지만 정확한/완전한 감별 진단을 생성하기 위해 해당 정보를 해석하는 데 PCP보다 우수함을 시사한다.

**정보 획득의 효율성** AMIE가 상담 중 응답에서 생성된 총 단어 수 측면에서 PCP에 비해 더 큰 장황함을 나타냈지만, 그림 A.11과 같이 대화 전환 수와 환자 행위자로부터 도출된 단어 수는 두 OSCE 에이전트 모두에서 유사했다. 이는 AMIE와 PCP 모두 만남 동안 환자로부터 유사한 양의 정보를 획득했음을 시사한다. AMIE 또는 PCP가 정확한 진단을 공식화하기 위해 충분한 정보를 수집하는 데 얼마나 효율적인지 조사하기 위해 다양한 턴 카운트에서 대화를 절단하고 AMIE를 사용하여 이러한 부분 대화를 기반으로 차등 진단을 생성했다. 그림 A.12는 모델에 제공된 회전 수의 함수로 Top-3 DDx 정확도를 나타낸다. 관찰된 정확도는 AMIE와 PCP 모두에 대해 초기 10회 대화 회전 내에서 안정되었다. 이는 AMIE와 PCP 모두 대화의 초기 단계에서 진단을 공식화하는 데 필요한 정보를 얻을 수 있었음을 시사한다. 또한 매회 유사한 성능은 AMIE나 PCP 모두 정보 획득의 효율성이나 품질에 큰 이점이 없음을 나타낸다.

### Conversation Quality

#### 4.2.1 AMIE는 전문가 및 환자 행위자당 대화 품질에서 PCP를 능가했습니다.

대화 품질은 환자 배우 평가, 전문가 평가 및 자동 평가의 결과를 사용하여 평가되었다. 그림 A.5 및 A.6은 각각 AMIE 및 PCP의 동일한 시뮬레이션 환자에 대한 두 가지 예제 상담을 보여준다.

그림 3: **전문 등급 top-k 진단 정확도.** AMIE 및 PCPs top-k DDx 정확도는 지상 진실 진단(**a**) 및 허용된 차등(**b**)의 모든 진단과 관련하여 149개 시나리오에 걸쳐 비교됩니다. 부트스트래핑(n=10,000)은 FDR 보정 후 AMIE와 PCP DDx 정확도 사이의 모든 top-k 차이가 \(p<0.05\)로 유의함을 확인시켜준다.

그림 4: **환자 행위자 등급.** 상담 종료 시 환자 행위자가 평가한 대화 품질입니다. 예시를 위해 5점 평가 척도의 모든 응답은 '매우 호의적'에서 '매우 비호의적'에 이르는 일반적인 5점 척도로 매핑되었다. 예/아니오 질문의 경우, (긍정적) '예' 응답은 '좋음'과 동일한 색으로 매핑되었고 (부정적) '아니오' 응답은 '좋음'과 동일한 색으로 매핑되었다. 평가 척도는 General Medical Council Patient Questionnaire (GMCPQ), Practical Assessment of Clinical Examination Skills (PACES), 그리고 Patient-Centered Communication Best Practice (PCCBP)에 대한 서술적 검토에서 채택되었다. 질문 문구와 응답 옵션에 대한 세부 정보는 섹션 A.1에 나와 있다. 별표는 통계적 유의성을 나타낸다(\(*:p<0.05\), \(**:p<0.01\), \(***:p<0.001\), \(n.s.\) : 유의하지 않다).

**환자 행위자 등급.** 그림 4는 OSCE 에이전트와의 상담 후 평가된 다양한 대화 품질을 보여준다. 전반적으로 AMIE의 상담은 26개 축 중 24개 축에서 PCP의 상담보다 환자 행위자에 의해 유의하게 더 나은(\(p<0.05\)) 평가를 받았다. 두 PCCBP 축 "환자의 개인 정보 존중"(N=108)과 "실수를 인정"(N=41)에 대해 등급의 유의한 차이는 감지되지 않았다. 후자의 기준은 OSCE 에이전트가 실수를 하고 대화에서 지적한 경우에만 적용되는 질문이기 때문에 제외 횟수가 실질적으로 더 많았다.

**전문 의사 등급.** 전문 의사는 도메인 전문 지식 내의 시나리오에 대해 대화 품질과 사후 질문에 대한 응답을 모두 평가했다(그림 5 참조). 다시 한 번, AMIE의 응답은 32개의 평가 축 중 28개의 평가 축에서 전문가에 의해 PCP에 의한 응답보다 훨씬 더 나은 평가를 받았으며, 전문가는 AMIE의 상담, 진단 및 관리 계획을 PCP에 의한 응답보다 선호했다. 이 평가에서는 AMIE와 PCP 간의 전문가 등급 차이가 통계적으로 유의하였다 (\(p<0.05\)). 진단 및 관리 루브릭의 4개 축, 즉 "에스컬레이션 권고 적절", "치료 부적절 회피", "후속 권고 적절" 및 "혼동 부재"에 대해 배제가 없음에도 불구하고 등급의 유의한 차이가 감지되지 않았다(N=149).

**4.2.2 자동 평가는 AMIE에 대한 내부 셀프 플레이의 효과를 입증했습니다.**

**대화 등급 자동 평가** 모델 기반 자체 CoT 자동 평가 전략을 활용하여 PACES 루브릭의 4가지 평가 축에 대한 대화를 평가했으며 이러한 자동 평가 등급이 전문가 등급과 정확하고 잘 일치하는지 확인했습니다(그림 A.17 및 A.18). 또한, 내적 자기 재생 루프가 모의 대화 품질을 향상시켰음을 입증하기 위해 자기 재생 절차 전후에 생성된 모의 대화문에 자동 평가 방법을 적용했다. 그림 A.19의 결과는 셀프-플레이 후의 시뮬레이션된 대화들이 셀프-비판 없이 베이스라인 대화들보다 더 자주 선호된다는 것을 보여주었다.

## 5 관련 작업

### 임상 기록 작성 및 진단 대화

의사-환자 의사소통에 대한 합의는 의사-환자 의사소통 관행을 수용하기 위해 발전했으며, 의사-환자 의사소통은 관계 증진, 정보 수집, 정보 제공, 의사 결정, 감정 반응 및 질병 및 치료 관련 행동을 가능하게 하는 6가지 핵심 기능을 다루어야 한다고 권고했다[43, 44, 20]. 이러한 목표를 달성하기 위한 특정 기술 및 행동도 검증된 도구를 사용하여 설명, 교육 및 평가[45, 20]되었다. 의료 협약은 제시된 불만 사항, 과거 병력 및 약물 병력, 사회 및 가족 병력, 시스템 검토[46, 47]와 같은 주제를 포함하여 임상 면담 중에 특정 범주의 정보를 수집해야 한다고 일관되게 인용한다. 이러한 목표를 달성하기 위한 임상의의 능력은 객관적인 구조화된 임상 검사(OSCE)의 프레임워크를 사용하여 일반적으로 평가된다[31, 32, 33]. 이러한 평가는 재현성 또는 구현에서 다양하며 COVID-19 대유행 동안 특히 관련성 있는 문제인 원격 의료 시나리오와 함께 가상 OSCE(vOSCE)로 원격 실습에 맞게 조정되었다[48].

### 대화형 AI 및 목표 지향 대화

목표 지향적 대화와 과제 완성을 위한 대화형 AI 시스템은 풍부한 역사를 가지고 있다[49, 50, 51]. 변압기[52]와 대형 언어 모델[15]의 등장은 이러한 방향에 대한 새로운 관심을 이끌었다. 정렬을 위한 전략[53], 자기 개선[54, 55, 56, 57] 및 확장 가능한 감독 메커니즘[58]의 개발은 현실 세계에서 그러한 대화 시스템의 대규모 배치를 가능하게 했다[59, 16]. 그러나 이러한 AI 시스템의 대화 및 작업 완료 능력에 대한 엄격한 평가 및 탐색은 임상 응용을 위해 여전히 제한적이며, 연구는 주로 질문 응답 또는 요약과 같은 단일 회전 상호 작용 사용 사례에 초점을 맞추었다.

그림 5: **전문 의사 등급.** 전문 의사가 평가한 대화 및 추론 특성입니다. 예시를 위해 5점 평가 척도의 모든 응답은 '매우 호의적'에서 '매우 비호의적'에 이르는 일반적인 5점 척도로 매핑되었다. 유일한 4점 척도(DDx 포괄성)는 '유리하지도 않고 불리하지도 않은' 옵션을 무시하고 동일한 척도로 매핑되었다. 예/아니오 질문의 경우, (긍정적) '예' 응답은 '좋음'과 동일한 색으로 매핑되었고 (부정적) '아니오' 응답은 '좋음'과 동일한 색으로 매핑되었다. 평가 척도는 임상 검사 기술의 실용 평가(PACES), 환자 중심 의사소통 모범 사례(PCCBP) 및 기타 출처에 대한 서술적 검토에서 채택되었다. 질문 문구와 응답 옵션에 대한 세부 정보는 섹션 A.1에 나와 있다. 별표는 통계적 유의성을 나타낸다(\(*:p<0.05\), \(**:p<0.01\), \(***:p<0.001\), \(n.s.\) : 유의하지 않다).

### 의료 상담 및 진단 대화용 AI

의료 상담을 수행하기 위한 도구로서 AI에 대한 대부분의 탐색은 완전한 자연스러운 대화가 아닌 "증상 체커" 응용 프로그램이나 임상 노트 또는 요약이 주어진 의료 오디오의 전사 또는 그럴듯한 대화 생성과 같은 주제에 초점을 맞추었다[60, 61, 62, 63]. 언어 모델은 임상 대화 데이터 세트를 사용하여 훈련되었지만 종합적으로 평가되지는 않았다[64]. 연구는 상업적인 채팅 플랫폼(1:1 의료 상담과 비교하여 의사-환자 참여를 변경했을 수 있음)에서 의사와 환자 사이의 메시지에 근거했다[65, 28, 66]. 많은 사람들이 임상적으로 의미 있는 메트릭보다는 기록된 교환에서 다음 회전을 예측하는 데 주로 초점을 맞췄다. 그리고 현재까지 대화 및 의사 소통 기술에서 인간 의사를 검사하고 훈련하는 데 사용되는 동일한 기준을 사용하여 진단 대화를 위한 AI 모델의 품질을 조사한 연구 및 OSCE와 같은 공통 프레임워크에서 AI 시스템을 평가하는 연구는 보고되지 않았다.

### 진단 대화 평가

진단 대화에서 AI 시스템의 성능에 대한 인간 평가를 위한 이전 프레임워크는 자세히 제한되었다. 그들은 의사소통 기술과 역사 촬영의 질을 평가하기 위한 확립된 기준에 고정되지 않았다. 예를 들어, [29]는 전체 "인간 평가"를 기술하는 5점 척도를 보고하였고, [65]는 "관련성, 정보성 및 인간 유사성", [66]은 "유창성, 전문성 및 관련성", [67]은 "유창성 및 적절성" 및 [68]은 "유창성"을 보고하였다. 이러한 기준은 의료 전문가가 가르치고 실행하는 기준보다 훨씬 덜 포괄적이고 구체적이다. LLM의 대화 능력을 평가하기 위한 다중 에이전트 프레임워크는 [64]에 소개되어 있지만, 연구는 피부과의 제한된 환경에서 수행되었고, 시뮬레이션된 상호 작용의 의사와 환자 측면을 모방하기 위해 AI 모델을 사용했으며, 이력 기록에 대한 제한된 전문가 평가를 "완료" 또는 "완료"로 수행했다.

## 6 Discussion

본 연구에서는 진단 추론 능력을 갖춘 임상 대화에 최적화된 LLM 기반 AI 시스템인 AMIE를 도입하였다. 우리는 AMIE 상담을 객관 구조 임상 검사(OSCE) 스타일의 인간 시뮬레이션 환자를 대상으로 무작위 이중 맹검 교차 연구를 사용하여 PCP에 의해 수행된 상담과 비교했다. 특히, 우리의 연구는 전통적인 OSCE 평가, 원격 또는 원격 의료 상담 관행 또는 임상의가 일반적으로 환자와 의사 소통하기 위해 텍스트 및 채팅 메시징을 사용하는 방식에 대한 임상 관례를 대표하도록 설계되지 않았다. 우리의 평가는 대신 오늘날 사람들이 LLM과 상호 작용하는 가장 일반적인 방식을 반영했으며 AI 시스템이 원격 진단 대화에 참여할 수 있도록 잠재적으로 확장 가능하고 친숙한 메커니즘을 활용했다. 이 환경에서 우리는 작업에 특별히 최적화된 AI 시스템인 AMIE가 임상적으로 의미 있는 상담 품질의 여러 축을 따라 평가할 때 시뮬레이션된 진단 대화에서 PCP를 능가하는 것을 관찰했다.

**진단 성능.** AMIE에서 제공한 차등 진단은 전문 의사가 평가할 때 보드 인증 PCP에서 제공한 진단보다 더 정확하고 완전했다. 이전 연구에서는 AI 시스템이 후향적 평가에서 특정, 좁은 작업[69, 70, 71]에서 인간 진단 성능과 일치하거나 초과할 수 있음을 보여주었다. 그러나 이러한 상황은 일반적으로 AI와 의사 모두가 동일한 고정 입력을 해석하는 것(예를 들어, 의료 이미지에서 특정 발견의 존재를 식별하는 것)을 포함했다. 우리의 연구는 인간의 노력에 의해 수집된 임상 정보에 의존하기보다는 AI 시스템이 대화를 통해 관련 정보를 적극적으로 획득하도록 요구했기 때문에 훨씬 더 어려웠다[72]. 따라서 시스템의 다운스트림 차등진단은 진단 추론 능력뿐만 아니라 자연스러운 대화와 라포 구축을 통해 불확실성 하에서 수집된 정보의 질에 의존하였다.

우리의 결과는 AMIE가 시뮬레이션 상담 동안 관련 정보를 도출하는 데 PCP만큼 적합하고 동일한 양의 획득된 정보가 주어진 경우 완전한 차등 진단을 공식화하는 데 PCP보다 더 정확하다는 것을 시사했다. 이 발견은 LLM이 도전적인 사례에서 의사와 동일한 임상 정보를 감안할 때 보다 완전한 차등 진단을 생성할 수 있다는 다른 연구를 확증한다[70]. 따라서 이 연구에서 탐구되지는 않았지만 AMIE의 보조 성능은 특히 의학과 같은 안전이 중요한 환경에서 AI 시스템에 대한 경험의 실제 중요성을 고려할 때 향후 연구를 위한 흥미롭고 중요한 길을 나타낸다.

우리의 연구는 캐나다와 인도에서 훈련된 행위자와 다양한 전문 분야에 걸친 시나리오로 구성된 다양한 시뮬레이션 환자를 활용했다. 이를 통해 전문 분야별, 시나리오가 도출되고 제정된 위치별, 다축을 따라 성능이 어떻게 달라지는지 탐구할 수 있었다. 우리는 PCP와 AMIE가 다른 전문 분야의 시나리오보다 산과/유전자학 및 내과 시나리오에서 더 나쁜 결과를 보였다는 것을 관찰했다(그림 A.8 참조). 이 연구는 다른 전문 분야 주제 간의 성능을 비교하도록 구동되거나 설계되지 않았으며 일부 전문 분야의 시나리오가 다른 전문 분야보다 어려울 수 있음을 배제할 수 없다. 우리는 AMIE와 PCP가 모두 인도 OSCE 실험실에서 제정된 상담과 비교하여 캐나다 OSCE 실험실에서 수행된 상담에서 진단 정확도가 더 높다는 것을 관찰했다(그림 A.13 참조). 그러나 그 차이는 통계적으로 유의하지 않았으며 캐나다 OSCE 랩과 인도 OSCE 랩 모두에서 제정된 40개 시나리오의 하위 집합에서 AMIE와 PCP의 성능은 동등했다(그림 A.14 참조).

**대화 성능.** 환자 행위자와 전문가 평가자는 모두 공감 및 커뮤니케이션 기술과 관련된 메트릭에 대해 AMIE의 성능이 PCP보다 높다고 평가했다. 이 축은 평가된 치수의 대부분을 구성했다. 이 일반적인 발견은 LLM 반응이 레딧[73]에 게시된 건강 질문에 대한 임상의의 반응보다 더 공감하는 것으로 밝혀진 이전 연구와 일치한다. 그러나 해당 연구의 결과는 연구 설계의 차이로 인해 우리의 설정으로 직접 일반화되지 않을 수 있다. 특히, 이전 작업은 동일한 환자와의 다중 회전 대화의 전향적 시뮬레이션에서 의사와 AI 시스템의 직접 무작위 비교를 포함하지 않았다. 두 설정 모두에서 음성 기반 및 비언어적 시각적 커뮤니케이션의 부족은 임상의에게 불공평한 불이익이 될 수 있다.

본 연구에서 사용한 텍스트 기반 채팅 인터페이스는 장단점을 모두 소개한다. 오늘날 사람들은 동기식 문자-채팅 인터페이스를 통해 LLMs에 가장 일반적으로 관여하며[74], 환자들은 종종 환자 포털을 사용하여 그들의 제공자에게 메시지를 전송한다. 따라서 우리는 이 상호작용 모드를 LLM이 멀티턴 대화를 수행하기 위한 대표적인 인터페이스로 선택했으며, 이에 따라 가상 OSCE 프레임워크를 적용했다. 이는 둘 다 동기식 문자 채팅으로 제한되었을 때 LLM과 임상의 사이의 진단 대화를 공정하게 비교할 수 있었지만, 우리의 실험이 실제 임상 실습(원격 의료 포함)에서 예상되는 진단 대화의 품질을 모방하지 않는다는 것을 인정하는 것이 중요하다. 의사는 동기식 문자-채팅 통신보다 전화 또는 화상 상담에 의한 이력 작성 및 진단 대화에 더 익숙할 수 있다[75, 76]. 대신, 텍스트는 특정 테스트 결과에 대한 처방 리필 또는 통신과 같은 일화적 또는 비동기적 요구를 위해 환자와 통신하기 위해 임상의에 의해 더 일반적으로 사용된다[77]. 따라서 의사는 이 연구에서 사용한 동기식 문자 채팅 매체보다 문자/SMS 또는 이메일에 더 친숙할 수 있다. 텍스트/SMS와 이메일 모두에서 자연스럽게 그리고 공감적인 스타일로 소통하기 위한 관습과 기대는 다를 수 있다[78]. 우리 연구의 PCP가 아직 설정에 익숙하지 않았을 수 있으며 특정 훈련 프로그램(AMIE를 위한 훈련 과정과 정신적으로 유사)을 받는 경우 다르게 수행되었을 수 있다. 연구에 참여한 임상의는 평가가 시작되기 전에 동기식 텍스트 인터페이스와 두 번의 준비 파일럿 세션의 상담을 수행했지만 이는 공식적인 훈련 프로그램이 아니며 임상의의 성능을 최적화하도록 설계되지 않았다. 향후 연구에서는 학습 곡선의 영향에 대한 모니터링을 포함하여 이 질문을 더 철저히 탐구하거나 참여 임상의 또는 시뮬레이션 환자가 원격 의료에 익숙한 정도에 따라 성능이 달라지는지 탐구할 수 있다.

또한, 공감 커뮤니케이션에 관한 우리의 발견은 AMIE 반응이 임상의 반응보다 상당히 길었고 더 큰 구조를 제시했다는 사실에 부분적으로 기인할 수 있다(그림 A.11에 표시됨). 이것은 잠재적으로 관찰자에게 환자의 만족도가 의사와 함께 보내는 시간이 증가함에 따라 증가한다는 알려진 발견과 유사하게 반응을 준비하는 데 더 많은 시간이 소요되었음을 시사할 수 있다[79, 80, 81].

종합적으로, 우리의 연구 결과는 언어 및 비언어 단서의 분석에서 임상의의 기술과 LLM의 잠재적 강점을 결합하여 공감 진술, 구조, 웅변 또는 보다 완전한 감별 진단을 포함한 보다 풍부한 대화 응답을 제안하는 인간-AI 상보성[82]을 활용할 수 있는 추가 연구를 위한 많은 방법을 제안한다.

**시뮬레이션된 대화** 시뮬레이션된 데이터를 사용하면 광범위한 조건 및 환자 컨텍스트로 빠르게 훈련을 확장할 수 있는 반면 검색에서 지식을 주입하면 이러한 대화가 근거 있고 사실적으로 유지되도록 장려되었습니다. 모의 환자는 광범위한 조건을 포함했지만 잠재적인 환자 배경, 성격 및 동기의 전체 범위를 포착하지 못했다. 내면적 자기놀이 절차를 통해 우리가 생성하고 미세 조정에 사용한 모의 대화를 반복적으로 개선할 수 있었다. 그러나 이러한 개선은 비평가 지시에서 좋은 대화를 만드는 것을 표현하는 능력, 효과적인 피드백을 생성하는 비평가의 능력, 그러한 피드백에 적응하는 AMIE의 능력에 의해 제한되었다. 예를 들어, 시뮬레이션된 환경에서 AMIE가 환자에 대해 제안된 차등 및 테스트/치료 계획에 도달한다고 부과하지만, 이러한 엔드포인트는 일부 조건, 특히 가상 채팅 기반 설정에서 비현실적일 수 있다.

**평가 프레임워크** 이전 작업과 달리 의사의 의사 소통 기술 및 이력 작성 품질을 평가하는 데 적합하도록 이미 설정된 기준에 평가를 고정했습니다. AI 시스템에 대한 이전 연구보다 더 광범위하고 다양한 인간 평가를 수행했으며 임상의 및 시뮬레이션 환자 관점에서 평가를 수행했다. 우리의 평가자와 시나리오는 북미, 인도 및 영국을 포함한 여러 지리적 위치에서 조달되었다. 우리의 파일럿 평가 루브릭은 우리가 아는 한 의사 스스로 실제 세계에서 측정된 축을 사용하여 LLM의 이력 작성 및 의사 소통 기술을 처음으로 평가하여 연구의 임상적 관련성을 높였다. 우리의 평가 프레임워크는 환자 중심 커뮤니케이션 모범 사례 또는 상담 품질의 임상적으로 관련된 축을 고려하지 않은 AI 생성 임상 대화의 이전 작업보다 훨씬 더 세분화되고 구체적이다.

그러나 우리의 파일럿 프레임워크는 확정적이지 않으며 향후 연구에서 더 개선될 수 있다. 역사 촬영 자체는 맥락적이며 "좋은 역사"를 결정하는 것은 특정 임상 상황, 환자 및 의사 속성, 문화적 특성 및 기타 많은 요인에 따라 다르다. 임상 이력 작성[83, 84, 85, 86]에 대한 모델의 변화에도 불구하고 연구에 따르면 좋은 임상 인터뷰는 문제 감지 및 진단 정확도와 관련이 있을 뿐만 아니라 환자 및 의사 만족도, 회복 탄력성, 스트레스 및 질병에 대한 회복력, 건강 결과 또는 비용에 이르는 관리 전달[87, 88]을 목표로 한다. 따라서 LLM 이력 작성 품질에 대한 향후 연구는 실제 환경에서 이러한 결과의 전향적 측정(예: 환자 불만 감소[89] 또는 비용 및 치료 효과, 환자 및 제공자 만족도의 개선)을 활용할 수 있지만, 이와 같은 평가는 동일한 개별 환자의 표준 관행과 비교하기 어렵거나 비현실적일 수 있으며 다른 접근 방식의 무작위화도 실제 환경에서 어려울 수 있다.

**평가 폭** 선택한 평가 축은 완전하지 않았으며 그 해석은 본질적으로 종종 주관적이었다. 우리는 북미와 인도 모두에 평가자가 있는 세 국가에서 시나리오 팩을 생성하여 임상의와 일반인의 평가를 수행했지만 모델을 평가하는 임상의와 일반인의 풀은 통찰력의 일반화를 개선하기 위해 더 확장될 수 있다. 우리의 실험은 또한 의도적으로 더 다양한 인간 평가자 풀(임상의 및 일반 사용자)을 사용한 향후 작업을 포함하여 관찰자 간 및 참가자 간 변동성과 같은 다른 측면을 탐색하기 위해 더 광범위한 복제를 겪을 수 있다. 임상 및 건강 형평성 영역 전문가뿐만 아니라 대표적인 환자 풀을 가진 모델 평가 도구 개발에 참여하는 설계도 가치가 있을 수 있다.

우리의 시나리오는 많은 다른 임상 조건과 전문 분야로 구성되었지만, 우리의 실험이 반드시 한 명의 의사(평균적으로 경력에서 수만 건의 상담을 수행할 수 있는)에 의해 축적된 수십 년의 임상 실습을 대표하는 것은 아니다[90]. 의학에서 검사할 수 있는 조건의 범위는 개별 질병의 표현 변화와 마찬가지로 방대하다. 우리의 실험은 다중 이환 및 동시 발생 병리학, 종단 사례 제시 또는 임상 조사의 순차적 정보를 고려하도록 설계되지 않았다. 우리는 정신과, 소아과, 중환자, 입원 환자 사례 관리 시나리오와 같은 일부 임상 환경이나 전문 분야를 완전히 제외했다. 고품질 역사 작업에 대한 요구 사항이 다를 수 있는 이와 같은 많은 환경에서 연구 결과의 적용 가능성을 이해하려면 추가 연구가 필요할 것이다[91, 92]. OSCE 프레임워크는 임상의의 기술 평가에 일반적으로 사용된다. 여기에는 실제 또는 시뮬레이션된 환자, 물리적 인공물 또는 임상 재료와의 상호 작용, 다양한 의료 전문 분야에 대한 적용, 작업 또는 설정, 원격 또는 직접 평가를 포함한 상당한 범위의 방법론이 포함된다. OSCE 접근법이 대중적이지만, 그 타당성에 상당한 한계가 있다[93]. 우리는 비언어적 증상, 징후 및 커뮤니케이션 기능을 통합할 수 없는 것과 같은 "가상 OSCE"의 패러다임으로 알려져 있는 문제를 복제하여 원격 텍스트 기반 평가를 활용했다. 추가적으로, 이 포맷은 PCP 참가자들의 통신에 익숙하지 않은 제약들을 도입할 수 있다[48].

우리 연구에서 OSCE 대화의 톤, 내용 및 특성은 실제 환자 집단을 대표하지 않을 가능성이 있다. 예를 들어, 환자 행위자는 많은 상담에서 일상적으로 예상할 수 있는 것보다 더 큰 구조, 깊이 또는 임상 세부 사항으로 증상을 설명하거나 일반적으로 예상할 수 있는 것보다 임상적 맥락에 대한 이해가 더 클 수 있다. 또한 평가가 맹검되었지만 AMIE의 응답 스타일은 연구 설계에서 맹검의 실질적인 범위를 제한하는 PCP에 의한 응답 스타일과 현저하게 달랐다.

따라서 우리가 다룬 질병과 전문 분야의 분포 내에서도 우리의 연구 결과는 겸손하고 신중하게 해석되어야 한다. 다양한 환자 요구, 선호도, 행동 및 상황의 상황에서 병력 작성 및 임상 대화를 평가하기 위한 대체 접근법의 탐색과 함께 동일한 질병의 다양한 프레젠테이션을 조사하기 위한 추가 연구가 필요하다.

**공정성 및 편향성** 이 논문에서 제시된 평가 프로토콜은 공정성 및 편향성과 관련된 잠재적인 문제를 포착하는 능력 측면에서 제한적이며, 이는 후속 시스템 평가에서 다루고자 하는 중요한 미해결 문제로 남아 있다. 대형 언어 모델에서 편향 검출을 위한 포괄적인 프레임워크의 개발의 최근 진보[94, 95]는 그러한 접근법을 확립하기 위한 유망한 출발점을 제시한다. 의료 진단 대화는 의료 영역의 복잡성, 대화의 대화형 정보 수집 특성 및 결과 기반 설정으로 인해 특히 어려운 사용 사례이며 잘못된 진단 또는 잘못된 의료 조언의 경우 관련 해악의 가능성이 있다는 점에 유의해야 한다. 그럼에도 불구하고, 영역의 LLM이 의료의 불평등을 전파하기보다는 극복해야 하는 경우 이러한 문제를 해결하는 것은 중요한 추가 연구 영역이다. 예를 들어, 이전 연구에서는 의사가 환자의 인종에 따라 평균적으로 환자와의 커뮤니케이션에 다르게 접근하여 환자 중심이 덜하고 긍정적인 영향을 덜 받는 커뮤니케이션을 받는 흑인 환자를 발견했으며[96], 다른 연구에서는 성별에 따라 의사의 커뮤니케이션 스타일과 대화 길이에 차이가 있음을 발견했다[97]. 효과적인 문화 간 의사소통 기술은 필수적이다[91]. 따라서 이러한 역사적 대화 편향이 AI 대화 시스템에서 복제되거나 증폭될 수 있다는 무시할 수 없는 위험이 있지만 동시에 개별 환자의 요구에 더 포괄적이고 더 개인화될 수 있는 대화 시스템을 설계하는 방향으로 일할 기회도 있다.

필요한 공정성, 편견 및 형평성 프레임워크의 개발을 알리는 데 도움이 되려면 광범위한 환자 인구 통계와 임상 및 건강 형평성 영역 전문가에 걸쳐 대표 견해를 요청하는 참여 접근법을 사용하는 것이 중요하다. 이러한 평가 프레임워크는 광범위한 모델 레드 학습과 나머지 갭 및 고장 모드를 식별하기 위한 적대적 접근법으로 보완되어야 한다. 적색 학습 LLM의 최근 발전은 이 시나리오에서 유용할 수 있다[98, 99, 100, 101]. 이러한 관행은 최종 모델의 평가뿐만 아니라 개발 및 반복적인 개선을 알려야 한다. 모델 개발은 확립된 데이터 및 모델 보고 관행을 따르고 훈련 데이터 및 관련 결정 프로세스에 투명성을 제공해야 한다[102, 103, 104]. 우리 연구에서 AMIE 훈련 데이터에 기여하는 대화 연구 데이터 세트는 비식별화되어 사회경제적 요인, 환자 인구 통계 및 임상 환경 및 위치에 대한 정보의 가용성을 줄였다.

다국어 설정에서 의료용 LLM(105, 106, 107, 108), 특히 자원이 적은 언어에서 성능을 보장하기 위한 추가 작업도 필요하다[109]. 매우 다양한 문화[110], 언어, 지역, 정체성 및 지역화된 의료 요구는 선험적 정적이지만 포괄적인 공정성 벤치마크를 실질적으로 실행할 수 없게 만든다. 편향의 측정 및 완화는 전역적으로 확장하지 못하는 특정 축에 대한 전통적인 좁은 초점을 넘어 이동해야 한다[111]. LLM 기반 평가자는 체계적인 벤치마크가 없는 언어에서 예비 평가를 위한 잠재적인 솔루션을 제시하지만, 선행 연구에서는 이러한 자동 평가 프레임워크가 편향되어 원어민 평가에 대한 보정 필요성을 강조하고 주의하여 사용한다[112].

**배포.** 이 연구는 진단 대화의 맥락에서 의료에서 향후 사용할 LLM의 잠재력을 보여줍니다. 본 연구에서 평가된 LLM 연구 프로토타입에서 의료 제공자, 관리자 및 사람들이 사용할 수 있는 안전하고 강력한 도구로 전환하는 것은 기술의 안전성, 신뢰성, 효능 및 개인 정보를 보장하기 위해 상당한 추가 연구가 필요할 것이다. 다양한 임상 환경에 걸친 엄격한 품질 평가와 필요할 때 인간 임상 전문가에게 연기할 수 있는 신뢰할 수 있는 불확실성 추정 방법[113, 114, 115, 116]에 대한 연구를 포함하여 이 기술의 윤리적 배치에 대한 신중한 고려가 필요할 것이다. 이러한 및 기타 가드레일은 LLM 기술에 대한 잠재적인 과의존을 완화하기 위해 필요하며, 향후 사용 사례에 특정한 윤리적 및 규제 요구 사항에 주의를 기울이고 모델 출력을 보호하기 위해 루프에 자격을 갖춘 의사의 존재를 위한 기타 특정 조치가 필요하다. 이전 작업 [12]에서 강조했듯이 기본 모델 또는 배치 사용 상황에서 편향 및 보안 취약성이 발생할 수 있는 정도를 평가하기 위한 추가 연구도 필요할 것이다. 임상 지식의 지속적인 진화를 고려할 때, LLM이 최신 임상 정보를 활용하는 방법을 개발하는 것도 중요할 것이다[117].

## 7 Conclusion

의료 AI 시스템이 적절한 수준의 공감 및 신뢰와 소통하면서 대규모 의료 지식에 고정하면서 대화적으로 더 잘 상호작용할 수 있다면 활용도가 크게 향상될 수 있다. 이 연구는 임상 이력 작성 및 진단 대화와 관련된 설정을 위한 LLM 기반 AI 시스템의 상당한 잠재적 능력을 보여준다. 시뮬레이션 상담에서 AMIE의 성능은 대화형 진단 의료 AI에 대한 여러 임상 관련 축을 고려한 평가 프레임워크를 따라 평가되었기 때문에 현장의 이정표를 나타낸다. 그러나 결과는 적절한 주의를 기울여 해석해야 한다. 실험 시뮬레이션된 역사 촬영 및 진단 대화의 제한된 범위에서 사람과 이를 돌보는 사람들을 위한 실제 도구로 번역하려면 기술의 안전성, 신뢰성, 공정성, 효능 및 프라이버시를 보장하기 위한 상당한 추가 연구 개발이 필요하다. 성공적인 경우 AMIE와 같은 AI 시스템이 모든 사람에게 세계 수준의 헬스케어를 확장하는 데 도움이 되는 차세대 학습 건강 시스템의 핵심이 될 수 있다고 믿습니다.

#### Acknowledgments

이 프로젝트는 구글 리서치와 구글 딥마인드의 많은 팀 간의 광범위한 협업이었습니다. 우리는 윤류, 다니엘 맥더프, 제이크 선샤인, 알리 코넬, 폴 맥거번, 주빈 가흐라마니가 원고에 대한 포괄적인 검토와 상세한 피드백에 감사한다. 우리는 또한 사미 라흐가, 로렌 와이너, 존 길리어드 그리고 매기 시엘스가 서사와 시각에 기여한 것에 대해 감사한다. 우리는 줄리 앤 세긴, 샐리 골드만, 유리 바실레프스키, 신잉 송, 악샤이 고엘, 추링 고, 아비나브 다스, 하이양 유, 창 류, 유첸 류, 시와이 맨, 브렛 해트필드, 션 리, 아제이 조시, 고든 터너, 아니사 엄라니, 디브야 판디아 및 프리티 싱에게 우리의 연구 중 귀중한 통찰력, 기술적 지원 및 피드백을 감사한다. 또한 OSCE 연구를 수행하는 데 파트너십을 맺은 캐나다와 인도의 임상 공급자 파트너에 감사드립니다. 마지막으로, 우리는 이 프로젝트 과정에서 그들의 지원에 대해 데일 웹스터, 에와 도미노프스카, 데이비드 플릿, 필립 맨스필드, 수샨트 프라카시, 르네 웡, 수잔 토마스, 마이클 하웰, 카렌 드살보, 제프 딘, 제임스 마니카, 주빈 가흐라마니, 데미스 하사비스에게 감사한다.

#### Data Availability

AMIE 개발에 사용되는 실제 데이터 세트 중 일부는 오픈 소스(MedQA)이다. OSCE 연구에 사용된 영국의 시나리오 팩은 또한 인터넷에서 다운로드할 수 있다.

#### Code Availability

AMIE는 진단 대화를 위한 LLM 기반 연구 AI 시스템이다. 우리는 의료 환경에서 모니터링되지 않은 시스템 사용의 안전 문제로 인해 오픈 소싱 모델 코드와 가중치가 아니다. 책임 있는 혁신을 위해, 우리는 AMIE의 안전한 향후 사용을 검증하고 탐색하기 위해 연구 파트너, 규제 기관 및 제공자와 협력할 것입니다. 재현성을 위해 임상 및 일반 과학 청중이 논문에 액세스할 수 있도록 유지하면서 기술적 딥러닝 방법을 문서화했다. 우리의 작업은 기술 보고서[10]에서 기술 세부 사항이 광범위하게 설명된 PaLM 2를 기반으로 한다.

#### Competing Interests

이 연구는 알파벳 주식회사 및/또는 그 자회사('알파벳')의 자금 지원을 받았다. 모든 저자는 알파벳의 직원이며 표준 보상 패키지의 일부로 주식을 소유할 수 있다.

## References

* [1] Engel, G. L. & Morgan, W. L. Interviewing the patient (1973).
* [2] Peterson, M. C., Holbrook, J. H., Von Hales, D., Smith, N. & Staker, L. Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses. _Western Journal of Medicine_**156,** 163 (1992).
* [3] Hampton, J. R., Harrison, M., Mitchell, J. R., Prichard, J. S. & Seymour, C. Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients. _Br Med J_**2,** 486-489 (1975).
* [4] Kassier, J. P. _Teaching clinical medicine by iterative hypothesis testing: let's preach what we practice_ 1983.
* [5] Roshan, M. & Rao, A. A study on relative contributions of the history, physical examination and investigations in making medical diagnosis. _The Journal of the Association of Physicians of India_**48,** 771-775 (2000).
* [6] Sandler, G. The importance of the history in the medical clinic and the cost of unnecessary tests. _American heart journal_**100,** 928-931 (1980).
* [7] Silverman, J., Kurtz, S. & Draper, J. _Skills for communicating with patients_ (crc press, 2016).
* [8] Rennie, T., Marriott, J. & Brock, T. P. Global supply of health professionals. _N Engl J Med_**370,** 2246-7 (2014).
* [9] OpenAI. _GPT-4 Technical Report_ 2023. arXiv: 2303.08774 [cs.CL].
* [10] Google. _PaLM 2 Technical Report_[https://ai.google/static/documents/palm2techreport.pdf](https://ai.google/static/documents/palm2techreport.pdf). 2023.
* [11] Deepmind, G. _Gemini: A Family of Highly Capable Multimodal Models_[https://assets.bwx.io/documents/users/iqjWHBFdKIU/rG7GTRrT6rnM/v0](https://assets.bwx.io/documents/users/iqjWHBFdKIU/rG7GTRrT6rnM/v0). 2023.
* [12] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., _et al._ Large Language Models Encode Clinical Knowledge. _arXiv preprint arXiv:2212.13138_ (2022).
* [13] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., _et al._ Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_ (2023).
* [14] Nori, H., Lee, Y. T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N., Larson, J., Li, Y., Liu, W., _et al._ Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine. _arXiv preprint arXiv:2311.16452_ (2023).
* [15] Thoppilian, R., De Freitas, D., Hall, J., Shazeer, N., Kultsreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., _et al._ LaMDA: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_ (2022).
* [16] OpenAI. _Introducing ChatGPT_ OpenAI. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
* [17] Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B. & Wang, B. Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. _arXiv preprint arXiv:2305.12031_ (2023).
* [18] Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., Kopf, A., Mohtashami, A., _et al._ MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. _arXiv preprint arXiv:2311.16079_ (2023).
* [19] Levine, D. History taking is a complex skill. _BMJ_**358** (2017).
* [20] King, A. & Hoppe, R. B. "Best practice" for patient-centered communication: a narrative review. _Journal of graduate medical education_**5,** 385-393 (2013).
* [21] Jin, D., Pan, E., Qufattole, N., Weg, W.-H., Fang, H. & Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_**11,** 6421 (2021).
* [22] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L. & Mark, R. G. MIMIC-III, a freely accessible critical care database. _Scientific data_**3,** 1-9 (2016).
* [23] Chiu, C.-C., Tripathi, A., Chou, K., Co, C., Jaitly, N., Jaunzeikare, D., Kannan, A., Nguyen, P., Sak, H., Sankar, A., _et al._ Speech recognition for medical conversations. _arXiv preprint arXiv:1711.07274_ (2017).
* [24] Sharma, A., Miner, A. S., Atkins, D. C. & Althoff, T. A computational approach to understanding empathy expressed in text-based mental health support. _arXiv preprint arXiv:2009.08441_ (2020).
* [25] Fu, Y., Peng, H., Khot, T. & Lapata, M. Improving language model negotiation with self-play and in-context learning from ai feedback. _arXiv preprint arXiv:2305.10142_ (2023).
* [26] Abacha, A. B., Yim, W.-W., Adams, G., Snider, N. & Yetiesgen-Yildiz, M. _Overview of the mediga-chat 2023 shared tasks on the summarization & generation of doctor-patient conversations_ in _Proceedings of the 5th Clinical Natural Language Processing Workshop_ (2023), 503-513.
* [27] Ionescu, B., Muller, H., Dragulinescu, A.-M., Yim, W.-W., Ben Abacha, A., Snider, N., Adams, G., Yetiesgen, M., Ruckert, J., G. Seco de Herrera, A., _et al._ Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications in International Conference of the Cross-Language Evaluation Forum for European Languages_ (2023), 370-396.
* [28] He, Z., Han, Y., Ouyang, Z., Gao, W., Chen, H., Xu, G. & Wu, J. DialMed: A Dataset for Dialogue-based Medication Recommendation. _arXiv preprint arXiv:2203.07094_ (2022).
* [29] Naseem, U., Bandi, A., Raza, S., Rashid, J. & Chakravarthi, B. R. _Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation_ in _Proceedings of the 21st Workshop on Biomedical Language Processing_ (2022), 110-115.
* [30] Dacre, J., Besser, M. & White, P. MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions (June 2001-July 2002). _Clinical Medicine_**3,** 452 (2003).
* [31] Sloan, D. A., Donnelly, M. B., Schwartz, R. W. & Strodel, W. E. The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance. _Annals of surgery_**222,** 735 (1995).
* [32] Carraccio, C. & Englander, R. The objective structured clinical examination: a step in the direction of competency-based evaluation. _Archives of pediatrics & adolescent medicine_**154,** 736-741 (2000).
* [33] Epstein, R. M. & Hundert, E. M. Defining and assessing professional competence. _Jama_**287,** 226-235 (2002).
* [34] Horowitz, J. L. in _Handbook of econometrics_ 3159-3228 (Elsevier, 2001).
* [35] Benjamini, Y. & Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. _Journal of the Royal statistical society: series B (Methodological)_**57,** 289-300 (1995).
* [* [36] Woolson, R. F. Wilcoxon signed-rank test. _ Wiley 백과사전 of clinical trials_, 1-3 (2007).
* [37] Keifenheim, K. E., Teufel, M., Ip, J., Speiser, N., Leehr, E. J., Zipfel, S. & Herrmann-Werner, A. Teaching history taking to medical students: a systematic review. _BMC medical education_**15,** 1-12 (2015).
* [38] Yedidia, M. J., Gillespie, C. C., Kachur, E., Schwartz, D. D., Ockene, J., Chepatitis, A. E., Snyder, C. W., Lazare, A. & Lipkin Jr, M. Effect of communications training on medical student performance. _Jama_**290,** 1157-1165 (2003).
* [39] Makoul, G. Communication skills education in medical school and beyond. _Jama_**289,** 93-93 (2003).
* [40] Tan, X. H., Foo, M. A., Lim, S. L. H., Lim, M. B. X. Y., Chin, A. M. C., Zhou, J., Chiam, M. & Krishna, L. K. R. Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review. _BMC medical education_**21,** 1-19 (2021).
* [41] Raper, S. E., Gupta, M., Oksunaya, O. & Morris, J. B. Improving communication skills: a course for academic medical center surgery residents and faculty. _Journal of Surgical education_**72,** 2620-e211 (2015).
* [42] Von Fargeten, M., Silverman, J., Cushing, A., Quilligan, S., Salisbury, H., Wiskin, C. & for Clinical Communication Skills Teaching in Undergraduate Medical Education, U. C. UK consensus statement on the content of communication curricula in undergraduate medical education. _Medical education_**42,** 1100-1107 (2008).
* [43] De Haes, H. & Bensing, J. Endpoints in medical communication research, proposing a framework of functions and outcomes. _Patient education and counseling_**74,** 287-294 (2009).
* [44] Epstein, R. M. & Street Jr, R. L. Patient-centered communication in cancer care: promoting healing and reducing suffering (2007).
* [45] Schirmer, J. M., Mauksch, L., Lang, F., Marvel, M. K., Zoppi, K., Epstein, R. M., Brock, D. & Pryzbylski, M. Assessing communication competence: a review of current tools. _Family Medicine_**37,** 184-92 (2005).
* [46] Nichol, J. R., Sundjaja, J. H. & Nelson, G. Medical history. [http://europepmc.org/books/NBK534249](http://europepmc.org/books/NBK534249) (2018).
* [47] Denness, C. What are consultation models for? _InnovAiT_**6,** 592-599 (2013).
* [48] Chan, S. C. C., Choa, G., Kelly, J., Maru, D. & Rashid, M. A. Implementation of virtual OSCE in health professions education: A systematic review. _Medical Education_ (2023).
* [49] Budzianowski, P., Wen, T.-H., Tseng, B.-H., Casanueva, I., Ultes, S., Ramadan, O. & Gasic, M. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. _arXiv preprint arXiv:1810.00278_ (2018).
* [50] Wei, W., Le, Q., Dai, A. & Li, J. _Aridialogue: An environment for goal-oriented dialogue research in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_ (2018), 3844-3854.
* [51] Lin, J., Tomlin, N., Andreas, J. & Eisner, J. _Decision-Oriented Dialogue for Human-AI Collaboration_ 2023. arXiv: 2305.20076 [cs.CL].
* [52] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. & Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_**30** (2017).
* [53] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., _et al._ Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_ (2022).
* [54] Zhao, J., Khashabi, D., Khot, T., Sabharwal, A. & Chang, K.-W. Ethical-advice taker: Do language models understand natural language interventions? _arXiv preprint arXiv:2106.01465_ (2021).
* [55] Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J. & Leike, J. Self-critiquing models for assisting human evaluators. _arXiv preprint arXiv:2206.05802_ (2022).
* [56] Scheurer, J., Campos, J. A., Korbak, T., Chan, J. S., Chen, A., Cho, K. & Perez, E. Training language models with language feedback at scale. _arXiv preprint arXiv:2303.16755_ (2023).
* [57] Glasee, A., McAlees, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., _et al._ Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_ (2022).
* [58] Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., _et al._ Constitutional AI: Harmlessness from AI feedback. _arXiv preprint arXiv:2212.08073_ (2022).
* [59] Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., _et al._ A general language assistant as a laboratory for alignment. _arXiv preprint arXiv:2112.00861_ (2021).
* [60] Shor, J., Bi, R. A., Venugopalan, S., Ibara, S., Goldenberg, R. & Rivien, E. Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. _arXiv preprint arXiv:2303.05737_ (2023).
* [61] Abacha, A. B., Agichtein, E., Pinter, Y. & Demner-Fushman, D. _Overview of the medical question answering task at TREC 2017 LiveQA._ in _TREC_ (2017), 1-12.
* [62] Wallace, W., Chan, C., Chidambaran, S., Hanna, L., Iqbal, F. M., Acharya, A., Normahani, P., Ashrafian, H., Markar, S. R., Sounderajah, V., _et al._ The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review. _NPJ Digital Medicine_**5,** 118 (2022).
* [63] Zeltzer, D., Herzog, L., Pickman, Y., Steuerman, Y., Ber, R. I., Kugler, Z., Shaul, R. & Ebbert, J. O. Diagnostic accuracy of artificial intelligence in virtual primary care. _Mayo Clinic Proceedings: Digital Health_**1,** 480-489 (2023).
* [64] Johri, S., Jeong, J., Tran, B. A., Schlessinger, D. I., Wongvibulsin, S., Cai, Z. R., Daneshjou, R. & Rajpurkar, P. Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment. _medRxiv_, 2023-09 (2023).
* [65] Zeng, G., Yang, W., Ju, Z., Yang, Y., Wang, S., Zhang, R., Zhou, M., Zeng, J., Dong, X., Zhang, R., _et al.__MedDialog: Large-scale medical dialogue datasets in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_ (2020), 9241-9250.
* [66] Liu, W., Tang, J., Cheng, Y., Li, W., Zheng, Y. & Liang, X. _MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation_ in _CCF International Conference on Natural Language Processing and Chinese Computing_ (2022), 447-459.
* [67] Varshney, D., Zafar, A., Behra, N. K. & Ekbal, A. Cdialog: A multi-turn COVID-19 conversation dataset for entity-aware dialog generation. _arXiv preprint arXiv:2212.06049_ (2022).
* [68] Yan, G., Pei, J., Ren, P., Ren, Z., Xin, X., Liang, H., de Rijke, M. & Chen, Z. _ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2022), 3013-3024.
* [69] Kelly, C. J., Karthikesalingam, A., Suleyman, M., Corrado, G. & King, D. Key challenges for delivering clinical impact with artificial intelligence. _BMC medicine_**17,** 1-9 (2019).
* [* [70] McDuff, D., Schaekermann, M., Tu, T., Palepu, A., Wang, A., Garrison, J., Singhal, K., Sharma, Y., Azizi, S., Kulkarni, K., _et al._ Towards Accurate Differential Diagnosis with Large Language Models. _ arXiv preprint arXiv:2312.00164_ (2023).
* [71] Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge. _JAMA_ (2023).
* [72] Semigran, H. L., Linder, J. A., Gidengli, C. & Mehrotra, A. Evaluation of symptom checkers for self diagnosis and triage: audit study. _BMJ_ **351** (2015).
* [73] Avers, J. W., Poliak, A., Dredze, M., Leas, E. C., Zhu, Z., Kelley, J. B., Faix, D. J., Goodman, A. M., Longhurst, C. A., Hogarth, M., _et al._ Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. _JAMA Internal Medicine_ (2023).
* [74] OpenAI. _ChaotOPT_ OpenAI. [https://chat.openai.com/chat](https://chat.openai.com/chat).
* [75] Carrillo de Albornoz, S., Sia, K.-L. & Harris, A. The effectiveness of teleconsultations in primary care: systematic review. _Family Practice_**39,** 168-182 (2022).
* [76] Wharton, G. A., Sood, H. S., Sissons, A. & Mossialos, E. Virtual primary care: fragmentation or integration? _The Lancet Digital Health_**1,** e330-e331 (2019).
* [77] Fuster-Casanova, A. & Vidal-Alaball, J. Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature. _International Journal of Integrated Care_**22** (2022).
* [78] Hammersley, V., Donaghy, E., Parker, R., McNeilly, H., Atherton, H., Bikker, A., Campbell, J. & McKinstry, B. Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care. _British Journal of General Practice_**69,** e595-e604 (2019).
* [79] Gross, D. A., Zyzanski, S. J., Borawski, E. A., Cebul, R. D. & Stange, K. C. Patient satisfaction with time spent with their physician. _Journal of Family Practice_**47,** 133-138 (1998).
* [80] Tates, K., Antheunis, M. L., Kanters, S., Nieboer, T. E. & Gerrites, M. B. The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients. _Journal of medical Internet research_**19,** e421 (2017).
* [81] Zyzanski, S. J., Stange, K. C., Langa, D. M. & Flocke, S. A. Trade-offs in high-volume primary care practice. _Journal of Family Practice_**46,** 397-402 (1998).
* [82] Dvijotham, K., Winkens, J., Barsby, M., Ghaisas, S., Stanforth, R., Pawlowski, N., Strachan, P., Ahmed, Z., Azizi, S., Bachrach, Y., _et al._ Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians. _Nature Medicine_**29,** 1814-1820 (2023).
* [83] Bird, J. & Cohen-Cole, S. A. in _Methods in teaching consultation-liaison psychiatry_ 65-88 (Karger Publishers, 1990).
* [84] Rezler, A. G., Woolliscroft, J. A. & Kalishman, S. G. What is missing from patient histories? _Medical Teacher_**13,** 245-252 (1991).
* [85] Rosenberg, E. E. Lessons for Clinicians From Physician-Patient. _Arch Fam Med_**6,** 279-283 (1997).
* [86] Smith, R. C. _Patient-centered interviewing: an evidence-based method_ (Lippincott Williams & Wilkins, 2002).
* [87] Berwick, D. M., Nolan, T. W. & Whittington, J. The triple aim: care, health, and cost. _Health affairs_**27,** 759-769 (2008).
* [88] Bodenheimer, T. & Sinsky, C. From triple to quadruple aim: care of the patient requires care of the provider. _The Annals of Family Medicine_**12,** 573-576 (2014).
* [89] Adamson, T. E., Tschann, J. M., Gullion, D. & Oppenberg, A. Physician communication skills and malpractice claims. A complex relationship. _Western Journal of Medicine_**150,** 356 (1989).
* [90] Silverman, J. & Kinnersley, P. _Doctors' non-verbal behaviour in consultations: look at the patient before you look at the computer_ 2010.
* [91] Rahman, U. & Cooling, N. Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review. _Medical Research Archives_**11** (2023).
* [92] Kantar, A., Marchant, J. M., Song, W.-J., Shields, M. D., Chatziparasidis, G., Zacharasiewicz, A., Moeller, A. & Chang, A. B. History taking as a diagnostic tool in children with chronic cough. _Frontiers in pediatrics_**10,** 850912 (2022).
* [93] Setsonuproho, W., Kennedy, K. M. & Kropmans, T. J. Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review. _Patient education and counseling_**98,** 1482-1491 (2015).
* [94] Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., _et al. Taxonomy of risks posed by language models in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_ (2022), 214-229.
* [95] Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R. & Ahmed, N. K. _Bias and Fairness in Large Language Models: A Survey_ 2023. arXiv: 2309.00770 [cs.CL].
* [96] Johnson, R. L., Roter, D., Powe, N. R. & Cooper, L. A. Patient race/ethnicity and quality of patient-physician communication during medical visits. _American journal of public health_**94,** 2084-2090 (2004).
* [97] Roter, D. L., Hall, J. A. & Aoki, Y. Physician gender effects in medical communication: a meta-analytic review. _Jama_**288,** 756-764 (2002).
* [98] Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N. & Irving, G. Red teaming language models with language models. _arXiv preprint arXiv:2202.03286_ (2022).
* [99] Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., _et al._ Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_ (2022).
* [100] Yu, J., Lin, X. & Xing, X. Optrazer: Red teaming large language models with auto-generated jailbreak prompts. _arXiv preprint arXiv:2309.10253_ (2023).
* [101] Ge, S., Zhou, C., Hou, R., Khabsa, M., Wang, Y.-C., Wang, Q., Han, J. & Mao, Y. MART: Improving LLM Safety with Multi-round Automatic Red-Teaming. _arXiv preprint arXiv:2311.07689_ (2023).
* [102] Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D. & Gebru, T. _Model cards for model reporting_ in _Proceedings of the conference on fairness, accountability, and transparency_ (2019), 220-229.
* [103] Crisan, A., Drouhard, M., Vig, J. & Rajani, N. _Interactive model cards: A human-centered approach to model documentation_ in _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_ (2022), 427-439.

* [104] Pushkarma, M., Zaldivar, A. & Kjartansson, O. _Data cards: Purposeful and transparent dataset documentation for responsible ai_ in _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_ (2022), 1776-1826.
* [105] Choudhury, M. & Deshpande, A. _How Linguistically Fair Are Multilingual Pre-Trained Language Models?_ in _Proceedings of the AAAI conference on artificial intelligence_ 35 (2021), 12710-12718.
* [106] Talat, Z., Neved, A., Biderman, S., Chinciu, M., Dey, M., Longpre, S., Luccioni, S., Masoud, M., Mitchell, M., Radev, D., _et al.__You reap what you sow: On the challenges of bias evaluation under multilingual settings_ in _Proceedings of BigScience Episode# 5-Workshop on Challenges & Perspectives in Creating Large Language Models_ (2022), 26-41.
* [107] Ahuja, S., Aggarwal, D., Gumm, V., Watts, I., Sathe, A., Ochieng, M., Hada, R., Jain, P., Axmed, M., Bali, K. & Sitaram, S. _GEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks_ 2023. arXiv: 2311.07463 [cs.CL].
* [108] ImanGoogbari, A., Lin, P., Kargaran, A. H., Severini, S., Jalili Sabet, M., Kassner, N., Ma, C., Schmid, H., Martins, A., Yvon, F. & Schutze, H. _Glo500: Scaling Multilingual Corpora and Language Models to 500 Languages_ in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ (Association for Computational Linguistics, 2023). [http://dx.doi.org/10.18653/v1/2023.acl-long.61](http://dx.doi.org/10.18653/v1/2023.acl-long.61).
* [109] Nguyen, X.-P., Aljunied, S. M., Joty, S. & Bing, L. _Demerametzing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts_ 2023. arXiv: 2306.11372 [cs.CL].
* [110] Naous, T., Ryan, M. J., Ritter, A. & Xu, W. _Having Beer after Prayer? Measuring Cultural Bias in Large Language Models_ 2023. arXiv: 2305.14456 [cs.CL].
* [111] Ramesh, K., Sitaram, S. & Choudhury, M. _Fairness in Language Models Beyond English: Gaps and Challenges_ 2023. arXiv: 2302.12578 [cs.CL].
* [112] Hada, R., Gumma, V., de Wynter, A., Diddee, H., Ahmed, M., Choudhury, M., Bali, K. & Sitaram, S. _Are Large Language Model-based Evalutators the Solution to Scaling Up Multilingual Evaluation?_ 2023. arXiv: 2309.07462 [cs.CL].
* [113] Quach, V., Fisch, A., Schuster, T., Yala, A., Sohn, J. H., Jaakkola, T. S. & Barzilay, R. _Conformal Language Modeling_ 2023. arXiv: 2306.10193 [cs.CL].
* [114] Chen, J. & Mueller, J. _Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness_ 2023. arXiv: 2308.16175 [cs.CL].
* [115] Huang, Y., Song, J., Wang, Z., Zhao, S., Chen, H., Juefei-Xu, F. & Ma, L. _Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models_ 2023. arXiv: 2307.10236 [cs.SE].
* [116] Yang, Q., Ravikumar, S., Schmitt-Ulms, F., Lolla, S., Demir, E., Elistratov, I., Lavaee, A., Lolla, S., Ahmadi, E., Rus, D., Amini, A. & Perez, A. _Uncertainty-aware Language Modeling for Selective Question Answering_ 2023. arXiv: 2311.15451 [cs.CL].
* [117] Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., de Masson d'Autume, C., Kocisky, T., Ruder, S., _et al._ Mind the gap: Assessing temporal generalization in neural language models. _Advances in Neural Information Processing Systems_ **34,** 29348-29363 (2021).

[MISSING_PAGE_FAIL:26]

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Question** & **Scale** & **Options** & **Assessed by** \\ \hline \hline \multicolumn{4}{c}{**Clinical Communication Skills**} \\ \hline To what extent did the doctor & 1 - Appears unystematic, unpractised, and unprofessional & \\ elicit the PRESENTING & 5-point scale & 5 - Elicits presenting compliant in a thorough, systematic, fluent and professional manner & Specialist \\ COMPLANIT? & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Appears unystematic, unpractised, and unprofessional & \\ elicit the SYSTEMS REVIEW? & 5-point scale & 5 - Elicits systems review in a thorough, systematic, fluent and professional manner & Specialist \\  & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Appears unystematic, unpractised, and unprofessional & \\ elicit the PAST MEDICAL & 5-point scale & 5 - Elicits past medical history in a thorough, systematic, fluent and professional manner & Specialist \\  & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Appears unystematic, unpractised, and unprofessional & \\ elicit the FAMILY HISTORY? & 5-point scale & 5 - Elicits family injury in a thorough, systematic, fluent and professional manner & Specialist \\  & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Appears unystematic, unpractised, and unprofessional & \\ elicit the MEDICATION HISTORY? & 5-point scale & 5 - Elicits medication history in a thorough, systematic, fluent and professional manner & Specialist \\  & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Glives inaccurate information & \\ explain relevant clinical & 5-point scale & 5 - Explains relevant clinical information in a accurate manner & Specialist \\  & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Glives jargon & \\ explain relevant clinical & 5-point scale & 5 - Explains relevant clinical information in a clear manner & Specialist \\ information CLEORY? & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Explains relevant clinical information in a poorly structured manner & \\ explain relevant clinical & 5-point scale & 5 - Explains relevant information in a structured manner & Specialist \\ information WITH STRUCTURE? & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Orinks important information & \\ explain relevant clinical & 5-point scale & 5 - Explains relevant clinical information in a comprehensive manner & Specialist \\ information COMPREHENS/VLT? & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline To what extent did the doctor & 1 - Explains relevant clinical information in an unprofessional manner & \\ explain relevant clinical & 5-point scale & 5 - Explains relevant clinical information in a professional manner & Specialist \\ information PROFESSIONALITY? & & **Camort rate / Does not apply / Doctor did not perform this** & \\ \hline \hline \multicolumn{4}{c}{**Differential Diagnosis**} \\ \hline To what extent did the doctor & & 1 - Poor differential diagnosis AND fails to consider the correct diagnosis & \\ construct a sensible & 5-point scale & 5 - Constructs a sensible differential diagnosis, including the correct diagnosis & Specialist \\  & & **Clinical Judgement** & \\ \hline To what extent did the doctor & & & \\ select a comprehensive, sensible & 1 - Unformaliz with correct management plan AND selects inappropriate management & \\  & & 5 - Selects a comprehensive, sensible and appropriate management plan & Specialist \\  & & **PlAN?** & \\ \hline \multicolumn{4}{c}{**Managing Patient Concerns**} \\ \hline To what extent did the doctor & & & \\ seek, detect, acknowledge and & 1 - Overtools patient’s concerns & Specialist \& \\ attempt to address the patients & 5-point scale & 5 - Seeks, detects, acknowledges and attempts to address patient’s concerns & Patient Actor \\ concerns? & & & \\ To what extent did the doctor & & & \\ confirm the patient’s knowledge & 5-point scale & 1 - Does not check knowledge and understanding & Specialist \& \\ and understanding? & & 5 - Conforms patient’s knowledge and understanding & Patient Actor \\ \hline \multicolumn{4}{c}{**Mantaining Patient Welfare**} \\ \hline To what extent did the doctor & & & \\ maintain the patient’s welfare? & & & \\ \hline \hline \end{tabular}
\end{table}
표 2: 임상 검사 기술(PACES) 루브릭 세부사항의 실제 평가.

## 부록 A Appendix

\begin{table}
\begin{tabular}{l c c c} \hline \hline

[MISSING_PAGE_EMPTY:29]

### 자기 비판 후 시뮬레이션된 대화 예제

**그림 A.1 \(|\) 시뮬레이션된 대화 및 자체 재생 비판 예** 내부 루프 자체 재생 중에 제공된 컨텍스트 내 피드백에 따라 AMIE가 동작을 수정하는 예이며, 이 비판이 시뮬레이션된 대화에서 AMIE의 동작에 영향을 미칠 수 있는 방법을 보여 줍니다. 우리는 그림 A.19에서 이 프로세스가 평균적으로 4개의 PACES 임상 기준 세트에서 개선된 시뮬레이션 대화 품질을 초래한다는 것을 보여준다. 이것은 반복 피드백의 단일 라운드의 하나의 예비 예이며 전체 시뮬레이션된 대화 프로세스를 반영하지 않는다는 점에 유의한다. 예를 들어, 이번 피드백 라운드에서 AMIE 비판은 관리 권장 사항이 심각성을 측정하고 원인이나 후유증에 대해 더 배제하기 위해 주로 의사의 직접 평가를 포함해야 한다는 것을 식별하지 못했다.

[MISSING_PAGE_EMPTY:31]

그림 A.4: **전문 의사 평가를 위한 인터페이스** 입니다.

### OSCE 에이전트를 사용한 예제 컨설팅

다음은 각각 AMIE 및 PCP의 동일한 시나리오 팩 및 환자 행위자에 대한 두 가지 예제 상담이다.

#### 예제 AMIE 상담

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통
6. Costochonditis
7. 소화성 배뇨관 질환
8. 담낭질환(담도산통)
9. 공황적 공격성
10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통

6. Costochonditis
7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 스테이블 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통

6. Costochonditis

7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통

6. Costochonditis

7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통

6. Costochonditis

7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 스테이블 안자나
4. 위식도역류질환(GERD)

5. 비정형 흉통

6. Costochondritis

7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 안정한 안자나
4. 위식도역류질환(GERD)
5. 비정형 흉통

6. Costochondritis

7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's Top 10 DDx:**

1. 불안정한 안자나
2. 급성심근경색
3. 스테이블 안자나
4. 위식도역류질환(GERD)

5. 비정형 흉통

6. Costochondritis

7. 소화성 배뇨관 질환

8. 담낭질환(담도산통)
9. 공황적 공격성

10. 폐색전증

**AMIE's

### 예제 PCP 상담

**그림 A.6 \(|\) PCP 예제 상담.** OSCE 연구 중에 환자 행위자가 PCP와 대화한 예제입니다.

### 매칭 정도에 따른 DDx Top-k 정확도

OSCE 연구에서 전문가는 다음 질문을 받았다.

**질문**: 의사 감별 진단(DDx)이 답변 키에서 PROBABLE DIAGNOSIS를 포함하는 데 얼마나 근접했습니까?

* **(관련 없음)** DDx의 어떤 것도 가능한 진단과 관련이 없습니다.
* **(다소 관련)** DDx에는 관련 있지만 가능한 진단을 결정하는 데 도움이 될 것 같지 않은 내용이 포함되어 있습니다.
* **(관련)** DDx에는 밀접하게 관련되어 있으며 가능한 진단을 결정하는 데 도움이 되었을 수 있는 항목이 포함되어 있습니다.
* **(극도로 관련됨)** DDx에는 매우 가깝지만 가능한 진단과 정확히 일치하지 않는 항목이 포함되어 있습니다.
* **(Exact Match)** DDx에는 가능한 진단이 포함됩니다.

여기서는 각 감별에 대해 전문가가 이 질문에 대한 답변에서 일치도가 지정된 일치도만큼 가깝다고 지적한 경우에만 일치 진단으로 간주하는 그라운드 트루스에 대한 다양한 일치도에 대한 절제 분석을 제시한다. 이 백서의 다른 모든 전문가 등급 DDx 평가는 정확도를 계산할 때 "관련" 임계값을 사용했다. DDx 정확도에서 AMIE와 PCP 간의 차이는 일치하는 수준 "Relevant", "Extremely Relevant" 및 "Exact Match"에서 k의 모든 값에 대해 통계적으로 유의했다.

그림 A.7: **전문가는 일치 정도에 따라 DDx 정확도를 평가했습니다.** (**a**) 전문가는 AMIE에서 수행한 상담에 대해 DDx Top-10 정확도를 평가했습니다. (**b**) 전문가는 PCP에서 수행한 상담에 대해 DDx 상위 10개 정확도를 평가했다. "Relevant", "Extremely Relevant" 및 "Exact Match" 수준의 경우 AMIE와 PCP DDx 정확도 간의 차이는 모든 k에 대해 통계적으로 유의하다(n=10,000 및 FDR 보정이 있는 부트스트랩). "다소 관련" 수준의 차이는 통계적으로 유의하지 않다.

### 특수성에 의한 DDx Top-k 정확도

그림 A.8은 전문가 등급을 기반으로 각 전문 분야에 대한 AMIE 및 PCP에 의해 달성된 DDx 정확도를 보여준다. 구체적으로, 우리는 AMIE의 성능이 모든 전문 분야에 대한 PCP 성능과 일치하거나 능가하는 것을 관찰했다.

그림 A.8: **전문가는 시나리오 전문 분야별 DDx 정확도를 평가했습니다.* * 각 전문 분야의 시나리오에 대한 상위 1/3/5/10 정확도입니다. 정확도는 지상 진실과 관련하여 AMIE 및 PCP 차등 진단에 대한 전문가 등급을 기반으로 한다. OSCE 제제당 대화의 수: 심장학(29), 위장관학(31), 내과(14), 신경학(30), 호흡기(30), OBGYN/비뇨기과(15).

### DDx에 대한 자동 평가

여기서는 자동 평가 방법으로 계산된 top-k DDx 정확도를 보고한다. AMIE 및 PCP에 의해 생성된 DDx 목록의 각 DDx에 대해 Med-PaLM 2를 사용하여 지상 진실 진단이 차등 진단 목록의 상위 k 위치 내에 나타나는지 여부를 결정했다. 예측 및 레이블이 주어지면 자동 평가자는 다음 질문과 함께 Med-PaLM 2를 프롬프트하여 일치 여부를 계산합니다.

**DDx Auto-evaluation Prompt**

우리의 예측 진단이 정확한가(Y/N)? 예측된 진단이 더 구체적이어도 괜찮습니다.

예측 진단: 예측, 참 진단: 레이블

Answer [Y/N]:

#### a.7.1 자동 평가를 통한 DDx 정확도 재현

자동 평가기를 통해 얻은 전체 성능 경향은 그림 A.9와 같이 계산된 정확도 값의 한계 차이에도 불구하고 그림 3의 전문가 평가와 잘 일치한다. 이러한 결과는 자동 평가기가 전문가 평가자에게 유효한 대리인임을 보여준다.

그림 A.9: **자동 평가는 모든 경우에 DDx 정확도를 평가했습니다.** (**a**) 지상 진실과 관련하여 AMIE 및 PCP의 Top-k 자동 평가 등급입니다. \(k>2\)에 대해 유의한(FDR 보정 포함)입니다. (**b**) 승인된 차등과 관련하여 AMIE 및 PCP의 Top-k 자동 평가 등급. (k>4\).

#### A.7.2 AMIE DDx 정확도 AMIE 및 PCP 상담

자체 상담을 기반으로 한 AMIE의 진단 정확도와 DDx 자동 평가기를 사용하여 해당 PCP 상담에서 생성된 정확도를 비교했다. 그림 A.10의 결과는 AMIE가 자체 대화에서 정보를 처리했는지 PCP의 대화에서 정보를 처리했는지 여부에 관계없이 진단 품질이 일관되게 유지되었음을 보여주었다.

그림 A.10 \(|\)** 자동 평가는 PCP 및 AMIE 상담에서 AMIE 생성 차동 진단에 대한 DDx 정확도를 평가했다.** AMIE는 PCP 및 AMIE 상담 모두에서 DDx를 생성하도록 요청되었습니다. (**a**) AMIE 및 PCP 상담에 대한 AMIE DDx의 Top-k 자동 평가 등급은 사실 관계와 관련이 있습니다. 통계적으로 유의한 차이는 없습니다. (**b**) 승인된 차등과 관련하여 AMIE 및 PCP 상담에 대한 AMIE DDx의 Top-k 자동 평가 등급. 통계적으로 유의한 차이는 없습니다.

### A.7.3 DDx 대화 전환 함수로서의 정확도

**단어 및 회전의 분포.** 그림 A.11은 OSCE 대화에 대한 단어 및 회전의 분포를 보여줍니다. 환자 행위자의 말과 돌림의 수가 집단 간 일관적이기 때문에 어느 대리인도 진단을 내리는 데 사용되는 정보의 양 측면에서 부당이득을 갖지 못한다. 그러나 AMIE는 전문가의 질적 평가에 영향을 미쳤을 수 있는 응답에서 훨씬 더 장황하다는 점에 유의하는 것이 중요하다.

**회전 수별 정확도** 여기서 AMIE 생성 차동 진단의 자동 평가를 회전 수의 함수로 표시했다. 우리는 첫 번째 \(T\) 회전으로 대화를 잘랐고, 이 잘린 대화로 AMIE에게 DDx를 생성하도록 요청했다. AMIE 및 PCP 대화 모두에 대해 AMIE의 평균 진단 정확도가 10회전 이내에 안정되기 시작했으며 추가 정보 수집은 그림 A.12와 같이 진단 성능에 대한 수익 감소를 보였다.

**그림 A.12 \(|\) 자동 평가는 모델에 제공된 상담 전환의 함수로 DDx(top-3) 정확도를 평가했다. (a) 지상 진실과 관련하여 AMIE 및 PCP 상담 상의 AMIE DDx에 대한 턴 수의 함수로서 Top-3 자동 평가 DDx 정확도. (b) 승인된 차등과 관련하여 AMIE 및 PCP 상담 상의 AMIE DDx에 대한 턴 수의 함수로서 Top-3 자동 평가 DDx 정확도. 통계적으로 유의한 차이는 없습니다.**

### 위치별 DDx 정확도

**위치별 정확도.** 캐나다에서 수행된 67개 시나리오와 인도에서 수행된 82개 시나리오에 대한 전문가 등급을 비교했다.

**공유 시나리오** 다른 위치에서 40개의 시나리오를 반복했습니다. 즉, 원래 캐나다에서 실행한 경우 인도와 그 반대로 실행했습니다. 여기에는 영국의 모든 시나리오와 인도의 26개 시나리오가 포함되었다. 이러한 대화에는 전문가 등급이 없었기 때문에 대신 자동 평가를 활용하여 생성된 차등 진단을 비교하고 OSCE 위치의 영향을 제거했다.

**결과.** 인도보다 캐나다에서 AMIE에 대해 더 높은 평균 진단 성능을 관찰했다(그림 A.13 참조). 그러나 두 연구 위치에서 수행된 시나리오를 비교할 때 AMIE와 PCP 성능이 연구 위치에 관계없이 일관되게 유지되는 것을 관찰했으며(그림 A.14 참조), 관찰된 성능 변화가 환자 행위자 또는 임상의 평가자 차이로 인한 것이 아니라 대신 각 위치에서 시나리오의 난이도 수준의 고유한 차이에 기인할 수 있음을 시사한다.

그림 A.13: **전문가는 위치별 DDx 정확도를 평가했다. (a)** 캐나다에서 수행된 67건의 사실 관계에 대한 AMIE 및 PCP의 DDx 전문가 등급. 모든 k 위치에서 정확도는 FDR 보정과 함께 중요하다. (b) 인도에서 수행된 82건의 사례에 대한 지상 진실에 대한 AMIE 및 PCP의 전문가 DDx 등급. 추세는 캐나다와 동일하지만 AMIE와 PCP의 차이는 FDR 보정에서 통계적으로 유의하지 않다.

그림 A.14: **자동 평가에서는 두 테스트 위치에서 수행된 시나리오에 대해 DDx 정확도를 평가했습니다. (a)** 자동 평가는 두 위치에서 수행된 40개 시나리오 세트에 대해 AMIE의 top-k DDx 성능을 평가했다. (b) 자동 평가는 두 위치에서 수행된 40개의 시나리오 세트에 대해 PCP의 top-k DDx 성능을 평가했다.

### 모델 기반 정성적 기준의 자동 평가

OSCE 평가 프레임워크에서 임상 기준에 대한 전문가의 등급을 정확하게 모방하기 위해 AMIE 모델을 활용하여 이러한 정성적 기준을 얼마나 잘 예시했는지에 따라 1에서 5까지의 대화를 점수화하는 모델 기반 자동 평가 절차를 개발했다. 우리는 처음에 PACES 기준의 4가지 임상 축 하위 집합에 초점을 맞췄지만(표 A.2 참조), 이 절차는 다른 등급으로 쉽게 확장될 수 있다.

이 연구에서 AMIE와 PCP가 제작한 298개의 대화를 사용하여 자동 평가 절차를 사용하여 이 4가지 기준에 대한 전문가 등급 결과를 확증했다. 자동 평가 순위가 이러한 전문가 등급과 잘 일치함을 검증했다(그림 A.17 및 A.18 참조). 또한, 이 반복 프로세스가 대화 품질의 측정 가능한 개선을 가져왔는지 여부를 테스트하기 위해 내부 루프 셀프-플레이 절차를 통해 생성된 시뮬레이션된 대화들에 이를 적용했다(도 A.19 참조).

**임상 기준의 자동 평가를 위한 자체 CoT 절차** 사용한 자동 평가 절차는 AMIE 자체에서 PACES 기준의 선택된 하위 집합에 대한 대화를 평가하도록 촉구하는 2단계 프로세스였다(표 A.2 참조).

1. 먼저, 우리는 AMIE가 여러 대화의 좋은 측면과 나쁜 측면을 요약하도록 유도하고 1과 5 사이의 제공된 인간 등급에 대한 설명을 제공한다(도 A.15 참조).
2. 다음으로, 우리는 새로운 대화를 평가하고 평가하기 위해 5-샷 프롬프트에서 각각의 대화와 함께 이러한 자체 생성된 설명을 예로 사용했다. 이 몇 번의 프롬프트에는 5점 등급 척도의 각 점에 대한 하나의 예가 포함되었다(그림 A.16 참조).

두 프롬프트에서 우리는 특정 기준에 대한 평가 척도와 전문가에서 파생된 선행 또는 악행 사례를 포함하여 표 A.2에 표시된 것과 일치했다. 이 프롬프트 방법을 인간 등급에 대한 그럴듯한 추론이 모델 자체에서 파생되었기 때문에 self-CoT(생각 사슬) [1]이라고 불렀다.

**순위 계약** OSCE 대화 상자의 전문가 순위와의 일치를 정량화하여 자동 평가 방법을 평가했습니다. 우리는 연구에서 149개의 대화 쌍으로 분석을 제한했다. 따라서 각 쌍은 AMIE 대화와 동일한 환자 행위자와의 PCP 대화로 구성되었으며 동일한 전문가에 의해 평가되었다. 두 대화 쌍의 경우 세 가지 가능성이 있었는데 첫 번째 대화는 두 번째 대화보다 더 좋게 평가되거나 동등하게 평가되거나 첫 번째 대화는 두 번째 대화보다 더 나쁘게 평가되었다. 순위 합의는 전문가 순위가 자동 평가 등급에 의해 보존되는 대화 쌍의 비율로 정의했다. 예를 들어, 각 방법이 할당된 정확한 점수에 관계없이 전문가도 AMIE의 대화를 더 좋게 평가하면 자동 평가가 AMIE의 대화를 PCP의 대화보다 더 좋게 평가할 때 정확하다고 계산했다.

**자동 평가 프롬프팅 전략** 순위 일치 메트릭을 사용하여 2단계 프롬프트의 효과를 제거하고 5단계 프롬프트(즉, 1단계 드롭), 지원 예제의 순서가 매번 무작위화된 5단계 자체 CoT 프롬프트, 등급 척도 설명 자체만 사용하여 0단계 프롬프트와 같은 다른 방법과 비교했다(그림 A.17 참조). 모든 방법이 기회 수준을 능가했으며 이 차이는 미미했지만 2단계 프로세스가 일반적으로 다른 방법을 능가했다. 셀프-CoT 프롬프트에서 예를 섞는 것은 평균적으로 차이가 없었다.

**벤치마킹 자동 평가** 자동 평가가 전문가 선호도와 일치할 때 무작위 추측보다 훨씬 우수했지만 결과 성능이 충분한지 여부는 불분명했다. 이를 테스트하기 위해 두 번째 전문가가 각각 139개의 대화 쌍을 평가했으며 이 하위 집합에 대한 두 전문가의 순위 일치를 계산했다(그림 A.18 참조). 우리는 자동 평가가 첫 번째 전문가의 순위를 예측하는 데 대체 전문가만큼 정확하다는 것을 관찰했으며, 이는 이러한 기준에 대한 자동 평가를 활용하는 것이 유용함을 시사한다.

**자가 플레이 대화 평가** 자가 플레이 비판을 통해 정제되기 전과 후에 1,142개의 대화(공통 조건에서 파생됨)에 자동 평가 절차를 적용했습니다. 평균적으로 비판/수정 반복 후 정제된 대화가 모든 기준 A.19에 걸쳐 원래 기준 대화보다 높게 평가되었음을 입증했다.

그림 A.15 **대화 및 인간 등급이 주어진 설명 생성 프롬프트** AMIE가 특정 대화의 좋은 측면과 나쁜 측면을 요약하고 1에서 5 사이의 주어진 인간 등급에 대한 설명을 제공하도록 프롬프트되었습니다.

그림 A.15 **대화 및 인간 등급에 대한 설명 생성 프롬프트**

그림 A.16: **정성적 기준의 자동 평가에 대한 프롬프트** 새 대화를 평가하고 평가하기 위해 각각의 대화와 함께 자체 생성된 설명을 사용하여 몇 개의 샷 프롬프트를 구성했습니다. 이 몇 번의 프롬프트에는 5점 평점 척도의 각 점에 대한 하나의 예가 포함되었다.

그림 A.17: 다양한 자동 평가 프롬프트 기술을 비교하여 149개의 대화 쌍 모두의 전문가 등급에 대한 순위 일치. 임상 기준의 자동 평가를 위해 자체 CoT 기술을 활용하기로 결정했다.

그림 A.18: 자가-CoT 자동 평가 기법과 비교하여 대체 전문가에 대한 139개의 대화 쌍(다중 전문가 등급이 없는 경우는 제외)의 전문가 등급에 대한 순위 합의. 첫 번째 전문가에 대한 자동 평가 합의는 전문가 간 합의에 필적한다. 검정색 점선은 AMIE와 PCP 대화의 무작위 순위에 대한 순위 일치를 나타내고 녹색 점선은 각 기준에 대한 전문가 선호도의 분포에 따라 무작위로 추측하는 전략으로 순위 일치를 나타낸다.

[MISSING_PAGE_EMPTY:45]

## References

* [1] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., _et al._ Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_**35**, 24824-24837 (2022).
* [2] Google. _PaLM 2 Technical Report_[https://ai.google/static/documents/palm2techreport.pdf](https://ai.google/static/documents/palm2techreport.pdf). 2023.
